[{"id": "1812.00020", "submitter": "Jingwei Huang", "authors": "Jingwei Huang, Haotian Zhang, Li Yi, Thomas Funkhouser, Matthias\n  Nie{\\ss}ner, Leonidas Guibas", "title": "TextureNet: Consistent Local Parametrizations for Learning from\n  High-Resolution Signals on Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce, TextureNet, a neural network architecture designed to extract\nfeatures from high-resolution signals associated with 3D surface meshes (e.g.,\ncolor texture maps). The key idea is to utilize a 4-rotational symmetric\n(4-RoSy) field to define a domain for convolution on a surface. Though 4-RoSy\nfields have several properties favorable for convolution on surfaces (low\ndistortion, few singularities, consistent parameterization, etc.), orientations\nare ambiguous up to 4-fold rotation at any sample point. So, we introduce a new\nconvolutional operator invariant to the 4-RoSy ambiguity and use it in a\nnetwork to extract features from high-resolution signals on geodesic\nneighborhoods of a surface. In comparison to alternatives, such as PointNet\nbased methods which lack a notion of orientation, the coherent structure given\nby these neighborhoods results in significantly stronger features. As an\nexample application, we demonstrate the benefits of our architecture for 3D\nsemantic segmentation of textured 3D meshes. The results show that our method\noutperforms all existing methods on the basis of mean IoU by a significant\nmargin in both geometry-only (6.4%) and RGB+Geometry (6.9-8.2%) settings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 19:01:09 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 05:36:09 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Huang", "Jingwei", ""], ["Zhang", "Haotian", ""], ["Yi", "Li", ""], ["Funkhouser", "Thomas", ""], ["Nie\u00dfner", "Matthias", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1812.00033", "submitter": "Tong Yu", "authors": "Tong Yu, Didier Mutter, Jacques Marescaux, Nicolas Padoy", "title": "Learning from a tiny dataset of manual annotations: a teacher/student\n  approach for surgical phase recognition", "comments": "Accepted at IPCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vision algorithms capable of interpreting scenes from a real-time video\nstream are necessary for computer-assisted surgery systems to achieve\ncontext-aware behavior. In laparoscopic procedures one particular algorithm\nneeded for such systems is the identification of surgical phases, for which the\ncurrent state of the art is a model based on a CNN-LSTM. A number of previous\nworks using models of this kind have trained them in a fully supervised manner,\nrequiring a fully annotated dataset. Instead, our work confronts the problem of\nlearning surgical phase recognition in scenarios presenting scarce amounts of\nannotated data (under 25% of all available video recordings). We propose a\nteacher/student type of approach, where a strong predictor called the teacher,\ntrained beforehand on a small dataset of ground truth-annotated videos,\ngenerates synthetic annotations for a larger dataset, which another model - the\nstudent - learns from. In our case, the teacher features a novel CNN-biLSTM-CRF\narchitecture, designed for offline inference only. The student, on the other\nhand, is a CNN-LSTM capable of making real-time predictions. Results for\nvarious amounts of manually annotated videos demonstrate the superiority of the\nnew CNN-biLSTM-CRF predictor as well as improved performance from the CNN-LSTM\ntrained using synthetic labels generated for unannotated videos. For both\noffline and online surgical phase recognition with very few annotated\nrecordings available, this new teacher/student strategy provides a valuable\nperformance improvement by efficiently leveraging the unannotated data.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 19:50:05 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 16:04:22 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 14:22:43 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Yu", "Tong", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1812.00037", "submitter": "Ronald Yu", "authors": "Bo Sun, Nian-hsuan Tsai, Fangchen Liu, Ronald Yu, Hao Su", "title": "Adversarial Defense by Stratified Convolutional Sparse Coding", "comments": "Published at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adversarial defense method that achieves state-of-the-art\nperformance among attack-agnostic adversarial defense methods while also\nmaintaining robustness to input resolution, scale of adversarial perturbation,\nand scale of dataset size. Based on convolutional sparse coding, we construct a\nstratified low-dimensional quasi-natural image space that faithfully\napproximates the natural image space while also removing adversarial\nperturbations. We introduce a novel Sparse Transformation Layer (STL) in\nbetween the input image and the first layer of the neural network to\nefficiently project images into our quasi-natural image space. Our experiments\nshow state-of-the-art performance of our method compared to other\nattack-agnostic adversarial defense methods in various adversarial settings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 20:01:15 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 16:06:41 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Sun", "Bo", ""], ["Tsai", "Nian-hsuan", ""], ["Liu", "Fangchen", ""], ["Yu", "Ronald", ""], ["Su", "Hao", ""]]}, {"id": "1812.00087", "submitter": "Da Zhang", "authors": "Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, Larry S. Davis", "title": "MAN: Moment Alignment Network for Natural Language Moment Retrieval via\n  Iterative Graph Adjustment", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research strives for natural language moment retrieval in long,\nuntrimmed video streams. The problem is not trivial especially when a video\ncontains multiple moments of interests and the language describes complex\ntemporal dependencies, which often happens in real scenarios. We identify two\ncrucial challenges: semantic misalignment and structural misalignment. However,\nexisting approaches treat different moments separately and do not explicitly\nmodel complex moment-wise temporal relations. In this paper, we present Moment\nAlignment Network (MAN), a novel framework that unifies the candidate moment\nencoding and temporal structural reasoning in a single-shot feed-forward\nnetwork. MAN naturally assigns candidate moment representations aligned with\nlanguage semantics over different temporal locations and scales. Most\nimportantly, we propose to explicitly model moment-wise temporal relations as a\nstructured graph and devise an iterative graph adjustment network to jointly\nlearn the best structure in an end-to-end manner. We evaluate the proposed\napproach on two challenging public benchmarks DiDeMo and Charades-STA, where\nour MAN significantly outperforms the state-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 23:04:10 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 23:15:57 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Zhang", "Da", ""], ["Dai", "Xiyang", ""], ["Wang", "Xin", ""], ["Wang", "Yuan-Fang", ""], ["Davis", "Larry S.", ""]]}, {"id": "1812.00090", "submitter": "Bichen Wu", "authors": "Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda,\n  Kurt Keutzer", "title": "Mixed Precision Quantization of ConvNets via Differentiable Neural\n  Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in network quantization has substantially reduced the time and\nspace complexity of neural network inference, enabling their deployment on\nembedded and mobile devices with limited computational and memory resources.\nHowever, existing quantization methods often represent all weights and\nactivations with the same precision (bit-width). In this paper, we explore a\nnew dimension of the design space: quantizing different layers with different\nbit-widths. We formulate this problem as a neural architecture search problem\nand propose a novel differentiable neural architecture search (DNAS) framework\nto efficiently explore its exponential search space with gradient-based\noptimization. Experiments show we surpass the state-of-the-art compression of\nResNet on CIFAR-10 and ImageNet. Our quantized models with 21.1x smaller model\nsize or 103.9x lower computational cost can still outperform baseline quantized\nor even full precision models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 23:15:45 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Wu", "Bichen", ""], ["Wang", "Yanghan", ""], ["Zhang", "Peizhao", ""], ["Tian", "Yuandong", ""], ["Vajda", "Peter", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1812.00099", "submitter": "Kush Varshney", "authors": "Vidya Muthukumar, Tejaswini Pedapati, Nalini Ratha, Prasanna\n  Sattigeri, Chai-Wah Wu, Brian Kingsbury, Abhishek Kumar, Samuel Thomas,\n  Aleksandra Mojsilovic, and Kush R. Varshney", "title": "Understanding Unequal Gender Classification Accuracy from Face Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work shows unequal performance of commercial face classification\nservices in the gender classification task across intersectional groups defined\nby skin type and gender. Accuracy on dark-skinned females is significantly\nworse than on any other group. In this paper, we conduct several analyses to\ntry to uncover the reason for this gap. The main finding, perhaps surprisingly,\nis that skin type is not the driver. This conclusion is reached via stability\nexperiments that vary an image's skin type via color-theoretic methods, namely\nluminance mode-shift and optimal transport. A second suspect, hair length, is\nalso shown not to be the driver via experiments on face images cropped to\nexclude the hair. Finally, using contrastive post-hoc explanation techniques\nfor neural networks, we bring forth evidence suggesting that differences in\nlip, eye and cheek structure across ethnicity lead to the differences. Further,\nlip and eye makeup are seen as strong predictors for a female face, which is a\ntroubling propagation of a gender stereotype.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 23:47:52 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Muthukumar", "Vidya", ""], ["Pedapati", "Tejaswini", ""], ["Ratha", "Nalini", ""], ["Sattigeri", "Prasanna", ""], ["Wu", "Chai-Wah", ""], ["Kingsbury", "Brian", ""], ["Kumar", "Abhishek", ""], ["Thomas", "Samuel", ""], ["Mojsilovic", "Aleksandra", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1812.00101", "submitter": "Guo Lu", "authors": "Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, Zhiyong Gao", "title": "DVC: An End-to-end Deep Video Compression Framework", "comments": "Accepted by CVPR 2019. Project page https://github.com/GuoLusjtu/DVC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional video compression approaches use the predictive coding\narchitecture and encode the corresponding motion information and residual\ninformation. In this paper, taking advantage of both classical architecture in\nthe conventional video compression method and the powerful non-linear\nrepresentation ability of neural networks, we propose the first end-to-end\nvideo compression deep model that jointly optimizes all the components for\nvideo compression. Specifically, learning based optical flow estimation is\nutilized to obtain the motion information and reconstruct the current frames.\nThen we employ two auto-encoder style neural networks to compress the\ncorresponding motion and residual information. All the modules are jointly\nlearned through a single loss function, in which they collaborate with each\nother by considering the trade-off between reducing the number of compression\nbits and improving quality of the decoded video. Experimental results show that\nthe proposed approach can outperform the widely used video coding standard\nH.264 in terms of PSNR and be even on par with the latest standard H.265 in\nterms of MS-SSIM. Code is released at https://github.com/GuoLusjtu/DVC.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 23:55:31 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 10:41:00 GMT"}, {"version": "v3", "created": "Sun, 7 Apr 2019 05:41:45 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Lu", "Guo", ""], ["Ouyang", "Wanli", ""], ["Xu", "Dong", ""], ["Zhang", "Xiaoyun", ""], ["Cai", "Chunlei", ""], ["Gao", "Zhiyong", ""]]}, {"id": "1812.00104", "submitter": "Mohamed Elfeki", "authors": "Mohamed Elfeki, Krishna Regmi, Shervin Ardeshir, and Ali Borji", "title": "From Third Person to First Person: Dataset and Baselines for Synthesis\n  and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-person (egocentric) and third person (exocentric) videos are\ndrastically different in nature. The relationship between these two views have\nbeen studied in recent years, however, it has yet to be fully explored. In this\nwork, we introduce two datasets (synthetic and natural/real) containing\nsimultaneously recorded egocentric and exocentric videos. We also explore\nrelating the two domains (egocentric and exocentric) in two aspects. First, we\nsynthesize images in the egocentric domain from the exocentric domain using a\nconditional generative adversarial network (cGAN). We show that with enough\ntraining data, our network is capable of hallucinating how the world would look\nlike from an egocentric perspective, given an exocentric video. Second, we\naddress the cross-view retrieval problem across the two views. Given an\negocentric query frame (or its momentary optical flow), we retrieve its\ncorresponding exocentric frame (or optical flow) from a gallery set. We show\nthat using synthetic data could be beneficial in retrieving real data. We show\nthat performing domain adaptation from the synthetic domain to the natural/real\ndomain, is helpful in tasks such as retrieval. We believe that the presented\ndatasets and the proposed baselines offer new opportunities for further\nresearch in this direction. The code and dataset are publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 00:10:03 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Elfeki", "Mohamed", ""], ["Regmi", "Krishna", ""], ["Ardeshir", "Shervin", ""], ["Borji", "Ali", ""]]}, {"id": "1812.00108", "submitter": "Mohamed Elfeki", "authors": "Mohamed Elfeki, Aidean Sharghi, Srikrishna Karanam, Ziyan Wu, and Ali\n  Borji", "title": "Multi-Stream Dynamic Video Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With vast amounts of video content being uploaded to the Internet every\nminute, video summarization becomes critical for efficient browsing, searching,\nand indexing of visual content. Nonetheless, the spread of social and\negocentric cameras creates an abundance of sparse scenarios captured by several\ndevices, and ultimately required to be jointly summarized. In this paper, we\ndiscuss the problem of summarizing videos recorded simultaneously by several\ndynamic cameras that intermittently share the field of view. We present a\nrobust framework that (a) identifies a diverse set of important events among\nmoving cameras that often are not capturing the same scene, and (b) selects the\nmost representative view(s) at each event to be included in a universal\nsummary. Due to the lack of an applicable alternative, we collected a new\nmulti-view egocentric dataset, Multi-Ego. Our dataset is recorded\nsimultaneously by three cameras, covering a wide variety of real-life\nscenarios. The footage is annotated by multiple individuals under various\nsummarization configurations, with a consensus analysis ensuring a reliable\nground truth. We conduct extensive experiments on the compiled dataset in\naddition to three other standard benchmarks that show the robustness and the\nadvantage of our approach in both supervised and unsupervised settings.\nAdditionally, we show that our approach learns collectively from data of varied\nnumber-of-views and orthogonal to other summarization methods, deeming it\nscalable and generic. Our materials are made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 00:44:12 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 01:30:10 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 21:21:41 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Elfeki", "Mohamed", ""], ["Sharghi", "Aidean", ""], ["Karanam", "Srikrishna", ""], ["Wu", "Ziyan", ""], ["Borji", "Ali", ""]]}, {"id": "1812.00123", "submitter": "Chenglin Yang", "authors": "Chenglin Yang, Lingxi Xie, Chi Su, Alan L. Yuille", "title": "Snapshot Distillation: Teacher-Student Optimization in One Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing a deep neural network is a fundamental task in computer vision,\nyet direct training methods often suffer from over-fitting. Teacher-student\noptimization aims at providing complementary cues from a model trained\npreviously, but these approaches are often considerably slow due to the\npipeline of training a few generations in sequence, i.e., time complexity is\nincreased by several times.\n  This paper presents snapshot distillation (SD), the first framework which\nenables teacher-student optimization in one generation. The idea of SD is very\nsimple: instead of borrowing supervision signals from previous generations, we\nextract such information from earlier epochs in the same generation, meanwhile\nmake sure that the difference between teacher and student is sufficiently large\nso as to prevent under-fitting. To achieve this goal, we implement SD in a\ncyclic learning rate policy, in which the last snapshot of each cycle is used\nas the teacher for all iterations in the next cycle, and the teacher signal is\nsmoothed to provide richer information. In standard image classification\nbenchmarks such as CIFAR100 and ILSVRC2012, SD achieves consistent accuracy\ngain without heavy computational overheads. We also verify that models\npre-trained with SD transfers well to object detection and semantic\nsegmentation in the PascalVOC dataset.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 02:08:38 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Yang", "Chenglin", ""], ["Xie", "Lingxi", ""], ["Su", "Chi", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1812.00124", "submitter": "Jiyang Gao", "authors": "JIyang Gao, Jiang Wang, Shengyang Dai, Li-Jia Li, Ram Nevatia", "title": "NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object\n  Detection", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The labeling cost of large number of bounding boxes is one of the main\nchallenges for training modern object detectors. To reduce the dependence on\nexpensive bounding box annotations, we propose a new semi-supervised object\ndetection formulation, in which a few seed box level annotations and a large\nscale of image level annotations are used to train the detector. We adopt a\ntraining-mining framework, which is widely used in weakly supervised object\ndetection tasks. However, the mining process inherently introduces various\nkinds of labelling noises: false negatives, false positives and inaccurate\nboundaries, which can be harmful for training the standard object detectors\n(e.g. Faster RCNN). We propose a novel NOise Tolerant Ensemble RCNN (NOTE-RCNN)\nobject detector to handle such noisy labels. Comparing to standard Faster RCNN,\nit contains three highlights: an ensemble of two classification heads and a\ndistillation head to avoid overfitting on noisy labels and improve the mining\nprecision, masking the negative sample loss in box predictor to avoid the harm\nof false negative labels, and training box regression head only on seed\nannotations to eliminate the harm from inaccurate boundaries of mined bounding\nboxes. We evaluate the methods on ILSVRC 2013 and MSCOCO 2017 dataset; we\nobserve that the detection accuracy consistently improves as we iterate between\nmining and training steps, and state-of-the-art performance is achieved.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 02:14:57 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Gao", "JIyang", ""], ["Wang", "Jiang", ""], ["Dai", "Shengyang", ""], ["Li", "Li-Jia", ""], ["Nevatia", "Ram", ""]]}, {"id": "1812.00137", "submitter": "Meng Li", "authors": "Meng Li, Yan Zhang, Haicheng She, Jinqiong Zhou, Jia Jia, Danmei He,\n  Li Zhang", "title": "Automated segmentaiton and classification of arterioles and venules\n  using Cascading Dilated Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The change of retinal vasculature is an early sign of many vascular and\nsystematic diseases, such as diabetes and hypertension. Different behaviors of\nretinal arterioles and venules form an important metric to measure the disease\nseverity. Therefore, an accurate classification of arterioles and venules is of\ngreat necessity. In this work, we propose a novel architecture of deep\nconvolutional neural network for segmenting and classifying arterioles and\nvenules on retinal fundus images. This network takes the original color fundus\nimage as inputs and multi-class labels as outputs. We adopt the\nencoding-decoding structure (Unet) as the backbone network of our proposed\nmodel. To improve the classification accuracy, we develop a special encoding\npath that couples InceptionV4 modules and Cascading Dilated Convolutions (CDCs)\non top of the backbone network. The model is thus able to extract and fuse\nhigh-level semantic features from multi-scale receptive fields. The proposed\nmethod has outperformed the previous state-of-the-art method on DRIVE dataset\nwith an accuracy of 0.955 $\\pm$ 0.002.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 04:05:14 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Li", "Meng", ""], ["Zhang", "Yan", ""], ["She", "Haicheng", ""], ["Zhou", "Jinqiong", ""], ["Jia", "Jia", ""], ["He", "Danmei", ""], ["Zhang", "Li", ""]]}, {"id": "1812.00155", "submitter": "Ding Jian", "authors": "Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, Qikai Lu", "title": "Learning RoI Transformer for Detecting Oriented Objects in Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in aerial images is an active yet challenging task in\ncomputer vision because of the birdview perspective, the highly complex\nbackgrounds, and the variant appearances of objects. Especially when detecting\ndensely packed objects in aerial images, methods relying on horizontal\nproposals for common object detection often introduce mismatches between the\nRegion of Interests (RoIs) and objects. This leads to the common misalignment\nbetween the final object classification confidence and localization accuracy.\nAlthough rotated anchors have been used to tackle this problem, the design of\nthem always multiplies the number of anchors and dramatically increases the\ncomputational complexity. In this paper, we propose a RoI Transformer to\naddress these problems. More precisely, to improve the quality of region\nproposals, we first designed a Rotated RoI (RRoI) learner to transform a\nHorizontal Region of Interest (HRoI) into a Rotated Region of Interest (RRoI).\nBased on the RRoIs, we then proposed a Rotated Position Sensitive RoI Align\n(RPS-RoI-Align) module to extract rotation-invariant features from them for\nboosting subsequent classification and regression. Our RoI Transformer is with\nlight weight and can be easily embedded into detectors for oriented object\ndetection. A simple implementation of the RoI Transformer has achieved\nstate-of-the-art performances on two common and challenging aerial datasets,\ni.e., DOTA and HRSC2016, with a neglectable reduction to detection speed. Our\nRoI Transformer exceeds the deformable Position Sensitive RoI pooling when\noriented bounding-box annotations are available. Extensive experiments have\nalso validated the flexibility and effectiveness of our RoI Transformer. The\nresults demonstrate that it can be easily integrated with other detector\narchitectures and significantly improve the performances.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 06:05:35 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ding", "Jian", ""], ["Xue", "Nan", ""], ["Long", "Yang", ""], ["Xia", "Gui-Song", ""], ["Lu", "Qikai", ""]]}, {"id": "1812.00169", "submitter": "Jun-Ting Hsieh", "authors": "David Xue, Anin Sayana, Evan Darke, Kelly Shen, Jun-Ting Hsieh, Zelun\n  Luo, Li-Jia Li, N. Lance Downing, Arnold Milstein, Li Fei-Fei", "title": "Vision-Based Gait Analysis for Senior Care", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/78", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the senior population rapidly increases, it is challenging yet crucial to\nprovide effective long-term care for seniors who live at home or in senior care\nfacilities. Smart senior homes, which have gained widespread interest in the\nhealthcare community, have been proposed to improve the well-being of seniors\nliving independently. In particular, non-intrusive, cost-effective sensors\nplaced in these senior homes enable gait characterization, which can provide\nclinically relevant information including mobility level and early\nneurodegenerative disease risk. In this paper, we present a method to perform\ngait analysis from a single camera placed within the home. We show that we can\naccurately calculate various gait parameters, demonstrating the potential for\nour system to monitor the long-term gait of seniors and thus aid clinicians in\nunderstanding a patient's medical profile.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 07:41:18 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Xue", "David", ""], ["Sayana", "Anin", ""], ["Darke", "Evan", ""], ["Shen", "Kelly", ""], ["Hsieh", "Jun-Ting", ""], ["Luo", "Zelun", ""], ["Li", "Li-Jia", ""], ["Downing", "N. Lance", ""], ["Milstein", "Arnold", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1812.00194", "submitter": "Mei Wang", "authors": "Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, Yaohai Huang", "title": "Racial Faces in-the-Wild: Reducing Racial Bias by Information\n  Maximization Adaptation Network", "comments": null, "journal-ref": "ICCV 2019: 692-702", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Racial bias is an important issue in biometric, but has not been thoroughly\nstudied in deep face recognition. In this paper, we first contribute a\ndedicated dataset called Racial Faces in-the-Wild (RFW) database, on which we\nfirmly validated the racial bias of four commercial APIs and four\nstate-of-the-art (SOTA) algorithms. Then, we further present the solution using\ndeep unsupervised domain adaptation and propose a deep information maximization\nadaptation network (IMAN) to alleviate this bias by using Caucasian as source\ndomain and other races as target domains. This unsupervised method\nsimultaneously aligns global distribution to decrease race gap at domain-level,\nand learns the discriminative target representations at cluster level. A novel\nmutual information loss is proposed to further enhance the discriminative\nability of network output without label information. Extensive experiments on\nRFW, GBU, and IJB-A databases show that IMAN successfully learns features that\ngeneralize well across different races and across different databases.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 12:10:39 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 09:43:36 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Mei", ""], ["Deng", "Weihong", ""], ["Hu", "Jiani", ""], ["Tao", "Xunqiang", ""], ["Huang", "Yaohai", ""]]}, {"id": "1812.00202", "submitter": "Ziad Al-Halah", "authors": "Ziad Al-Halah, Andreas M. Lehrmann, Leonid Sigal", "title": "Traversing the Continuous Spectrum of Image Retrieval with Deep Dynamic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first work to tackle the image retrieval problem as a\ncontinuous operation. While the proposed approaches in the literature can be\nroughly categorized into two main groups: category- and instance-based\nretrieval, in this work we show that the retrieval task is much richer and more\ncomplex. Image similarity goes beyond this discrete vantage point and spans a\ncontinuous spectrum among the classical operating points of category and\ninstance similarity. However, current retrieval models are static and incapable\nof exploring this rich structure of the retrieval space since they are trained\nand evaluated with a single operating point as a target objective. Hence, we\nintroduce a novel retrieval model that for a given query is capable of\nproducing a dynamic embedding that can target an arbitrary point along the\ncontinuous retrieval spectrum. Our model disentangles the visual signal of a\nquery image into its basic components of categorical and attribute information.\nFurthermore, using a continuous control parameter our model learns to\nreconstruct a dynamic embedding of the query by mixing these components with\ndifferent proportions to target a specific point along the retrieval simplex.\nWe demonstrate our idea in a comprehensive evaluation of the proposed model and\nhighlight the advantages of our approach against a set of well-established\ndiscrete retrieval models.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 12:40:01 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2019 13:57:47 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Al-Halah", "Ziad", ""], ["Lehrmann", "Andreas M.", ""], ["Sigal", "Leonid", ""]]}, {"id": "1812.00231", "submitter": "Assaf Shocher", "authors": "Assaf Shocher, Shai Bagon, Phillip Isola, Michal Irani", "title": "InGAN: Capturing and Remapping the \"DNA\" of a Natural Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) typically learn a distribution of\nimages in a large image dataset, and are then able to generate new images from\nthis distribution. However, each natural image has its own internal statistics,\ncaptured by its unique distribution of patches. In this paper we propose an\n\"Internal GAN\" (InGAN) - an image-specific GAN - which trains on a single input\nimage and learns its internal distribution of patches. It is then able to\nsynthesize a plethora of new natural images of significantly different sizes,\nshapes and aspect-ratios - all with the same internal patch-distribution (same\n\"DNA\") as the input image. In particular, despite large changes in global\nsize/shape of the image, all elements inside the image maintain their local\nsize/shape. InGAN is fully unsupervised, requiring no additional data other\nthan the input image itself. Once trained on the input image, it can remap the\ninput to any size or shape in a single feedforward pass, while preserving the\nsame internal patch distribution. InGAN provides a unified framework for a\nvariety of tasks, bridging the gap between textures and natural images.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 17:48:02 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 11:38:55 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Shocher", "Assaf", ""], ["Bagon", "Shai", ""], ["Isola", "Phillip", ""], ["Irani", "Michal", ""]]}, {"id": "1812.00235", "submitter": "Tingke Shen", "authors": "Kevin Shen, Amlan Kar, Sanja Fidler", "title": "Learning to Caption Images through a Lifetime by Asking Questions", "comments": "Fixed typos and added contribution list in intro, results remain the\n  same", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to bring artificial agents into our lives, we will need to go beyond\nsupervised learning on closed datasets to having the ability to continuously\nexpand knowledge. Inspired by a student learning in a classroom, we present an\nagent that can continuously learn by posing natural language questions to\nhumans. Our agent is composed of three interacting modules, one that performs\ncaptioning, another that generates questions and a decision maker that learns\nwhen to ask questions by implicitly reasoning about the uncertainty of the\nagent and expertise of the teacher. As compared to current active learning\nmethods which query images for full captions, our agent is able to ask pointed\nquestions to improve the generated captions. The agent trains on the improved\ncaptions, expanding its knowledge. We show that our approach achieves better\nperformance using less human supervision than the baselines on the challenging\nMSCOCO dataset.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 18:12:35 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 20:09:39 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 16:11:45 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Shen", "Kevin", ""], ["Kar", "Amlan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1812.00252", "submitter": "Georgia Chalvatzaki", "authors": "Georgia Chalvatzaki, Petros Koutras, Jack Hadfield, Xanthi S.\n  Papageorgiou, Costas S. Tzafestas and Petros Maragos", "title": "LSTM-based Network for Human Gait Stability Prediction in an Intelligent\n  Robotic Rollator", "comments": "8 pages, 4 figures accepted to ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel framework for on-line human gait stability\nprediction of the elderly users of an intelligent robotic rollator using Long\nShort Term Memory (LSTM) networks, fusing multimodal RGB-D and Laser Range\nFinder (LRF) data from non-wearable sensors. A Deep Learning (DL) based\napproach is used for the upper body pose estimation. The detected pose is used\nfor estimating the body Center of Mass (CoM) using Unscented Kalman Filter\n(UKF). An Augmented Gait State Estimation framework exploits the LRF data to\nestimate the legs' positions and the respective gait phase. These estimates are\nthe inputs of an encoder-decoder sequence to sequence model which predicts the\ngait stability state as Safe or Fall Risk walking. It is validated with data\nfrom real patients, by exploring different network architectures,\nhyperparameter settings and by comparing the proposed method with other\nbaselines. The presented LSTM-based human gait stability predictor is shown to\nprovide robust predictions of the human stability state, and thus has the\npotential to be integrated into a general user-adaptive control architecture as\na fall-risk alarm.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 19:50:56 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 11:27:43 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Chalvatzaki", "Georgia", ""], ["Koutras", "Petros", ""], ["Hadfield", "Jack", ""], ["Papageorgiou", "Xanthi S.", ""], ["Tzafestas", "Costas S.", ""], ["Maragos", "Petros", ""]]}, {"id": "1812.00253", "submitter": "Georgia Chalvatzaki", "authors": "Jack Hadfield, Georgia Chalvatzaki, Petros Koutras, Mehdi Khamassi,\n  Costas S. Tzafestas and Petros Maragos", "title": "A Deep Learning Approach for Multi-View Engagement Estimation of\n  Children in a Child-Robot Joint Attention task", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we tackle the problem of child engagement estimation while\nchildren freely interact with a robot in their room. We propose a deep-based\nmulti-view solution that takes advantage of recent developments in human pose\ndetection. We extract the child's pose from different RGB-D cameras placed\nelegantly in the room, fuse the results and feed them to a deep neural network\ntrained for classifying engagement levels. The deep network contains a\nrecurrent layer, in order to exploit the rich temporal information contained in\nthe pose data. The resulting method outperforms a number of baseline\nclassifiers, and provides a promising tool for better automatic understanding\nof a child's attitude, interest and attention while cooperating with a robot.\nThe goal is to integrate this model in next generation social robots as an\nattention monitoring tool during various CRI tasks both for Typically Developed\n(TD) children and children affected by autism (ASD).\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 19:51:16 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Hadfield", "Jack", ""], ["Chalvatzaki", "Georgia", ""], ["Koutras", "Petros", ""], ["Khamassi", "Mehdi", ""], ["Tzafestas", "Costas S.", ""], ["Maragos", "Petros", ""]]}, {"id": "1812.00273", "submitter": "Hugo Prol Pereira", "authors": "Hugo Prol, Vincent Dumoulin, Luis Herranz", "title": "Cross-Modulation Networks for Few-Shot Learning", "comments": "Accepted at NIPS 2018 Workshop on Meta-Learning. Source code\n  available at https://github.com/hprop/cross-modulation-nets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of recent successful approaches to few-shot learning relies on\nlearning an embedding space in which predictions are made by computing\nsimilarities between examples. This corresponds to combining information\nbetween support and query examples at a very late stage of the prediction\npipeline. Inspired by this observation, we hypothesize that there may be\nbenefits to combining the information at various levels of abstraction along\nthe pipeline. We present an architecture called Cross-Modulation Networks which\nallows support and query examples to interact throughout the feature extraction\nprocess via a feature-wise modulation mechanism. We adapt the Matching Networks\narchitecture to take advantage of these interactions and show encouraging\ninitial results on miniImageNet in the 5-way, 1-shot setting, where we close\nthe gap with state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 22:02:04 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Prol", "Hugo", ""], ["Dumoulin", "Vincent", ""], ["Herranz", "Luis", ""]]}, {"id": "1812.00281", "submitter": "Zhixuan Yu", "authors": "Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth Venkatesh, Jaesik\n  Park, Jihun Yu, Hyun Soo Park", "title": "HUMBI: A Large Multiview Dataset of Human Body Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new large multiview dataset called HUMBI for human body\nexpressions with natural clothing. The goal of HUMBI is to facilitate modeling\nview-specific appearance and geometry of gaze, face, hand, body, and garment\nfrom assorted people. 107 synchronized HD cameras are used to capture 772\ndistinctive subjects across gender, ethnicity, age, and physical condition.\nWith the multiview image streams, we reconstruct high fidelity body expressions\nusing 3D mesh models, which allows representing view-specific appearance using\ntheir canonical atlas. We demonstrate that HUMBI is highly effective in\nlearning and reconstructing a complete human model and is complementary to the\nexisting datasets of human body expressions with limited views and subjects\nsuch as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 23:11:22 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 00:42:23 GMT"}, {"version": "v3", "created": "Sat, 23 May 2020 01:22:21 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Yu", "Zhixuan", ""], ["Yoon", "Jae Shin", ""], ["Lee", "In Kyu", ""], ["Venkatesh", "Prashanth", ""], ["Park", "Jaesik", ""], ["Yu", "Jihun", ""], ["Park", "Hyun Soo", ""]]}, {"id": "1812.00287", "submitter": "Fabian Manhardt", "authors": "Fabian Manhardt and Diego Martin Arroyo and Christian Rupprecht and\n  Benjamin Busam and Tolga Birdal and Nassir Navab and Federico Tombari", "title": "Explaining the Ambiguity of Object Detection and 6D Pose From Visual\n  Data", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection and pose estimation from a single image are two\ninherently ambiguous problems. Oftentimes, objects appear similar from\ndifferent viewpoints due to shape symmetries, occlusion and repetitive\ntextures. This ambiguity in both detection and pose estimation means that an\nobject instance can be perfectly described by several different poses and even\nclasses. In this work we propose to explicitly deal with this uncertainty. For\neach object instance we predict multiple pose and class outcomes to estimate\nthe specific pose distribution generated by symmetries and repetitive textures.\nThe distribution collapses to a single outcome when the visual appearance\nuniquely identifies just one valid pose. We show the benefits of our approach\nwhich provides not only a better explanation for pose ambiguity, but also a\nhigher accuracy in terms of pose estimation.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 23:31:50 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 14:08:33 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Manhardt", "Fabian", ""], ["Arroyo", "Diego Martin", ""], ["Rupprecht", "Christian", ""], ["Busam", "Benjamin", ""], ["Birdal", "Tolga", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1812.00291", "submitter": "Sagi Eppel", "authors": "Sagi Eppel", "title": "Classifying a specific image region using convolutional nets with an ROI\n  mask as input", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Convolutional neural nets (CNN) are the leading computer vision method for\nclassifying images. In some cases, it is desirable to classify only a specific\nregion of the image that corresponds to a certain object. Hence, assuming that\nthe region of the object in the image is known in advance and is given as a\nbinary region of interest (ROI) mask, the goal is to classify the object in\nthis region using a convolutional neural net. This goal is achieved using a\nstandard image classification net with the addition of a side branch, which\nconverts the ROI mask into an attention map. This map is then combined with the\nimage classification net. This allows the net to focus the attention on the\nobject region while still extracting contextual cues from the background. This\napproach was evaluated using the COCO object dataset and the OpenSurfaces\nmaterials dataset. In both cases, it gave superior results to methods that\ncompletely ignore the background region. In addition, it was found that\ncombining the attention map at the first layer of the net gave better results\nthan combining it at higher layers of the net. The advantages of this method\nare most apparent in the classification of small regions which demands a great\ndeal of contextual information from the background.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 23:52:37 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 22:38:14 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Eppel", "Sagi", ""]]}, {"id": "1812.00301", "submitter": "Yantian Zha", "authors": "Yantian Zha, Yikang Li, Tianshu Yu, Subbarao Kambhampati, Baoxin Li", "title": "Plan-Recognition-Driven Attention Modeling for Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual recognition of activities or external agents involves an\ninterplay between high-level plan recognition and low-level perception. Given\nthat, a natural question to ask is: can low-level perception be improved by\nhigh-level plan recognition? We formulate the problem of leveraging recognized\nplans to generate better top-down attention maps\n\\cite{gazzaniga2009,baluch2011} to improve the perception performance. We call\nthese top-down attention maps specifically as plan-recognition-driven attention\nmaps. To address this problem, we introduce the Pixel Dynamics Network. Pixel\nDynamics Network serves as an observation model, which predicts next states of\nobject points at each pixel location given observation of pixels and\npixel-level action feature. This is like internally learning a pixel-level\ndynamics model. Pixel Dynamics Network is a kind of Convolutional Neural\nNetwork (ConvNet), with specially-designed architecture. Therefore, Pixel\nDynamics Network could take the advantage of parallel computation of ConvNets,\nwhile learning the pixel-level dynamics model. We further prove the equivalence\nbetween Pixel Dynamics Network as an observation model, and the belief update\nin partially observable Markov decision process (POMDP) framework. We evaluate\nour Pixel Dynamics Network in event recognition tasks. We build an event\nrecognition system, ER-PRN, which takes Pixel Dynamics Network as a subroutine,\nto recognize events based on observations augmented by plan-recognition-driven\nattention.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 02:07:06 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zha", "Yantian", ""], ["Li", "Yikang", ""], ["Yu", "Tianshu", ""], ["Kambhampati", "Subbarao", ""], ["Li", "Baoxin", ""]]}, {"id": "1812.00303", "submitter": "Yogesh Rawat", "authors": "Bruce McIntosh, Kevin Duarte, Yogesh S Rawat, Mubarak Shah", "title": "Multi-modal Capsule Routing for Actor and Action Video Segmentation\n  Conditioned on Natural Language Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end capsule network for pixel level\nlocalization of actors and actions present in a video. The localization is\nperformed based on a natural language query through which an actor and action\nare specified. We propose to encode both the video as well as textual input in\nthe form of capsules, which provide more effective representation in comparison\nwith standard convolution based features. We introduce a novel capsule based\nattention mechanism for fusion of video and text capsules for text selected\nvideo segmentation. The attention mechanism is performed via joint EM routing\nover video and text capsules for text selected actor and action localization.\nThe existing works on actor-action localization are mainly focused on\nlocalization in a single frame instead of the full video. Different from\nexisting works, we propose to perform the localization on all frames of the\nvideo. To validate the potential of the proposed network for actor and action\nlocalization on all the frames of a video, we extend an existing actor-action\ndataset (A2D) with annotations for all the frames. The experimental evaluation\ndemonstrates the effectiveness of the proposed capsule network for text\nselective actor and action localization in videos, and it also improves upon\nthe performance of the existing state-of-the art works on single frame-based\nlocalization.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 02:08:02 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["McIntosh", "Bruce", ""], ["Duarte", "Kevin", ""], ["Rawat", "Yogesh S", ""], ["Shah", "Mubarak", ""]]}, {"id": "1812.00312", "submitter": "Jayant Sharma", "authors": "Jayant Sharma, Zixing Wang, Alberto Speranzon, Vijay Venkataraman,\n  Hyun Soo Park", "title": "ECO: Egocentric Cognitive Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method to localize a camera within a previously unseen\nenvironment perceived from an egocentric point of view. Although this is, in\ngeneral, an ill-posed problem, humans can effortlessly and efficiently\ndetermine their relative location and orientation and navigate into a\npreviously unseen environments, e.g., finding a specific item in a new grocery\nstore. To enable such a capability, we design a new egocentric representation,\nwhich we call ECO (Egocentric COgnitive map). ECO is biologically inspired, by\nthe cognitive map that allows human navigation, and it encodes the surrounding\nvisual semantics with respect to both distance and orientation. ECO possesses\nthree main properties: (1) reconfigurability: complex semantics and geometry is\ncaptured via the synthesis of atomic visual representations (e.g., image\npatch); (2) robustness: the visual semantics are registered in a geometrically\nconsistent way (e.g., aligning with respect to the gravity vector,\nfrontalizing, and rescaling to canonical depth), thus enabling us to learn\nmeaningful atomic representations; (3) adaptability: a domain adaptation\nframework is designed to generalize the learned representation without manual\ncalibration. As a proof-of-concept, we use ECO to localize a camera within\nreal-world scenes---various grocery stores---and demonstrate performance\nimprovements when compared to existing semantic localization approaches.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 03:12:47 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Sharma", "Jayant", ""], ["Wang", "Zixing", ""], ["Speranzon", "Alberto", ""], ["Venkataraman", "Vijay", ""], ["Park", "Hyun Soo", ""]]}, {"id": "1812.00324", "submitter": "Jiefeng Li", "authors": "Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, Cewu Lu", "title": "CrowdPose: Efficient Crowded Scenes Pose Estimation and A New Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation is fundamental to many computer vision tasks and\nhas made significant progress in recent years. However, few previous methods\nexplored the problem of pose estimation in crowded scenes while it remains\nchallenging and inevitable in many scenarios. Moreover, current benchmarks\ncannot provide an appropriate evaluation for such cases. In this paper, we\npropose a novel and efficient method to tackle the problem of pose estimation\nin the crowd and a new dataset to better evaluate algorithms. Our model\nconsists of two key components: joint-candidate single person pose estimation\n(SPPE) and global maximum joints association. With multi-peak prediction for\neach joint and global association using graph model, our method is robust to\ninevitable interference in crowded scenes and very efficient in inference. The\nproposed method surpasses the state-of-the-art methods on CrowdPose dataset by\n5.2 mAP and results on MSCOCO dataset demonstrate the generalization ability of\nour method. Source code and dataset will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 04:40:40 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 03:53:48 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Li", "Jiefeng", ""], ["Wang", "Can", ""], ["Zhu", "Hao", ""], ["Mao", "Yihuan", ""], ["Fang", "Hao-Shu", ""], ["Lu", "Cewu", ""]]}, {"id": "1812.00328", "submitter": "Nilanjan Ray", "authors": "Nhat M. Nguyen and Nilanjan Ray", "title": "End-to-end Learning of Convolutional Neural Net and Dynamic Programming\n  for Left Ventricle Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable programming is able to combine different functions or programs\nin a processing pipeline with the goal of applying end-to-end learning or\noptimization. A significant impediment is the non-differentiable nature of some\nalgorithms. We propose to use synthetic gradients (SG) to overcome this\ndifficulty. SG uses the universal function approximation property of neural\nnetworks. We apply SG to combine convolutional neural network (CNN) with\ndynamic programming (DP) in end-to-end learning for segmenting left ventricle\nfrom short axis view of heart MRI. Our experiments show that end-to-end\ncombination of CNN and DP requires fewer labeled images to achieve a\nsignificantly better segmentation accuracy than using only CNN.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 05:00:57 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 03:08:49 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Nguyen", "Nhat M.", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1812.00329", "submitter": "Chen Wei", "authors": "Chen Wei, Lingxi Xie, Xutong Ren, Yingda Xia, Chi Su, Jiaying Liu, Qi\n  Tian, Alan L. Yuille", "title": "Iterative Reorganization with Weak Spatial Constraints: Solving\n  Arbitrary Jigsaw Puzzles for Unsupervised Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning visual features from unlabeled image data is an important yet\nchallenging task, which is often achieved by training a model on some\nannotation-free information. We consider spatial contexts, for which we solve\nso-called jigsaw puzzles, i.e., each image is cut into grids and then\ndisordered, and the goal is to recover the correct configuration. Existing\napproaches formulated it as a classification task by defining a fixed mapping\nfrom a small subset of configurations to a class set, but these approaches\nignore the underlying relationship between different configurations and also\nlimit their application to more complex scenarios. This paper presents a novel\napproach which applies to jigsaw puzzles with an arbitrary grid size and\ndimensionality. We provide a fundamental and generalized principle, that weaker\ncues are easier to be learned in an unsupervised manner and also transfer\nbetter. In the context of puzzle recognition, we use an iterative manner which,\ninstead of solving the puzzle all at once, adjusts the order of the patches in\neach step until convergence. In each step, we combine both unary and binary\nfeatures on each patch into a cost function judging the correctness of the\ncurrent configuration. Our approach, by taking similarity between puzzles into\nconsideration, enjoys a more reasonable way of learning visual knowledge. We\nverify the effectiveness of our approach in two aspects. First, it is able to\nsolve arbitrarily complex puzzles, including high-dimensional puzzles, that\nprior methods are difficult to handle. Second, it serves as a reliable way of\nnetwork initialization, which leads to better transfer performance in a few\nvisual recognition tasks including image classification, object detection, and\nsemantic segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 05:07:24 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Wei", "Chen", ""], ["Xie", "Lingxi", ""], ["Ren", "Xutong", ""], ["Xia", "Yingda", ""], ["Su", "Chi", ""], ["Liu", "Jiaying", ""], ["Tian", "Qi", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1812.00332", "submitter": "Han Cai", "authors": "Han Cai, Ligeng Zhu, Song Han", "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and\n  Hardware", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural architecture search (NAS) has a great impact by automatically\ndesigning effective neural network architectures. However, the prohibitive\ncomputational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours)\nmakes it difficult to \\emph{directly} search the architectures on large-scale\ntasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via\na continuous representation of network architecture but suffers from the high\nGPU memory consumption issue (grow linearly w.r.t. candidate set size). As a\nresult, they need to utilize~\\emph{proxy} tasks, such as training on a smaller\ndataset, or learning with only a few blocks, or training just for a few epochs.\nThese architectures optimized on proxy tasks are not guaranteed to be optimal\non the target task. In this paper, we present \\emph{ProxylessNAS} that can\n\\emph{directly} learn the architectures for large-scale target tasks and target\nhardware platforms. We address the high memory consumption issue of\ndifferentiable NAS and reduce the computational cost (GPU hours and GPU memory)\nto the same level of regular training while still allowing a large candidate\nset. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of\ndirectness and specialization. On CIFAR-10, our model achieves 2.08\\% test\nerror with only 5.7M parameters, better than the previous state-of-the-art\narchitecture AmoebaNet-B, while using 6$\\times$ fewer parameters. On ImageNet,\nour model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being\n1.2$\\times$ faster with measured GPU latency. We also apply ProxylessNAS to\nspecialize neural architectures for hardware with direct hardware metrics (e.g.\nlatency) and provide insights for efficient CNN architecture design.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 05:29:53 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 01:36:47 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Cai", "Han", ""], ["Zhu", "Ligeng", ""], ["Han", "Song", ""]]}, {"id": "1812.00333", "submitter": "Haoxuan You", "authors": "Haoxuan You, Yifan Feng, Xibin Zhao, Changqing Zou, Rongrong Ji, Yue\n  Gao", "title": "PVRNet: Point-View Relation Neural Network for 3D Shape Recognition", "comments": "9 pages, 6 figures, the 33th AAAI Conference on Artificial\n  Intelligence (AAAI2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional (3D) shape recognition has drawn much research attention in\nthe field of computer vision. The advances of deep learning encourage various\ndeep models for 3D feature representation. For point cloud and multi-view data,\ntwo popular 3D data modalities, different models are proposed with remarkable\nperformance. However the relation between point cloud and views has been rarely\ninvestigated. In this paper, we introduce Point-View Relation Network (PVRNet),\nan effective network designed to well fuse the view features and the point\ncloud feature with a proposed relation score module. More specifically, based\non the relation score module, the point-single-view fusion feature is first\nextracted by fusing the point cloud feature and each single view feature with\npoint-singe-view relation, then the point-multi-view fusion feature is\nextracted by fusing the point cloud feature and the features of different\nnumber of views with point-multi-view relation. Finally, the point-single-view\nfusion feature and point-multi-view fusion feature are further combined\ntogether to achieve a unified representation for a 3D shape. Our proposed\nPVRNet has been evaluated on ModelNet40 dataset for 3D shape classification and\nretrieval. Experimental results indicate our model can achieve significant\nperformance improvement compared with the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 05:38:59 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["You", "Haoxuan", ""], ["Feng", "Yifan", ""], ["Zhao", "Xibin", ""], ["Zou", "Changqing", ""], ["Ji", "Rongrong", ""], ["Gao", "Yue", ""]]}, {"id": "1812.00334", "submitter": "Simiao Zuo", "authors": "Simiao Zuo and Jialin Wu", "title": "Image Score: How to Select Useful Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has long been debates on how we could interpret neural networks and\nunderstand the decisions our models make. Specifically, why deep neural\nnetworks tend to be error-prone when dealing with samples that output low\nsoftmax scores. We present an efficient approach to measure the confidence of\ndecision-making steps by statistically investigating each unit's contribution\nto that decision. Instead of focusing on how the models react on datasets, we\nstudy the datasets themselves given a pre-trained model. Our approach is\ncapable of assigning a score to each sample within a dataset that measures the\nfrequency of occurrence of that sample's chain of activation. We demonstrate\nwith experiments that our method could select useful samples to improve deep\nneural networks in a semi-supervised leaning setting.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 05:45:16 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zuo", "Simiao", ""], ["Wu", "Jialin", ""]]}, {"id": "1812.00344", "submitter": "Shaojie Wang", "authors": "Shaojie Wang, Wentian Zhao, Ziyi Kou, Chenliang Xu", "title": "How to Make a BLT Sandwich? Learning to Reason towards Understanding Web\n  Instructional Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Understanding web instructional videos is an essential branch of video\nunderstanding in two aspects. First, most existing video methods focus on\nshort-term actions for a-few-second-long video clips; these methods are not\ndirectly applicable to long videos. Second, unlike unconstrained long videos,\ne.g., movies, instructional videos are more structured in that they have\nstep-by-step procedure constraining the understanding task. In this paper, we\nstudy reasoning on instructional videos via question-answering (QA).\nSurprisingly, it has not been an emphasis in the video community despite its\nrich applications. We thereby introduce YouQuek, an annotated QA dataset for\ninstructional videos based on the recent YouCook2. The questions in YouQuek are\nnot limited to cues on one frame but related to logical reasoning in the\ntemporal dimension. Observing the lack of effective representations for\nmodeling long videos, we propose a set of carefully designed models including a\nnovel Recurrent Graph Convolutional Network (RGCN) that captures both temporal\norder and relation information. Furthermore, we study multiple modalities\nincluding description and transcripts for the purpose of boosting video\nunderstanding. Extensive experiments on YouQuek suggest that RGCN performs the\nbest in terms of QA accuracy and a better performance is gained by introducing\nhuman annotated description.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 06:58:06 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 17:04:01 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Wang", "Shaojie", ""], ["Zhao", "Wentian", ""], ["Kou", "Ziyi", ""], ["Xu", "Chenliang", ""]]}, {"id": "1812.00352", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang, Yuzhen Jin, Jilan Xu, Xiaowei Xu, Yanchun Zhang", "title": "MDU-Net: Multi-scale Densely Connected U-Net for biomedical image\n  segmentation", "comments": "10 pages, 6 figures, 6 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiologist is \"doctor's doctor\", biomedical image segmentation plays a\ncentral role in quantitative analysis, clinical diagnosis, and medical\nintervention. In the light of the fully convolutional networks (FCN) and U-Net,\ndeep convolutional networks (DNNs) have made significant contributions in\nbiomedical image segmentation applications. In this paper, based on U-Net, we\npropose MDUnet, a multi-scale densely connected U-net for biomedical image\nsegmentation. we propose three different multi-scale dense connections for U\nshaped architectures encoder, decoder and across them. The highlights of our\narchitecture is directly fuses the neighboring different scale feature maps\nfrom both higher layers and lower layers to strengthen feature propagation in\ncurrent layer. Which can largely improves the information flow encoder, decoder\nand across them. Multi-scale dense connections, which means containing shorter\nconnections between layers close to the input and output, also makes much\ndeeper U-net possible. We adopt the optimal model based on the experiment and\npropose a novel Multi-scale Dense U-Net (MDU-Net) architecture with\nquantization. Which reduce overfitting in MDU-Net for better accuracy. We\nevaluate our purpose model on the MICCAI 2015 Gland Segmentation dataset\n(GlaS). The three multi-scale dense connections improve U-net performance by up\nto 1.8% on test A and 3.5% on test B in the MICCAI Gland dataset. Meanwhile the\nMDU-net with quantization achieves the superiority over U-Net performance by up\nto 3% on test A and 4.1% on test B.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 08:09:55 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 02:41:35 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Zhang", "Jiawei", ""], ["Jin", "Yuzhen", ""], ["Xu", "Jilan", ""], ["Xu", "Xiaowei", ""], ["Zhang", "Yanchun", ""]]}, {"id": "1812.00399", "submitter": "Yaman Kumar", "authors": "Nupur Baghel, Yaman Kumar, Paavini Nanda, Rajiv Ratn Shah, Debanjan\n  Mahata, Roger Zimmermann", "title": "Kiki Kills: Identifying Dangerous Challenge Videos from Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been upsurge in the number of people participating in challenges\nmade popular through social media channels. One of the examples of such a\nchallenge is the Kiki Challenge, in which people step out of their moving cars\nand dance to the tunes of the song, 'Kiki, Do you love me?'. Such an action\nmakes the people taking the challenge prone to accidents and can also create\nnuisance for the others traveling on the road. In this work, we introduce the\nprevalence of such challenges in social media and show how the machine learning\ncommunity can aid in preventing dangerous situations triggered by them by\ndeveloping models that can distinguish between dangerous and non-dangerous\nchallenge videos. Towards this objective, we release a new dataset namely\nMIDAS-KIKI dataset, consisting of manually annotated dangerous and\nnon-dangerous Kiki challenge videos. Further, we train a deep learning model to\nidentify dangerous and non-dangerous videos, and report our results.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 14:46:44 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 12:58:29 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Baghel", "Nupur", ""], ["Kumar", "Yaman", ""], ["Nanda", "Paavini", ""], ["Shah", "Rajiv Ratn", ""], ["Mahata", "Debanjan", ""], ["Zimmermann", "Roger", ""]]}, {"id": "1812.00408", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Xiaobo Wang, Ajian Liu, Chenxu Zhao, Jun Wan, Sergio\n  Escalera, Hailin Shi, Zezheng Wang, Stan Z. Li", "title": "A Dataset and Benchmark for Large-scale Multi-modal Face Anti-spoofing", "comments": "CVPR2019 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is essential to prevent face recognition systems from a\nsecurity breach. Much of the progresses have been made by the availability of\nface anti-spoofing benchmark datasets in recent years. However, existing face\nanti-spoofing benchmarks have limited number of subjects ($\\le\\negmedspace170$)\nand modalities ($\\leq\\negmedspace2$), which hinder the further development of\nthe academic community. To facilitate face anti-spoofing research, we introduce\na large-scale multi-modal dataset, namely CASIA-SURF, which is the largest\npublicly available dataset for face anti-spoofing in terms of both subjects and\nvisual modalities. Specifically, it consists of $1,000$ subjects with $21,000$\nvideos and each sample has $3$ modalities (i.e., RGB, Depth and IR). We also\nprovide a measurement set, evaluation protocol and training/validation/testing\nsubsets, developing a new benchmark for face anti-spoofing. Moreover, we\npresent a new multi-modal fusion method as baseline, which performs feature\nre-weighting to select the more informative channel features while suppressing\nthe less useful ones for each modal. Extensive experiments have been conducted\non the proposed dataset to verify its significance and generalization\ncapability. The dataset is available at\nhttps://sites.google.com/qq.com/chalearnfacespoofingattackdete\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 15:34:16 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 15:05:26 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 15:09:45 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Zhang", "Shifeng", ""], ["Wang", "Xiaobo", ""], ["Liu", "Ajian", ""], ["Zhao", "Chenxu", ""], ["Wan", "Jun", ""], ["Escalera", "Sergio", ""], ["Shi", "Hailin", ""], ["Wang", "Zezheng", ""], ["Li", "Stan Z.", ""]]}, {"id": "1812.00412", "submitter": "Taimoor Tariq Mr.", "authors": "Taimoor Tariq, Okan Tarhan Tursun, Munchurl Kim, Piotr Didyk", "title": "Why Are Deep Representations Good Perceptual Quality Features?", "comments": "To be presented at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, intermediate feature maps of pre-trained convolutional neural\nnetworks have shown significant perceptual quality improvements, when they are\nused in the loss function for training new networks. It is believed that these\nfeatures are better at encoding the perceptual quality and provide more\nefficient representations of input images compared to other perceptual metrics\nsuch as SSIM and PSNR. However, there have been no systematic studies to\ndetermine the underlying reason. Due to the lack of such an analysis, it is not\npossible to evaluate the performance of a particular set of features or to\nimprove the perceptual quality even more by carefully selecting a subset of\nfeatures from a pre-trained CNN. This work shows that the capabilities of\npre-trained deep CNN features in optimizing the perceptual quality are\ncorrelated with their success in capturing basic human visual perception\ncharacteristics. In particular, we focus our analysis on fundamental aspects of\nhuman perception, such as the contrast sensitivity and orientation selectivity.\nWe introduce two new formulations to measure the frequency and orientation\nselectivity of the features learned by convolutional layers for evaluating deep\nfeatures learned by widely-used deep CNNs such as VGG-16. We demonstrate that\nthe pre-trained CNN features which receive higher scores are better at\npredicting human quality judgment. Furthermore, we show the possibility of\nusing our method to select deep features to form a new loss function, which\nimproves the image reconstruction quality for the well-known single-image\nsuper-resolution problem.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 15:54:29 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 07:57:16 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 17:36:46 GMT"}, {"version": "v4", "created": "Thu, 23 Jul 2020 13:20:17 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Tariq", "Taimoor", ""], ["Tursun", "Okan Tarhan", ""], ["Kim", "Munchurl", ""], ["Didyk", "Piotr", ""]]}, {"id": "1812.00422", "submitter": "Qingyu Chen", "authors": "Qingyu Chen, Yifan Peng, Tiarnan Keenan, Shazia Dharssi, Elvira Agron,\n  Wai T. Wong, Emily Y. Chew, Zhiyong Lu", "title": "A multi-task deep learning model for the classification of Age-related\n  Macular Degeneration", "comments": "10 pages, 5 figures, and 5 tables To appear in the Proceeding of AMIA\n  Informatics 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age-related Macular Degeneration (AMD) is a leading cause of blindness.\nAlthough the Age-Related Eye Disease Study group previously developed a 9-step\nAMD severity scale for manual classification of AMD severity from color fundus\nimages, manual grading of images is time-consuming and expensive. Built on our\nprevious work DeepSeeNet, we developed a novel deep learning model for\nautomated classification of images into the 9-step scale. Instead of predicting\nthe 9-step score directly, our approach simulates the reading center grading\nprocess. It first detects four AMD characteristics (drusen area, geographic\natrophy, increased pigment, and depigmentation), then combines these to derive\nthe overall 9-step score. Importantly, we applied multi-task learning\ntechniques, which allowed us to train classification of the four\ncharacteristics in parallel, share representation, and prevent overfitting.\nEvaluation on two image datasets showed that the accuracy of the model exceeded\nthe current state-of-the-art model by > 10%.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 16:47:02 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Chen", "Qingyu", ""], ["Peng", "Yifan", ""], ["Keenan", "Tiarnan", ""], ["Dharssi", "Shazia", ""], ["Agron", "Elvira", ""], ["Wong", "Wai T.", ""], ["Chew", "Emily Y.", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1812.00426", "submitter": "Yoni Kasten", "authors": "Yoni Kasten, Amnon Geifman, Meirav Galun, Ronen Basri", "title": "GPSfM: Global Projective SFM Using Algebraic Constraints on Multi-View\n  Fundamental Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of recovering projective camera matrices\nfrom collections of fundamental matrices in multiview settings. We make two\nmain contributions. First, given ${n \\choose 2}$ fundamental matrices computed\nfor $n$ images, we provide a complete algebraic characterization in the form of\nconditions that are both necessary and sufficient to enabling the recovery of\ncamera matrices. These conditions are based on arranging the fundamental\nmatrices as blocks in a single matrix, called the $n$-view fundamental matrix,\nand characterizing this matrix in terms of the signs of its eigenvalues and\nrank structures. Secondly, we propose a concrete algorithm for projective\nstructure-from-motion that utilizes this characterization. Given a complete or\npartial collection of measured fundamental matrices, our method seeks camera\nmatrices that minimize a global algebraic error for the measured fundamental\nmatrices. In contrast to existing methods, our optimization, without any\ninitialization, produces a consistent set of fundamental matrices that\ncorresponds to a unique set of cameras (up to a choice of projective frame).\nOur experiments indicate that our method achieves state of the art performance\nin both accuracy and running time.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 16:56:09 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 08:29:27 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 12:33:16 GMT"}, {"version": "v4", "created": "Sun, 6 Jun 2021 20:14:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kasten", "Yoni", ""], ["Geifman", "Amnon", ""], ["Galun", "Meirav", ""], ["Basri", "Ronen", ""]]}, {"id": "1812.00438", "submitter": "Cedric Scheerlinck", "authors": "Cedric Scheerlinck, Nick Barnes, Robert Mahony", "title": "Asynchronous Spatial Image Convolutions for Event Cameras", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters, Vol. 4, No. 2, April 2019,\n  pp. 816-822", "doi": "10.1109/LRA.2019.2893427", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial convolution is arguably the most fundamental of 2D image processing\noperations. Conventional spatial image convolution can only be applied to a\nconventional image, that is, an array of pixel values (or similar image\nrepresentation) that are associated with a single instant in time. Event\ncameras have serial, asynchronous output with no natural notion of an image\nframe, and each event arrives with a different timestamp. In this paper, we\npropose a method to compute the convolution of a linear spatial kernel with the\noutput of an event camera. The approach operates on the event stream output of\nthe camera directly without synthesising pseudo-image frames as is common in\nthe literature. The key idea is the introduction of an internal state that\ndirectly encodes the convolved image information, which is updated\nasynchronously as each event arrives from the camera. The state can be read-off\nas-often-as and whenever required for use in higher level vision algorithms for\nreal-time robotic systems. We demonstrate the application of our method to\ncorner detection, providing an implementation of a Harris corner-response\n\"state\" that can be used in real-time for feature detection and tracking on\nrobotic systems.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 18:01:25 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 10:21:39 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 16:37:57 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Scheerlinck", "Cedric", ""], ["Barnes", "Nick", ""], ["Mahony", "Robert", ""]]}, {"id": "1812.00440", "submitter": "Garrick Brazil", "authors": "Garrick Brazil, Xiaoming Liu", "title": "Pedestrian Detection with Autoregressive Network Phases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an autoregressive pedestrian detection framework with cascaded\nphases designed to progressively improve precision. The proposed framework\nutilizes a novel lightweight stackable decoder-encoder module which uses\nconvolutional re-sampling layers to improve features while maintaining\nefficient memory and runtime cost. Unlike previous cascaded detection systems,\nour proposed framework is designed within a region proposal network and thus\nretains greater context of nearby detections compared to independently\nprocessed RoI systems. We explicitly encourage increasing levels of precision\nby assigning strict labeling policies to each consecutive phase such that early\nphases develop features primarily focused on achieving high recall and later on\naccurate precision. In consequence, the final feature maps form more peaky\nradial gradients emulating from the centroids of unique pedestrians. Using our\nproposed autoregressive framework leads to new state-of-the-art performance on\nthe reasonable and occlusion settings of the Caltech pedestrian dataset, and\nachieves competitive state-of-the-art performance on the KITTI dataset.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 18:18:22 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Brazil", "Garrick", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1812.00442", "submitter": "Nicolai Wojke", "authors": "Nicolai Wojke and Alex Bewley", "title": "Deep Cosine Metric Learning for Person Re-Identification", "comments": null, "journal-ref": null, "doi": "10.1109/WACV.2018.00087", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning aims to construct an embedding where two extracted features\ncorresponding to the same identity are likely to be closer than features from\ndifferent identities. This paper presents a method for learning such a feature\nspace where the cosine similarity is effectively optimized through a simple\nre-parametrization of the conventional softmax classification regime. At test\ntime, the final classification layer can be stripped from the network to\nfacilitate nearest neighbor queries on unseen individuals using the cosine\nsimilarity metric. This approach presents a simple alternative to direct metric\nlearning objectives such as siamese networks that have required sophisticated\npair or triplet sampling strategies in the past. The method is evaluated on two\nlarge-scale pedestrian re-identification datasets where competitive results are\nachieved overall. In particular, we achieve better generalization on the test\nset compared to a network trained with triplet loss.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 18:31:45 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Wojke", "Nicolai", ""], ["Bewley", "Alex", ""]]}, {"id": "1812.00452", "submitter": "Hang Gao", "authors": "Hang Gao, Huazhe Xu, Qi-Zhi Cai, Ruth Wang, Fisher Yu, Trevor Darrell", "title": "Disentangling Propagation and Generation for Video Prediction", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic scene has two types of elements: those that move fluidly and can be\npredicted from previous frames, and those which are disoccluded (exposed) and\ncannot be extrapolated. Prior approaches to video prediction typically learn\neither to warp or to hallucinate future pixels, but not both. In this paper, we\ndescribe a computational model for high-fidelity video prediction which\ndisentangles motion-specific propagation from motion-agnostic generation. We\nintroduce a confidence-aware warping operator which gates the output of pixel\npredictions from a flow predictor for non-occluded regions and from a context\nencoder for occluded regions. Moreover, in contrast to prior works where\nconfidence is jointly learned with flow and appearance using a single network,\nwe compute confidence after a warping step, and employ a separate network to\ninpaint exposed regions. Empirical results on both synthetic and real datasets\nshow that our disentangling approach provides better occlusion maps and\nproduces both sharper and more realistic predictions compared to strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 19:31:30 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 07:15:09 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Gao", "Hang", ""], ["Xu", "Huazhe", ""], ["Cai", "Qi-Zhi", ""], ["Wang", "Ruth", ""], ["Yu", "Fisher", ""], ["Darrell", "Trevor", ""]]}, {"id": "1812.00467", "submitter": "Yossi Gandelsman", "authors": "Yossi Gandelsman, Assaf Shocher, Michal Irani", "title": "\"Double-DIP\": Unsupervised Image Decomposition via Coupled\n  Deep-Image-Priors", "comments": "Project page: http://www.wisdom.weizmann.ac.il/~vision/DoubleDIP/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many seemingly unrelated computer vision tasks can be viewed as a special\ncase of image decomposition into separate layers. For example, image\nsegmentation (separation into foreground and background layers); transparent\nlayer separation (into reflection and transmission layers); Image dehazing\n(separation into a clear image and a haze map), and more. In this paper we\npropose a unified framework for unsupervised layer decomposition of a single\nimage, based on coupled \"Deep-image-Prior\" (DIP) networks. It was shown\n[Ulyanov et al] that the structure of a single DIP generator network is\nsufficient to capture the low-level statistics of a single image. We show that\ncoupling multiple such DIPs provides a powerful tool for decomposing images\ninto their basic components, for a wide variety of applications. This\ncapability stems from the fact that the internal statistics of a mixture of\nlayers is more complex than the statistics of each of its individual\ncomponents. We show the power of this approach for Image-Dehazing, Fg/Bg\nSegmentation, Watermark-Removal, Transparency Separation in images and video,\nand more. These capabilities are achieved in a totally unsupervised way, with\nno training examples other than the input image/video itself.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 21:23:25 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 16:32:45 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Gandelsman", "Yossi", ""], ["Shocher", "Assaf", ""], ["Irani", "Michal", ""]]}, {"id": "1812.00469", "submitter": "Yuanyi Zhong", "authors": "Yuanyi Zhong, Jianfeng Wang, Jian Peng, Lei Zhang", "title": "Anchor Box Optimization for Object Detection", "comments": "WACV 2020 camera ready; more experiment results (ablation, Faster\n  RCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general approach to optimize anchor boxes for\nobject detection. Nowadays, anchor boxes are widely adopted in state-of-the-art\ndetection frameworks. However, these frameworks usually pre-define anchor box\nshapes in heuristic ways and fix the sizes during training. To improve the\naccuracy and reduce the effort of designing anchor boxes, we propose to\ndynamically learn the anchor shapes, which allows the anchors to automatically\nadapt to the data distribution and the network learning capability. The\nlearning approach can be easily implemented with stochastic gradient descent\nand can be plugged into any anchor box-based detection framework. The extra\ntraining cost is almost negligible and it has no impact on the inference time\nor memory cost. Exhaustive experiments demonstrate that the proposed anchor\noptimization method consistently achieves significant improvement ($\\ge 1\\%$\nmAP absolute gain) over the baseline methods on several benchmark datasets\nincluding Pascal VOC 07+12, MS COCO and Brainwash. Meanwhile, the robustness is\nalso verified towards different anchor initialization methods and the number of\nanchor shapes, which greatly simplifies the problem of anchor box design.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 21:30:32 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 10:37:40 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Zhong", "Yuanyi", ""], ["Wang", "Jianfeng", ""], ["Peng", "Jian", ""], ["Zhang", "Lei", ""]]}, {"id": "1812.00477", "submitter": "Liang Yang", "authors": "Liang Yang, Hao Jiang, Jizhong Xiao, Zhouyuan Huo", "title": "Ego-Downward and Ambient Video based Person Location Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using an ego-centric camera to do localization and tracking is highly needed\nfor urban navigation and indoor assistive system when GPS is not available or\nnot accurate enough. The traditional hand-designed feature tracking and\nestimation approach would fail without visible features. Recently, there are\nseveral works exploring to use context features to do localization. However,\nall of these suffer severe accuracy loss if given no visual context\ninformation. To provide a possible solution to this problem, this paper\nproposes a camera system with both ego-downward and third-static view to\nperform localization and tracking in a learning approach. Besides, we also\nproposed a novel action and motion verification model for cross-view\nverification and localization. We performed comparative experiments based on\nour collected dataset which considers the same dressing, gender, and background\ndiversity. Results indicate that the proposed model can achieve $18.32 \\%$\nimprovement in accuracy performance. Eventually, we tested the model on\nmulti-people scenarios and obtained an average $67.767 \\%$ accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 22:21:41 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Yang", "Liang", ""], ["Jiang", "Hao", ""], ["Xiao", "Jizhong", ""], ["Huo", "Zhouyuan", ""]]}, {"id": "1812.00479", "submitter": "Eman Hassan", "authors": "Eman T. Hassan, Xin Chen, David Crandall", "title": "Unsupervised Domain Adaptation using Generative Models and\n  Self-ensembling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge across different datasets is an important approach to\nsuccessfully train deep models with a small-scale target dataset or when few\nlabeled instances are available. In this paper, we aim at developing a model\nthat can generalize across multiple domain shifts, so that this model can adapt\nfrom a single source to multiple targets. This can be achieved by randomizing\nthe generation of the data of various styles to mitigate the domain mismatch.\nFirst, we present a new adaptation to the CycleGAN model to produce stochastic\nstyle transfer between two image batches of different domains. Second, we\nenhance the classifier performance by using a self-ensembling technique with a\nteacher and student model to train on both original and generated data.\nFinally, we present experimental results on three datasets Office-31,\nOffice-Home, and Visual Domain adaptation. The results suggest that\nselfensembling is better than simple data augmentation with the newly generated\ndata and a single model trained this way can have the best performance across\nall different transfer tasks.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 22:29:04 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Hassan", "Eman T.", ""], ["Chen", "Xin", ""], ["Crandall", "David", ""]]}, {"id": "1812.00481", "submitter": "Siyuan Qiao", "authors": "Siyuan Qiao, Zhe Lin, Jianming Zhang, Alan Yuille", "title": "Neural Rejuvenation: Improving Deep Network Training by Enhancing\n  Computational Resource Utilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of improving computational resource\nutilization of neural networks. Deep neural networks are usually\nover-parameterized for their tasks in order to achieve good performances, thus\nare likely to have underutilized computational resources. This observation\nmotivates a lot of research topics, e.g. network pruning, architecture search,\netc. As models with higher computational costs (e.g. more parameters or more\ncomputations) usually have better performances, we study the problem of\nimproving the resource utilization of neural networks so that their potentials\ncan be further realized. To this end, we propose a novel optimization method\nnamed Neural Rejuvenation. As its name suggests, our method detects dead\nneurons and computes resource utilization in real time, rejuvenates dead\nneurons by resource reallocation and reinitialization, and trains them with new\ntraining schemes. By simply replacing standard optimizers with Neural\nRejuvenation, we are able to improve the performances of neural networks by a\nvery large margin while using similar training efforts and maintaining their\noriginal resource usages.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 22:43:47 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Qiao", "Siyuan", ""], ["Lin", "Zhe", ""], ["Zhang", "Jianming", ""], ["Yuille", "Alan", ""]]}, {"id": "1812.00488", "submitter": "Zhaopeng Cui", "authors": "Jiaxiong Qiu, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang, Shuaicheng Liu,\n  Bing Zeng, Marc Pollefeys", "title": "DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene\n  from Sparse LiDAR Data and Single Color Image", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep learning architecture that produces accurate\ndense depth for the outdoor scene from a single color image and a sparse depth.\nInspired by the indoor depth completion, our network estimates surface normals\nas the intermediate representation to produce dense depth, and can be trained\nend-to-end. With a modified encoder-decoder structure, our network effectively\nfuses the dense color image and the sparse LiDAR depth. To address outdoor\nspecific challenges, our network predicts a confidence mask to handle mixed\nLiDAR signals near foreground boundaries due to occlusion, and combines\nestimates from the color image and surface normals with learned attention maps\nto improve the depth accuracy especially for distant areas. Extensive\nexperiments demonstrate that our model improves upon the state-of-the-art\nperformance on KITTI depth completion benchmark. Ablation study shows the\npositive impact of each model components to the final performance, and\ncomprehensive analysis shows that our model generalizes well to the input with\nhigher sparsity or from indoor scenes.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 23:36:22 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 21:42:39 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Qiu", "Jiaxiong", ""], ["Cui", "Zhaopeng", ""], ["Zhang", "Yinda", ""], ["Zhang", "Xingdi", ""], ["Liu", "Shuaicheng", ""], ["Zeng", "Bing", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1812.00491", "submitter": "Rawal Khirodkar", "authors": "Rawal Khirodkar, Donghyun Yoo, Kris M. Kitani", "title": "VADRA: Visual Adversarial Domain Randomization and Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of learning from synthetic domain randomized data\neffectively. While previous works have showcased domain randomization as an\neffective learning approach, it lacks in challenging the learner and wastes\nvaluable compute on generating easy examples. This can be attributed to uniform\nrandomization over the rendering parameter distribution. In this work, firstly\nwe provide a theoretical perspective on characteristics of domain randomization\nand analyze its limitations. As a solution to these limitations, we propose a\nnovel algorithm which closes the loop between the synthetic generative model\nand the learner in an adversarial fashion. Our framework easily extends to the\nscenario when there is unlabelled target data available, thus incorporating\ndomain adaptation. We evaluate our method on diverse vision tasks using\nstate-of-the-art simulators for public datasets like CLEVR, Syn2Real, and\nVIRAT, where we demonstrate that a learner trained using adversarial data\ngeneration performs better than using a random data generation strategy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 00:00:28 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Khirodkar", "Rawal", ""], ["Yoo", "Donghyun", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1812.00500", "submitter": "Duy-Kien Nguyen", "authors": "Duy-Kien Nguyen and Takayuki Okatani", "title": "Multi-task Learning of Hierarchical Vision-Language Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is still challenging to build an AI system that can perform tasks that\ninvolve vision and language at human level. So far, researchers have singled\nout individual tasks separately, for each of which they have designed networks\nand trained them on its dedicated datasets. Although this approach has seen a\ncertain degree of success, it comes with difficulties of understanding\nrelations among different tasks and transferring the knowledge learned for a\ntask to others. We propose a multi-task learning approach that enables to learn\nvision-language representation that is shared by many tasks from their diverse\ndatasets. The representation is hierarchical, and prediction for each task is\ncomputed from the representation at its corresponding level of the hierarchy.\nWe show through experiments that our method consistently outperforms previous\nsingle-task-learning methods on image caption retrieval, visual question\nanswering, and visual grounding. We also analyze the learned hierarchical\nrepresentation by visualizing attention maps generated in our network.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 00:37:31 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Nguyen", "Duy-Kien", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1812.00518", "submitter": "Tianwei Ni", "authors": "Tianwei Ni, Lingxi Xie, Huangjie Zheng, Elliot K. Fishman, Alan L.\n  Yuille", "title": "Elastic Boundary Projection for 3D Medical Image Segmentation", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on an important yet challenging problem: using a 2D deep network to\ndeal with 3D segmentation for medical image analysis. Existing approaches\neither applied multi-view planar (2D) networks or directly used volumetric (3D)\nnetworks for this purpose, but both of them are not ideal: 2D networks cannot\ncapture 3D contexts effectively, and 3D networks are both memory-consuming and\nless stable arguably due to the lack of pre-trained models.\n  In this paper, we bridge the gap between 2D and 3D using a novel approach\nnamed Elastic Boundary Projection (EBP). The key observation is that, although\nthe object is a 3D volume, what we really need in segmentation is to find its\nboundary which is a 2D surface. Therefore, we place a number of pivot points in\nthe 3D space, and for each pivot, we determine its distance to the object\nboundary along a dense set of directions. This creates an elastic shell around\neach pivot which is initialized as a perfect sphere. We train a 2D deep network\nto determine whether each ending point falls within the object, and gradually\nadjust the shell so that it gradually converges to the actual shape of the\nboundary and thus achieves the goal of segmentation. EBP allows boundary-based\nsegmentation without cutting a 3D volume into slices or patches, which stands\nout from conventional 2D and 3D approaches. EBP achieves promising accuracy in\nabdominal organ segmentation. Our code has been open-sourced\nhttps://github.com/twni2016/Elastic-Boundary-Projection.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 02:05:57 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 12:24:29 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Ni", "Tianwei", ""], ["Xie", "Lingxi", ""], ["Zheng", "Huangjie", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1812.00527", "submitter": "Mo Zhang", "authors": "Jie Zhao, Quanzheng Li, Xiang Li, Hongfeng Li, Li Zhang", "title": "Automated Segmentation of Cervical Nuclei in Pap Smear Images using\n  Deformable Multi-path Ensemble Model", "comments": "ISBI2019 Oral", "journal-ref": null, "doi": null, "report-no": "Paper WeS62.2", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pap smear testing has been widely used for detecting cervical cancers based\non the morphology properties of cell nuclei in microscopic image. An accurate\nnuclei segmentation could thus improve the success rate of cervical cancer\nscreening. In this work, a method of automated cervical nuclei segmentation\nusing Deformable Multipath Ensemble Model (D-MEM) is proposed. The approach\nadopts a U-shaped convolutional network as a backbone network, in which dense\nblocks are used to transfer feature information more effectively. To increase\nthe flexibility of the model, we then use deformable convolution to deal with\ndifferent nuclei irregular shapes and sizes. To reduce the predictive bias, we\nfurther construct multiple networks with different settings, which form an\nensemble model. The proposed segmentation framework has achieved\nstate-of-the-art accuracy on Herlev dataset with Zijdenbos similarity index\n(ZSI) of 0.933, and has the potential to be extended for solving other medical\nimage segmentation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 02:30:45 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 03:43:45 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Zhao", "Jie", ""], ["Li", "Quanzheng", ""], ["Li", "Xiang", ""], ["Li", "Hongfeng", ""], ["Zhang", "Li", ""]]}, {"id": "1812.00535", "submitter": "Mengkai Song", "authors": "Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, Hairong\n  Qi", "title": "Beyond Inferring Class Representatives: User-Level Privacy Leakage From\n  Federated Learning", "comments": "The 38th Annual IEEE International Conference on Computer\n  Communications (INFOCOM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning, i.e., a mobile edge computing framework for deep\nlearning, is a recent advance in privacy-preserving machine learning, where the\nmodel is trained in a decentralized manner by the clients, i.e., data curators,\npreventing the server from directly accessing those private data from the\nclients. This learning mechanism significantly challenges the attack from the\nserver side. Although the state-of-the-art attacking techniques that\nincorporated the advance of Generative adversarial networks (GANs) could\nconstruct class representatives of the global data distribution among all\nclients, it is still challenging to distinguishably attack a specific client\n(i.e., user-level privacy leakage), which is a stronger privacy threat to\nprecisely recover the private data from a specific client. This paper gives the\nfirst attempt to explore user-level privacy leakage against the federated\nlearning by the attack from a malicious server. We propose a framework\nincorporating GAN with a multi-task discriminator, which simultaneously\ndiscriminates category, reality, and client identity of input samples. The\nnovel discrimination on client identity enables the generator to recover user\nspecified private data. Unlike existing works that tend to interfere the\ntraining process of the federated learning, the proposed method works\n\"invisibly\" on the server side. The experimental results demonstrate the\neffectiveness of the proposed attacking approach and the superior to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 03:12:39 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 02:14:01 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 01:17:05 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Wang", "Zhibo", ""], ["Song", "Mengkai", ""], ["Zhang", "Zhifei", ""], ["Song", "Yang", ""], ["Wang", "Qian", ""], ["Qi", "Hairong", ""]]}, {"id": "1812.00548", "submitter": "Joseph Bullock", "authors": "Joseph Bullock, Carolina Cuesta-Lazaro, Arnau Quera-Bofarull", "title": "XNet: A convolutional neural network (CNN) implementation for medical\n  X-Ray image segmentation suitable for small datasets", "comments": "11 pages, 5 figures, 2 tables", "journal-ref": "Proc. SPIE 10953, Medical Imaging 2019: Biomedical Applications in\n  Molecular, Structural, and Functional Imaging, 109531Z (15 March 2019)", "doi": "10.1117/12.2512451", "report-no": null, "categories": "cs.CV cs.AI physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-Ray image enhancement, along with many other medical image processing\napplications, requires the segmentation of images into bone, soft tissue, and\nopen beam regions. We apply a machine learning approach to this problem,\npresenting an end-to-end solution which results in robust and efficient\ninference. Since medical institutions frequently do not have the resources to\nprocess and label the large quantity of X-Ray images usually needed for neural\nnetwork training, we design an end-to-end solution for small datasets, while\nachieving state-of-the-art results. Our implementation produces an overall\naccuracy of 92%, F1 score of 0.92, and an AUC of 0.98, surpassing classical\nimage processing techniques, such as clustering and entropy based methods,\nwhile improving upon the output of existing neural networks used for\nsegmentation in non-medical contexts. The code used for this project is\navailable online.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 04:17:27 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 10:22:52 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Bullock", "Joseph", ""], ["Cuesta-Lazaro", "Carolina", ""], ["Quera-Bofarull", "Arnau", ""]]}, {"id": "1812.00552", "submitter": "Jie Li", "authors": "Jie Li, Rongrong Ji, Hong Liu, Xiaopeng Hong, Yue Gao and Qi Tian", "title": "Universal Perturbation Attack Against Image Retrieval", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal adversarial perturbations (UAPs), a.k.a. input-agnostic\nperturbations, has been proved to exist and be able to fool cutting-edge deep\nlearning models on most of the data samples. Existing UAP methods mainly focus\non attacking image classification models. Nevertheless, little attention has\nbeen paid to attacking image retrieval systems. In this paper, we make the\nfirst attempt in attacking image retrieval systems. Concretely, image retrieval\nattack is to make the retrieval system return irrelevant images to the query at\nthe top ranking list. It plays an important role to corrupt the neighbourhood\nrelationships among features in image retrieval attack. To this end, we propose\na novel method to generate retrieval-against UAP to break the neighbourhood\nrelationships of image features via degrading the corresponding ranking metric.\nTo expand the attack method to scenarios with varying input sizes or\nuntouchable network parameters, a multi-scale random resizing scheme and a\nranking distillation strategy are proposed. We evaluate the proposed method on\nfour widely-used image retrieval datasets, and report a significant performance\ndrop in terms of different metrics, such as mAP and mP@10. Finally, we test our\nattack methods on the real-world visual search engine, i.e., Google Images,\nwhich demonstrates the practical potentials of our methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 04:52:06 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 02:27:12 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Li", "Jie", ""], ["Ji", "Rongrong", ""], ["Liu", "Hong", ""], ["Hong", "Xiaopeng", ""], ["Gao", "Yue", ""], ["Tian", "Qi", ""]]}, {"id": "1812.00555", "submitter": "Fang Liu", "authors": "Fang Liu", "title": "SUSAN: Segment Unannotated image Structure using Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of magnetic resonance (MR) images is a fundamental step in many\nmedical imaging-based applications. The recent implementation of deep\nconvolutional neural networks (CNNs) in image processing has been shown to have\nsignificant impacts on medical image segmentation. Network training of\nsegmentation CNNs typically requires images and paired annotation data\nrepresenting pixel-wise tissue labels referred to as masks. However, the\nsupervised training of highly efficient CNNs with deeper structure and more\nnetwork parameters requires a large number of training images and paired tissue\nmasks. Thus, there is great need to develop a generalized CNN-based\nsegmentation method which would be applicable for a wide variety of MR image\ndatasets with different tissue contrasts. The purpose of this study was to\ndevelop and evaluate a generalized CNN-based method for fully-automated\nsegmentation of different MR image datasets using a single set of annotated\ntraining data. A technique called cycle-consistent generative adversarial\nnetwork (CycleGAN) is applied as the core of the proposed method to perform\nimage-to-image translation between MR image datasets with different tissue\ncontrasts. A joint segmentation network is incorporated into the adversarial\nnetwork to obtain additional segmentation functionality. The proposed method\nwas evaluated for segmenting bone and cartilage on two clinical knee MR image\ndatasets acquired at our institution using only a single set of annotated data\nfrom a publicly available knee MR image dataset. The new technique may further\nimprove the applicability and efficiency of CNN-based segmentation of medical\nimages while eliminating the need for large amounts of annotated training data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 05:00:07 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Liu", "Fang", ""]]}, {"id": "1812.00568", "submitter": "Frederik Ebert", "authors": "Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee,\n  Sergey Levine", "title": "Visual Foresight: Model-Based Deep Reinforcement Learning for\n  Vision-Based Robotic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) algorithms can learn complex robotic skills\nfrom raw sensory inputs, but have yet to achieve the kind of broad\ngeneralization and applicability demonstrated by deep learning methods in\nsupervised domains. We present a deep RL method that is practical for\nreal-world robotics tasks, such as robotic manipulation, and generalizes\neffectively to never-before-seen tasks and objects. In these settings, ground\ntruth reward signals are typically unavailable, and we therefore propose a\nself-supervised model-based approach, where a predictive model learns to\ndirectly predict the future from raw sensory readings, such as camera images.\nAt test time, we explore three distinct goal specification methods: designated\npixels, where a user specifies desired object manipulation tasks by selecting\nparticular pixels in an image and corresponding goal positions, goal images,\nwhere the desired goal state is specified with an image, and image classifiers,\nwhich define spaces of goal states. Our deep predictive models are trained\nusing data collected autonomously and continuously by a robot interacting with\nhundreds of objects, without human supervision. We demonstrate that visual MPC\ncan generalize to never-before-seen objects---both rigid and deformable---and\nsolve a range of user-defined object manipulation tasks using the same model.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 06:06:25 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ebert", "Frederik", ""], ["Finn", "Chelsea", ""], ["Dasari", "Sudeep", ""], ["Xie", "Annie", ""], ["Lee", "Alex", ""], ["Levine", "Sergey", ""]]}, {"id": "1812.00572", "submitter": "Hyunkwang Lee", "authors": "Hyunkwang Lee, Myeongchan Kim, Synho Do", "title": "Practical Window Setting Optimization for Medical Image Deep Learning", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:cs/0101200", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/204", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advancements in deep learning have allowed for numerous\napplications in computed tomography (CT), with potential to improve diagnostic\naccuracy, speed of interpretation, and clinical efficiency. However, the deep\nlearning community has to date neglected window display settings - a key\nfeature of clinical CT interpretation and opportunity for additional\noptimization. Here we propose a window setting optimization (WSO) module that\nis fully trainable with convolutional neural networks (CNNs) to find optimal\nwindow settings for clinical performance. Our approach was inspired by the\nmethod commonly used by practicing radiologists to interpret CT images by\nadjusting window settings to increase the visualization of certain pathologies.\nOur approach provides optimal window ranges to enhance the conspicuity of\nabnormalities, and was used to enable performance enhancement for intracranial\nhemorrhage and urinary stone detection. On each task, the WSO model\noutperformed models trained over the full range of Hounsfield unit values in CT\nimages, as well as images windowed with pre-defined settings. The WSO module\ncan be readily applied to any analysis of CT images, and can be further\ngeneralized to tasks on other medical imaging modalities.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 06:23:52 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Lee", "Hyunkwang", ""], ["Kim", "Myeongchan", ""], ["Do", "Synho", ""]]}, {"id": "1812.00573", "submitter": "Jie Hu", "authors": "Jie Hu, Rongrong Ji, Hong Liu, Shengchuan Zhang, Cheng Deng, Qi Tian", "title": "Towards Visual Feature Translation", "comments": "Accepted as a CVPR 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing visual search systems are deployed based upon fixed kinds of\nvisual features, which prohibits the feature reusing across different systems\nor when upgrading systems with a new type of feature. Such a setting is\nobviously inflexible and time/memory consuming, which is indeed mendable if\nvisual features can be \"translated\" across systems. In this paper, we make the\nfirst attempt towards visual feature translation to break through the barrier\nof using features across different visual search systems. To this end, we\npropose a Hybrid Auto-Encoder (HAE) to translate visual features, which learns\na mapping by minimizing the translation and reconstruction errors. Based upon\nHAE, an Undirected Affinity Measurement (UAM) is further designed to quantify\nthe affinity among different types of visual features. Extensive experiments\nhave been conducted on several public datasets with sixteen different types of\nwidely-used features in visual search systems. Quantitative results show the\nencouraging possibilities of feature translation. For the first time, the\naffinity among widely-used features like SIFT and DELF is reported.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 06:37:02 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 05:07:00 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Hu", "Jie", ""], ["Ji", "Rongrong", ""], ["Liu", "Hong", ""], ["Zhang", "Shengchuan", ""], ["Deng", "Cheng", ""], ["Tian", "Qi", ""]]}, {"id": "1812.00615", "submitter": "Yaocong Hu", "authors": "Yaocong Hu and MingQi Lu and Xiaobo Lu", "title": "Spatial-temporal Fusion Convolutional Neural Network for Simulated\n  Driving Behavior Recognition", "comments": "International Conference on Control, Automation, Robotics and Vision\n  (ICARCV2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormal driving behaviour is one of the leading cause of terrible traffic\naccidents endangering human life. Therefore, study on driving behaviour\nsurveillance has become essential to traffic security and public management. In\nthis paper, we conduct this promising research and employ a two stream CNN\nframework for video-based driving behaviour recognition, in which spatial\nstream CNN captures appearance information from still frames, whilst temporal\nstream CNN captures motion information with pre-computed optical flow\ndisplacement between a few adjacent video frames. We investigate different\nspatial-temporal fusion strategies to combine the intra frame static clues and\ninter frame dynamic clues for final behaviour recognition. So as to validate\nthe effectiveness of the designed spatial-temporal deep learning based model,\nwe create a simulated driving behaviour dataset, containing 1237 videos with 6\ndifferent driving behavior for recognition. Experiment result shows that our\nproposed method obtains noticeable performance improvements compared to the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 09:18:18 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Hu", "Yaocong", ""], ["Lu", "MingQi", ""], ["Lu", "Xiaobo", ""]]}, {"id": "1812.00645", "submitter": "Lixiang Ru", "authors": "Bo Du, Lixiang Ru, Chen Wu, Liangpei Zhang", "title": "Unsupervised Deep Slow Feature Analysis for Change Detection in\n  Multi-Temporal Remote Sensing Images", "comments": "17 pages, 14 figures, accepted by IEEE Transactions of Geoscience and\n  Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2019.2930682", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection has been a hotspot in remote sensing technology for a long\ntime. With the increasing availability of multi-temporal remote sensing images,\nnumerous change detection algorithms have been proposed. Among these methods,\nimage transformation methods with feature extraction and mapping could\neffectively highlight the changed information and thus has better change\ndetection performance. However, changes of multi-temporal images are usually\ncomplex, existing methods are not effective enough. In recent years, deep\nnetwork has shown its brilliant performance in many fields including feature\nextraction and projection. Therefore, in this paper, based on deep network and\nslow feature analysis (SFA) theory, we proposed a new change detection\nalgorithm for multi-temporal remotes sensing images called Deep Slow Feature\nAnalysis (DSFA). In DSFA model, two symmetric deep networks are utilized for\nprojecting the input data of bi-temporal imagery. Then, the SFA module is\ndeployed to suppress the unchanged components and highlight the changed\ncomponents of the transformed features. The CVA pre-detection is employed to\nfind unchanged pixels with high confidence as training samples. Finally, the\nchange intensity is calculated with chi-square distance and the changes are\ndetermined by threshold algorithms. The experiments are performed on two\nreal-world datasets and a public hyperspectral dataset. The visual comparison\nand quantitative evaluation have both shown that DSFA could outperform the\nother state-of-the-art algorithms, including other SFA-based and deep learning\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 10:22:59 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 11:11:30 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Du", "Bo", ""], ["Ru", "Lixiang", ""], ["Wu", "Chen", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1812.00647", "submitter": "Shichao Li", "authors": "Shichao Li, Xin Yang and Tim Cheng", "title": "Deep Hierarchical Machine: a Flexible Divide-and-Conquer Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Deep Hierarchical Machine (DHM), a model inspired from the\ndivide-and-conquer strategy while emphasizing representation learning ability\nand flexibility. A stochastic routing framework as used by recent deep neural\ndecision/regression forests is incorporated, but we remove the need to evaluate\nunnecessary computation paths by utilizing a different topology and introducing\na probabilistic pruning technique. We also show a specified version of DHM\n(DSHM) for efficiency, which inherits the sparse feature extraction process as\nin traditional decision tree with pixel-difference feature. To achieve sparse\nfeature extraction, we propose to utilize sparse convolution operation in DSHM\nand show one possibility of introducing sparse convolution kernels by using\nlocal binary convolution layer. DHM can be applied to both classification and\nregression problems, and we validate it on standard image classification and\nface alignment tasks to show its advantages over past architectures.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 10:34:01 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Li", "Shichao", ""], ["Yang", "Xin", ""], ["Cheng", "Tim", ""]]}, {"id": "1812.00648", "submitter": "Laurent Jacques", "authors": "Sandrine Anthoine, Yannick Boursier, Laurent Jacques", "title": "Proceedings of the fourth \"international Traveling Workshop on\n  Interactions between low-complexity data models and Sensing Techniques\"\n  (iTWIST'18)", "comments": "Final version, conference website:\n  https://sites.google.com/view/itwist18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iTWIST workshop series aim at fostering collaboration between\ninternational scientific teams for developing new theories, applications and\ngeneralizations of low-complexity models. These events emphasize dissemination\nof ideas through both specific oral and poster presentations, as well as free\ndiscussions. For this fourth edition, iTWIST'18 gathered in CIRM, Marseille,\nFrance, 74 international participants and featured 7 invited talks, 16 oral\npresentations, and 21 posters.\n  From iTWIST'18, the scientific committee has decided that the workshop\nproceedings will adopt the episcience.org philosophy, combined with arXiv.org:\nin a nutshell, \"the proceedings are equivalent to an overlay page, built above\narXiv.org; they add value to these archives by attaching a scientific caution\nto the validated papers.\"\n  This means that all papers listed in the HTML page of this arxiv publication\n(see the menu on the right) have been thoroughly evaluated and approved by two\nindependent reviewers, and authors have revised their work according to the\ncomments provided by these reviewers.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 10:34:09 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 08:49:14 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Anthoine", "Sandrine", ""], ["Boursier", "Yannick", ""], ["Jacques", "Laurent", ""]]}, {"id": "1812.00660", "submitter": "Chia-Che Chang", "authors": "Wei-Chun Chen, Chia-Che Chang, Chien-Yu Lu, and Che-Rung Lee", "title": "Knowledge Distillation with Feature Maps for Image Classification", "comments": "Knowledge Distillation, Model Compression, and Generative Adversarial\n  Network, ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model reduction problem that eases the computation costs and latency of\ncomplex deep learning architectures has received an increasing number of\ninvestigations owing to its importance in model deployment. One promising\nmethod is knowledge distillation (KD), which creates a fast-to-execute student\nmodel to mimic a large teacher network. In this paper, we propose a method,\ncalled KDFM (Knowledge Distillation with Feature Maps), which improves the\neffectiveness of KD by learning the feature maps from the teacher network. Two\nmajor techniques used in KDFM are shared classifier and generative adversarial\nnetwork. Experimental results show that KDFM can use a four layers CNN to mimic\nDenseNet-40 and use MobileNet to mimic DenseNet-100. Both student networks have\nless than 1\\% accuracy loss comparing to their teacher models for CIFAR-100\ndatasets. The student networks are 2-6 times faster than their teacher models\nfor inference, and the model size of MobileNet is less than half of\nDenseNet-100's.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 11:03:04 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Chen", "Wei-Chun", ""], ["Chang", "Chia-Che", ""], ["Lu", "Chien-Yu", ""], ["Lee", "Che-Rung", ""]]}, {"id": "1812.00668", "submitter": "Eoin Brophy", "authors": "Eoin Brophy and Jos\\'e Juan Dominguez Veiga and Zhengwei Wang and Alan\n  F. Smeaton and Tomas E. Ward", "title": "An Interpretable Machine Vision Approach to Human Activity Recognition\n  using Photoplethysmograph Sensor Data", "comments": "26th AIAI Irish Conference on Artificial Intelligence and Cognitive\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current gold standard for human activity recognition (HAR) is based on\nthe use of cameras. However, the poor scalability of camera systems renders\nthem impractical in pursuit of the goal of wider adoption of HAR in mobile\ncomputing contexts. Consequently, researchers instead rely on wearable sensors\nand in particular inertial sensors. A particularly prevalent wearable is the\nsmart watch which due to its integrated inertial and optical sensing\ncapabilities holds great potential for realising better HAR in a non-obtrusive\nway. This paper seeks to simplify the wearable approach to HAR through\ndetermining if the wrist-mounted optical sensor alone typically found in a\nsmartwatch or similar device can be used as a useful source of data for\nactivity recognition. The approach has the potential to eliminate the need for\nthe inertial sensing element which would in turn reduce the cost of and\ncomplexity of smartwatches and fitness trackers. This could potentially\ncommoditise the hardware requirements for HAR while retaining the functionality\nof both heart rate monitoring and activity capture all from a single optical\nsensor. Our approach relies on the adoption of machine vision for activity\nrecognition based on suitably scaled plots of the optical signals. We take this\napproach so as to produce classifications that are easily explainable and\ninterpretable by non-technical users. More specifically, images of\nphotoplethysmography signal time series are used to retrain the penultimate\nlayer of a convolutional neural network which has initially been trained on the\nImageNet database. We then use the 2048 dimensional features from the\npenultimate layer as input to a support vector machine. Results from the\nexperiment yielded an average classification accuracy of 92.3%. This result\noutperforms that of an optical and inertial sensor combined (78%) and\nillustrates the capability of HAR systems using...\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 11:10:59 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Brophy", "Eoin", ""], ["Veiga", "Jos\u00e9 Juan Dominguez", ""], ["Wang", "Zhengwei", ""], ["Smeaton", "Alan F.", ""], ["Ward", "Tomas E.", ""]]}, {"id": "1812.00688", "submitter": "Yu-Bang Zheng", "authors": "Yu-Bang Zheng and Ting-Zhu Huang and Xi-Le Zhao and Tai-Xiang Jiang\n  and Teng-Yu Ji and Tian-Hui Ma", "title": "Tensor N-tubal rank and its convex relaxation for low-rank tensor\n  recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As low-rank modeling has achieved great success in tensor recovery, many\nresearch efforts devote to defining the tensor rank. Among them, the recent\npopular tensor tubal rank, defined based on the tensor singular value\ndecomposition (t-SVD), obtains promising results. However, the framework of the\nt-SVD and the tensor tubal rank are applicable only to three-way tensors and\nlack of flexibility to handle different correlations along different modes. To\ntackle these two issues, we define a new tensor unfolding operator, named\nmode-$k_1k_2$ tensor unfolding, as the process of lexicographically stacking\nthe mode-$k_1k_2$ slices of an $N$-way tensor into a three-way tensor, which is\na three-way extension of the well-known mode-$k$ tensor matricization. Based on\nit, we define a novel tensor rank, the tensor $N$-tubal rank, as a vector whose\nelements contain the tubal rank of all mode-$k_1k_2$ unfolding tensors, to\ndepict the correlations along different modes. To efficiently minimize the\nproposed $N$-tubal rank, we establish its convex relaxation: the weighted sum\nof tensor nuclear norm (WSTNN). Then, we apply WSTNN to low-rank tensor\ncompletion (LRTC) and tensor robust principal component analysis (TRPCA). The\ncorresponding WSTNN-based LRTC and TRPCA models are proposed, and two efficient\nalternating direction method of multipliers (ADMM)-based algorithms are\ndeveloped to solve the proposed models. Numerical experiments demonstrate that\nthe proposed models significantly outperform the compared ones.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 11:51:59 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zheng", "Yu-Bang", ""], ["Huang", "Ting-Zhu", ""], ["Zhao", "Xi-Le", ""], ["Jiang", "Tai-Xiang", ""], ["Ji", "Teng-Yu", ""], ["Ma", "Tian-Hui", ""]]}, {"id": "1812.00693", "submitter": "Stefan Reinhold", "authors": "Stefan Reinhold. Timo Damm, Lukas Huber, Reimer Andresen, Reinhard\n  Barkmann, Claus-C. Gl\\\"uer, Reinhard Koch", "title": "An Analysis by Synthesis Approach for Automatic Vertebral Shape\n  Identification in Clinical QCT", "comments": "Presented on German Conference on Pattern Recognition (GCPR) 2018 in\n  Stuttgart", "journal-ref": null, "doi": "10.1007/978-3-030-12939-2_6", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative computed tomography (QCT) is a widely used tool for osteoporosis\ndiagnosis and monitoring. The assessment of cortical markers like cortical bone\nmineral density (BMD) and thickness is a demanding task, mainly because of the\nlimited spatial resolution of QCT. We propose a direct model based method to\nautomatically identify the surface through the center of the cortex of human\nvertebra. We develop a statistical bone model and analyze its probability\ndistribution after the imaging process. Using an as-rigid-as-possible\ndeformation we find the cortical surface that maximizes the likelihood of our\nmodel given the input volume. Using the European Spine Phantom (ESP) and a high\nresolution \\mu CT scan of a cadaveric vertebra, we show that the proposed\nmethod is able to accurately identify the real center of cortex ex-vivo. To\ndemonstrate the in-vivo applicability of our method we use manually obtained\nsurfaces for comparison.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 12:07:01 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Damm", "Stefan Reinhold. Timo", ""], ["Huber", "Lukas", ""], ["Andresen", "Reimer", ""], ["Barkmann", "Reinhard", ""], ["Gl\u00fcer", "Claus-C.", ""], ["Koch", "Reinhard", ""]]}, {"id": "1812.00709", "submitter": "Yizhak Ben-Shabat", "authors": "Yizhak Ben-Shabat, Michael Lindenbaum, Anath Fischer", "title": "Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds using\n  Convolutional Neural Networks", "comments": "Code will be available after publication. Figure quality reduced to\n  fit size requirement. Higher quality images will be available in the final\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a normal estimation method for unstructured 3D\npoint clouds. This method, called Nesti-Net, builds on a new local point cloud\nrepresentation which consists of multi-scale point statistics (MuPS), estimated\non a local coarse Gaussian grid. This representation is a suitable input to a\nCNN architecture. The normals are estimated using a mixture-of-experts (MoE)\narchitecture, which relies on a data-driven approach for selecting the optimal\nscale around each point and encourages sub-network specialization. Interesting\ninsights into the network's resource distribution are provided. The scale\nprediction significantly improves robustness to different noise levels, point\ndensity variations and different levels of detail. We achieve state-of-the-art\nresults on a benchmark synthetic dataset and present qualitative results on\nreal scanned scenes.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 12:27:02 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ben-Shabat", "Yizhak", ""], ["Lindenbaum", "Michael", ""], ["Fischer", "Anath", ""]]}, {"id": "1812.00722", "submitter": "Petros Koutras", "authors": "Petros Koutras and Petros Maragos", "title": "SUSiNet: See, Understand and Summarize it", "comments": "CVPR Workshops 2019 (Mutual benefits of cognitive and computer\n  vision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a multi-task spatio-temporal network, called SUSiNet,\nthat can jointly tackle the spatio-temporal problems of saliency estimation,\naction recognition and video summarization. Our approach employs a single\nnetwork that is jointly end-to-end trained for all tasks with multiple and\ndiverse datasets related to the exploring tasks. The proposed network uses a\nunified architecture that includes global and task specific layer and produces\nmultiple output types, i.e., saliency maps or classification labels, by\nemploying the same video input. Moreover, one additional contribution is that\nthe proposed network can be deeply supervised through an attention module that\nis related to human attention as it is expressed by eye-tracking data. From the\nextensive evaluation, on seven different datasets, we have observed that the\nmulti-task network performs as well as the state-of-the-art single-task methods\n(or in some cases better), while it requires less computational budget than\nhaving one independent network per each task.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 13:21:51 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 17:58:25 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Koutras", "Petros", ""], ["Maragos", "Petros", ""]]}, {"id": "1812.00723", "submitter": "Lianwen Jin", "authors": "Shuaitao Zhang, Yuliang Liu, Lianwen Jin, Yaoxiong Huang, Songxuan Lai", "title": "EnsNet: Ensconce Text in the Wild", "comments": "8 pages, 8 figures, 2 tables, accepted to appear in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed for removing text from natural images. The challenge\nis to first accurately localize text on the stroke-level and then replace it\nwith a visually plausible background. Unlike previous methods that require\nimage patches to erase scene text, our method, namely ensconce network\n(EnsNet), can operate end-to-end on a single image without any prior knowledge.\nThe overall structure is an end-to-end trainable FCN-ResNet-18 network with a\nconditional generative adversarial network (cGAN). The feature of the former is\nfirst enhanced by a novel lateral connection structure and then refined by four\ncarefully designed losses: multiscale regression loss and content loss, which\ncapture the global discrepancy of different level features; texture loss and\ntotal variation loss, which primarily target filling the text region and\npreserving the reality of the background. The latter is a novel local-sensitive\nGAN, which attentively assesses the local consistency of the text erased\nregions. Both qualitative and quantitative sensitivity experiments on synthetic\nimages and the ICDAR 2013 dataset demonstrate that each component of the EnsNet\nis essential to achieve a good performance. Moreover, our EnsNet can\nsignificantly outperform previous state-of-the-art methods in terms of all\nmetrics. In addition, a qualitative experiment conducted on the SMBNet dataset\nfurther demonstrates that the proposed method can also preform well on general\nobject (such as pedestrians) removal tasks. EnsNet is extremely fast, which can\npreform at 333 fps on an i5-8600 CPU device.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 13:25:26 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhang", "Shuaitao", ""], ["Liu", "Yuliang", ""], ["Jin", "Lianwen", ""], ["Huang", "Yaoxiong", ""], ["Lai", "Songxuan", ""]]}, {"id": "1812.00725", "submitter": "Yiming Zuo", "authors": "Yiming Zuo, Weichao Qiu, Lingxi Xie, Fangwei Zhong, Yizhou Wang, Alan\n  L. Yuille", "title": "CRAVES: Controlling Robotic Arm with a Vision-based Economic System", "comments": "10 pages, 6 figures", "journal-ref": "In Proceedings of the IEEE Conference on Computer Vision and\n  Pattern Recognition (2019) 4214-4223", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a robotic arm to accomplish real-world tasks has been attracting\nincreasing attention in both academia and industry. This work discusses the\nrole of computer vision algorithms in this field. We focus on low-cost arms on\nwhich no sensors are equipped and thus all decisions are made upon visual\nrecognition, e.g., real-time 3D pose estimation. This requires annotating a lot\nof training data, which is not only time-consuming but also laborious.\n  In this paper, we present an alternative solution, which uses a 3D model to\ncreate a large number of synthetic data, trains a vision model in this virtual\ndomain, and applies it to real-world images after domain adaptation. To this\nend, we design a semi-supervised approach, which fully leverages the geometric\nconstraints among keypoints. We apply an iterative algorithm for optimization.\nWithout any annotations on real images, our algorithm generalizes well and\nproduces satisfying results on 3D pose estimation, which is evaluated on two\nreal-world datasets. We also construct a vision-based control system for task\naccomplishment, for which we train a reinforcement learning agent in a virtual\nenvironment and apply it to the real-world. Moreover, our approach, with merely\na 3D model being required, has the potential to generalize to other types of\nmulti-rigid-body dynamic systems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 13:28:29 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 13:26:04 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Zuo", "Yiming", ""], ["Qiu", "Weichao", ""], ["Xie", "Lingxi", ""], ["Zhong", "Fangwei", ""], ["Wang", "Yizhou", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1812.00733", "submitter": "Masanori Suganuma", "authors": "Masanori Suganuma, Xing Liu, Takayuki Okatani", "title": "Attention-based Adaptive Selection of Operations for Image Restoration\n  in the Presence of Unknown Combined Distortions", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have been conducted so far on image restoration, the problem of\nrestoring a clean image from its distorted version. There are many different\ntypes of distortion which affect image quality. Previous studies have focused\non single types of distortion, proposing methods for removing them. However,\nimage quality degrades due to multiple factors in the real world. Thus,\ndepending on applications, e.g., vision for autonomous cars or surveillance\ncameras, we need to be able to deal with multiple combined distortions with\nunknown mixture ratios. For this purpose, we propose a simple yet effective\nlayer architecture of neural networks. It performs multiple operations in\nparallel, which are weighted by an attention mechanism to enable selection of\nproper operations depending on the input. The layer can be stacked to form a\ndeep network, which is differentiable and thus can be trained in an end-to-end\nfashion by gradient descent. The experimental results show that the proposed\nmethod works better than previous methods by a good margin on tasks of\nrestoring images with multiple combined distortions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 13:50:40 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 11:51:26 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Suganuma", "Masanori", ""], ["Liu", "Xing", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1812.00739", "submitter": "Aryaman Gupta", "authors": "Aryaman Gupta, Kalpit Thakkar, Vineet Gandhi and P J Narayanan", "title": "Nose, eyes and ears: Head pose estimation by locating facial keypoints", "comments": "4 pages, ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular head pose estimation requires learning a model that computes the\nintrinsic Euler angles for pose (yaw, pitch, roll) from an input image of human\nface. Annotating ground truth head pose angles for images in the wild is\ndifficult and requires ad-hoc fitting procedures (which provides only coarse\nand approximate annotations). This highlights the need for approaches which can\ntrain on data captured in controlled environment and generalize on the images\nin the wild (with varying appearance and illumination of the face). Most\npresent day deep learning approaches which learn a regression function directly\non the input images fail to do so. To this end, we propose to use a higher\nlevel representation to regress the head pose while using deep learning\narchitectures. More specifically, we use the uncertainty maps in the form of 2D\nsoft localization heatmap images over five facial keypoints, namely left ear,\nright ear, left eye, right eye and nose, and pass them through an convolutional\nneural network to regress the head-pose. We show head pose estimation results\non two challenging benchmarks BIWI and AFLW and our approach surpasses the\nstate of the art on both the datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 14:04:04 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Gupta", "Aryaman", ""], ["Thakkar", "Kalpit", ""], ["Gandhi", "Vineet", ""], ["Narayanan", "P J", ""]]}, {"id": "1812.00740", "submitter": "David Stutz", "authors": "David Stutz, Matthias Hein, Bernt Schiele", "title": "Disentangling Adversarial Robustness and Generalization", "comments": "Conference on Computer Vision and Pattern Recognition 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining deep networks that are robust against adversarial examples and\ngeneralize well is an open problem. A recent hypothesis even states that both\nrobust and accurate models are impossible, i.e., adversarial robustness and\ngeneralization are conflicting goals. In an effort to clarify the relationship\nbetween robustness and generalization, we assume an underlying, low-dimensional\ndata manifold and show that: 1. regular adversarial examples leave the\nmanifold; 2. adversarial examples constrained to the manifold, i.e.,\non-manifold adversarial examples, exist; 3. on-manifold adversarial examples\nare generalization errors, and on-manifold adversarial training boosts\ngeneralization; 4. regular robustness and generalization are not necessarily\ncontradicting goals. These assumptions imply that both robust and accurate\nmodels are possible. However, different models (architectures, training\nstrategies etc.) can exhibit different robustness and generalization\ncharacteristics. To confirm our claims, we present extensive experiments on\nsynthetic data (with known manifold) as well as on EMNIST, Fashion-MNIST and\nCelebA.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 14:04:35 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 10:25:38 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Stutz", "David", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1812.00786", "submitter": "Bradley Gram-Hansen", "authors": "Patrick Helber, Bradley Gram-Hansen, Indhu Varatharajan, Faiza Azam,\n  Alejandro Coca-Castro, Veronika Kopackova, Piotr Bilinski", "title": "Generating Material Maps to Map Informal Settlements", "comments": "Appeared at the 32nd Conference on Neural Information Processing\n  Systems (NeurlPS 2018) Machine Learning for the Developing World (ML4DW)\n  Workshop", "journal-ref": "NeurlPS workshop on Machine Learning for the Developing World\n  (ML4DW), 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and mapping informal settlements encompasses several of the United\nNations sustainable development goals. This is because informal settlements are\nhome to the most socially and economically vulnerable people on the planet.\nThus, understanding where these settlements are is of paramount importance to\nboth government and non-government organizations (NGOs), such as the United\nNations Children's Fund (UNICEF), who can use this information to deliver\neffective social and economic aid. We propose a method that detects and maps\nthe locations of informal settlements using only freely available, Sentinel-2\nlow-resolution satellite spectral data and socio-economic data. This is in\ncontrast to previous studies that only use costly very-high resolution (VHR)\nsatellite and aerial imagery. We show how we can detect informal settlements by\ncombining both domain knowledge and machine learning techniques, to build a\nclassifier that looks for known roofing materials used in informal settlements.\nPlease find additional material at\nhttps://frontierdevelopmentlab.github.io/informal-settlements/.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 09:09:41 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 12:19:50 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Helber", "Patrick", ""], ["Gram-Hansen", "Bradley", ""], ["Varatharajan", "Indhu", ""], ["Azam", "Faiza", ""], ["Coca-Castro", "Alejandro", ""], ["Kopackova", "Veronika", ""], ["Bilinski", "Piotr", ""]]}, {"id": "1812.00805", "submitter": "Berk Kaya", "authors": "Berk Kaya, Yigit Baran Can, Radu Timofte", "title": "Towards Spectral Estimation from a Single RGB Image in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to the current literature, we address the problem of estimating\nthe spectrum from a single common trichromatic RGB image obtained under\nunconstrained settings (e.g. unknown camera parameters, unknown scene radiance,\nunknown scene contents). For this we use a reference spectrum as provided by a\nhyperspectral image camera, and propose efficient deep learning solutions for\nsensitivity function estimation and spectral reconstruction from a single RGB\nimage. We further expand the concept of spectral reconstruction such that to\nwork for RGB images taken in the wild and propose a solution based on a\nconvolutional network conditioned on the estimated sensitivity function.\nBesides the proposed solutions, we study also generic and sensitivity\nspecialized models and discuss their limitations. We achieve state-of-the-art\ncompetitive results on the standard example-based spectral reconstruction\nbenchmarks: ICVL, CAVE, NUS and NTIRE. Moreover, our experiments show that, for\nthe first time, accurate spectral estimation from a single RGB image in the\nwild is within our reach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 14:58:26 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Kaya", "Berk", ""], ["Can", "Yigit Baran", ""], ["Timofte", "Radu", ""]]}, {"id": "1812.00810", "submitter": "Lijun Zhang", "authors": "Lijun Zhang and Yujin Zhang and Yongbin Gao", "title": "A Wasserstein GAN model with the total variational regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the generative adversarial nets (GANs) are remarkably\ndifficult to train. The recently proposed Wasserstein GAN (WGAN) creates\nprincipled research directions towards addressing these issues. But we found in\npractice that gradient penalty WGANs (GP-WGANs) still suffer from training\ninstability. In this paper, we combine a Total Variational (TV) regularizing\nterm into the WGAN formulation instead of weight clipping or gradient penalty,\nwhich implies that the Lipschitz constraint is enforced on the critic network.\nOur proposed method is more stable at training than GP-WGANs and works well\nacross varied GAN architectures. We also present a method to control the\ntrade-off between image diversity and visual quality. It does not bring any\ncomputation burden.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 15:00:33 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhang", "Lijun", ""], ["Zhang", "Yujin", ""], ["Gao", "Yongbin", ""]]}, {"id": "1812.00812", "submitter": "Patrick Helber", "authors": "Patrick Helber, Bradley Gram-Hansen, Indhu Varatharajan, Faiza Azam,\n  Alejandro Coca-Castro, Veronika Kopackova, Piotr Bilinski", "title": "Mapping Informal Settlements in Developing Countries with\n  Multi-resolution, Multi-spectral Data", "comments": "arXiv admin note: text overlap with arXiv:1812.00786", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and mapping informal settlements encompasses several of the United\nNations sustainable development goals. This is because informal settlements are\nhome to the most socially and economically vulnerable people on the planet.\nThus, understanding where these settlements are is of paramount importance to\nboth government and non-government organizations (NGOs), such as the United\nNations Children's Fund (UNICEF), who can use this information to deliver\neffective social and economic aid. We propose two effective methods for\ndetecting and mapping the locations of informal settlements. One uses only\nlow-resolution (LR), freely available, Sentinel-2 multispectral satellite\nimagery with noisy annotations, whilst the other is a deep learning approach\nthat uses only costly very-high-resolution (VHR) satellite imagery. To our\nknowledge, we are the first to map informal settlements successfully with\nlow-resolution satellite imagery. We extensively evaluate and compare the\nproposed methods. Please find additional material at\nhttps://frontierdevelopmentlab.github.io/informal-settlements/.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 10:38:37 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Helber", "Patrick", ""], ["Gram-Hansen", "Bradley", ""], ["Varatharajan", "Indhu", ""], ["Azam", "Faiza", ""], ["Coca-Castro", "Alejandro", ""], ["Kopackova", "Veronika", ""], ["Bilinski", "Piotr", ""]]}, {"id": "1812.00825", "submitter": "Po-Hsuan Cameron Chen", "authors": "Po-Hsuan Cameron Chen, Krishna Gadepalli, Robert MacDonald, Yun Liu,\n  Kunal Nagpal, Timo Kohlberger, Jeffrey Dean, Greg S. Corrado, Jason D. Hipp,\n  Martin C. Stumpe", "title": "Microscope 2.0: An Augmented Reality Microscope with Real-time\n  Artificial Intelligence Integration", "comments": null, "journal-ref": "Nature Medicine (2019)", "doi": "10.1038/s41591-019-0539-7", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brightfield microscope is instrumental in the visual examination of both\nbiological and physical samples at sub-millimeter scales. One key clinical\napplication has been in cancer histopathology, where the microscopic assessment\nof the tissue samples is used for the diagnosis and staging of cancer and thus\nguides clinical therapy. However, the interpretation of these samples is\ninherently subjective, resulting in significant diagnostic variability.\nMoreover, in many regions of the world, access to pathologists is severely\nlimited due to lack of trained personnel. In this regard, Artificial\nIntelligence (AI) based tools promise to improve the access and quality of\nhealthcare. However, despite significant advances in AI research, integration\nof these tools into real-world cancer diagnosis workflows remains challenging\nbecause of the costs of image digitization and difficulties in deploying AI\nsolutions. Here we propose a cost-effective solution to the integration of AI:\nthe Augmented Reality Microscope (ARM). The ARM overlays AI-based information\nonto the current view of the sample through the optical pathway in real-time,\nenabling seamless integration of AI into the regular microscopy workflow. We\ndemonstrate the utility of ARM in the detection of lymph node metastases in\nbreast cancer and the identification of prostate cancer with a latency that\nsupports real-time workflows. We anticipate that ARM will remove barriers\ntowards the use of AI in microscopic analysis and thus improve the accuracy and\nefficiency of cancer diagnosis. This approach is applicable to other microscopy\ntasks and AI algorithms in the life sciences and beyond.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 21:02:50 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 05:36:36 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Chen", "Po-Hsuan Cameron", ""], ["Gadepalli", "Krishna", ""], ["MacDonald", "Robert", ""], ["Liu", "Yun", ""], ["Nagpal", "Kunal", ""], ["Kohlberger", "Timo", ""], ["Dean", "Jeffrey", ""], ["Corrado", "Greg S.", ""], ["Hipp", "Jason D.", ""], ["Stumpe", "Martin C.", ""]]}, {"id": "1812.00828", "submitter": "Md Sahidullah", "authors": "Arnab Poddar, Md Sahidullah, Goutam Saha", "title": "Novel Quality Metric for Duration Variability Compensation in Speaker\n  Verification using i-Vectors", "comments": "Accepted and presented in ICAPR 2017, Bangalore, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speaker verification (ASV) is the process to recognize persons\nusing voice as biometric. The ASV systems show considerable recognition\nperformance with sufficient amount of speech from matched condition. One of the\ncrucial challenges of ASV technology is to improve recognition performance with\nspeech segments of short duration. In short duration condition, the model\nparameters are not properly estimated due to inadequate speech information, and\nthis results poor recognition accuracy even with the state-of-the-art i-vector\nbased ASV system. We hypothesize that considering the estimation quality during\nrecognition process would help to improve the ASV performance. This can be\nincorporated as a quality measure during fusion of ASV systems. This paper\ninvestigates a new quality measure for i-vector representation of speech\nutterances computed directly from Baum-Welch statistics. The proposed metric is\nsubsequently used as quality measure during fusion of ASV systems. In\nexperiments with the NIST SRE 2008 corpus, We have shown that inclusion of\nproposed quality metric exhibits considerable improvement in speaker\nverification performance. The results also indicate the potentiality of the\nproposed method in real-world scenario with short test utterances.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 15:20:59 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Poddar", "Arnab", ""], ["Sahidullah", "Md", ""], ["Saha", "Goutam", ""]]}, {"id": "1812.00862", "submitter": "Lukas Kiefer", "authors": "Lukas Kiefer, Martin Storath, Andreas Weinmann", "title": "Iterative Potts minimization for the recovery of signals with\n  discontinuities from indirect measurements -- the multivariate case", "comments": "44 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signals and images with discontinuities appear in many problems in such\ndiverse areas as biology, medicine, mechanics, and electrical engineering. The\nconcrete data are often discrete, indirect and noisy measurements of some\nquantities describing the signal under consideration. A frequent task is to\nfind the segments of the signal or image which corresponds to finding the\ndiscontinuities or jumps in the data. Methods based on minimizing the piecewise\nconstant Mumford-Shah functional -- whose discretized version is known as Potts\nfunctional -- are advantageous in this scenario, in particular, in connection\nwith segmentation. However, due to their non-convexity, minimization of such\nfunctionals is challenging. In this paper we propose a new iterative\nminimization strategy for the multivariate Potts functional dealing with\nindirect, noisy measurements. We provide a convergence analysis and underpin\nour findings with numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 16:19:05 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 10:01:06 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kiefer", "Lukas", ""], ["Storath", "Martin", ""], ["Weinmann", "Andreas", ""]]}, {"id": "1812.00874", "submitter": "Chiranjoy Chattopadhyay", "authors": "Shreya Goyal, Satya Bhavsar, Shreya Patel, Chiranjoy Chattopadhyay,\n  Gaurav Bhatnagar", "title": "SUGAMAN: Describing Floor Plans for Visually Impaired by Annotation\n  Learning and Proximity based Grammar", "comments": "19 pages, 20 figures, Under review in IET Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose SUGAMAN (Supervised and Unified framework using\nGrammar and Annotation Model for Access and Navigation). SUGAMAN is a Hindi\nword meaning \"easy passage from one place to another\". SUGAMAN synthesizes\ntextual description from a given floor plan image for the visually impaired. A\nvisually impaired person can navigate in an indoor environment using the\ntextual description generated by SUGAMAN. With the help of a text reader\nsoftware, the target user can understand the rooms within the building and\narrangement of furniture to navigate. SUGAMAN is the first framework for\ndescribing a floor plan and giving direction for obstacle-free movement within\na building. We learn $5$ classes of room categories from $1355$ room image\nsamples under a supervised learning paradigm. These learned annotations are fed\ninto a description synthesis framework to yield a holistic description of a\nfloor plan image. We demonstrate the performance of various supervised\nclassifiers on room learning. We also provide a comparative analysis of system\ngenerated and human written descriptions. SUGAMAN gives state of the art\nperformance on challenging, real-world floor plan images. This work can be\napplied to areas like understanding floor plans of historical monuments,\nstability analysis of buildings, and retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 05:38:40 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Goyal", "Shreya", ""], ["Bhavsar", "Satya", ""], ["Patel", "Shreya", ""], ["Chattopadhyay", "Chiranjoy", ""], ["Bhatnagar", "Gaurav", ""]]}, {"id": "1812.00875", "submitter": "Henry Adams", "authors": "Henry Adams, Johnathan Bush, Brittany Carr, Lara Kassab, Joshua Mirth", "title": "A torus model for optical flow", "comments": null, "journal-ref": "Pattern Recognition Letters 129 (2020), 304-310", "doi": "10.1016/j.patrec.2019.11.029", "report-no": null, "categories": "cs.CV math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a torus model for high-contrast patches of optical flow. Our model\nis derived from a database of ground-truth optical flow from the\ncomputer-generated video \\emph{Sintel}, collected by Butler et al.\\ in \\emph{A\nnaturalistic open source movie for optical flow evaluation}. Using persistent\nhomology and zigzag persistence, popular tools from the field of computational\ntopology, we show that the high-contrast $3\\times 3$ patches from this video\nare well-modeled by a \\emph{torus}, a nonlinear 2-dimensional manifold.\nFurthermore, we show that the optical flow torus model is naturally equipped\nwith the structure of a fiber bundle, related to the statistics of range image\npatches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 22:50:29 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 23:58:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Adams", "Henry", ""], ["Bush", "Johnathan", ""], ["Carr", "Brittany", ""], ["Kassab", "Lara", ""], ["Mirth", "Joshua", ""]]}, {"id": "1812.00876", "submitter": "Richard Jiang", "authors": "Ranjith K Dinakaran, Philip Easom, Ahmed Bouridane, Li Zhang, Richard\n  Jiang, Fozia Mehboob and Abdul Rauf", "title": "Deep Learning based Pedestrian Detection at Distance in Smart Cities", "comments": "Artificial Intelligence Conference 2019 | IntelliSys 2019 |\n  https://saiconference.com/IntelliSys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks (GANs) have been promising for many computer\nvision problems due to their powerful capabilities to enhance the data for\ntraining and test. In this paper, we leveraged GANs and proposed a new\narchitecture with a cascaded Single Shot Detector (SSD) for pedestrian\ndetection at distance, which is yet a challenge due to the varied sizes of\npedestrians in videos at distance. To overcome the low-resolution issues in\npedestrian detection at distance, DCGAN is employed to improve the resolution\nfirst to reconstruct more discriminative features for a SSD to detect objects\nin images or videos. A crucial advantage of our method is that it learns a\nmulti-scale metric to distinguish multiple objects at different distances under\none image, while DCGAN serves as an encoder-decoder platform to generate parts\nof an image that contain better discriminative information. To measure the\neffectiveness of our proposed method, experiments were carried out on the\nCanadian Institute for Advanced Research (CIFAR) dataset, and it was\ndemonstrated that the proposed new architecture achieved a much better\ndetection rate, particularly on vehicles and pedestrians at distance, making it\nhighly suitable for smart cities applications that need to discover key objects\nor pedestrians at distance.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 17:15:24 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 13:17:00 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 13:28:52 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 17:17:59 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Dinakaran", "Ranjith K", ""], ["Easom", "Philip", ""], ["Bouridane", "Ahmed", ""], ["Zhang", "Li", ""], ["Jiang", "Richard", ""], ["Mehboob", "Fozia", ""], ["Rauf", "Abdul", ""]]}, {"id": "1812.00877", "submitter": "Glib Kechyn", "authors": "Glib Kechyn", "title": "Automatic lesion boundary detection in dermoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript addresses the problem of the automatic lesion boundary\ndetection in dermoscopy, using deep neural networks. An approach is based on\nthe adaptation of the U-net convolutional neural network with skip connections\nfor lesion boundary segmentation task. I hope this paper could serve, to some\nextent, as an experiment of using deep convolutional networks in biomedical\nsegmentation task and as a guideline of the boundary detection benchmark,\ninspiring further attempts and researches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 22:36:36 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Kechyn", "Glib", ""]]}, {"id": "1812.00879", "submitter": "Sa\\'ul Alonso-Monsalve", "authors": "Sa\\'ul Alonso-Monsalve and Leigh H. Whitehead", "title": "Image-based model parameter optimization using Model-Assisted Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2020.2969327", "report-no": null, "categories": "cs.CV cs.LG hep-ex stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose and demonstrate the use of a model-assisted generative adversarial\nnetwork (GAN) to produce fake images that accurately match true images through\nthe variation of the parameters of the model that describes the features of the\nimages. The generator learns the model parameter values that produce fake\nimages that best match the true images. Two case studies show excellent\nagreement between the generated best match parameters and the true parameters.\nThe best match model parameter values can be used to retune the default\nsimulation to minimize any bias when applying image recognition techniques to\nfake and true images. In the case of a real-world experiment, the true images\nare experimental data with unknown true model parameter values, and the fake\nimages are produced by a simulation that takes the model parameters as input.\nThe model-assisted GAN uses a convolutional neural network to emulate the\nsimulation for all parameter values that, when trained, can be used as a\nconditional generator for fast fake-image production.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 17:27:53 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 08:58:22 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Alonso-Monsalve", "Sa\u00fal", ""], ["Whitehead", "Leigh H.", ""]]}, {"id": "1812.00880", "submitter": "Jonathan P. Chen", "authors": "Jonathan P. Chen, Fritz Obermeyer, Vladimir Lyapunov, Lionel Gueguen,\n  Noah D. Goodman", "title": "Joint Mapping and Calibration via Differentiable Sensor Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We leverage automatic differentiation (AD) and probabilistic programming to\ndevelop an end-to-end optimization algorithm for batch triangulation of a large\nnumber of unknown objects. Given noisy detections extracted from noisily\ngeo-located street level imagery without depth information, we jointly estimate\nthe number and location of objects of different types, together with parameters\nfor sensor noise characteristics and prior distribution of objects conditioned\non side information. The entire algorithm is framed as nested stochastic\nvariational inference. An inner loop solves a soft data association problem via\nloopy belief propagation; a middle loop performs soft EM clustering using a\nregularized Newton solver (leveraging an AD framework); an outer loop\nbackpropagates through the inner loops to train global parameters. We place\npriors over sensor parameters for different traffic object types, and\ndemonstrate improvements with richer priors incorporating knowledge of the\nenvironment.\n  We test our algorithm on detections of road signs observed by cars with\nmounted cameras, though in practice this technique can be used for any\ngeo-tagged images. The detections were extracted by neural image detectors and\nclassifiers, and we independently triangulate each type of sign (e.g. stop,\ntraffic light). We find that our model is more robust to DNN misclassifications\nthan current methods, generalizes across sign types, and can use geometric\ninformation to increase precision. Our algorithm outperforms our current\nproduction baseline based on k-means clustering. We show that variational\ninference training allows generalization by learning sign-specific parameters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 06:22:06 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 04:58:10 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Chen", "Jonathan P.", ""], ["Obermeyer", "Fritz", ""], ["Lyapunov", "Vladimir", ""], ["Gueguen", "Lionel", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1812.00882", "submitter": "Sami Brandt", "authors": "Sami Sebastian Brandt", "title": "Integral Geometric Dual Distributions of Multilinear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an integral geometric approach for computing dual distributions\nfor the parameter distributions of multilinear models. The dual distributions\ncan be computed from, for example, the parameter distributions of conics,\nmultiple view tensors, homographies, or as simple entities as points, lines,\nand planes. The dual distributions have analytical forms that follow from the\nasymptotic normality property of the maximum likelihood estimator and an\napplication of integral transforms, fundamentally the generalised Radon\ntransforms, on the probability density of the parameters. The approach allows\nus, for instance, to look at the uncertainty distributions in feature\ndistributions, which are essentially tied to the distribution of training data,\nand helps us to derive conditional distributions for interesting variables and\ncharacterise confidence intervals of the estimates.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:12:51 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Brandt", "Sami Sebastian", ""]]}, {"id": "1812.00883", "submitter": "Shishira Maiya", "authors": "Sudharshan Chandra Babu, Shishira R Maiya, Sivasankar Elango", "title": "Relation Networks for Optic Disc and Fovea Localization in Retinal\n  Images", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy is the leading cause of blindness in the world. At least\n90\\% of new cases can be reduced with proper treatment and monitoring of the\neyes. However, scanning the entire population of patients is a difficult\nendeavor. Computer-aided diagnosis tools in retinal image analysis can make the\nprocess scalable and efficient. In this work, we focus on the problem of\nlocalizing the centers of the Optic disc and Fovea, a task crucial to the\nanalysis of retinal scans. Current systems recognize the Optic disc and Fovea\nindividually, without exploiting their relations during learning. We propose a\nnovel approach to localizing the centers of the Optic disc and Fovea by\nsimultaneously processing them and modeling their relative geometry and\nappearance. We show that our approach improves localization and recognition by\nincorporating object-object relations efficiently, and achieves highly\ncompetitive results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 14:51:55 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Babu", "Sudharshan Chandra", ""], ["Maiya", "Shishira R", ""], ["Elango", "Sivasankar", ""]]}, {"id": "1812.00884", "submitter": "Shazia Akbar", "authors": "Shazia Akbar, Anne L. Martel", "title": "Cluster-Based Learning from Weakly Labeled Bags in Digital Pathology", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/27", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To alleviate the burden of gathering detailed expert annotations when\ntraining deep neural networks, we propose a weakly supervised learning approach\nto recognize metastases in microscopic images of breast lymph nodes. We\ndescribe an alternative training loss which clusters weakly labeled bags in\nlatent space to inform relevance of patch-instances during training of a\nconvolutional neural network. We evaluate our method on the Camelyon dataset\nwhich contains high-resolution digital slides of breast lymph nodes, where\nlabels are provided at the image-level and only subsets of patches are made\navailable during training.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 15:05:22 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Akbar", "Shazia", ""], ["Martel", "Anne L.", ""]]}, {"id": "1812.00886", "submitter": "Wei Wei", "authors": "Wei Wei, Lingjie Xu, Lingling Jin, Wei Zhang, Tianjun Zhang", "title": "AI Matrix - Synthetic Benchmarks for DNN", "comments": "Accepted by SC' 18\n  https://sc18.supercomputing.org/proceedings/tech_poster/tech_poster_pages/post153.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) architectures, such as convolutional neural\nnetworks (CNN), involve heavy computation and require hardware, such as CPU,\nGPU, and AI accelerators, to provide the massive computing power. With the many\nvarieties of AI hardware prevailing on the market, it is often hard to decide\nwhich one is the best to use. Thus, benchmarking AI hardware effectively\nbecomes important and is of great help to select and optimize AI hardware.\nUnfortunately, there are few AI benchmarks available in both academia and\nindustry. Examples are BenchNN[1], DeepBench[2], and Dawn Bench[3], which are\nusually a collection of typical real DNN applications. While these benchmarks\nprovide performance comparison across different AI hardware, they suffer from a\nnumber of drawbacks. First, they cannot adapt to the emerging changes of DNN\nalgorithms and are fixed once selected. Second, they contain tens to hundreds\nof applications and take very long time to finish running. Third, they are\nmainly selected from open sources, which are restricted by copyright and are\nnot representable to proprietary applications. In this work, a synthetic\nbenchmarks framework is firstly proposed to address the above drawbacks of AI\nbenchmarks. Instead of pre-selecting a set of open-sourced benchmarks and\nrunning all of them, the synthetic approach generates only a one or few\nbenchmarks that best represent a broad range of applications using profiled\nworkload characteristics data of these applications. Thus, it can adapt to\nemerging changes of new DNN algorithms by re-profiling new applications and\nupdating itself, greatly reduce benchmark count and running time, and strongly\nrepresent DNN applications of interests. The generated benchmarks are called AI\nMatrix, serving as a performance benchmarks matching the statistical workload\ncharacteristics of a combination of applications of interests.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:20:35 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Wei", "Wei", ""], ["Xu", "Lingjie", ""], ["Jin", "Lingling", ""], ["Zhang", "Wei", ""], ["Zhang", "Tianjun", ""]]}, {"id": "1812.00887", "submitter": "Donghui Yan", "authors": "Donghui Yan, Timothy W. Randolph, Jian Zou and Peng Gong", "title": "Incorporating Deep Features in the Analysis of Tissue Microarray Images", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue microarray (TMA) images have been used increasingly often in cancer\nstudies and the validation of biomarkers. TACOMA---a cutting-edge automatic\nscoring algorithm for TMA images---is comparable to pathologists in terms of\naccuracy and repeatability. Here we consider how this algorithm may be further\nimproved. Inspired by the recent success of deep learning, we propose to\nincorporate representations learnable through computation. We explore\nrepresentations of a group nature through unsupervised learning, e.g.,\nhierarchical clustering and recursive space partition. Information carried by\nclustering or spatial partitioning may be more concrete than the labels when\nthe data are heterogeneous, or could help when the labels are noisy. The use of\nsuch information could be viewed as regularization in model fitting. It is\nmotivated by major challenges in TMA image scoring---heterogeneity and label\nnoise, and the cluster assumption in semi-supervised learning. Using this\ninformation on TMA images of breast cancer, we have reduced the error rate of\nTACOMA by about 6%. Further simulations on synthetic data provide insights on\nwhen such representations would likely help. Although we focus on TMAs,\nlearnable representations of this type are expected to be applicable in other\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 04:18:17 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Yan", "Donghui", ""], ["Randolph", "Timothy W.", ""], ["Zou", "Jian", ""], ["Gong", "Peng", ""]]}, {"id": "1812.00888", "submitter": "D Yoan L Mekontchou Yomba", "authors": "D Yoan L. Mekontchou Yomba", "title": "A Consolidated Approach to Convolutional Neural Networks and the\n  Kolmogorov Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to precisely quantify similarity between various entities has\nbeen a fundamental complication in various problem spaces specifically in the\nclassification of cellular images. Contemporary similarity measures applied in\nthe domain of image processing proposed by the scientific community are mainly\npursued in supervised settings. In this work, we will explore the innovative\nalgorithmic normalized compression distance metric based on the information\ntheoretic concept of Kolmogorov Complexity. Additionally we will observe its\npossible implementation in Convolutional Neural Networks to facilitate and\nautomate the classification of Retinal Pigment Epithelial cell cultures for use\nin Age Related Macular Degeneration Stem Cell therapy in an unsupervised\nsetting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 01:58:39 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Yomba", "D Yoan L. Mekontchou", ""]]}, {"id": "1812.00889", "submitter": "Eduardo Ruiz", "authors": "Eduardo Ruiz and Walterio Mayol-Cuevas", "title": "What can I do here? Leveraging Deep 3D saliency and geometry for fast\n  and scalable multiple affordance detection", "comments": "10 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops and evaluates a novel method that allows for the\ndetection of affordances in a scalable and multiple-instance manner on visually\nrecovered pointclouds. Our approach has many advantages over alternative\nmethods, as it is based on highly parallelizable, one-shot learning that is\nfast in commodity hardware. The approach is hybrid in that it uses a geometric\nrepresentation together with a state-of-the-art deep learning method capable of\nidentifying 3D scene saliency. The geometric component allows for a compact and\nefficient representation, boosting the performance of the deep network\narchitecture which proved insufficient on its own. Moreover, our approach\nallows not only to predict whether an input scene affords or not the\ninteractions, but also the pose of the objects that allow these interactions to\ntake place. Our predictions align well with crowd-sourced human judgment as\nthey are preferred with 87% probability, show high rates of improvement with\nalmost four times (4x) better performance over a deep learning-only baseline\nand are seven times (7x) faster than previous art.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 16:39:32 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ruiz", "Eduardo", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "1812.00893", "submitter": "Deng Weijian", "authors": "Weijian Deng, Liang Zheng, and Jianbin Jiao", "title": "Domain Alignment with Triplets", "comments": "10 pages;This version is not fully edited and will be updated soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep domain adaptation methods can reduce the distribution discrepancy by\nlearning domain-invariant embedddings. However, these methods only focus on\naligning the whole data distributions, without considering the class-level\nrelations among source and target images. Thus, a target embeddings of a bird\nmight be aligned to source embeddings of an airplane. This semantic\nmisalignment can directly degrade the classifier performance on the target\ndataset. To alleviate this problem, we present a similarity constrained\nalignment (SCA) method for unsupervised domain adaptation. When aligning the\ndistributions in the embedding space, SCA enforces a similarity-preserving\nconstraint to maintain class-level relations among the source and target\nimages, i.e., if a source image and a target image are of the same class label,\ntheir corresponding embeddings are supposed to be aligned nearby, and vise\nversa. In the absence of target labels, we assign pseudo labels for target\nimages. Given labeled source images and pseudo-labeled target images, the\nsimilarity-preserving constraint can be implemented by minimizing the triplet\nloss. With the joint supervision of domain alignment loss and\nsimilarity-preserving constraint, we train a network to obtain domain-invariant\nembeddings with two critical characteristics, intra-class compactness and\ninter-class separability. Extensive experiments conducted on the two datasets\nwell demonstrate the effectiveness of SCA.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 16:46:29 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 11:59:40 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Deng", "Weijian", ""], ["Zheng", "Liang", ""], ["Jiao", "Jianbin", ""]]}, {"id": "1812.00898", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Mateusz Malinowski, Felix Hill, Ali Eslami, Oriol\n  Vinyals, Tejas Kulkarni", "title": "Generating Diverse Programs with Instruction Conditioned Reinforced\n  Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Deep Reinforcement Learning have led to agents that perform well\nacross a variety of sensory-motor domains. In this work, we study the setting\nin which an agent must learn to generate programs for diverse scenes\nconditioned on a given symbolic instruction. Final goals are specified to our\nagent via images of the scenes. A symbolic instruction consistent with the goal\nimages is used as the conditioning input for our policies. Since a single\ninstruction corresponds to a diverse set of different but still consistent\nend-goal images, the agent needs to learn to generate a distribution over\nprograms given an instruction. We demonstrate that with simple changes to the\nreinforced adversarial learning objective, we can learn instruction conditioned\npolicies to achieve the corresponding diverse set of goals. Most importantly,\nour agent's stochastic policy is shown to more accurately capture the diversity\nin the goal distribution than a fixed pixel-based reward function baseline. We\ndemonstrate the efficacy of our approach on two domains: (1) drawing MNIST\ndigits with a paint software conditioned on instructions and (2) constructing\nscenes in a 3D editor that satisfies a certain instruction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 16:51:35 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Malinowski", "Mateusz", ""], ["Hill", "Felix", ""], ["Eslami", "Ali", ""], ["Vinyals", "Oriol", ""], ["Kulkarni", "Tejas", ""]]}, {"id": "1812.00913", "submitter": "Tom Bruls M.Sc.", "authors": "Tom Bruls, Horia Porav, Lars Kunze and Paul Newman", "title": "The Right (Angled) Perspective: Improving the Understanding of Road\n  Scenes Using Boosted Inverse Perspective Mapping", "comments": "equal contribution of first two authors, 8 full pages, 6 figures,\n  accepted at IV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks performed by autonomous vehicles such as road marking detection,\nobject tracking, and path planning are simpler in bird's-eye view. Hence,\nInverse Perspective Mapping (IPM) is often applied to remove the perspective\neffect from a vehicle's front-facing camera and to remap its images into a 2D\ndomain, resulting in a top-down view. Unfortunately, however, this leads to\nunnatural blurring and stretching of objects at further distance, due to the\nresolution of the camera, limiting applicability. In this paper, we present an\nadversarial learning approach for generating a significantly improved IPM from\na single camera image in real time. The generated bird's-eye-view images\ncontain sharper features (e.g. road markings) and a more homogeneous\nillumination, while (dynamic) objects are automatically removed from the scene,\nthus revealing the underlying road layout in an improved fashion. We\ndemonstrate our framework using real-world data from the Oxford RobotCar\nDataset and show that scene understanding tasks directly benefit from our\nboosted IPM approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 17:12:41 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 08:39:02 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Bruls", "Tom", ""], ["Porav", "Horia", ""], ["Kunze", "Lars", ""], ["Newman", "Paul", ""]]}, {"id": "1812.00929", "submitter": "Eric Tzeng", "authors": "Eric Tzeng, Kaylee Burns, Kate Saenko, Trevor Darrell", "title": "SPLAT: Semantic Pixel-Level Adaptation Transforms for Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation of visual detectors is a critical challenge, yet existing\nmethods have overlooked pixel appearance transformations, focusing instead on\nbootstrapping and/or domain confusion losses. We propose a Semantic Pixel-Level\nAdaptation Transform (SPLAT) approach to detector adaptation that efficiently\ngenerates cross-domain image pairs. Our model uses aligned-pair and/or\npseudo-label losses to adapt an object detector to the target domain, and can\nlearn transformations with or without densely labeled data in the source (e.g.\nsemantic segmentation annotations). Without dense labels, as is the case when\nonly detection labels are available in the source, transformations are learned\nusing CycleGAN alignment. Otherwise, when dense labels are available we\nintroduce a more efficient cycle-free method, which exploits pixel-level\nsemantic labels to condition the training of the transformation network. The\nend task is then trained using detection box labels from the source,\npotentially including labels inferred on unlabeled source data. We show both\nthat pixel-level transforms outperform prior approaches to detector domain\nadaptation, and that our cycle-free method outperforms prior models for\nunconstrained cycle-based learning of generic transformations while running 3.8\ntimes faster. Our combined model improves on prior detection baselines by 12.5\nmAP adapting from Sim 10K to Cityscapes, recovering over 50% of the missing\nperformance between the unadapted baseline and the labeled-target upper bound.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 17:38:52 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Tzeng", "Eric", ""], ["Burns", "Kaylee", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1812.00940", "submitter": "Saurabh Gupta", "authors": "Ashish Kumar, Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra\n  Malik", "title": "Visual Memory for Robust Path Following", "comments": "Neural Information Processing Systems (NeurIPS) 2018. Oral\n  Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans routinely retrace paths in a novel environment both forwards and\nbackwards despite uncertainty in their motion. This paper presents an approach\nfor doing so. Given a demonstration of a path, a first network generates a path\nabstraction. Equipped with this abstraction, a second network observes the\nworld and decides how to act to retrace the path under noisy actuation and a\nchanging environment. The two networks are optimized end-to-end at training\ntime. We evaluate the method in two realistic simulators, performing path\nfollowing and homing under actuation noise and environmental changes. Our\nexperiments show that our approach outperforms classical approaches and other\nlearning based baselines.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 18:06:53 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Kumar", "Ashish", ""], ["Gupta", "Saurabh", ""], ["Fouhey", "David", ""], ["Levine", "Sergey", ""], ["Malik", "Jitendra", ""]]}, {"id": "1812.00964", "submitter": "Davide Belli", "authors": "Davide Belli, Shi Hu, Ecem Sogancioglu and Bram van Ginneken", "title": "Context Encoding Chest X-rays", "comments": "11 pages, 8 figures; fixed sentence in abstract, updated title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-rays are one of the most commonly used technologies for medical\ndiagnosis. Many deep learning models have been proposed to improve and automate\nthe abnormality detection task on this type of data. In this paper, we propose\na different approach based on image inpainting under adversarial training first\nintroduced by Goodfellow et al. We configure the context encoder model for this\ntask and train it over 1.1M 128x128 images from healthy X-rays. The goal of our\nmodel is to reconstruct the missing central 64x64 patch. Once the model has\nlearned how to inpaint healthy tissue, we test its performance on images with\nand without abnormalities. We discuss and motivate our results considering\nPSNR, MSE and SSIM scores as evaluation metrics. In addition, we conduct a 2AFC\nobserver study showing that in half of the times an expert is unable to\ndistinguish real images from the ones reconstructed using our model. By\ncomputing and visualizing the pixel-wise difference between the source and the\nreconstructed images, we can highlight abnormalities to simplify further\ndetection and classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 18:37:46 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 10:05:27 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Belli", "Davide", ""], ["Hu", "Shi", ""], ["Sogancioglu", "Ecem", ""], ["van Ginneken", "Bram", ""]]}, {"id": "1812.00971", "submitter": "Roozbeh Mottaghi", "authors": "Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi,\n  Roozbeh Mottaghi", "title": "Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using\n  Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning is an inherently continuous phenomenon. When humans learn a new task\nthere is no explicit distinction between training and inference. As we learn a\ntask, we keep learning about it while performing the task. What we learn and\nhow we learn it varies during different stages of learning. Learning how to\nlearn and adapt is a key property that enables us to generalize effortlessly to\nnew settings. This is in contrast with conventional settings in machine\nlearning where a trained model is frozen during inference. In this paper we\nstudy the problem of learning to learn at both training and test time in the\ncontext of visual navigation. A fundamental challenge in navigation is\ngeneralization to unseen scenes. In this paper we propose a self-adaptive\nvisual navigation method (SAVN) which learns to adapt to new environments\nwithout any explicit supervision. Our solution is a meta-reinforcement learning\napproach where an agent learns a self-supervised interaction loss that\nencourages effective navigation. Our experiments, performed in the AI2-THOR\nframework, show major improvements in both success rate and SPL for visual\nnavigation in novel scenes. Our code and data are available at:\nhttps://github.com/allenai/savn .\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 18:46:02 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 23:55:19 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Wortsman", "Mitchell", ""], ["Ehsani", "Kiana", ""], ["Rastegari", "Mohammad", ""], ["Farhadi", "Ali", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "1812.01002", "submitter": "Linlin Yang", "authors": "Linlin Yang, Angela Yao", "title": "Disentangling Latent Hands for Image Synthesis and Pose Estimation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand image synthesis and pose estimation from RGB images are both highly\nchallenging tasks due to the large discrepancy between factors of variation\nranging from image background content to camera viewpoint. To better analyze\nthese factors of variation, we propose the use of disentangled representations\nand a disentangled variational autoencoder (dVAE) that allows for specific\nsampling and inference of these factors. The derived objective from the\nvariational lower bound as well as the proposed training strategy are highly\nflexible, allowing us to handle cross-modal encoders and decoders as well as\nsemi-supervised learning scenarios. Experiments show that our dVAE can\nsynthesize highly realistic images of the hand specifiable by both pose and\nimage background content and also estimate 3D hand poses from RGB images with\naccuracy competitive with state-of-the-art on two public benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 19:40:41 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 19:18:10 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Yang", "Linlin", ""], ["Yao", "Angela", ""]]}, {"id": "1812.01024", "submitter": "Vincent Sitzmann", "authors": "Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie{\\ss}ner,\n  Gordon Wetzstein, Michael Zollh\\\"ofer", "title": "DeepVoxels: Learning Persistent 3D Feature Embeddings", "comments": "Video: https://www.youtube.com/watch?v=HM_WsZhoGXw Supplemental\n  material:\n  https://drive.google.com/file/d/1BnZRyNcVUty6-LxAstN83H79ktUq8Cjp/view?usp=sharing\n  Code: https://github.com/vsitzmann/deepvoxels Project page:\n  https://vsitzmann.github.io/deepvoxels/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the lack of 3D understanding of generative neural\nnetworks by introducing a persistent 3D feature embedding for view synthesis.\nTo this end, we propose DeepVoxels, a learned representation that encodes the\nview-dependent appearance of a 3D scene without having to explicitly model its\ngeometry. At its core, our approach is based on a Cartesian 3D grid of\npersistent embedded features that learn to make use of the underlying 3D scene\nstructure. Our approach combines insights from 3D geometric computer vision\nwith recent advances in learning image-to-image mappings based on adversarial\nloss functions. DeepVoxels is supervised, without requiring a 3D reconstruction\nof the scene, using a 2D re-rendering loss and enforces perspective and\nmulti-view geometry in a principled manner. We apply our persistent 3D scene\nrepresentation to the problem of novel view synthesis demonstrating\nhigh-quality results for a variety of challenging scenes.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 19:01:01 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 01:10:03 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Sitzmann", "Vincent", ""], ["Thies", "Justus", ""], ["Heide", "Felix", ""], ["Nie\u00dfner", "Matthias", ""], ["Wetzstein", "Gordon", ""], ["Zollh\u00f6fer", "Michael", ""]]}, {"id": "1812.01037", "submitter": "Ximeng Sun", "authors": "Ximeng Sun, Huijuan Xu, Kate Saenko", "title": "TwoStreamVAN: Improving Motion Modeling in Video Generation", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video generation is an inherently challenging task, as it requires modeling\nrealistic temporal dynamics as well as spatial content. Existing methods\nentangle the two intrinsically different tasks of motion and content creation\nin a single generator network, but this approach struggles to simultaneously\ngenerate plausible motion and content. To im-prove motion modeling in video\ngeneration tasks, we propose a two-stream model that disentangles motion\ngeneration from content generation, called a Two-Stream Variational Adversarial\nNetwork (TwoStreamVAN). Given an action label and a noise vector, our model is\nable to create clear and consistent motion, and thus yields photorealistic\nvideos. The key idea is to progressively generate and fuse multi-scale motion\nwith its corresponding spatial content. Our model significantly outperforms\nexisting methods on the standard Weizmann Human Action, MUG Facial Expression,\nand VoxCeleb datasets, as well as our new dataset of diverse human actions with\nchallenging and complex motion. Our code is available at\nhttps://github.com/sunxm2357/TwoStreamVAN/.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 19:11:45 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 00:07:12 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Sun", "Ximeng", ""], ["Xu", "Huijuan", ""], ["Saenko", "Kate", ""]]}, {"id": "1812.01043", "submitter": "Chowdhury Rahman", "authors": "Chowdhury Rafeed Rahman, Preetom Saha Arko, Mohammed Eunus Ali,\n  Mohammad Ashik Iqbal Khan, Sajid Hasan Apon, Farzana Nowrin, Abu Wasif", "title": "Identification and Recognition of Rice Diseases and Pests Using\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.biosystemseng.2020.03.020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate and timely detection of diseases and pests in rice plants can\nhelp farmers in applying timely treatment on the plants and thereby can reduce\nthe economic losses substantially. Recent developments in deep learning based\nconvolutional neural networks (CNN) have greatly improved the image\nclassification accuracy. Being motivated by the success of CNNs in image\nclassification, deep learning based approaches have been developed in this\npaper for detecting diseases and pests from rice plant images. The contribution\nof this paper is two fold: (i) State-of-the-art large scale architectures such\nas VGG16 and InceptionV3 have been adopted and fine tuned for detecting and\nrecognizing rice diseases and pests. Experimental results show the\neffectiveness of these models with real datasets. (ii) Since large scale\narchitectures are not suitable for mobile devices, a two-stage small CNN\narchitecture has been proposed, and compared with the state-of-the-art memory\nefficient CNN architectures such as MobileNet, NasNet Mobile and SqueezeNet.\nExperimental results show that the proposed architecture can achieve the\ndesired accuracy of 93.3\\% with a significantly reduced model size (e.g., 99\\%\nless size compared to that of VGG16).\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 19:30:07 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 21:12:33 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 08:31:24 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Rahman", "Chowdhury Rafeed", ""], ["Arko", "Preetom Saha", ""], ["Ali", "Mohammed Eunus", ""], ["Khan", "Mohammad Ashik Iqbal", ""], ["Apon", "Sajid Hasan", ""], ["Nowrin", "Farzana", ""], ["Wasif", "Abu", ""]]}, {"id": "1812.01049", "submitter": "Xue Feng", "authors": "Xue Feng, Nicholas Tustison, Craig Meyer", "title": "Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall\n  Survival Prediction using Radiomic Features", "comments": "arXiv admin note: text overlap with arXiv:1810.04274 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of different sub-regions of gliomas including\nperitumoral edema, necrotic core, enhancing and non-enhancing tumor core from\nmultimodal MRI scans has important clinical relevance in diagnosis, prognosis\nand treatment of brain tumors. However, due to the highly heterogeneous\nappearance and shape, segmentation of the sub-regions is very challenging.\nRecent development using deep learning models has proved its effectiveness in\nthe past several brain segmentation challenges as well as other semantic and\nmedical image segmentation problems. Most models in brain tumor segmentation\nuse a 2D/3D patch to predict the class label for the center voxel and variant\npatch sizes and scales are used to improve the model performance. However, it\nhas low computation efficiency and also has limited receptive field. U-Net is a\nwidely used network structure for end-to-end segmentation and can be used on\nthe entire image or extracted patches to provide classification labels over the\nentire input voxels so that it is more efficient and expect to yield better\nperformance with larger input size. Furthermore, instead of picking the best\nnetwork structure, an ensemble of multiple models, trained on different dataset\nor different hyper-parameters, can generally improve the segmentation\nperformance. In this study we propose to use an ensemble of 3D U-Nets with\ndifferent hyper-parameters for brain tumor segmentation. Preliminary results\nshowed effectiveness of this model. In addition, we developed a linear model\nfor survival prediction using extracted imaging and non-imaging features,\nwhich, despite the simplicity, can effectively reduce overfitting and\nregression errors.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 19:33:36 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Feng", "Xue", ""], ["Tustison", "Nicholas", ""], ["Meyer", "Craig", ""]]}, {"id": "1812.01053", "submitter": "Hamid Reza Vaezi Joze", "authors": "Hamid Reza Vaezi Joze and Oscar Koller", "title": "MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American\n  Sign Language", "comments": null, "journal-ref": "British Machine Vision Conference, September 2019, Cardiff, UK", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language recognition is a challenging and often underestimated problem\ncomprising multi-modal articulators (handshape, orientation, movement, upper\nbody and face) that integrate asynchronously on multiple streams. Learning\npowerful statistical models in such a scenario requires much data, particularly\nto apply recent advances of the field. However, labeled data is a scarce\nresource for sign language due to the enormous cost of transcribing these\nunwritten languages.\n  We propose the first real-life large-scale sign language data set comprising\nover 25,000 annotated videos, which we thoroughly evaluate with\nstate-of-the-art methods from sign and related action recognition. Unlike the\ncurrent state-of-the-art, the data set allows to investigate the generalization\nto unseen individuals (signer-independent test) in a realistic setting with\nover 200 signers. Previous work mostly deals with limited vocabulary tasks,\nwhile here, we cover a large class count of 1000 signs in challenging and\nunconstrained real-life recording conditions. We further propose I3D, known\nfrom video classifications, as a powerful and suitable architecture for sign\nlanguage recognition, outperforming the current state-of-the-art by a large\nmargin. The data set is publicly available to the community.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 19:41:16 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 22:42:52 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Joze", "Hamid Reza Vaezi", ""], ["Koller", "Oscar", ""]]}, {"id": "1812.01063", "submitter": "Azin Asgarian", "authors": "Azin Asgarian, Parinaz Sobhani, Ji Chao Zhang, Madalin Mihailescu,\n  Ariel Sibilia, Ahmed Bilal Ashraf, Babak Taati", "title": "A Hybrid Instance-based Transfer Learning Method", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:cs/0101200", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/174", "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, supervised machine learning models have demonstrated\ntremendous success in a variety of application domains. Despite the promising\nresults, these successful models are data hungry and their performance relies\nheavily on the size of training data. However, in many healthcare applications\nit is difficult to collect sufficiently large training datasets. Transfer\nlearning can help overcome this issue by transferring the knowledge from\nreadily available datasets (source) to a new dataset (target). In this work, we\npropose a hybrid instance-based transfer learning method that outperforms a set\nof baselines including state-of-the-art instance-based transfer learning\napproaches. Our method uses a probabilistic weighting strategy to fuse\ninformation from the source domain to the model learned in the target domain.\nOur method is generic, applicable to multiple source domains, and robust with\nrespect to negative transfer. We demonstrate the effectiveness of our approach\nthrough extensive experiments for two different applications.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 20:15:05 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Asgarian", "Azin", ""], ["Sobhani", "Parinaz", ""], ["Zhang", "Ji Chao", ""], ["Mihailescu", "Madalin", ""], ["Sibilia", "Ariel", ""], ["Ashraf", "Ahmed Bilal", ""], ["Taati", "Babak", ""]]}, {"id": "1812.01065", "submitter": "Shubhang Bhatnagar", "authors": "Ishan Bhatnagar and Shubhang Bhatnagar", "title": "QR code denoising using parallel Hopfield networks", "comments": "12 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for using Hopfield networks to denoise QR codes.\nHopfield networks have mostly been used as a noise tolerant memory or to solve\ndifficult combinatorial problems. One of the major drawbacks in their use in\nnoise tolerant associative memory is their low capacity of storage, scaling\nonly linearly with the number of nodes in the network. A larger capacity\ntherefore requires a larger number of nodes, thereby reducing the speed of\nconvergence of the network in addition to increasing hardware costs for\nacquiring more precise data to be fed to a larger number of nodes. Our paper\nproposes a new algorithm to allow the use of several Hopfield networks in\nparallel thereby increasing the cumulative storage capacity of the system many\ntimes as compared to a single Hopfield network. Our algorithm would also be\nmuch faster than a larger single Hopfield network with the same total capacity.\nThis enables their use in applications like denoising QR codes, which we have\ndemonstrated in our paper. We then test our network on a large set of QR code\nimages with different types of noise and demonstrate that such a system of\nHopfield networks can be used to denoise and recognize QR codes in real time.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 20:24:51 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 20:22:30 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Bhatnagar", "Ishan", ""], ["Bhatnagar", "Shubhang", ""]]}, {"id": "1812.01068", "submitter": "Hyunkwang Lee", "authors": "Hyunkwang Lee, Chao Huang, Sehyo Yune, Shahein H. Tajmir, Myeongchan\n  Kim, Synho Do", "title": "Machine Friendly Machine Learning: Interpretation of Computed Tomography\n  Without Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in deep learning for automated image processing and\nclassification have accelerated many new applications for medical image\nanalysis. However, most deep learning applications have been developed using\nreconstructed, human-interpretable medical images. While image reconstruction\nfrom raw sensor data is required for the creation of medical images, the\nreconstruction process only uses a partial representation of all the data\nacquired. Here we report the development of a system to directly process raw\ncomputed tomography (CT) data in sinogram-space, bypassing the intermediary\nstep of image reconstruction. Two classification tasks were evaluated for their\nfeasibility for sinogram-space machine learning: body region identification and\nintracranial hemorrhage (ICH) detection. Our proposed SinoNet performed\nfavorably compared to conventional reconstructed image-space-based systems for\nboth tasks, regardless of scanning geometries in terms of projections or\ndetectors. Further, SinoNet performed significantly better when using sparsely\nsampled sinograms than conventional networks operating in image-space. As a\nresult, sinogram-space algorithms could be used in field settings for binary\ndiagnosis testing, triage, and in clinical settings where low radiation dose is\ndesired. These findings also demonstrate another strength of deep learning\nwhere it can analyze and interpret sinograms that are virtually impossible for\nhuman experts.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 20:26:39 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Lee", "Hyunkwang", ""], ["Huang", "Chao", ""], ["Yune", "Sehyo", ""], ["Tajmir", "Shahein H.", ""], ["Kim", "Myeongchan", ""], ["Do", "Synho", ""]]}, {"id": "1812.01071", "submitter": "Patricia Vitoria", "authors": "Patricia Vitoria, Joan Sintes and Coloma Ballester", "title": "Semantic Image Inpainting Through Improved Wasserstein Generative\n  Adversarial Networks", "comments": "Accepted as Oral Presentation in VISAPP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting is the task of filling-in missing regions of a damaged or\nincomplete image. In this work we tackle this problem not only by using the\navailable visual data but also by incorporating image semantics through the use\nof generative models. Our contribution is twofold: First, we learn a data\nlatent space by training an improved version of the Wasserstein generative\nadversarial network, for which we incorporate a new generator and discriminator\narchitecture. Second, the learned semantic information is combined with a new\noptimization loss for inpainting whose minimization infers the missing content\nconditioned by the available data. It takes into account powerful contextual\nand perceptual content inherent in the image itself. The benefits include the\nability to recover large regions by accumulating semantic information even it\nis not fully present in the damaged image. Experiments show that the presented\nmethod obtains qualitative and quantitative top-tier results in different\nexperimental situations and also achieves accurate photo-realism comparable to\nstate-of-the-art works.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 20:28:17 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Vitoria", "Patricia", ""], ["Sintes", "Joan", ""], ["Ballester", "Coloma", ""]]}, {"id": "1812.01081", "submitter": "Humayun Irshad Dr.", "authors": "Humayun Irshad, Qazaleh Mirsharif, Jennifer Prendki", "title": "Crowd Sourcing based Active Learning Approach for Parking Sign\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have been used extensively to solve real-world problems\nin recent years. The performance of such models relies heavily on large amounts\nof labeled data for training. While the advances of data collection technology\nhave enabled the acquisition of a massive volume of data, labeling the data\nremains an expensive and time-consuming task. Active learning techniques are\nbeing progressively adopted to accelerate the development of machine learning\nsolutions by allowing the model to query the data they learn from. In this\npaper, we introduce a real-world problem, the recognition of parking signs, and\npresent a framework that combines active learning techniques with a transfer\nlearning approach and crowd-sourcing tools to create and train a machine\nlearning solution to the problem. We discuss how such a framework contributes\nto building an accurate model in a cost-effective and fast way to solve the\nparking sign recognition problem in spite of the unevenness of the data\nassociated with the fact that street-level images (such as parking signs) vary\nin shape, color, orientation and scale, and often appear on top of different\ntypes of background.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 21:04:41 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Irshad", "Humayun", ""], ["Mirsharif", "Qazaleh", ""], ["Prendki", "Jennifer", ""]]}, {"id": "1812.01082", "submitter": "Zhiyu Sun", "authors": "Zhiyu Sun, Ethan Rooke, Jerome Charton, Yusen He, Jia Lu and Stephen\n  Baek", "title": "ZerNet: Convolutional Neural Networks on Arbitrary Surfaces via Zernike\n  Local Tangent Space Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel formulation to extend CNNs to\ntwo-dimensional (2D) manifolds using orthogonal basis functions, called Zernike\npolynomials. In many areas, geometric features play a key role in understanding\nscientific phenomena. Thus, an ability to codify geometric features into a\nmathematical quantity can be critical. Recently, convolutional neural networks\n(CNNs) have demonstrated the promising capability of extracting and codifying\nfeatures from visual information. However, the progress has been concentrated\nin computer vision applications where there exists an inherent grid-like\nstructure. In contrast, many geometry processing problems are defined on curved\nsurfaces, and the generalization of CNNs is not quite trivial. The difficulties\nare rooted in the lack of key ingredients such as the canonical grid-like\nrepresentation, the notion of consistent orientation, and a compatible local\ntopology across the domain. In this paper, we prove that the convolution of two\nfunctions can be represented as a simple dot product between Zernike polynomial\ncoefficients; and the rotation of a convolution kernel is essentially a set of\n2-by-2 rotation matrices applied to the coefficients. As such, the key\ncontribution of this work resides in a concise but rigorous mathematical\ngeneralization of the CNN building blocks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 21:11:48 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 03:44:26 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 07:40:15 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Sun", "Zhiyu", ""], ["Rooke", "Ethan", ""], ["Charton", "Jerome", ""], ["He", "Yusen", ""], ["Lu", "Jia", ""], ["Baek", "Stephen", ""]]}, {"id": "1812.01087", "submitter": "Nathaniel Braman", "authors": "Nathaniel Braman, David Beymer, Ehsan Dehghan", "title": "Disease Detection in Weakly Annotated Volumetric Medical Images using a\n  Convolutional LSTM Network", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216 Medical Imaging Meets NeurIPS Workshop at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/19", "categories": "cs.CV cs.LG q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a solution for learning disease signatures from weakly, yet easily\nobtainable, annotated volumetric medical imaging data by analyzing 3D volumes\nas a sequence of 2D images. We demonstrate the performance of our solution in\nthe detection of emphysema in lung cancer screening low-dose CT images. Our\napproach utilizes convolutional long short-term memory (LSTM) to \"scan\"\nsequentially through an imaging volume for the presence of disease in a portion\nof scanned region. This framework allowed effective learning given only\nvolumetric images and binary disease labels, thus enabling training from a\nlarge dataset of 6,631 un-annotated image volumes from 4,486 patients. When\nevaluated in a testing set of 2,163 volumes from 2,163 patients, our model\ndistinguished emphysema with area under the receiver operating characteristic\ncurve (AUC) of .83. This approach was found to outperform 2D convolutional\nneural networks (CNN) implemented with various multiple-instance learning\nschemes (AUC=0.69-0.76) and a 3D CNN (AUC=.77).\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 21:32:28 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Braman", "Nathaniel", ""], ["Beymer", "David", ""], ["Dehghan", "Ehsan", ""]]}, {"id": "1812.01106", "submitter": "Jo\\~ao Caldeira", "authors": "Jo\\~ao Caldeira, Alex Fout, Aniket Kesari, Raesetje Sefala, Joseph\n  Walsh, Katy Dupre, Muhammad Rizal Khaefi, Setiaji, George Hodge, Zakiya\n  Aryana Pramestri, Muhammad Adib Imtiyazi", "title": "Improving Traffic Safety Through Video Analysis in Jakarta, Indonesia", "comments": "6 pages; LaTeX; Presented at NeurIPS 2018 Workshop on Machine\n  Learning for the Developing World; Presented at NeurIPS 2018 Workshop on AI\n  for Social Good", "journal-ref": "Proceedings of the 2019 Intelligent Systems Conference\n  (IntelliSys) Volume 2, 642-649", "doi": "10.1007/978-3-030-29513-4_48", "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project presents the results of a partnership between the Data Science\nfor Social Good fellowship, Jakarta Smart City and Pulse Lab Jakarta to create\na video analysis pipeline for the purpose of improving traffic safety in\nJakarta. The pipeline transforms raw traffic video footage into databases that\nare ready to be used for traffic analysis. By analyzing these patterns, the\ncity of Jakarta will better understand how human behavior and built\ninfrastructure contribute to traffic challenges and safety risks. The results\nof this work should also be broadly applicable to smart city initiatives around\nthe globe as they improve urban planning and sustainability through data\nscience approaches.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 18:51:16 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Caldeira", "Jo\u00e3o", ""], ["Fout", "Alex", ""], ["Kesari", "Aniket", ""], ["Sefala", "Raesetje", ""], ["Walsh", "Joseph", ""], ["Dupre", "Katy", ""], ["Khaefi", "Muhammad Rizal", ""], ["Setiaji", "", ""], ["Hodge", "George", ""], ["Pramestri", "Zakiya Aryana", ""], ["Imtiyazi", "Muhammad Adib", ""]]}, {"id": "1812.01157", "submitter": "Yaron Meirovitch", "authors": "Yaron Meirovitch, Lu Mi, Hayk Saribekyan, Alexander Matveev, David\n  Rolnick, Nir Shavit", "title": "Cross-Classification Clustering: An Efficient Multi-Object Tracking\n  Technique for 3-D Instance Segmentation in Connectomics", "comments": "11 figures", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019, pp. 8425-8435", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-accurate tracking of objects is a key element in many computer vision\napplications, often solved by iterated individual object tracking or instance\nsegmentation followed by object matching. Here we introduce\ncross-classification clustering (3C), a technique that simultaneously tracks\ncomplex, interrelated objects in an image stack. The key idea in\ncross-classification is to efficiently turn a clustering problem into a\nclassification problem by running a logarithmic number of independent\nclassifications per image, letting the cross-labeling of these classifications\nuniquely classify each pixel to the object labels. We apply the 3C mechanism to\nachieve state-of-the-art accuracy in connectomics -- the nanoscale mapping of\nneural tissue from electron microscopy volumes. Our reconstruction system\nincreases scalability by an order of magnitude over existing single-object\ntracking methods (such as flood-filling networks). This scalability is\nimportant for the deployment of connectomics pipelines, since currently the\nbest performing techniques require computing infrastructures that are beyond\nthe reach of most laboratories. Our algorithm may offer benefits in other\ndomains that require pixel-accurate tracking of multiple objects, such as\nsegmentation of videos and medical imagery.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 01:18:05 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 19:43:19 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Meirovitch", "Yaron", ""], ["Mi", "Lu", ""], ["Saribekyan", "Hayk", ""], ["Matveev", "Alexander", ""], ["Rolnick", "David", ""], ["Shavit", "Nir", ""]]}, {"id": "1812.01180", "submitter": "Lucas Caccia", "authors": "Lucas Caccia, Herke van Hoof, Aaron Courville, Joelle Pineau", "title": "Deep Generative Modeling of LiDAR Data", "comments": "Presented at IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building models capable of generating structured output is a key challenge\nfor AI and robotics. While generative models have been explored on many types\nof data, little work has been done on synthesizing lidar scans, which play a\nkey role in robot mapping and localization. In this work, we show that one can\nadapt deep generative models for this task by unravelling lidar scans into a 2D\npoint map. Our approach can generate high quality samples, while simultaneously\nlearning a meaningful latent representation of the data. We demonstrate\nsignificant improvements against state-of-the-art point cloud generation\nmethods. Furthermore, we propose a novel data representation that augments the\n2D signal with absolute positional information. We show that this helps\nrobustness to noisy and imputed input; the learned model can recover the\nunderlying lidar scan from seemingly uninformative data\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 02:56:00 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 22:27:23 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 20:16:31 GMT"}, {"version": "v4", "created": "Mon, 2 Dec 2019 20:44:35 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Caccia", "Lucas", ""], ["van Hoof", "Herke", ""], ["Courville", "Aaron", ""], ["Pineau", "Joelle", ""]]}, {"id": "1812.01187", "submitter": "Tong He", "authors": "Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li", "title": "Bag of Tricks for Image Classification with Convolutional Neural\n  Networks", "comments": "10 pages, 9 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the recent progress made in image classification research can be\ncredited to training procedure refinements, such as changes in data\naugmentations and optimization methods. In the literature, however, most\nrefinements are either briefly mentioned as implementation details or only\nvisible in source code. In this paper, we will examine a collection of such\nrefinements and empirically evaluate their impact on the final model accuracy\nthrough ablation study. We will show that, by combining these refinements\ntogether, we are able to improve various CNN models significantly. For example,\nwe raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on\nImageNet. We will also demonstrate that improvement on image classification\naccuracy leads to better transfer learning performance in other application\ndomains such as object detection and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 03:07:35 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 22:17:01 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["He", "Tong", ""], ["Zhang", "Zhi", ""], ["Zhang", "Hang", ""], ["Zhang", "Zhongyue", ""], ["Xie", "Junyuan", ""], ["Li", "Mu", ""]]}, {"id": "1812.01192", "submitter": "Jie Li", "authors": "Jie Li, Allan Raventos, Arjun Bhargava, Takaaki Tagawa, Adrien Gaidon", "title": "Learning to Fuse Things and Stuff", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end learning approach for panoptic segmentation, a novel\ntask unifying instance (things) and semantic (stuff) segmentation. Our model,\nTASCNet, uses feature maps from a shared backbone network to predict in a\nsingle feed-forward pass both things and stuff segmentations. We explicitly\nconstrain these two output distributions through a global things and stuff\nbinary mask to enforce cross-task consistency. Our proposed unified network is\ncompetitive with the state of the art on several benchmarks for panoptic\nsegmentation as well as on the individual semantic and instance segmentation\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 03:13:07 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 20:23:31 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Li", "Jie", ""], ["Raventos", "Allan", ""], ["Bhargava", "Arjun", ""], ["Tagawa", "Takaaki", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1812.01203", "submitter": "Micha Livne", "authors": "Micha Livne and Leonid Sigal and Marcus A. Brubaker and David J. Fleet", "title": "Walking on Thin Air: Environment-Free Physics-based Markerless Motion\n  Capture", "comments": "8 pages, 9 figures, accepted to CRV 2018 (Conference on Computer and\n  Robot Vision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative approach to physics-based motion capture. Unlike\nprior attempts to incorporate physics into tracking that assume the subject and\nscene geometry are calibrated and known a priori, our approach is automatic and\nonline. This distinction is important since calibration of the environment is\noften difficult, especially for motions with props, uneven surfaces, or outdoor\nscenes. The use of physics in this context provides a natural framework to\nreason about contact and the plausibility of recovered motions. We propose a\nfast data-driven parametric body model, based on linear-blend skinning, which\ndecouples deformations due to pose, anthropometrics and body shape. Pose (and\nshape) parameters are estimated using robust ICP optimization with\nphysics-based dynamic priors that incorporate contact. Contact is estimated\nfrom torque trajectories and predictions of which contact points were active.\nTo our knowledge, this is the first approach to take physics into account\nwithout explicit {\\em a priori} knowledge of the environment or body\ndimensions. We demonstrate effective tracking from a noisy single depth camera,\nimproving on state-of-the-art results quantitatively and producing better\nqualitative results, reducing visual artifacts like foot-skate and jitter.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 03:47:13 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Livne", "Micha", ""], ["Sigal", "Leonid", ""], ["Brubaker", "Marcus A.", ""], ["Fleet", "David J.", ""]]}, {"id": "1812.01210", "submitter": "Tao Kong", "authors": "Liangzhe Yuan, Yibo Chen, Hantian Liu, Tao Kong, Jianbo Shi", "title": "Zoom-In-to-Check: Boosting Video Interpolation via Instance-level\n  Discrimination", "comments": "CVPR 2019 camera-ready, supplementary video:\n  https://youtu.be/q-_wIRq26DY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a light-weight video frame interpolation algorithm. Our key\ninnovation is an instance-level supervision that allows information to be\nlearned from the high-resolution version of similar objects. Our experiment\nshows that the proposed method can generate state-of-the-art results across\ndifferent datasets, with fractional computation resources (time and memory) of\ncompeting methods. Given two image frames, a cascade network creates an\nintermediate frame with 1) a flow-warping module that computes coarse\nbi-directional optical flow and creates an interpolated image via flow-based\nwarping, followed by 2) an image synthesis module to make fine-scale\ncorrections. In the learning stage, object detection proposals are generated on\nthe interpolated image.Lower resolution objects are zoomed into, and the\nlearning algorithms using an adversarial loss trained on high-resolution\nobjects to guide the system towards the instance-level refinement corrects\ndetails of object shape and boundaries.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 04:17:42 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 02:12:48 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yuan", "Liangzhe", ""], ["Chen", "Yibo", ""], ["Liu", "Hantian", ""], ["Kong", "Tao", ""], ["Shi", "Jianbo", ""]]}, {"id": "1812.01214", "submitter": "Sascha Saralajew", "authors": "Sascha Saralajew and Lars Holdijk and Maike Rees and Thomas Villmann", "title": "Prototype-based Neural Network Layers: Incorporating Vector Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks currently dominate the machine learning community and they do\nso for good reasons. Their accuracy on complex tasks such as image\nclassification is unrivaled at the moment and with recent improvements they are\nreasonably easy to train. Nevertheless, neural networks are lacking robustness\nand interpretability. Prototype-based vector quantization methods on the other\nhand are known for being robust and interpretable. For this reason, we propose\ntechniques and strategies to merge both approaches. This contribution will\nparticularly highlight the similarities between them and outline how to\nconstruct a prototype-based classification layer for multilayer networks.\nAdditionally, we provide an alternative, prototype-based, approach to the\nclassical convolution operation. Numerical results are not part of this report,\ninstead the focus lays on establishing a strong theoretical framework. By\npublishing our framework and the respective theoretical considerations and\njustifications before finalizing our numerical experiments we hope to\njump-start the incorporation of prototype-based learning in neural networks and\nvice versa.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 04:33:12 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 11:07:39 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Saralajew", "Sascha", ""], ["Holdijk", "Lars", ""], ["Rees", "Maike", ""], ["Villmann", "Thomas", ""]]}, {"id": "1812.01222", "submitter": "Julian B\\\"uchel", "authors": "Julian B\\\"uchel and Okan Ersoy", "title": "Ladder Networks for Semi-Supervised Hyperspectral Image Classification", "comments": "Technical Report, 5 pages, 8 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.33254.27208", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We used the Ladder Network [Rasmus et al. (2015)] to perform Hyperspectral\nImage Classification in a semi-supervised setting. The Ladder Network\ndistinguishes itself from other semi-supervised methods by jointly optimizing a\nsupervised and unsupervised cost. In many settings this has proven to be more\nsuccessful than other semi-supervised techniques, such as pretraining using\nunlabeled data. We furthermore show that the convolutional Ladder Network\noutperforms most of the current techniques used in hyperspectral image\nclassification and achieves new state-of-the-art performance on the Pavia\nUniversity dataset given only 5 labeled data points per class.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 05:24:47 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["B\u00fcchel", "Julian", ""], ["Ersoy", "Okan", ""]]}, {"id": "1812.01232", "submitter": "Dylan Campbell", "authors": "Dylan Campbell, Lars Petersson, Laurent Kneip, Hongdong Li, and\n  Stephen Gould", "title": "The Alignment of the Spheres: Globally-Optimal Spherical Mixture\n  Alignment for Camera Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the position and orientation of a calibrated camera from a single\nimage with respect to a 3D model is an essential task for many applications.\nWhen 2D-3D correspondences can be obtained reliably, perspective-n-point\nsolvers can be used to recover the camera pose. However, without the pose it is\nnon-trivial to find cross-modality correspondences between 2D images and 3D\nmodels, particularly when the latter only contains geometric information.\nConsequently, the problem becomes one of estimating pose and correspondences\njointly. Since outliers and local optima are so prevalent, robust objective\nfunctions and global search strategies are desirable. Hence, we cast the\nproblem as a 2D-3D mixture model alignment task and propose the first\nglobally-optimal solution to this formulation under the robust $L_2$ distance\nbetween mixture distributions. We search the 6D camera pose space using\nbranch-and-bound, which requires novel bounds, to obviate the need for a pose\nestimate and guarantee global optimality. To accelerate convergence, we\nintegrate local optimization, implement GPU bound computations, and provide an\nintuitive way to incorporate side information such as semantic labels. The\nalgorithm is evaluated on challenging synthetic and real datasets,\noutperforming existing approaches and reliably converging to the global\noptimum.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 05:54:17 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 09:05:54 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Campbell", "Dylan", ""], ["Petersson", "Lars", ""], ["Kneip", "Laurent", ""], ["Li", "Hongdong", ""], ["Gould", "Stephen", ""]]}, {"id": "1812.01233", "submitter": "Roei Herzig", "authors": "Roei Herzig, Elad Levi, Huijuan Xu, Hang Gao, Eli Brosh, Xiaolong\n  Wang, Amir Globerson, Trevor Darrell", "title": "Spatio-Temporal Action Graph Networks", "comments": "IEEE/CVF International Conference on Computer Vision Workshop\n  (ICCVW), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Events defined by the interaction of objects in a scene are often of critical\nimportance; yet important events may have insufficient labeled examples to\ntrain a conventional deep model to generalize to future object appearance.\nActivity recognition models that represent object interactions explicitly have\nthe potential to learn in a more efficient manner than those that represent\nscenes with global descriptors. We propose a novel inter-object graph\nrepresentation for activity recognition based on a disentangled graph embedding\nwith direct observation of edge appearance. We employ a novel factored\nembedding of the graph structure, disentangling a representation hierarchy\nformed over spatial dimensions from that found over temporal variation. We\ndemonstrate the effectiveness of our model on the Charades activity recognition\nbenchmark, as well as a new dataset of driving activities focusing on\nmulti-object interactions with near-collision events. Our model offers\nsignificantly improved performance compared to baseline approaches without\nobject-graph representations, or with previous graph-based models.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 05:58:20 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 16:57:16 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Herzig", "Roei", ""], ["Levi", "Elad", ""], ["Xu", "Huijuan", ""], ["Gao", "Hang", ""], ["Brosh", "Eli", ""], ["Wang", "Xiaolong", ""], ["Globerson", "Amir", ""], ["Darrell", "Trevor", ""]]}, {"id": "1812.01243", "submitter": "Zhuoran Shen", "authors": "Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, Hongsheng Li", "title": "Efficient Attention: Attention with Linear Complexities", "comments": "To appear at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dot-product attention has wide applications in computer vision and natural\nlanguage processing. However, its memory and computational costs grow\nquadratically with the input size. Such growth prohibits its application on\nhigh-resolution inputs. To remedy this drawback, this paper proposes a novel\nefficient attention mechanism equivalent to dot-product attention but with\nsubstantially less memory and computational costs. Its resource efficiency\nallows more widespread and flexible integration of attention modules into a\nnetwork, which leads to better accuracies. Empirical evaluations demonstrated\nthe effectiveness of its advantages. Efficient attention modules brought\nsignificant performance boosts to object detectors and instance segmenters on\nMS-COCO 2017. Further, the resource efficiency democratizes attention to\ncomplex models, where high costs prohibit the use of dot-product attention. As\nan exemplar, a model with efficient attention achieved state-of-the-art\naccuracies for stereo depth estimation on the Scene Flow dataset. Code is\navailable at https://github.com/cmsflash/efficient-attention.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 06:41:46 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 11:07:37 GMT"}, {"version": "v3", "created": "Sat, 23 Mar 2019 14:12:01 GMT"}, {"version": "v4", "created": "Sun, 31 Mar 2019 04:45:33 GMT"}, {"version": "v5", "created": "Sat, 16 Nov 2019 09:58:55 GMT"}, {"version": "v6", "created": "Fri, 22 Nov 2019 15:57:06 GMT"}, {"version": "v7", "created": "Sat, 7 Dec 2019 01:49:31 GMT"}, {"version": "v8", "created": "Mon, 13 Jan 2020 02:16:11 GMT"}, {"version": "v9", "created": "Wed, 11 Nov 2020 03:40:08 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Shen", "Zhuoran", ""], ["Zhang", "Mingyuan", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""], ["Li", "Hongsheng", ""]]}, {"id": "1812.01261", "submitter": "Antonio Tejero-de-Pablos", "authors": "Shohei Yamamoto, Antonio Tejero-de-Pablos, Yoshitaka Ushiku, Tatsuya\n  Harada", "title": "Conditional Video Generation Using Action-Appearance Captions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of automatic video generation has received a boost thanks to the\nrecent Generative Adversarial Networks (GANs). However, most existing methods\ncannot control the contents of the generated video using a text caption, losing\ntheir usefulness to a large extent. This particularly affects human videos due\nto their great variety of actions and appearances. This paper presents\nConditional Flow and Texture GAN (CFT-GAN), a GAN-based video generation method\nfrom action-appearance captions. We propose a novel way of generating video by\nencoding a caption (e.g., \"a man in blue jeans is playing golf\") in a two-stage\ngeneration pipeline. Our CFT-GAN uses such caption to generate an optical flow\n(action) and a texture (appearance) for each frame. As a result, the output\nvideo reflects the content specified in the caption in a plausible way.\nMoreover, to train our method, we constructed a new dataset for human video\ngeneration with captions. We evaluated the proposed method qualitatively and\nquantitatively via an ablation study and a user study. The results demonstrate\nthat CFT-GAN is able to successfully generate videos containing the action and\nappearances indicated in the captions.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 07:54:39 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 04:19:27 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yamamoto", "Shohei", ""], ["Tejero-de-Pablos", "Antonio", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1812.01263", "submitter": "Atsushi Kanehira Mr.", "authors": "Atsushi Kanehira, Kentaro Takemoto, Sho Inayoshi, and Tatsuya Harada", "title": "Multimodal Explanations by Predicting Counterfactuality in Videos", "comments": "Camera ready version of CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study addresses generating counterfactual explanations with multimodal\ninformation. Our goal is not only to classify a video into a specific category,\nbut also to provide explanations on why it is not categorized to a specific\nclass with combinations of visual-linguistic information. Requirements that the\nexpected output should satisfy are referred to as counterfactuality in this\npaper: (1) Compatibility of visual-linguistic explanations, and (2)\nPositiveness/negativeness for the specific positive/negative class. Exploiting\na spatio-temporal region (tube) and an attribute as visual and linguistic\nexplanations respectively, the explanation model is trained to predict the\ncounterfactuality for possible combinations of multimodal information in a\npost-hoc manner. The optimization problem, which appears during\ntraining/inference, can be efficiently solved by inserting a novel neural\nnetwork layer, namely the maximum subpath layer. We demonstrated the\neffectiveness of this method by comparison with a baseline of the action\nrecognition datasets extended for this task. Moreover, we provide\ninformation-theoretical insight into the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 08:02:23 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 03:18:25 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Kanehira", "Atsushi", ""], ["Takemoto", "Kentaro", ""], ["Inayoshi", "Sho", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1812.01273", "submitter": "Pranoy Panda", "authors": "Sanchayan Santra, Ranjan Mondal, Pranoy Panda, Nishant Mohanty,\n  Shubham Bhuyan", "title": "Image Dehazing via Joint Estimation of Transmittance Map and\n  Environmental Illumination", "comments": "6 pages, 9 figures, Presented at the Ninth International Conference\n  on Advances in Pattern Recognition(ICAPR), December 2017, Bengaluru, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haze limits the visibility of outdoor images, due to the existence of fog,\nsmoke and dust in the atmosphere. Image dehazing methods try to recover\nhaze-free image by removing the effect of haze from a given input image. In\nthis paper, we present an end to end system, which takes a hazy image as its\ninput and returns a dehazed image. The proposed method learns the mapping\nbetween a hazy image and its corresponding transmittance map and the\nenvironmental illumination, by using a multi-scale Convolutional Neural\nNetwork. Although most of the time haze appears grayish in color, its color may\nvary depending on the color of the environmental illumination. Very few of the\nexisting image dehazing methods have laid stress on its accurate estimation.\nBut the color of the dehazed image and the estimated transmittance depends on\nthe environmental illumination. Our proposed method exploits the relationship\nbetween the transmittance values and the environmental illumination as per the\nhaze imaging model and estimates both of them. Qualitative and quantitative\nevaluations show, the estimates are accurate enough.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 08:28:23 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Santra", "Sanchayan", ""], ["Mondal", "Ranjan", ""], ["Panda", "Pranoy", ""], ["Mohanty", "Nishant", ""], ["Bhuyan", "Shubham", ""]]}, {"id": "1812.01280", "submitter": "Atsushi Kanehira Mr.", "authors": "Atsushi Kanehira and Tatsuya Harada", "title": "Learning to Explain with Complemental Examples", "comments": "Camera ready version of CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the generation of explanations with visual examples.\nGiven an input sample, we build a system that not only classifies it to a\nspecific category, but also outputs linguistic explanations and a set of visual\nexamples that render the decision interpretable. Focusing especially on the\ncomplementarity of the multimodal information, i.e., linguistic and visual\nexamples, we attempt to achieve it by maximizing the interaction information,\nwhich provides a natural definition of complementarity from an information\ntheoretical viewpoint. We propose a novel framework to generate complemental\nexplanations, on which the joint distribution of the variables to explain, and\nthose to be explained is parameterized by three different neural networks:\npredictor, linguistic explainer, and example selector. Explanation models are\ntrained collaboratively to maximize the interaction information to ensure the\ngenerated explanation are complemental to each other for the target. The\nresults of experiments conducted on several datasets demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 08:52:05 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 10:00:37 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Kanehira", "Atsushi", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1812.01281", "submitter": "Rahul Venkataramani", "authors": "Rahul Venkataramani, Hariharan Ravishankar, Saihareesh Anamandra", "title": "Towards Continuous Domain adaptation for Healthcare", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have demonstrated tremendous success on challenging\nmedical imaging problems. However, post-deployment, these algorithms are\nsusceptible to data distribution variations owing to \\emph{limited data issues}\nand \\emph{diversity} in medical images. In this paper, we propose\n\\emph{ContextNets}, a generic memory-augmented neural network framework for\nsemantic segmentation to achieve continuous domain adaptation without the\nnecessity of retraining. Unlike existing methods which require access to entire\nsource and target domain images, our algorithm can adapt to a target domain\nwith a few similar images. We condition the inference on any new input with\nfeatures computed on its support set of images (and masks, if available)\nthrough contextual embeddings to achieve site-specific adaptation. We\ndemonstrate state-of-the-art domain adaptation performance on the X-ray lung\nsegmentation problem from three independent cohorts that differ in disease\ntype, gender, contrast and intensity variations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 08:59:03 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Venkataramani", "Rahul", ""], ["Ravishankar", "Hariharan", ""], ["Anamandra", "Saihareesh", ""]]}, {"id": "1812.01285", "submitter": "Ryuhei Hamaguchi", "authors": "Ryuhei Hamaguchi, Ken Sakurada, and Ryosuke Nakamura", "title": "Rare Event Detection using Disentangled Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for rare event detection from an image\npair with class-imbalanced datasets. A straightforward approach for event\ndetection tasks is to train a detection network from a large-scale dataset in\nan end-to-end manner. However, in many applications such as building change\ndetection on satellite images, few positive samples are available for the\ntraining. Moreover, scene image pairs contain many trivial events, such as in\nillumination changes or background motions. These many trivial events and the\nclass imbalance problem lead to false alarms for rare event detection. In order\nto overcome these difficulties, we propose a novel method to learn disentangled\nrepresentations from only low-cost negative samples. The proposed method\ndisentangles different aspects in a pair of observations: variant and invariant\nfactors that represent trivial events and image contents, respectively. The\neffectiveness of the proposed approach is verified by the quantitative\nevaluations on four change detection datasets, and the qualitative analysis\nshows that the proposed method can acquire the representations that disentangle\nrare events from trivial ones.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 09:05:34 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Hamaguchi", "Ryuhei", ""], ["Sakurada", "Ken", ""], ["Nakamura", "Ryosuke", ""]]}, {"id": "1812.01288", "submitter": "Yujun Shen", "authors": "Yujun Shen, Bolei Zhou, Ping Luo, Xiaoou Tang", "title": "FaceFeat-GAN: a Two-Stage Approach for Identity-Preserving Face\n  Synthesis", "comments": "12 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advance of Generative Adversarial Networks (GANs) enables realistic face\nimage synthesis. However, synthesizing face images that preserve facial\nidentity as well as have high diversity within each identity remains\nchallenging. To address this problem, we present FaceFeat-GAN, a novel\ngenerative model that improves both image quality and diversity by using two\nstages. Unlike existing single-stage models that map random noise to image\ndirectly, our two-stage synthesis includes the first stage of diverse feature\ngeneration and the second stage of feature-to-image rendering. The competitions\nbetween generators and discriminators are carefully designed in both stages\nwith different objective functions. Specially, in the first stage, they compete\nin the feature domain to synthesize various facial features rather than images.\nIn the second stage, they compete in the image domain to render photo-realistic\nimages that contain high diversity but preserve identity. Extensive experiments\nshow that FaceFeat-GAN generates images that not only retain identity\ninformation but also have high diversity and quality, significantly\noutperforming previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 09:11:46 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Shen", "Yujun", ""], ["Zhou", "Bolei", ""], ["Luo", "Ping", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1812.01289", "submitter": "Noureldien Hussein", "authors": "Noureldien Hussein, Efstratios Gavves, Arnold W.M. Smeulders", "title": "Timeception for Complex Action Recognition", "comments": "IEEE CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the temporal aspect for recognizing human activities in\nvideos; an important visual cue that has long been undervalued. We revisit the\nconventional definition of activity and restrict it to Complex Action: a set of\none-actions with a weak temporal pattern that serves a specific purpose.\nRelated works use spatiotemporal 3D convolutions with fixed kernel size, too\nrigid to capture the varieties in temporal extents of complex actions, and too\nshort for long-range temporal modeling. In contrast, we use multi-scale\ntemporal convolutions, and we reduce the complexity of 3D convolutions. The\noutcome is Timeception convolution layers, which reasons about minute-long\ntemporal patterns, a factor of 8 longer than best related works. As a result,\nTimeception achieves impressive accuracy in recognizing the human activities of\nCharades, Breakfast Actions, and MultiTHUMOS. Further, we demonstrate that\nTimeception learns long-range temporal dependencies and tolerate temporal\nextents of complex actions.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 09:17:38 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 09:53:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Hussein", "Noureldien", ""], ["Gavves", "Efstratios", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1812.01335", "submitter": "Victor Boutin", "authors": "Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent. U Perrinet", "title": "From biological vision to unsupervised hierarchical sparse coding", "comments": "in Proceedings of iTWIST'18, Paper-ID: 17, Marseille, France,\n  November, 21-23, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The formation of connections between neural cells is emerging essentially\nfrom an unsupervised learning process. For instance, during the development of\nthe primary visual cortex of mammals (V1), we observe the emergence of cells\nselective to localized and oriented features. This leads to the development of\na rough contour-based representation of the retinal image in area V1. We\npropose a biological model of the formation of this representation along the\nthalamo-cortical pathway. To achieve this goal, we replicated the Multi-Layer\nConvolutional Sparse Coding (ML-CSC) algorithm developed by Michael Elad's\ngroup.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 11:07:11 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Boutin", "Victor", ""], ["Franciosini", "Angelo", ""], ["Ruffier", "Franck", ""], ["Perrinet", "Laurent. U", ""]]}, {"id": "1812.01366", "submitter": "Chinedu Nwoye", "authors": "Chinedu Innocent Nwoye, Didier Mutter, Jacques Marescaux, Nicolas\n  Padoy", "title": "Weakly Supervised Convolutional LSTM Approach for Tool Tracking in\n  Laparoscopic Videos", "comments": "14 pages, 3 figures, 3 tables, Supplementary video:\n  https://youtu.be/vnMwlS5tvHE and https://youtu.be/SNhd1yzOe50", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: Real-time surgical tool tracking is a core component of the future\nintelligent operating room (OR), because it is highly instrumental to analyze\nand understand the surgical activities. Current methods for surgical tool\ntracking in videos need to be trained on data in which the spatial positions of\nthe tools are manually annotated. Generating such training data is difficult\nand time-consuming. Instead, we propose to use solely binary presence\nannotations to train a tool tracker for laparoscopic videos. Methods: The\nproposed approach is composed of a CNN + Convolutional LSTM (ConvLSTM) neural\nnetwork trained end-to-end, but weakly supervised on tool binary presence\nlabels only. We use the ConvLSTM to model the temporal dependencies in the\nmotion of the surgical tools and leverage its spatio-temporal ability to smooth\nthe class peak activations in the localization heat maps (Lh-maps).\n  Results: We build a baseline tracker on top of the CNN model and demonstrate\nthat our approach based on the ConvLSTM outperforms the baseline in tool\npresence detection, spatial localization, and motion tracking by over 5.0%,\n13.9%, and 12.6%, respectively.\n  Conclusions: In this paper, we demonstrate that binary presence labels are\nsufficient for training a deep learning tracking model using our proposed\nmethod. We also show that the ConvLSTM can leverage the spatio-temporal\ncoherence of consecutive image frames across a surgical video to improve tool\npresence detection, spatial localization, and motion tracking.\n  keywords: Surgical workflow analysis, tool tracking, weak supervision,\nspatio-temporal coherence, ConvLSTM, endoscopic videos\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 12:21:13 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 10:57:00 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Nwoye", "Chinedu Innocent", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1812.01387", "submitter": "Zelin Zhao", "authors": "Zelin Zhao, Gao Peng, Haoyu Wang, Hao-Shu Fang, Chengkun Li, Cewu Lu", "title": "Estimating 6D Pose From Localizing Designated Surface Keypoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an accurate yet effective solution for 6D pose\nestimation from an RGB image. The core of our approach is that we first\ndesignate a set of surface points on target object model as keypoints and then\ntrain a keypoint detector (KPD) to localize them. Finally a PnP algorithm can\nrecover the 6D pose according to the 2D-3D relationship of keypoints. Different\nfrom recent state-of-the-art CNN-based approaches that rely on a time-consuming\npost-processing procedure, our method can achieve competitive accuracy without\nany refinement after pose prediction. Meanwhile, we obtain a 30% relative\nimprovement in terms of ADD accuracy among methods without using refinement.\nMoreover, we succeed in handling heavy occlusion by selecting the most\nconfident keypoints to recover the 6D pose. For the sake of reproducibility, we\nwill make our code and models publicly available soon.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 12:55:06 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Zhao", "Zelin", ""], ["Peng", "Gao", ""], ["Wang", "Haoyu", ""], ["Fang", "Hao-Shu", ""], ["Li", "Chengkun", ""], ["Lu", "Cewu", ""]]}, {"id": "1812.01389", "submitter": "Roman Rolon ReR", "authors": "R.E. Rol\\'on, L.E. Di Persia, R.D. Spies and H.L. Rufiner", "title": "A multi-class structured dictionary learning method using discriminant\n  atom selection", "comments": "18 pages, 8 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, traditional dictionary learning methods have been\nsuccessfully applied to various pattern classification tasks. Although these\nmethods produce sparse representations of signals which are robust against\ndistortions and missing data, such representations quite often turn out to be\nunsuitable if the final objective is signal classification. In order to\novercome or at least to attenuate such a weakness, several new methods which\nincorporate discriminative information into sparse-inducing models have emerged\nin recent years. In particular, methods for discriminative dictionary learning\nhave shown to be more accurate (in terms of signal classification) than the\ntraditional ones, which are only focused on minimizing the total representation\nerror. In this work, we present both a novel multi-class discriminative measure\nand an innovative dictionary learning method. For a given dictionary, this new\nmeasure, which takes into account not only when a particular atom is used for\nrepresenting signals coming from a certain class and the magnitude of its\ncorresponding representation coefficient, but also the effect that such an atom\nhas in the total representation error, is capable of efficiently quantifying\nthe degree of discriminability of each one of the atoms. On the other hand, the\nnew dictionary construction method yields dictionaries which are highly\nsuitable for multi-class classification tasks. Our method was tested with a\nwidely used database for handwritten digit recognition and compared with three\nstate-of-the-art classification methods. The results show that our method\nsignificantly outperforms the other three achieving good recognition rates and\nadditionally, reducing the computational cost of the classifier.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 13:02:40 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Rol\u00f3n", "R. E.", ""], ["Di Persia", "L. E.", ""], ["Spies", "R. D.", ""], ["Rufiner", "H. L.", ""]]}, {"id": "1812.01391", "submitter": "Devraj Mandal", "authors": "Devraj Mandal, Pramod Rao, Soma Biswas", "title": "Semi-Supervised Cross-Modal Retrieval with Label Prediction", "comments": "Updated Version of the Paper has been accepted in IEEE Transactions\n  on Multimedia {https://ieeexplore.ieee.org/document/8907496/}", "journal-ref": null, "doi": "10.1109/TMM.2019.2954741", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to abundance of data from multiple modalities, cross-modal retrieval\ntasks with image-text, audio-image, etc. are gaining increasing importance. Of\nthe different approaches proposed, supervised methods usually give significant\nimprovement over their unsupervised counterparts at the additional cost of\nlabeling or annotation of the training data. Semi-supervised methods are\nrecently becoming popular as they provide an elegant framework to balance the\nconflicting requirement of labeling cost and accuracy. In this work, we propose\na novel deep semi-supervised framework which can seamlessly handle both labeled\nas well as unlabeled data. The network has two important components: (a) the\nlabel prediction component predicts the labels for the unlabeled portion of the\ndata and then (b) a common modality-invariant representation is learned for\ncross-modal retrieval. The two parts of the network are trained sequentially\none after the other. Extensive experiments on three standard benchmark\ndatasets, Wiki, Pascal VOC and NUS-WIDE demonstrate that the proposed framework\noutperforms the state-of-the-art for both supervised and semi-supervised\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 13:07:15 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 07:23:40 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Mandal", "Devraj", ""], ["Rao", "Pramod", ""], ["Biswas", "Soma", ""]]}, {"id": "1812.01393", "submitter": "Yongchao Xu", "authors": "Yongchao Xu, Yukang Wang, Wei Zhou, Yongpan Wang, Zhibo Yang, Xiang\n  Bai", "title": "TextField: Learning A Deep Direction Field for Irregular Scene Text\n  Detection", "comments": "To appear in IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2019.2900589", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection is an important step of scene text reading system. The\nmain challenges lie on significantly varied sizes and aspect ratios, arbitrary\norientations and shapes. Driven by recent progress in deep learning, impressive\nperformances have been achieved for multi-oriented text detection. Yet, the\nperformance drops dramatically in detecting curved texts due to the limited\ntext representation (e.g., horizontal bounding boxes, rotated rectangles, or\nquadrilaterals). It is of great interest to detect curved texts, which are\nactually very common in natural scenes. In this paper, we present a novel text\ndetector named TextField for detecting irregular scene texts. Specifically, we\nlearn a direction field pointing away from the nearest text boundary to each\ntext point. This direction field is represented by an image of two-dimensional\nvectors and learned via a fully convolutional neural network. It encodes both\nbinary text mask and direction information used to separate adjacent text\ninstances, which is challenging for classical segmentation-based approaches.\nBased on the learned direction field, we apply a simple yet effective\nmorphological-based post-processing to achieve the final detection.\nExperimental results show that the proposed TextField outperforms the\nstate-of-the-art methods by a large margin (28% and 8%) on two curved text\ndatasets: Total-Text and CTW1500, respectively, and also achieves very\ncompetitive performance on multi-oriented datasets: ICDAR 2015 and MSRA-TD500.\nFurthermore, TextField is robust in generalizing to unseen datasets. The code\nis available at https://github.com/YukangWang/TextField.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 13:12:58 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 06:18:33 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Xu", "Yongchao", ""], ["Wang", "Yukang", ""], ["Zhou", "Wei", ""], ["Wang", "Yongpan", ""], ["Yang", "Zhibo", ""], ["Bai", "Xiang", ""]]}, {"id": "1812.01397", "submitter": "Harkirat Behl", "authors": "Harkirat Singh Behl, Mohammad Najafi, Anurag Arnab, Philip H.S. Torr", "title": "Meta Learning Deep Visual Words for Fast Video Object Segmentation", "comments": null, "journal-ref": "In Proceedings of International Conference on Intelligent Robots\n  and Systems (IROS) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal robots and driverless cars need to be able to operate in novel\nenvironments and thus quickly and efficiently learn to recognise new object\nclasses. We address this problem by considering the task of video object\nsegmentation. Previous accurate methods for this task finetune a model using\nthe first annotated frame, and/or use additional inputs such as optical flow\nand complex post-processing. In contrast, we develop a fast, causal algorithm\nthat requires no finetuning, auxiliary inputs or post-processing, and segments\na variable number of objects in a single forward-pass. We represent an object\nwith clusters, or \"visual words\", in the embedding space, which correspond to\nobject parts in the image space. This allows us to robustly match to the\nreference objects throughout the video, because although the global appearance\nof an object changes as it undergoes occlusions and deformations, the\nappearance of more local parts may stay consistent. We learn these visual words\nin an unsupervised manner, using meta-learning to ensure that our training\nobjective matches our inference procedure. We achieve comparable accuracy to\nfinetuning based methods (whilst being 1 to 2 orders of magnitude faster), and\nstate-of-the-art in terms of speed/accuracy trade-offs on four video\nsegmentation datasets. Code is available at\nhttps://github.com/harkiratbehl/MetaVOS.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 13:23:05 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 15:26:11 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 00:02:54 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Behl", "Harkirat Singh", ""], ["Najafi", "Mohammad", ""], ["Arnab", "Anurag", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1812.01402", "submitter": "Wei Zeng", "authors": "Wei Zeng, Sezer Karaoglu and Theo Gevers", "title": "Inferring Point Clouds from Single Monocular Images by Depth\n  Intermediation", "comments": "Statement: This paper is under consideration at Computer Vision and\n  Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a pipeline to generate 3D point cloud of an object\nfrom a single-view RGB image. Most previous work predict the 3D point\ncoordinates from single RGB images directly. We decompose this problem into\ndepth estimation from single images and point cloud completion from partial\npoint clouds.\n  Our method sequentially predicts the depth maps from images and then infers\nthe complete 3D object point clouds based on the predicted partial point\nclouds. We explicitly impose the camera model geometrical constraint in our\npipeline and enforce the alignment of the generated point clouds and estimated\ndepth maps.\n  Experimental results for the single image 3D object reconstruction task show\nthat the proposed method outperforms existing state-of-the-art methods. Both\nthe qualitative and quantitative results demonstrate the generality and\nsuitability of our method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 13:32:51 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 10:58:26 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 12:30:49 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zeng", "Wei", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""]]}, {"id": "1812.01410", "submitter": "Vincent Schellekens", "authors": "Vincent Schellekens and Laurent Jacques", "title": "Compressive Classification (Machine Learning without learning)", "comments": "in Proceedings of iTWIST'18, Paper-ID: 8, Marseille, France,\n  November, 21-23, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive learning is a framework where (so far unsupervised) learning\ntasks use not the entire dataset but a compressed summary (sketch) of it. We\npropose a compressive learning classification method, and a novel sketch\nfunction for images.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 13:50:11 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Schellekens", "Vincent", ""], ["Jacques", "Laurent", ""]]}, {"id": "1812.01458", "submitter": "Qingguo Xiao", "authors": "Qingguo Xiao, Guangyao Li, Qiaochuan Chen", "title": "Deep Inception Generative Network for Cognitive Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have shown exciting promise in filling large\nholes and lead to another orientation for image inpainting. However, existing\nlearning-based methods often create artifacts and fallacious textures because\nof insufficient cognition understanding. Previous generative networks are\nlimited with single receptive type and give up pooling in consideration of\ndetail sharpness. Human cognition is constant regardless of the target\nattribute. As multiple receptive fields improve the ability of abstract image\ncharacterization and pooling can keep feature invariant, specifically, deep\ninception learning is adopted to promote high-level feature representation and\nenhance model learning capacity for local patches. Moreover, approaches for\ngenerating diverse mask images are introduced and a random mask dataset is\ncreated. We benchmark our methods on ImageNet, Places2 dataset, and CelebA-HQ.\nExperiments for regular, irregular, and custom regions completion are all\nperformed and free-style image inpainting is also presented. Quantitative\ncomparisons with previous state-of-the-art methods show that ours obtain much\nmore natural image completions.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 03:20:47 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Xiao", "Qingguo", ""], ["Li", "Guangyao", ""], ["Chen", "Qiaochuan", ""]]}, {"id": "1812.01461", "submitter": "Jean-Baptiste Alayrac", "authors": "Jean-Baptiste Alayrac and Jo\\~ao Carreira and Andrew Zisserman", "title": "The Visual Centrifuge: Model-Free Layered Video Representations", "comments": "Appears in: 2019 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2019). This arXiv contains the CVPR Camera Ready version of\n  the paper (although we have included larger figures) as well as an appendix\n  detailing the model architecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  True video understanding requires making sense of non-lambertian scenes where\nthe color of light arriving at the camera sensor encodes information about not\njust the last object it collided with, but about multiple mediums -- colored\nwindows, dirty mirrors, smoke or rain. Layered video representations have the\npotential of accurately modelling realistic scenes but have so far required\nstringent assumptions on motion, lighting and shape. Here we propose a\nlearning-based approach for multi-layered video representation: we introduce\nnovel uncertainty-capturing 3D convolutional architectures and train them to\nseparate blended videos. We show that these models then generalize to single\nvideos, where they exhibit interesting abilities: color constancy, factoring\nout shadows and separating reflections. We present quantitative and qualitative\nresults on real world videos.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 14:47:23 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 10:44:27 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Alayrac", "Jean-Baptiste", ""], ["Carreira", "Jo\u00e3o", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1812.01464", "submitter": "Nils Gessert", "authors": "Nils Gessert, Lukas Wittig, Daniel Dr\\\"omann, Tobias Keck, Alexander\n  Schlaefer, David B. Ellebrecht", "title": "Feasibility of Colon Cancer Detection in Confocal Laser Microscopy\n  Images Using Convolution Neural Networks", "comments": "Accepted at BVM Workshop 2019", "journal-ref": null, "doi": "10.1007/978-3-658-25326-4_72", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histological evaluation of tissue samples is a typical approach to identify\ncolorectal cancer metastases in the peritoneum. For immediate assessment,\nreliable and real-time in-vivo imaging would be required. For example,\nintraoperative confocal laser microscopy has been shown to be suitable for\ndistinguishing organs and also malignant and benign tissue. So far, the\nanalysis is done by human experts. We investigate the feasibility of automatic\ncolon cancer classification from confocal laser microscopy images using deep\nlearning models. We overcome very small dataset sizes through transfer learning\nwith state-of-the-art architectures. We achieve an accuracy of 89.1% for cancer\ndetection in the peritoneum which indicates viability as an intraoperative\ndecision support system.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 14:49:39 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 10:00:00 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Gessert", "Nils", ""], ["Wittig", "Lukas", ""], ["Dr\u00f6mann", "Daniel", ""], ["Keck", "Tobias", ""], ["Schlaefer", "Alexander", ""], ["Ellebrecht", "David B.", ""]]}, {"id": "1812.01465", "submitter": "Bappaditya Mandal Dr.", "authors": "S. S. Behera and Bappaditya Mandal and N. B. Puhan", "title": "Cross-spectral Periocular Recognition: A Survey", "comments": "12 pages, 4 figures, 1 table, accepted in the Third International\n  Conference on Emerging Research in Electronics, Computer science and\n  Technology (ICERECT), during August 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among many biometrics such as face, iris, fingerprint and others, periocular\nregion has the advantages over other biometrics because it is non-intrusive and\nserves as a balance between iris or eye region (very stringent, small area) and\nthe whole face region (very relaxed large area). Research have shown that this\nis the region which does not get affected much because of various poses, aging,\nexpression, facial changes and other artifacts, which otherwise would change to\na large variation. Active research has been carried out on this topic since\npast few years due to its obvious advantages over face and iris biometrics in\nunconstrained and uncooperative scenarios. Many researchers have explored\nperiocular biometrics involving both visible (VIS) and infra-red (IR) spectrum\nimages. For a system to work for 24/7 (such as in surveillance scenarios), the\nregistration process may depend on the day time VIS periocular images (or any\nmug shot image) and the testing or recognition process may occur in the night\ntime involving only IR periocular images. This gives rise to a challenging\nresearch problem called the cross-spectral matching of images where VIS images\nare used for registration or as gallery images and IR images are used for\ntesting or recognition process and vice versa. After intensive research of more\nthan two decades on face and iris biometrics in cross-spectral domain, a number\nof researchers have now focused their work on matching heterogeneous\n(cross-spectral) periocular images. Though a number of surveys have been made\non existing periocular biometric research, no study has been done on its\ncross-spectral aspect. This paper analyses and reviews current state-of-the-art\ntechniques in cross-spectral periocular recognition including various\nmethodologies, databases, their protocols and current-state-of-the-art\nrecognition performances.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 14:52:15 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Behera", "S. S.", ""], ["Mandal", "Bappaditya", ""], ["Puhan", "N. B.", ""]]}, {"id": "1812.01496", "submitter": "Wenwen Li", "authors": "Wenwen Li, Jian Lou, Shuo Zhou and Haiping Lu", "title": "Sturm: Sparse Tubal-Regularized Multilinear Regression for fMRI", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While functional magnetic resonance imaging (fMRI) is important for\nhealthcare/neuroscience applications, it is challenging to classify or\ninterpret due to its multi-dimensional structure, high dimensionality, and\nsmall number of samples available. Recent sparse multilinear regression methods\nbased on tensor are emerging as promising solutions for fMRI, yet existing\nworks rely on unfolding/folding operations and a tensor rank relaxation with\nlimited tightness. The newly proposed tensor singular value decomposition\n(t-SVD) sheds light on new directions. In this work, we study t-SVD for sparse\nmultilinear regression and propose a Sparse tubal-regularized multilinear\nregression (Sturm) method for fMRI. Specifically, the Sturm model performs\nmultilinear regression with two regularization terms: a tubal tensor nuclear\nnorm based on t-SVD and a standard L1 norm. We further derive the algorithm\nunder the alternating direction method of multipliers framework. We perform\nexperiments on four classification problems, including both resting-state fMRI\nfor disease diagnosis and task-based fMRI for neural decoding. The results show\nthe superior performance of Sturm in classifying fMRI using just a small number\nof voxels.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 15:52:51 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Li", "Wenwen", ""], ["Lou", "Jian", ""], ["Zhou", "Shuo", ""], ["Lu", "Haiping", ""]]}, {"id": "1812.01497", "submitter": "Zuoyue Li", "authors": "Zuoyue Li, Jan Dirk Wegner, Aur\\'elien Lucchi", "title": "Topological Map Extraction from Overhead Images", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach, named PolyMapper, to circumvent the conventional\npixel-wise segmentation of (aerial) images and predict objects in a vector\nrepresentation directly. PolyMapper directly extracts the topological map of a\ncity from overhead images as collections of building footprints and road\nnetworks. In order to unify the shape representation for different types of\nobjects, we also propose a novel sequentialization method that reformulates a\ngraph structure as closed polygons. Experiments are conducted on both existing\nand self-collected large-scale datasets of several cities. Our empirical\nresults demonstrate that our end-to-end learnable model is capable of drawing\npolygons of building footprints and road networks that very closely approximate\nthe structure of existing online map services, in a fully automated manner.\nQuantitative and qualitative comparison to the state-of-the-art also shows that\nour approach achieves good levels of performance. To the best of our knowledge,\nthe automatic extraction of large-scale topological maps is a novel\ncontribution in the remote sensing community that we believe will help develop\nmodels with more informed geometrical constraints.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 15:52:52 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 22:05:21 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2019 12:34:35 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Li", "Zuoyue", ""], ["Wegner", "Jan Dirk", ""], ["Lucchi", "Aur\u00e9lien", ""]]}, {"id": "1812.01516", "submitter": "Pawe{\\l} Korus", "authors": "Pawel Korus, Nasir Memon", "title": "Content Authentication for Neural Imaging Pipelines: End-to-end\n  Optimization of Photo Provenance in Complex Distribution Channels", "comments": "Camera ready + supplement, CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic analysis of digital photo provenance relies on intrinsic traces left\nin the photograph at the time of its acquisition. Such analysis becomes\nunreliable after heavy post-processing, such as down-sampling and\nre-compression applied upon distribution in the Web. This paper explores\nend-to-end optimization of the entire image acquisition and distribution\nworkflow to facilitate reliable forensic analysis at the end of the\ndistribution channel. We demonstrate that neural imaging pipelines can be\ntrained to replace the internals of digital cameras, and jointly optimized for\nhigh-fidelity photo development and reliable provenance analysis. In our\nexperiments, the proposed approach increased image manipulation detection\naccuracy from 45% to over 90%. The findings encourage further research towards\nbuilding more reliable imaging pipelines with explicit provenance-guaranteeing\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 16:38:47 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 16:46:19 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Korus", "Pawel", ""], ["Memon", "Nasir", ""]]}, {"id": "1812.01519", "submitter": "Hang Chu", "authors": "Hang Chu, Wei-Chiu Ma, Kaustav Kundu, Raquel Urtasun, Sanja Fidler", "title": "SurfConv: Bridging 3D and 2D Convolution for RGBD Images", "comments": "Published at CVPR 2018", "journal-ref": "CVPR (2018) 3002-3011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of using 3D information in convolutional neural\nnetworks for down-stream recognition tasks. Using depth as an additional\nchannel alongside the RGB input has the scale variance problem present in image\nconvolution based approaches. On the other hand, 3D convolution wastes a large\namount of memory on mostly unoccupied 3D space, which consists of only the\nsurface visible to the sensor. Instead, we propose SurfConv, which \"slides\"\ncompact 2D filters along the visible 3D surface. SurfConv is formulated as a\nsimple depth-aware multi-scale 2D convolution, through a new Data-Driven Depth\nDiscretization (D4) scheme. We demonstrate the effectiveness of our method on\nindoor and outdoor 3D semantic segmentation datasets. Our method achieves\nstate-of-the-art performance with less than 30% parameters used by the 3D\nconvolution-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 16:42:29 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Chu", "Hang", ""], ["Ma", "Wei-Chiu", ""], ["Kundu", "Kaustav", ""], ["Urtasun", "Raquel", ""], ["Fidler", "Sanja", ""]]}, {"id": "1812.01525", "submitter": "Hang Chu", "authors": "Hang Chu, Daiqing Li, Sanja Fidler", "title": "A Face-to-Face Neural Conversation Model", "comments": "Published at CVPR 2018", "journal-ref": "CVPR (2018) 7113-7121", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have recently become good at engaging in dialog. However,\ncurrent approaches are based solely on verbal text, lacking the richness of a\nreal face-to-face conversation. We propose a neural conversation model that\naims to read and generate facial gestures alongside with text. This allows our\nmodel to adapt its response based on the \"mood\" of the conversation. In\nparticular, we introduce an RNN encoder-decoder that exploits the movement of\nfacial muscles, as well as the verbal conversation. The decoder consists of two\nlayers, where the lower layer aims at generating the verbal response and coarse\nfacial expressions, while the second layer fills in the subtle gestures, making\nthe generated output more smooth and natural. We train our neural network by\nhaving it \"watch\" 250 movies. We showcase our joint face-text model in\ngenerating more natural conversations through automatic metrics and a human\nstudy. We demonstrate an example application with a face-to-face chatting\navatar.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 16:55:25 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Chu", "Hang", ""], ["Li", "Daiqing", ""], ["Fidler", "Sanja", ""]]}, {"id": "1812.01547", "submitter": "Samuel Finlayson", "authors": "Samuel G. Finlayson, Hyunkwang Lee, Isaac S. Kohane, Luke\n  Oakden-Rayner", "title": "Towards generative adversarial networks as a new paradigm for radiology\n  education", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:cs/0101200", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/224", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical students and radiology trainees typically view thousands of images in\norder to \"train their eye\" to detect the subtle visual patterns necessary for\ndiagnosis. Nevertheless, infrastructural and legal constraints often make it\ndifficult to access and quickly query an abundance of images with a\nuser-specified feature set. In this paper, we use a conditional generative\nadversarial network (GAN) to synthesize $1024\\times1024$ pixel pelvic\nradiographs that can be queried with conditioning on fracture status. We\ndemonstrate that the conditional GAN learns features that distinguish fractures\nfrom non-fractures by training a convolutional neural network exclusively on\nimages sampled from the GAN and achieving an AUC of $>0.95$ on a held-out set\nof real images. We conduct additional analysis of the images sampled from the\nGAN and describe ongoing work to validate educational efficacy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 17:32:16 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Finlayson", "Samuel G.", ""], ["Lee", "Hyunkwang", ""], ["Kohane", "Isaac S.", ""], ["Oakden-Rayner", "Luke", ""]]}, {"id": "1812.01555", "submitter": "Rizwan Ahmed Khan", "authors": "Rizwan Ahmed Khan, Crenn Arthur, Alexandre Meyer, Saida Bouakaz", "title": "A novel database of Children's Spontaneous Facial Expressions\n  (LIRIS-CSE)", "comments": null, "journal-ref": "Image and Vision Computing, 2019", "doi": "10.1016/j.imavis.2019.02.004", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing environment is moving towards human-centered designs instead of\ncomputer centered designs and human's tend to communicate wealth of information\nthrough affective states or expressions. Traditional Human Computer Interaction\n(HCI) based systems ignores bulk of information communicated through those\naffective states and just caters for user's intentional input. Generally, for\nevaluating and benchmarking different facial expression analysis algorithms,\nstandardized databases are needed to enable a meaningful comparison. In the\nabsence of comparative tests on such standardized databases it is difficult to\nfind relative strengths and weaknesses of different facial expression\nrecognition algorithms. In this article we present a novel video database for\nChildren's Spontaneous facial Expressions (LIRIS-CSE). Proposed video database\ncontains six basic spontaneous facial expressions shown by 12 ethnically\ndiverse children between the ages of 6 and 12 years with mean age of 7.3 years.\nTo the best of our knowledge, this database is first of its kind as it records\nand shows spontaneous facial expressions of children. Previously there were few\ndatabase of children expressions and all of them show posed or exaggerated\nexpressions which are different from spontaneous or natural expressions. Thus,\nthis database will be a milestone for human behavior researchers. This database\nwill be a excellent resource for vision community for benchmarking and\ncomparing results. In this article, we have also proposed framework for\nautomatic expression recognition based on convolutional neural network (CNN)\narchitecture with transfer learning approach. Proposed architecture achieved\naverage classification accuracy of 75% on our proposed database i.e. LIRIS-CSE.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 17:49:25 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2019 16:39:21 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Khan", "Rizwan Ahmed", ""], ["Arthur", "Crenn", ""], ["Meyer", "Alexandre", ""], ["Bouakaz", "Saida", ""]]}, {"id": "1812.01584", "submitter": "Andre Araujo", "authors": "Marvin Teichmann, Andre Araujo, Menglong Zhu, Jack Sim", "title": "Detect-to-Retrieve: Efficient Regional Aggregation for Image Search", "comments": "CVPR 2019. Code and dataset available:\n  https://github.com/tensorflow/models/tree/master/research/delf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Retrieving object instances among cluttered scenes efficiently requires\ncompact yet comprehensive regional image representations. Intuitively, object\nsemantics can help build the index that focuses on the most relevant regions.\nHowever, due to the lack of bounding-box datasets for objects of interest among\nretrieval benchmarks, most recent work on regional representations has focused\non either uniform or class-agnostic region selection. In this paper, we first\nfill the void by providing a new dataset of landmark bounding boxes, based on\nthe Google Landmarks dataset, that includes $86k$ images with manually curated\nboxes from $15k$ unique landmarks. Then, we demonstrate how a trained landmark\ndetector, using our new dataset, can be leveraged to index image regions and\nimprove retrieval accuracy while being much more efficient than existing\nregional methods. In addition, we introduce a novel regional aggregated\nselective match kernel (R-ASMK) to effectively combine information from\ndetected regions into an improved holistic image representation. R-ASMK boosts\nimage retrieval accuracy substantially with no dimensionality increase, while\neven outperforming systems that index image regions independently. Our complete\nimage retrieval system improves upon the previous state-of-the-art by\nsignificant margins on the Revisited Oxford and Paris datasets. Code and data\navailable at the project webpage:\nhttps://github.com/tensorflow/models/tree/master/research/delf.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 18:40:20 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 00:47:47 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Teichmann", "Marvin", ""], ["Araujo", "Andre", ""], ["Zhu", "Menglong", ""], ["Sim", "Jack", ""]]}, {"id": "1812.01593", "submitter": "Yi Zhu", "authors": "Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn Newsam,\n  Andrew Tao and Bryan Catanzaro", "title": "Improving Semantic Segmentation via Video Propagation and Label\n  Relaxation", "comments": "CVPR 2019 Oral. Code link:\n  https://github.com/NVIDIA/semantic-segmentation. YouTube link:\n  https://www.youtube.com/watch?v=aEbXjGZDZSQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation requires large amounts of pixel-wise annotations to\nlearn accurate models. In this paper, we present a video prediction-based\nmethodology to scale up training sets by synthesizing new training samples in\norder to improve the accuracy of semantic segmentation networks. We exploit\nvideo prediction models' ability to predict future frames in order to also\npredict future labels. A joint propagation strategy is also proposed to\nalleviate mis-alignments in synthesized samples. We demonstrate that training\nsegmentation models on datasets augmented by the synthesized samples leads to\nsignificant improvements in accuracy. Furthermore, we introduce a novel\nboundary label relaxation technique that makes training robust to annotation\nnoise and propagation artifacts along object boundaries. Our proposed methods\nachieve state-of-the-art mIoUs of 83.5% on Cityscapes and 82.9% on CamVid. Our\nsingle model, without model ensembles, achieves 72.8% mIoU on the KITTI\nsemantic segmentation test set, which surpasses the winning entry of the ROB\nchallenge 2018. Our code and videos can be found at\nhttps://nv-adlr.github.io/publication/2018-Segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 18:49:54 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 18:56:34 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 03:16:39 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zhu", "Yi", ""], ["Sapra", "Karan", ""], ["Reda", "Fitsum A.", ""], ["Shih", "Kevin J.", ""], ["Newsam", "Shawn", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1812.01598", "submitter": "Donglai Xiang", "authors": "Donglai Xiang, Hanbyul Joo, Yaser Sheikh", "title": "Monocular Total Capture: Posing Face, Body, and Hands in the Wild", "comments": "17 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first method to capture the 3D total motion of a target person\nfrom a monocular view input. Given an image or a monocular video, our method\nreconstructs the motion from body, face, and fingers represented by a 3D\ndeformable mesh model. We use an efficient representation called 3D Part\nOrientation Fields (POFs), to encode the 3D orientations of all body parts in\nthe common 2D image space. POFs are predicted by a Fully Convolutional Network\n(FCN), along with the joint confidence maps. To train our network, we collect a\nnew 3D human motion dataset capturing diverse total body motion of 40 subjects\nin a multiview system. We leverage a 3D deformable human model to reconstruct\ntotal body pose from the CNN outputs by exploiting the pose and shape prior in\nthe model. We also present a texture-based tracking method to obtain temporally\ncoherent motion capture output. We perform thorough quantitative evaluations\nincluding comparison with the existing body-specific and hand-specific methods,\nand performance analysis on camera viewpoint and human pose changes. Finally,\nwe demonstrate the results of our total body motion capture on various\nchallenging in-the-wild videos. Our code and newly collected human motion\ndataset will be publicly shared.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 18:55:33 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Xiang", "Donglai", ""], ["Joo", "Hanbyul", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1812.01600", "submitter": "Mahyar Najibi", "authors": "Mahyar Najibi, Bharat Singh, Larry S. Davis", "title": "AutoFocus: Efficient Multi-Scale Inference", "comments": "To appear in Proceedings of International Conference on Computer\n  Vision (ICCV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes AutoFocus, an efficient multi-scale inference algorithm\nfor deep-learning based object detectors. Instead of processing an entire image\npyramid, AutoFocus adopts a coarse to fine approach and only processes regions\nwhich are likely to contain small objects at finer scales. This is achieved by\npredicting category agnostic segmentation maps for small objects at coarser\nscales, called FocusPixels. FocusPixels can be predicted with high recall, and\nin many cases, they only cover a small fraction of the entire image. To make\nefficient use of FocusPixels, an algorithm is proposed which generates compact\nrectangular FocusChips which enclose FocusPixels. The detector is only applied\ninside FocusChips, which reduces computation while processing finer scales.\nDifferent types of error can arise when detections from FocusChips of multiple\nscales are combined, hence techniques to correct them are proposed. AutoFocus\nobtains an mAP of 47.9% (68.3% at 50% overlap) on the COCO test-dev set while\nprocessing 6.4 images per second on a Titan X (Pascal) GPU. This is 2.5X faster\nthan our multi-scale baseline detector and matches its mAP. The number of\npixels processed in the pyramid can be reduced by 5X with a 1% drop in mAP.\nAutoFocus obtains more than 10% mAP gain compared to RetinaNet but runs at the\nsame speed with the same ResNet-101 backbone.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 18:57:08 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 17:47:18 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Najibi", "Mahyar", ""], ["Singh", "Bharat", ""], ["Davis", "Larry S.", ""]]}, {"id": "1812.01601", "submitter": "Angjoo Kanazawa", "authors": "Angjoo Kanazawa and Jason Y. Zhang and Panna Felsen and Jitendra Malik", "title": "Learning 3D Human Dynamics from Video", "comments": "To appear in CVPR 2019. Changelog: v3. +an experiment to compare\n  improvement from pseudo-gt data on single view vs temporal context model. v2.\n  camready ver: Minor update in model training where the gaussian shape prior\n  is used, updated results (similar results, same trends), added more ablation\n  study in the appendix. v1. +evaluation protocol subsection in appendix,\n  updated results due to bug fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From an image of a person in action, we can easily guess the 3D motion of the\nperson in the immediate past and future. This is because we have a mental model\nof 3D human dynamics that we have acquired from observing visual sequences of\nhumans in motion. We present a framework that can similarly learn a\nrepresentation of 3D dynamics of humans from video via a simple but effective\ntemporal encoding of image features. At test time, from video, the learned\ntemporal representation give rise to smooth 3D mesh predictions. From a single\nimage, our model can recover the current 3D mesh as well as its 3D past and\nfuture motion. Our approach is designed so it can learn from videos with 2D\npose annotations in a semi-supervised manner. Though annotated data is always\nlimited, there are millions of videos uploaded daily on the Internet. In this\nwork, we harvest this Internet-scale source of unlabeled data by training our\nmodel on unlabeled video with pseudo-ground truth 2D pose obtained from an\noff-the-shelf 2D pose detector. Our experiments show that adding more videos\nwith pseudo-ground truth 2D pose monotonically improves 3D prediction\nperformance. We evaluate our model, Human Mesh and Motion Recovery (HMMR), on\nthe recent challenging dataset of 3D Poses in the Wild and obtain\nstate-of-the-art performance on the 3D prediction task without any fine-tuning.\nThe project website with video, code, and data can be found at\nhttps://akanazawa.github.io/human_dynamics/.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 18:57:10 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 17:49:44 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 06:10:49 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 23:15:29 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Kanazawa", "Angjoo", ""], ["Zhang", "Jason Y.", ""], ["Felsen", "Panna", ""], ["Malik", "Jitendra", ""]]}, {"id": "1812.01608", "submitter": "Jacob Menick", "authors": "Jacob Menick, Nal Kalchbrenner", "title": "Generating High Fidelity Images with Subscale Pixel Networks and\n  Multidimensional Upscaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unconditional generation of high fidelity images is a longstanding\nbenchmark for testing the performance of image decoders. Autoregressive image\nmodels have been able to generate small images unconditionally, but the\nextension of these methods to large images where fidelity can be more readily\nassessed has remained an open problem. Among the major challenges are the\ncapacity to encode the vast previous context and the sheer difficulty of\nlearning a distribution that preserves both global semantic coherence and\nexactness of detail. To address the former challenge, we propose the Subscale\nPixel Network (SPN), a conditional decoder architecture that generates an image\nas a sequence of sub-images of equal size. The SPN compactly captures\nimage-wide spatial dependencies and requires a fraction of the memory and the\ncomputation required by other fully autoregressive models. To address the\nlatter challenge, we propose to use Multidimensional Upscaling to grow an image\nin both size and depth via intermediate stages utilising distinct SPNs. We\nevaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of\nImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in\nmultiple settings, set up new benchmark results in previously unexplored\nsettings and are able to generate very high fidelity large scale samples on the\nbasis of both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 15:47:44 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Menick", "Jacob", ""], ["Kalchbrenner", "Nal", ""]]}, {"id": "1812.01659", "submitter": "Oren Dovrat", "authors": "Oren Dovrat, Itai Lang, Shai Avidan", "title": "Learning to Sample", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing large point clouds is a challenging task. Therefore, the data is\noften sampled to a size that can be processed more easily. The question is how\nto sample the data? A popular sampling technique is Farthest Point Sampling\n(FPS). However, FPS is agnostic to a downstream application (classification,\nretrieval, etc.). The underlying assumption seems to be that minimizing the\nfarthest point distance, as done by FPS, is a good proxy to other objective\nfunctions.\n  We show that it is better to learn how to sample. To do that, we propose a\ndeep network to simplify 3D point clouds. The network, termed S-NET, takes a\npoint cloud and produces a smaller point cloud that is optimized for a\nparticular task. The simplified point cloud is not guaranteed to be a subset of\nthe original point cloud. Therefore, we match it to a subset of the original\npoints in a post-processing step. We contrast our approach with FPS by\nexperimenting on two standard data sets and show significantly better results\nfor a variety of applications. Our code is publicly available at:\nhttps://github.com/orendv/learning_to_sample\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 19:58:44 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 16:10:37 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Dovrat", "Oren", ""], ["Lang", "Itai", ""], ["Avidan", "Shai", ""]]}, {"id": "1812.01677", "submitter": "Ning Jin", "authors": "Ning Jin, Yilin Zhu, Zhenglin Geng and Ronald Fedkiw", "title": "A Pixel-Based Framework for Data-Driven Clothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim of creating virtual cloth deformations more similar to real\nworld clothing, we propose a new computational framework that recasts three\ndimensional cloth deformation as an RGB image in a two dimensional pattern\nspace. Then a three dimensional animation of cloth is equivalent to a sequence\nof two dimensional RGB images, which in turn are driven/choreographed via\nanimation parameters such as joint angles. This allows us to leverage popular\nCNNs to learn cloth deformations in image space. The two dimensional cloth\npixels are extended into the real world via standard body skinning techniques,\nafter which the RGB values are interpreted as texture offsets and displacement\nmaps. Notably, we illustrate that our approach does not require accurate\nunclothed body shapes or robust skinning techniques. Additionally, we discuss\nhow standard image based techniques such as image partitioning for higher\nresolution, GANs for merging partitioned image regions back together, etc., can\nreadily be incorporated into our framework.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 04:52:10 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Jin", "Ning", ""], ["Zhu", "Yilin", ""], ["Geng", "Zhenglin", ""], ["Fedkiw", "Ronald", ""]]}, {"id": "1812.01681", "submitter": "Fabio De Sousa Ribeiro", "authors": "Fabio De Sousa Ribeiro, Francesco Caliva, Mark Swainson, Kjartan\n  Gudmundsson, Georgios Leontidis, Stefanos Kollias", "title": "Deep Bayesian Self-Training", "comments": "16 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised Deep Learning has been highly successful in recent years,\nachieving state-of-the-art results in most tasks. However, with the ongoing\nuptake of such methods in industrial applications, the requirement for large\namounts of annotated data is often a challenge. In most real world problems,\nmanual annotation is practically intractable due to time/labour constraints,\nthus the development of automated and adaptive data annotation systems is\nhighly sought after. In this paper, we propose both a (i) Deep Bayesian\nSelf-Training methodology for automatic data annotation, by leveraging\npredictive uncertainty estimates using variational inference and modern Neural\nNetwork architectures, as well as (ii) a practical adaptation procedure for\nhandling high label variability between different dataset distributions through\nclustering of Neural Network latent variable representations. An experimental\nstudy on both public and private datasets is presented illustrating the\nsuperior performance of the proposed approach over standard Self-Training\nbaselines, highlighting the importance of predictive uncertainty estimates in\nsafety-critical domains.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:59:06 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 16:04:57 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 15:38:40 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Ribeiro", "Fabio De Sousa", ""], ["Caliva", "Francesco", ""], ["Swainson", "Mark", ""], ["Gudmundsson", "Kjartan", ""], ["Leontidis", "Georgios", ""], ["Kollias", "Stefanos", ""]]}, {"id": "1812.01687", "submitter": "Tianhang Zheng", "authors": "Tianhang Zheng, Changyou Chen, Junsong Yuan, Bo Li, Kui Ren", "title": "PointCloud Saliency Maps", "comments": "Accepted to ICCV19 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point-cloud recognition with PointNet and its variants has received\nremarkable progress. A missing ingredient, however, is the ability to\nautomatically evaluate point-wise importance w.r.t.\\! classification\nperformance, which is usually reflected by a saliency map. A saliency map is an\nimportant tool as it allows one to perform further processes on point-cloud\ndata. In this paper, we propose a novel way of characterizing critical points\nand segments to build point-cloud saliency maps. Our method assigns each point\na score reflecting its contribution to the model-recognition loss. The saliency\nmap explicitly explains which points are the key for model recognition.\nFurthermore, aggregations of highly-scored points indicate important\nsegments/subsets in a point-cloud. Our motivation for constructing a saliency\nmap is by point dropping, which is a non-differentiable operator. To overcome\nthis issue, we approximate point-dropping with a differentiable procedure of\nshifting points towards the cloud centroid. Consequently, each saliency score\ncan be efficiently measured by the corresponding gradient of the loss w.r.t the\npoint under the spherical coordinates. Extensive evaluations on several\nstate-of-the-art point-cloud recognition models, including PointNet, PointNet++\nand DGCNN, demonstrate the veracity and generality of our proposed saliency\nmap. Code for experiments is released on\n\\url{https://github.com/tianzheng4/PointCloud-Saliency-Maps}.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 21:50:49 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 21:03:02 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 18:19:28 GMT"}, {"version": "v4", "created": "Sun, 31 Mar 2019 18:51:59 GMT"}, {"version": "v5", "created": "Thu, 1 Aug 2019 09:48:49 GMT"}, {"version": "v6", "created": "Thu, 12 Sep 2019 19:29:12 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Zheng", "Tianhang", ""], ["Chen", "Changyou", ""], ["Yuan", "Junsong", ""], ["Li", "Bo", ""], ["Ren", "Kui", ""]]}, {"id": "1812.01689", "submitter": "KouZi Xing", "authors": "KouZi Xing", "title": "Training for 'Unstable' CNN Accelerator:A Case Study on FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the great advancements of convolution neural networks(CNN), CNN\naccelerators are increasingly developed and deployed in the major computing\nsystems.To make use of the CNN accelerators, CNN models are trained via the\noff-line training systems such as Caffe, Pytorch and Tensorflow on multi-core\nCPUs and GPUs first and then compiled to the target accelerators. Although the\ntwo-step process seems to be natural and has been widely applied, it assumes\nthat the accelerators' behavior can be fully modeled on CPUs and GPUs. This\ndoes not hold true and the behavior of the CNN accelerators is un-deterministic\nwhen the circuit works at 'unstable' mode when it is overclocked or is affected\nby the environment like fault-prone aerospace. The exact behaviors of the\naccelerators are determined by both the chip fabrication and the working\nenvironment or status. In this case, applying the conventional off-line\ntraining result to the accelerators directly may lead to considerable accuracy\nloss.\n  To address this problem, we propose to train for the 'unstable' CNN\naccelerator and have the 'un-determined behavior' learned together with the\ndata in the same framework. Basically, it starts from the off-line trained\nmodel and then integrates the uncertain circuit behaviors into the CNN models\nthrough additional accelerator-specific training. The fine-tuned training makes\nthe CNN models less sensitive to the circuit uncertainty. We apply the design\nmethod to both an overclocked CNN accelerator and a faulty accelerator.\nAccording to our experiments on a subset of ImageNet, the accelerator-specific\ntraining can improve the top 5 accuracy up to 3.4% and 2.4% on average when the\nCNN accelerator is at extreme overclocking. When the accelerator is exposed to\na faulty environment, the top 5 accuracy improves up to 6.8% and 4.28% on\naverage under the most severe fault injection.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 11:45:07 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Xing", "KouZi", ""]]}, {"id": "1812.01690", "submitter": "Tatsuki Koga", "authors": "Tatsuki Koga, Naoki Nonaka, Jun Sakuma, Jun Seita", "title": "General-to-Detailed GAN for Infrequent Class Medical Images", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/64", "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has significant potential for medical imaging. However, since\nthe incident rate of each disease varies widely, the frequency of classes in a\nmedical image dataset is imbalanced, leading to poor accuracy for such\ninfrequent classes. One possible solution is data augmentation of infrequent\nclasses using synthesized images created by Generative Adversarial Networks\n(GANs), but conventional GANs also require certain amount of images to learn.\nTo overcome this limitation, here we propose General-to-detailed GAN (GDGAN),\nserially connected two GANs, one for general labels and the other for detailed\nlabels. GDGAN produced diverse medical images, and the network trained with an\naugmented dataset outperformed other networks using existing methods with\nrespect to Area-Under-Curve (AUC) of Receiver Operating Characteristic (ROC)\ncurve.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 05:40:15 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Koga", "Tatsuki", ""], ["Nonaka", "Naoki", ""], ["Sakuma", "Jun", ""], ["Seita", "Jun", ""]]}, {"id": "1812.01699", "submitter": "Jay Taneja", "authors": "Gabriel Cadamuro, Aggrey Muhebwa and Jay Taneja", "title": "Assigning a Grade: Accurate Measurement of Road Quality Using Satellite\n  Imagery", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roads are critically important infrastructure to societal and economic\ndevelopment, with huge investments made by governments every year. However,\nmethods for monitoring those investments tend to be time-consuming, laborious,\nand expensive, placing them out of reach for many developing regions. In this\nwork, we develop a model for monitoring the quality of road infrastructure\nusing satellite imagery. For this task, we harness two trends: the increasing\navailability of high-resolution, often-updated satellite imagery, and the\nenormous improvement in speed and accuracy of convolutional neural\nnetwork-based methods for performing computer vision tasks. We employ a unique\ndataset of road quality information on 7000km of roads in Kenya combined with\n50cm resolution satellite imagery. We create models for a binary classification\ntask as well as a comprehensive 5-category classification task, with accuracy\nscores of 88 and 73 percent respectively. We also provide evidence of the\nrobustness of our methods with challenging held-out scenarios, though we note\nsome improvement is still required for confident analysis of a never before\nseen road. We believe these results are well-positioned to have substantial\nimpact on a broad set of transport applications.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 01:43:26 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 02:38:23 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Cadamuro", "Gabriel", ""], ["Muhebwa", "Aggrey", ""], ["Taneja", "Jay", ""]]}, {"id": "1812.01710", "submitter": "Sebastian Bujwid", "authors": "Sebastian Bujwid, Miquel Mart\\'i, Hossein Azizpour, Alessandro\n  Pieropan", "title": "GANtruth - an unpaired image-to-image translation method for driving\n  scenarios", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS),\n  Machine Learning for Intelligent Transportation Systems Workshop, Montr\\'eal,\n  Canada. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic image translation has significant potentials in autonomous\ntransportation systems. That is due to the expense of data collection and\nannotation as well as the unmanageable diversity of real-words situations. The\nmain issue with unpaired image-to-image translation is the ill-posed nature of\nthe problem. In this work, we propose a novel method for constraining the\noutput space of unpaired image-to-image translation. We make the assumption\nthat the environment of the source domain is known (e.g. synthetically\ngenerated), and we propose to explicitly enforce preservation of the\nground-truth labels on the translated images.\n  We experiment on preserving ground-truth information such as semantic\nsegmentation, disparity, and instance segmentation. We show significant\nevidence that our method achieves improved performance over the\nstate-of-the-art model of UNIT for translating images from SYNTHIA to\nCityscapes. The generated images are perceived as more realistic in human\nsurveys and outperforms UNIT when used in a domain adaptation scenario for\nsemantic segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 23:19:43 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Bujwid", "Sebastian", ""], ["Mart\u00ed", "Miquel", ""], ["Azizpour", "Hossein", ""], ["Pieropan", "Alessandro", ""]]}, {"id": "1812.01711", "submitter": "Yingxue Zhang", "authors": "Yingxue Zhang, Michael Rabbat", "title": "A Graph-CNN for 3D Point Cloud Classification", "comments": "Published as a conference paper at ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional neural networks (Graph-CNNs) extend traditional CNNs to\nhandle data that is supported on a graph. Major challenges when working with\ndata on graphs are that the support set (the vertices of the graph) do not\ntypically have a natural ordering, and in general, the topology of the graph is\nnot regular (i.e., vertices do not all have the same number of neighbors).\nThus, Graph-CNNs have huge potential to deal with 3D point cloud data which has\nbeen obtained from sampling a manifold. In this paper, we develop a Graph-CNN\nfor classifying 3D point cloud data, called PointGCN. The architecture combines\nlocalized graph convolutions with two types of graph downsampling operations\n(also known as pooling). By the effective exploration of the point cloud local\nstructure using the Graph-CNN, the proposed architecture achieves competitive\nperformance on the 3D object classification benchmark ModelNet, and our\narchitecture is more stable than competing schemes.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:00:34 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Yingxue", ""], ["Rabbat", "Michael", ""]]}, {"id": "1812.01712", "submitter": "Ye Zhu", "authors": "Ye Zhu and Sven Ewan Shepstone and Pablo Mart\\'inez-Nuevo and Miklas\n  Str{\\o}m Kristoffersen and Fabien Moutarde and Zhuang Fu", "title": "Multiview Based 3D Scene Understanding On Partial Point Sets", "comments": "This paper has been submitted to IEEE Transactions on Neural Networks\n  and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning within the context of point clouds has gained much research\ninterest in recent years mostly due to the promising results that have been\nachieved on a number of challenging benchmarks, such as 3D shape recognition\nand scene semantic segmentation. In many realistic settings however, snapshots\nof the environment are often taken from a single view, which only contains a\npartial set of the scene due to the field of view restriction of commodity\ncameras. 3D scene semantic understanding on partial point clouds is considered\nas a challenging task. In this work, we propose a processing approach for 3D\npoint cloud data based on a multiview representation of the existing 360{\\deg}\npoint clouds. By fusing the original 360{\\deg} point clouds and their\ncorresponding 3D multiview representations as input data, a neural network is\nable to recognize partial point sets while improving the general performance on\ncomplete point sets, resulting in an overall increase of 31.9% and 4.3% in\nsegmentation accuracy for partial and complete scene semantic understanding,\nrespectively. This method can also be applied in a wider 3D recognition context\nsuch as 3D part segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 12:23:59 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhu", "Ye", ""], ["Shepstone", "Sven Ewan", ""], ["Mart\u00ednez-Nuevo", "Pablo", ""], ["Kristoffersen", "Miklas Str\u00f8m", ""], ["Moutarde", "Fabien", ""], ["Fu", "Zhuang", ""]]}, {"id": "1812.01713", "submitter": "Haibin Zheng", "authors": "Jinyin Chen, Haibin Zheng, Hui Xiong, Mengmeng Su", "title": "FineFool: Fine Object Contour Attack via Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have been shown vulnerable to adversarial attacks\nlaunched by adversarial examples which are carefully crafted by attacker to\ndefeat classifiers. Deep learning models cannot escape the attack either. Most\nof adversarial attack methods are focused on success rate or perturbations\nsize, while we are more interested in the relationship between adversarial\nperturbation and the image itself. In this paper, we put forward a novel\nadversarial attack based on contour, named FineFool. Finefool not only has\nbetter attack performance compared with other state-of-art white-box attacks in\naspect of higher attack success rate and smaller perturbation, but also capable\nof visualization the optimal adversarial perturbation via attention on object\ncontour. To the best of our knowledge, Finefool is for the first time combines\nthe critical feature of the original clean image with the optimal perturbations\nin a visible manner. Inspired by the correlations between adversarial\nperturbations and object contour, slighter perturbations is produced via\nfocusing on object contour features, which is more imperceptible and difficult\nto be defended, especially network add-on defense methods with the trade-off\nbetween perturbations filtering and contour feature loss. Compared with\nexisting state-of-art attacks, extensive experiments are conducted to show that\nFinefool is capable of efficient attack against defensive deep models.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 12:58:56 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Chen", "Jinyin", ""], ["Zheng", "Haibin", ""], ["Xiong", "Hui", ""], ["Su", "Mengmeng", ""]]}, {"id": "1812.01714", "submitter": "Yuji Yoshimura", "authors": "Yuji Yoshimura, Bill Cai, Zhoutong Wang, Carlo Ratti", "title": "Deep Learning Architect: Classification for Architectural Design through\n  the Eye of Artificial Intelligence", "comments": "22 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies state-of-the-art techniques in deep learning and computer\nvision to measure visual similarities between architectural designs by\ndifferent architects. Using a dataset consisting of web scraped images and an\noriginal collection of images of architectural works, we first train a deep\nconvolutional neural network (DCNN) model capable of achieving 73% accuracy in\nclassifying works belonging to 34 different architects. Through examining the\nweights in the trained DCNN model, we are able to quantitatively measure the\nvisual similarities between architects that are implicitly learned by our\nmodel. Using this measure, we cluster architects that are identified to be\nsimilar and compare our findings to conventional classification made by\narchitectural historians and theorists. Our clustering of architectural designs\nremarkably corroborates conventional views in architectural history, and the\nlearned architectural features also coheres with the traditional understanding\nof architectural designs.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 00:30:59 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yoshimura", "Yuji", ""], ["Cai", "Bill", ""], ["Wang", "Zhoutong", ""], ["Ratti", "Carlo", ""]]}, {"id": "1812.01716", "submitter": "Shehroz Khan", "authors": "Ahmed Ashraf, Shehroz Khan, Nikhil Bhagwat, Mallar Chakravarty, Babak\n  Taati", "title": "Learning to Unlearn: Building Immunity to Dataset Bias in Medical\n  Imaging Studies", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216 Submission Id: 207", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/207", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging machine learning algorithms are usually evaluated on a single\ndataset. Although training and testing are performed on different subsets of\nthe dataset, models built on one study show limited capability to generalize to\nother studies. While database bias has been recognized as a serious problem in\nthe computer vision community, it has remained largely unnoticed in medical\nimaging research. Transfer learning thus remains confined to the re-use of\nfeature representations requiring re-training on the new dataset. As a result,\nmachine learning models do not generalize even when trained on imaging datasets\nthat were captured to study the same variable of interest. The ability to\ntransfer knowledge gleaned from one study to another, without the need for\nre-training, if possible, would provide reassurance that the models are\nlearning knowledge fundamental to the problem under study instead of latching\nonto the idiosyncracies of a dataset. In this paper, we situate the problem of\ndataset bias in the context of medical imaging studies. We show empirical\nevidence that such a problem exists in medical datasets. We then present a\nframework to unlearn study membership as a means to handle the problem of\ndatabase bias. Our main idea is to take the data from the original feature\nspace to an intermediate space where the data points are indistinguishable in\nterms of which study they come from, while maintaining the recognition\ncapability with respect to the variable of interest. This will promote models\nwhich learn the more general properties of the etiology under study instead of\naligning to dataset-specific peculiarities. Essentially, our proposed model\nlearns to unlearn the dataset bias.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 04:46:35 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Ashraf", "Ahmed", ""], ["Khan", "Shehroz", ""], ["Bhagwat", "Nikhil", ""], ["Chakravarty", "Mallar", ""], ["Taati", "Babak", ""]]}, {"id": "1812.01717", "submitter": "Sjoerd van Steenkiste", "authors": "Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael\n  Marinier, Marcin Michalski, Sylvain Gelly", "title": "Towards Accurate Generative Models of Video: A New Metric & Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deep generative models have lead to remarkable progress in\nsynthesizing high quality images. Following their successful application in\nimage processing and representation learning, an important next step is to\nconsider videos. Learning generative models of video is a much harder task,\nrequiring a model to capture the temporal dynamics of a scene, in addition to\nthe visual presentation of objects. While recent attempts at formulating\ngenerative models of video have had some success, current progress is hampered\nby (1) the lack of qualitative metrics that consider visual quality, temporal\ncoherence, and diversity of samples, and (2) the wide gap between purely\nsynthetic video data sets and challenging real-world data sets in terms of\ncomplexity. To this extent we propose Fr\\'{e}chet Video Distance (FVD), a new\nmetric for generative models of video, and StarCraft 2 Videos (SCV), a\nbenchmark of game play from custom starcraft 2 scenarios that challenge the\ncurrent capabilities of generative models of video. We contribute a large-scale\nhuman study, which confirms that FVD correlates well with qualitative human\njudgment of generated videos, and provide initial benchmark results on SCV.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 03:57:42 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 16:43:17 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Unterthiner", "Thomas", ""], ["van Steenkiste", "Sjoerd", ""], ["Kurach", "Karol", ""], ["Marinier", "Raphael", ""], ["Michalski", "Marcin", ""], ["Gelly", "Sylvain", ""]]}, {"id": "1812.01718", "submitter": "David Ha", "authors": "Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb,\n  Kazuaki Yamamoto, David Ha", "title": "Deep Learning for Classical Japanese Literature", "comments": "To appear at Neural Information Processing Systems 2018 Workshop on\n  Machine Learning for Creativity and Design", "journal-ref": null, "doi": "10.20676/00000341", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of machine learning research focuses on producing models which perform\nwell on benchmark tasks, in turn improving our understanding of the challenges\nassociated with those tasks. From the perspective of ML researchers, the\ncontent of the task itself is largely irrelevant, and thus there have\nincreasingly been calls for benchmark tasks to more heavily focus on problems\nwhich are of social or cultural relevance. In this work, we introduce\nKuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as\nwell as two larger, more challenging datasets, Kuzushiji-49 and\nKuzushiji-Kanji. Through these datasets, we wish to engage the machine learning\ncommunity into the world of classical Japanese literature. Dataset available at\nhttps://github.com/rois-codh/kmnist\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 12:37:31 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Clanuwat", "Tarin", ""], ["Bober-Irizar", "Mikel", ""], ["Kitamoto", "Asanobu", ""], ["Lamb", "Alex", ""], ["Yamamoto", "Kazuaki", ""], ["Ha", "David", ""]]}, {"id": "1812.01719", "submitter": "Patrick McClure", "authors": "Patrick McClure, Nao Rho, John A. Lee, Jakub R. Kaczmarzyk, Charles\n  Zheng, Satrajit S. Ghosh, Dylan Nielson, Adam G. Thomas, Peter Bandettini,\n  and Francisco Pereira", "title": "Knowing what you know in brain segmentation using Bayesian deep neural\n  networks", "comments": "Submitted to Frontiers in Neuroinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a Bayesian deep neural network (DNN) for\npredicting FreeSurfer segmentations of structural MRI volumes, in minutes\nrather than hours. The network was trained and evaluated on a large dataset (n\n= 11,480), obtained by combining data from more than a hundred different sites,\nand also evaluated on another completely held-out dataset (n = 418). The\nnetwork was trained using a novel spike-and-slab dropout-based variational\ninference approach. We show that, on these datasets, the proposed Bayesian DNN\noutperforms previously proposed methods, in terms of the similarity between the\nsegmentation predictions and the FreeSurfer labels, and the usefulness of the\nestimate uncertainty of these predictions. In particular, we demonstrated that\nthe prediction uncertainty of this network at each voxel is a good indicator of\nwhether the network has made an error and that the uncertainty across the whole\nbrain can predict the manual quality control ratings of a scan. The proposed\nBayesian DNN method should be applicable to any new network architecture for\naddressing the segmentation problem.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 13:23:30 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 20:29:08 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 18:55:28 GMT"}, {"version": "v4", "created": "Sun, 16 Jun 2019 20:50:59 GMT"}, {"version": "v5", "created": "Wed, 18 Sep 2019 10:30:08 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["McClure", "Patrick", ""], ["Rho", "Nao", ""], ["Lee", "John A.", ""], ["Kaczmarzyk", "Jakub R.", ""], ["Zheng", "Charles", ""], ["Ghosh", "Satrajit S.", ""], ["Nielson", "Dylan", ""], ["Thomas", "Adam G.", ""], ["Bandettini", "Peter", ""], ["Pereira", "Francisco", ""]]}, {"id": "1812.01737", "submitter": "Siqi Liu", "authors": "Siqi Liu, Eli Gibson, Sasa Grbic, Zhoubing Xu, Arnaud Arindra Adiyoso\n  Setio, Jie Yang, Bogdan Georgescu, Dorin Comaniciu", "title": "Decompose to manipulate: Manipulable Object Synthesis in 3D Medical\n  Images with Structured Image Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of medical image analysis systems is constrained by the\nquantity of high-quality image annotations. Such systems require data to be\nannotated by experts with years of training, especially when diagnostic\ndecisions are involved. Such datasets are thus hard to scale up. In this\ncontext, it is hard for supervised learning systems to generalize to the cases\nthat are rare in the training set but would be present in real-world clinical\npractices. We believe that the synthetic image samples generated by a system\ntrained on the real data can be useful for improving the supervised learning\ntasks in the medical image analysis applications. Allowing the image synthesis\nto be manipulable could help synthetic images provide complementary information\nto the training data rather than simply duplicating the real-data manifold. In\nthis paper, we propose a framework for synthesizing 3D objects, such as\npulmonary nodules, in 3D medical images with manipulable properties. The\nmanipulation is enabled by decomposing of the object of interests into its\nsegmentation mask and a 1D vector containing the residual information. The\nsynthetic object is refined and blended into the image context with two\nadversarial discriminators. We evaluate the proposed framework on lung nodules\nin 3D chest CT images and show that the proposed framework could generate\nrealistic nodules with manipulable shapes, textures and locations, etc. By\nsampling from both the synthetic nodules and the real nodules from 2800 3D CT\nvolumes during the classifier training, we show the synthetic patches could\nimprove the overall nodule detection performance by average 8.44% competition\nperformance metric (CPM) score.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 22:52:57 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 15:38:08 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Liu", "Siqi", ""], ["Gibson", "Eli", ""], ["Grbic", "Sasa", ""], ["Xu", "Zhoubing", ""], ["Setio", "Arnaud Arindra Adiyoso", ""], ["Yang", "Jie", ""], ["Georgescu", "Bogdan", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "1812.01738", "submitter": "Yuan Yao", "authors": "Yuan Yao, Hyun Soo Park", "title": "Multiview Cross-supervision for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a semi-supervised learning framework for a customized\nsemantic segmentation task using multiview image streams. A key challenge of\nthe customized task lies in the limited accessibility of the labeled data due\nto the requirement of prohibitive manual annotation effort. We hypothesize that\nit is possible to leverage multiview image streams that are linked through the\nunderlying 3D geometry, which can provide an additional supervisionary signal\nto train a segmentation model. We formulate a new cross-supervision method\nusing a shape belief transfer---the segmentation belief in one image is used to\npredict that of the other image through epipolar geometry analogous to\nshape-from-silhouette. The shape belief transfer provides the upper and lower\nbounds of the segmentation for the unlabeled data where its gap approaches\nasymptotically to zero as the number of the labeled views increases. We\nintegrate this theory to design a novel network that is agnostic to camera\ncalibration, network model, and semantic category and bypasses the intermediate\nprocess of suboptimal 3D reconstruction. We validate this network by\nrecognizing a customized semantic category per pixel from realworld visual data\nincluding non-human species and a subject of interest in social videos where\nattaining large-scale annotation data is infeasible.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 22:57:22 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yao", "Yuan", ""], ["Park", "Hyun Soo", ""]]}, {"id": "1812.01742", "submitter": "Pedro O. Pinheiro", "authors": "Pedro O. Pinheiro and Negar Rostamzadeh and Sungjin Ahn", "title": "Domain-Adaptive Single-View 3D Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-view 3D shape reconstruction is an important but challenging problem,\nmainly for two reasons. First, as shape annotation is very expensive to\nacquire, current methods rely on synthetic data, in which ground-truth 3D\nannotation is easy to obtain. However, this results in domain adaptation\nproblem when applied to natural images. The second challenge is that there are\nmultiple shapes that can explain a given 2D image. In this paper, we propose a\nframework to improve over these challenges using adversarial training. On one\nhand, we impose domain confusion between natural and synthetic image\nrepresentations to reduce the distribution gap. On the other hand, we impose\nthe reconstruction to be `realistic' by forcing it to lie on a (learned)\nmanifold of realistic object shapes. Our experiments show that these\nconstraints improve performance by a large margin over baseline reconstruction\nmodels. We achieve results competitive with the state of the art with a much\nsimpler architecture.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 23:01:00 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 17:15:06 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Pinheiro", "Pedro O.", ""], ["Rostamzadeh", "Negar", ""], ["Ahn", "Sungjin", ""]]}, {"id": "1812.01748", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Eric Kim, Jure Leskovec, Charles Rosenberg, Julian\n  McAuley", "title": "Complete the Look: Scene-based Complementary Product Recommendation", "comments": "Accepted to CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling fashion compatibility is challenging due to its complexity and\nsubjectivity. Existing work focuses on predicting compatibility between product\nimages (e.g. an image containing a t-shirt and an image containing a pair of\njeans). However, these approaches ignore real-world 'scene' images (e.g.\nselfies); such images are hard to deal with due to their complexity, clutter,\nvariations in lighting and pose (etc.) but on the other hand could potentially\nprovide key context (e.g. the user's body type, or the season) for making more\naccurate recommendations. In this work, we propose a new task called 'Complete\nthe Look', which seeks to recommend visually compatible products based on scene\nimages. We design an approach to extract training data for this task, and\npropose a novel way to learn the scene-product compatibility from fashion or\ninterior design images. Our approach measures compatibility both globally and\nlocally via CNNs and attention mechanisms. Extensive experiments show that our\nmethod achieves significant performance gains over alternative systems. Human\nevaluation and qualitative analysis are also conducted to further understand\nmodel behavior. We hope this work could lead to useful applications which link\nlarge corpora of real-world scenes with shoppable products.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 23:30:22 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 21:32:32 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Kim", "Eric", ""], ["Leskovec", "Jure", ""], ["Rosenberg", "Charles", ""], ["McAuley", "Julian", ""]]}, {"id": "1812.01752", "submitter": "Pedro Sanches", "authors": "Pedro Sanches, Cyril Meyer, Vincent Vigon, Beno\\^it Naegel", "title": "Cerebrovascular Network Segmentation on MRA Images with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been shown to produce state of the art results in many\ntasks in biomedical imaging, especially in segmentation. Moreover, segmentation\nof the cerebrovascular structure from magnetic resonance angiography is a\nchallenging problem because its complex geometry and topology have a large\ninter-patient variability. Therefore, in this work, we present a convolutional\nneural network approach for this problem. Particularly, a new network topology\ninspired by the U-net 3D and by the Inception modules, entitled Uception. In\naddition, a discussion about the best objective function for sparse data also\nguided most choices during the project. State of the art models are also\nimplemented for a comparison purpose and final results show that the proposed\narchitecture has the best performance in this particular context.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 23:38:54 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Sanches", "Pedro", ""], ["Meyer", "Cyril", ""], ["Vigon", "Vincent", ""], ["Naegel", "Beno\u00eet", ""]]}, {"id": "1812.01754", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, Bo Wang", "title": "Moment Matching for Multi-Source Domain Adaptation", "comments": "Accepted As Oral Paper in the IEEE International Conference on\n  Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional unsupervised domain adaptation (UDA) assumes that training data\nare sampled from a single domain. This neglects the more practical scenario\nwhere training data are collected from multiple sources, requiring multi-source\ndomain adaptation. We make three major contributions towards addressing this\nproblem. First, we collect and annotate by far the largest UDA dataset, called\nDomainNet, which contains six domains and about 0.6 million images distributed\namong 345 categories, addressing the gap in data availability for multi-source\nUDA research. Second, we propose a new deep learning approach, Moment Matching\nfor Multi-Source Domain Adaptation M3SDA, which aims to transfer knowledge\nlearned from multiple labeled source domains to an unlabeled target domain by\ndynamically aligning moments of their feature distributions. Third, we provide\nnew theoretical insights specifically for moment matching approaches in both\nsingle and multiple source domain adaptation. Extensive experiments are\nconducted to demonstrate the power of our new dataset in benchmarking\nstate-of-the-art multi-source domain adaptation methods, as well as the\nadvantage of our proposed model. Dataset and Code are available at\n\\url{http://ai.bu.edu/M3SDA/}.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 23:43:37 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 22:18:34 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 20:33:48 GMT"}, {"version": "v4", "created": "Tue, 27 Aug 2019 19:47:46 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Peng", "Xingchao", ""], ["Bai", "Qinxun", ""], ["Xia", "Xide", ""], ["Huang", "Zijun", ""], ["Saenko", "Kate", ""], ["Wang", "Bo", ""]]}, {"id": "1812.01756", "submitter": "Tim G. J. Rudner", "authors": "Tim G. J. Rudner, Marc Ru{\\ss}wurm, Jakub Fil, Ramona Pelich, Benjamin\n  Bischke, Veronika Kopackova, Piotr Bilinski", "title": "Multi$^{\\mathbf{3}}$Net: Segmenting Flooded Buildings via Fusion of\n  Multiresolution, Multisensor, and Multitemporal Satellite Imagery", "comments": "To appear in Proceedings of the Thirty-Third AAAI Conference on\n  Artificial Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for rapid segmentation of flooded buildings by\nfusing multiresolution, multisensor, and multitemporal satellite imagery in a\nconvolutional neural network. Our model significantly expedites the generation\nof satellite imagery-based flood maps, crucial for first responders and local\nauthorities in the early stages of flood events. By incorporating multitemporal\nsatellite imagery, our model allows for rapid and accurate post-disaster damage\nassessment and can be used by governments to better coordinate medium- and\nlong-term financial assistance programs for affected areas. The network\nconsists of multiple streams of encoder-decoder architectures that extract\nspatiotemporal information from medium-resolution images and spatial\ninformation from high-resolution images before fusing the resulting\nrepresentations into a single medium-resolution segmentation map of flooded\nbuildings. We compare our model to state-of-the-art methods for building\nfootprint segmentation as well as to alternative fusion approaches for the\nsegmentation of flooded buildings and find that our model performs best on both\ntasks. We also demonstrate that our model produces highly accurate segmentation\nmaps of flooded buildings using only publicly available medium-resolution data\ninstead of significantly more detailed but sparsely available very\nhigh-resolution data. We release the first open-source dataset of fully\npreprocessed and labeled multiresolution, multispectral, and multitemporal\nsatellite images of disaster sites along with our source code.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 00:05:01 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Rudner", "Tim G. J.", ""], ["Ru\u00dfwurm", "Marc", ""], ["Fil", "Jakub", ""], ["Pelich", "Ramona", ""], ["Bischke", "Benjamin", ""], ["Kopackova", "Veronika", ""], ["Bilinski", "Piotr", ""]]}, {"id": "1812.01783", "submitter": "Xiu Li", "authors": "Xiu Li, Yebin Liu, Hanbyul Joo, Qionghai Dai, Yaser Sheikh", "title": "Capture Dense: Markerless Motion Capture Meets Dense Pose Estimation", "comments": "Withdraw due to incomplete experiment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to combine markerless motion capture and dense pose\nfeature estimation into a single framework. We demonstrate that dense pose\ninformation can help for multiview/single-view motion capture, and multiview\nmotion capture can help the collection of a high-quality dataset for training\nthe dense pose detector. Specifically, we first introduce a novel markerless\nmotion capture method that can take advantage of dense parsing capability\nprovided by the dense pose detector. Thanks to the introduced dense human\nparsing ability, our method is demonstrated much more efficient, and accurate\ncompared with the available state-of-the-art markerless motion capture\napproach. Second, we improve the performance of available dense pose detector\nby using multiview markerless motion capture data. Such dataset is beneficial\nto dense pose training because they are more dense and accurate and consistent,\nand can compensate for the corner cases such as unusual viewpoints. We\nquantitatively demonstrate the improved performance of our dense pose detector\nover the available DensePose. Our dense pose dataset and detector will be made\npublic.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 02:19:24 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 04:42:25 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Li", "Xiu", ""], ["Liu", "Yebin", ""], ["Joo", "Hanbyul", ""], ["Dai", "Qionghai", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1812.01784", "submitter": "Edgar Sch\\\"onfeld", "authors": "Edgar Sch\\\"onfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell,\n  Zeynep Akata", "title": "Generalized Zero- and Few-Shot Learning via Aligned Variational\n  Autoencoders", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches in generalized zero-shot learning rely on cross-modal mapping\nbetween the image feature space and the class embedding space. As labeled\nimages are expensive, one direction is to augment the dataset by generating\neither images or image features. However, the former misses fine-grained\ndetails and the latter requires learning a mapping associated with class\nembeddings. In this work, we take feature generation one step further and\npropose a model where a shared latent space of image features and class\nembeddings is learned by modality-specific aligned variational autoencoders.\nThis leaves us with the required discriminative information about the image and\nclasses in the latent features, on which we train a softmax classifier. The key\nto our approach is that we align the distributions learned from images and from\nside-information to construct latent features that contain the essential\nmulti-modal information associated with unseen classes. We evaluate our learned\nlatent features on several benchmark datasets, i.e. CUB, SUN, AWA1 and AWA2,\nand establish a new state of the art on generalized zero-shot as well as on\nfew-shot learning. Moreover, our results on ImageNet with various zero-shot\nsplits show that our latent features generalize well in large-scale settings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 02:20:12 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 14:50:48 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 15:11:06 GMT"}, {"version": "v4", "created": "Fri, 5 Apr 2019 12:52:25 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Sch\u00f6nfeld", "Edgar", ""], ["Ebrahimi", "Sayna", ""], ["Sinha", "Samarth", ""], ["Darrell", "Trevor", ""], ["Akata", "Zeynep", ""]]}, {"id": "1812.01802", "submitter": "Sourav Pal", "authors": "Sourav Pal, Tharun Mohandoss, Pabitra Mitra", "title": "Visual Attention for Behavioral Cloning in Autonomous Driving", "comments": "Paper Accepted at ICMV (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of our work is to use visual attention to enhance autonomous driving\nperformance. We present two methods of predicting visual attention maps. The\nfirst method is a supervised learning approach in which we collect eye-gaze\ndata for the task of driving and use this to train a model for predicting the\nattention map. The second method is a novel unsupervised approach where we\ntrain a model to learn to predict attention as it learns to drive a car.\nFinally, we present a comparative study of our results and show that the\nsupervised approach for predicting attention when incorporated performs better\nthan other approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 03:30:54 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Pal", "Sourav", ""], ["Mohandoss", "Tharun", ""], ["Mitra", "Pabitra", ""]]}, {"id": "1812.01819", "submitter": "Yujun Shen", "authors": "Mengya Gao, Yujun Shen, Quanquan Li, Junjie Yan, Liang Wan, Dahua Lin,\n  Chen Change Loy, Xiaoou Tang", "title": "An Embarrassingly Simple Approach for Knowledge Distillation", "comments": "8 pages and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) aims at improving the performance of a\nlow-capacity student model by inheriting knowledge from a high-capacity teacher\nmodel. Previous KD methods typically train a student by minimizing a\ntask-related loss and the KD loss simultaneously, using a pre-defined loss\nweight to balance these two terms. In this work, we propose to first transfer\nthe backbone knowledge from a teacher to the student, and then only learn the\ntask-head of the student network. Such a decomposition of the training process\ncircumvents the need of choosing an appropriate loss weight, which is often\ndifficult in practice, and thus makes it easier to apply to different datasets\nand tasks. Importantly, the decomposition permits the core of our method,\nStage-by-Stage Knowledge Distillation (SSKD), which facilitates progressive\nfeature mimicking from teacher to student. Extensive experiments on CIFAR-100\nand ImageNet suggest that SSKD significantly narrows down the performance gap\nbetween student and teacher, outperforming state-of-the-art approaches. We also\ndemonstrate the generalization ability of SSKD on other challenging benchmarks,\nincluding face recognition on IJB-A dataset as well as object detection on COCO\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 05:09:45 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 16:46:52 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Gao", "Mengya", ""], ["Shen", "Yujun", ""], ["Li", "Quanquan", ""], ["Yan", "Junjie", ""], ["Wan", "Liang", ""], ["Lin", "Dahua", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1812.01821", "submitter": "Yifan Chen", "authors": "Yifan Chen and Yevgeniy Vorobeychik", "title": "Regularized Ensembles and Transferability in Adversarial Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the considerable success of convolutional neural networks in a broad\narray of domains, recent research has shown these to be vulnerable to small\nadversarial perturbations, commonly known as adversarial examples. Moreover,\nsuch examples have shown to be remarkably portable, or transferable, from one\nmodel to another, enabling highly successful black-box attacks. We explore this\nissue of transferability and robustness from two dimensions: first, considering\nthe impact of conventional $l_p$ regularization as well as replacing the top\nlayer with a linear support vector machine (SVM), and second, the value of\ncombining regularized models into an ensemble. We show that models trained with\ndifferent regularizers present barriers to transferability, as does partial\ninformation about the models comprising the ensemble.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 05:32:00 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Chen", "Yifan", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1812.01855", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Hanwang Zhang, Juanzi Li", "title": "Explainable and Explicit Visual Reasoning over Scene Graphs", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to dismantle the prevalent black-box neural architectures used in\ncomplex visual reasoning tasks, into the proposed eXplainable and eXplicit\nNeural Modules (XNMs), which advance beyond existing neural module networks\ntowards using scene graphs --- objects as nodes and the pairwise relationships\nas edges --- for explainable and explicit reasoning with structured knowledge.\nXNMs allow us to pay more attention to teach machines how to \"think\",\nregardless of what they \"look\". As we will show in the paper, by using scene\ngraphs as an inductive bias, 1) we can design XNMs in a concise and flexible\nfashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce\nthe number of parameters by 10 to 100 times, and 2) we can explicitly trace the\nreasoning-flow in terms of graph attentions. XNMs are so generic that they\nsupport a wide range of scene graph implementations with various qualities. For\nexample, when the graphs are detected perfectly, XNMs achieve 100% accuracy on\nboth CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound\nfor visual reasoning; when the graphs are noisily detected from real-world\nimages, XNMs are still robust to achieve a competitive 67.5% accuracy on\nVQAv2.0, surpassing the popular bag-of-objects attention models without graph\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 08:35:05 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 12:55:14 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Shi", "Jiaxin", ""], ["Zhang", "Hanwang", ""], ["Li", "Juanzi", ""]]}, {"id": "1812.01859", "submitter": "Alessa Hering", "authors": "Alessa Hering, Sven Kuckertz, Stefan Heldmann, Mattias Heinrich", "title": "Enhancing Label-Driven Deep Deformable Image Registration with Local\n  Distance Metrics for State-of-the-Art Cardiac Motion Tracking", "comments": "Accepted at BVM Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has achieved significant advances in accuracy for medical\nimage segmentation, its benefits for deformable image registration have so far\nremained limited to reduced computation times. Previous work has either focused\non replacing the iterative optimization of distance and smoothness terms with\nCNN-layers or using supervised approaches driven by labels. Our method is the\nfirst to combine the complementary strengths of global semantic information\n(represented by segmentation labels) and local distance metrics that help align\nsurrounding structures. We demonstrate significant higher Dice scores (of\n86.5\\%) for deformable cardiac image registration compared to classic\nregistration (79.0\\%) as well as label-driven deep learning frameworks\n(83.4\\%).\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 08:49:47 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Hering", "Alessa", ""], ["Kuckertz", "Sven", ""], ["Heldmann", "Stefan", ""], ["Heinrich", "Mattias", ""]]}, {"id": "1812.01866", "submitter": "Bingyi Kang", "authors": "Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, Trevor\n  Darrell", "title": "Few-shot Object Detection via Feature Reweighting", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional training of a deep CNN based object detector demands a large\nnumber of bounding box annotations, which may be unavailable for rare\ncategories. In this work we develop a few-shot object detector that can learn\nto detect novel objects from only a few annotated examples. Our proposed model\nleverages fully labeled base classes and quickly adapts to novel classes, using\na meta feature learner and a reweighting module within a one-stage detection\narchitecture. The feature learner extracts meta features that are generalizable\nto detect novel object classes, using training data from base classes with\nsufficient samples. The reweighting module transforms a few support examples\nfrom the novel classes to a global vector that indicates the importance or\nrelevance of meta features for detecting the corresponding objects. These two\nmodules, together with a detection prediction module, are trained end-to-end\nbased on an episodic few-shot learning scheme and a carefully designed loss\nfunction. Through extensive experiments we demonstrate that our model\noutperforms well-established baselines by a large margin for few-shot object\ndetection, on multiple datasets and settings. We also present analysis on\nvarious aspects of our proposed model, aiming to provide some inspiration for\nfuture few-shot detection works.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 09:23:41 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 08:50:16 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Kang", "Bingyi", ""], ["Liu", "Zhuang", ""], ["Wang", "Xin", ""], ["Yu", "Fisher", ""], ["Feng", "Jiashi", ""], ["Darrell", "Trevor", ""]]}, {"id": "1812.01874", "submitter": "Adrian W\\\"alchli", "authors": "Qiyang Hu, Adrian W\\\"alchli, Tiziano Portenier, Matthias Zwicker,\n  Paolo Favaro", "title": "Learning to Take Directions One Step at a Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to generate a video sequence given a single image.\nBecause items in an image can be animated in arbitrarily many different ways,\nwe introduce as control signal a sequence of motion strokes. Such control\nsignal can be automatically transferred from other videos, e.g., via bounding\nbox tracking. Each motion stroke provides the direction to the moving object in\nthe input image and we aim to train a network to generate an animation\nfollowing a sequence of such directions. To address this task we design a novel\nrecurrent architecture, which can be trained easily and effectively thanks to\nan explicit separation of past, future and current states. As we demonstrate in\nthe experiments, our proposed architecture is capable of generating an\narbitrary number of frames from a single image and a sequence of motion\nstrokes. Key components of our architecture are an autoencoding constraint to\nensure consistency with the past and a generative adversarial scheme to ensure\nthat images look realistic and are temporally smooth. We demonstrate the\neffectiveness of our approach on the MNIST, KTH, Human3.6M, Push and Weizmann\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 09:42:12 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 00:05:47 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 06:39:39 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Hu", "Qiyang", ""], ["W\u00e4lchli", "Adrian", ""], ["Portenier", "Tiziano", ""], ["Zwicker", "Matthias", ""], ["Favaro", "Paolo", ""]]}, {"id": "1812.01880", "submitter": "Kaihua Tang", "authors": "Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, Wei Liu", "title": "Learning to Compose Dynamic Tree Structures for Visual Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to compose dynamic tree structures that place the objects in an\nimage into a visual context, helping visual reasoning tasks such as scene graph\ngeneration and visual Q&A. Our visual context tree model, dubbed VCTree, has\ntwo key advantages over existing structured object representations including\nchains and fully-connected graphs: 1) The efficient and expressive binary tree\nencodes the inherent parallel/hierarchical relationships among objects, e.g.,\n\"clothes\" and \"pants\" are usually co-occur and belong to \"person\"; 2) the\ndynamic structure varies from image to image and task to task, allowing more\ncontent-/task-specific message passing among objects. To construct a VCTree, we\ndesign a score function that calculates the task-dependent validity between\neach object pair, and the tree is the binary version of the maximum spanning\ntree from the score matrix. Then, visual contexts are encoded by bidirectional\nTreeLSTM and decoded by task-specific models. We develop a hybrid learning\nprocedure which integrates end-task supervised learning and the tree structure\nreinforcement learning, where the former's evaluation result serves as a\nself-critic for the latter's structure exploration. Experimental results on two\nbenchmarks, which require reasoning over contexts: Visual Genome for scene\ngraph generation and VQA2.0 for visual Q&A, show that VCTree outperforms\nstate-of-the-art results while discovering interpretable visual context\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 09:51:19 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Tang", "Kaihua", ""], ["Zhang", "Hanwang", ""], ["Wu", "Baoyuan", ""], ["Luo", "Wenhan", ""], ["Liu", "Wei", ""]]}, {"id": "1812.01888", "submitter": "Eirikur Agustsson", "authors": "Eirikur Agustsson, Jasper R. R. Uijlings, Vittorio Ferrari", "title": "Interactive Full Image Segmentation by Considering All Regions Jointly", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address interactive full image annotation, where the goal is to accurately\nsegment all object and stuff regions in an image. We propose an interactive,\nscribble-based annotation framework which operates on the whole image to\nproduce segmentations for all regions. This enables sharing scribble\ncorrections across regions, and allows the annotator to focus on the largest\nerrors made by the machine across the whole image. To realize this, we adapt\nMask-RCNN into a fast interactive segmentation framework and introduce an\ninstance-aware loss measured at the pixel-level in the full image canvas, which\nlets predictions for nearby regions properly compete for space. Finally, we\ncompare to interactive single object segmentation on the COCO panoptic dataset.\nWe demonstrate that our interactive full image segmentation approach leads to a\n5% IoU gain, reaching 90% IoU at a budget of four extreme clicks and four\ncorrective scribbles per region.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 10:09:15 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 15:51:32 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Agustsson", "Eirikur", ""], ["Uijlings", "Jasper R. R.", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1812.01894", "submitter": "Wei Shen", "authors": "Wei Shen and Rujie Liu", "title": "Learning to generate filters for convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventionally, convolutional neural networks (CNNs) process different images\nwith the same set of filters. However, the variations in images pose a\nchallenge to this fashion. In this paper, we propose to generate\nsample-specific filters for convolutional layers in the forward pass. Since the\nfilters are generated on-the-fly, the model becomes more flexible and can\nbetter fit the training data compared to traditional CNNs. In order to obtain\nsample-specific features, we extract the intermediate feature maps from an\nautoencoder. As filters are usually high dimensional, we propose to learn a set\nof coefficients instead of a set of filters. These coefficients are used to\nlinearly combine the base filters from a filter repository to generate the\nfinal filters for a CNN. The proposed method is evaluated on MNIST, MTFL and\nCIFAR10 datasets. Experiment results demonstrate that the classification\naccuracy of the baseline model can be improved by using the proposed filter\ngeneration method.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 10:16:38 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Shen", "Wei", ""], ["Liu", "Rujie", ""]]}, {"id": "1812.01922", "submitter": "Yan Zhang", "authors": "Yan Zhang, Siyu Tang, Krikamol Muandet, Christian Jarvers and Heiko\n  Neumann", "title": "Local Temporal Bilinear Pooling for Fine-grained Action Parsing", "comments": "11 pages, 2 figures. Cam.R", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained temporal action parsing is important in many applications, such\nas daily activity understanding, human motion analysis, surgical robotics and\nothers requiring subtle and precise operations in a long-term period. In this\npaper we propose a novel bilinear pooling operation, which is used in\nintermediate layers of a temporal convolutional encoder-decoder net. In\ncontrast to other work, our proposed bilinear pooling is learnable and hence\ncan capture more complex local statistics than the conventional counterpart. In\naddition, we introduce exact lower-dimension representations of our bilinear\nforms, so that the dimensionality is reduced with neither information loss nor\nextra computation. We perform intensive experiments to quantitatively analyze\nour model and show the superior performances to other state-of-the-art work on\nvarious datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 11:25:31 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 13:29:42 GMT"}, {"version": "v3", "created": "Sun, 26 May 2019 10:15:18 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Zhang", "Yan", ""], ["Tang", "Siyu", ""], ["Muandet", "Krikamol", ""], ["Jarvers", "Christian", ""], ["Neumann", "Heiko", ""]]}, {"id": "1812.01923", "submitter": "Yan Zhang", "authors": "Yan Zhang and Heiko Neumann", "title": "An Empirical Study towards Understanding How Deep Convolutional Nets\n  Recognize Falls", "comments": "published at the sixth International Workshop on Assistive Computer\n  Vision and Robotics (ACVR), in conjunction with European Conference on\n  Computer Vision (ECCV), Munich, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting unintended falls is essential for ambient intelligence and\nhealthcare of elderly people living alone. In recent years, deep convolutional\nnets are widely used in human action analysis, based on which a number of fall\ndetection methods have been proposed. Despite their highly effective\nperformances, the behaviors of how the convolutional nets recognize falls are\nstill not clear. In this paper, instead of proposing a novel approach, we\nperform a systematical empirical study, attempting to investigate the\nunderlying fall recognition process. We propose four tasks to investigate,\nwhich involve five types of input modalities, seven net instances and different\ntraining samples. The obtained quantitative and qualitative results reveal the\npatterns that the nets tend to learn, and several factors that can heavily\ninfluence the performances on fall recognition. We expect that our conclusions\nare favorable to proposing better deep learning solutions to fall detection\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 11:27:12 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Yan", ""], ["Neumann", "Heiko", ""]]}, {"id": "1812.01936", "submitter": "Jiankang Deng", "authors": "Jia Guo, Jiankang Deng, Niannan Xue, Stefanos Zafeiriou", "title": "Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark localisation in images captured in-the-wild is an important\nand challenging problem. The current state-of-the-art revolves around certain\nkinds of Deep Convolutional Neural Networks (DCNNs) such as stacked U-Nets and\nHourglass networks. In this work, we innovatively propose stacked dense U-Nets\nfor this task. We design a novel scale aggregation network topology structure\nand a channel aggregation building block to improve the model's capacity\nwithout sacrificing the computational complexity and model size. With the\nassistance of deformable convolutions inside the stacked dense U-Nets and\ncoherent loss for outside data transformation, our model obtains the ability to\nbe spatially invariant to arbitrary input face images. Extensive experiments on\nmany in-the-wild datasets, validate the robustness of the proposed method under\nextreme poses, exaggerated expressions and heavy occlusions. Finally, we show\nthat accurate 3D face alignment can assist pose-invariant face recognition\nwhere we achieve a new state-of-the-art accuracy on CFP-FP.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 12:02:11 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Guo", "Jia", ""], ["Deng", "Jiankang", ""], ["Xue", "Niannan", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1812.01946", "submitter": "Hoang-An Le", "authors": "Hoang-An Le, Tushar Nimbhorkar, Thomas Mensink, Anil S. Baslamisli,\n  Sezer Karaoglu, Theo Gevers", "title": "Automatic Generation of Dense Non-rigid Optical Flow", "comments": "The paper is under consideration at Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There hardly exists any large-scale datasets with dense optical flow of\nnon-rigid motion from real-world imagery as of today. The reason lies mainly in\nthe difficulty of human annotation to generate optical flow ground-truth. To\ncircumvent the need for human annotation, we propose a framework to\nautomatically generate optical flow from real-world videos. The method extracts\nand matches objects from video frames to compute initial constraints, and\napplies a deformation over the objects of interest to obtain dense optical flow\nfields. We propose several ways to augment the optical flow variations.\nExtensive experimental results show that training on our automatically\ngenerated optical flow outperforms methods that are trained on rigid synthetic\ndata using FlowNet-S, PWC-Net, and LiteFlowNet. Datasets and algorithms of our\noptical flow generation framework is available at\nhttps://github.com/lhoangan/arap_flow.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 12:10:06 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 09:57:00 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 13:05:08 GMT"}, {"version": "v4", "created": "Thu, 30 Jul 2020 12:08:49 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Le", "Hoang-An", ""], ["Nimbhorkar", "Tushar", ""], ["Mensink", "Thomas", ""], ["Baslamisli", "Anil S.", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""]]}, {"id": "1812.01965", "submitter": "Joseph Bethge", "authors": "Joseph Bethge, Marvin Bornstein, Adrian Loy, Haojin Yang, Christoph\n  Meinel", "title": "Training Competitive Binary Neural Networks from Scratch", "comments": "Our source code can be found at https://github.com/hpi-xnor/BMXNet-v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have achieved astonishing results in different\napplication areas. Various methods that allow us to use these models on mobile\nand embedded devices have been proposed. Especially binary neural networks are\na promising approach for devices with low computational power. However,\ntraining accurate binary models from scratch remains a challenge. Previous work\noften uses prior knowledge from full-precision models and complex training\nstrategies. In our work, we focus on increasing the performance of binary\nneural networks without such prior knowledge and a much simpler training\nstrategy. In our experiments we show that we are able to achieve\nstate-of-the-art results on standard benchmark datasets. Further, to the best\nof our knowledge, we are the first to successfully adopt a network architecture\nwith dense connections for binary networks, which lets us improve the\nstate-of-the-art even further.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 12:48:50 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Bethge", "Joseph", ""], ["Bornstein", "Marvin", ""], ["Loy", "Adrian", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1812.01969", "submitter": "Jiri Fajtl", "authors": "Jiri Fajtl, Hajar Sadeghi Sokeh, Vasileios Argyriou, Dorothy\n  Monekosso, Paolo Remagnino", "title": "Summarizing Videos with Attention", "comments": "Presented at ACCV2018 AIU2018 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel method for supervised, keyshots based video\nsummarization by applying a conceptually simple and computationally efficient\nsoft, self-attention mechanism. Current state of the art methods leverage\nbi-directional recurrent networks such as BiLSTM combined with attention. These\nnetworks are complex to implement and computationally demanding compared to\nfully connected networks. To that end we propose a simple, self-attention based\nnetwork for video summarization which performs the entire sequence to sequence\ntransformation in a single feed forward pass and single backward pass during\ntraining. Our method sets a new state of the art results on two benchmarks\nTvSum and SumMe, commonly used in this domain.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 13:00:04 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 09:52:11 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Fajtl", "Jiri", ""], ["Sokeh", "Hajar Sadeghi", ""], ["Argyriou", "Vasileios", ""], ["Monekosso", "Dorothy", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1812.01973", "submitter": "Martin Engilberge", "authors": "Romain Cohendet, Claire-H\\'el\\`ene Demarty, Ngoc Q. K. Duong, Martin\n  Engilberge", "title": "VideoMem: Constructing, Analyzing, Predicting Short-term and Long-term\n  Video Memorability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans share a strong tendency to memorize/forget some of the visual\ninformation they encounter. This paper focuses on providing computational\nmodels for the prediction of the intrinsic memorability of visual content. To\naddress this new challenge, we introduce a large scale dataset (VideoMem)\ncomposed of 10,000 videos annotated with memorability scores. In contrast to\nprevious work on image memorability -- where memorability was measured a few\nminutes after memorization -- memory performance is measured twice: a few\nminutes after memorization and again 24-72 hours later. Hence, the dataset\ncomes with short-term and long-term memorability annotations. After an in-depth\nanalysis of the dataset, we investigate several deep neural network based\nmodels for the prediction of video memorability. Our best model using a ranking\nloss achieves a Spearman's rank correlation of 0.494 for short-term\nmemorability prediction, while our proposed model with attention mechanism\nprovides insights of what makes a content memorable. The VideoMem dataset with\npre-extracted features is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 13:03:11 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Cohendet", "Romain", ""], ["Demarty", "Claire-H\u00e9l\u00e8ne", ""], ["Duong", "Ngoc Q. K.", ""], ["Engilberge", "Martin", ""]]}, {"id": "1812.02019", "submitter": "Menglin Wang", "authors": "Ken Chen, Fei Chen, Baisheng Lai, Zhongming Jin, Yong Liu, Kai Li,\n  Long Wei, Pengfei Wang, Yandong Tang, Jianqiang Huang, Xian-Sheng Hua", "title": "Dynamic Spatio-temporal Graph-based CNNs for Traffic Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting future traffic flows from previous ones is a challenging problem\nbecause of their complex and dynamic nature of spatio-temporal structures. Most\nexisting graph-based CNNs attempt to capture the static relations while largely\nneglecting the dynamics underlying sequential data. In this paper, we present\ndynamic spatio-temporal graph-based CNNs (DST-GCNNs) by learning expressive\nfeatures to represent spatio-temporal structures and predict future traffic\nflows from surveillance video data. In particular, DST-GCNN is a two stream\nnetwork. In the flow prediction stream, we present a novel graph-based\nspatio-temporal convolutional layer to extract features from a graph\nrepresentation of traffic flows. Then several such layers are stacked together\nto predict future flows over time. Meanwhile, the relations between traffic\nflows in the graph are often time variant as the traffic condition changes over\ntime. To capture the graph dynamics, we use the graph prediction stream to\npredict the dynamic graph structures, and the predicted structures are fed into\nthe flow prediction stream. Experiments on real datasets demonstrate that the\nproposed model achieves competitive performances compared with the other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 14:34:10 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 08:07:47 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 09:47:36 GMT"}, {"version": "v4", "created": "Thu, 5 Mar 2020 14:35:59 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Chen", "Ken", ""], ["Chen", "Fei", ""], ["Lai", "Baisheng", ""], ["Jin", "Zhongming", ""], ["Liu", "Yong", ""], ["Li", "Kai", ""], ["Wei", "Long", ""], ["Wang", "Pengfei", ""], ["Tang", "Yandong", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "1812.02041", "submitter": "Guido Borghi", "authors": "Stefano Pini, Guido Borghi, Roberto Vezzani", "title": "Learn to See by Events: Color Frame Synthesis from Event and RGB Cameras", "comments": "Accepted as full oral at the 15th International Conference on\n  Computer Vision Theory and Applications (VISAPP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are biologically-inspired sensors that gather the temporal\nevolution of the scene. They capture pixel-wise brightness variations and\noutput a corresponding stream of asynchronous events. Despite having multiple\nadvantages with respect to traditional cameras, their use is partially\nprevented by the limited applicability of traditional data processing and\nvision algorithms. To this aim, we present a framework which exploits the\noutput stream of event cameras to synthesize RGB frames, relying on an initial\nor a periodic set of color key-frames and the sequence of intermediate events.\nDifferently from existing work, we propose a deep learning-based frame\nsynthesis method, consisting of an adversarial architecture combined with a\nrecurrent module. Qualitative results and quantitative per-pixel, perceptual,\nand semantic evaluation on four public datasets confirm the quality of the\nsynthesized images.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 15:22:47 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 15:00:10 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Pini", "Stefano", ""], ["Borghi", "Guido", ""], ["Vezzani", "Roberto", ""]]}, {"id": "1812.02050", "submitter": "Shile Li", "authors": "Shile Li and Dongheui Lee", "title": "Point-to-Pose Voting based Hand Pose Estimation using Residual\n  Permutation Equivariant Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, 3D input data based hand pose estimation methods have shown\nstate-of-the-art performance, because 3D data capture more spatial information\nthan the depth image. Whereas 3D voxel-based methods need a large amount of\nmemory, PointNet based methods need tedious preprocessing steps such as\nK-nearest neighbour search for each point. In this paper, we present a novel\ndeep learning hand pose estimation method for an unordered point cloud. Our\nmethod takes 1024 3D points as input and does not require additional\ninformation. We use Permutation Equivariant Layer (PEL) as the basic element,\nwhere a residual network version of PEL is proposed for the hand pose\nestimation task. Furthermore, we propose a voting based scheme to merge\ninformation from individual points to the final pose output. In addition to the\npose estimation task, the voting-based scheme can also provide point cloud\nsegmentation result without ground-truth for segmentation. We evaluate our\nmethod on both NYU dataset and the Hands2017Challenge dataset. Our method\noutperforms recent state-of-the-art methods, where our pose accuracy is\ncurrently the best for the Hands2017Challenge dataset.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 15:31:26 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Li", "Shile", ""], ["Lee", "Dongheui", ""]]}, {"id": "1812.02068", "submitter": "Qiaoying Huang", "authors": "Qiaoying Huang and Xiao Chen and Dimitris Metaxas and Mariappan S.\n  Nadar", "title": "Brain Segmentation from k-space with End-to-end Recurrent Attention\n  Network", "comments": "Accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of medical image segmentation commonly involves an image\nreconstruction step to convert acquired raw data to images before any analysis.\nHowever, noises, artifacts and loss of information due to the reconstruction\nprocess are almost inevitable, which compromises the final performance of\nsegmentation. We present a novel learning framework that performs magnetic\nresonance brain image segmentation directly from k-space data. The end-to-end\nframework consists of a unique task-driven attention module that recurrently\nutilizes intermediate segmentation estimation to facilitate image-domain\nfeature extraction from the raw data, thus closely bridging the reconstruction\nand the segmentation tasks. In addition, to address the challenge of manual\nlabeling, we introduce a novel workflow to generate labeled training data for\nsegmentation by exploiting imaging modality simulators and digital phantoms.\nExtensive experimental results show that the proposed method outperforms\nseveral state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 16:01:28 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 18:56:39 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Huang", "Qiaoying", ""], ["Chen", "Xiao", ""], ["Metaxas", "Dimitris", ""], ["Nadar", "Mariappan S.", ""]]}, {"id": "1812.02100", "submitter": "Jindong Gu", "authors": "Jindong Gu and Yinchong Yang and Volker Tresp", "title": "Understanding Individual Decisions of CNNs via Contrastive\n  Backpropagation", "comments": "16 pages, 5 figures, ACCV", "journal-ref": "the 14th Asian Conference on Computer Vision (ACCV) 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of backpropagation-based approaches such as DeConvNets, vanilla\nGradient Visualization and Guided Backpropagation have been proposed to better\nunderstand individual decisions of deep convolutional neural networks. The\nsaliency maps produced by them are proven to be non-discriminative. Recently,\nthe Layer-wise Relevance Propagation (LRP) approach was proposed to explain the\nclassification decisions of rectifier neural networks. In this work, we\nevaluate the discriminativeness of the generated explanations and analyze the\ntheoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments\nand analysis conclude that the explanations generated by LRP are not\nclass-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance\nPropagation (CLRP), which is capable of producing instance-specific,\nclass-discriminative, pixel-wise explanations. In the experiments, we use the\nCLRP to explain the decisions and understand the difference between neurons in\nindividual classification decisions. We also evaluate the explanations\nquantitatively with a Pointing Game and an ablation study. Both qualitative and\nquantitative evaluations show that the CLRP generates better explanations than\nthe LRP. The code is available.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 16:43:04 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 00:16:47 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Gu", "Jindong", ""], ["Yang", "Yinchong", ""], ["Tresp", "Volker", ""]]}, {"id": "1812.02122", "submitter": "Nan Xue", "authors": "Nan Xue and Song Bai and Fudong Wang and Gui-Song Xia and Tianfu Wu\n  and Liangpei Zhang", "title": "Learning Attraction Field Representation for Robust Line Segment\n  Detection", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a region-partition based attraction field dual\nrepresentation for line segment maps, and thus poses the problem of line\nsegment detection (LSD) as the region coloring problem. The latter is then\naddressed by learning deep convolutional neural networks (ConvNets) for\naccuracy, robustness and efficiency. For a 2D line segment map, our dual\nrepresentation consists of three components: (i) A region-partition map in\nwhich every pixel is assigned to one and only one line segment; (ii) An\nattraction field map in which every pixel in a partition region is encoded by\nits 2D projection vector w.r.t. the associated line segment; and (iii) A\nsqueeze module which squashes the attraction field to a line segment map that\nalmost perfectly recovers the input one. By leveraging the duality, we learn\nConvNets to compute the attraction field maps for raw in-put images, followed\nby the squeeze module for LSD, in an end-to-end manner. Our method rigorously\naddresses several challenges in LSD such as local ambiguity and class\nimbalance. Our method also harnesses the best practices developed in ConvNets\nbased semantic segmentation methods such as the encoder-decoder architecture\nand the a-trous convolution. In experiments, our method is tested on the\nWireFrame dataset and the YorkUrban dataset with state-of-the-art performance\nobtained. Especially, we advance the performance by 4.5 percents on the\nWireFrame dataset. Our method is also fast with 6.6~10.4 FPS, outperforming\nmost of existing line segment detectors.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:24:02 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 23:36:21 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Xue", "Nan", ""], ["Bai", "Song", ""], ["Wang", "Fudong", ""], ["Xia", "Gui-Song", ""], ["Wu", "Tianfu", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1812.02132", "submitter": "Matthias M\\\"uller", "authors": "Abdullah Hamdi, Matthias M\\\"uller, Bernard Ghanem", "title": "SADA: Semantic Adversarial Diagnostic Attacks for Autonomous\n  Applications", "comments": "Accepted at AAAI'20", "journal-ref": "AAAI 2020", "doi": "10.1609/aaai.v34i07.6722", "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major factor impeding more widespread adoption of deep neural networks\n(DNNs) is their lack of robustness, which is essential for safety-critical\napplications such as autonomous driving. This has motivated much recent work on\nadversarial attacks for DNNs, which mostly focus on pixel-level perturbations\nvoid of semantic meaning. In contrast, we present a general framework for\nadversarial attacks on trained agents, which covers semantic perturbations to\nthe environment of the agent performing the task as well as pixel-level\nattacks. To do this, we re-frame the adversarial attack problem as learning a\ndistribution of parameters that always fools the agent. In the semantic case,\nour proposed adversary (denoted as BBGAN) is trained to sample parameters that\ndescribe the environment with which the black-box agent interacts, such that\nthe agent performs its dedicated task poorly in this environment. We apply\nBBGAN on three different tasks, primarily targeting aspects of autonomous\nnavigation: object detection, self-driving, and autonomous UAV racing. On these\ntasks, BBGAN can generate failure cases that consistently fool a trained agent.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:42:36 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 21:28:55 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 16:20:42 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hamdi", "Abdullah", ""], ["M\u00fcller", "Matthias", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1812.02134", "submitter": "Jose Oramas", "authors": "Kaili Wang, Liqian Ma, Jose Oramas, Luc Van Gool, Tinne Tuytelaars", "title": "Unsupervised shape transformer for image translation and cross-domain\n  retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of unsupervised geometric image-to-image translation.\nRather than transferring the style of an image as a whole, our goal is to\ntranslate the geometry of an object as depicted in different domains while\npreserving its appearance characteristics. Our model is trained in an\nunsupervised fashion, i.e. without the need of paired images during training.\nIt performs all steps of the shape transfer within a single model and without\nadditional post-processing stages. Extensive experiments on the VITON,\nCMU-Multi-PIE and our own FashionStyle datasets show the effectiveness of the\nmethod. In addition, we show that despite their low-dimensionality, the\nfeatures learned by our model are useful to the item retrieval task.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:47:07 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 21:05:04 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Wang", "Kaili", ""], ["Ma", "Liqian", ""], ["Oramas", "Jose", ""], ["Van Gool", "Luc", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1812.02162", "submitter": "Xiaoxiao Sun", "authors": "Xiaoxiao Sun and Liang Zheng", "title": "Dissecting Person Re-identification from the Viewpoint of Viewpoint", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variations in visual factors such as viewpoint, pose, illumination and\nbackground, are usually viewed as important challenges in person\nre-identification (re-ID). In spite of acknowledging these factors to be\ninfluential, quantitative studies on how they affect a re-ID system are still\nlacking. To derive insights in this scientific campaign, this paper makes an\nearly attempt in studying a particular factor, viewpoint. We narrow the\nviewpoint problem down to the pedestrian rotation angle to obtain focused\nconclusions. In this regard, this paper makes two contributions to the\ncommunity. First, we introduce a large-scale synthetic data engine, PersonX.\nComposed of hand-crafted 3D person models, the salient characteristic of this\nengine is \"controllable\". That is, we are able to synthesize pedestrians by\nsetting the visual variables to arbitrary values. Second, on the 3D data\nengine, we quantitatively analyze the influence of pedestrian rotation angle on\nre-ID accuracy. Comprehensively, the person rotation angles are precisely\ncustomized from 0 to 360, allowing us to investigate its effect on the\ntraining, query, and gallery sets. Extensive experiment helps us have a deeper\nunderstanding of the fundamental problems in person re-ID. Our research also\nprovides useful insights for dataset building and future practical usage, e.g.,\na person of a side view makes a better query.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 18:56:10 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 06:15:46 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 09:14:50 GMT"}, {"version": "v4", "created": "Sat, 6 Apr 2019 16:25:53 GMT"}, {"version": "v5", "created": "Wed, 10 Apr 2019 16:17:21 GMT"}, {"version": "v6", "created": "Mon, 17 Jun 2019 14:29:25 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Sun", "Xiaoxiao", ""], ["Zheng", "Liang", ""]]}, {"id": "1812.02246", "submitter": "Chung-Yi Weng", "authors": "Chung-Yi Weng, Brian Curless, Ira Kemelmacher-Shlizerman", "title": "Photo Wake-Up: 3D Character Animation from a Single Photo", "comments": "The project page is at\n  https://grail.cs.washington.edu/projects/wakeup/, and the supplementary video\n  is at https://youtu.be/G63goXc5MyU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method and application for animating a human subject from a\nsingle photo. E.g., the character can walk out, run, sit, or jump in 3D. The\nkey contributions of this paper are: 1) an application of viewing and animating\nhumans in single photos in 3D, 2) a novel 2D warping method to deform a posable\ntemplate body model to fit the person's complex silhouette to create an\nanimatable mesh, and 3) a method for handling partial self occlusions. We\ncompare to state-of-the-art related methods and evaluate results with human\nstudies. Further, we present an interactive interface that allows re-posing the\nperson in 3D, and an augmented reality setup where the animated 3D person can\nemerge from the photo into the real world. We demonstrate the method on photos,\nposters, and art.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 22:09:52 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Weng", "Chung-Yi", ""], ["Curless", "Brian", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "1812.02302", "submitter": "Steven Damelin Dr", "authors": "Steven B. Damelin, David L. Ragozin and Michael Werman", "title": "On Min-Max affine approximants of convex or concave real valued\n  functions from $\\mathbb R^k$, Chebyshev equioscillation and graphics", "comments": "We replace \"best uniform\" by \"Min-Max\" when needed. The former term\n  was used in a previous version", "journal-ref": "Excursions in Harmonic Analysis, Vol 6, \"In Honor of John\n  Benedetto's 80th Birthday\". 2020", "doi": null, "report-no": null, "categories": "math.OC cs.CV math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Min-Max affine approximants of a continuous convex or concave\nfunction $f:\\Delta\\subset \\mathbb R^k\\xrightarrow{} \\mathbb R$ where $\\Delta$\nis a convex compact subset of $\\mathbb R^k$. In the case when $\\Delta$ is a\nsimplex we prove that there is a vertical translate of the supporting\nhyperplane in $\\mathbb R^{k+1}$ of the graph of $f$ at the vertices which is\nthe unique best affine approximant to $f$ on $\\Delta$. For $k=1$, this result\nprovides an extension of the Chebyshev equioscillation theorem for linear\napproximants. Our result has interesting connections to the computer graphics\nproblem of rapid rendering of projective transformations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 12:17:25 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 20:07:42 GMT"}, {"version": "v3", "created": "Sat, 4 May 2019 15:18:12 GMT"}, {"version": "v4", "created": "Mon, 13 May 2019 13:31:36 GMT"}, {"version": "v5", "created": "Sun, 1 Sep 2019 16:43:55 GMT"}, {"version": "v6", "created": "Sat, 7 Mar 2020 17:29:25 GMT"}, {"version": "v7", "created": "Mon, 15 Jun 2020 13:56:50 GMT"}, {"version": "v8", "created": "Tue, 23 Jun 2020 15:14:51 GMT"}, {"version": "v9", "created": "Tue, 16 Feb 2021 16:30:30 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Damelin", "Steven B.", ""], ["Ragozin", "David L.", ""], ["Werman", "Michael", ""]]}, {"id": "1812.02316", "submitter": "Danilo Mendes", "authors": "Danilo Barros Mendes, Nilton Correia da Silva", "title": "Skin Lesions Classification Using Convolutional Neural Networks in\n  Clinical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin lesions are conditions that appear on a patient due to many different\nreasons. One of these can be because of an abnormal growth in skin tissue,\ndefined as cancer. This disease plagues more than 14.1 million patients and had\nbeen the cause of more than 8.2 million deaths, worldwide. Therefore, the\nconstruction of a classification model for 12 lesions, including Malignant\nMelanoma and Basal Cell Carcinoma, is proposed. Furthermore, in this work, it\nis used a ResNet-152 architecture, which was trained over 3,797 images, later\naugmented by a factor of 29 times, using positional, scale, and lighting\ntransformations. Finally, the network was tested with 956 images and achieve an\narea under the curve (AUC) of 0.96 for Melanoma and 0.91 for Basal Cell\nCarcinoma.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 02:54:28 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Mendes", "Danilo Barros", ""], ["da Silva", "Nilton Correia", ""]]}, {"id": "1812.02342", "submitter": "Kwang Hee Lee", "authors": "Dae Young Park and Kwang Hee Lee", "title": "Arbitrary Style Transfer with Style-Attentional Networks", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary style transfer aims to synthesize a content image with the style of\nan image to create a third image that has never been seen before. Recent\narbitrary style transfer algorithms find it challenging to balance the content\nstructure and the style patterns. Moreover, simultaneously maintaining the\nglobal and local style patterns is difficult due to the patch-based mechanism.\nIn this paper, we introduce a novel style-attentional network (SANet) that\nefficiently and flexibly integrates the local style patterns according to the\nsemantic spatial distribution of the content image. A new identity loss\nfunction and multi-level feature embeddings enable our SANet and decoder to\npreserve the content structure as much as possible while enriching the style\npatterns. Experimental results demonstrate that our algorithm synthesizes\nstylized images in real-time that are higher in quality than those produced by\nthe state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 04:31:54 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 15:06:13 GMT"}, {"version": "v3", "created": "Thu, 3 Jan 2019 06:25:05 GMT"}, {"version": "v4", "created": "Tue, 9 Apr 2019 17:27:21 GMT"}, {"version": "v5", "created": "Thu, 23 May 2019 10:57:50 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Park", "Dae Young", ""], ["Lee", "Kwang Hee", ""]]}, {"id": "1812.02347", "submitter": "Long Chen", "authors": "Long Chen, Hanwang Zhang, Jun Xiao, Xiangnan He, Shiliang Pu, Shih-Fu\n  Chang", "title": "Counterfactual Critic Multi-Agent Training for Scene Graph Generation", "comments": "International Conference on Computer Vision (ICCV), 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graphs -- objects as nodes and visual relationships as edges --\ndescribe the whereabouts and interactions of the things and stuff in an image\nfor comprehensive scene understanding. To generate coherent scene graphs,\nalmost all existing methods exploit the fruitful visual context by modeling\nmessage passing among objects, fitting the dynamic nature of reasoning with\nvisual context, eg, \"person\" on \"bike\" can help to determine the relationship\n\"ride\", which in turn contributes to the category confidence of the two\nobjects. However, we argue that the scene dynamics is not properly learned by\nusing the prevailing cross-entropy based supervised learning paradigm, which is\nnot sensitive to graph inconsistency: errors at the hub or non-hub nodes are\nunfortunately penalized equally. To this end, we propose a Counterfactual\ncritic Multi-Agent Training (CMAT) approach to resolve the mismatch. CMAT is a\nmulti-agent policy gradient method that frames objects as cooperative agents,\nand then directly maximizes a graph-level metric as the reward. In particular,\nto assign the reward properly to each agent, CMAT uses a counterfactual\nbaseline that disentangles the agent-specific reward by fixing the dynamics of\nother agents. Extensive validations on the challenging Visual Genome benchmark\nshow that CMAT achieves a state-of-the-art by significant performance gains\nunder various settings and metrics.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 04:54:14 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 05:28:25 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 13:08:58 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Chen", "Long", ""], ["Zhang", "Hanwang", ""], ["Xiao", "Jun", ""], ["He", "Xiangnan", ""], ["Pu", "Shiliang", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1812.02350", "submitter": "Donghoon Lee", "authors": "Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, Ming-Hsuan Yang, Jan\n  Kautz", "title": "Context-Aware Synthesis and Placement of Object Instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to insert an object instance into an image in a semantically\ncoherent manner is a challenging and interesting problem. Solving it requires\n(a) determining a location to place an object in the scene and (b) determining\nits appearance at the location. Such an object insertion model can potentially\nfacilitate numerous image editing and scene parsing applications. In this\npaper, we propose an end-to-end trainable neural network for the task of\ninserting an object instance mask of a specified class into the semantic label\nmap of an image. Our network consists of two generative modules where one\ndetermines where the inserted object mask should be (i.e., location and scale)\nand the other determines what the object mask shape (and pose) should look\nlike. The two modules are connected together via a spatial transformation\nnetwork and jointly trained. We devise a learning procedure that leverage both\nsupervised and unsupervised data and show our model can insert an object at\ndiverse locations with various appearances. We conduct extensive experimental\nvalidations with comparisons to strong baselines to verify the effectiveness of\nthe proposed network.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 05:04:35 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 16:46:05 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Lee", "Donghoon", ""], ["Liu", "Sifei", ""], ["Gu", "Jinwei", ""], ["Liu", "Ming-Yu", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1812.02375", "submitter": "Shuai Zhang", "authors": "Yuhui Xu, Shuai Zhang, Yingyong Qi, Jiaxian Guo, Weiyao Lin and\n  Hongkai Xiong", "title": "DNQ: Dynamic Network Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network quantization is an effective method for the deployment of neural\nnetworks on memory and energy constrained mobile devices. In this paper, we\npropose a Dynamic Network Quantization (DNQ) framework which is composed of two\nmodules: a bit-width controller and a quantizer. Unlike most existing\nquantization methods that use a universal quantization bit-width for the whole\nnetwork, we utilize policy gradient to train an agent to learn the bit-width of\neach layer by the bit-width controller. This controller can make a trade-off\nbetween accuracy and compression ratio. Given the quantization bit-width\nsequence, the quantizer adopts the quantization distance as the criterion of\nthe weights importance during quantization. We extensively validate the\nproposed approach on various main-stream neural networks and obtain impressive\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 07:06:17 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Xu", "Yuhui", ""], ["Zhang", "Shuai", ""], ["Qi", "Yingyong", ""], ["Guo", "Jiaxian", ""], ["Lin", "Weiyao", ""], ["Xiong", "Hongkai", ""]]}, {"id": "1812.02378", "submitter": "Xu Yang", "authors": "Xu Yang, Kaihua Tang, Hanwang Zhang, Jianfei Cai", "title": "Auto-Encoding Scene Graphs for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Scene Graph Auto-Encoder (SGAE) that incorporates the language\ninductive bias into the encoder-decoder image captioning framework for more\nhuman-like captions. Intuitively, we humans use the inductive bias to compose\ncollocations and contextual inference in discourse. For example, when we see\nthe relation `person on bike', it is natural to replace `on' with `ride' and\ninfer `person riding bike on a road' even the `road' is not evident. Therefore,\nexploiting such bias as a language prior is expected to help the conventional\nencoder-decoder models less likely overfit to the dataset bias and focus on\nreasoning. Specifically, we use the scene graph --- a directed graph\n($\\mathcal{G}$) where an object node is connected by adjective nodes and\nrelationship nodes --- to represent the complex structural layout of both image\n($\\mathcal{I}$) and sentence ($\\mathcal{S}$). In the textual domain, we use\nSGAE to learn a dictionary ($\\mathcal{D}$) that helps to reconstruct sentences\nin the $\\mathcal{S}\\rightarrow \\mathcal{G} \\rightarrow \\mathcal{D} \\rightarrow\n\\mathcal{S}$ pipeline, where $\\mathcal{D}$ encodes the desired language prior;\nin the vision-language domain, we use the shared $\\mathcal{D}$ to guide the\nencoder-decoder in the $\\mathcal{I}\\rightarrow \\mathcal{G}\\rightarrow\n\\mathcal{D} \\rightarrow \\mathcal{S}$ pipeline. Thanks to the scene graph\nrepresentation and shared dictionary, the inductive bias is transferred across\ndomains in principle. We validate the effectiveness of SGAE on the challenging\nMS-COCO image captioning benchmark, e.g., our SGAE-based single-model achieves\na new state-of-the-art $127.8$ CIDEr-D on the Karpathy split, and a competitive\n$125.5$ CIDEr-D (c40) on the official server even compared to other ensemble\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 07:13:33 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 05:09:49 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2018 01:32:43 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Yang", "Xu", ""], ["Tang", "Kaihua", ""], ["Zhang", "Hanwang", ""], ["Cai", "Jianfei", ""]]}, {"id": "1812.02391", "submitter": "Qianru Sun", "authors": "Qianru Sun, Yaoyao Liu, Tat-Seng Chua, Bernt Schiele", "title": "Meta-Transfer Learning for Few-Shot Learning", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Meta-learning has been proposed as a framework to address the challenging\nfew-shot learning setting. The key idea is to leverage a large number of\nsimilar few-shot tasks in order to learn how to adapt a base-learner to a new\ntask for which only a few labeled samples are available. As deep neural\nnetworks (DNNs) tend to overfit using a few samples only, meta-learning\ntypically uses shallow neural networks (SNNs), thus limiting its effectiveness.\nIn this paper we propose a novel few-shot learning method called meta-transfer\nlearning (MTL) which learns to adapt a deep NN for few shot learning tasks.\nSpecifically, \"meta\" refers to training multiple tasks, and \"transfer\" is\nachieved by learning scaling and shifting functions of DNN weights for each\ntask. In addition, we introduce the hard task (HT) meta-batch scheme as an\neffective learning curriculum for MTL. We conduct experiments using (5-class,\n1-shot) and (5-class, 5-shot) recognition tasks on two challenging few-shot\nlearning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisons\nto related works validate that our meta-transfer learning approach trained with\nthe proposed HT meta-batch scheme achieves top performance. An ablation study\nalso shows that both components contribute to fast convergence and high\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 07:45:08 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 07:52:50 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 13:39:11 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Sun", "Qianru", ""], ["Liu", "Yaoyao", ""], ["Chua", "Tat-Seng", ""], ["Schiele", "Bernt", ""]]}, {"id": "1812.02393", "submitter": "Yingbin Zheng", "authors": "Xingjiao Wu and Yingbin Zheng and Hao Ye and Wenxin Hu and Jing Yang\n  and Liang He", "title": "Adaptive Scenario Discovery for Crowd Counting", "comments": null, "journal-ref": "IEEE ICASSP 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting, i.e., estimation number of the pedestrian in crowd images, is\nemerging as an important research problem with the public security\napplications. A key component for the crowd counting systems is the\nconstruction of counting models which are robust to various scenarios under\nfacts such as camera perspective and physical barriers. In this paper, we\npresent an adaptive scenario discovery framework for crowd counting. The system\nis structured with two parallel pathways that are trained with different sizes\nof the receptive field to represent different scales and crowd densities. After\nensuring that these components are present in the proper geometric\nconfiguration, a third branch is designed to adaptively recalibrate the\npathway-wise responses by discovering and modeling the dynamic scenarios\nimplicitly. Our system is able to represent highly variable crowd images and\nachieves state-of-the-art results in two challenging benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 07:51:06 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 12:01:16 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Wu", "Xingjiao", ""], ["Zheng", "Yingbin", ""], ["Ye", "Hao", ""], ["Hu", "Wenxin", ""], ["Yang", "Jing", ""], ["He", "Liang", ""]]}, {"id": "1812.02402", "submitter": "Yuhui Xu", "authors": "Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi,\n  Yiran Chen, Weiyao Lin, Hongkai Xiong", "title": "Trained Rank Pruning for Efficient Deep Neural Networks", "comments": "Accepted by NIPS2019 EMC2 workshop, the same version as the withdrawn\n  arXiv:1910.04576", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of Deep Neural Networks (DNNs) keeps elevating in recent\nyears with increasing network depth and width. To enable DNNs on edge devices\nlike mobile phones, researchers proposed several network compression methods\nincluding pruning, quantization and factorization. Among the\nfactorization-based approaches, low-rank approximation has been widely adopted\nbecause of its solid theoretical rationale and efficient implementations.\nSeveral previous works attempted to directly approximate a pre-trained model by\nlow-rank decomposition; however, small approximation errors in parameters can\nripple a large prediction loss. As a result, performance usually drops\nsignificantly and a sophisticated fine-tuning is required to recover accuracy.\nWe argue that it is not optimal to separate low-rank approximation from\ntraining. Unlike previous works, this paper integrates low rank approximation\nand regularization into the training. We propose Trained Rank Pruning (TRP),\nwhich iterates low rank approximation and training. TRP maintains the capacity\nof original network while imposes low-rank constraints during training. A\nstochastic sub-gradient descent optimized nuclear regularization is utilized to\nfurther encourage low rank in TRP. The TRP trained network has low-rank\nstructure in nature, and can be approximated with negligible performance loss,\neliminating fine-tuning after low rank approximation. The methods are\ncomprehensively evaluated on CIFAR-10 and ImageNet, outperforming previous\ncompression methods using low rank approximation. Code is available:\nhttps://github.com/yuhuixu1993/Trained-Rank-Pruning\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 08:37:54 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 07:12:59 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 21:01:43 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Xu", "Yuhui", ""], ["Li", "Yuxi", ""], ["Zhang", "Shuai", ""], ["Wen", "Wei", ""], ["Wang", "Botao", ""], ["Qi", "Yingyong", ""], ["Chen", "Yiran", ""], ["Lin", "Weiyao", ""], ["Xiong", "Hongkai", ""]]}, {"id": "1812.02405", "submitter": "Mijung Kim", "authors": "Mijung Kim, Olivier Janssens, Ho-min Park, Jasper Zuallaert, Sofie Van\n  Hoecke, and Wesley De Neve", "title": "Web Applicable Computer-aided Diagnosis of Glaucoma Using Deep Learning", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:cs/0101200", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/191", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is a major eye disease, leading to vision loss in the absence of\nproper medical treatment. Current diagnosis of glaucoma is performed by\nophthalmologists who are often analyzing several types of medical images\ngenerated by different types of medical equipment. Capturing and analyzing\nthese medical images is labor-intensive and expensive. In this paper, we\npresent a novel computational approach towards glaucoma diagnosis and\nlocalization, only making use of eye fundus images that are analyzed by\nstate-of-the-art deep learning techniques. Specifically, our approach leverages\nConvolutional Neural Networks (CNNs) and Gradient-weighted Class Activation\nMapping (Grad-CAM) for glaucoma diagnosis and localization, respectively.\nQuantitative and qualitative results, as obtained for a small-sized dataset\nwith no segmentation ground truth, demonstrate that the proposed approach is\npromising, for instance achieving an accuracy of 0.91$\\pm0.02$ and an ROC-AUC\nscore of 0.94 for the diagnosis task. Furthermore, we present a publicly\navailable prototype web application that integrates our predictive model, with\nthe goal of making effective glaucoma diagnosis available to a wide audience.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 08:55:53 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 01:56:26 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Kim", "Mijung", ""], ["Janssens", "Olivier", ""], ["Park", "Ho-min", ""], ["Zuallaert", "Jasper", ""], ["Van Hoecke", "Sofie", ""], ["De Neve", "Wesley", ""]]}, {"id": "1812.02415", "submitter": "Oshri Halimi", "authors": "Oshri Halimi, Or Litany, Emanuele Rodol\\`a, Alex Bronstein, Ron Kimmel", "title": "Self-supervised Learning of Dense Shape Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first completely unsupervised correspondence learning\napproach for deformable 3D shapes. Key to our model is the understanding that\nnatural deformations (such as changes in pose) approximately preserve the\nmetric structure of the surface, yielding a natural criterion to drive the\nlearning process toward distortion-minimizing predictions. On this basis, we\novercome the need for annotated data and replace it by a purely geometric\ncriterion. The resulting learning model is class-agnostic, and is able to\nleverage any type of deformable geometric data for the training phase. In\ncontrast to existing supervised approaches which specialize on the class seen\nat training time, we demonstrate stronger generalization as well as\napplicability to a variety of challenging settings. We showcase our method on a\nwide selection of correspondence benchmarks, where we outperform other methods\nin terms of accuracy, generalization, and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 09:26:03 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Halimi", "Oshri", ""], ["Litany", "Or", ""], ["Rodol\u00e0", "Emanuele", ""], ["Bronstein", "Alex", ""], ["Kimmel", "Ron", ""]]}, {"id": "1812.02425", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Zhankui He and Xiangyang Xue", "title": "MEAL: Multi-Model Ensemble via Adversarial Learning", "comments": "To appear in AAAI 2019. Code and models are available at:\n  https://github.com/AaronHeee/MEAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often the best performing deep neural models are ensembles of multiple\nbase-level networks. Unfortunately, the space required to store these many\nnetworks, and the time required to execute them at test-time, prohibits their\nuse in applications where test sets are large (e.g., ImageNet). In this paper,\nwe present a method for compressing large, complex trained ensembles into a\nsingle network, where knowledge from a variety of trained deep neural networks\n(DNNs) is distilled and transferred to a single DNN. In order to distill\ndiverse knowledge from different trained (teacher) models, we propose to use\nadversarial-based learning strategy where we define a block-wise training loss\nto guide and optimize the predefined student network to recover the knowledge\nin teacher models, and to promote the discriminator network to distinguish\nteacher vs. student features simultaneously. The proposed ensemble method\n(MEAL) of transferring distilled knowledge with adversarial learning exhibits\nthree important advantages: (1) the student network that learns the distilled\nknowledge with discriminators is optimized better than the original model; (2)\nfast inference is realized by a single forward pass, while the performance is\neven better than traditional ensembles from multi-original models; (3) the\nstudent network can learn the distilled knowledge from a teacher model that has\narbitrary structures. Extensive experiments on CIFAR-10/100, SVHN and ImageNet\ndatasets demonstrate the effectiveness of our MEAL method. On ImageNet, our\nResNet-50 based MEAL achieves top-1/5 21.79%/5.99% val error, which outperforms\nthe original model by 2.06%/1.14%. Code and models are available at:\nhttps://github.com/AaronHeee/MEAL\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 09:48:49 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 06:32:03 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Shen", "Zhiqiang", ""], ["He", "Zhankui", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1812.02427", "submitter": "Old\\v{r}ich Kodym", "authors": "Old\\v{r}ich Kodym, Michal \\v{S}pan\\v{e}l, Adam Herout", "title": "Segmentation of Head and Neck Organs at Risk Using CNN with Batch Dice\n  Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with segmentation of organs at risk (OAR) in head and neck\narea in CT images which is a crucial step for reliable intensity modulated\nradiotherapy treatment. We introduce a convolution neural network with\nencoder-decoder architecture and a new loss function, the batch soft Dice loss\nfunction, used to train the network. The resulting model produces segmentations\nof every OAR in the public MICCAI 2015 Head And Neck Auto-Segmentation\nChallenge dataset. Despite the heavy class imbalance in the data, we improve\naccuracy of current state-of-the-art methods by 0.33 mm in terms of average\nsurface distance and by 0.11 in terms of Dice overlap coefficient on average.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 09:50:49 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Kodym", "Old\u0159ich", ""], ["\u0160pan\u011bl", "Michal", ""], ["Herout", "Adam", ""]]}, {"id": "1812.02464", "submitter": "Craig Atkinson", "authors": "Craig Atkinson and Brendan McCane and Lech Szymanski and Anthony\n  Robins", "title": "Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without\n  Catastrophic Forgetting", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.11.050", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Neural networks can achieve excellent results in a wide variety of\napplications. However, when they attempt to sequentially learn, they tend to\nlearn the new task while catastrophically forgetting previous ones. We propose\na model that overcomes catastrophic forgetting in sequential reinforcement\nlearning by combining ideas from continual learning in both the image\nclassification domain and the reinforcement learning domain. This model\nfeatures a dual memory system which separates continual learning from\nreinforcement learning and a pseudo-rehearsal system that \"recalls\" items\nrepresentative of previous tasks via a deep generative network. Our model\nsequentially learns Atari 2600 games without demonstrating catastrophic\nforgetting and continues to perform above human level on all three games. This\nresult is achieved without: demanding additional storage requirements as the\nnumber of tasks increases, storing raw data or revisiting past tasks. In\ncomparison, previous state-of-the-art solutions are substantially more\nvulnerable to forgetting on these complex deep reinforcement learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 11:20:18 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 22:31:47 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 02:16:29 GMT"}, {"version": "v4", "created": "Mon, 7 Oct 2019 01:42:20 GMT"}, {"version": "v5", "created": "Mon, 3 Aug 2020 05:52:01 GMT"}, {"version": "v6", "created": "Wed, 16 Dec 2020 21:38:56 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Atkinson", "Craig", ""], ["McCane", "Brendan", ""], ["Szymanski", "Lech", ""], ["Robins", "Anthony", ""]]}, {"id": "1812.02465", "submitter": "Evgeny Izutov", "authors": "Evgeny Izutov", "title": "Fast and Accurate Person Re-Identification with RMNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new neural network architecture designed to use\nin embedded vision applications. It merges the best working practices of\nnetwork architectures like MobileNets and ResNets to our named RMNet\narchitecture. We also focus on key moments of building mobile architectures to\ncarry out in the limited computation budget. Additionally, to demonstrate the\neffectiveness of our architecture we evaluate the RMNet backbone on Person\nRe-identification task. The proposed approach is in top 3 of state of the art\nsolutions on Market-1501 challenge, however our method significantly\noutperforms them by the inference speed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 11:20:37 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Izutov", "Evgeny", ""]]}, {"id": "1812.02466", "submitter": "Anand Mishra Mr.", "authors": "Anand Mishra and Ajeet Kumar Singh", "title": "Deep Embedding using Bayesian Risk Minimization with Application to\n  Sketch Recognition", "comments": "Accepted at ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of hand-drawn sketch recognition.\nInspired by the Bayesian decision theory, we present a deep metric learning\nloss with the objective to minimize the Bayesian risk of misclassification. We\nestimate this risk for every mini-batch during training, and learn robust deep\nembeddings by backpropagating it to a deep neural network in an end-to-end\ntrainable paradigm. Our learnt embeddings are discriminative and robust despite\nof intra-class variations and inter-class similarities naturally present in\nhand-drawn sketch images. Outperforming the state of the art on sketch\nrecognition, our method achieves 82.2% and 88.7% on TU-Berlin-250 and\nTU-Berlin-160 benchmarks respectively.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 11:20:58 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Mishra", "Anand", ""], ["Singh", "Ajeet Kumar", ""]]}, {"id": "1812.02475", "submitter": "Ram Krishna Pandey", "authors": "Ram Krishna Pandey, K Vignesh, A G Ramakrishnan and Chandrahasa B", "title": "Binary Document Image Super Resolution for Improved Readability and OCR\n  Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a need for information retrieval from large collections of\nlow-resolution (LR) binary document images, which can be found in digital\nlibraries across the world, where the high-resolution (HR) counterpart is not\navailable. This gives rise to the problem of binary document image\nsuper-resolution (BDISR). The objective of this paper is to address the\ninteresting and challenging problem of super resolution of binary Tamil\ndocument images for improved readability and better optical character\nrecognition (OCR). We propose multiple deep neural network architectures to\naddress this problem and analyze their performance. The proposed models are all\nsingle image super-resolution techniques, which learn a generalized spatial\ncorrespondence between the LR and HR binary document images. We employ\nconvolutional layers for feature extraction followed by transposed convolution\nand sub-pixel convolution layers for upscaling the features. Since the outputs\nof the neural networks are gray scale, we utilize the advantage of power law\ntransformation as a post-processing technique to improve the character level\npixel connectivity. The performance of our models is evaluated by comparing the\nOCR accuracies and the mean opinion scores given by human evaluators on LR\nimages and the corresponding model-generated HR images.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 11:41:07 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Pandey", "Ram Krishna", ""], ["Vignesh", "K", ""], ["Ramakrishnan", "A G", ""], ["B", "Chandrahasa", ""]]}, {"id": "1812.02486", "submitter": "Vassilis C. Nicodemou Mr.", "authors": "Vassilis C. Nicodemou, Iason Oikonomidis, Georgios Tzimiropoulos,\n  Antonis Argyros", "title": "Learning to Infer the Depth Map of a Hand from its Color Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first approach to the problem of inferring the depth map of a\nhuman hand based on a single RGB image. We achieve this with a Convolutional\nNeural Network (CNN) that employs a stacked hourglass model as its main\nbuilding block. Intermediate supervision is used in several outputs of the\nproposed architecture in a staged approach. To aid the process of training and\ninference, hand segmentation masks are also estimated in such an intermediate\nsupervision step, and used to guide the subsequent depth estimation process. In\norder to train and evaluate the proposed method we compile and make publicly\navailable HandRGBD, a new dataset of 20,601 views of hands, each consisting of\nan RGB image and an aligned depth map. Based on HandRGBD, we explore variants\nof the proposed approach in an ablative study and determine the best performing\none. The results of an extensive experimental evaluation demonstrate that hand\ndepth estimation from a single RGB frame can be achieved with an accuracy of\n22mm, which is comparable to the accuracy achieved by contemporary low-cost\ndepth cameras. Such a 3D reconstruction of hands based on RGB information is\nvaluable as a final result on its own right, but also as an input to several\nother hand analysis and perception algorithms that require depth input.\nEssentially, in such a context, the proposed approach bridges the gap between\nRGB and RGBD, by making all existing RGBD-based methods applicable to RGB\ninput.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 12:16:37 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Nicodemou", "Vassilis C.", ""], ["Oikonomidis", "Iason", ""], ["Tzimiropoulos", "Georgios", ""], ["Argyros", "Antonis", ""]]}, {"id": "1812.02496", "submitter": "David Robben", "authors": "David Robben, Anna M.M. Boers, Henk A. Marquering, Lucianne L.C.M.\n  Langezaal, Yvo B.W.E.M. Roos, Robert J. van Oostenbrugge, Wim H. van Zwam,\n  Diederik W.J. Dippel, Charles B.L.M. Majoie, Aad van der Lugt, Robin Lemmens,\n  Paul Suetens", "title": "Prediction of final infarct volume from native CT perfusion and\n  treatment parameters using deep learning", "comments": "Accepted for publication in Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2019.101589", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CT Perfusion (CTP) imaging has gained importance in the diagnosis of acute\nstroke. Conventional perfusion analysis performs a deconvolution of the\nmeasurements and thresholds the perfusion parameters to determine the tissue\nstatus. We pursue a data-driven and deconvolution-free approach, where a deep\nneural network learns to predict the final infarct volume directly from the\nnative CTP images and metadata such as the time parameters and treatment. This\nwould allow clinicians to simulate various treatments and gain insight into\npredicted tissue status over time. We demonstrate on a multicenter dataset that\nour approach is able to predict the final infarct and effectively uses the\nmetadata. An ablation study shows that using the native CTP measurements\ninstead of the deconvolved measurements improves the prediction.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 12:34:18 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 12:21:53 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Robben", "David", ""], ["Boers", "Anna M. M.", ""], ["Marquering", "Henk A.", ""], ["Langezaal", "Lucianne L. C. M.", ""], ["Roos", "Yvo B. W. E. M.", ""], ["van Oostenbrugge", "Robert J.", ""], ["van Zwam", "Wim H.", ""], ["Dippel", "Diederik W. J.", ""], ["Majoie", "Charles B. L. M.", ""], ["van der Lugt", "Aad", ""], ["Lemmens", "Robin", ""], ["Suetens", "Paul", ""]]}, {"id": "1812.02501", "submitter": "Fadime Sener", "authors": "Fadime Sener and Angela Yao", "title": "Zero-Shot Anticipation for Instructional Activities", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we teach a robot to predict what will happen next for an activity it\nhas never seen before? We address this problem of zero-shot anticipation by\npresenting a hierarchical model that generalizes instructional knowledge from\nlarge-scale text-corpora and transfers the knowledge to the visual domain.\nGiven a portion of an instructional video, our model predicts coherent and\nplausible actions multiple steps into the future, all in rich natural language.\nTo demonstrate the anticipation capabilities of our model, we introduce the\nTasty Videos dataset, a collection of 2511 recipes for zero-shot learning,\nrecognition and anticipation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 12:48:45 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 08:37:26 GMT"}, {"version": "v3", "created": "Sun, 20 Oct 2019 22:44:45 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Sener", "Fadime", ""], ["Yao", "Angela", ""]]}, {"id": "1812.02510", "submitter": "Davide Cozzolino", "authors": "Davide Cozzolino and Justus Thies and Andreas R\\\"ossler and Christian\n  Riess and Matthias Nie{\\ss}ner and Luisa Verdoliva", "title": "ForensicTransfer: Weakly-supervised Domain Adaptation for Forgery\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distinguishing manipulated from real images is becoming increasingly\ndifficult as new sophisticated image forgery approaches come out by the day.\nNaive classification approaches based on Convolutional Neural Networks (CNNs)\nshow excellent performance in detecting image manipulations when they are\ntrained on a specific forgery method. However, on examples from unseen\nmanipulation approaches, their performance drops significantly. To address this\nlimitation in transferability, we introduce Forensic-Transfer (FT). We devise a\nlearning-based forensic detector which adapts well to new domains, i.e., novel\nmanipulation methods and can handle scenarios where only a handful of fake\nexamples are available during training. To this end, we learn a forensic\nembedding based on a novel autoencoder-based architecture that can be used to\ndistinguish between real and fake imagery. The learned embedding acts as a form\nof anomaly detector; namely, an image manipulated from an unseen method will be\ndetected as fake provided it maps sufficiently far away from the cluster of\nreal images. Comparing to prior works, FT shows significant improvements in\ntransferability, which we demonstrate in a series of experiments on\ncutting-edge benchmarks. For instance, on unseen examples, we achieve up to 85%\nin terms of accuracy, and with only a handful of seen examples, our performance\nalready reaches around 95%.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 13:11:09 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 10:02:18 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Cozzolino", "Davide", ""], ["Thies", "Justus", ""], ["R\u00f6ssler", "Andreas", ""], ["Riess", "Christian", ""], ["Nie\u00dfner", "Matthias", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1812.02518", "submitter": "Shuman Jia", "authors": "Shuman Jia, Antoine Despinasse, Zihao Wang, Herv\\'e Delingette, Xavier\n  Pennec, Pierre Ja\\\"is, Hubert Cochet, and Maxime Sermesant", "title": "Automatically Segmenting the Left Atrium from Cardiac Images Using\n  Successive 3D U-Nets and a Contour Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiological imaging offers effective measurement of anatomy, which is useful\nin disease diagnosis and assessment. Previous study has shown that the left\natrial wall remodeling can provide information to predict treatment outcome in\natrial fibrillation. Nevertheless, the segmentation of the left atrial\nstructures from medical images is still very time-consuming. Current advances\nin neural network may help creating automatic segmentation models that reduce\nthe workload for clinicians. In this preliminary study, we propose automated,\ntwo-stage, three-dimensional U-Nets with convolutional neural network, for the\nchallenging task of left atrial segmentation. Unlike previous two-dimensional\nimage segmentation methods, we use 3D U-Nets to obtain the heart cavity\ndirectly in 3D. The dual 3D U-Net structure consists of, a first U-Net to\ncoarsely segment and locate the left atrium, and a second U-Net to accurately\nsegment the left atrium under higher resolution. In addition, we introduce a\nContour loss based on additional distance information to adjust the final\nsegmentation. We randomly split the data into training datasets (80 subjects)\nand validation datasets (20 subjects) to train multiple models, with different\naugmentation setting. Experiments show that the average Dice coefficients for\nvalidation datasets are around 0.91 - 0.92, the sensitivity around 0.90-0.94\nand the specificity 0.99. Compared with traditional Dice loss, models trained\nwith Contour loss in general offer smaller Hausdorff distance with similar Dice\ncoefficient, and have less connected components in predictions. Finally, we\nintegrate several trained models in an ensemble prediction to segment testing\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 13:34:24 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Jia", "Shuman", ""], ["Despinasse", "Antoine", ""], ["Wang", "Zihao", ""], ["Delingette", "Herv\u00e9", ""], ["Pennec", "Xavier", ""], ["Ja\u00efs", "Pierre", ""], ["Cochet", "Hubert", ""], ["Sermesant", "Maxime", ""]]}, {"id": "1812.02523", "submitter": "Antonio Fonseca", "authors": "Ant\\'onio Filipe Fonseca", "title": "Representing pictures with emotions", "comments": "V1.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern research in content-based image retrieval systems (CIBR) has become\nprogressively more focused on the richness of human semantics. Several\napproaches may be used to reduced the 'semantic gap' between the high-level\nhuman experience and the low level visual features of pictures. Object\nontology, among others, is one of the methods. In this paper we investigate the\nuse of a codified emotion ontology over global color features of images to\nannotate the images at a high semantic level. In order to speed up the\nannotation process the images are sampled so that each digital image is\nrepresented by a random subset of its content. We test within controlled\nconditions how this random subset may represent the adequate high level\nemotional concept presented in the image. We monitor this information reducing\nprocess with entropy measures, showing that controlled random sampling can\ncapture with significant relevance high level concepts for picture\nrepresentation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 13:50:40 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 13:35:05 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Fonseca", "Ant\u00f3nio Filipe", ""]]}, {"id": "1812.02524", "submitter": "Hsin-Pai Cheng", "authors": "Jingyang Zhang, Hsin-Pai Cheng, Chunpeng Wu, Hai Li, Yiran Chen", "title": "Towards Leveraging the Information of Gradients in Optimization-based\n  Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks demonstrated state-of-the-art\nperformance in a large variety of tasks and therefore have been adopted in many\napplications. On the other hand, the latest studies revealed that neural\nnetworks are vulnerable to adversarial examples obtained by carefully adding\nsmall perturbation to legitimate samples. Based upon the observation, many\nattack methods were proposed. Among them, the optimization-based CW attack is\nthe most powerful as the produced adversarial samples present much less\ndistortion compared to other methods. The better attacking effect, however,\ncomes at the cost of running more iterations and thus longer computation time\nto reach desirable results. In this work, we propose to leverage the\ninformation of gradients as a guidance during the search of adversaries. More\nspecifically, directly incorporating the gradients into the perturbation can be\nregarded as a constraint added to the optimization process. We intuitively and\nempirically prove the rationality of our method in reducing the search space.\nOur experiments show that compared to the original CW attack, the proposed\nmethod requires fewer iterations towards adversarial samples, obtaining a\nhigher success rate and resulting in smaller $\\ell_2$ distortion.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 13:55:23 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Zhang", "Jingyang", ""], ["Cheng", "Hsin-Pai", ""], ["Wu", "Chunpeng", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1812.02541", "submitter": "Yinlin Hu", "authors": "Yinlin Hu, Joachim Hugonot, Pascal Fua, Mathieu Salzmann", "title": "Segmentation-driven 6D Object Pose Estimation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The most recent trend in estimating the 6D pose of rigid objects has been to\ntrain deep networks to either directly regress the pose from the image or to\npredict the 2D locations of 3D keypoints, from which the pose can be obtained\nusing a PnP algorithm. In both cases, the object is treated as a global entity,\nand a single pose estimate is computed. As a consequence, the resulting\ntechniques can be vulnerable to large occlusions.\n  In this paper, we introduce a segmentation-driven 6D pose estimation\nframework where each visible part of the objects contributes a local pose\nprediction in the form of 2D keypoint locations. We then use a predicted\nmeasure of confidence to combine these pose candidates into a robust set of\n3D-to-2D correspondences, from which a reliable pose estimate can be obtained.\nWe outperform the state-of-the-art on the challenging Occluded-LINEMOD and\nYCB-Video datasets, which is evidence that our approach deals well with\nmultiple poorly-textured objects occluding each other. Furthermore, it relies\non a simple enough architecture to achieve real-time performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 14:15:24 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 10:01:55 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 05:33:21 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Hu", "Yinlin", ""], ["Hugonot", "Joachim", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1812.02542", "submitter": "Rohit Gandikota", "authors": "Rohit Gandikota", "title": "Computer Vision for Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we try to implement Image Processing techniques in the area of\nautonomous vehicles, both indoor and outdoor. The challenges for both are\ndifferent and the ways to tackle them vary too. We also showed deep learning\nmakes things easier and precise. We also made base models for all the problems\nwe tackle while building an autonomous car for Indian Institute of Space\nscience and Technology.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 14:16:56 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Gandikota", "Rohit", ""]]}, {"id": "1812.02591", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Maharshi Gor, R. Venkatesh Babu", "title": "BiHMP-GAN: Bidirectional 3D Human Motion Prediction GAN", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction model has applications in various fields of computer\nvision. Without taking into account the inherent stochasticity in the\nprediction of future pose dynamics, such methods often converges to a\ndeterministic undesired mean of multiple probable outcomes. Devoid of this, we\npropose a novel probabilistic generative approach called Bidirectional Human\nmotion prediction GAN, or BiHMP-GAN. To be able to generate multiple probable\nhuman-pose sequences, conditioned on a given starting sequence, we introduce a\nrandom extrinsic factor r, drawn from a predefined prior distribution.\nFurthermore, to enforce a direct content loss on the predicted motion sequence\nand also to avoid mode-collapse, a novel bidirectional framework is\nincorporated by modifying the usual discriminator architecture. The\ndiscriminator is trained also to regress this extrinsic factor r, which is used\nalongside with the intrinsic factor (encoded starting pose sequence) to\ngenerate a particular pose sequence. To further regularize the training, we\nintroduce a novel recursive prediction strategy. In spite of being in a\nprobabilistic framework, the enhanced discriminator architecture allows\npredictions of an intermediate part of pose sequence to be used as a\nconditioning for prediction of the latter part of the sequence. The\nbidirectional setup also provides a new direction to evaluate the prediction\nquality against a given test sequence. For a fair assessment of BiHMP-GAN, we\nreport performance of the generated motion sequence using (i) a critic model\ntrained to discriminate between real and fake motion sequence, and (ii) an\naction classifier trained on real human motion dynamics. Outcomes of both\nqualitative and quantitative evaluations, on the probabilistic generations of\nthe model, demonstrate the superiority of BiHMP-GAN over previously available\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 15:07:56 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Gor", "Maharshi", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1812.02592", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Maharshi Gor, Phani Krishna Uppala, R. Venkatesh\n  Babu", "title": "Unsupervised Feature Learning of Human Actions as Trajectories in Pose\n  Embedding Manifold", "comments": "Accepted at WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsupervised human action modeling framework can provide useful\npose-sequence representation, which can be utilized in a variety of pose\nanalysis applications. In this work we propose a novel temporal pose-sequence\nmodeling framework, which can embed the dynamics of 3D human-skeleton joints to\na continuous latent space in an efficient manner. In contrast to end-to-end\nframework explored by previous works, we disentangle the task of individual\npose representation learning from the task of learning actions as a trajectory\nin pose embedding space. In order to realize a continuous pose embedding\nmanifold with improved reconstructions, we propose an unsupervised, manifold\nlearning procedure named Encoder GAN, (or EnGAN). Further, we use the pose\nembeddings generated by EnGAN to model human actions using a bidirectional RNN\nauto-encoder architecture, PoseRNN. We introduce first-order gradient loss to\nexplicitly enforce temporal regularity in the predicted motion sequence. A\nhierarchical feature fusion technique is also investigated for simultaneous\nmodeling of local skeleton joints along with global pose variations. We\ndemonstrate state-of-the-art transfer-ability of the learned representation\nagainst other supervisedly and unsupervisedly learned motion embeddings for the\ntask of fine-grained action recognition on SBU interaction dataset. Further, we\nshow the qualitative strengths of the proposed framework by visualizing\nskeleton pose reconstructions and interpolations in pose-embedding space, and\nlow dimensional principal component projections of the reconstructed pose\ntrajectories.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 15:08:02 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Gor", "Maharshi", ""], ["Uppala", "Phani Krishna", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1812.02605", "submitter": "Xiaobin Chang", "authors": "Xiaobin Chang, Yongxin Yang, Tao Xiang, Timothy M. Hospedales", "title": "Disjoint Label Space Transfer Learning with Common Factorised Space", "comments": "AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a unified approach is presented to transfer learning that\naddresses several source and target domain label-space and annotation\nassumptions with a single model. It is particularly effective in handling a\nchallenging case, where source and target label-spaces are disjoint, and\noutperforms alternatives in both unsupervised and semi-supervised settings. The\nkey ingredient is a common representation termed Common Factorised Space. It is\nshared between source and target domains, and trained with an unsupervised\nfactorisation loss and a graph-based loss. With a wide range of experiments, we\ndemonstrate the flexibility, relevance and efficacy of our method, both in the\nchallenging cases with disjoint label spaces, and in the more conventional\ncases such as unsupervised domain adaptation, where the source and target\ndomains share the same label-sets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 15:33:36 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Chang", "Xiaobin", ""], ["Yang", "Yongxin", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1812.02611", "submitter": "Alexandre Rame", "authors": "Alexandre Rame, Emilien Garreau, Hedi Ben-Younes and Charles Ollion", "title": "OMNIA Faster R-CNN: Detection in the wild through dataset merging and\n  soft distillation", "comments": "9 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detectors tend to perform poorly in new or open domains, and require\nexhaustive yet costly annotations from fully labeled datasets. We aim at\nbenefiting from several datasets with different categories but without\nadditional labelling, not only to increase the number of categories detected,\nbut also to take advantage from transfer learning and to enhance domain\nindependence.\n  Our dataset merging procedure starts with training several initial Faster\nR-CNN on the different datasets while considering the complementary datasets'\nimages for domain adaptation. Similarly to self-training methods, the\npredictions of these initial detectors mitigate the missing annotations on the\ncomplementary datasets. The final OMNIA Faster R-CNN is trained with all\ncategories on the union of the datasets enriched by predictions. The joint\ntraining handles unsafe targets with a new classification loss called SoftSig\nin a softly supervised way.\n  Experimental results show that in the case of fashion detection for images in\nthe wild, merging Modanet with COCO increases the final performance from 45.5%\nto 57.4% in mAP. Applying our soft distillation to the task of detection with\ndomain shift between GTA and Cityscapes enables to beat the state-of-the-art by\n5.3 points. Our methodology could unlock object detection for real-world\napplications without immense datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 15:38:43 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 17:28:03 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Rame", "Alexandre", ""], ["Garreau", "Emilien", ""], ["Ben-Younes", "Hedi", ""], ["Ollion", "Charles", ""]]}, {"id": "1812.02619", "submitter": "Tuan-Hung Vu", "authors": "Tuan-Hung Vu, Anton Osokin, Ivan Laptev", "title": "Tube-CNN: Modeling temporal evolution of appearance for object detection\n  in video", "comments": "13 pages, 8 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in video is crucial for many applications. Compared to\nimages, video provides additional cues which can help to disambiguate the\ndetection problem. Our goal in this paper is to learn discriminative models for\nthe temporal evolution of object appearance and to use such models for object\ndetection. To model temporal evolution, we introduce space-time tubes\ncorresponding to temporal sequences of bounding boxes. We propose two CNN\narchitectures for generating and classifying tubes, respectively. Our tube\nproposal network (TPN) first generates a large number of spatio-temporal tube\nproposals maximizing object recall. The Tube-CNN then implements a tube-level\nobject detector in the video. Our method improves state of the art on two\nlarge-scale datasets for object detection in video: HollywoodHeads and ImageNet\nVID. Tube models show particular advantages in difficult dynamic scenes.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 15:48:54 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Vu", "Tuan-Hung", ""], ["Osokin", "Anton", ""], ["Laptev", "Ivan", ""]]}, {"id": "1812.02621", "submitter": "Mihir Chauhan", "authors": "Mohammad Abuzar Shaikh, Mihir Chauhan, Jun Chu and Sargur Srihari", "title": "Hybrid Feature Learning for Handwriting Verification", "comments": "Accepted and presented in International Conference on Frontiers in\n  Handwriting Recognition (ICFHR) 2018", "journal-ref": null, "doi": "10.1109/ICFHR-2018.2018.00041", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective Hybrid Deep Learning (HDL) architecture for the task\nof determining the probability that a questioned handwritten word has been\nwritten by a known writer. HDL is an amalgamation of Auto-Learned Features\n(ALF) and Human-Engineered Features (HEF). To extract auto-learned features we\nuse two methods: First, Two Channel Convolutional Neural Network (TC-CNN);\nSecond, Two Channel Autoencoder (TC-AE). Furthermore, human-engineered features\nare extracted by using two methods: First, Gradient Structural Concavity (GSC);\nSecond, Scale Invariant Feature Transform (SIFT). Experiments are performed by\ncomplementing one of the HEF methods with one ALF method on 150000 pairs of\nsamples of the word \"AND\" cropped from handwritten notes written by 1500\nwriters. Our results indicate that HDL architecture with AE-GSC achieves 99.7%\naccuracy on seen writer dataset and 92.16% accuracy on shuffled writer dataset\nwhich out performs CEDAR-FOX, as for unseen writer dataset, AE-SIFT performs\ncomparable to this sophisticated handwriting comparison tool.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:02:28 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Shaikh", "Mohammad Abuzar", ""], ["Chauhan", "Mihir", ""], ["Chu", "Jun", ""], ["Srihari", "Sargur", ""]]}, {"id": "1812.02622", "submitter": "Jenn-Bing Ong", "authors": "Jenn-Bing Ong, Wee-Keong Ng, C.-C. Jay Kuo", "title": "Convolutional Neural Networks with Transformed Input based on Robust\n  Tensor Network Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor network decomposition, originated from quantum physics to model\nentangled many-particle quantum systems, turns out to be a promising\nmathematical technique to efficiently represent and process big data in\nparsimonious manner. In this study, we show that tensor networks can\nsystematically partition structured data, e.g. color images, for distributed\nstorage and communication in privacy-preserving manner. Leveraging the sea of\nbig data and metadata privacy, empirical results show that neighbouring\nsubtensors with implicit information stored in tensor network formats cannot be\nidentified for data reconstruction. This technique complements the existing\nencryption and randomization techniques which store explicit data\nrepresentation at one place and highly susceptible to adversarial attacks such\nas side-channel attacks and de-anonymization. Furthermore, we propose a theory\nfor adversarial examples that mislead convolutional neural networks to\nmisclassification using subspace analysis based on singular value decomposition\n(SVD). The theory is extended to analyze higher-order tensors using\ntensor-train SVD (TT-SVD); it helps to explain the level of susceptibility of\ndifferent datasets to adversarial attacks, the structural similarity of\ndifferent adversarial attacks including global and localized attacks, and the\nefficacy of different adversarial defenses based on input transformation. An\nefficient and adaptive algorithm based on robust TT-SVD is then developed to\ndetect strong and static adversarial attacks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 10:57:25 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 10:24:34 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Ong", "Jenn-Bing", ""], ["Ng", "Wee-Keong", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1812.02626", "submitter": "Andrea Zunino", "authors": "Sarah Adel Bargal, Andrea Zunino, Vitali Petsiuk, Jianming Zhang, Kate\n  Saenko, Vittorio Murino, Stan Sclaroff", "title": "Guided Zoom: Questioning Network Evidence for Fine-grained\n  Classification", "comments": "BMVC 2019 Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Guided Zoom, an approach that utilizes spatial grounding of a\nmodel's decision to make more informed predictions. It does so by making sure\nthe model has \"the right reasons\" for a prediction, defined as reasons that are\ncoherent with those used to make similar correct decisions at training time.\nThe reason/evidence upon which a deep convolutional neural network makes a\nprediction is defined to be the spatial grounding, in the pixel space, for a\nspecific class conditional probability in the model output. Guided Zoom\nexamines how reasonable such evidence is for each of the top-k predicted\nclasses, rather than solely trusting the top-1 prediction. We show that Guided\nZoom improves the classification accuracy of a deep convolutional neural\nnetwork model and obtains state-of-the-art results on three fine-grained\nclassification benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 16:00:05 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 11:16:06 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Bargal", "Sarah Adel", ""], ["Zunino", "Andrea", ""], ["Petsiuk", "Vitali", ""], ["Zhang", "Jianming", ""], ["Saenko", "Kate", ""], ["Murino", "Vittorio", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1812.02636", "submitter": "Gil Avraham", "authors": "Yan Zuo, Gil Avraham, Tom Drummond", "title": "Traversing Latent Space using Decision Ferns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practice of transforming raw data to a feature space so that inference\ncan be performed in that space has been popular for many years. Recently, rapid\nprogress in deep neural networks has given both researchers and practitioners\nenhanced methods that increase the richness of feature representations, be it\nfrom images, text or speech. In this work we show how a constructed latent\nspace can be explored in a controlled manner and argue that this complements\nwell founded inference methods. For constructing the latent space a Variational\nAutoencoder is used. We present a novel controller module that allows for\nsmooth traversal in the latent space and construct an end-to-end trainable\nframework. We explore the applicability of our method for performing spatial\ntransformations as well as kinematics for predicting future latent vectors of a\nvideo sequence.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 16:15:24 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Zuo", "Yan", ""], ["Avraham", "Gil", ""], ["Drummond", "Tom", ""]]}, {"id": "1812.02640", "submitter": "Yuhao Niu", "authors": "Yuhao Niu, Lin Gu, Feng Lu, Feifan Lv, Zongji Wang, Imari Sato, Zijian\n  Zhang, Yangyan Xiao, Xunzhang Dai, Tingting Cheng", "title": "Pathological Evidence Exploration in Deep Retinal Image Diagnosis", "comments": "to appear in AAAI (2019). The first two authors contributed equally\n  to the paper. Corresponding Author: Feng Lu", "journal-ref": "AAAI 2019: 1093-1101", "doi": "10.1609/aaai.v33i01.33011093", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep learning has shown successful performance in classifying the\nlabel and severity stage of certain disease, most of them give few evidence on\nhow to make prediction. Here, we propose to exploit the interpretability of\ndeep learning application in medical diagnosis. Inspired by Koch's Postulates,\na well-known strategy in medical research to identify the property of pathogen,\nwe define a pathological descriptor that can be extracted from the activated\nneurons of a diabetic retinopathy detector. To visualize the symptom and\nfeature encoded in this descriptor, we propose a GAN based method to synthesize\npathological retinal image given the descriptor and a binary vessel\nsegmentation. Besides, with this descriptor, we can arbitrarily manipulate the\nposition and quantity of lesions. As verified by a panel of 5 licensed\nophthalmologists, our synthesized images carry the symptoms that are directly\nrelated to diabetic retinopathy diagnosis. The panel survey also shows that our\ngenerated images is both qualitatively and quantitatively superior to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 16:18:33 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Niu", "Yuhao", ""], ["Gu", "Lin", ""], ["Lu", "Feng", ""], ["Lv", "Feifan", ""], ["Wang", "Zongji", ""], ["Sato", "Imari", ""], ["Zhang", "Zijian", ""], ["Xiao", "Yangyan", ""], ["Dai", "Xunzhang", ""], ["Cheng", "Tingting", ""]]}, {"id": "1812.02664", "submitter": "Yulei Niu", "authors": "Yulei Niu, Hanwang Zhang, Manli Zhang, Jianhong Zhang, Zhiwu Lu,\n  Ji-Rong Wen", "title": "Recursive Visual Attention in Visual Dialog", "comments": "CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual dialog is a challenging vision-language task, which requires the agent\nto answer multi-round questions about an image. It typically needs to address\ntwo major problems: (1) How to answer visually-grounded questions, which is the\ncore challenge in visual question answering (VQA); (2) How to infer the\nco-reference between questions and the dialog history. An example of visual\nco-reference is: pronouns (\\eg, ``they'') in the question (\\eg, ``Are they on\nor off?'') are linked with nouns (\\eg, ``lamps'') appearing in the dialog\nhistory (\\eg, ``How many lamps are there?'') and the object grounded in the\nimage. In this work, to resolve the visual co-reference for visual dialog, we\npropose a novel attention mechanism called Recursive Visual Attention (RvA).\nSpecifically, our dialog agent browses the dialog history until the agent has\nsufficient confidence in the visual co-reference resolution, and refines the\nvisual attention recursively. The quantitative and qualitative experimental\nresults on the large-scale VisDial v0.9 and v1.0 datasets demonstrate that the\nproposed RvA not only outperforms the state-of-the-art methods, but also\nachieves reasonable recursion and interpretable attention maps without\nadditional annotations. The code is available at\n\\url{https://github.com/yuleiniu/rva}.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 17:00:16 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 15:02:24 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Niu", "Yulei", ""], ["Zhang", "Hanwang", ""], ["Zhang", "Manli", ""], ["Zhang", "Jianhong", ""], ["Lu", "Zhiwu", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1812.02676", "submitter": "Nam Le", "authors": "Nam Le and Jean-Marc Odobez", "title": "Theoretical Guarantees of Deep Embedding Losses Under Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting labeled data to train deep neural networks is costly and even\nimpractical for many tasks. Thus, research effort has been focused in\nautomatically curated datasets or unsupervised and weakly supervised learning.\nThe common problem in these directions is learning with unreliable label\ninformation. In this paper, we address the tolerance of deep embedding learning\nlosses against label noise, i.e. when the observed labels are different from\nthe true labels. Specifically, we provide the sufficient conditions to achieve\ntheoretical guarantees for the 2 common loss functions: marginal loss and\ntriplet loss. From these theoretical results, we can estimate how sampling\nstrategies and initialization can affect the level of resistance against label\nnoise. The analysis also helps providing more effective guidelines in\nunsupervised and weakly supervised deep embedding learning.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 17:19:01 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 17:43:16 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Le", "Nam", ""], ["Odobez", "Jean-Marc", ""]]}, {"id": "1812.02699", "submitter": "Ravi Teja Mullapudi", "authors": "Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, Kayvon\n  Fatahalian", "title": "Online Model Distillation for Efficient Video Inference", "comments": null, "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality computer vision models typically address the problem of\nunderstanding the general distribution of real-world images. However, most\ncameras observe only a very small fraction of this distribution. This offers\nthe possibility of achieving more efficient inference by specializing compact,\nlow-cost models to the specific distribution of frames observed by a single\ncamera. In this paper, we employ the technique of model distillation\n(supervising a low-cost student model using the output of a high-cost teacher)\nto specialize accurate, low-cost semantic segmentation models to a target video\nstream. Rather than learn a specialized student model on offline data from the\nvideo stream, we train the student in an online fashion on the live video,\nintermittently running the teacher to provide a target for learning. Online\nmodel distillation yields semantic segmentation models that closely approximate\ntheir Mask R-CNN teacher with 7 to 17$\\times$ lower inference runtime cost (11\nto 26$\\times$ in FLOPs), even when the target video's distribution is\nnon-stationary. Our method requires no offline pretraining on the target video\nstream, achieves higher accuracy and lower cost than solutions based on flow or\nvideo object segmentation, and can exhibit better temporal stability than the\noriginal teacher. We also provide a new video dataset for evaluating the\nefficiency of inference over long running video streams.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:29:59 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 21:57:10 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Mullapudi", "Ravi Teja", ""], ["Chen", "Steven", ""], ["Zhang", "Keyi", ""], ["Ramanan", "Deva", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "1812.02707", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar, Jo\\~ao Carreira, Carl Doersch and Andrew Zisserman", "title": "Video Action Transformer Network", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Action Transformer model for recognizing and localizing\nhuman actions in video clips. We repurpose a Transformer-style architecture to\naggregate features from the spatiotemporal context around the person whose\nactions we are trying to classify. We show that by using high-resolution,\nperson-specific, class-agnostic queries, the model spontaneously learns to\ntrack individual people and to pick up on semantic context from the actions of\nothers. Additionally its attention mechanism learns to emphasize hands and\nfaces, which are often crucial to discriminate an action - all without explicit\nsupervision other than boxes and class labels. We train and test our Action\nTransformer network on the Atomic Visual Actions (AVA) dataset, outperforming\nthe state-of-the-art by a significant margin using only raw RGB frames as\ninput.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:42:25 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 14:17:25 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Girdhar", "Rohit", ""], ["Carreira", "Jo\u00e3o", ""], ["Doersch", "Carl", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1812.02713", "submitter": "Kaichun Mo", "authors": "Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi,\n  Leonidas J. Guibas, Hao Su", "title": "PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical\n  Part-level 3D Object Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PartNet: a consistent, large-scale dataset of 3D objects annotated\nwith fine-grained, instance-level, and hierarchical 3D part information. Our\ndataset consists of 573,585 part instances over 26,671 3D models covering 24\nobject categories. This dataset enables and serves as a catalyst for many tasks\nsuch as shape analysis, dynamic 3D scene modeling and simulation, affordance\nanalysis, and others. Using our dataset, we establish three benchmarking tasks\nfor evaluating 3D part recognition: fine-grained semantic segmentation,\nhierarchical semantic segmentation, and instance segmentation. We benchmark\nfour state-of-the-art 3D deep learning algorithms for fine-grained semantic\nsegmentation and three baseline methods for hierarchical semantic segmentation.\nWe also propose a novel method for part instance segmentation and demonstrate\nits superior performance over existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:48:25 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Mo", "Kaichun", ""], ["Zhu", "Shilin", ""], ["Chang", "Angel X.", ""], ["Yi", "Li", ""], ["Tripathi", "Subarna", ""], ["Guibas", "Leonidas J.", ""], ["Su", "Hao", ""]]}, {"id": "1812.02716", "submitter": "Carlos Esteves", "authors": "Carlos Esteves, Avneesh Sud, Zhengyi Luo, Kostas Daniilidis, Ameesh\n  Makadia", "title": "Cross-Domain 3D Equivariant Image Embeddings", "comments": "Accepted to the International Conference on Machine Learning, ICML\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spherical convolutional networks have been introduced recently as tools to\nlearn powerful feature representations of 3D shapes. Spherical CNNs are\nequivariant to 3D rotations making them ideally suited to applications where 3D\ndata may be observed in arbitrary orientations. In this paper we learn 2D image\nembeddings with a similar equivariant structure: embedding the image of a 3D\nobject should commute with rotations of the object. We introduce a cross-domain\nembedding from 2D images into a spherical CNN latent space. This embedding\nencodes images with 3D shape properties and is equivariant to 3D rotations of\nthe observed object. The model is supervised only by target embeddings obtained\nfrom a spherical CNN pretrained for 3D shape classification. We show that\nlearning a rich embedding for images with appropriate geometric structure is\nsufficient for tackling varied applications, such as relative pose estimation\nand novel view synthesis, without requiring additional task-specific\nsupervision.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:51:12 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 19:21:59 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Esteves", "Carlos", ""], ["Sud", "Avneesh", ""], ["Luo", "Zhengyi", ""], ["Daniilidis", "Kostas", ""], ["Makadia", "Ameesh", ""]]}, {"id": "1812.02725", "submitter": "Jun-Yan Zhu", "authors": "Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio\n  Torralba, Joshua B. Tenenbaum, William T. Freeman", "title": "Visual Object Networks: Image Generation with Disentangled 3D\n  Representation", "comments": "NeurIPS 2018. Code: https://github.com/junyanz/VON Website:\n  http://von.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in deep generative models has led to tremendous breakthroughs\nin image generation. However, while existing models can synthesize\nphotorealistic images, they lack an understanding of our underlying 3D world.\nWe present a new generative model, Visual Object Networks (VON), synthesizing\nnatural images of objects with a disentangled 3D representation. Inspired by\nclassic graphics rendering pipelines, we unravel our image formation process\ninto three conditionally independent factors---shape, viewpoint, and\ntexture---and present an end-to-end adversarial learning framework that jointly\nmodels 3D shapes and 2D images. Our model first learns to synthesize 3D shapes\nthat are indistinguishable from real shapes. It then renders the object's 2.5D\nsketches (i.e., silhouette and depth map) from its shape under a sampled\nviewpoint. Finally, it learns to add realistic texture to these 2.5D sketches\nto generate natural images. The VON not only generates images that are more\nrealistic than state-of-the-art 2D image synthesis methods, but also enables\nmany 3D operations such as changing the viewpoint of a generated image, editing\nof shape and texture, linear interpolation in texture and shape space, and\ntransferring appearance across different objects and viewpoints.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:58:34 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Zhu", "Jun-Yan", ""], ["Zhang", "Zhoutong", ""], ["Zhang", "Chengkai", ""], ["Wu", "Jiajun", ""], ["Torralba", "Antonio", ""], ["Tenenbaum", "Joshua B.", ""], ["Freeman", "William T.", ""]]}, {"id": "1812.02766", "submitter": "Tribhuvanesh Orekondy", "authors": "Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz", "title": "Knockoff Nets: Stealing Functionality of Black-Box Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine Learning (ML) models are increasingly deployed in the wild to perform\na wide range of tasks. In this work, we ask to what extent can an adversary\nsteal functionality of such \"victim\" models based solely on blackbox\ninteractions: image in, predictions out. In contrast to prior work, we present\nan adversary lacking knowledge of train/test data used by the model, its\ninternals, and semantics over model outputs. We formulate model functionality\nstealing as a two-step approach: (i) querying a set of input images to the\nblackbox model to obtain predictions; and (ii) training a \"knockoff\" with\nqueried image-prediction pairs. We make multiple remarkable observations: (a)\nquerying random images from a different distribution than that of the blackbox\ntraining data results in a well-performing knockoff; (b) this is possible even\nwhen the knockoff is represented using a different architecture; and (c) our\nreinforcement learning approach additionally improves query sample efficiency\nin certain settings and provides performance gains. We validate model\nfunctionality stealing on a range of datasets and tasks, as well as on a\npopular image analysis API where we create a reasonable knockoff for as little\nas $30.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 19:34:33 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Orekondy", "Tribhuvanesh", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1812.02771", "submitter": "Tomas Wilkinson", "authors": "Tomas Wilkinson, Jonas Lindstr\\\"om, Anders Brun", "title": "Neural Word Search in Historical Manuscript Collections", "comments": "Extension of arXiv:1703.07645. This version adds results on two\n  additional benchmark datasets (Botany and Konzilsprotokolle) and improves the\n  experiment done in section 5.3.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of segmenting and retrieving word images in\ncollections of historical manuscripts given a text query. This is commonly\nreferred to as \"word spotting\". To this end, we first propose an end-to-end\ntrainable model based on deep neural networks that we dub Ctrl-F-Net. The model\nsimultaneously generates region proposals and embeds them into a word embedding\nspace, wherein a search is performed. We further introduce a simplified version\ncalled Ctrl-F-Mini. It is faster with similar performance, though it is limited\nto more easily segmented manuscripts. We evaluate both models on common\nbenchmark datasets and surpass the previous state of the art. Finally, in\ncollaboration with historians, we employ the Ctrl-F-Net to search within a\nlarge manuscript collection of over 100 thousand pages, written across two\ncenturies. With only 11 training pages, we enable large scale data collection\nin manuscript-based historical research. This results in a speed up of data\ncollection and the number of manuscripts processed by orders of magnitude.\nGiven the time consuming manual work required to study old manuscripts in the\nhumanities, quick and robust tools for word spotting has the potential to\nrevolutionise domains like history, religion and language.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 19:48:42 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 18:51:13 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Wilkinson", "Tomas", ""], ["Lindstr\u00f6m", "Jonas", ""], ["Brun", "Anders", ""]]}, {"id": "1812.02772", "submitter": "Christopher Xie", "authors": "Christopher Xie, Yu Xiang, Zaid Harchaoui, Dieter Fox", "title": "Object Discovery in Videos as Foreground Motion Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of providing dense segmentation masks for object\ndiscovery in videos. We formulate the object discovery problem as foreground\nmotion clustering, where the goal is to cluster foreground pixels in videos\ninto different objects. We introduce a novel pixel-trajectory recurrent neural\nnetwork that learns feature embeddings of foreground pixel trajectories linked\nacross time. By clustering the pixel trajectories using the learned feature\nembeddings, our method establishes correspondences between foreground object\nmasks across video frames. To demonstrate the effectiveness of our framework\nfor object discovery, we conduct experiments on commonly used datasets for\nmotion segmentation, where we achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 19:51:45 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 01:52:22 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Xie", "Christopher", ""], ["Xiang", "Yu", ""], ["Harchaoui", "Zaid", ""], ["Fox", "Dieter", ""]]}, {"id": "1812.02781", "submitter": "Fabian Manhardt", "authors": "Fabian Manhardt and Wadim Kehl and Adrien Gaidon", "title": "ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning method for end-to-end monocular 3D object\ndetection and metric shape retrieval. We propose a novel loss formulation by\nlifting 2D detection, orientation, and scale estimation into 3D space. Instead\nof optimizing these quantities separately, the 3D instantiation allows to\nproperly measure the metric misalignment of boxes. We experimentally show that\nour 10D lifting of sparse 2D Regions of Interests (RoIs) achieves great results\nboth for 6D pose and recovery of the textured metric geometry of instances.\nThis further enables 3D synthetic data augmentation via inpainting recovered\nmeshes directly onto the 2D scenes. We evaluate on KITTI3D against other strong\nmonocular methods and demonstrate that our approach doubles the AP on the 3D\npose metrics on the official test set, defining the new state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 20:08:39 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 17:38:10 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 11:43:18 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Manhardt", "Fabian", ""], ["Kehl", "Wadim", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1812.02784", "submitter": "Yitong Li", "authors": "Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu,\n  Lawrence Carin, David Carlson, Jianfeng Gao", "title": "StoryGAN: A Sequential Conditional GAN for Story Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new task, called Story Visualization. Given a multi-sentence\nparagraph, the story is visualized by generating a sequence of images, one for\neach sentence. In contrast to video generation, story visualization focuses\nless on the continuity in generated images (frames), but more on the global\nconsistency across dynamic scenes and characters -- a challenge that has not\nbeen addressed by any single-image or video generation methods. We therefore\npropose a new story-to-image-sequence generation model, StoryGAN, based on the\nsequential conditional GAN framework. Our model is unique in that it consists\nof a deep Context Encoder that dynamically tracks the story flow, and two\ndiscriminators at the story and image levels, to enhance the image quality and\nthe consistency of the generated sequences. To evaluate the model, we modified\nexisting datasets to create the CLEVR-SV and Pororo-SV datasets. Empirically,\nStoryGAN outperforms state-of-the-art models in image quality, contextual\nconsistency metrics, and human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 20:10:23 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 04:59:05 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Li", "Yitong", ""], ["Gan", "Zhe", ""], ["Shen", "Yelong", ""], ["Liu", "Jingjing", ""], ["Cheng", "Yu", ""], ["Wu", "Yuexin", ""], ["Carin", "Lawrence", ""], ["Carlson", "David", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1812.02817", "submitter": "Yanyi Zhang", "authors": "Yanyi Zhang, Xinyu Li, Kaixiang Huang, Yehan Wang, Shuhong Chen and\n  Ivan Marsic", "title": "Tri-axial Self-Attention for Concurrent Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for concurrent activity recognition. To extract features\nassociated with different activities, we propose a feature-to-activity\nattention that maps the extracted global features to sub-features associated\nwith individual activities. To model the temporal associations of individual\nactivities, we propose a transformer-network encoder that models independent\ntemporal associations for each activity. To make the concurrent activity\nprediction aware of the potential associations between activities, we propose\nself-attention with an association mask. Our system achieved state-of-the-art\nor comparable performance on three commonly used concurrent activity detection\ndatasets. Our visualizations demonstrate that our system is able to locate the\nimportant spatial-temporal features for final decision making. We also showed\nthat our system can be applied to general multilabel classification problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 21:39:14 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Zhang", "Yanyi", ""], ["Li", "Xinyu", ""], ["Huang", "Kaixiang", ""], ["Wang", "Yehan", ""], ["Chen", "Shuhong", ""], ["Marsic", "Ivan", ""]]}, {"id": "1812.02822", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen and Hao Zhang", "title": "Learning Implicit Fields for Generative Shape Modeling", "comments": "Accepted to CVPR 2019. Code:\n  https://github.com/czq142857/implicit-decoder Project page:\n  https://www.sfu.ca/~zhiqinc/imgan/Readme.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate the use of implicit fields for learning generative models of\nshapes and introduce an implicit field decoder, called IM-NET, for shape\ngeneration, aimed at improving the visual quality of the generated shapes. An\nimplicit field assigns a value to each point in 3D space, so that a shape can\nbe extracted as an iso-surface. IM-NET is trained to perform this assignment by\nmeans of a binary classifier. Specifically, it takes a point coordinate, along\nwith a feature vector encoding a shape, and outputs a value which indicates\nwhether the point is outside the shape or not. By replacing conventional\ndecoders by our implicit decoder for representation learning (via IM-AE) and\nshape generation (via IM-GAN), we demonstrate superior results for tasks such\nas generative shape modeling, interpolation, and single-view 3D reconstruction,\nparticularly in terms of visual quality. Code and supplementary material are\navailable at https://github.com/czq142857/implicit-decoder.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 21:52:33 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 04:36:31 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 04:43:34 GMT"}, {"version": "v4", "created": "Mon, 3 Jun 2019 22:25:57 GMT"}, {"version": "v5", "created": "Mon, 16 Sep 2019 20:29:07 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Chen", "Zhiqin", ""], ["Zhang", "Hao", ""]]}, {"id": "1812.02831", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja, Dave Marwood, Nick Johnston, Michele Covell", "title": "Neural Image Decompression: Learning to Render Better Image Previews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rapidly increasing portion of Internet traffic is dominated by requests\nfrom mobile devices with limited- and metered-bandwidth constraints. To satisfy\nthese requests, it has become standard practice for websites to transmit small\nand extremely compressed image previews as part of the initial page-load\nprocess. Recent work, based on an adaptive triangulation of the target image,\nhas shown the ability to generate thumbnails of full images at extreme\ncompression rates: 200 bytes or less with impressive gains (in terms of PSNR\nand SSIM) over both JPEG and WebP standards. However, qualitative assessments\nand preservation of semantic content can be less favorable. We present a novel\nmethod to significantly improve the reconstruction quality of the original\nimage with no changes to the encoded information. Our neural-based decoding not\nonly achieves higher PSNR and SSIM scores than the original methods, but also\nyields a substantial increase in semantic-level content preservation. In\naddition, by keeping the same encoding stream, our solution is completely\ninter-operable with the original decoder. The end result is suitable for a\nrange of small-device deployments, as it involves only a single forward-pass\nthrough a small, scalable network.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 22:12:39 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Baluja", "Shumeet", ""], ["Marwood", "Dave", ""], ["Johnston", "Nick", ""], ["Covell", "Michele", ""]]}, {"id": "1812.02836", "submitter": "Michael Bao", "authors": "Michael Bao, Matthew Cong, St\\'ephane Grabli, Ronald Fedkiw", "title": "High-Quality Face Capture Using Anatomical Muscles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Muscle-based systems have the potential to provide both anatomical accuracy\nand semantic interpretability as compared to blendshape models; however, a lack\nof expressivity and differentiability has limited their impact. Thus, we\npropose modifying a recently developed rather expressive muscle-based system in\norder to make it fully-differentiable; in fact, our proposed modifications\nallow this physically robust and anatomically accurate muscle model to\nconveniently be driven by an underlying blendshape basis. Our formulation is\nintuitive, natural, as well as monolithically and fully coupled such that one\ncan differentiate the model from end to end, which makes it viable for both\noptimization and learning-based approaches for a variety of applications. We\nillustrate this with a number of examples including both shape matching of\nthree-dimensional geometry as as well as the automatic determination of a\nthree-dimensional facial pose from a single two-dimensional RGB image without\nusing markers or depth information.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 22:30:31 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Bao", "Michael", ""], ["Cong", "Matthew", ""], ["Grabli", "St\u00e9phane", ""], ["Fedkiw", "Ronald", ""]]}, {"id": "1812.02843", "submitter": "Akshayvarun Subramanya", "authors": "Akshayvarun Subramanya, Vipin Pillai, Hamed Pirsiavash", "title": "Fooling Network Interpretation in Image Classification", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to be fooled rather easily using\nadversarial attack algorithms. Practical methods such as adversarial patches\nhave been shown to be extremely effective in causing misclassification.\nHowever, these patches are highlighted using standard network interpretation\nalgorithms, thus revealing the identity of the adversary. We show that it is\npossible to create adversarial patches which not only fool the prediction, but\nalso change what we interpret regarding the cause of the prediction. Moreover,\nwe introduce our attack as a controlled setting to measure the accuracy of\ninterpretation algorithms. We show this using extensive experiments for\nGrad-CAM interpretation that transfers to occluding patch interpretation as\nwell. We believe our algorithms can facilitate developing more robust network\ninterpretation tools that truly explain the network's underlying decision\nmaking process.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 22:53:53 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 23:48:28 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Subramanya", "Akshayvarun", ""], ["Pillai", "Vipin", ""], ["Pirsiavash", "Hamed", ""]]}, {"id": "1812.02863", "submitter": "Jianfeng Chi", "authors": "Jianfeng Chi, Emmanuel Owusu, Xuwang Yin, Tong Yu, William Chan,\n  Patrick Tague, Yuan Tian", "title": "Privacy Partitioning: Protecting User Data During the Deep Learning\n  Inference Phase", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical method for protecting data during the inference phase\nof deep learning based on bipartite topology threat modeling and an interactive\nadversarial deep network construction. We term this approach \\emph{Privacy\nPartitioning}. In the proposed framework, we split the machine learning models\nand deploy a few layers into users' local devices, and the rest of the layers\ninto a remote server. We propose an approach to protect user's data during the\ninference phase, while still achieve good classification accuracy.\n  We conduct an experimental evaluation of this approach on benchmark datasets\nof three computer vision tasks. The experimental results indicate that this\napproach can be used to significantly attenuate the capacity for an adversary\nwith access to the state-of-the-art deep network's intermediate states to learn\nprivacy-sensitive inputs to the network. For example, we demonstrate that our\napproach can prevent attackers from inferring the private attributes such as\ngender from the Face image dataset without sacrificing the classification\naccuracy of the original machine learning task such as Face Identification.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 00:42:06 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Chi", "Jianfeng", ""], ["Owusu", "Emmanuel", ""], ["Yin", "Xuwang", ""], ["Yu", "Tong", ""], ["Chan", "William", ""], ["Tague", "Patrick", ""], ["Tian", "Yuan", ""]]}, {"id": "1812.02872", "submitter": "Yapeng Tian", "authors": "Yapeng Tian, Chenxiao Guan, Justin Goodman, Marc Moore, Chenliang Xu", "title": "An Attempt towards Interpretable Audio-Visual Video Captioning", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatically generating a natural language sentence to describe the content\nof an input video is a very challenging problem. It is an essential multimodal\ntask in which auditory and visual contents are equally important. Although\naudio information has been exploited to improve video captioning in previous\nworks, it is usually regarded as an additional feature fed into a black box\nfusion machine. How are the words in the generated sentences associated with\nthe auditory and visual modalities? The problem is still not investigated. In\nthis paper, we make the first attempt to design an interpretable audio-visual\nvideo captioning network to discover the association between words in sentences\nand audio-visual sequences. To achieve this, we propose a multimodal\nconvolutional neural network-based audio-visual video captioning framework and\nintroduce a modality-aware module for exploring modality selection during\nsentence generation. Besides, we collect new audio captioning and visual\ncaptioning datasets for further exploring the interactions between auditory and\nvisual modalities for high-level video understanding. Extensive experiments\ndemonstrate that the modality-aware module makes our model interpretable on\nmodality selection during sentence generation. Even with the added\ninterpretability, our video captioning network can still achieve comparable\nperformance with recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 01:57:42 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Tian", "Yapeng", ""], ["Guan", "Chenxiao", ""], ["Goodman", "Justin", ""], ["Moore", "Marc", ""], ["Xu", "Chenliang", ""]]}, {"id": "1812.02891", "submitter": "Yi Luo", "authors": "Yi Luo, Henry Pfister", "title": "Adversarial Defense of Image Classification Using a Variational\n  Auto-Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to be vulnerable to adversarial attacks. This\nexposes them to potential exploits in security-sensitive applications and\nhighlights their lack of robustness. This paper uses a variational auto-encoder\n(VAE) to defend against adversarial attacks for image classification tasks.\nThis VAE defense has a few nice properties: (1) it is quite flexible and its\nuse of randomness makes it harder to attack; (2) it can learn disentangled\nrepresentations that prevent blurry reconstruction; and (3) a patch-wise VAE\ndefense strategy is used that does not require retraining for different size\nimages. For moderate to severe attacks, this system outperforms or closely\nmatches the performance of JPEG compression, with the best quality parameter.\nIt also has more flexibility and potential for improvement via training.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 03:33:14 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Luo", "Yi", ""], ["Pfister", "Henry", ""]]}, {"id": "1812.02895", "submitter": "Tat-Jun Chin Dr", "authors": "Tat-Jun Chin and Samya Bagchi and Anders Eriksson and Andre van Schaik", "title": "Star Tracking using an Event Camera", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Star trackers are primarily optical devices that are used to estimate the\nattitude of a spacecraft by recognising and tracking star patterns. Currently,\nmost star trackers use conventional optical sensors. In this application paper,\nwe propose the usage of event sensors for star tracking. There are potentially\ntwo benefits of using event sensors for star tracking: lower power consumption\nand higher operating speeds. Our main contribution is to formulate an\nalgorithmic pipeline for star tracking from event data that includes novel\nformulations of rotation averaging and bundle adjustment. In addition, we also\nrelease with this paper a dataset for star tracking using event cameras. With\nthis work, we introduce the problem of star tracking using event cameras to the\ncomputer vision community, whose expertise in SLAM and geometric optimisation\ncan be brought to bear on this commercially important application.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 03:50:09 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 01:51:23 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Chin", "Tat-Jun", ""], ["Bagchi", "Samya", ""], ["Eriksson", "Anders", ""], ["van Schaik", "Andre", ""]]}, {"id": "1812.02897", "submitter": "David Hyde", "authors": "Michael Bao, David Hyde, Xinru Hua, Ronald Fedkiw", "title": "Improved Search Strategies with Application to Estimating Facial\n  Blendshape Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that popular optimization techniques can lead to overfitting\nor even a lack of convergence altogether; thus, practitioners often utilize ad\nhoc regularization terms added to the energy functional. When carefully\ncrafted, these regularizations can produce compelling results. However,\nregularization changes both the energy landscape and the solution to the\noptimization problem, which can result in underfitting. Surprisingly, many\npractitioners both add regularization and claim that their model lacks the\nexpressivity to fit the data. Motivated by a geometric interpretation of the\nlinearized search space, we propose an approach that ameliorates overfitting\nwithout the need for regularization terms that restrict the expressiveness of\nthe underlying model. We illustrate the efficacy of our approach on\nminimization problems related to three-dimensional facial expression estimation\nwhere overfitting clouds semantic understanding and regularization may lead to\nunderfitting that misses or misinterprets subtle expressions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 03:57:39 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 14:39:42 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 22:29:00 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Bao", "Michael", ""], ["Hyde", "David", ""], ["Hua", "Xinru", ""], ["Fedkiw", "Ronald", ""]]}, {"id": "1812.02898", "submitter": "Yapeng Tian", "authors": "Yapeng Tian, Yulun Zhang, Yun Fu, Chenliang Xu", "title": "TDAN: Temporally Deformable Alignment Network for Video Super-Resolution", "comments": "10 pages, 6 figures, demo link\n  http://www.youtube.com/watch?v=eZExENE50I0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR) aims to restore a photo-realistic\nhigh-resolution (HR) video frame from both its corresponding low-resolution\n(LR) frame (reference frame) and multiple neighboring frames (supporting\nframes). Due to varying motion of cameras or objects, the reference frame and\neach support frame are not aligned. Therefore, temporal alignment is a\nchallenging yet important problem for VSR. Previous VSR methods usually utilize\noptical flow between the reference frame and each supporting frame to wrap the\nsupporting frame for temporal alignment. Therefore, the performance of these\nimage-level wrapping-based models will highly depend on the prediction accuracy\nof optical flow, and inaccurate optical flow will lead to artifacts in the\nwrapped supporting frames, which also will be propagated into the reconstructed\nHR video frame. To overcome the limitation, in this paper, we propose a\ntemporal deformable alignment network (TDAN) to adaptively align the reference\nframe and each supporting frame at the feature level without computing optical\nflow. The TDAN uses features from both the reference frame and each supporting\nframe to dynamically predict offsets of sampling convolution kernels. By using\nthe corresponding kernels, TDAN transforms supporting frames to align with the\nreference frame. To predict the HR video frame, a reconstruction network taking\naligned frames and the reference frame is utilized. Experimental results\ndemonstrate the effectiveness of the proposed TDAN-based VSR model.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 03:58:43 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Tian", "Yapeng", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""], ["Xu", "Chenliang", ""]]}, {"id": "1812.02899", "submitter": "Michael Bao", "authors": "Michael Bao, Jane Wu, Xinwei Yao, Ronald Fedkiw", "title": "Deep Energies for Estimating Three-Dimensional Facial Pose and\n  Expression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While much progress has been made in capturing high-quality facial\nperformances using motion capture markers and shape-from-shading, high-end\nsystems typically also rely on rotoscope curves hand-drawn on the image. These\ncurves are subjective and difficult to draw consistently; moreover, ad-hoc\nprocedural methods are required for generating matching rotoscope curves on\nsynthetic renders embedded in the optimization used to determine\nthree-dimensional facial pose and expression. We propose an alternative\napproach whereby these curves and other keypoints are detected automatically on\nboth the image and the synthetic renders using trained neural networks,\neliminating artist subjectivity and the ad-hoc procedures meant to mimic it.\nMore generally, we propose using machine learning networks to implicitly define\ndeep energies which when minimized using classical optimization techniques lead\nto three-dimensional facial pose and expression estimation.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 04:00:53 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Bao", "Michael", ""], ["Wu", "Jane", ""], ["Yao", "Xinwei", ""], ["Fedkiw", "Ronald", ""]]}, {"id": "1812.02925", "submitter": "Zhihao Fang", "authors": "Zhihao Fang, He Ma, Xuemin Zhu, Xutao Guo and Ruixin Zhou", "title": "SeFM: A Sequential Feature Point Matching Algorithm for Object 3D\n  Reconstruction", "comments": "This is a pre-print of a contribution published in Frontier Computing\n  - Theory, Technologies and Applications (FC 2019) published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction is a fundamental issue in many applications and the feature\npoint matching problem is a key step while reconstructing target objects.\nConventional algorithms can only find a small number of feature points from two\nimages which is quite insufficient for reconstruction. To overcome this\nproblem, we propose SeFM a sequential feature point matching algorithm. We\nfirst utilize the epipolar geometry to find the epipole of each image. Rotating\nalong the epipole, we generate a set of the epipolar lines and reserve those\nintersecting with the input image. Next, a rough matching phase, followed by a\ndense matching phase, is applied to find the matching dot-pairs using dynamic\nprogramming. Furthermore, we also remove wrong matching dot-pairs by\ncalculating the validity. Experimental results illustrate that SeFM can achieve\naround 1,000 to 10,000 times matching dot-pairs, depending on individual image,\ncompared to conventional algorithms and the object reconstruction with only two\nimages is semantically visible. Moreover, it outperforms conventional\nalgorithms, such as SIFT and SURF, regarding precision and recall.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 06:13:07 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 12:44:00 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 10:37:23 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Fang", "Zhihao", ""], ["Ma", "He", ""], ["Zhu", "Xuemin", ""], ["Guo", "Xutao", ""], ["Zhou", "Ruixin", ""]]}, {"id": "1812.02937", "submitter": "Idoia Ruiz", "authors": "Idoia Ruiz, Bogdan Raducanu, Rakesh Mehta, Jaume Amores", "title": "Optimizing speed/accuracy trade-off for person re-identification via\n  knowledge distillation", "comments": "Published on the journal \"Engineering Applications of Artificial\n  Intelligence\"", "journal-ref": "Engineering Applications of Artificial Intelligence, Volume 87,\n  January 2020, 103309", "doi": "10.1016/j.engappai.2019.103309", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a person across a camera network plays an important role in video\nsurveillance. For a real-world person re-identification application, in order\nto guarantee an optimal time response, it is crucial to find the balance\nbetween accuracy and speed. We analyse this trade-off, comparing a classical\nmethod, that comprises hand-crafted feature description and metric learning, in\nparticular, LOMO and XQDA, to deep learning based techniques, using image\nclassification networks, ResNet and MobileNets. Additionally, we propose and\nanalyse network distillation as a learning strategy to reduce the computational\ncost of the deep learning approach at test time. We evaluate both methods on\nthe Market-1501 and DukeMTMC-reID large-scale datasets, showing that\ndistillation helps reducing the computational cost at inference time while even\nincreasing the accuracy performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 08:11:06 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 16:40:11 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Ruiz", "Idoia", ""], ["Raducanu", "Bogdan", ""], ["Mehta", "Rakesh", ""], ["Amores", "Jaume", ""]]}, {"id": "1812.02956", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski, Tomasz Kajdanowicz, Nitesh Chawla", "title": "LNEMLC: Label Network Embeddings for Multi-Label Classification", "comments": "submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Multi-label classification aims to classify instances with discrete\nnon-exclusive labels. Most approaches on multi-label classification focus on\neffective adaptation or transformation of existing binary and multi-class\nlearning approaches but fail in modelling the joint probability of labels or do\nnot preserve generalization abilities for unseen label combinations. To address\nthese issues we propose a new multi-label classification scheme, LNEMLC - Label\nNetwork Embedding for Multi-Label Classification, that embeds the label network\nand uses it to extend input space in learning and inference of any base\nmulti-label classifier. The approach allows capturing of labels' joint\nprobability at low computational complexity providing results comparable to the\nbest methods reported in the literature. We demonstrate how the method reveals\nstatistically significant improvements over the simple kNN baseline classifier.\nWe also provide hints for selecting the robust configuration that works\nsatisfactorily across data domains.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 09:30:18 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 21:11:09 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["Kajdanowicz", "Tomasz", ""], ["Chawla", "Nitesh", ""]]}, {"id": "1812.02967", "submitter": "Soumajit Majumder", "authors": "Soumajit Majumder, Angela Yao", "title": "Scale-aware multi-level guidance for interactive instance segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In interactive instance segmentation, users give feedback to iteratively\nrefine segmentation masks. The user-provided clicks are transformed into\nguidance maps which provide the network with necessary cues on the whereabouts\nof the object of interest. Guidance maps used in current systems are purely\ndistance-based and are either too localized or non-informative. We propose a\nnovel transformation of user clicks to generate scale-aware guidance maps that\nleverage the hierarchical structural information present in an image. Using our\nguidance maps, even the most basic FCNs are able to outperform existing\napproaches that require state-of-the-art segmentation networks pre-trained on\nlarge scale segmentation datasets. We demonstrate the effectiveness of our\nproposed transformation strategy through comprehensive experimentation in which\nwe significantly raise state-of-the-art on four standard interactive\nsegmentation benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 10:19:01 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Majumder", "Soumajit", ""], ["Yao", "Angela", ""]]}, {"id": "1812.02984", "submitter": "Ehsan Pajouheshgar", "authors": "Ehsan Pajouheshgar, Christoph H. Lampert", "title": "Back to square one: probabilistic trajectory forecasting without bells\n  and whistles", "comments": "4 pages, 3 figures, Workshop on Modeling and Decision-Making in the\n  Spatiotemporal Domain, 32nd Conference on Neural Information Processing\n  Systems (NIPS 2018), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a spatio-temporal convolutional neural network model for\ntrajectory forecasting from visual sources. Applied in an auto-regressive way\nit provides an explicit probability distribution over continuations of a given\ninitial trajectory segment. We discuss it in relation to (more complicated)\nexisting work and report on experiments on two standard datasets for trajectory\nforecasting: MNISTseq and Stanford Drones, achieving results on-par with or\nbetter than previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 11:31:05 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Pajouheshgar", "Ehsan", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1812.03015", "submitter": "Zunjie Zhu", "authors": "Zunjie Zhu and Feng Xu", "title": "Real-time Indoor Scene Reconstruction with RGBD and Inertia Input", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera motion estimation is a key technique for 3D scene reconstruction and\nSimultaneous localization and mapping (SLAM). To make it be feasibly achieved,\nprevious works usually assume slow camera motions, which limits its usage in\nmany real cases. We propose an end-to-end 3D reconstruction system which\ncombines color, depth and inertial measurements to achieve robust\nreconstruction with fast sensor motions. Our framework extends Kalman filter to\nfuse the three kinds of information and involve an iterative method to jointly\noptimize feature correspondences, camera poses and scene geometry. We also\npropose a novel geometry-aware patch deformation technique to adapt the feature\nappearance in image domain, leading to a more accurate feature matching under\nfast camera motions. Experiments show that our patch deformation method\nimproves the accuracy of feature tracking, and our 3D reconstruction\noutperforms the state-of-the-art solutions under fast camera motions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 13:07:36 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Zhu", "Zunjie", ""], ["Xu", "Feng", ""]]}, {"id": "1812.03026", "submitter": "Silvia Tozza", "authors": "Maurizio Falcone, Giulio Paolucci, Silvia Tozza", "title": "A High-Order Scheme for Image Segmentation via a modified Level-Set\n  method", "comments": "Accepted version for publication in SIAM Journal on Imaging Sciences,\n  86 figures", "journal-ref": null, "doi": null, "report-no": "Roma01.Math.NA", "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a high-order accurate scheme for image segmentation\nbased on the level-set method. In this approach, the curve evolution is\ndescribed as the 0-level set of a representation function but we modify the\nvelocity that drives the curve to the boundary of the object in order to obtain\na new velocity with additional properties that are extremely useful to develop\na more stable high-order approximation with a small additional cost. The\napproximation scheme proposed here is the first 2D version of an adaptive\n\"filtered\" scheme recently introduced and analyzed by the authors in 1D. This\napproach is interesting since the implementation of the filtered scheme is\nrather efficient and easy. The scheme combines two building blocks (a monotone\nscheme and a high-order scheme) via a filter function and smoothness indicators\nthat allow to detect the regularity of the approximate solution adapting the\nscheme in an automatic way. Some numerical tests on synthetic and real images\nconfirm the accuracy of the proposed method and the advantages given by the new\nvelocity.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 13:50:17 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 13:27:25 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Falcone", "Maurizio", ""], ["Paolucci", "Giulio", ""], ["Tozza", "Silvia", ""]]}, {"id": "1812.03050", "submitter": "Lisa Tse", "authors": "Lisa Tse, Peter Mountney, Paul Klein, Simone Severini", "title": "Graph Cut Segmentation Methods Revisited with a Quantum Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and performance of computer vision algorithms are greatly\ninfluenced by the hardware on which they are implemented. CPUs, multi-core\nCPUs, FPGAs and GPUs have inspired new algorithms and enabled existing ideas to\nbe realized. This is notably the case with GPUs, which has significantly\nchanged the landscape of computer vision research through deep learning. As the\nend of Moores law approaches, researchers and hardware manufacturers are\nexploring alternative hardware computing paradigms. Quantum computers are a\nvery promising alternative and offer polynomial or even exponential speed-ups\nover conventional computing for some problems. This paper presents a novel\napproach to image segmentation that uses new quantum computing hardware.\nSegmentation is formulated as a graph cut problem that can be mapped to the\nquantum approximate optimization algorithm (QAOA). This algorithm can be\nimplemented on current and near-term quantum computers. Encouraging results are\npresented on artificial and medical imaging data. This represents an important,\npractical step towards leveraging quantum computers for computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 14:53:16 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 16:32:18 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Tse", "Lisa", ""], ["Mountney", "Peter", ""], ["Klein", "Paul", ""], ["Severini", "Simone", ""]]}, {"id": "1812.03079", "submitter": "Mayank Bansal PhD", "authors": "Mayank Bansal, Alex Krizhevsky, Abhijit Ogale", "title": "ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing\n  the Worst", "comments": "Video results: https://sites.google.com/view/waymo-learn-to-drive", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to train a policy for autonomous driving via imitation learning\nthat is robust enough to drive a real vehicle. We find that standard behavior\ncloning is insufficient for handling complex driving scenarios, even when we\nleverage a perception system for preprocessing the input and a controller for\nexecuting the output on the car: 30 million examples are still not enough. We\npropose exposing the learner to synthesized data in the form of perturbations\nto the expert's driving, which creates interesting situations such as\ncollisions and/or going off the road. Rather than purely imitating all data, we\naugment the imitation loss with additional losses that penalize undesirable\nevents and encourage progress -- the perturbations then provide an important\nsignal for these losses and lead to robustness of the learned model. We show\nthat the ChauffeurNet model can handle complex situations in simulation, and\npresent ablation experiments that emphasize the importance of each of our\nproposed changes and show that the model is responding to the appropriate\ncausal factors. Finally, we demonstrate the model driving a car in the real\nworld.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 16:04:00 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Bansal", "Mayank", ""], ["Krizhevsky", "Alex", ""], ["Ogale", "Abhijit", ""]]}, {"id": "1812.03085", "submitter": "Anil Baslamisli", "authors": "Partha Das, Anil S. Baslamisli, Yang Liu, Sezer Karaoglu, Theo Gevers", "title": "Color Constancy by GANs: An Experimental Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formulate the color constancy task as an image-to-image\ntranslation problem using GANs. By conducting a large set of experiments on\ndifferent datasets, an experimental survey is provided on the use of different\ntypes of GANs to solve for color constancy i.e. CC-GANs (Color Constancy GANs).\nBased on the experimental review, recommendations are given for the design of\nCC-GAN architectures based on different criteria, circumstances and datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 16:20:51 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Das", "Partha", ""], ["Baslamisli", "Anil S.", ""], ["Liu", "Yang", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""]]}, {"id": "1812.03115", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su and Kristen Grauman", "title": "Kernel Transformer Networks for Compact Spherical Convolution", "comments": "In Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideally, 360{\\deg} imagery could inherit the deep convolutional neural\nnetworks (CNNs) already trained with great success on perspective projection\nimages. However, existing methods to transfer CNNs from perspective to\nspherical images introduce significant computational costs and/or degradations\nin accuracy. In this work, we present the Kernel Transformer Network (KTN).\nKTNs efficiently transfer convolution kernels from perspective images to the\nequirectangular projection of 360{\\deg} images. Given a source CNN for\nperspective images as input, the KTN produces a function parameterized by a\npolar angle and kernel as output. Given a novel 360{\\deg} image, that function\nin turn can compute convolutions for arbitrary layers and kernels as would the\nsource CNN on the corresponding tangent plane projections. Distinct from all\nexisting methods, KTNs allow model transfer: the same model can be applied to\ndifferent source CNNs with the same base architecture. This enables application\nto multiple recognition tasks without re-training the KTN. Validating our\napproach with multiple source CNNs and datasets, we show that KTNs improve the\nstate of the art for spherical convolution. KTNs successfully preserve the\nsource CNN's accuracy, while offering transferability, scalability to typical\nimage resolutions, and, in many cases, a substantially lower memory footprint.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 17:26:28 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 15:46:38 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1812.03128", "submitter": "Walter Scheirer", "authors": "Jacob Dumford, Walter Scheirer", "title": "Backdooring Convolutional Neural Networks via Targeted Weight\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new type of backdoor attack that exploits a vulnerability of\nconvolutional neural networks (CNNs) that has been previously unstudied. In\nparticular, we examine the application of facial recognition. Deep learning\ntechniques are at the top of the game for facial recognition, which means they\nhave now been implemented in many production-level systems. Alarmingly, unlike\nother commercial technologies such as operating systems and network devices,\ndeep learning-based facial recognition algorithms are not presently designed\nwith security requirements or audited for security vulnerabilities before\ndeployment. Given how young the technology is and how abstract many of the\ninternal workings of these algorithms are, neural network-based facial\nrecognition systems are prime targets for security breaches. As more and more\nof our personal information begins to be guarded by facial recognition (e.g.,\nthe iPhone X), exploring the security vulnerabilities of these systems from a\npenetration testing standpoint is crucial. Along these lines, we describe a\ngeneral methodology for backdooring CNNs via targeted weight perturbations.\nUsing a five-layer CNN and ResNet-50 as case studies, we show that an attacker\nis able to significantly increase the chance that inputs they supply will be\nfalsely accepted by a CNN while simultaneously preserving the error rates for\nlegitimate enrolled classes.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 17:41:40 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Dumford", "Jacob", ""], ["Scheirer", "Walter", ""]]}, {"id": "1812.03170", "submitter": "Jason Ramapuram", "authors": "Jason Ramapuram, Maurits Diephuis, Frantzeska Lavda, Russ Webb,\n  Alexandros Kalousis", "title": "Variational Saccading: Efficient Inference for Large Resolution Images", "comments": "Published BMVC 2019 & NIPS 2018 Bayesian Deep Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification with deep neural networks is typically restricted to\nimages of small dimensionality such as 224 x 244 in Resnet models [24]. This\nlimitation excludes the 4000 x 3000 dimensional images that are taken by modern\nsmartphone cameras and smart devices. In this work, we aim to mitigate the\nprohibitive inferential and memory costs of operating in such large dimensional\nspaces. To sample from the high-resolution original input distribution, we\npropose using a smaller proxy distribution to learn the co-ordinates that\ncorrespond to regions of interest in the high-dimensional space. We introduce a\nnew principled variational lower bound that captures the relationship of the\nproxy distribution's posterior and the original image's co-ordinate space in a\nway that maximizes the conditional classification likelihood. We empirically\ndemonstrate on one synthetic benchmark and one real world large resolution DSLR\ncamera image dataset that our method produces comparable results with ~10x\nfaster inference and lower memory consumption than a model that utilizes the\nentire original input distribution. Finally, we experiment with a more complex\nsetting using mini-maps from Starcraft II [56] to infer the number of\ncharacters in a complex 3d-rendered scene. Even in such complicated scenes our\nmodel provides strong localization: a feature missing from traditional\nclassification models.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 16:53:02 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 14:53:06 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 11:41:23 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Ramapuram", "Jason", ""], ["Diephuis", "Maurits", ""], ["Lavda", "Frantzeska", ""], ["Webb", "Russ", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1812.03186", "submitter": "Dhruv Gosain", "authors": "Dhruv Gosain, Rishabh Joshi", "title": "Removal of Parameter Adjustment of Frangi Filters in Case of Coronary\n  Angiograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frangi Filters are one of the widely used filters for enhancing vessels in\nmedical images. Since they were first proposed, the threshold of the vesselness\nfunction of Frangi Filters is to be arranged for each individual application.\nThese thresholds are changed manually for individual fluoroscope, for enhancing\ncoronary angiogram images. Hence it is felt, there is a need of mitigating the\ntuning procedure of threshold values for every fluoroscope. The current papers\napproach has been devised in order to treat the coronary angiogram images\nuniformly, irrespective of the fluoroscopes through which they were obtained\nand the patient demographics for further stenosis detection. This problem to\nthe best of our knowledge has not been addressed yet. In the approach, before\nfeeding the image to Frangi Filters, non uniform illumination of the input\nimage is removed using homomorphic filters and the image is enhanced using Non\nSubsampled Contourlet Transform (NSCT). The experiment was conducted on the\ndata that has been accumulated from various hospitals in India and the results\nobtained verifies dependency removal of parameters without compromising the\nresults obtained by Frangi filters.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 19:10:01 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Gosain", "Dhruv", ""], ["Joshi", "Rishabh", ""]]}, {"id": "1812.03205", "submitter": "Matej Ulicny", "authors": "Matej Ulicny, Vladimir A. Krylov, Rozenn Dahyot", "title": "Harmonic Networks: Integrating Spectral Information into CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) learn filters in order to capture local\ncorrelation patterns in feature space. In contrast, in this paper we propose\nharmonic blocks that produce features by learning optimal combinations of\nspectral filters defined by the Discrete Cosine Transform. The harmonic blocks\nare used to replace conventional convolutional layers to construct partial or\nfully harmonic CNNs. We extensively validate our approach and show that the\nintroduction of harmonic blocks into state-of-the-art CNN baseline\narchitectures results in comparable or better performance in classification\ntasks on small NORB, CIFAR10 and CIFAR100 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 20:21:48 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Ulicny", "Matej", ""], ["Krylov", "Vladimir A.", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "1812.03213", "submitter": "Rama Kovvuri", "authors": "Rama Kovvuri, Ram Nevatia", "title": "PIRC Net : Using Proposal Indexing, Relationships and Context for Phrase\n  Grounding", "comments": "Accepted in ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Phrase Grounding aims to detect and localize objects in images that are\nreferred to and are queried by natural language phrases. Phrase grounding finds\napplications in tasks such as Visual Dialog, Visual Search and Image-text\nco-reference resolution. In this paper, we present a framework that leverages\ninformation such as phrase category, relationships among neighboring phrases in\na sentence and context to improve the performance of phrase grounding systems.\nWe propose three modules: Proposal Indexing Network(PIN); Inter-phrase\nRegression Network(IRN) and Proposal Ranking Network(PRN) each of which analyze\nthe region proposals of an image at increasing levels of detail by\nincorporating the above information. Also, in the absence of ground-truth\nspatial locations of the phrases(weakly-supervised), we propose knowledge\ntransfer mechanisms that leverages the framework of PIN module. We demonstrate\nthe effectiveness of our approach on the Flickr 30k Entities and ReferItGame\ndatasets, for which we achieve improvements over state-of-the-art approaches in\nboth supervised and weakly-supervised variants.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 20:50:59 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Kovvuri", "Rama", ""], ["Nevatia", "Ram", ""]]}, {"id": "1812.03245", "submitter": "Daniel DeTone", "authors": "Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich", "title": "Self-Improving Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised learning framework that uses unlabeled monocular\nvideo sequences to generate large-scale supervision for training a Visual\nOdometry (VO) frontend, a network which computes pointwise data associations\nacross images. Our self-improving method enables a VO frontend to learn over\ntime, unlike other VO and SLAM systems which require time-consuming hand-tuning\nor expensive data collection to adapt to new environments. Our proposed\nfrontend operates on monocular images and consists of a single multi-task\nconvolutional neural network which outputs 2D keypoints locations, keypoint\ndescriptors, and a novel point stability score. We use the output of VO to\ncreate a self-supervised dataset of point correspondences to retrain the\nfrontend. When trained using VO at scale on 2.5 million monocular images from\nScanNet, the stability classifier automatically discovers a ranking for\nkeypoints that are not likely to help in VO, such as t-junctions across depth\ndiscontinuities, features on shadows and highlights, and dynamic objects like\npeople. The resulting frontend outperforms both traditional methods (SIFT, ORB,\nAKAZE) and deep learning methods (SuperPoint and LF-Net) in a 3D-to-2D pose\nestimation task on ScanNet.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 00:26:40 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["DeTone", "Daniel", ""], ["Malisiewicz", "Tomasz", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1812.03247", "submitter": "Daniel DeTone", "authors": "Danying Hu, Daniel DeTone, Vikram Chauhan, Igor Spivak, Tomasz\n  Malisiewicz", "title": "Deep ChArUco: Dark ChArUco Marker Pose Estimation", "comments": "Published in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ChArUco boards are used for camera calibration, monocular pose estimation,\nand pose verification in both robotics and augmented reality. Such fiducials\nare detectable via traditional computer vision methods (as found in OpenCV) in\nwell-lit environments, but classical methods fail when the lighting is poor or\nwhen the image undergoes extreme motion blur. We present Deep ChArUco, a\nreal-time pose estimation system which combines two custom deep networks,\nChArUcoNet and RefineNet, with the Perspective-n-Point (PnP) algorithm to\nestimate the marker's 6DoF pose. ChArUcoNet is a two-headed marker-specific\nconvolutional neural network (CNN) which jointly outputs ID-specific\nclassifiers and 2D point locations. The 2D point locations are further refined\ninto subpixel coordinates using RefineNet. Our networks are trained using a\ncombination of auto-labeled videos of the target marker, synthetic subpixel\ncorner data, and extreme data augmentation. We evaluate Deep ChArUco in\nchallenging low-light, high-motion, high-blur scenarios and demonstrate that\nour approach is superior to a traditional OpenCV-based method for ChArUco\nmarker detection and pose estimation.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 00:52:47 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 00:35:41 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Hu", "Danying", ""], ["DeTone", "Daniel", ""], ["Chauhan", "Vikram", ""], ["Spivak", "Igor", ""], ["Malisiewicz", "Tomasz", ""]]}, {"id": "1812.03252", "submitter": "Haofu Liao", "authors": "Haofu Liao, Gareth Funka-Lea, Yefeng Zheng, Jiebo Luo, S. Kevin Zhou", "title": "Face Completion with Semantic Knowledge and Collaborative Adversarial\n  Learning", "comments": "To be appear in ACCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unlike a conventional background inpainting approach that infers a missing\narea from image patches similar to the background, face completion requires\nsemantic knowledge about the target object for realistic outputs. Current image\ninpainting approaches utilize generative adversarial networks (GANs) to achieve\nsuch semantic understanding. However, in adversarial learning, the semantic\nknowledge is learned implicitly and hence good semantic understanding is not\nalways guaranteed. In this work, we propose a collaborative adversarial\nlearning approach to face completion to explicitly induce the training process.\nOur method is formulated under a novel generative framework called\ncollaborative GAN (collaGAN), which allows better semantic understanding of a\ntarget object through collaborative learning of multiple tasks including face\ncompletion, landmark detection, and semantic segmentation. Together with the\ncollaGAN, we also introduce an inpainting concentrated scheme such that the\nmodel emphasizes more on inpainting instead of autoencoding. Extensive\nexperiments show that the proposed designs are indeed effective and\ncollaborative adversarial learning provides better feature representations of\nthe faces. In comparison with other generative image inpainting models and\nsingle task learning methods, our solution produces superior performances on\nall tasks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 01:42:40 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 03:40:33 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Liao", "Haofu", ""], ["Funka-Lea", "Gareth", ""], ["Zheng", "Yefeng", ""], ["Luo", "Jiebo", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "1812.03264", "submitter": "Bo Li", "authors": "Bo Li, Caiming Xiong, Tianfu Wu, Yu Zhou, Lun Zhang, Rufeng Chu", "title": "Neural Abstract Style Transfer for Chinese Traditional Painting", "comments": "Conference: ACCV 2018. Project Page:\n  https://github.com/lbsswu/Chinese_style_transfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese traditional painting is one of the most historical artworks in the\nworld. It is very popular in Eastern and Southeast Asia due to being\naesthetically appealing. Compared with western artistic painting, it is usually\nmore visually abstract and textureless. Recently, neural network based style\ntransfer methods have shown promising and appealing results which are mainly\nfocused on western painting. It remains a challenging problem to preserve\nabstraction in neural style transfer. In this paper, we present a Neural\nAbstract Style Transfer method for Chinese traditional painting. It learns to\npreserve abstraction and other style jointly end-to-end via a novel\nMXDoG-guided filter (Modified version of the eXtended Difference-of-Gaussians)\nand three fully differentiable loss terms. To the best of our knowledge, there\nis little work study on neural style transfer of Chinese traditional painting.\nTo promote research on this direction, we collect a new dataset with diverse\nphoto-realistic images and Chinese traditional paintings. In experiments, the\nproposed method shows more appealing stylized results in transferring the style\nof Chinese traditional painting than state-of-the-art neural style transfer\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 04:18:49 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 00:43:03 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Li", "Bo", ""], ["Xiong", "Caiming", ""], ["Wu", "Tianfu", ""], ["Zhou", "Yu", ""], ["Zhang", "Lun", ""], ["Chu", "Rufeng", ""]]}, {"id": "1812.03278", "submitter": "Fang Liu", "authors": "Fang Liu, Lihua Chen, Richard Kijowski, Li Feng", "title": "SANTIS: Sampling-Augmented Neural neTwork with Incoherent Structure for\n  MR image reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning holds great promise in the reconstruction of undersampled\nMagnetic Resonance Imaging (MRI) data, providing new opportunities to escalate\nthe performance of rapid MRI. In existing deep learning-based reconstruction\nmethods, supervised training is performed using artifact-free reference images\nand their corresponding undersampled pairs. The undersampled images are\ngenerated by a fixed undersampling pattern in the training, and the trained\nnetwork is then applied to reconstruct new images acquired with the same\npattern in the inference. While such a training strategy can maintain a\nfavorable reconstruction for a pre-selected undersampling pattern, the\nrobustness of the trained network against any discrepancy of undersampling\nschemes is typically poor. We developed a novel deep learning-based\nreconstruction framework called SANTIS for efficient MR image reconstruction\nwith improved robustness against sampling pattern discrepancy. SANTIS uses a\ndata cycle-consistent adversarial network combining efficient end-to-end\nconvolutional neural network mapping, data fidelity enforcement and adversarial\ntraining for reconstructing accelerated MR images more faithfully. A training\nstrategy employing sampling augmentation with extensive variation of\nundersampling patterns was further introduced to promote the robustness of the\ntrained network. Compared to conventional reconstruction and standard deep\nlearning methods, SANTIS achieved consistent better reconstruction performance,\nwith lower errors, greater image sharpness and higher similarity with respect\nto the reference regardless of the undersampling patterns during inference.\nThis novel concept behind SANTIS can particularly be useful towards improving\nthe robustness of deep learning-based image reconstruction against discrepancy\nbetween training and evaluation, which is currently an important but less\nstudied open question.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 07:36:21 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Liu", "Fang", ""], ["Chen", "Lihua", ""], ["Kijowski", "Richard", ""], ["Feng", "Li", ""]]}, {"id": "1812.03282", "submitter": "Guangcong Wang", "authors": "Guangcong Wang, Jianhuang Lai, Peigen Huang, Xiaohua Xie", "title": "Spatial-Temporal Person Re-identification", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of current person re-identification (ReID) methods neglect a\nspatial-temporal constraint. Given a query image, conventional methods compute\nthe feature distances between the query image and all the gallery images and\nreturn a similarity ranked table. When the gallery database is very large in\npractice, these approaches fail to obtain a good performance due to appearance\nambiguity across different camera views. In this paper, we propose a novel\ntwo-stream spatial-temporal person ReID (st-ReID) framework that mines both\nvisual semantic information and spatial-temporal information. To this end, a\njoint similarity metric with Logistic Smoothing (LS) is introduced to integrate\ntwo kinds of heterogeneous information into a unified framework. To approximate\na complex spatial-temporal probability distribution, we develop a fast\nHistogram-Parzen (HP) method. With the help of the spatial-temporal constraint,\nthe st-ReID model eliminates lots of irrelevant images and thus narrows the\ngallery database. Without bells and whistles, our st-ReID method achieves\nrank-1 accuracy of 98.1\\% on Market-1501 and 94.4\\% on DukeMTMC-reID, improving\nfrom the baselines 91.2\\% and 83.8\\%, respectively, outperforming all previous\nstate-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 08:09:58 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Wang", "Guangcong", ""], ["Lai", "Jianhuang", ""], ["Huang", "Peigen", ""], ["Xie", "Xiaohua", ""]]}, {"id": "1812.03283", "submitter": "Jiajun Du", "authors": "Jiajun Du, Yu Qin, Hongtao Lu, Yonghua Zhang", "title": "Attend More Times for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most attention-based image captioning models attend to the image once per\nword. However, attending once per word is rigid and is easy to miss some\ninformation. Attending more times can adjust the attention position, find the\nmissing information back and avoid generating the wrong word. In this paper, we\nshow that attending more times per word can gain improvements in the image\ncaptioning task, without increasing the number of parameters. We propose a\nflexible two-LSTM merge model to make it convenient to encode more attentions\nthan words. Our captioning model uses two LSTMs to encode the word sequence and\nthe attention sequence respectively. The information of the two LSTMs and the\nimage feature are combined to predict the next word. Experiments on the MSCOCO\ncaption dataset show that our method outperforms the state-of-the-art. Using\nbottom up features and self-critical training method, our method gets BLEU-4,\nMETEOR, ROUGE-L, CIDEr and SPICE scores of 0.381, 0.283, 0.580, 1.261 and 0.220\non the Karpathy test split.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 08:23:33 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 12:10:54 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Du", "Jiajun", ""], ["Qin", "Yu", ""], ["Lu", "Hongtao", ""], ["Zhang", "Yonghua", ""]]}, {"id": "1812.03299", "submitter": "Daqing Liu", "authors": "Daqing Liu, Hanwang Zhang, Feng Wu, Zheng-Jun Zha", "title": "Learning to Assemble Neural Module Tree Networks for Visual Grounding", "comments": "Accepted at ICCV 2019 (Oral); Code available at\n  https://github.com/daqingliu/NMTree", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual grounding, a task to ground (i.e., localize) natural language in\nimages, essentially requires composite visual reasoning. However, existing\nmethods over-simplify the composite nature of language into a monolithic\nsentence embedding or a coarse composition of subject-predicate-object triplet.\nIn this paper, we propose to ground natural language in an intuitive,\nexplainable, and composite fashion as it should be. In particular, we develop a\nnovel modular network called Neural Module Tree network (NMTree) that\nregularizes the visual grounding along the dependency parsing tree of the\nsentence, where each node is a neural module that calculates visual attention\naccording to its linguistic feature, and the grounding score is accumulated in\na bottom-up direction where as needed. NMTree disentangles the visual grounding\nfrom the composite reasoning, allowing the former to only focus on primitive\nand easy-to-generalize patterns. To reduce the impact of parsing errors, we\ntrain the modules and their assembly end-to-end by using the Gumbel-Softmax\napproximation and its straight-through gradient estimator, accounting for the\ndiscrete nature of module assembly. Overall, the proposed NMTree consistently\noutperforms the state-of-the-arts on several benchmarks. Qualitative results\nshow explainable grounding score calculation in great detail.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 11:04:34 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 08:47:37 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 12:31:10 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Liu", "Daqing", ""], ["Zhang", "Hanwang", ""], ["Wu", "Feng", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "1812.03303", "submitter": "Stefanos Pertigkiozoglou", "authors": "Stefanos Pertigkiozoglou, Petros Maragos", "title": "Detecting Adversarial Examples in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The great success of convolutional neural networks has caused a massive\nspread of the use of such models in a large variety of Computer Vision\napplications. However, these models are vulnerable to certain inputs, the\nadversarial examples, which although are not easily perceived by humans, they\ncan lead a neural network to produce faulty results. This paper focuses on the\ndetection of adversarial examples, which are created for convolutional neural\nnetworks that perform image classification. We propose three methods for\ndetecting possible adversarial examples and after we analyze and compare their\nperformance, we combine their best aspects to develop an even more robust\napproach. The first proposed method is based on the regularization of the\nfeature vector that the neural network produces as output. The second method\ndetects adversarial examples by using histograms, which are created from the\noutputs of the hidden layers of the neural network. These histograms create a\nfeature vector which is used as the input of an SVM classifier, which\nclassifies the original input either as an adversarial or as a real input.\nFinally, for the third method we introduce the concept of the residual image,\nwhich contains information about the parts of the input pattern that are\nignored by the neural network. This method aims at the detection of possible\nadversarial examples, by using the residual image and reinforcing the parts of\nthe input pattern that are ignored by the neural network. Each one of these\nmethods has some novelties and by combining them we can further improve the\ndetection results. For the proposed methods and their combination, we present\nthe results of detecting adversarial examples on the MNIST dataset. The\ncombination of the proposed methods offers some improvements over similar state\nof the art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 11:52:43 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Pertigkiozoglou", "Stefanos", ""], ["Maragos", "Petros", ""]]}, {"id": "1812.03320", "submitter": "Li Yi", "authors": "Li Yi, Wang Zhao, He Wang, Minhyuk Sung, Leonidas Guibas", "title": "GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in\n  Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel 3D object proposal approach named Generative Shape\nProposal Network (GSPN) for instance segmentation in point cloud data. Instead\nof treating object proposal as a direct bounding box regression problem, we\ntake an analysis-by-synthesis strategy and generate proposals by reconstructing\nshapes from noisy observations in a scene. We incorporate GSPN into a novel 3D\ninstance segmentation framework named Region-based PointNet (R-PointNet) which\nallows flexible proposal refinement and instance segmentation generation. We\nachieve state-of-the-art performance on several 3D instance segmentation tasks.\nThe success of GSPN largely comes from its emphasis on geometric understandings\nduring object proposal, which greatly reducing proposals with low objectness.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 13:41:05 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Yi", "Li", ""], ["Zhao", "Wang", ""], ["Wang", "He", ""], ["Sung", "Minhyuk", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1812.03368", "submitter": "Lipu Zhou", "authors": "Lipu Zhou, Jiamin Ye, Montiel Abello, Shengze Wang, Michael Kaess", "title": "Unsupervised Learning of Monocular Depth Estimation with Bundle\n  Adjustment, Super-Resolution and Clip Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel unsupervised learning framework for single view depth\nestimation using monocular videos. It is well known in 3D vision that enlarging\nthe baseline can increase the depth estimation accuracy, and jointly optimizing\na set of camera poses and landmarks is essential. In previous monocular\nunsupervised learning frameworks, only part of the photometric and geometric\nconstraints within a sequence are used as supervisory signals. This may result\nin a short baseline and overfitting. Besides, previous works generally estimate\na low resolution depth from a low resolution impute image. The low resolution\ndepth is then interpolated to recover the original resolution. This strategy\nmay generate large errors on object boundaries, as the depth of background and\nforeground are mixed to yield the high resolution depth. In this paper, we\nintroduce a bundle adjustment framework and a super-resolution network to solve\nthe above two problems. In bundle adjustment, depths and poses of an image\nsequence are jointly optimized, which increases the baseline by establishing\nthe relationship between farther frames. The super resolution network learns to\nestimate a high resolution depth from a low resolution image. Additionally, we\nintroduce the clip loss to deal with moving objects and occlusion. Experimental\nresults on the KITTI dataset show that the proposed algorithm outperforms the\nstate-of-the-art unsupervised methods using monocular sequences, and achieves\ncomparable or even better result compared to unsupervised methods using stereo\nsequences.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 18:59:29 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Zhou", "Lipu", ""], ["Ye", "Jiamin", ""], ["Abello", "Montiel", ""], ["Wang", "Shengze", ""], ["Kaess", "Michael", ""]]}, {"id": "1812.03385", "submitter": "Rahul Jaiswal", "authors": "Rahul Kumar Jaiswal, Gaurav Saxena", "title": "Biometric Recognition System (Algorithm)", "comments": "Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprints are the most widely deployed form of biometric identification.\nNo two individuals share the same fingerprint because they have unique\nbiometric identifiers. This paper presents an efficient fingerprint\nverification algorithm which improves matching accuracy. Fingerprint images get\ndegraded and corrupted due to variations in skin and impression conditions.\nThus, image enhancement techniques are employed prior to singular point\ndetection and minutiae extraction. Singular point is the point of maximum\ncurvature. It is determined by the normal of each fingerprint ridge, and then\nfollowing them inward towards the centre. The local ridge features known as\nminutiae is extracted using cross-number method to find ridge endings and ridge\nbifurcations. The proposed algorithm chooses a radius and draws a circle with\ncore point as centre, making fingerprint images rotationally invariant and\nuniform. The radius can be varied according to the accuracy depending on the\nparticular application. Morphological techniques such as clean, spur and\nH-break is employed to remove noise, followed by removing spurious minutiae.\nTemplates are created based on feature vector extraction and databases are made\nfor verification and identification for the fingerprint images taken from\nFingerprint Verification Competition (FVC2002). Minimum Euclidean distance is\ncalculated between saved template and the test fingerprint image template and\ncompared with the set threshold for matching decision. For the performance\nevaluation of the proposed algorithm various measures, equal error rate (EER),\nDmin at EER, accuracy and threshold are evaluated and plotted. The measures\ndemonstrate that the proposed algorithm is more effective and robust.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 21:12:38 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Jaiswal", "Rahul Kumar", ""], ["Saxena", "Gaurav", ""]]}, {"id": "1812.03402", "submitter": "Karan Sikka", "authors": "Zachary Seymour, Karan Sikka, Han-Pang Chiu, Supun Samarasekera,\n  Rakesh Kumar", "title": "Semantically-Aware Attentive Neural Embeddings for Image-based Visual\n  Localization", "comments": "Appearing in BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach that combines appearance and semantic information for\n2D image-based localization (2D-VL) across large perceptual changes and time\nlags. Compared to appearance features, the semantic layout of a scene is\ngenerally more invariant to appearance variations. We use this intuition and\npropose a novel end-to-end deep attention-based framework that utilizes\nmultimodal cues to generate robust embeddings for 2D-VL. The proposed attention\nmodule predicts a shared channel attention and modality-specific spatial\nattentions to guide the embeddings to focus on more reliable image regions. We\nevaluate our model against state-of-the-art (SOTA) methods on three challenging\nlocalization datasets. We report an average (absolute) improvement of $19\\%$\nover current SOTA for 2D-VL. Furthermore, we present an extensive study\ndemonstrating the contribution of each component of our model, showing\n$8$--$15\\%$ and $4\\%$ improvement from adding semantic information and our\nproposed attention module. We finally show the predicted attention maps to\noffer useful insights into our model.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 22:57:17 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 21:47:29 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Seymour", "Zachary", ""], ["Sikka", "Karan", ""], ["Chiu", "Han-Pang", ""], ["Samarasekera", "Supun", ""], ["Kumar", "Rakesh", ""]]}, {"id": "1812.03405", "submitter": "Blerta Lindqvist", "authors": "Blerta Lindqvist, Shridatt Sugrim, Rauf Izmailov", "title": "AutoGAN: Robust Classifier Against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers fail to classify correctly input images that have been\npurposefully and imperceptibly perturbed to cause misclassification. This\nsusceptability has been shown to be consistent across classifiers, regardless\nof their type, architecture or parameters. Common defenses against adversarial\nattacks modify the classifer boundary by training on additional adversarial\nexamples created in various ways. In this paper, we introduce AutoGAN, which\ncounters adversarial attacks by enhancing the lower-dimensional manifold\ndefined by the training data and by projecting perturbed data points onto it.\nAutoGAN mitigates the need for knowing the attack type and magnitude as well as\nthe need for having adversarial samples of the attack. Our approach uses a\nGenerative Adversarial Network (GAN) with an autoencoder generator and a\ndiscriminator that also serves as a classifier. We test AutoGAN against\nadversarial samples generated with state-of-the-art Fast Gradient Sign Method\n(FGSM) as well as samples generated with random Gaussian noise, both using the\nMNIST dataset. For different magnitudes of perturbation in training and\ntesting, AutoGAN can surpass the accuracy of FGSM method by up to 25\\% points\non samples perturbed using FGSM. Without an augmented training dataset, AutoGAN\nachieves an accuracy of 89\\% compared to 1\\% achieved by FGSM method on FGSM\ntesting adversarial samples.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 23:50:11 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Lindqvist", "Blerta", ""], ["Sugrim", "Shridatt", ""], ["Izmailov", "Rauf", ""]]}, {"id": "1812.03407", "submitter": "Chi Nhan Duong", "authors": "Thanh-Dat Truong, Chi Nhan Duong, Khoa Luu, Minh-Triet Tran, Minh Do", "title": "Beyond Domain Adaptation: Unseen Domain Encapsulation via Universal\n  Non-volume Preserving Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition across domains has recently become an active topic in the\nresearch community. However, it has been largely overlooked in the problem of\nrecognition in new unseen domains. Under this condition, the delivered deep\nnetwork models are unable to be updated, adapted or fine-tuned. Therefore,\nrecent deep learning techniques, such as: domain adaptation, feature\ntransferring, and fine-tuning, cannot be applied. This paper presents a novel\nUniversal Non-volume Preserving approach to the problem of domain\ngeneralization in the context of deep learning. The proposed method can be\neasily incorporated with any other ConvNet framework within an end-to-end deep\nnetwork design to improve the performance. On digit recognition, we benchmark\non four popular digit recognition databases, i.e. MNIST, USPS, SVHN and\nMNIST-M. The proposed method is also experimented on face recognition on\nExtended Yale-B, CMU-PIE and CMU-MPIE databases and compared against other the\nstate-of-the-art methods. In the problem of pedestrian detection, we\nempirically observe that the proposed method learns models that improve\nperformance across a priori unknown data distributions.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 00:10:38 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Truong", "Thanh-Dat", ""], ["Duong", "Chi Nhan", ""], ["Luu", "Khoa", ""], ["Tran", "Minh-Triet", ""], ["Do", "Minh", ""]]}, {"id": "1812.03411", "submitter": "Cihang Xie", "authors": "Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He", "title": "Feature Denoising for Improving Adversarial Robustness", "comments": "CVPR 2019, code is available at:\n  https://github.com/facebookresearch/ImageNet-Adversarial-Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks to image classification systems present challenges to\nconvolutional networks and opportunities for understanding them. This study\nsuggests that adversarial perturbations on images lead to noise in the features\nconstructed by these networks. Motivated by this observation, we develop new\nnetwork architectures that increase adversarial robustness by performing\nfeature denoising. Specifically, our networks contain blocks that denoise the\nfeatures using non-local means or other filters; the entire networks are\ntrained end-to-end. When combined with adversarial training, our feature\ndenoising networks substantially improve the state-of-the-art in adversarial\nrobustness in both white-box and black-box attack settings. On ImageNet, under\n10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our\nmethod achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks,\nour method secures 42.6% accuracy. Our method was ranked first in Competition\non Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6%\nclassification accuracy on a secret, ImageNet-like test dataset against 48\nunknown attackers, surpassing the runner-up approach by ~10%. Code is available\nat https://github.com/facebookresearch/ImageNet-Adversarial-Training.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 01:55:31 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 17:44:19 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Xie", "Cihang", ""], ["Wu", "Yuxin", ""], ["van der Maaten", "Laurens", ""], ["Yuille", "Alan", ""], ["He", "Kaiming", ""]]}, {"id": "1812.03413", "submitter": "Yingwei Li", "authors": "Yingwei Li, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, Alan\n  Yuille", "title": "Learning Transferable Adversarial Examples via Ghost Networks", "comments": "To appear in AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development of adversarial attacks has proven that ensemble-based\nmethods outperform traditional, non-ensemble ones in black-box attack. However,\nas it is computationally prohibitive to acquire a family of diverse models,\nthese methods achieve inferior performance constrained by the limited number of\nmodels to be ensembled.\n  In this paper, we propose Ghost Networks to improve the transferability of\nadversarial examples. The critical principle of ghost networks is to apply\nfeature-level perturbations to an existing model to potentially create a huge\nset of diverse models. After that, models are subsequently fused by\nlongitudinal ensemble. Extensive experimental results suggest that the number\nof networks is essential for improving the transferability of adversarial\nexamples, but it is less necessary to independently train different networks\nand ensemble them in an intensive aggregation way. Instead, our work can be\nused as a computationally cheap and easily applied plug-in to improve\nadversarial approaches both in single-model and multi-model attack, compatible\nwith residual and non-residual networks. By reproducing the NeurIPS 2017\nadversarial competition, our method outperforms the No.1 attack submission by a\nlarge margin, demonstrating its effectiveness and efficiency. Code is available\nat https://github.com/LiYingwei/ghost-network.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 02:11:03 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 01:55:44 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 15:34:55 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Li", "Yingwei", ""], ["Bai", "Song", ""], ["Zhou", "Yuyin", ""], ["Xie", "Cihang", ""], ["Zhang", "Zhishuai", ""], ["Yuille", "Alan", ""]]}, {"id": "1812.03426", "submitter": "Xinpeng Chen", "authors": "Xinpeng Chen, Lin Ma, Jingyuan Chen, Zequn Jie, Wei Liu, Jiebo Luo", "title": "Real-Time Referring Expression Comprehension by Single-Stage Grounding\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a novel end-to-end model, namely Single-Stage\nGrounding network (SSG), to localize the referent given a referring expression\nwithin an image. Different from previous multi-stage models which rely on\nobject proposals or detected regions, our proposed model aims to comprehend a\nreferring expression through one single stage without resorting to region\nproposals as well as the subsequent region-wise feature extraction.\nSpecifically, a multimodal interactor is proposed to summarize the local region\nfeatures regarding the referring expression attentively. Subsequently, a\ngrounder is proposed to localize the referring expression within the given\nimage directly. For further improving the localization accuracy, a guided\nattention mechanism is proposed to enforce the grounder to focus on the central\nregion of the referent. Moreover, by exploiting and predicting visual attribute\ninformation, the grounder can further distinguish the referent objects within\nan image and thereby improve the model performance. Experiments on RefCOCO,\nRefCOCO+, and RefCOCOg datasets demonstrate that our proposed SSG without\nrelying on any region proposals can achieve comparable performance with other\nadvanced models. Furthermore, our SSG outperforms the previous models and\nachieves the state-of-art performance on the ReferItGame dataset. More\nimportantly, our SSG is time efficient and can ground a referring expression in\na 416*416 image from the RefCOCO dataset in 25ms (40 referents per second) on\naverage with a Nvidia Tesla P40, accomplishing more than 9* speedups over the\nexisting multi-stage models.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 04:30:11 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Chen", "Xinpeng", ""], ["Ma", "Lin", ""], ["Chen", "Jingyuan", ""], ["Jie", "Zequn", ""], ["Liu", "Wei", ""], ["Luo", "Jiebo", ""]]}, {"id": "1812.03434", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi, Bernard Chiu, Chris H. Rycroft", "title": "Area-preserving mapping of 3D ultrasound carotid artery images using\n  density-equalizing reference map", "comments": null, "journal-ref": "IEEE Transactions on Biomedical Engineering, 67(9), 1507-1517\n  (2020)", "doi": "10.1109/TBME.2019.2963783", "report-no": null, "categories": "cs.CG cs.CV math.NA physics.med-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carotid atherosclerosis is a focal disease at the bifurcations of the carotid\nartery. To quantitatively monitor the local changes in the\nvessel-wall-plus-plaque thickness (VWT) and compare the VWT distributions for\ndifferent patients or for the same patients at different ultrasound scanning\nsessions, a mapping technique is required to adjust for the geometric\nvariability of different carotid artery models. In this work, we propose a\nnovel method called density-equalizing reference map (DERM) for mapping 3D\ncarotid surfaces to a standardized 2D carotid template, with an emphasis on\npreserving the local geometry of the carotid surface by minimizing the local\narea distortion. The initial map was generated by a previously described\narc-length scaling (ALS) mapping method, which projects a 3D carotid surface\nonto a 2D non-convex L-shaped domain. A smooth and area-preserving flattened\nmap was subsequently constructed by deforming the ALS map using the proposed\nalgorithm that combines the density-equalizing map and the reference map\ntechniques. This combination allows, for the first time, one-to-one mapping\nfrom a 3D surface to a standardized non-convex planar domain in an\narea-preserving manner. Evaluations using 20 carotid surface models show that\nthe proposed method reduced the area distortion of the flattening maps by over\n80% as compared to the ALS mapping method.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 06:35:15 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Choi", "Gary P. T.", ""], ["Chiu", "Bernard", ""], ["Rycroft", "Chris H.", ""]]}, {"id": "1812.03443", "submitter": "Bichen Wu", "authors": "Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming\n  Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, Kurt Keutzer", "title": "FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural\n  Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing accurate and efficient ConvNets for mobile devices is challenging\nbecause the design space is combinatorially large. Due to this, previous neural\narchitecture search (NAS) methods are computationally expensive. ConvNet\narchitecture optimality depends on factors such as input resolution and target\ndevices. However, existing approaches are too expensive for case-by-case\nredesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP\ncount does not always reflect actual latency. To address these, we propose a\ndifferentiable neural architecture search (DNAS) framework that uses\ngradient-based methods to optimize ConvNet architectures, avoiding enumerating\nand training individual architectures separately as in previous methods.\nFBNets, a family of models discovered by DNAS surpass state-of-the-art models\nboth designed manually and generated automatically. FBNet-B achieves 74.1%\ntop-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8\nphone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3 with similar accuracy.\nDespite higher accuracy and lower latency than MnasNet, we estimate FBNet-B's\nsearch cost is 420x smaller than MnasNet's, at only 216 GPU-hours. Searched for\ndifferent resolutions and channel sizes, FBNets achieve 1.5% to 6.4% higher\naccuracy than MobileNetV2. The smallest FBNet achieves 50.2% accuracy and 2.9\nms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized\nFBNet, the iPhone-X-optimized model achieves a 1.4x speedup on an iPhone X.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 08:24:50 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 19:22:48 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 05:47:40 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Wu", "Bichen", ""], ["Dai", "Xiaoliang", ""], ["Zhang", "Peizhao", ""], ["Wang", "Yanghan", ""], ["Sun", "Fei", ""], ["Wu", "Yiming", ""], ["Tian", "Yuandong", ""], ["Vajda", "Peter", ""], ["Jia", "Yangqing", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1812.03446", "submitter": "Chong Chen", "authors": "Chong Chen, Barbara Gris and Ozan \\\"Oktem", "title": "A New Variational Model for Joint Image Reconstruction and Motion\n  Estimation in Spatiotemporal Imaging", "comments": "35 pages, 5 figures, 3 tables, revised", "journal-ref": "SIAM Journal on Imaging Sciences 2019", "doi": "10.1137/18M1234047", "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new variational model for joint image reconstruction and motion\nestimation in spatiotemporal imaging, which is investigated along a general\nframework that we present with shape theory. This model consists of two\ncomponents, one for conducting modified static image reconstruction, and the\nother performs sequentially indirect image registration. For the latter, we\ngeneralize the large deformation diffeomorphic metric mapping framework into\nthe sequentially indirect registration setting. The proposed model is compared\ntheoretically against alternative approaches (optical flow based model and\ndiffeomorphic motion models), and we demonstrate that the proposed model has\ndesirable properties in terms of the optimal solution. The theoretical\nderivations and efficient algorithms are also presented for a time-discretized\nscenario of the proposed model, which show that the optimal solution of the\ntime-discretized version is consistent with that of the time-continuous one,\nand most of the computational components is the easy-implemented linearized\ndeformation. The complexity of the algorithm is analyzed as well. This work is\nconcluded by some numerical examples in 2D space + time tomography with very\nsparse and/or highly noisy data.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 08:28:27 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 14:20:52 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Chen", "Chong", ""], ["Gris", "Barbara", ""], ["\u00d6ktem", "Ozan", ""]]}, {"id": "1812.03451", "submitter": "Mahdi Maktab Dar Oghaz", "authors": "Chloe Eunhyang Kim, Mahdi Maktab Dar Oghaz, Jiri Fajtl, Vasileios\n  Argyriou, Paolo Remagnino", "title": "A Comparison of Embedded Deep Learning Methods for Person Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in parallel computing, GPU technology and deep learning\nprovide a new platform for complex image processing tasks such as person\ndetection to flourish. Person detection is fundamental preliminary operation\nfor several high level computer vision tasks. One industry that can\nsignificantly benefit from person detection is retail. In recent years, various\nstudies attempt to find an optimal solution for person detection using neural\nnetworks and deep learning. This study conducts a comparison among the state of\nthe art deep learning base object detector with the focus on person detection\nperformance in indoor environments. Performance of various implementations of\nYOLO, SSD, RCNN, R-FCN and SqueezeDet have been assessed using our in-house\nproprietary dataset which consists of over 10 thousands indoor images captured\nform shopping malls, retails and stores. Experimental results indicate that,\nTiny YOLO-416 and SSD (VGG-300) are the fastest and Faster-RCNN (Inception\nResNet-v2) and R-FCN (ResNet-101) are the most accurate detectors investigated\nin this study. Further analysis shows that YOLO v3-416 delivers relatively\naccurate result in a reasonable amount of time, which makes it an ideal model\nfor person detection in embedded platforms.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 09:29:28 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 15:26:19 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Kim", "Chloe Eunhyang", ""], ["Oghaz", "Mahdi Maktab Dar", ""], ["Fajtl", "Jiri", ""], ["Argyriou", "Vasileios", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1812.03473", "submitter": "Maciej P\\k{e}\\'sko", "authors": "Maciej P\\k{e}\\'sko, Adam Svystun, Pawe{\\l} Andruszkiewicz,\n  Przemys{\\l}aw Rokita and Tomasz Trzci\\'nski", "title": "Comixify: Transform video into a comics", "comments": "14 pages. arXiv admin note: substantial text overlap with\n  arXiv:1809.01726", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a solution to transform a video into a comics. We\napproach this task using a neural style algorithm based on Generative\nAdversarial Networks (GANs). Several recent works in the field of Neural Style\nTransfer showed that producing an image in the style of another image is\nfeasible. In this paper, we build up on these works and extend the existing set\nof style transfer use cases with a working application of video comixification.\nTo that end, we train an end-to-end solution that transforms input video into a\ncomics in two stages. In the first stage, we propose a state-of-the-art\nkeyframes extraction algorithm that selects a subset of frames from the video\nto provide the most comprehensive video context and we filter those frames\nusing image aesthetic estimation engine. In the second stage, the style of\nselected keyframes is transferred into a comics. To provide the most\naesthetically compelling results, we selected the most state-of-the art style\ntransfer solution and based on that implement our own ComixGAN framework. The\nfinal contribution of our work is a Web-based working application of video\ncomixification available at http://comixify.ii.pw.edu.pl.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 12:36:38 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["P\u0119\u015bko", "Maciej", ""], ["Svystun", "Adam", ""], ["Andruszkiewicz", "Pawe\u0142", ""], ["Rokita", "Przemys\u0142aw", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "1812.03500", "submitter": "Haofu Liao", "authors": "Haofu Liao, Addisu Mesfin, Jiebo Luo", "title": "Joint Vertebrae Identification and Localization in Spinal CT Images by\n  Combining Short- and Long-Range Contextual Information", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, vol. 37, no. 5, pp.\n  1266-1275, May 2018", "doi": "10.1109/TMI.2018.2798293", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic vertebrae identification and localization from arbitrary CT images\nis challenging. Vertebrae usually share similar morphological appearance.\nBecause of pathology and the arbitrary field-of-view of CT scans, one can\nhardly rely on the existence of some anchor vertebrae or parametric methods to\nmodel the appearance and shape. To solve the problem, we argue that one should\nmake use of the short-range contextual information, such as the presence of\nsome nearby organs (if any), to roughly estimate the target vertebrae; due to\nthe unique anatomic structure of the spine column, vertebrae have fixed\nsequential order which provides the important long-range contextual information\nto further calibrate the results.\n  We propose a robust and efficient vertebrae identification and localization\nsystem that can inherently learn to incorporate both the short-range and\nlong-range contextual information in a supervised manner. To this end, we\ndevelop a multi-task 3D fully convolutional neural network (3D FCN) to\neffectively extract the short-range contextual information around the target\nvertebrae. For the long-range contextual information, we propose a multi-task\nbidirectional recurrent neural network (Bi-RNN) to encode the spatial and\ncontextual information among the vertebrae of the visible spine column. We\ndemonstrate the effectiveness of the proposed approach on a challenging dataset\nand the experimental results show that our approach outperforms the\nstate-of-the-art methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 15:25:43 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Liao", "Haofu", ""], ["Mesfin", "Addisu", ""], ["Luo", "Jiebo", ""]]}, {"id": "1812.03503", "submitter": "Haofu Liao", "authors": "Haofu Liao, Zhimin Huo, William J. Sehnert, Shaohua Kevin Zhou, Jiebo\n  Luo,", "title": "Adversarial Sparse-View CBCT Artifact Reduction", "comments": null, "journal-ref": "Medical Image Computing and Computer Assisted Intervention\n  (MICCAI) 2018. Lecture Notes in Computer Science, vol 11070. Springer, Cham", "doi": "10.1007/978-3-030-00928-1_18", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an effective post-processing method to reduce the artifacts from\nsparsely reconstructed cone-beam CT (CBCT) images. The proposed method is based\non the state-of-the-art, image-to-image generative models with a perceptual\nloss as regulation. Unlike the traditional CT artifact-reduction approaches,\nour method is trained in an adversarial fashion that yields more perceptually\nrealistic outputs while preserving the anatomical structures. To address the\nstreak artifacts that are inherently local and appear across various scales, we\nfurther propose a novel discriminator architecture based on feature pyramid\nnetworks and a differentially modulated focus map to induce the adversarial\ntraining. Our experimental results show that the proposed method can greatly\ncorrect the cone-beam artifacts from clinical CBCT images reconstructed using\n1/3 projections, and outperforms strong baseline methods both quantitatively\nand qualitatively.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 15:45:59 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Liao", "Haofu", ""], ["Huo", "Zhimin", ""], ["Sehnert", "William J.", ""], ["Zhou", "Shaohua Kevin", ""], ["Luo", "Jiebo", ""]]}, {"id": "1812.03506", "submitter": "Paul-Edouard Sarlin", "authors": "Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, Marcin Dymczyk", "title": "From Coarse to Fine: Robust Hierarchical Localization at Large Scale", "comments": "Camera-ready for CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and accurate visual localization is a fundamental capability for\nnumerous applications, such as autonomous driving, mobile robotics, or\naugmented reality. It remains, however, a challenging task, particularly for\nlarge-scale environments and in presence of significant appearance changes.\nState-of-the-art methods not only struggle with such scenarios, but are often\ntoo resource intensive for certain real-time applications. In this paper we\npropose HF-Net, a hierarchical localization approach based on a monolithic CNN\nthat simultaneously predicts local features and global descriptors for accurate\n6-DoF localization. We exploit the coarse-to-fine localization paradigm: we\nfirst perform a global retrieval to obtain location hypotheses and only later\nmatch local features within those candidate places. This hierarchical approach\nincurs significant runtime savings and makes our system suitable for real-time\noperation. By leveraging learned descriptors, our method achieves remarkable\nlocalization robustness across large variations of appearance and sets a new\nstate-of-the-art on two challenging benchmarks for large-scale localization.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 15:56:36 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 14:25:53 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sarlin", "Paul-Edouard", ""], ["Cadena", "Cesar", ""], ["Siegwart", "Roland", ""], ["Dymczyk", "Marcin", ""]]}, {"id": "1812.03507", "submitter": "Haofu Liao", "authors": "Haofu Liao, Yucheng Tang, Gareth Funka-Lea, Jiebo Luo, Shaohua Kevin\n  Zhou", "title": "More Knowledge is Better: Cross-Modality Volume Completion and 3D+2D\n  Segmentation for Intracardiac Echocardiography Contouring", "comments": null, "journal-ref": "Medical Image Computing and Computer Assisted Intervention\n  (MICCAI) 2018. Lecture Notes in Computer Science, vol 11071. Springer, Cham", "doi": "10.1007/978-3-030-00934-2_60", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Using catheter ablation to treat atrial fibrillation increasingly relies on\nintracardiac echocardiography (ICE) for an anatomical delineation of the left\natrium and the pulmonary veins that enter the atrium. However, it is a\nchallenge to build an automatic contouring algorithm because ICE is noisy and\nprovides only a limited 2D view of the 3D anatomy. This work provides the first\nautomatic solution to segment the left atrium and the pulmonary veins from ICE.\nIn this solution, we demonstrate the benefit of building a cross-modality\nframework that can leverage a database of diagnostic images to supplement the\nless available interventional images. To this end, we develop a novel deep\nneural network approach that uses the (i) 3D geometrical information provided\nby a position sensor embedded in the ICE catheter and the (ii) 3D image\nappearance information from a set of computed tomography cardiac volumes. We\nevaluate the proposed approach over 11,000 ICE images collected from 150\nclinical patients. Experimental results show that our model is significantly\nbetter than a direct 2D image-to-image deep neural network segmentation,\nespecially for less-observed structures.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 16:03:38 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 03:14:49 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Liao", "Haofu", ""], ["Tang", "Yucheng", ""], ["Funka-Lea", "Gareth", ""], ["Luo", "Jiebo", ""], ["Zhou", "Shaohua Kevin", ""]]}, {"id": "1812.03520", "submitter": "Haofu Liao", "authors": "Haofu Liao, Yuncheng Li, Jiebo Luo", "title": "Skin Disease Classification versus Skin Lesion Characterization:\n  Achieving Robust Diagnosis using Multi-label Deep Neural Networks", "comments": null, "journal-ref": "2016 23rd International Conference on Pattern Recognition (ICPR),\n  Cancun, 2016, pp. 355-360", "doi": "10.1109/ICPR.2016.7899659", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, we investigate what a practically useful approach is in order\nto achieve robust skin disease diagnosis. A direct approach is to target the\nground truth diagnosis labels, while an alternative approach instead focuses on\ndetermining skin lesion characteristics that are more visually consistent and\ndiscernible. We argue that, for computer-aided skin disease diagnosis, it is\nboth more realistic and more useful that lesion type tags should be considered\nas the target of an automated diagnosis system such that the system can first\nachieve a high accuracy in describing skin lesions, and in turn facilitate\ndisease diagnosis using lesion characteristics in conjunction with other\nevidence. To further meet such an objective, we employ convolutional neural\nnetworks (CNNs) for both the disease-targeted and lesion-targeted\nclassifications. We have collected a large-scale and diverse dataset of 75,665\nskin disease images from six publicly available dermatology atlantes. Then we\ntrain and compare both disease-targeted and lesion-targeted classifiers,\nrespectively. For disease-targeted classification, only 27.6% top-1 accuracy\nand 57.9% top-5 accuracy are achieved with a mean average precision (mAP) of\n0.42. In contrast, for lesion-targeted classification, we can achieve a much\nhigher mAP of 0.70.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 16:56:14 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 03:04:53 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Liao", "Haofu", ""], ["Li", "Yuncheng", ""], ["Luo", "Jiebo", ""]]}, {"id": "1812.03527", "submitter": "Haofu Liao", "authors": "Haofu Liao, Jiebo Luo", "title": "A Deep Multi-task Learning Approach to Skin Lesion Classification", "comments": "AAAI 2017 Joint Workshop on Health Intelligence W3PHIAI 2017 (W3PHI &\n  HIAI), San Francisco, CA, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Skin lesion identification is a key step toward dermatological diagnosis.\nWhen describing a skin lesion, it is very important to note its body site\ndistribution as many skin diseases commonly affect particular parts of the\nbody. To exploit the correlation between skin lesions and their body site\ndistributions, in this study, we investigate the possibility of improving skin\nlesion classification using the additional context information provided by body\nlocation. Specifically, we build a deep multi-task learning (MTL) framework to\njointly optimize skin lesion classification and body location classification\n(the latter is used as an inductive bias). Our MTL framework uses the\nstate-of-the-art ImageNet pretrained model with specialized loss functions for\nthe two related tasks. Our experiments show that the proposed MTL based method\nperforms more robustly than its standalone (single-task) counterpart.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 17:19:54 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 02:59:49 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Liao", "Haofu", ""], ["Luo", "Jiebo", ""]]}, {"id": "1812.03544", "submitter": "Yubo Zhang", "authors": "Yubo Zhang, Pavel Tokmakov, Martial Hebert, Cordelia Schmid", "title": "A Structured Model For Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dominant paradigm for learning-based approaches in computer vision is\ntraining generic models, such as ResNet for image recognition, or I3D for video\nunderstanding, on large datasets and allowing them to discover the optimal\nrepresentation for the problem at hand. While this is an obviously attractive\napproach, it is not applicable in all scenarios. We claim that action detection\nis one such challenging problem - the models that need to be trained are large,\nand labeled data is expensive to obtain. To address this limitation, we propose\nto incorporate domain knowledge into the structure of the model, simplifying\noptimization. In particular, we augment a standard I3D network with a tracking\nmodule to aggregate long term motion patterns, and use a graph convolutional\nnetwork to reason about interactions between actors and objects. Evaluated on\nthe challenging AVA dataset, the proposed approach improves over the I3D\nbaseline by 5.5% mAP and over the state-of-the-art by 4.8% mAP.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 18:57:33 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 19:03:39 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2019 17:58:30 GMT"}, {"version": "v4", "created": "Fri, 12 Apr 2019 15:04:33 GMT"}, {"version": "v5", "created": "Wed, 5 Jun 2019 17:55:42 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Zhang", "Yubo", ""], ["Tokmakov", "Pavel", ""], ["Hebert", "Martial", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1812.03559", "submitter": "Rada Deeb", "authors": "Rada Deeb, Joost Van De Weijer, Damien Muselet, Mathieu Hebert, Alain\n  Tremeau", "title": "Deep Spectral Reflectance and Illuminant Estimation from\n  Self-Interreflections", "comments": "Accepted by JOSA A", "journal-ref": null, "doi": "10.1364/JOSAA.36.000105", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a CNN-based approach to estimate the spectral\nreflectance of a surface and the spectral power distribution of the light from\na single RGB image of a V-shaped surface. Interreflections happening in a\nconcave surface lead to gradients of RGB values over its area. These gradients\ncarry a lot of information concerning the physical properties of the surface\nand the illuminant. Our network is trained with only simulated data constructed\nusing a physics-based interreflection model. Coupling interreflection effects\nwith deep learning helps to retrieve the spectral reflectance under an unknown\nlight and to estimate the spectral power distribution of this light as well. In\naddition, it is more robust to the presence of image noise than the classical\napproaches. Our results show that the proposed approach outperforms the state\nof the art learning-based approaches on simulated data. In addition, it gives\nbetter results on real data compared to other interreflection-based approaches.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 21:02:28 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Deeb", "Rada", ""], ["Van De Weijer", "Joost", ""], ["Muselet", "Damien", ""], ["Hebert", "Mathieu", ""], ["Tremeau", "Alain", ""]]}, {"id": "1812.03570", "submitter": "Fadime Sener", "authors": "Divyansh Aggarwal, Elchin Valiyev, Fadime Sener, Angela Yao", "title": "Learning Style Compatibility for Furniture", "comments": "German Conference on Pattern Recognition(GCPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When judging style, a key question that often arises is whether or not a pair\nof objects are compatible with each other. In this paper we investigate how\nSiamese networks can be used efficiently for assessing the style compatibility\nbetween images of furniture items. We show that the middle layers of pretrained\nCNNs can capture essential information about furniture style, which allows for\nefficient applications of such networks for this task. We also use a joint\nimage-text embedding method that allows for the querying of stylistically\ncompatible furniture items, along with additional attribute constraints based\non text. To evaluate our methods, we collect and present a large scale dataset\nof images of furniture of different style categories accompanied by text\nattributes.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 22:54:41 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Aggarwal", "Divyansh", ""], ["Valiyev", "Elchin", ""], ["Sener", "Fadime", ""], ["Yao", "Angela", ""]]}, {"id": "1812.03595", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee", "title": "PoseFix: Model-agnostic General Human Pose Refinement Network", "comments": "Published at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation from a 2D image is an essential technique for\nhuman behavior understanding. In this paper, we propose a human pose refinement\nnetwork that estimates a refined pose from a tuple of an input image and input\npose. The pose refinement was performed mainly through an end-to-end trainable\nmulti-stage architecture in previous methods. However, they are highly\ndependent on pose estimation models and require careful model design. By\ncontrast, we propose a model-agnostic pose refinement method. According to a\nrecent study, state-of-the-art 2D human pose estimation methods have similar\nerror distributions. We use this error statistics as prior information to\ngenerate synthetic poses and use the synthesized poses to train our model. In\nthe testing stage, pose estimation results of any other methods can be input to\nthe proposed method. Moreover, the proposed model does not require code or\nknowledge about other methods, which allows it to be easily used in the\npost-processing step. We show that the proposed approach achieves better\nperformance than the conventional multi-stage refinement models and\nconsistently improves the performance of various state-of-the-art pose\nestimation methods on the commonly used benchmark. The code is available in\nthis https URL\\footnote{\\url{https://github.com/mks0601/PoseFix_RELEASE}}.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 01:58:12 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 07:34:35 GMT"}, {"version": "v3", "created": "Sun, 10 Mar 2019 14:11:09 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Chang", "Ju Yong", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1812.03596", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi and Klaas Kelchtermans and Tinne Tuytelaars", "title": "Task-Free Continual Learning", "comments": "Accepted as a conference paper in CVPR 2019. Rahaf Aljundi and Klaas\n  Kelchtermans have contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods proposed in the literature towards continual deep learning typically\noperate in a task-based sequential learning setup. A sequence of tasks is\nlearned, one at a time, with all data of current task available but not of\nprevious or future tasks. Task boundaries and identities are known at all\ntimes. This setup, however, is rarely encountered in practical applications.\nTherefore we investigate how to transform continual learning to an online\nsetup. We develop a system that keeps on learning over time in a streaming\nfashion, with data distributions gradually changing and without the notion of\nseparate tasks. To this end, we build on the work on Memory Aware Synapses, and\nshow how this method can be made online by providing a protocol to decide i)\nwhen to update the importance weights, ii) which data to use to update them,\nand iii) how to accumulate the importance weights at each update step.\nExperimental results show the validity of the approach in the context of two\napplications: (self-)supervised learning of a face recognition model by\nwatching soap series and learning a robot to avoid collisions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 02:07:57 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 08:09:27 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 10:42:15 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Kelchtermans", "Klaas", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1812.03621", "submitter": "Longyin Wen", "authors": "Longyin Wen, Dawei Du, Shengkun Li, Xiao Bian, Siwei Lyu", "title": "Learning Non-Uniform Hypergraph for Multi-Object Tracking", "comments": "11 pages, 4 figures, accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of Multi-Object Tracking (MOT) algorithms based on the\ntracking-by-detection scheme do not use higher order dependencies among objects\nor tracklets, which makes them less effective in handling complex scenarios. In\nthis work, we present a new near-online MOT algorithm based on non-uniform\nhypergraph, which can model different degrees of dependencies among tracklets\nin a unified objective. The nodes in the hypergraph correspond to the tracklets\nand the hyperedges with different degrees encode various kinds of dependencies\namong them. Specifically, instead of setting the weights of hyperedges with\ndifferent degrees empirically, they are learned automatically using the\nstructural support vector machine algorithm (SSVM). Several experiments are\ncarried out on various challenging datasets (i.e., PETS09, ParkingLot sequence,\nSubwayFace, and MOT16 benchmark), to demonstrate that our method achieves\nfavorable performance against the state-of-the-art MOT methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 04:53:02 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Wen", "Longyin", ""], ["Du", "Dawei", ""], ["Li", "Shengkun", ""], ["Bian", "Xiao", ""], ["Lyu", "Siwei", ""]]}, {"id": "1812.03622", "submitter": "Daichi Ono", "authors": "Daichi Ono, Hiroyuki Yabe, Tsutomu Horikawa", "title": "3D Scene Parsing via Class-Wise Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the method that uses only computer graphics datasets to parse the\nreal world 3D scenes. 3D scene parsing based on semantic segmentation is\nrequired to implement the categorical interaction in the virtual world.\nConvolutional Neural Networks (CNNs) have recently shown state-of-theart\nperformance on computer vision tasks including semantic segmentation. However,\ncollecting and annotating a huge amount of data are needed to train CNNs.\nEspecially in the case of semantic segmentation, annotating pixel by pixel\ntakes a significant amount of time and often makes mistakes. In contrast,\ncomputer graphics can generate a lot of accurate annotated data and easily\nscale up by changing camera positions, textures and lights. Despite these\nadvantages, models trained on computer graphics datasets cannot perform well on\nreal data, which is known as the domain shift. To address this issue, we first\npresent that depth modal and synthetic noise are effective to reduce the domain\nshift. Then, we develop the class-wise adaptation which obtains domain\ninvariant features of CNNs. To reduce the domain shift, we create computer\ngraphics rooms with a lot of props, and provide photo-realistic rendered\nimages.We also demonstrate the application which is combined semantic\nsegmentation with Simultaneous Localization and Mapping (SLAM). Our application\nperforms accurate 3D scene parsing in real-time on an actual room.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 04:53:14 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 03:29:53 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Ono", "Daichi", ""], ["Yabe", "Hiroyuki", ""], ["Horikawa", "Tsutomu", ""]]}, {"id": "1812.03626", "submitter": "Giulio Zhou", "authors": "Giulio Zhou, Subramanya Dulloor, David G. Andersen, Michael Kaminsky", "title": "EDF: Ensemble, Distill, and Fuse for Easy Video Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a way to rapidly bootstrap object detection on unseen videos using\nminimal human annotations. We accomplish this by combining two complementary\nsources of knowledge (one generic and the other specific) using bounding box\nmerging and model distillation. The first (generic) knowledge source is\nobtained from ensembling pre-trained object detectors using a novel bounding\nbox merging and confidence reweighting scheme. We make the observation that\nmodel distillation with data augmentation can train a specialized detector that\noutperforms the noisy labels it was trained on, and train a Student Network on\nthe ensemble detections that obtains higher mAP than the ensemble itself. The\nsecond (specialized) knowledge source comes from training a detector (which we\ncall the Supervised Labeler) on a labeled subset of the video to generate\ndetections on the unlabeled portion. We demonstrate on two popular vehicular\ndatasets that these techniques work to emit bounding boxes for all vehicles in\nthe frame with higher mean average precision (mAP) than any of the reference\nnetworks used, and that the combination of ensembled and human-labeled data\nproduces object detections that outperform either alone.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 05:18:57 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Zhou", "Giulio", ""], ["Dulloor", "Subramanya", ""], ["Andersen", "David G.", ""], ["Kaminsky", "Michael", ""]]}, {"id": "1812.03631", "submitter": "Rudra Saha", "authors": "Somak Aditya, Rudra Saha, Yezhou Yang, Chitta Baral", "title": "Spatial Knowledge Distillation to aid Visual Reasoning", "comments": "Equal contribution by first two authors. Accepted in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For tasks involving language and vision, the current state-of-the-art methods\ntend not to leverage any additional information that might be present to gather\nrelevant (commonsense) knowledge. A representative task is Visual Question\nAnswering where large diagnostic datasets have been proposed to test a system's\ncapability of answering questions about images. The training data is often\naccompanied by annotations of individual object properties and spatial\nlocations. In this work, we take a step towards integrating this additional\nprivileged information in the form of spatial knowledge to aid in visual\nreasoning. We propose a framework that combines recent advances in knowledge\ndistillation (teacher-student framework), relational reasoning and\nprobabilistic logical languages to incorporate such knowledge in existing\nneural networks for the task of Visual Question Answering. Specifically, for a\nquestion posed against an image, we use a probabilistic logical language to\nencode the spatial knowledge and the spatial understanding about the question\nin the form of a mask that is directly provided to the teacher network. The\nstudent network learns from the ground-truth information as well as the\nteachers prediction via distillation. We also demonstrate the impact of\npredicting such a mask inside the teachers network using attention.\nEmpirically, we show that both the methods improve the test accuracy over a\nstate-of-the-art approach on a publicly available dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 05:36:23 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 16:42:29 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Aditya", "Somak", ""], ["Saha", "Rudra", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""]]}, {"id": "1812.03664", "submitter": "Han-Jia Ye", "authors": "Han-Jia Ye and Hexiang Hu and De-Chuan Zhan and Fei Sha", "title": "Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions", "comments": "Accepted by CVPR 2020; The code is available at\n  https://github.com/Sha-Lab/FEAT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with limited data is a key challenge for visual recognition. Many\nfew-shot learning methods address this challenge by learning an instance\nembedding function from seen classes and apply the function to instances from\nunseen classes with limited labels. This style of transfer learning is\ntask-agnostic: the embedding function is not learned optimally discriminative\nwith respect to the unseen classes, where discerning among them leads to the\ntarget task. In this paper, we propose a novel approach to adapt the instance\nembeddings to the target classification task with a set-to-set function,\nyielding embeddings that are task-specific and are discriminative. We\nempirically investigated various instantiations of such set-to-set functions\nand observed the Transformer is most effective -- as it naturally satisfies key\nproperties of our desired model. We denote this model as FEAT (few-shot\nembedding adaptation w/ Transformer) and validate it on both the standard\nfew-shot classification benchmark and four extended few-shot learning settings\nwith essential use cases, i.e., cross-domain, transductive, generalized\nfew-shot learning, and low-shot learning. It archived consistent improvements\nover baseline models as well as previous methods and established the new\nstate-of-the-art results on two benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 07:55:56 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 14:58:22 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 14:26:21 GMT"}, {"version": "v4", "created": "Mon, 30 Mar 2020 07:01:45 GMT"}, {"version": "v5", "created": "Tue, 31 Mar 2020 07:33:46 GMT"}, {"version": "v6", "created": "Sun, 13 Jun 2021 06:16:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ye", "Han-Jia", ""], ["Hu", "Hexiang", ""], ["Zhan", "De-Chuan", ""], ["Sha", "Fei", ""]]}, {"id": "1812.03680", "submitter": "Najoua Rahal", "authors": "Najoua Rahal and Maroua Tounsi and Adel M. Alimi", "title": "Neural Probabilistic System for Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained text recognition is a stimulating field in the branch of\npattern recognition. This field is still an open search due to the unlimited\nvocabulary, multi styles, mixed-font and their great morphological variability.\nRecent trends show a potential improvement of recognition by adoption a novel\nrepresentation of extracted features. In the present paper, we propose a novel\nfeature extraction model by learning a Bag of Features Framework for text\nrecognition based on Sparse Auto-Encoder. The Hidden Markov Models are then\nused for sequences modeling. For features learned quality evaluation, our\nproposed system was tested on two printed text datasets PKHATT text line images\nand APTI word images benchmark. Our method achieves promising recognition on\nboth datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 09:12:01 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 08:10:01 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 13:38:00 GMT"}, {"version": "v4", "created": "Tue, 8 Jan 2019 19:21:54 GMT"}, {"version": "v5", "created": "Tue, 25 Jun 2019 15:00:47 GMT"}, {"version": "v6", "created": "Fri, 19 Jul 2019 06:57:58 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Rahal", "Najoua", ""], ["Tounsi", "Maroua", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1812.03704", "submitter": "Andr\\'es Felipe Romero Vergara", "authors": "Andr\\'es Romero, Pablo Arbel\\'aez, Luc Van Gool, Radu Timofte", "title": "SMIT: Stochastic Multi-Label Image-to-Image Translation", "comments": "ICCV Workshops, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain mapping has been a very active topic in recent years. Given one\nimage, its main purpose is to translate it to the desired target domain, or\nmultiple domains in the case of multiple labels. This problem is highly\nchallenging due to three main reasons: (i) unpaired datasets, (ii) multiple\nattributes, and (iii) the multimodality (e.g., style) associated with the\ntranslation. Most of the existing state-of-the-art has focused only on two\nreasons, i.e. either on (i) and (ii), or (i) and (iii). In this work, we\npropose a joint framework (i, ii, iii) of diversity and multi-mapping\nimage-to-image translations, using a single generator to conditionally produce\ncountless and unique fake images that hold the underlying characteristics of\nthe source image. Our system does not use style regularization, instead, it\nuses an embedding representation that we call domain embedding for both domain\nand style. Extensive experiments over different datasets demonstrate the\neffectiveness of our proposed approach in comparison with the state-of-the-art\nin both multi-label and multimodal problems. Additionally, our method is able\nto generalize under different scenarios: continuous style interpolation,\ncontinuous label interpolation, and fine-grained mapping. Code and pretrained\nmodels are available at https://github.com/BCV-Uniandes/SMIT.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 10:00:24 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 22:42:06 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 10:18:24 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Romero", "Andr\u00e9s", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "1812.03705", "submitter": "Chaithanya Kumar Mummadi", "authors": "Chaithanya Kumar Mummadi, Thomas Brox, Jan Hendrik Metzen", "title": "Defending Against Universal Perturbations With Shared Adversarial\n  Training", "comments": "ICCV 2019, 8 main pages, 9 appendix pages, 16 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers such as deep neural networks have been shown to be vulnerable\nagainst adversarial perturbations on problems with high-dimensional input\nspace. While adversarial training improves the robustness of image classifiers\nagainst such adversarial perturbations, it leaves them sensitive to\nperturbations on a non-negligible fraction of the inputs. In this work, we show\nthat adversarial training is more effective in preventing universal\nperturbations, where the same perturbation needs to fool a classifier on many\ninputs. Moreover, we investigate the trade-off between robustness against\nuniversal perturbations and performance on unperturbed data and propose an\nextension of adversarial training that handles this trade-off more gracefully.\nWe present results for image classification and semantic segmentation to\nshowcase that universal perturbations that fool a model hardened with\nadversarial training become clearly perceptible and show patterns of the target\nscene.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 10:02:45 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 11:58:27 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Mummadi", "Chaithanya Kumar", ""], ["Brox", "Thomas", ""], ["Metzen", "Jan Hendrik", ""]]}, {"id": "1812.03707", "submitter": "Hugo Germain", "authors": "Hugo Germain, Guillaume Bourmaud, Vincent Lepetit", "title": "Improving Nighttime Retrieval-Based Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor visual localization is a crucial component to many computer vision\nsystems. We propose an approach to localization from images that is designed to\nexplicitly handle the strong variations in appearance happening between daytime\nand nighttime. As revealed by recent long-term localization benchmarks, both\ntraditional feature-based and retrieval-based approaches still struggle to\nhandle such changes. Our novel localization method combines a state-of-the-art\nimage retrieval architecture with condition-specific sub-networks allowing the\ncomputation of global image descriptors that are explicitly dependent of the\ncapturing conditions. We show that our approach improves localization by a\nfactor of almost 300\\% compared to the popular VLAD-based methods on nighttime\nlocalization.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 10:05:40 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 16:59:56 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 08:50:02 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Germain", "Hugo", ""], ["Bourmaud", "Guillaume", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1812.03719", "submitter": "Marion G\u00f6del", "authors": "Marion G\\\"odel, Gerta K\\\"oster, Daniel Lehmberg, Manfred Gruber,\n  Angelika Kneidl, Florian Sesser", "title": "Can we learn where people go?", "comments": "Proceedings of the 9th International Conference on Pedestrian and\n  Evacuation Dynamics (PED2018) in Lund, Sweden, August 21-23, 2018 Paper No.\n  50, 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most agent-based simulators, pedestrians navigate from origins to\ndestinations. Consequently, destinations are essential input parameters to the\nsimulation. While many other relevant parameters as positions, speeds and\ndensities can be obtained from sensors, like cameras, destinations cannot be\nobserved directly. Our research question is: Can we obtain this information\nfrom video data using machine learning methods? We use density heatmaps, which\nindicate the pedestrian density within a given camera cutout, as input to\npredict the destination distributions. For our proof of concept, we train a\nRandom Forest predictor on an exemplary data set generated with the Vadere\nmicroscopic simulator. The scenario is a crossroad where pedestrians can head\nleft, straight or right. In addition, we gain first insights on suitable\nplacement of the camera. The results motivate an in-depth analysis of the\nmethodology.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 10:24:51 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 10:13:58 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["G\u00f6del", "Marion", ""], ["K\u00f6ster", "Gerta", ""], ["Lehmberg", "Daniel", ""], ["Gruber", "Manfred", ""], ["Kneidl", "Angelika", ""], ["Sesser", "Florian", ""]]}, {"id": "1812.03794", "submitter": "Abhishek Sharma", "authors": "Jean-Michel Roufosse, Abhishek Sharma, Maks Ovsjanikov", "title": "Unsupervised Deep Learning for Structured Shape Matching", "comments": "Oral Presentation at ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for computing correspondences across 3D shapes\nusing unsupervised learning. Our method computes a non-linear transformation of\ngiven descriptor functions, while optimizing for global structural properties\nof the resulting maps, such as their bijectivity or approximate isometry. To\nthis end, we use the functional maps framework, and build upon the recent FMNet\narchitecture for descriptor learning. Unlike that approach, however, we show\nthat learning can be done in a purely \\emph{unsupervised setting}, without\nhaving access to any ground truth correspondences. This results in a very\ngeneral shape matching method that we call SURFMNet for Spectral Unsupervised\nFMNet, and which can be used to establish correspondences within 3D shape\ncollections without any prior information. We demonstrate on a wide range of\nchallenging benchmarks, that our approach leads to state-of-the-art results\ncompared to the existing unsupervised methods and achieves results that are\ncomparable even to the supervised learning techniques. Moreover, our framework\nis an order of magnitude faster, and does not rely on geodesic distance\ncomputation or expensive post-processing.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 13:50:34 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 20:07:31 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 10:53:58 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Roufosse", "Jean-Michel", ""], ["Sharma", "Abhishek", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1812.03795", "submitter": "Janine Thoma", "authors": "Janine Thoma, Danda Pani Paudel, Ajad Chhatkuli, Thomas Probst, Luc\n  Van Gool", "title": "Mapping, Localization and Path Planning for Image-based Navigation using\n  Visual Features and Map", "comments": "CVPR 2019, for implementation see https://github.com/janinethoma", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on progress in feature representations for image retrieval,\nimage-based localization has seen a surge of research interest. Image-based\nlocalization has the advantage of being inexpensive and efficient, often\navoiding the use of 3D metric maps altogether. That said, the need to maintain\na large number of reference images as an effective support of localization in a\nscene, nonetheless calls for them to be organized in a map structure of some\nkind.\n  The problem of localization often arises as part of a navigation process. We\nare, therefore, interested in summarizing the reference images as a set of\nlandmarks, which meet the requirements for image-based navigation. A\ncontribution of this paper is to formulate such a set of requirements for the\ntwo sub-tasks involved: map construction and self-localization. These\nrequirements are then exploited for compact map representation and accurate\nself-localization, using the framework of a network flow problem. During this\nprocess, we formulate the map construction and self-localization problems as\nconvex quadratic and second-order cone programs, respectively. We evaluate our\nmethods on publicly available indoor and outdoor datasets, where they\noutperform existing methods significantly.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 13:52:58 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 12:38:53 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Thoma", "Janine", ""], ["Paudel", "Danda Pani", ""], ["Chhatkuli", "Ajad", ""], ["Probst", "Thomas", ""], ["Van Gool", "Luc", ""]]}, {"id": "1812.03813", "submitter": "Marcel Nassar", "authors": "Marcel Nassar", "title": "Hierarchical Bipartite Graph Convolution Networks", "comments": "Appeared in the Workshop on Relational Representation Learning (R2L)\n  at NIPS 2018. 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph neural networks have been adopted in a wide variety of\napplications ranging from relational representations to modeling irregular data\ndomains such as point clouds and social graphs. However, the space of graph\nneural network architectures remains highly fragmented impeding the development\nof optimized implementations similar to what is available for convolutional\nneural networks. In this work, we present BiGraphNet, a graph neural network\narchitecture that generalizes many popular graph neural network models and\nenables new efficient operations similar to those supported by ConvNets. By\nexplicitly separating the input and output nodes, BiGraphNet: (i) generalizes\nthe graph convolution to support new efficient operations such as coarsened\ngraph convolutions (similar to strided convolution in convnets), multiple input\ngraphs convolution and graph expansions (unpooling) which can be used to\nimplement various graph architectures such as graph autoencoders, and graph\nresidual nets; and (ii) accelerates and scales the computations and memory\nrequirements in hierarchical networks by performing computations only at\nspecified output nodes.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 02:43:59 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 02:05:11 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Nassar", "Marcel", ""]]}, {"id": "1812.03823", "submitter": "Alex Bewley", "authors": "Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke, Richard Shen,\n  Vinh-Dieu Lam, Alex Kendall", "title": "Learning to Drive from Simulation without Real World Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation can be a powerful tool for understanding machine learning systems\nand designing methods to solve real-world problems. Training and evaluating\nmethods purely in simulation is often \"doomed to succeed\" at the desired task\nin a simulated environment, but the resulting models are incapable of operation\nin the real world. Here we present and evaluate a method for transferring a\nvision-based lane following driving policy from simulation to operation on a\nrural road without any real-world labels. Our approach leverages recent\nadvances in image-to-image translation to achieve domain transfer while jointly\nlearning a single-camera control policy from simulation control labels. We\nassess the driving performance of this method using both open-loop regression\nmetrics, and closed-loop performance operating an autonomous vehicle on rural\nand urban roads.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 14:31:58 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 17:33:07 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Bewley", "Alex", ""], ["Rigley", "Jessica", ""], ["Liu", "Yuxuan", ""], ["Hawke", "Jeffrey", ""], ["Shen", "Richard", ""], ["Lam", "Vinh-Dieu", ""], ["Kendall", "Alex", ""]]}, {"id": "1812.03828", "submitter": "Lars Mescheder", "authors": "Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin,\n  Andreas Geiger", "title": "Occupancy Networks: Learning 3D Reconstruction in Function Space", "comments": "To be presented at CVPR 2019. Supplementary material and code is\n  available at http://avg.is.tuebingen.mpg.de/publications/occupancy-networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of deep neural networks, learning-based approaches for 3D\nreconstruction have gained popularity. However, unlike for images, in 3D there\nis no canonical representation which is both computationally and memory\nefficient yet allows for representing high-resolution geometry of arbitrary\ntopology. Many of the state-of-the-art learning-based 3D reconstruction\napproaches can hence only represent very coarse 3D geometry or are limited to a\nrestricted domain. In this paper, we propose Occupancy Networks, a new\nrepresentation for learning-based 3D reconstruction methods. Occupancy networks\nimplicitly represent the 3D surface as the continuous decision boundary of a\ndeep neural network classifier. In contrast to existing approaches, our\nrepresentation encodes a description of the 3D output at infinite resolution\nwithout excessive memory footprint. We validate that our representation can\nefficiently encode 3D structure and can be inferred from various kinds of\ninput. Our experiments demonstrate competitive results, both qualitatively and\nquantitatively, for the challenging tasks of 3D reconstruction from single\nimages, noisy point clouds and coarse discrete voxel grids. We believe that\noccupancy networks will become a useful tool in a wide variety of\nlearning-based 3D tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 14:36:52 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 14:43:13 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Mescheder", "Lars", ""], ["Oechsle", "Michael", ""], ["Niemeyer", "Michael", ""], ["Nowozin", "Sebastian", ""], ["Geiger", "Andreas", ""]]}, {"id": "1812.03849", "submitter": "Xuguang Duan", "authors": "Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong Wang, Wenwu Zhu and\n  Junzhou Huang", "title": "Weakly Supervised Dense Event Captioning in Videos", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Dense event captioning aims to detect and describe all events of interest\ncontained in a video. Despite the advanced development in this area, existing\nmethods tackle this task by making use of dense temporal annotations, which is\ndramatically source-consuming. This paper formulates a new problem: weakly\nsupervised dense event captioning, which does not require temporal segment\nannotations for model training. Our solution is based on the one-to-one\ncorrespondence assumption, each caption describes one temporal segment, and\neach temporal segment has one caption, which holds in current benchmark\ndatasets and most real-world cases. We decompose the problem into a pair of\ndual problems: event captioning and sentence localization and present a cycle\nsystem to train our model. Extensive experimental results are provided to\ndemonstrate the ability of our model on both dense event captioning and\nsentence localization in videos.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 14:58:24 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Duan", "Xuguang", ""], ["Huang", "Wenbing", ""], ["Gan", "Chuang", ""], ["Wang", "Jingdong", ""], ["Zhu", "Wenwu", ""], ["Huang", "Junzhou", ""]]}, {"id": "1812.03858", "submitter": "Ankur Singh", "authors": "Ankur Singh, Anurag Chanani and Harish Karnick", "title": "Video Colorization using CNNs and Keyframes extraction: An application\n  in saving bandwidth", "comments": "arXiv admin note: text overlap with arXiv:1712.03400 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of colorization of grayscale videos to\nreduce bandwidth usage. For this task, we use some colored keyframes as\nreference images from the colored version of the grayscale video. We propose a\nmodel that extracts keyframes from a colored video and trains a Convolutional\nNeural Network from scratch on these colored frames. Through the extracted\nkeyframes we get a good knowledge of the colors that have been used in the\nvideo which helps us in colorizing the grayscale version of the video\nefficiently. An application of the technique that we propose in this paper, is\nin saving bandwidth while sending raw colored videos that haven't gone through\nany compression. A raw colored video takes up around three times more memory\nsize than its grayscale version. We can exploit this fact and send a grayscale\nvideo along with out trained model instead of a colored video. Later on, in\nthis paper we show how this technique can help to save bandwidth usage to upto\nthree times while transmitting raw colored videos.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 09:24:39 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 14:40:41 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 11:46:11 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Singh", "Ankur", ""], ["Chanani", "Anurag", ""], ["Karnick", "Harish", ""]]}, {"id": "1812.03887", "submitter": "Guanbin Li", "authors": "Lingbo Liu, Guanbin Li, Yuan Xie, Yizhou Yu, Qing Wang, Liang Lin", "title": "Facial Landmark Machines: A Backbone-Branches Architecture with\n  Progressive Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark localization plays a critical role in face recognition and\nanalysis. In this paper, we propose a novel cascaded backbone-branches fully\nconvolutional neural network~(BB-FCN) for rapidly and accurately localizing\nfacial landmarks in unconstrained and cluttered settings. Our proposed BB-FCN\ngenerates facial landmark response maps directly from raw images without any\npreprocessing. BB-FCN follows a coarse-to-fine cascaded pipeline, which\nconsists of a backbone network for roughly detecting the locations of all\nfacial landmarks and one branch network for each type of detected landmark for\nfurther refining their locations. Furthermore, to facilitate the facial\nlandmark localization under unconstrained settings, we propose a large-scale\nbenchmark named SYSU16K, which contains 16000 faces with large variations in\npose, expression, illumination and resolution. Extensive experimental\nevaluations demonstrate that our proposed BB-FCN can significantly outperform\nthe state-of-the-art under both constrained (i.e., within detected facial\nregions only) and unconstrained settings. We further confirm that high-quality\nfacial landmarks localized with our proposed network can also improve the\nprecision and recall of face detection.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 15:50:37 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Liu", "Lingbo", ""], ["Li", "Guanbin", ""], ["Xie", "Yuan", ""], ["Yu", "Yizhou", ""], ["Wang", "Qing", ""], ["Lin", "Liang", ""]]}, {"id": "1812.03904", "submitter": "Yanwei Li", "authors": "Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du,\n  Xingang Wang", "title": "Attention-guided Unified Network for Panoptic Segmentation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies panoptic segmentation, a recently proposed task which\nsegments foreground (FG) objects at the instance level as well as background\n(BG) contents at the semantic level. Existing methods mostly dealt with these\ntwo problems separately, but in this paper, we reveal the underlying\nrelationship between them, in particular, FG objects provide complementary cues\nto assist BG understanding. Our approach, named the Attention-guided Unified\nNetwork (AUNet), is a unified framework with two branches for FG and BG\nsegmentation simultaneously. Two sources of attentions are added to the BG\nbranch, namely, RPN and FG segmentation mask to provide object-level and\npixel-level attentions, respectively. Our approach is generalized to different\nbackbones with consistent accuracy gain in both FG and BG segmentation, and\nalso sets new state-of-the-arts both in the MS-COCO (46.5% PQ) and Cityscapes\n(59.0% PQ) benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 16:25:10 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 04:32:23 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Li", "Yanwei", ""], ["Chen", "Xinze", ""], ["Zhu", "Zheng", ""], ["Xie", "Lingxi", ""], ["Huang", "Guan", ""], ["Du", "Dalong", ""], ["Wang", "Xingang", ""]]}, {"id": "1812.03910", "submitter": "Hung-Yu Chen", "authors": "Hung-Yu Chen, I-Sheng Fang, Wei-Chen Chiu", "title": "Self-Contained Stylization via Steganography for Reverse and Serial\n  Style Transfer", "comments": "21 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer has been widely applied to give real-world images a new\nartistic look. However, given a stylized image, the attempts to use typical\nstyle transfer methods for de-stylization or transferring it again into another\nstyle usually lead to artifacts or undesired results. We realize that these\nissues are originated from the content inconsistency between the original image\nand its stylized output. Therefore, in this paper we advance to keep the\ncontent information of the input image during the process of style transfer by\nthe power of steganography, with two approaches proposed: a two-stage model and\nan end-to-end model. We conduct extensive experiments to successfully verify\nthe capacity of our models, in which both of them are able to not only generate\nstylized images of quality comparable with the ones produced by typical style\ntransfer methods, but also effectively eliminate the artifacts introduced in\nreconstructing original input from a stylized image as well as performing\nmultiple times of style transfer in series.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 16:43:49 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 02:46:05 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 16:21:59 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Chen", "Hung-Yu", ""], ["Fang", "I-Sheng", ""], ["Chiu", "Wei-Chen", ""]]}, {"id": "1812.03928", "submitter": "Yan Zhang", "authors": "Yan Zhang, Jonathon Hare, Adam Pr\\\"ugel-Bennett", "title": "Learning Representations of Sets through Optimized Permutations", "comments": "Published in ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations of sets are challenging to learn because operations on sets\nshould be permutation-invariant. To this end, we propose a\nPermutation-Optimisation module that learns how to permute a set end-to-end.\nThe permuted set can be further processed to learn a permutation-invariant\nrepresentation of that set, avoiding a bottleneck in traditional set models. We\ndemonstrate our model's ability to learn permutations and set representations\nwith either explicit or implicit supervision on four datasets, on which we\nachieve state-of-the-art results: number sorting, image mosaics, classification\nfrom image mosaics, and visual question answering.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 17:26:25 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 15:36:35 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2019 03:18:33 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Zhang", "Yan", ""], ["Hare", "Jonathon", ""], ["Pr\u00fcgel-Bennett", "Adam", ""]]}, {"id": "1812.03944", "submitter": "Saheb Chhabra", "authors": "Saheb Chhabra, Puspita Majumdar, Mayank Vatsa, and Richa Singh", "title": "Data Fine-tuning", "comments": "Accepted in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world applications, commercial off-the-shelf systems are utilized for\nperforming automated facial analysis including face recognition, emotion\nrecognition, and attribute prediction. However, a majority of these commercial\nsystems act as black boxes due to the inaccessibility of the model parameters\nwhich makes it challenging to fine-tune the models for specific applications.\nStimulated by the advances in adversarial perturbations, this research proposes\nthe concept of Data Fine-tuning to improve the classification accuracy of a\ngiven model without changing the parameters of the model. This is accomplished\nby modeling it as data (image) perturbation problem. A small amount of \"noise\"\nis added to the input with the objective of minimizing the classification loss\nwithout affecting the (visual) appearance. Experiments performed on three\npublicly available datasets LFW, CelebA, and MUCT, demonstrate the\neffectiveness of the proposed concept.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 17:57:59 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Chhabra", "Saheb", ""], ["Majumdar", "Puspita", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "1812.03945", "submitter": "Hao Zheng", "authors": "Hao Zheng, Yizhe Zhang, Lin Yang, Peixian Liang, Zhuo Zhao, Chaoli\n  Wang, Danny Z. Chen", "title": "A New Ensemble Learning Framework for 3D Biomedical Image Segmentation", "comments": "To appear in AAAI-2019. The first three authors contributed equally\n  to the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D image segmentation plays an important role in biomedical image analysis.\nMany 2D and 3D deep learning models have achieved state-of-the-art segmentation\nperformance on 3D biomedical image datasets. Yet, 2D and 3D models have their\nown strengths and weaknesses, and by unifying them together, one may be able to\nachieve more accurate results. In this paper, we propose a new ensemble\nlearning framework for 3D biomedical image segmentation that combines the\nmerits of 2D and 3D models. First, we develop a fully convolutional network\nbased meta-learner to learn how to improve the results from 2D and 3D models\n(base-learners). Then, to minimize over-fitting for our sophisticated\nmeta-learner, we devise a new training method that uses the results of the\nbase-learners as multiple versions of \"ground truths\". Furthermore, since our\nnew meta-learner training scheme does not depend on manual annotation, it can\nutilize abundant unlabeled 3D image data to further improve the model.\nExtensive experiments on two public datasets (the HVSMR 2016 Challenge dataset\nand the mouse piriform cortex dataset) show that our approach is effective\nunder fully-supervised, semi-supervised, and transductive settings, and attains\nsuperior performance over state-of-the-art image segmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 17:58:00 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Zheng", "Hao", ""], ["Zhang", "Yizhe", ""], ["Yang", "Lin", ""], ["Liang", "Peixian", ""], ["Zhao", "Zhuo", ""], ["Wang", "Chaoli", ""], ["Chen", "Danny Z.", ""]]}, {"id": "1812.03953", "submitter": "Hadi Abdi Khojasteh", "authors": "Hadi Abdi Khojasteh, Alireza Abbas Alipour, Ebrahim Ansari and Parvin\n  Razzaghi", "title": "An Intelligent Safety System for Human-Centered Semi-Autonomous Vehicles", "comments": "15 pages and 5 figures, Submitted to the international conference on\n  Contemporary issues in Data Science (CiDaS 2019), Learn more about this\n  project at https://iasbs.ac.ir/~ansari/faraz", "journal-ref": "Nature Switzerland AG - Springer LNDECT 45(2020) 322-336", "doi": "10.1007/978-3-030-37309-2_26", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, automobile manufacturers make efforts to develop ways to make cars\nfully safe. Monitoring driver's actions by computer vision techniques to detect\ndriving mistakes in real-time and then planning for autonomous driving to avoid\nvehicle collisions is one of the most important issues that has been\ninvestigated in the machine vision and Intelligent Transportation Systems\n(ITS). The main goal of this study is to prevent accidents caused by fatigue,\ndrowsiness, and driver distraction. To avoid these incidents, this paper\nproposes an integrated safety system that continuously monitors the driver's\nattention and vehicle surroundings, and finally decides whether the actual\nsteering control status is safe or not. For this purpose, we equipped an\nordinary car called FARAZ with a vision system consisting of four mounted\ncameras along with a universal car tool for communicating with surrounding\nfactory-installed sensors and other car systems, and sending commands to\nactuators. The proposed system leverages a scene understanding pipeline using\ndeep convolutional encoder-decoder networks and a driver state detection\npipeline. We have been identifying and assessing domestic capabilities for the\ndevelopment of technologies specifically of the ordinary vehicles in order to\nmanufacture smart cars and eke providing an intelligent system to increase\nsafety and to assist the driver in various conditions/situations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 18:08:18 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 22:57:02 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Khojasteh", "Hadi Abdi", ""], ["Alipour", "Alireza Abbas", ""], ["Ansari", "Ebrahim", ""], ["Razzaghi", "Parvin", ""]]}, {"id": "1812.03974", "submitter": "Hung Phi Do", "authors": "Hung P. Do, Yi Guo, Andrew J. Yoon, and Krishna S. Nayak", "title": "Accuracy, Uncertainty, and Adaptability of Automatic Myocardial ASL\n  Segmentation using Deep CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PURPOSE: To apply deep CNN to the segmentation task in myocardial arterial\nspin labeled (ASL) perfusion imaging and to develop methods that measure\nuncertainty and that adapt the CNN model to a specific false positive vs. false\nnegative tradeoff.\n  METHODS: The Monte Carlo dropout (MCD) U-Net was trained on data from 22\nsubjects and tested on data from 6 heart transplant recipients. Manual\nsegmentation and regional myocardial blood flow (MBF) were available for\ncomparison. We consider two global uncertainty measures, named Dice Uncertainty\nand MCD Uncertainty, which were calculated with and without the use of manual\nsegmentation, respectively. Tversky loss function with a hyperparameter $\\beta$\nwas used to adapt the model to a specific false positive vs. false negative\ntradeoff.\n  RESULTS: The MCD U-Net achieved Dice coefficient of mean(std) = 0.91(0.04) on\nthe test set. MBF measured using automatic segmentations was highly correlated\nto that measured using the manual segmentation ($R^2$ = 0.96). Dice Uncertainty\nand MCD Uncertainty were in good agreement ($R^2$ = 0.64). As $\\beta$\nincreased, the false positive rate systematically decreased and false negative\nrate systematically increased.\n  CONCLUSION: We demonstrate the feasibility of deep CNN for automatic\nsegmentation of myocardial ASL, with good accuracy. We also introduce two\nsimple methods for assessing model uncertainty. Finally, we demonstrate the\nability to adapt the CNN model to a specific false positive vs. false negative\ntradeoff. These findings are directly relevant to automatic segmentation in\nquantitative cardiac MRI and are broadly applicable to automatic segmentation\nproblems in diagnostic imaging.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 18:49:32 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 05:12:54 GMT"}, {"version": "v3", "created": "Sun, 20 Oct 2019 21:43:22 GMT"}, {"version": "v4", "created": "Tue, 5 Nov 2019 04:42:31 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Do", "Hung P.", ""], ["Guo", "Yi", ""], ["Yoon", "Andrew J.", ""], ["Nayak", "Krishna S.", ""]]}, {"id": "1812.03982", "submitter": "Christoph Feichtenhofer", "authors": "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He", "title": "SlowFast Networks for Video Recognition", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SlowFast networks for video recognition. Our model involves (i) a\nSlow pathway, operating at low frame rate, to capture spatial semantics, and\n(ii) a Fast pathway, operating at high frame rate, to capture motion at fine\ntemporal resolution. The Fast pathway can be made very lightweight by reducing\nits channel capacity, yet can learn useful temporal information for video\nrecognition. Our models achieve strong performance for both action\nclassification and detection in video, and large improvements are pin-pointed\nas contributions by our SlowFast concept. We report state-of-the-art accuracy\non major video recognition benchmarks, Kinetics, Charades and AVA. Code has\nbeen made available at: https://github.com/facebookresearch/SlowFast\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 18:59:07 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 23:28:58 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 06:26:37 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Feichtenhofer", "Christoph", ""], ["Fan", "Haoqi", ""], ["Malik", "Jitendra", ""], ["He", "Kaiming", ""]]}, {"id": "1812.04042", "submitter": "Gianni Franchi", "authors": "Gianni Franchi, Angela Yao, Andreas Kolb", "title": "Supervised Deep Kriging for Single-Image Super-Resolution", "comments": "3 figures, for a better quality read the hal or GCPR version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel single-image super-resolution approach based on the\ngeostatistical method of kriging. Kriging is a zero-bias minimum-variance\nestimator that performs spatial interpolation based on a weighted average of\nknown observations. Rather than solving for the kriging weights via the\ntraditional method of inverting covariance matrices, we propose a supervised\nform in which we learn a deep network to generate said weights. We combine the\nkriging weight generation and kriging process into a joint network that can be\nlearned end-to-end. Our network achieves competitive super-resolution results\nas other state-of-the-art methods. In addition, since the super-resolution\nprocess follows a known statistical framework, we are able to estimate bias and\nvariance, something which is rarely possible for other deep networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 19:32:23 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Franchi", "Gianni", ""], ["Yao", "Angela", ""], ["Kolb", "Andreas", ""]]}, {"id": "1812.04056", "submitter": "Georgios Georgiadis", "authors": "Georgios Georgiadis", "title": "Accelerating Convolutional Neural Networks via Activation Map\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep learning revolution brought us an extensive array of neural network\narchitectures that achieve state-of-the-art performance in a wide variety of\nComputer Vision tasks including among others, classification, detection and\nsegmentation. In parallel, we have also been observing an unprecedented demand\nin computational and memory requirements, rendering the efficient use of neural\nnetworks in low-powered devices virtually unattainable. Towards this end, we\npropose a three-stage compression and acceleration pipeline that sparsifies,\nquantizes and entropy encodes activation maps of Convolutional Neural Networks.\nSparsification increases the representational power of activation maps leading\nto both acceleration of inference and higher model accuracy. Inception-V3 and\nMobileNet-V1 can be accelerated by as much as $1.6\\times$ with an increase in\naccuracy of $0.38\\%$ and $0.54\\%$ on the ImageNet and CIFAR-10 datasets\nrespectively. Quantizing and entropy coding the sparser activation maps lead to\nhigher compression over the baseline, reducing the memory cost of the network\nexecution. Inception-V3 and MobileNet-V1 activation maps, quantized to $16$\nbits, are compressed by as much as $6\\times$ with an increase in accuracy of\n$0.36\\%$ and $0.55\\%$ respectively.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 19:50:44 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 17:42:08 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Georgiadis", "Georgios", ""]]}, {"id": "1812.04058", "submitter": "Jingxiao Zheng", "authors": "Jingxiao Zheng, Rajeev Ranjan, Ching-Hui Chen, Jun-Cheng Chen, Carlos\n  D. Castillo, Rama Chellappa", "title": "An Automatic System for Unconstrained Video-Based Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning approaches have achieved performance surpassing humans\nfor still image-based face recognition, unconstrained video-based face\nrecognition is still a challenging task due to large volume of data to be\nprocessed and intra/inter-video variations on pose, illumination, occlusion,\nscene, blur, video quality, etc. In this work, we consider challenging\nscenarios for unconstrained video-based face recognition from multiple-shot\nvideos and surveillance videos with low-quality frames. To handle these\nproblems, we propose a robust and efficient system for unconstrained\nvideo-based face recognition, which is composed of modules for face/fiducial\ndetection, face association, and face recognition. First, we use multi-scale\nsingle-shot face detectors to efficiently localize faces in videos. The\ndetected faces are then grouped respectively through carefully designed face\nassociation methods, especially for multi-shot videos. Finally, the faces are\nrecognized by the proposed face matcher based on an unsupervised subspace\nlearning approach and a subspace-to-subspace similarity metric. Extensive\nexperiments on challenging video datasets, such as Multiple Biometric Grand\nChallenge (MBGC), Face and Ocular Challenge Series (FOCS), IARPA Janus\nSurveillance Video Benchmark (IJB-S) for low-quality surveillance videos and\nIARPA JANUS Benchmark B (IJB-B) for multiple-shot videos, demonstrate that the\nproposed system can accurately detect and associate faces from unconstrained\nvideos and effectively learn robust and discriminative features for\nrecognition.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 19:51:38 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 02:13:57 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 23:45:46 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zheng", "Jingxiao", ""], ["Ranjan", "Rajeev", ""], ["Chen", "Ching-Hui", ""], ["Chen", "Jun-Cheng", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1812.04072", "submitter": "Chen Liu", "authors": "Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, Jan Kautz", "title": "PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep neural architecture, PlaneRCNN, that detects and\nreconstructs piecewise planar surfaces from a single RGB image. PlaneRCNN\nemploys a variant of Mask R-CNN to detect planes with their plane parameters\nand segmentation masks. PlaneRCNN then jointly refines all the segmentation\nmasks with a novel loss enforcing the consistency with a nearby view during\ntraining. The paper also presents a new benchmark with more fine-grained plane\nsegmentations in the ground-truth, in which, PlaneRCNN outperforms existing\nstate-of-the-art methods with significant margins in the plane detection,\nsegmentation, and reconstruction metrics. PlaneRCNN makes an important step\ntowards robust plane extraction, which would have an immediate impact on a wide\nrange of applications including Robotics, Augmented Reality, and Virtual\nReality.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 20:35:55 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 00:24:15 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Liu", "Chen", ""], ["Kim", "Kihwan", ""], ["Gu", "Jinwei", ""], ["Furukawa", "Yasutaka", ""], ["Kautz", "Jan", ""]]}, {"id": "1812.04082", "submitter": "John Mern", "authors": "John Mern, Kyle Julian, Rachael E. Tompa, Mykel J. Kochenderfer", "title": "Visual Depth Mapping from Monocular Images using Recurrent Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable sense-and-avoid system is critical to enabling safe autonomous\noperation of unmanned aircraft. Existing sense-and-avoid methods often require\nspecialized sensors that are too large or power intensive for use on small\nunmanned vehicles. This paper presents a method to estimate object distances\nbased on visual image sequences, allowing for the use of low-cost, on-board\nmonocular cameras as simple collision avoidance sensors. We present a deep\nrecurrent convolutional neural network and training method to generate depth\nmaps from video sequences. Our network is trained using simulated camera and\ndepth data generated with Microsoft's AirSim simulator. Empirically, we show\nthat our model achieves superior performance compared to models generated using\nprior methods.We further demonstrate that the method can be used for\nsense-and-avoid of obstacles in simulation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 20:53:49 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Mern", "John", ""], ["Julian", "Kyle", ""], ["Tompa", "Rachael E.", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1812.04098", "submitter": "Jacob Shermeyer", "authors": "Jacob Shermeyer and Adam Van Etten", "title": "The Effects of Super-Resolution on Object Detection Performance in\n  Satellite Imagery", "comments": "To appear in EarthVision 2019,IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the application of super-resolution techniques to satellite\nimagery, and the effects of these techniques on object detection algorithm\nperformance. Specifically, we enhance satellite imagery beyond its native\nresolution, and test if we can identify various types of vehicles, planes, and\nboats with greater accuracy than native resolution. Using the Very Deep\nSuper-Resolution (VDSR) framework and a custom Random Forest Super-Resolution\n(RFSR) framework we generate enhancement levels of 2x, 4x, and 8x over five\ndistinct resolutions ranging from 30 cm to 4.8 meters. Using both native and\nsuper-resolved data, we then train several custom detection models using the\nSIMRDWN object detection framework. SIMRDWN combines a number of popular object\ndetection algorithms (e.g. SSD, YOLO) into a unified framework that is designed\nto rapidly detect objects in large satellite images. This approach allows us to\nquantify the effects of super-resolution techniques on object detection\nperformance across multiple classes and resolutions. We also quantify the\nperformance of object detection as a function of native resolution and object\npixel size. For our test set we note that performance degrades from mean\naverage precision (mAP) = 0.53 at 30 cm resolution, down to mAP = 0.11 at 4.8 m\nresolution. Super-resolving native 30 cm imagery to 15 cm yields the greatest\nbenefit; a 13-36% improvement in mAP. Super-resolution is less beneficial at\ncoarser resolutions, though still provides a small improvement in performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 21:19:28 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 19:47:15 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 16:58:39 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Shermeyer", "Jacob", ""], ["Van Etten", "Adam", ""]]}, {"id": "1812.04103", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Na Zou, Dinggang Shen, Shuiwang Ji", "title": "Non-local U-Net for Biomedical Image Segmentation", "comments": "In Proceedings of the 34th AAAI Conference on Artificial Intelligence\n  (AAAI), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown its great promise in various biomedical image\nsegmentation tasks. Existing models are typically based on U-Net and rely on an\nencoder-decoder architecture with stacked local operators to aggregate\nlong-range information gradually. However, only using the local operators\nlimits the efficiency and effectiveness. In this work, we propose the non-local\nU-Nets, which are equipped with flexible global aggregation blocks, for\nbiomedical image segmentation. These blocks can be inserted into U-Net as\nsize-preserving processes, as well as down-sampling and up-sampling layers. We\nperform thorough experiments on the 3D multimodality isointense infant brain MR\nimage segmentation task to evaluate the non-local U-Nets. Results show that our\nproposed models achieve top performances with fewer parameters and faster\ncomputation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 21:28:55 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 21:00:45 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Wang", "Zhengyang", ""], ["Zou", "Na", ""], ["Shen", "Dinggang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1812.04155", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill Dolan", "title": "Vision-based Navigation with Language-based Assistance via Imitation\n  Learning with Indirect Intervention", "comments": "In CVPR 2019, 16 pages, appendix included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Vision-based Navigation with Language-based Assistance (VNLA), a\ngrounded vision-language task where an agent with visual perception is guided\nvia language to find objects in photorealistic indoor environments. The task\nemulates a real-world scenario in that (a) the requester may not know how to\nnavigate to the target objects and thus makes requests by only specifying\nhigh-level end-goals, and (b) the agent is capable of sensing when it is lost\nand querying an advisor, who is more qualified at the task, to obtain language\nsubgoals to make progress. To model language-based assistance, we develop a\ngeneral framework termed Imitation Learning with Indirect Intervention (I3L),\nand propose a solution that is effective on the VNLA task. Empirical results\nshow that this approach significantly improves the success rate of the learning\nagent over other baselines in both seen and unseen environments. Our code and\ndata are publicly available at https://github.com/debadeepta/vnla .\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 23:48:25 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 13:40:30 GMT"}, {"version": "v3", "created": "Sat, 22 Dec 2018 03:27:28 GMT"}, {"version": "v4", "created": "Sat, 6 Apr 2019 02:02:42 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Nguyen", "Khanh", ""], ["Dey", "Debadeepta", ""], ["Brockett", "Chris", ""], ["Dolan", "Bill", ""]]}, {"id": "1812.04172", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Christoph Feichtenhofer, Du Tran, Jianbo Shi, Lorenzo\n  Torresani", "title": "Learning Discriminative Motion Features Through Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite huge success in the image domain, modern detection models such as\nFaster R-CNN have not been used nearly as much for video analysis. This is\narguably due to the fact that detection models are designed to operate on\nsingle frames and as a result do not have a mechanism for learning motion\nrepresentations directly from video. We propose a learning procedure that\nallows detection models such as Faster R-CNN to learn motion features directly\nfrom the RGB video data while being optimized with respect to a pose estimation\ntask. Given a pair of video frames---Frame A and Frame B---we force our model\nto predict human pose in Frame A using the features from Frame B. We do so by\nleveraging deformable convolutions across space and time. Our network learns to\nspatially sample features from Frame B in order to maximize pose detection\naccuracy in Frame A. This naturally encourages our network to learn motion\noffsets encoding the spatial correspondences between the two frames. We refer\nto these motion offsets as DiMoFs (Discriminative Motion Features).\n  In our experiments we show that our training scheme helps learn effective\nmotion cues, which can be used to estimate and localize salient human motion.\nFurthermore, we demonstrate that as a byproduct, our model also learns features\nthat lead to improved pose detection in still-images, and better keypoint\ntracking. Finally, we show how to leverage our learned model for the tasks of\nspatiotemporal action localization and fine-grained action recognition.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 01:06:56 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Bertasius", "Gedas", ""], ["Feichtenhofer", "Christoph", ""], ["Tran", "Du", ""], ["Shi", "Jianbo", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1812.04179", "submitter": "Fengchao Xiong", "authors": "Fengchao Xiong, Jun Zhou and Yuntao Qian", "title": "Material Based Object Tracking in Hyperspectral Videos: Benchmark and\n  Algorithms", "comments": "Update results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional color images only depict color intensities in red, green and blue\nchannels, often making object trackers fail in challenging scenarios, e.g.,\nbackground clutter and rapid changes of target appearance. Alternatively,\nmaterial information of targets contained in a large amount of bands of\nhyperspectral images (HSI) is more robust to these difficult conditions. In\nthis paper, we conduct a comprehensive study on how material information can be\nutilized to boost object tracking from three aspects: benchmark dataset,\nmaterial feature representation and material based tracking. In terms of\nbenchmark, we construct a dataset of fully-annotated videos, which contain both\nhyperspectral and color sequences of the same scene. Material information is\nrepresented by spectral-spatial histogram of multidimensional gradient, which\ndescribes the 3D local spectral-spatial structure in an HSI, and fractional\nabundances of constituted material components which encode the underlying\nmaterial distribution. These two types of features are embedded into\ncorrelation filters, yielding material based tracking. Experimental results on\nthe collected benchmark dataset show the potentials and advantages of material\nbased object tracking.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 01:35:15 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 12:17:17 GMT"}, {"version": "v3", "created": "Sun, 23 Dec 2018 17:03:21 GMT"}, {"version": "v4", "created": "Fri, 5 Apr 2019 00:45:07 GMT"}, {"version": "v5", "created": "Wed, 10 Jul 2019 14:34:15 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Xiong", "Fengchao", ""], ["Zhou", "Jun", ""], ["Qian", "Yuntao", ""]]}, {"id": "1812.04180", "submitter": "Charles Herrmann", "authors": "Charles Herrmann and Richard Strong Bowen and Ramin Zabih", "title": "Channel selection using Gumbel Softmax", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important applications such as mobile computing require reducing the\ncomputational costs of neural network inference. Ideally, applications would\nspecify their preferred tradeoff between accuracy and speed, and the network\nwould optimize this end-to-end, using classification error to remove parts of\nthe network. Increasing speed can be done either during training - e.g.,\npruning filters - or during inference - e.g., conditionally executing a subset\nof the layers. We propose a single end-to-end framework that can improve\ninference efficiency in both settings. We use a combination of batch activation\nloss and classification loss, and Gumbel reparameterization to learn network\nstructure. We train end-to-end, and the same technique supports pruning as well\nas conditional computation. We obtain promising experimental results for\nImageNet classification with ResNet (45-52% less computation).\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 01:41:07 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 15:13:54 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 21:44:10 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2020 22:35:18 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Herrmann", "Charles", ""], ["Bowen", "Richard Strong", ""], ["Zabih", "Ramin", ""]]}, {"id": "1812.04194", "submitter": "Lu Liu", "authors": "Lu Liu, Robby T. Tan, Shaodi You", "title": "Loss Guided Activation for Action Recognition in Still Images", "comments": "Accepted to appear in ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One significant problem of deep-learning based human action recognition is\nthat it can be easily misled by the presence of irrelevant objects or\nbackgrounds. Existing methods commonly address this problem by employing\nbounding boxes on the target humans as part of the input, in both training and\ntesting stages. This requirement of bounding boxes as part of the input is\nneeded to enable the methods to ignore irrelevant contexts and extract only\nhuman features. However, we consider this solution is inefficient, since the\nbounding boxes might not be available. Hence, instead of using a person\nbounding box as an input, we introduce a human-mask loss to automatically guide\nthe activations of the feature maps to the target human who is performing the\naction, and hence suppress the activations of misleading contexts. We propose a\nmulti-task deep learning method that jointly predicts the human action class\nand human location heatmap. Extensive experiments demonstrate our approach is\nmore robust compared to the baseline methods under the presence of irrelevant\nmisleading contexts. Our method achieves 94.06\\% and 40.65\\% (in terms of mAP)\non Stanford40 and MPII dataset respectively, which are 3.14\\% and 12.6\\%\nrelative improvements over the best results reported in the literature, and\nthus set new state-of-the-art results. Additionally, unlike some existing\nmethods, we eliminate the requirement of using a person bounding box as an\ninput during testing.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 02:43:45 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Liu", "Lu", ""], ["Tan", "Robby T.", ""], ["You", "Shaodi", ""]]}, {"id": "1812.04204", "submitter": "Ruohan Gao", "authors": "Ruohan Gao and Kristen Grauman", "title": "2.5D Visual Sound", "comments": "Published in CVPR 2019, project page:\n  http://vision.cs.utexas.edu/projects/2.5D_visual_sound/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binaural audio provides a listener with 3D sound sensation, allowing a rich\nperceptual experience of the scene. However, binaural recordings are scarcely\navailable and require nontrivial expertise and equipment to obtain. We propose\nto convert common monaural audio into binaural audio by leveraging video. The\nkey idea is that visual frames reveal significant spatial cues that, while\nexplicitly lacking in the accompanying single-channel audio, are strongly\nlinked to it. Our multi-modal approach recovers this link from unlabeled video.\nWe devise a deep convolutional neural network that learns to decode the\nmonaural (single-channel) soundtrack into its binaural counterpart by injecting\nvisual information about object and scene configurations. We call the resulting\noutput 2.5D visual sound---the visual stream helps \"lift\" the flat single\nchannel audio into spatialized sound. In addition to sound generation, we show\nthe self-supervised representation learned by our network benefits audio-visual\nsource separation. Our video results:\nhttp://vision.cs.utexas.edu/projects/2.5D_visual_sound/\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 03:23:10 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 16:42:48 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 17:22:40 GMT"}, {"version": "v4", "created": "Tue, 9 Apr 2019 16:11:53 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Gao", "Ruohan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1812.04207", "submitter": "Yanwei Li", "authors": "Yanwei Li, Xingang Wang, Shilei Zhang, Lingxi Xie, Wenqi Wu, Hongyuan\n  Yu, Zheng Zhu", "title": "Identity-Enhanced Network for Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition is a challenging task, arguably because of\nlarge intra-class variations and high inter-class similarities. The core\ndrawback of the existing approaches is the lack of ability to discriminate the\nchanges in appearance caused by emotions and identities. In this paper, we\npresent a novel identity-enhanced network (IDEnNet) to eliminate the negative\nimpact of identity factor and focus on recognizing facial expressions. Spatial\nfusion combined with self-constrained multi-task learning are adopted to\njointly learn the expression representations and identity-related information.\nWe evaluate our approach on three popular datasets, namely Oulu-CASIA, CK+ and\nMMI. IDEnNet improves the baseline consistently, and achieves the best or\ncomparable state-of-the-art on all three datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 03:40:52 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Li", "Yanwei", ""], ["Wang", "Xingang", ""], ["Zhang", "Shilei", ""], ["Xie", "Lingxi", ""], ["Wu", "Wenqi", ""], ["Yu", "Hongyuan", ""], ["Zhu", "Zheng", ""]]}, {"id": "1812.04210", "submitter": "Yudian Li", "authors": "Yinghao Xu, Xin Dong, Yudian Li, Hao Su", "title": "A Main/Subsidiary Network Framework for Simplifying Binary Neural\n  Network", "comments": "9 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce memory footprint and run-time latency, techniques such as neural\nnetwork pruning and binarization have been explored separately. However, it is\nunclear how to combine the best of the two worlds to get extremely small and\nefficient models. In this paper, we, for the first time, define the\nfilter-level pruning problem for binary neural networks, which cannot be solved\nby simply migrating existing structural pruning methods for full-precision\nmodels. A novel learning-based approach is proposed to prune filters in our\nmain/subsidiary network framework, where the main network is responsible for\nlearning representative features to optimize the prediction performance, and\nthe subsidiary component works as a filter selector on the main network. To\navoid gradient mismatch when training the subsidiary component, we propose a\nlayer-wise and bottom-up scheme. We also provide the theoretical and\nexperimental comparison between our learning-based and greedy rule-based\nmethods. Finally, we empirically demonstrate the effectiveness of our approach\napplied on several binary models, including binarized NIN, VGG-11, and\nResNet-18, on various image classification datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 03:55:00 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Xu", "Yinghao", ""], ["Dong", "Xin", ""], ["Li", "Yudian", ""], ["Su", "Hao", ""]]}, {"id": "1812.04215", "submitter": "Chiranjoy Chattopadhyay", "authors": "Asheet Kumar, Shivam Choudhary, Vaibhav Singh Khokhar, Vikas Meena,\n  Chiranjoy Chattopadhyay", "title": "Automatic Feature Weight Determination using Indexing and\n  Pseudo-Relevance Feedback for Multi-feature Content-Based Image Retrieval", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval (CBIR) is one of the most active research areas\nin multimedia information retrieval. Given a query image, the task is to search\nrelevant images in a repository. Low level features like color, texture, and\nshape feature vectors of an image are always considered to be an important\nattribute in CBIR system. Thus the performance of the CBIR system can be\nenhanced by combining these feature vectors. In this paper, we propose a novel\nCBIR framework by applying to index using multiclass SVM and finding the\nappropriate weights of the individual features automatically using the\nrelevance ratio and mean difference. We have taken four feature descriptors to\nrepresent color, texture and shape features. During retrieval, feature vectors\nof query image are combined, weighted and compared with feature vectors of\nimages in the database to rank order the results. Experiments were performed on\nfour benchmark datasets and performance is compared with existing techniques to\nvalidate the superiority of our proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 04:21:07 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Kumar", "Asheet", ""], ["Choudhary", "Shivam", ""], ["Khokhar", "Vaibhav Singh", ""], ["Meena", "Vikas", ""], ["Chattopadhyay", "Chiranjoy", ""]]}, {"id": "1812.04239", "submitter": "Xiu-Shen Wei", "authors": "Xiu-Shen Wei, Chen-Lin Zhang, Lingqiao Liu, Chunhua Shen and Jianxin\n  Wu", "title": "Coarse-to-fine: A RNN-based hierarchical attention model for vehicle\n  re-identification", "comments": "ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification is an important problem and becomes desirable with\nthe rapid expansion of applications in video surveillance and intelligent\ntransportation. By recalling the identification process of human vision, we are\naware that there exists a native hierarchical dependency when humans identify\ndifferent vehicles. Specifically, humans always firstly determine one vehicle's\ncoarse-grained category, i.e., the car model/type. Then, under the branch of\nthe predicted car model/type, they are going to identify specific vehicles by\nrelying on subtle visual cues, e.g., customized paintings and windshield\nstickers, at the fine-grained level. Inspired by the coarse-to-fine\nhierarchical process, we propose an end-to-end RNN-based Hierarchical Attention\n(RNN-HA) classification model for vehicle re-identification. RNN-HA consists of\nthree mutually coupled modules: the first module generates image\nrepresentations for vehicle images, the second hierarchical module models the\naforementioned hierarchical dependent relationship, and the last attention\nmodule focuses on capturing the subtle visual information distinguishing\nspecific vehicles from each other. By conducting comprehensive experiments on\ntwo vehicle re-identification benchmark datasets VeRi and VehicleID, we\ndemonstrate that the proposed model achieves superior performance over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 07:05:15 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Wei", "Xiu-Shen", ""], ["Zhang", "Chen-Lin", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Wu", "Jianxin", ""]]}, {"id": "1812.04240", "submitter": "Tianyu Zhao", "authors": "Tianyu Zhao, Wenqi Ren, Changqing Zhang, Dongwei Ren, Qinghua Hu", "title": "Unsupervised Degradation Learning for Single Image Super-Resolution", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolution Neural Networks (CNN) have achieved significant performance\non single image super-resolution (SR) recently. However, existing CNN-based\nmethods use artificially synthetic low-resolution (LR) and high-resolution (HR)\nimage pairs to train networks, which cannot handle real-world cases since the\ndegradation from HR to LR is much more complex than manually designed. To solve\nthis problem, we propose a real-world LR images guided bi-cycle network for\nsingle image super-resolution, in which the bidirectional structural\nconsistency is exploited to train both the degradation and SR reconstruction\nnetworks in an unsupervised way. Specifically, we propose a degradation network\nto model the real-world degradation process from HR to LR via generative\nadversarial networks, and these generated realistic LR images paired with\nreal-world HR images are exploited for training the SR reconstruction network,\nforming the first cycle. Then in the second reverse cycle, consistency of\nreal-world LR images are exploited to further stabilize the training of SR\nreconstruction and degradation networks. Extensive experiments on both\nsynthetic and real-world images demonstrate that the proposed algorithm\nperforms favorably against state-of-the-art single image SR methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 07:07:58 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 07:10:41 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Zhao", "Tianyu", ""], ["Ren", "Wenqi", ""], ["Zhang", "Changqing", ""], ["Ren", "Dongwei", ""], ["Hu", "Qinghua", ""]]}, {"id": "1812.04243", "submitter": "Wei He", "authors": "Wei He, Quanming Yao, Chao Li, Naoto Yokoya, and Qibin Zhao", "title": "Non-local Meets Global: An Integrated Paradigm for Hyperspectral\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local low-rank tensor approximation has been developed as a\nstate-of-the-art method for hyperspectral image (HSI) denoising. Unfortunately,\nwith more spectral bands for HSI, while the running time of these methods\nsignificantly increases, their denoising performance benefits little. In this\npaper, we claim that the HSI underlines a global spectral low-rank subspace,\nand the spectral subspaces of each full band patch groups should underlie this\nglobal low-rank subspace. This motivates us to propose a unified\nspatial-spectral paradigm for HSI denoising. As the new model is hard to\noptimize, we further propose an efficient algorithm for optimization, which is\nmotivated by alternating minimization. This is done by first learning a\nlow-dimensional projection and the related reduced image from the noisy HSI.\nThen, the non-local low-rank denoising and iterative regularization are\ndeveloped to refine the reduced image and projection, respectively. Finally,\nexperiments on synthetic and both real datasets demonstrate the superiority\nagainst the other state-of-the-arts HSI denoising methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 07:26:07 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 04:00:22 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["He", "Wei", ""], ["Yao", "Quanming", ""], ["Li", "Chao", ""], ["Yokoya", "Naoto", ""], ["Zhao", "Qibin", ""]]}, {"id": "1812.04244", "submitter": "Shaoshuai Shi", "authors": "Shaoshuai Shi, Xiaogang Wang, Hongsheng Li", "title": "PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose PointRCNN for 3D object detection from raw point\ncloud. The whole framework is composed of two stages: stage-1 for the bottom-up\n3D proposal generation and stage-2 for refining proposals in the canonical\ncoordinates to obtain the final detection results. Instead of generating\nproposals from RGB image or projecting point cloud to bird's view or voxels as\nprevious methods do, our stage-1 sub-network directly generates a small number\nof high-quality 3D proposals from point cloud in a bottom-up manner via\nsegmenting the point cloud of the whole scene into foreground points and\nbackground. The stage-2 sub-network transforms the pooled points of each\nproposal to canonical coordinates to learn better local spatial features, which\nis combined with global semantic features of each point learned in stage-1 for\naccurate box refinement and confidence prediction. Extensive experiments on the\n3D detection benchmark of KITTI dataset show that our proposed architecture\noutperforms state-of-the-art methods with remarkable margins by using only\npoint cloud as input. The code is available at\nhttps://github.com/sshaoshuai/PointRCNN.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 07:27:56 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 15:50:01 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Shi", "Shaoshuai", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1812.04246", "submitter": "Ryota Yoshihashi", "authors": "Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida,\n  Takeshi Naemura", "title": "Classification-Reconstruction Learning for Open-Set Recognition", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-set classification is a problem of handling `unknown' classes that are\nnot contained in the training dataset, whereas traditional classifiers assume\nthat only known classes appear in the test environment. Existing open-set\nclassifiers rely on deep networks trained in a supervised manner on known\nclasses in the training set; this causes specialization of learned\nrepresentations to known classes and makes it hard to distinguish unknowns from\nknowns. In contrast, we train networks for joint classification and\nreconstruction of input data. This enhances the learned representation so as to\npreserve information useful for separating unknowns from knowns, as well as to\ndiscriminate classes of knowns. Our novel Classification-Reconstruction\nlearning for Open-Set Recognition (CROSR) utilizes latent representations for\nreconstruction and enables robust unknown detection without harming the\nknown-class classification accuracy. Extensive experiments reveal that the\nproposed method outperforms existing deep open-set classifiers in multiple\nstandard datasets and is robust to diverse outliers. The code is available in\nhttps://nae-lab.org/~rei/research/crosr/.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 07:34:28 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 06:24:38 GMT"}, {"version": "v3", "created": "Sun, 6 Oct 2019 07:55:48 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yoshihashi", "Ryota", ""], ["Shao", "Wen", ""], ["Kawakami", "Rei", ""], ["You", "Shaodi", ""], ["Iida", "Makoto", ""], ["Naemura", "Takeshi", ""]]}, {"id": "1812.04275", "submitter": "Peng Lu", "authors": "Peng Lu, Gao Huang, Hangyu Lin, Wenming Yang, Guodong Guo, Yanwei Fu", "title": "Domain-Aware SE Network for Sketch-based Image Retrieval with\n  Multiplicative Euclidean Margin Softmax", "comments": "Accepted in ACMMM 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for Sketch-Based Image Retrieval (SBIR),\nfor which the key is to bridge the gap between sketches and photos in terms of\nthe data representation. Inspired by channel-wise attention explored in recent\nyears, we present a Domain-Aware Squeeze-and-Excitation (DASE) network, which\nseamlessly incorporates the prior knowledge of sample sketch or photo into SE\nmodule and make the SE module capable of emphasizing appropriate channels\naccording to domain signal. Accordingly, the proposed network can switch its\nmode to achieve a better domain feature with lower intra-class discrepancy.\nMoreover, while previous works simply focus on minimizing intra-class distance\nand maximizing inter-class distance, we introduce a loss function, named\nMultiplicative Euclidean Margin Softmax (MEMS), which introduces multiplicative\nEuclidean margin into feature space and ensure that the maximum intra-class\ndistance is smaller than the minimum inter-class distance. This facilitates\nlearning a highly discriminative feature space and ensures a more accurate\nimage retrieval result. Extensive experiments are conducted on two widely used\nSBIR benchmark datasets. Our approach achieves better results on both datasets,\nsurpassing the state-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 08:57:31 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 05:44:41 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Lu", "Peng", ""], ["Huang", "Gao", ""], ["Lin", "Hangyu", ""], ["Yang", "Wenming", ""], ["Guo", "Guodong", ""], ["Fu", "Yanwei", ""]]}, {"id": "1812.04287", "submitter": "Yazhou Ren", "authors": "Yazhou Ren, Ni Wang, Mingxia Li, Zenglin Xu", "title": "Deep Density-based Image Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep clustering, which is able to perform feature learning that\nfavors clustering tasks via deep neural networks, has achieved remarkable\nperformance in image clustering applications. However, the existing deep\nclustering algorithms generally need the number of clusters in advance, which\nis usually unknown in real-world tasks. In addition, the initial cluster\ncenters in the learned feature space are generated by $k$-means. This only\nworks well on spherical clusters and probably leads to unstable clustering\nresults. In this paper, we propose a two-stage deep density-based image\nclustering (DDC) framework to address these issues. The first stage is to train\na deep convolutional autoencoder (CAE) to extract low-dimensional feature\nrepresentations from high-dimensional image data, and then apply t-SNE to\nfurther reduce the data to a 2-dimensional space favoring density-based\nclustering algorithms. The second stage is to apply the developed density-based\nclustering technique on the 2-dimensional embedded data to automatically\nrecognize an appropriate number of clusters with arbitrary shapes. Concretely,\na number of local clusters are generated to capture the local structures of\nclusters, and then are merged via their density relationship to form the final\nclustering result. Experiments demonstrate that the proposed DDC achieves\ncomparable or even better clustering performance than state-of-the-art deep\nclustering methods, even though the number of clusters is not given.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 09:27:20 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Ren", "Yazhou", ""], ["Wang", "Ni", ""], ["Li", "Mingxia", ""], ["Xu", "Zenglin", ""]]}, {"id": "1812.04302", "submitter": "Weikai Chen", "authors": "Weikai Chen, Xiaoguang Han, Guanbin Li, Chao Chen, Jun Xing, Yajie\n  Zhao, Hao Li", "title": "Deep RBFNet: Point Cloud Feature Learning using Radial Basis Functions", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional object recognition has recently achieved great progress\nthanks to the development of effective point cloud-based learning frameworks,\nsuch as PointNet and its extensions. However, existing methods rely heavily on\nfully connected layers, which introduce a significant amount of parameters,\nmaking the network harder to train and prone to overfitting problems. In this\npaper, we propose a simple yet effective framework for point set feature\nlearning by leveraging a nonlinear activation layer encoded by Radial Basis\nFunction (RBF) kernels. Unlike PointNet variants, that fail to recognize local\npoint patterns, our approach explicitly models the spatial distribution of\npoint clouds by aggregating features from sparsely distributed RBF kernels. A\ntypical RBF kernel, e.g. Gaussian function, naturally penalizes long-distance\nresponse and is only activated by neighboring points. Such localized response\ngenerates highly discriminative features given different point distributions.\nIn addition, our framework allows the joint optimization of kernel distribution\nand its receptive field, automatically evolving kernel configurations in an\nend-to-end manner. We demonstrate that the proposed network with a single RBF\nlayer can outperform the state-of-the-art Pointnet++ in terms of classification\naccuracy for 3D object recognition tasks. Moreover, the introduction of\nnonlinear mappings significantly reduces the number of network parameters and\ncomputational cost, enabling significantly faster training and a deployable\npoint cloud recognition solution on portable devices with limited resources.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 09:45:04 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 11:39:29 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Chen", "Weikai", ""], ["Han", "Xiaoguang", ""], ["Li", "Guanbin", ""], ["Chen", "Chao", ""], ["Xing", "Jun", ""], ["Zhao", "Yajie", ""], ["Li", "Hao", ""]]}, {"id": "1812.04303", "submitter": "Enrico Grisan", "authors": "Marco Virgulin and Marco Castellaro and Enrico Grisan and Fabio\n  Marcuzzi", "title": "Analytic heuristics for a fast DSC-MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we propose a deterministic approach for the reconstruction of\nDynamic Susceptibility Contrast magnetic resonance imaging data and compare it\nwith the compressed sensing solution existing in the literature for the same\nproblem. Our study is based on the mathematical analysis of the problem, which\nis computationally intractable because of its non polynomial complexity, but\nsuggests simple heuristics that perform quite well. We give results on real\nimages and on artificial phantoms with added noise.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 09:46:45 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Virgulin", "Marco", ""], ["Castellaro", "Marco", ""], ["Grisan", "Enrico", ""], ["Marcuzzi", "Fabio", ""]]}, {"id": "1812.04351", "submitter": "Kohei Watanabe", "authors": "Kohei Watanabe, Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada", "title": "Multichannel Semantic Segmentation with Unsupervised Domain Adaptation", "comments": "published on AUTONUE Workshops of ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most contemporary robots have depth sensors, and research on semantic\nsegmentation with RGBD images has shown that depth images boost the accuracy of\nsegmentation. Since it is time-consuming to annotate images with semantic\nlabels per pixel, it would be ideal if we could avoid this laborious work by\nutilizing an existing dataset or a synthetic dataset which we can generate on\nour own. Robot motions are often tested in a synthetic environment, where\nmultichannel (eg, RGB + depth + instance boundary) images plus their\npixel-level semantic labels are available. However, models trained simply on\nsynthetic images tend to demonstrate poor performance on real images. In order\nto address this, we propose two approaches that can efficiently exploit\nmultichannel inputs combined with an unsupervised domain adaptation (UDA)\nalgorithm. One is a fusion-based approach that uses depth images as inputs. The\nother is a multitask learning approach that uses depth images as outputs. We\ndemonstrated that the segmentation results were improved by using a multitask\nlearning approach with a post-process and created a benchmark for this task.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 12:26:38 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Watanabe", "Kohei", ""], ["Saito", "Kuniaki", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1812.04353", "submitter": "Thalaiyasingam Ajanthan", "authors": "Thalaiyasingam Ajanthan, Puneet K. Dokania, Richard Hartley, and\n  Philip H. S. Torr", "title": "Proximal Mean-field for Neural Network Quantization", "comments": null, "journal-ref": "ICCV, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing large Neural Networks (NN) by quantizing the parameters, while\nmaintaining the performance is highly desirable due to reduced memory and time\ncomplexity. In this work, we cast NN quantization as a discrete labelling\nproblem, and by examining relaxations, we design an efficient iterative\noptimization procedure that involves stochastic gradient descent followed by a\nprojection. We prove that our simple projected gradient descent approach is, in\nfact, equivalent to a proximal version of the well-known mean-field method.\nThese findings would allow the decades-old and theoretically grounded research\non MRF optimization to be used to design better network quantization schemes.\nOur experiments on standard classification datasets (MNIST, CIFAR10/100,\nTinyImageNet) with convolutional and residual architectures show that our\nalgorithm obtains fully-quantized networks with accuracies very close to the\nfloating-point reference networks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 12:27:54 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 06:21:21 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 23:27:28 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Ajanthan", "Thalaiyasingam", ""], ["Dokania", "Puneet K.", ""], ["Hartley", "Richard", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1812.04368", "submitter": "Yuchao Li", "authors": "Yuchao Li, Shaohui Lin, Baochang Zhang, Jianzhuang Liu, David\n  Doermann, Yongjian Wu, Feiyue Huang, Rongrong Ji", "title": "Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression", "comments": "10 pagers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing convolutional neural networks (CNNs) has received ever-increasing\nresearch focus. However, most existing CNN compression methods do not interpret\ntheir inherent structures to distinguish the implicit redundancy. In this\npaper, we investigate the problem of CNN compression from a novel interpretable\nperspective. The relationship between the input feature maps and 2D kernels is\nrevealed in a theoretical framework, based on which a kernel sparsity and\nentropy (KSE) indicator is proposed to quantitate the feature map importance in\na feature-agnostic manner to guide model compression. Kernel clustering is\nfurther conducted based on the KSE indicator to accomplish high-precision CNN\ncompression. KSE is capable of simultaneously compressing each layer in an\nefficient way, which is significantly faster compared to previous data-driven\nfeature map pruning methods. We comprehensively evaluate the compression and\nspeedup of the proposed method on CIFAR-10, SVHN and ImageNet 2012. Our method\ndemonstrates superior performance gains over previous ones. In particular, it\nachieves 4.7 \\times FLOPs reduction and 2.9 \\times compression on ResNet-50\nwith only a Top-5 accuracy drop of 0.35% on ImageNet 2012, which significantly\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 12:52:22 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 08:44:02 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Yuchao", ""], ["Lin", "Shaohui", ""], ["Zhang", "Baochang", ""], ["Liu", "Jianzhuang", ""], ["Doermann", "David", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""]]}, {"id": "1812.04377", "submitter": "Arindam Chowdhury", "authors": "Vishwanath D, Rohit Rahul, Gunjan Sehgal, Swati, Arindam Chowdhury,\n  Monika Sharma, Lovekesh Vig, Gautam Shroff, and Ashwin Srinivasan", "title": "Deep Reader: Information extraction from Document images via relation\n  extraction and Natural Language", "comments": "Published in 3rd International Workshop on Robust Reading at Asian\n  Conference of Computer Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in the area of Computer Vision with state-of-art Neural\nNetworks has given a boost to Optical Character Recognition (OCR) accuracies.\nHowever, extracting characters/text alone is often insufficient for relevant\ninformation extraction as documents also have a visual structure that is not\ncaptured by OCR. Extracting information from tables, charts, footnotes, boxes,\nheadings and retrieving the corresponding structured representation for the\ndocument remains a challenge and finds application in a large number of\nreal-world use cases. In this paper, we propose a novel enterprise based\nend-to-end framework called DeepReader which facilitates information extraction\nfrom document images via identification of visual entities and populating a\nmeta relational model across different entities in the document image. The\nmodel schema allows for an easy to understand abstraction of the entities\ndetected by the deep vision models and the relationships between them.\nDeepReader has a suite of state-of-the-art vision algorithms which are applied\nto recognize handwritten and printed text, eliminate noisy effects, identify\nthe type of documents and detect visual entities like tables, lines and boxes.\nDeep Reader maps the extracted entities into a rich relational schema so as to\ncapture all the relevant relationships between entities (words, textboxes,\nlines etc) detected in the document. Relevant information and fields can then\nbe extracted from the document by writing SQL queries on top of the\nrelationship tables. A natural language based interface is added on top of the\nrelationship schema so that a non-technical user, specifying the queries in\nnatural language, can fetch the information with minimal effort. In this paper,\nwe also demonstrate many different capabilities of Deep Reader and report\nresults on a real-world use case.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 13:09:13 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 16:46:59 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["D", "Vishwanath", ""], ["Rahul", "Rohit", ""], ["Sehgal", "Gunjan", ""], ["Swati", "", ""], ["Chowdhury", "Arindam", ""], ["Sharma", "Monika", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""], ["Srinivasan", "Ashwin", ""]]}, {"id": "1812.04418", "submitter": "Matthias K\\\"orschens", "authors": "Matthias K\\\"orschens, Bj\\\"orn Barz, Joachim Denzler", "title": "Towards Automatic Identification of Elephants in the Wild", "comments": "Presented at the AI for Wildlife Conservation (AIWC) 2018 workshop in\n  Stockholm (https://sites.google.com/a/usc.edu/aiwc/home)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying animals from a large group of possible individuals is very\nimportant for biodiversity monitoring and especially for collecting data on a\nsmall number of particularly interesting individuals, as these have to be\nidentified first before this can be done. Identifying them can be a very\ntime-consuming task. This is especially true, if the animals look very similar\nand have only a small number of distinctive features, like elephants do. In\nmost cases the animals stay at one place only for a short period of time during\nwhich the animal needs to be identified for knowing whether it is important to\ncollect new data on it. For this reason, a system supporting the researchers in\nidentifying elephants to speed up this process would be of great benefit. In\nthis paper, we present such a system for identifying elephants in the face of a\nlarge number of individuals with only few training images per individual. For\nthat purpose, we combine object part localization, off-the-shelf CNN features,\nand support vector machine classification to provide field researches with\nproposals of possible individuals given new images of an elephant. The\nperformance of our system is demonstrated on a dataset comprising a total of\n2078 images of 276 individual elephants, where we achieve 56% top-1 test\naccuracy and 80% top-10 accuracy. To deal with occlusion, varying viewpoints,\nand different poses present in the dataset, we furthermore enable the analysts\nto provide the system with multiple images of the same elephant to be\nidentified and aggregate confidence values generated by the classifier. With\nthat, our system achieves a top-1 accuracy of 74% and a top-10 accuracy of 88%\non the held-out test dataset.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 14:13:16 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["K\u00f6rschens", "Matthias", ""], ["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "1812.04427", "submitter": "Zhiwu Lu", "authors": "Nanyi Fei, Jiechao Guan, Zhiwu Lu, Tao Xiang, and Ji-Rong Wen", "title": "Zero-Shot Learning with Sparse Attribute Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize a set of unseen classes without\nany training images. The standard approach to ZSL requires a set of training\nimages annotated with seen class labels and a semantic descriptor for\nseen/unseen classes (attribute vector is the most widely used). Class\nlabel/attribute annotation is expensive; it thus severely limits the\nscalability of ZSL. In this paper, we define a new ZSL setting where only a few\nannotated images are collected from each seen class. This is clearly more\nchallenging yet more realistic than the conventional ZSL setting. To overcome\nthe resultant image-level attribute sparsity, we propose a novel inductive ZSL\nmodel termed sparse attribute propagation (SAP) by propagating attribute\nannotations to more unannotated images using sparse coding. This is followed by\nlearning bidirectional projections between features and attributes for ZSL. An\nefficient solver is provided, together with rigorous theoretic algorithm\nanalysis. With our SAP, we show that a ZSL training dataset can now be\naugmented by the abundant web images returned by image search engine, to\nfurther improve the model performance. Moreover, the general applicability of\nSAP is demonstrated on solving the social image annotation (SIA) problem.\nExtensive experiments show that our model achieves superior performance on both\nZSL and SIA.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 14:28:20 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 07:25:39 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Fei", "Nanyi", ""], ["Guan", "Jiechao", ""], ["Lu", "Zhiwu", ""], ["Xiang", "Tao", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1812.04429", "submitter": "Zhiwu Lu", "authors": "Mingyu Ding, An Zhao, Zhiwu Lu, Tao Xiang, and Ji-Rong Wen", "title": "Face-Focused Cross-Stream Network for Deception Detection in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated deception detection (ADD) from real-life videos is a challenging\ntask. It specifically needs to address two problems: (1) Both face and body\ncontain useful cues regarding whether a subject is deceptive. How to\neffectively fuse the two is thus key to the effectiveness of an ADD model. (2)\nReal-life deceptive samples are hard to collect; learning with limited training\ndata thus challenges most deep learning based ADD models. In this work, both\nproblems are addressed. Specifically, for face-body multimodal learning, a\nnovel face-focused cross-stream network (FFCSN) is proposed. It differs\nsignificantly from the popular two-stream networks in that: (a) face detection\nis added into the spatial stream to capture the facial expressions explicitly,\nand (b) correlation learning is performed across the spatial and temporal\nstreams for joint deep feature learning across both face and body. To address\nthe training data scarcity problem, our FFCSN model is trained with both meta\nlearning and adversarial learning. Extensive experiments show that our FFCSN\nmodel achieves state-of-the-art results. Further, the proposed FFCSN model as\nwell as its robust training strategy are shown to be generally applicable to\nother human-centric video analysis tasks such as emotion recognition from\nuser-generated videos.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 14:32:20 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Ding", "Mingyu", ""], ["Zhao", "An", ""], ["Lu", "Zhiwu", ""], ["Xiang", "Tao", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1812.04451", "submitter": "Zhizhan Zheng", "authors": "Zhizhan Zheng, Feiyun Zhang", "title": "Coconditional Autoencoding Adversarial Networks for Chinese Font Feature\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel framework named Coconditional Autoencoding\nAdversarial Networks (CocoAAN) for Chinese font learning, which jointly learns\na generation network and two encoding networks of different feature domains\nusing an adversarial process. The encoding networks map the glyph images into\nstyle and content features respectively via the pairwise substitution\noptimization strategy, and the generation network maps these two kinds of\nfeatures to glyph samples. Together with a discriminative network conditioned\non the extracted features, our framework succeeds in producing\nrealistic-looking Chinese glyph images flexibly. Unlike previous models relying\non the complex segmentation of Chinese components or strokes, our model can\n\"parse\" structures in an unsupervised way, through which the content feature\nrepresentation of each character is captured. Experiments demonstrate our\nframework has a powerful generalization capacity to other unseen fonts and\ncharacters.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 15:08:23 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 14:22:34 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Zheng", "Zhizhan", ""], ["Zhang", "Feiyun", ""]]}, {"id": "1812.04510", "submitter": "Fuzail Khan", "authors": "Fuzail Khan", "title": "Facial Expression Recognition using Facial Landmark Detection and\n  Feature Extraction via Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposed framework in this paper has the primary objective of classifying\nthe facial expression shown by a person. These classifiable expressions can be\nany one of the six universal emotions along with the neutral emotion. After the\ninitial facial localization is performed, facial landmark detection and feature\nextraction are applied where in the landmarks are determined to be the fiducial\nfeatures: the eyebrows, eyes, nose and lips. This is primarily done using\nstate-of-the-art facial landmark detection algorithms as well as traditional\nedge and corner point detection methods using Sobel filters and Shi Tomasi\ncorner point detection methods respectively. This leads to generation of input\nfeature vectors being formulated using Euclidean distances and trained into a\nMulti-Layer Perceptron (MLP) neural network in order to classify the expression\nbeing displayed. The results achieved have further dealt with higher uniformity\nin certain emotions and the inherently subjective nature of expression.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 18:34:19 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 21:36:37 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 21:22:22 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Khan", "Fuzail", ""]]}, {"id": "1812.04558", "submitter": "Tushar Nagarajan", "authors": "Tushar Nagarajan, Christoph Feichtenhofer, Kristen Grauman", "title": "Grounded Human-Object Interaction Hotspots from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to interact with objects is an important step towards embodied\nvisual intelligence, but existing techniques suffer from heavy supervision or\nsensing requirements. We propose an approach to learn human-object interaction\n\"hotspots\" directly from video. Rather than treat affordances as a manually\nsupervised semantic segmentation task, our approach learns about interactions\nby watching videos of real human behavior and anticipating afforded actions.\nGiven a novel image or video, our model infers a spatial hotspot map indicating\nhow an object would be manipulated in a potential interaction-- even if the\nobject is currently at rest. Through results with both first and third person\nvideo, we show the value of grounding affordances in real human-object\ninteractions. Not only are our weakly supervised hotspots competitive with\nstrongly supervised affordance methods, but they can also anticipate object\ninteraction for novel object categories.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 17:25:57 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 23:53:56 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Nagarajan", "Tushar", ""], ["Feichtenhofer", "Christoph", ""], ["Grauman", "Kristen", ""]]}, {"id": "1812.04571", "submitter": "Pawel Mlynarski", "authors": "Pawel Mlynarski, Herv\\'e Delingette, Antonio Criminisi, Nicholas\n  Ayache", "title": "Deep Learning with Mixed Supervision for Brain Tumor Segmentation", "comments": "Submitted to SPIE Journal of Medical Imaging", "journal-ref": null, "doi": "10.1117/1.JMI.6.3.034002", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current state-of-the-art methods for tumor segmentation are based\non machine learning models trained on manually segmented images. This type of\ntraining data is particularly costly, as manual delineation of tumors is not\nonly time-consuming but also requires medical expertise. On the other hand,\nimages with a provided global label (indicating presence or absence of a tumor)\nare less informative but can be obtained at a substantially lower cost. In this\npaper, we propose to use both types of training data (fully-annotated and\nweakly-annotated) to train a deep learning model for segmentation. The idea of\nour approach is to extend segmentation networks with an additional branch\nperforming image-level classification. The model is jointly trained for\nsegmentation and classification tasks in order to exploit information contained\nin weakly-annotated images while preventing the network to learn features which\nare irrelevant for the segmentation task. We evaluate our method on the\nchallenging task of brain tumor segmentation in Magnetic Resonance images from\nBRATS 2018 challenge. We show that the proposed approach provides a significant\nimprovement of segmentation performance compared to the standard supervised\nlearning. The observed improvement is proportional to the ratio between\nweakly-annotated and fully-annotated images available for training.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 16:03:27 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Mlynarski", "Pawel", ""], ["Delingette", "Herv\u00e9", ""], ["Criminisi", "Antonio", ""], ["Ayache", "Nicholas", ""]]}, {"id": "1812.04599", "submitter": "Konrad Zolna", "authors": "Konrad Zolna and Michal Zajac and Negar Rostamzadeh and Pedro O.\n  Pinheiro", "title": "Adversarial Framing for Image and Video Classification", "comments": "This is an extended version of the paper published at 33rd AAAI\n  Conference on Artificial Intelligence (see\n  https://doi.org/10.1609/aaai.v33i01.330110077 )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are prone to adversarial attacks. In general, such attacks\ndeteriorate the quality of the input by either slightly modifying most of its\npixels, or by occluding it with a patch. In this paper, we propose a method\nthat keeps the image unchanged and only adds an adversarial framing on the\nborder of the image. We show empirically that our method is able to\nsuccessfully attack state-of-the-art methods on both image and video\nclassification problems. Notably, the proposed method results in a universal\nattack which is very fast at test time. Source code can be found at\nhttps://github.com/zajaczajac/adv_framing .\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 18:39:29 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 18:18:56 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 14:03:07 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Zolna", "Konrad", ""], ["Zajac", "Michal", ""], ["Rostamzadeh", "Negar", ""], ["Pinheiro", "Pedro O.", ""]]}, {"id": "1812.04604", "submitter": "David Chan", "authors": "Biye Jiang, David M. Chan, Tianhao Zhang, John F. Canny", "title": "Diagnostic Visualization for Deep Neural Networks Using Stochastic\n  Gradient Langevin Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internal states of most deep neural networks are difficult to interpret,\nwhich makes diagnosis and debugging during training challenging. Activation\nmaximization methods are widely used, but lead to multiple optima and are hard\nto interpret (appear noise-like) for complex neurons. Image-based methods use\nmaximally-activating image regions which are easier to interpret, but do not\nprovide pixel-level insight into why the neuron responds to them. In this work\nwe introduce an MCMC method: Langevin Dynamics Activation Maximization (LDAM),\nwhich is designed for diagnostic visualization. LDAM provides two affordances\nin combination: the ability to explore the set of maximally activating\npre-images, and the ability to trade-off interpretability and pixel-level\naccuracy using a GAN-style discriminator as a regularizer. We present case\nstudies on MNIST, CIFAR and ImageNet datasets exploring these trade-offs.\nFinally we show that diagnostic visualization using LDAM leads to a novel\ninsight into the parameter averaging method for deep net training.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 18:43:52 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Jiang", "Biye", ""], ["Chan", "David M.", ""], ["Zhang", "Tianhao", ""], ["Canny", "John F.", ""]]}, {"id": "1812.04605", "submitter": "Zachary Teed", "authors": "Zachary Teed and Jia Deng", "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeepV2D, an end-to-end deep learning architecture for predicting\ndepth from video. DeepV2D combines the representation ability of neural\nnetworks with the geometric principles governing image formation. We compose a\ncollection of classical geometric algorithms, which are converted into\ntrainable modules and combined into an end-to-end differentiable architecture.\nDeepV2D interleaves two stages: motion estimation and depth estimation. During\ninference, motion and depth estimation are alternated and converge to accurate\ndepth. Code is available https://github.com/princeton-vl/DeepV2D.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 18:47:12 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 16:17:04 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 21:59:13 GMT"}, {"version": "v4", "created": "Mon, 27 Apr 2020 19:17:43 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Teed", "Zachary", ""], ["Deng", "Jia", ""]]}, {"id": "1812.04606", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Mantas Mazeika and Thomas Dietterich", "title": "Deep Anomaly Detection with Outlier Exposure", "comments": "ICLR 2019; PyTorch code available at\n  https://github.com/hendrycks/outlier-exposure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to detect anomalous inputs when deploying machine learning\nsystems. The use of larger and more complex inputs in deep learning magnifies\nthe difficulty of distinguishing between anomalous and in-distribution\nexamples. At the same time, diverse image and text data are available in\nenormous quantities. We propose leveraging these data to improve deep anomaly\ndetection by training anomaly detectors against an auxiliary dataset of\noutliers, an approach we call Outlier Exposure (OE). This enables anomaly\ndetectors to generalize and detect unseen anomalies. In extensive experiments\non natural language processing and small- and large-scale vision tasks, we find\nthat Outlier Exposure significantly improves detection performance. We also\nobserve that cutting-edge generative models trained on CIFAR-10 may assign\nhigher likelihoods to SVHN images than to CIFAR-10 images; we use OE to\nmitigate this issue. We also analyze the flexibility and robustness of Outlier\nExposure, and identify characteristics of the auxiliary dataset that improve\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 18:49:50 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 18:57:19 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 20:34:44 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Hendrycks", "Dan", ""], ["Mazeika", "Mantas", ""], ["Dietterich", "Thomas", ""]]}, {"id": "1812.04652", "submitter": "Jacob Reinhold", "authors": "Jacob C. Reinhold, Blake E. Dewey, Aaron Carass and Jerry L. Prince", "title": "Evaluating the Impact of Intensity Normalization on MR Image Synthesis", "comments": "SPIE Medical Imaging 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image synthesis learns a transformation from the intensity features of an\ninput image to yield a different tissue contrast of the output image. This\nprocess has been shown to have application in many medical image analysis tasks\nincluding imputation, registration, and segmentation. To carry out synthesis,\nthe intensities of the input images are typically scaled--i.e.,\nnormalized--both in training to learn the transformation and in testing when\napplying the transformation, but it is not presently known what type of input\nscaling is optimal. In this paper, we consider seven different intensity\nnormalization algorithms and three different synthesis methods to evaluate the\nimpact of normalization. Our experiments demonstrate that intensity\nnormalization as a preprocessing step improves the synthesis results across all\ninvestigated synthesis algorithms. Furthermore, we show evidence that suggests\nintensity normalization is vital for successful deep learning-based MR image\nsynthesis.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 19:08:44 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Reinhold", "Jacob C.", ""], ["Dewey", "Blake E.", ""], ["Carass", "Aaron", ""], ["Prince", "Jerry L.", ""]]}, {"id": "1812.04682", "submitter": "Bruno Assump\\c{c}\\~ao Gomes Da Silva", "authors": "Bruno A. G. da Silva, Alvaro L. Fazenda, Fabiano C. Paixao", "title": "Femural Head Autosegmentation for 3D Radiotherapy Planning: Preliminary\n  Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contouring of organs at risk is an important but time consuming part of\nradiotherapy treatment planning. Several authors proposed methods for automatic\ndelineation but the clinical experts eye remains the gold standard method. In\nthis paper, we present a totally visual software for automated delineation of\nthe femural head. The software was successfully characterized in pelvic CT Scan\nof prostate patients (n=11). The automatic delineation was compared with manual\nand approved delineation through blind test evaluated by a panel of seniors\nradiation oncologists (n=9). Clinical experts evaluated that no any contouring\ncorrection were need in 77.8% and 67.8% of manual and automatic delineation\nrespectively. Our results show that the software is robust, the automated\ndelineation was reproducible in all patient, and its performance was similar to\nmanually delineation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 20:48:18 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["da Silva", "Bruno A. G.", ""], ["Fazenda", "Alvaro L.", ""], ["Paixao", "Fabiano C.", ""]]}, {"id": "1812.04693", "submitter": "Milad Salem", "authors": "Milad Salem, Shayan Taheri, Jiann Shiun-Yuan", "title": "ECG Arrhythmia Classification Using Transfer Learning from 2-Dimensional\n  Deep CNN Features", "comments": "Accepted and presented for IEEE Biomedical Circuits and Systems\n  (BioCAS) on 17th-19th October 2018 in Ohio, USA", "journal-ref": null, "doi": "10.1109/BIOCAS.2018.8584808", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent advances in the area of deep learning, it has been\ndemonstrated that a deep neural network, trained on a huge amount of data, can\nrecognize cardiac arrhythmias better than cardiologists. Moreover,\ntraditionally feature extraction was considered an integral part of ECG pattern\nrecognition; however, recent findings have shown that deep neural networks can\ncarry out the task of feature extraction directly from the data itself. In\norder to use deep neural networks for their accuracy and feature extraction,\nhigh volume of training data is required, which in the case of independent\nstudies is not pragmatic. To arise to this challenge, in this work, the\nidentification and classification of four ECG patterns are studied from a\ntransfer learning perspective, transferring knowledge learned from the image\nclassification domain to the ECG signal classification domain. It is\ndemonstrated that feature maps learned in a deep neural network trained on\ngreat amounts of generic input images can be used as general descriptors for\nthe ECG signal spectrograms and result in features that enable classification\nof arrhythmias. Overall, an accuracy of 97.23 percent is achieved in\nclassifying near 7000 instances by ten-fold cross validation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 21:11:30 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Salem", "Milad", ""], ["Taheri", "Shayan", ""], ["Shiun-Yuan", "Jiann", ""]]}, {"id": "1812.04697", "submitter": "Milad Salem", "authors": "Milad Salem, Shayan Taheri, Jiann Shiun Yuan", "title": "Anomaly Generation using Generative Adversarial Networks in Host Based\n  Intrusion Detection", "comments": "Accepted and presented at IEEE Annual Ubiquitous Computing,\n  Electronics, and Mobile Communications Conference (IEEE UEMCON) on 8th-10th\n  November 2018", "journal-ref": null, "doi": "10.1109/UEMCON.2018.8796769", "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks have been able to generate striking results\nin various domains. This generation capability can be general while the\nnetworks gain deep understanding regarding the data distribution. In many\ndomains, this data distribution consists of anomalies and normal data, with the\nanomalies commonly occurring relatively less, creating datasets that are\nimbalanced. The capabilities that generative adversarial networks offer can be\nleveraged to examine these anomalies and help alleviate the challenge that\nimbalanced datasets propose via creating synthetic anomalies. This anomaly\ngeneration can be specifically beneficial in domains that have costly data\ncreation processes as well as inherently imbalanced datasets. One of the\ndomains that fits this description is the host-based intrusion detection\ndomain. In this work, ADFA-LD dataset is chosen as the dataset of interest\ncontaining system calls of small foot-print next generation attacks. The data\nis first converted into images, and then a Cycle-GAN is used to create images\nof anomalous data from images of normal data. The generated data is combined\nwith the original dataset and is used to train a model to detect anomalies. By\ndoing so, it is shown that the classification results are improved, with the\nAUC rising from 0.55 to 0.71, and the anomaly detection rate rising from 17.07%\nto 80.49%. The results are also compared to SMOTE, showing the potential\npresented by generative adversarial networks in anomaly generation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 21:21:09 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Salem", "Milad", ""], ["Taheri", "Shayan", ""], ["Yuan", "Jiann Shiun", ""]]}, {"id": "1812.04706", "submitter": "Hubert Cecotti", "authors": "Hubert Cecotti", "title": "Rotation Invariant Descriptors for Galaxy Morphological Classification", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.GA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of objects that are multi-oriented is a difficult pattern\nrecognition problem. In this paper, we propose to evaluate the performance of\ndifferent families of descriptors for the classification of galaxy\nmorphologies. We investigate the performance of the Hu moments, Flusser\nmoments, Zernike moments, Fourier-Mellin moments, and ring projection\ntechniques based on 1D moment and the Fourier transform. We consider two main\ndatasets for the performance evaluation. The first dataset is an artificial\ndataset based on representative templates from 11 types of galaxies, which are\nevaluated with different transformations (noise, smoothing), alone or combined.\nThe evaluation is based on image retrieval performance to estimate the\nrobustness of the rotation invariant descriptors with this type of images. The\nsecond dataset is composed of real images extracted from the Galaxy Zoo 2\nproject. The binary classification of elliptical and spiral galaxies is\nachieved with pre-processing steps including morphological filtering and a\nLaplacian pyramid. For the binary classification, we compare the different set\nof features with Support Vector Machines (SVM), Extreme Learning Machine, and\ndifferent types of linear discriminant analysis techniques. The results support\nthe conclusion that the proposed framework for the binary classification of\nelliptical and spiral galaxies provides an area under the ROC curve reaching\n99.54%, proving the robustness of the approach for helping astronomers to study\ngalaxies.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 21:45:52 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 21:14:27 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Cecotti", "Hubert", ""]]}, {"id": "1812.04708", "submitter": "Fabio Cappabianco", "authors": "F\\'abio A. M. Cappabianco, Petrus P. C. E. da Silva", "title": "Non-local Operational Anisotropic Diffusion Filter", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-frequency noise is present in several modalities of medical images. It\noriginates from the acquisition process and may be related to the scanner\nconfigurations, the scanned body, or to other external factors. This way,\nprospective filters are an important tool to improve the image quality. In this\npaper, we propose a non-local weighted operational anisotropic diffusion filter\nand evaluate its effect on magnetic resonance images and on kV/CBCT\nradiotherapy images. We also provide a detailed analysis of non-local parameter\nsettings. Results show that the new filter enhances previous local\nimplementations and has potential application in radiotherapy treatments.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 21:49:07 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Cappabianco", "F\u00e1bio A. M.", ""], ["da Silva", "Petrus P. C. E.", ""]]}, {"id": "1812.04748", "submitter": "Imad Rida", "authors": "Imad Rida, Romain H\\'erault, Gilles Gasso", "title": "An efficient supervised dictionary learning method for audio signal\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine hearing or listening represents an emerging area. Conventional\napproaches rely on the design of handcrafted features specialized to a specific\naudio task and that can hardly generalized to other audio fields. For example,\nMel-Frequency Cepstral Coefficients (MFCCs) and its variants were successfully\napplied to computational auditory scene recognition while Chroma vectors are\ngood at music chord recognition. Unfortunately, these predefined features may\nbe of variable discrimination power while extended to other tasks or even\nwithin the same task due to different nature of clips. Motivated by this need\nof a principled framework across domain applications for machine listening, we\npropose a generic and data-driven representation learning approach. For this\nsake, a novel and efficient supervised dictionary learning method is presented.\nThe method learns dissimilar dictionaries, one per each class, in order to\nextract heterogeneous information for classification. In other words, we are\nseeking to minimize the intra-class homogeneity and maximize class\nseparability. This is made possible by promoting pairwise orthogonality between\nclass specific dictionaries and controlling the sparsity structure of the audio\nclip's decomposition over these dictionaries. The resulting optimization\nproblem is non-convex and solved using a proximal gradient descent method.\nExperiments are performed on both computational auditory scene (East Anglia and\nRouen) and synthetic music chord recognition datasets. Obtained results show\nthat our method is capable to reach state-of-the-art hand-crafted features for\nboth applications.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 00:04:35 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Rida", "Imad", ""], ["H\u00e9rault", "Romain", ""], ["Gasso", "Gilles", ""]]}, {"id": "1812.04751", "submitter": "Akbir M Khan Mr", "authors": "Akbir Khan and Marwa Mahmoud", "title": "Considering Race a Problem of Transfer Learning", "comments": "Accepted for Oral presentation at DVPBA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As biometric applications are fielded to serve large population groups,\nissues of performance differences between individual sub-groups are becoming\nincreasingly important. In this paper we examine cases where we believe race is\none such factor. We look in particular at two forms of problem; facial\nclassification and image synthesis. We take the novel approach of considering\nrace as a boundary for transfer learning in both the task (facial\nclassification) and the domain (synthesis over distinct datasets). We\ndemonstrate a series of techniques to improve transfer learning of facial\nclassification; outperforming similar models trained in the target's own\ndomain. We conduct a study to evaluate the performance drop of Generative\nAdversarial Networks trained to conduct image synthesis, in this process, we\nproduce a new annotation for the Celeb-A dataset by race. These networks are\ntrained solely on one race and tested on another - demonstrating the subsets of\nthe CelebA to be distinct domains for this task.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 00:30:39 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Khan", "Akbir", ""], ["Mahmoud", "Marwa", ""]]}, {"id": "1812.04777", "submitter": "Orazio Gallo", "authors": "Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min H. Kim, Jan Kautz", "title": "Extreme View Synthesis", "comments": "Accepted as an oral presentation at IEEE ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Extreme View Synthesis, a solution for novel view extrapolation\nthat works even when the number of input images is small--as few as two. In\nthis context, occlusions and depth uncertainty are two of the most pressing\nissues, and worsen as the degree of extrapolation increases. We follow the\ntraditional paradigm of performing depth-based warping and refinement, with a\nfew key improvements. First, we estimate a depth probability volume, rather\nthan just a single depth value for each pixel of the novel view. This allows us\nto leverage depth uncertainty in challenging regions, such as depth\ndiscontinuities. After using it to get an initial estimate of the novel view,\nwe explicitly combine learned image priors and the depth uncertainty to\nsynthesize a refined image with less artifacts. Our method is the first to show\nvisually pleasing results for baseline magnifications of up to 30X.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 02:15:27 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 01:13:18 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Choi", "Inchang", ""], ["Gallo", "Orazio", ""], ["Troccoli", "Alejandro", ""], ["Kim", "Min H.", ""], ["Kautz", "Jan", ""]]}, {"id": "1812.04794", "submitter": "Peng Wang", "authors": "Peng Wang, Qi Wu, Jiewei Cao, Chunhua Shen, Lianli Gao, Anton van den\n  Hengel", "title": "Neighbourhood Watch: Referring Expression Comprehension via\n  Language-guided Graph Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task in referring expression comprehension is to localise the object\ninstance in an image described by a referring expression phrased in natural\nlanguage. As a language-to-vision matching task, the key to this problem is to\nlearn a discriminative object feature that can adapt to the expression used. To\navoid ambiguity, the expression normally tends to describe not only the\nproperties of the referent itself, but also its relationships to its\nneighbourhood. To capture and exploit this important information we propose a\ngraph-based, language-guided attention mechanism. Being composed of node\nattention component and edge attention component, the proposed graph attention\nmechanism explicitly represents inter-object relationships, and properties with\na flexibility and power impossible with competing approaches. Furthermore, the\nproposed graph attention mechanism enables the comprehension decision to be\nvisualisable and explainable. Experiments on three referring expression\ncomprehension datasets show the advantage of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 03:30:41 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Wang", "Peng", ""], ["Wu", "Qi", ""], ["Cao", "Jiewei", ""], ["Shen", "Chunhua", ""], ["Gao", "Lianli", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1812.04798", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko", "title": "Strong-Weak Distribution Alignment for Adaptive Object Detection", "comments": "Accepted to CVPR2019, project page\n  http://cs-people.bu.edu/keisaito/research/CVPR2019.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for unsupervised adaptation of object detectors from\nlabel-rich to label-poor domains which can significantly reduce annotation\ncosts associated with detection. Recently, approaches that align distributions\nof source and target images using an adversarial loss have been proven\neffective for adapting object classifiers. However, for object detection, fully\nmatching the entire distributions of source and target images to each other at\nthe global image level may fail, as domains could have distinct scene layouts\nand different combinations of objects. On the other hand, strong matching of\nlocal features such as texture and color makes sense, as it does not change\ncategory level semantics. This motivates us to propose a novel method for\ndetector adaptation based on strong local alignment and weak global alignment.\nOur key contribution is the weak alignment model, which focuses the adversarial\nalignment loss on images that are globally similar and puts less emphasis on\naligning images that are globally dissimilar. Additionally, we design the\nstrong domain alignment model to only look at local receptive fields of the\nfeature map. We empirically verify the effectiveness of our method on four\ndatasets comprising both large and small domain shifts. Our code is available\nat \\url{https://github.com/VisionLearningGroup/DA_Detection}\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 04:02:38 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 17:45:27 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 19:26:15 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Saito", "Kuniaki", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""], ["Saenko", "Kate", ""]]}, {"id": "1812.04816", "submitter": "Chong Yang Zhang", "authors": "Chongyang Zhang, Guofeng Zhu, Minxin Chen, Hong Chen, Chenjian Wu", "title": "Image Segmentation Based on Multiscale Fast Spectral Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, spectral clustering has become one of the most popular\nclustering algorithms for image segmentation. However, it has restricted\napplicability to large-scale images due to its high computational complexity.\nIn this paper, we first propose a novel algorithm called Fast Spectral\nClustering based on quad-tree decomposition. The algorithm focuses on the\nspectral clustering at superpixel level and its computational complexity is\nO(nlogn) + O(m) + O(m^(3/2)); its memory cost is O(m), where n and m are the\nnumbers of pixels and the superpixels of a image. Then we propose Multiscale\nFast Spectral Clustering by improving Fast Spectral Clustering, which is based\non the hierarchical structure of the quad-tree. The computational complexity of\nMultiscale Fast Spectral Clustering is O(nlogn) and its memory cost is O(m).\nExtensive experiments on real large-scale images demonstrate that Multiscale\nFast Spectral Clustering outperforms Normalized cut in terms of lower\ncomputational complexity and memory cost, with comparable clustering accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 05:50:06 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Zhang", "Chongyang", ""], ["Zhu", "Guofeng", ""], ["Chen", "Minxin", ""], ["Chen", "Hong", ""], ["Wu", "Chenjian", ""]]}, {"id": "1812.04821", "submitter": "Shervin Minaee", "authors": "Harsh Nilesh Pathak, Xinxin Li, Shervin Minaee, Brooke Cowan", "title": "Efficient Super Resolution For Large-Scale Images Using Attentional GAN", "comments": "Accepted by IEEE International Conference on Big Data, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Image Super Resolution (SISR) is a well-researched problem with broad\ncommercial relevance. However, most of the SISR literature focuses on\nsmall-size images under 500px, whereas business needs can mandate the\ngeneration of very high resolution images. At Expedia Group, we were tasked\nwith generating images of at least 2000px for display on the website, four\ntimes greater than the sizes typically reported in the literature. This\nrequirement poses a challenge that state-of-the-art models, validated on small\nimages, have not been proven to handle. In this paper, we investigate solutions\nto the problem of generating high-quality images for large-scale super\nresolution in a commercial setting. We find that training a generative\nadversarial network (GAN) with attention from scratch using a large-scale\nlodging image data set generates images with high PSNR and SSIM scores. We\ndescribe a novel attentional SISR model for large-scale images, A-SRGAN, that\nuses a Flexible Self Attention layer to enable processing of large-scale\nimages. We also describe a distributed algorithm which speeds up training by\naround a factor of five.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 06:11:32 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 06:13:47 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 19:43:02 GMT"}, {"version": "v4", "created": "Sun, 13 Jan 2019 07:17:18 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Pathak", "Harsh Nilesh", ""], ["Li", "Xinxin", ""], ["Minaee", "Shervin", ""], ["Cowan", "Brooke", ""]]}, {"id": "1812.04822", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Amirali Abdolrashidi", "title": "Iris-GAN: Learning to Generate Realistic Iris Images Using Convolutional\n  GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating iris images which look realistic is both an interesting and\nchallenging problem. Most of the classical statistical models are not powerful\nenough to capture the complicated texture representation in iris images, and\ntherefore fail to generate iris images which look realistic. In this work, we\npresent a machine learning framework based on generative adversarial network\n(GAN), which is able to generate iris images sampled from a prior distribution\n(learned from a set of training images). We apply this framework to two popular\niris databases, and generate images which look very realistic, and similar to\nthe image distribution in those databases. Through experimental results, we\nshow that the generated iris images have a good diversity, and are able to\ncapture different part of the prior distribution.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 06:11:45 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 17:00:22 GMT"}, {"version": "v3", "created": "Tue, 25 Dec 2018 20:44:21 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "Amirali", ""]]}, {"id": "1812.04826", "submitter": "Yuxi Chi", "authors": "Yuxi Chi, Bing Pan", "title": "Spatial-Temporal Digital Image Correlation: A Unified Framework", "comments": "20 pages, 7 figures v2:add figures and revise the contents", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A comprehensive and systematic framework for easily extending and\nimplementing the subset-based spatial-temporal digital image correlation (DIC)\nalgorithm is presented. The framework decouples the three main factors (i.e.\nshape function, correlation criterion, and optimization algorithm) involved in\nalgorithm implementation of DIC and represents different algorithms in a\nuniform form. One can freely choose and combine the three factors to meet his\nown need, or freely add more parameters to extract analytic results. Subpixel\ntranslation and a simulated image series with different velocity characters are\nanalyzed using different algorithms based on the proposed framework, confirming\nthe merit of noise suppression and velocity compatibility. An application of\nmitigating air disturbance due to heat haze using spatial-temporal DIC is given\nto demonstrate the applicability of the framework.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 06:45:28 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2019 13:49:36 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Chi", "Yuxi", ""], ["Pan", "Bing", ""]]}, {"id": "1812.04831", "submitter": "Shisha Liao", "authors": "Shisha Liao, Yongqing Sun, Chenqiang Gao, Pranav Shenoy K P, Song Mu,\n  Jun Shimamura, Atsushi Sagata", "title": "Weakly Supervised Instance Segmentation Using Hybrid Network", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised instance segmentation, which could greatly save labor and\ntime cost of pixel mask annotation, has attracted increasing attention in\nrecent years. The commonly used pipeline firstly utilizes conventional image\nsegmentation methods to automatically generate initial masks and then use them\nto train an off-the-shelf segmentation network in an iterative way. However,\nthe initial generated masks usually contains a notable proportion of invalid\nmasks which are mainly caused by small object instances. Directly using these\ninitial masks to train segmentation model is harmful for the performance. To\naddress this problem, we propose a hybrid network in this paper. In our\narchitecture, there is a principle segmentation network which is used to handle\nthe normal samples with valid generated masks. In addition, a complementary\nbranch is added to handle the small and dim objects without valid masks.\nExperimental results indicate that our method can achieve significantly\nperformance improvement both on the small object instances and large ones, and\noutperforms all state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 07:12:50 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Liao", "Shisha", ""], ["Sun", "Yongqing", ""], ["Gao", "Chenqiang", ""], ["P", "Pranav Shenoy K", ""], ["Mu", "Song", ""], ["Shimamura", "Jun", ""], ["Sagata", "Atsushi", ""]]}, {"id": "1812.04857", "submitter": "Gr\\'egoire Nieto", "authors": "Gr\\'egoire Nieto, Salma Jiddi, Philippe Robert", "title": "Robust Point Light Source Estimation Using Differentiable Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illumination estimation is often used in mixed reality to re-render a scene\nfrom another point of view, to change the color/texture of an object, or to\ninsert a virtual object consistently lit into a real video or photograph.\nSpecifically, the estimation of a point light source is required for the\nshadows cast by the inserted object to be consistent with the real scene. We\ntackle the problem of illumination retrieval given an RGBD image of the scene\nas an inverse problem: we aim to find the illumination that minimizes the\nphotometric error between the rendered image and the observation. In particular\nwe propose a novel differentiable renderer based on the Blinn-Phong model with\ncast shadows. We compare our differentiable renderer to state-of-the-art\nmethods and demonstrate its robustness to an incorrect reflectance estimation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 09:05:11 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Nieto", "Gr\u00e9goire", ""], ["Jiddi", "Salma", ""], ["Robert", "Philippe", ""]]}, {"id": "1812.04860", "submitter": "Sonu Gupta", "authors": "Sonu Gupta, Deepak Srivatsav, A. V. Subramanyam, Ponnurangam\n  Kumaraguru", "title": "Attentional Road Safety Networks", "comments": "8 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road safety mapping using satellite images is a cost-effective but a\nchallenging problem for smart city planning. The scarcity of labeled data,\nmisalignment and ambiguity makes it hard for supervised deep networks to learn\nefficient embeddings in order to classify between safe and dangerous road\nsegments. In this paper, we address the challenges using a region guided\nattention network. In our model, we extract global features from a base network\nand augment it with local features obtained using the region guided attention\nnetwork. In addition, we perform domain adaptation for unlabeled target data.\nIn order to bridge the gap between safe samples and dangerous samples from\nsource and target respectively, we propose a loss function based on within and\nbetween class covariance matrices. We conduct experiments on a public dataset\nof London to show that the algorithm achieves significant results with the\nclassification accuracy of 86.21%. We obtain an increase of 4% accuracy for NYC\nusing domain adaptation network. Besides, we perform a user study and\ndemonstrate that our proposed algorithm achieves 23.12% better accuracy\ncompared to subjective analysis.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 09:11:31 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 09:21:29 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Gupta", "Sonu", ""], ["Srivatsav", "Deepak", ""], ["Subramanyam", "A. V.", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "1812.04901", "submitter": "Lei Zhang", "authors": "Lei Zhang, Helen Gray, Xujiong Ye, Lisa Collins, Nigel Allinson", "title": "Automatic individual pig detection and tracking in surveillance videos", "comments": "19 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual pig detection and tracking is an important requirement in many\nvideo-based pig monitoring applications. However, it still remains a\nchallenging task in complex scenes, due to problems of light fluctuation,\nsimilar appearances of pigs, shape deformations and occlusions. To tackle these\nproblems, we propose a robust real time multiple pig detection and tracking\nmethod which does not require manual marking or physical identification of the\npigs, and works under both daylight and infrared light conditions. Our method\ncouples a CNN-based detector and a correlation filter-based tracker via a novel\nhierarchical data association algorithm. The detector gains the best\naccuracy/speed trade-off by using the features derived from multiple layers at\ndifferent scales in a one-stage prediction network. We define a tag-box for\neach pig as the tracking target, and the multiple object tracking is conducted\nin a key-points tracking manner using learned correlation filters. Under\nchallenging conditions, the tracking failures are modelled based on the\nrelations between responses of detector and tracker, and the data association\nalgorithm allows the detection hypotheses to be refined, meanwhile the drifted\ntracks can be corrected by probing the tracking failures followed by the\nre-initialization of tracking. As a result, the optimal tracklets can\nsequentially grow with on-line refined detections, and tracking fragments are\ncorrectly integrated into respective tracks while keeping the original\nidentifications. Experiments with a dataset captured from a commercial farm\nshow that our method can robustly detect and track multiple pigs under\nchallenging conditions. The promising performance of the proposed method also\ndemonstrates a feasibility of long-term individual pig tracking in a complex\nenvironment and thus promises a commercial potential.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 11:22:27 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Zhang", "Lei", ""], ["Gray", "Helen", ""], ["Ye", "Xujiong", ""], ["Collins", "Lisa", ""], ["Allinson", "Nigel", ""]]}, {"id": "1812.04914", "submitter": "Zhanwei Xu", "authors": "Zhanwei Xu, Ziyi Wu, Jianjiang Feng", "title": "CFUN: Combining Faster R-CNN and U-net Network for Efficient Whole Heart\n  Segmentation", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel heart segmentation pipeline Combining\nFaster R-CNN and U-net Network (CFUN). Due to Faster R-CNN's precise\nlocalization ability and U-net's powerful segmentation ability, CFUN needs only\none-step detection and segmentation inference to get the whole heart\nsegmentation result, obtaining good results with significantly reduced\ncomputational cost. Besides, CFUN adopts a new loss function based on edge\ninformation named 3D Edge-loss as an auxiliary loss to accelerate the\nconvergence of training and improve the segmentation results. Extensive\nexperiments on the public dataset show that CFUN exhibits competitive\nsegmentation performance in a sharply reduced inference time. Our source code\nand the model are publicly available at https://github.com/Wuziyi616/CFUN.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 12:22:22 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Xu", "Zhanwei", ""], ["Wu", "Ziyi", ""], ["Feng", "Jianjiang", ""]]}, {"id": "1812.04920", "submitter": "Hyojin Park", "authors": "Hyojin Park, Youngjoon Yoo, Geonseok Seo, Dongyoon Han, Sangdoo Yun,\n  Nojun Kwak", "title": "C3: Concentrated-Comprehensive Convolution and its application to\n  semantic segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the practical choices for making a lightweight semantic segmentation\nmodel is to combine a depth-wise separable convolution with a dilated\nconvolution. However, the simple combination of these two methods results in an\nover-simplified operation which causes severe performance degradation due to\nloss of information contained in the feature map. To resolve this problem, we\npropose a new block called Concentrated-Comprehensive Convolution (C3) which\napplies the asymmetric convolutions before the depth-wise separable dilated\nconvolution to compensate for the information loss due to dilated convolution.\nThe C3 block consists of a concentration stage and a comprehensive convolution\nstage. The first stage uses two depth-wise asymmetric convolutions for\ncompressed information from the neighboring pixels to alleviate the information\nloss. The second stage increases the receptive field by using a depth-wise\nseparable dilated convolution from the feature map of the first stage. We\napplied the C3 block to various segmentation frameworks (ESPNet, DRN, ERFNet,\nENet) for proving the beneficial properties of our proposed method.\nExperimental results show that the proposed method preserves the original\naccuracies on Cityscapes dataset while reducing the complexity. Furthermore, we\nmodified ESPNet to achieve about 2% better performance while reducing the\nnumber of parameters by half and the number of FLOPs by 35% compared with the\noriginal ESPNet. Finally, experiments on ImageNet classification task show that\nC3 block can successfully replace dilated convolutions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 12:48:27 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 17:31:00 GMT"}, {"version": "v3", "created": "Sun, 28 Jul 2019 13:56:06 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Park", "Hyojin", ""], ["Yoo", "Youngjoon", ""], ["Seo", "Geonseok", ""], ["Han", "Dongyoon", ""], ["Yun", "Sangdoo", ""], ["Kwak", "Nojun", ""]]}, {"id": "1812.04922", "submitter": "Jonathan Andersson", "authors": "Jonathan Andersson, H{\\aa}kan Ahlstr\\\"om, Joel Kullberg", "title": "Separation of water and fat signal in whole-body gradient echo scans\n  using convolutional neural networks", "comments": "26 pages, 12 figures", "journal-ref": null, "doi": "10.1002/mrm.27786", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To perform and evaluate water-fat signal separation of whole-body\ngradient echo scans using convolutional neural networks.\n  Methods: Whole-body gradient echo scans of 240 subjects, each consisting of 5\nbipolar echoes, were used. Reference fat fraction maps were created using a\nconventional method. Convolutional neural networks, more specifically 2D\nU-nets, were trained using 5-fold cross-validation with 1 or several echoes as\ninput, using the squared difference between the output and the reference fat\nfraction maps as the loss function. The outputs of the networks were assessed\nby the loss function, measured liver fat fractions, and visually. Training was\nperformed using a graphics processing unit (GPU). Inference was performed using\nthe GPU as well as a central processing unit (CPU).\n  Results: The loss curves indicated convergence, and the final loss of the\nvalidation data decreased when using more echoes as input. The liver fat\nfractions could be estimated using only 1 echo, but results were improved by\nuse of more echoes. Visual assessment found the quality of the outputs of the\nnetworks to be similar to the reference even when using only 1 echo, with\nslight improvements when using more echoes. Training a network took at most\n28.6 h. Inference time of a whole-body scan took at most 3.7 s using the GPU\nand 5.8 min using the CPU.\n  Conclusion: It is possible to perform water-fat signal separation of\nwhole-body gradient echo scans using convolutional neural networks. Separation\nwas possible using only 1 echo, although using more echoes improved the\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 12:51:12 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 13:08:22 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Andersson", "Jonathan", ""], ["Ahlstr\u00f6m", "H\u00e5kan", ""], ["Kullberg", "Joel", ""]]}, {"id": "1812.04929", "submitter": "Chaofeng Chen", "authors": "Chaofeng Chen, Wei Liu, Xiao Tan and Kwan-Yee K. Wong", "title": "Semi-Supervised Learning for Face Sketch Synthesis in the Wild", "comments": "Correct some syntax errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face sketch synthesis has made great progress in the past few years. Recent\nmethods based on deep neural networks are able to generate high quality\nsketches from face photos. However, due to the lack of training data\n(photo-sketch pairs), none of such deep learning based methods can be applied\nsuccessfully to face photos in the wild. In this paper, we propose a\nsemi-supervised deep learning architecture which extends face sketch synthesis\nto handle face photos in the wild by exploiting additional face photos in\ntraining. Instead of supervising the network with ground truth sketches, we\nfirst perform patch matching in feature space between the input photo and\nphotos in a small reference set of photo-sketch pairs. We then compose a pseudo\nsketch feature representation using the corresponding sketch feature patches to\nsupervise our network. With the proposed approach, we can train our networks\nusing a small reference set of photo-sketch pairs together with a large face\nphoto dataset without ground truth sketches. Experiments show that our method\nachieve state-of-the-art performance both on public benchmarks and face photos\nin the wild. Codes are available at\nhttps://github.com/chaofengc/Face-Sketch-Wild.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 13:13:02 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 23:20:08 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Chen", "Chaofeng", ""], ["Liu", "Wei", ""], ["Tan", "Xiao", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "1812.04945", "submitter": "Tianyi Wu", "authors": "Tianyi Wu, Sheng Tang, Rui Zhang, Juan Cao, Jintao Li", "title": "Tree-structured Kronecker Convolutional Network for Semantic\n  Segmentation", "comments": "Code: https://github.com/wutianyiRosun/TKCN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing semantic segmentation methods employ atrous convolution to\nenlarge the receptive field of filters, but neglect partial information. To\ntackle this issue, we firstly propose a novel Kronecker convolution which\nadopts Kronecker product to expand the standard convolutional kernel for taking\ninto account the partial feature neglected by atrous convolutions. Therefore,\nit can capture partial information and enlarge the receptive field of filters\nsimultaneously without introducing extra parameters. Secondly, we propose\nTree-structured Feature Aggregation (TFA) module which follows a recursive rule\nto expand and forms a hierarchical structure. Thus, it can naturally learn\nrepresentations of multi-scale objects and encode hierarchical contextual\ninformation in complex scenes. Finally, we design Tree-structured Kronecker\nConvolutional Networks (TKCN) which employs Kronecker convolution and TFA\nmodule. Extensive experiments on three datasets, PASCAL VOC 2012,\nPASCAL-Context and Cityscapes, verify the effectiveness of our proposed\napproach. We make the code and the trained model publicly available at\nhttps://github.com/wutianyiRosun/TKCN.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 13:57:50 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 02:44:57 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Wu", "Tianyi", ""], ["Tang", "Sheng", ""], ["Zhang", "Rui", ""], ["Cao", "Juan", ""], ["Li", "Jintao", ""]]}, {"id": "1812.04949", "submitter": "Miklas S. Kristoffersen", "authors": "Andrea Coifman, P\\'eter Rohoska, Miklas S. Kristoffersen, Sven E.\n  Shepstone, Zheng-Hua Tan", "title": "Subjective Annotations for Vision-Based Attention Level Estimation", "comments": "14th International Conference on Computer Vision Theory and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention level estimation systems have a high potential in many use cases,\nsuch as human-robot interaction, driver modeling and smart home systems, since\nbeing able to measure a person's attention level opens the possibility to\nnatural interaction between humans and computers. The topic of estimating a\nhuman's visual focus of attention has been actively addressed recently in the\nfield of HCI. However, most of these previous works do not consider attention\nas a subjective, cognitive attentive state. New research within the field also\nfaces the problem of the lack of annotated datasets regarding attention level\nin a certain context. The novelty of our work is two-fold: First, we introduce\na new annotation framework that tackles the subjective nature of attention\nlevel and use it to annotate more than 100,000 images with three attention\nlevels and second, we introduce a novel method to estimate attention levels,\nrelying purely on extracted geometric features from RGB and depth images, and\nevaluate it with a deep learning fusion framework. The system achieves an\noverall accuracy of 80.02%. Our framework and attention level annotations are\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 14:00:46 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 11:43:12 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Coifman", "Andrea", ""], ["Rohoska", "P\u00e9ter", ""], ["Kristoffersen", "Miklas S.", ""], ["Shepstone", "Sven E.", ""], ["Tan", "Zheng-Hua", ""]]}, {"id": "1812.04955", "submitter": "Yunxiao Qin", "authors": "Yunxiao Qin, Weiguo Zhang, Chenxu Zhao, Zezheng Wang, Xiangyu Zhu,\n  Guojun Qi, Jingping Shi, Zhen Lei", "title": "Prior-Knowledge and Attention-based Meta-Learning for Few-Shot Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, meta-learning has been shown as a promising way to solve few-shot\nlearning. In this paper, inspired by the human cognition process which utilizes\nboth prior-knowledge and vision attention in learning new knowledge, we present\na novel paradigm of meta-learning approach with three developments to introduce\nattention mechanism and prior-knowledge for meta-learning. In our approach,\nprior-knowledge is responsible for helping meta-learner expressing the input\ndata into high-level representation space, and attention mechanism enables\nmeta-learner focusing on key features of the data in the representation space.\nCompared with existing meta-learning approaches that pay little attention to\nprior-knowledge and vision attention, our approach alleviates the\nmeta-learner's few-shot cognition burden. Furthermore, a Task-Over-Fitting\n(TOF) problem, which indicates that the meta-learner has poor generalization on\ndifferent K-shot learning tasks, is discovered and we propose a Cross-Entropy\nacross Tasks (CET) metric to model and solve the TOF problem. Extensive\nexperiments demonstrate that we improve the meta-learner with state-of-the-art\nperformance on several few-shot learning benchmarks, and at the same time the\nTOF problem can also be released greatly.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 08:56:47 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 02:53:44 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 12:33:37 GMT"}, {"version": "v4", "created": "Mon, 24 Dec 2018 04:23:05 GMT"}, {"version": "v5", "created": "Sat, 15 Feb 2020 06:23:11 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Qin", "Yunxiao", ""], ["Zhang", "Weiguo", ""], ["Zhao", "Chenxu", ""], ["Wang", "Zezheng", ""], ["Zhu", "Xiangyu", ""], ["Qi", "Guojun", ""], ["Shi", "Jingping", ""], ["Lei", "Zhen", ""]]}, {"id": "1812.04960", "submitter": "Radu Tudor Ionescu", "authors": "Radu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana Georgescu,\n  Ling Shao", "title": "Object-centric Auto-encoders and Dummy Anomalies for Abnormal Event\n  Detection in Video", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormal event detection in video is a challenging vision problem. Most\nexisting approaches formulate abnormal event detection as an outlier detection\ntask, due to the scarcity of anomalous data during training. Because of the\nlack of prior information regarding abnormal events, these methods are not\nfully-equipped to differentiate between normal and abnormal events. In this\nwork, we formalize abnormal event detection as a one-versus-rest binary\nclassification problem. Our contribution is two-fold. First, we introduce an\nunsupervised feature learning framework based on object-centric convolutional\nauto-encoders to encode both motion and appearance information. Second, we\npropose a supervised classification approach based on clustering the training\nsamples into normality clusters. A one-versus-rest abnormal event classifier is\nthen employed to separate each normality cluster from the rest. For the purpose\nof training the classifier, the other clusters act as dummy anomalies. During\ninference, an object is labeled as abnormal if the highest classification score\nassigned by the one-versus-rest classifiers is negative. Comprehensive\nexperiments are performed on four benchmarks: Avenue, ShanghaiTech, UCSD and\nUMN. Our approach provides superior results on all four data sets. On the\nlarge-scale ShanghaiTech data set, our method provides an absolute gain of 8.4%\nin terms of frame-level AUC compared to the state-of-the-art method [Sultani et\nal., CVPR 2018].\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 13:22:02 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 09:14:28 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ionescu", "Radu Tudor", ""], ["Khan", "Fahad Shahbaz", ""], ["Georgescu", "Mariana-Iuliana", ""], ["Shao", "Ling", ""]]}, {"id": "1812.04980", "submitter": "Guojun Yin", "authors": "Huihui Zhu, Bin Liu, Guojun Yin, Yan Lu, Weihai Li, Nenghai Yu", "title": "Real-Time Anomaly Detection With HMOF Feature", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection is a challenging problem in intelligent video surveillance.\nMost existing methods are computation consuming, which cannot satisfy the\nreal-time requirement. In this paper, we propose a real-time anomaly detection\nframework with low computational complexity and high efficiency. A new feature,\nnamed Histogram of Magnitude Optical Flow (HMOF), is proposed to capture the\nmotion of video patches. Compared with existing feature descriptors, HMOF is\nmore sensitive to motion magnitude and more efficient to distinguish anomaly\ninformation. The HMOF features are computed for foreground patches, and are\nreconstructed by the auto-encoder for better clustering. Then, we use Gaussian\nMixture Model (GMM) Classifiers to distinguish anomalies from normal activities\nin videos. Experimental results show that our framework outperforms\nstate-of-the-art methods, and can reliably detect anomalies in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 14:56:14 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Zhu", "Huihui", ""], ["Liu", "Bin", ""], ["Yin", "Guojun", ""], ["Lu", "Yan", ""], ["Li", "Weihai", ""], ["Yu", "Nenghai", ""]]}, {"id": "1812.05038", "submitter": "Chao-Yuan Wu", "authors": "Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp\n  Kr\\\"ahenb\\\"uhl, Ross Girshick", "title": "Long-Term Feature Banks for Detailed Video Understanding", "comments": "Code and models are available at\n  https://github.com/facebookresearch/video-long-term-feature-banks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the world, we humans constantly need to relate the present to\nthe past, and put events in context. In this paper, we enable existing video\nmodels to do the same. We propose a long-term feature bank---supportive\ninformation extracted over the entire span of a video---to augment\nstate-of-the-art video models that otherwise would only view short clips of 2-5\nseconds. Our experiments demonstrate that augmenting 3D convolutional networks\nwith a long-term feature bank yields state-of-the-art results on three\nchallenging video datasets: AVA, EPIC-Kitchens, and Charades.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 17:13:55 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 19:05:30 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Wu", "Chao-Yuan", ""], ["Feichtenhofer", "Christoph", ""], ["Fan", "Haoqi", ""], ["He", "Kaiming", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Girshick", "Ross", ""]]}, {"id": "1812.05040", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Wen Li, Xiaoran Chen, Luc Van Gool", "title": "Learning Semantic Segmentation from Synthetic Data: A Geometrically\n  Guided Input-Output Adaptation Approach", "comments": "v2: fixed some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, increasing attention has been drawn to training semantic\nsegmentation models using synthetic data and computer-generated annotation.\nHowever, domain gap remains a major barrier and prevents models learned from\nsynthetic data from generalizing well to real-world applications. In this work,\nwe take the advantage of additional geometric information from synthetic data,\na powerful yet largely neglected cue, to bridge the domain gap. Such geometric\ninformation can be generated easily from synthetic data, and is proven to be\nclosely coupled with semantic information. With the geometric information, we\npropose a model to reduce domain shift on two levels: on the input level, we\naugment the traditional image translation network with the additional geometric\ninformation to translate synthetic images into realistic styles; on the output\nlevel, we build a task network which simultaneously performs depth estimation\nand semantic segmentation on the synthetic data. Meanwhile, we encourage the\nnetwork to preserve correlation between depth and semantics by adversarial\ntraining on the output space. We then validate our method on two pairs of\nsynthetic to real dataset: Virtual KITTI to KITTI, and SYNTHIA to Cityscapes,\nwhere we achieve a significant performance gain compared to the non-adapt\nbaseline and methods using only semantic label. This demonstrates the\nusefulness of geometric information from synthetic data for cross-domain\nsemantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 17:23:24 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 23:49:13 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Chen", "Yuhua", ""], ["Li", "Wen", ""], ["Chen", "Xiaoran", ""], ["Van Gool", "Luc", ""]]}, {"id": "1812.05050", "submitter": "Qiang Wang", "authors": "Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip H.S. Torr", "title": "Fast Online Object Tracking and Segmentation: A Unifying Approach", "comments": "CVPR 2019 camera ready. Code available at\n  https://github.com/foolwood/SiamMask", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we illustrate how to perform both visual object tracking and\nsemi-supervised video object segmentation, in real-time, with a single simple\napproach. Our method, dubbed SiamMask, improves the offline training procedure\nof popular fully-convolutional Siamese approaches for object tracking by\naugmenting their loss with a binary segmentation task. Once trained, SiamMask\nsolely relies on a single bounding box initialisation and operates online,\nproducing class-agnostic object segmentation masks and rotated bounding boxes\nat 55 frames per second. Despite its simplicity, versatility and fast speed,\nour strategy allows us to establish a new state of the art among real-time\ntrackers on VOT-2018, while at the same time demonstrating competitive\nperformance and the best speed for the semi-supervised video object\nsegmentation task on DAVIS-2016 and DAVIS-2017. The project website is\nhttp://www.robots.ox.ac.uk/~qwang/SiamMask.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 17:43:04 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 03:49:18 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wang", "Qiang", ""], ["Zhang", "Li", ""], ["Bertinetto", "Luca", ""], ["Hu", "Weiming", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1812.05069", "submitter": "Michael Tschannen", "authors": "Michael Tschannen, Olivier Bachem, Mario Lucic", "title": "Recent Advances in Autoencoder-Based Representation Learning", "comments": "Presented at the third workshop on Bayesian Deep Learning (NeurIPS\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning useful representations with little or no supervision is a key\nchallenge in artificial intelligence. We provide an in-depth review of recent\nadvances in representation learning with a focus on autoencoder-based models.\nTo organize these results we make use of meta-priors believed useful for\ndownstream tasks, such as disentanglement and hierarchical organization of\nfeatures. In particular, we uncover three main mechanisms to enforce such\nproperties, namely (i) regularizing the (approximate or aggregate) posterior\ndistribution, (ii) factorizing the encoding and decoding distribution, or (iii)\nintroducing a structured prior distribution. While there are some promising\nresults, implicit or explicit supervision remains a key enabler and all current\nmethods use strong inductive biases and modeling assumptions. Finally, we\nprovide an analysis of autoencoder-based representation learning through the\nlens of rate-distortion theory and identify a clear tradeoff between the amount\nof prior knowledge available about the downstream tasks, and how useful the\nrepresentation is for this task.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 18:13:41 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Tschannen", "Michael", ""], ["Bachem", "Olivier", ""], ["Lucic", "Mario", ""]]}, {"id": "1812.05082", "submitter": "Juan Manuel Fern\\'andez Montenegro JMFMontenegro", "authors": "Juan Manuel Fernandez Montenegro, Mahdi Maktab Dar Oghaz, Athanasios\n  Gkelias, Georgios Tzimiropoulos, Vasileios Argyriou", "title": "Features Extraction Based on an Origami Representation of 3D Landmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Feature extraction analysis has been widely investigated during the last\ndecades in computer vision community due to the large range of possible\napplications. Significant work has been done in order to improve the\nperformance of the emotion detection methods. Classification algorithms have\nbeen refined, novel preprocessing techniques have been applied and novel\nrepresentations from images and videos have been introduced. In this paper, we\npropose a preprocessing method and a novel facial landmarks' representation\naiming to improve the facial emotion detection accuracy. We apply our novel\nmethodology on the extended Cohn-Kanade (CK+) dataset and other datasets for\naffect classification based on Action Units (AU). The performance evaluation\ndemonstrates an improvement on facial emotion classification (accuracy and F1\nscore) that indicates the superiority of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 18:37:46 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Montenegro", "Juan Manuel Fernandez", ""], ["Oghaz", "Mahdi Maktab Dar", ""], ["Gkelias", "Athanasios", ""], ["Tzimiropoulos", "Georgios", ""], ["Argyriou", "Vasileios", ""]]}, {"id": "1812.05083", "submitter": "Miriam Cha", "authors": "Miriam Cha, Youngjune L. Gwon, H.T. Kung", "title": "Adversarial Learning of Semantic Relevance in Text to Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new approach that improves the training of generative\nadversarial nets (GANs) for synthesizing diverse images from a text input. Our\napproach is based on the conditional version of GANs and expands on previous\nwork leveraging an auxiliary task in the discriminator. Our generated images\nare not limited to certain classes and do not suffer from mode collapse while\nsemantically matching the text input. A key to our training methods is how to\nform positive and negative training examples with respect to the class label of\na given image. Instead of selecting random training examples, we perform\nnegative sampling based on the semantic distance from a positive example in the\nclass. We evaluate our approach using the Oxford-102 flower dataset, adopting\nthe inception score and multi-scale structural similarity index (MS-SSIM)\nmetrics to assess discriminability and diversity of the generated images. The\nempirical results indicate greater diversity in the generated images,\nespecially when we gradually select more negative training examples closer to a\npositive example in the semantic space.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 18:44:23 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 21:33:04 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Cha", "Miriam", ""], ["Gwon", "Youngjune L.", ""], ["Kung", "H. T.", ""]]}, {"id": "1812.05129", "submitter": "Itay Benou", "authors": "Itay Benou and Tammy Riklin-Raviv", "title": "DeepTract: A Probabilistic Deep Learning Framework for White Matter\n  Fiber Tractography", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-32248-9_70", "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepTract, a deep-learning framework for estimating white matter\nfibers orientation and streamline tractography. We adopt a data-driven approach\nfor fiber reconstruction from diffusion weighted images (DWI), which does not\nassume a specific diffusion model. We use a recurrent neural network for\nmapping sequences of DWI values into probabilistic fiber orientation\ndistributions. Based on these estimations, our model facilitates both\ndeterministic and probabilistic streamline tractography. We quantitatively\nevaluate our method using the Tractometer tool, demonstrating competitive\nperformance with state-of-the art classical and machine learning based\ntractography algorithms. We further present qualitative results of\nbundle-specific probabilistic tractography obtained using our method. The code\nis publicly available at: https://github.com/itaybenou/DeepTract.git.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 19:19:07 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 11:03:19 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 08:35:53 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Benou", "Itay", ""], ["Riklin-Raviv", "Tammy", ""]]}, {"id": "1812.05155", "submitter": "He Zhang", "authors": "He Zhang, Benjamin S. Riggan, Shuowen Hu, Nathaniel J. Short, Vishal\n  M.Patel", "title": "Synthesis of High-Quality Visible Faces from Polarimetric Thermal Faces\n  using Generative Adversarial Networks", "comments": "Note that the extended dataset is available upon request. Researchers\n  can contact Dr. Sean Hu from ARL at shuowen.hu.civ@mail.mil to obtain the\n  dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large domain discrepancy between faces captured in polarimetric (or\nconventional) thermal and visible domain makes cross-domain face verification a\nhighly challenging problem for human examiners as well as computer vision\nalgorithms. Previous approaches utilize either a two-step procedure (visible\nfeature estimation and visible image reconstruction) or an input-level fusion\ntechnique, where different Stokes images are concatenated and used as a\nmulti-channel input to synthesize the visible image given the corresponding\npolarimetric signatures. Although these methods have yielded improvements, we\nargue that input-level fusion alone may not be sufficient to realize the full\npotential of the available Stokes images. We propose a Generative Adversarial\nNetworks (GAN) based multi-stream feature-level fusion technique to synthesize\nhigh-quality visible images from prolarimetric thermal images. The proposed\nnetwork consists of a generator sub-network, constructed using an\nencoder-decoder network based on dense residual blocks, and a multi-scale\ndiscriminator sub-network. The generator network is trained by optimizing an\nadversarial loss in addition to a perceptual loss and an identity preserving\nloss to enable photo realistic generation of visible images while preserving\ndiscriminative characteristics. An extended dataset consisting of polarimetric\nthermal facial signatures of 111 subjects is also introduced. Multiple\nexperiments evaluated on different experimental protocols demonstrate that the\nproposed method achieves state-of-the-art performance. Code will be made\navailable at https://github.com/hezhangsprinter.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 21:04:32 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Zhang", "He", ""], ["Riggan", "Benjamin S.", ""], ["Hu", "Shuowen", ""], ["Short", "Nathaniel J.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1812.05177", "submitter": "Gukyeong Kwon", "authors": "Mohammed A. Aabed, Gukyeong Kwon, and Ghassan AlRegib", "title": "Power of Tempospatially Unified Spectral Density for Perceptual Video\n  Quality Assessment", "comments": "6 pages, 4 figures, 3 tables", "journal-ref": "M. A. Aabed, G. Kwon, and G. AlRegib, \"Power of Tempospatially\n  Unified Spectral Density for Perceptual Video Quality Assessment,\" 2017 IEEE\n  International Conference on Multimedia and Expo (ICME), Hong Kong, 2017, pp.\n  1476-1481", "doi": "10.1109/ICME.2017.8019333", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a perceptual video quality assessment (PVQA) metric for distorted\nvideos by analyzing the power spectral density (PSD) of a group of pictures.\nThis is an estimation approach that relies on the changes in video dynamic\ncalculated in the frequency domain and are primarily caused by distortion. We\nobtain a feature map by processing a 3D PSD tensor obtained from a set of\ndistorted frames. This is a full-reference tempospatial approach that considers\nboth temporal and spatial PSD characteristics. This makes it ubiquitously\nsuitable for videos with varying motion patterns and spatial contents. Our\ntechnique does not make any assumptions on the coding conditions, streaming\nconditions or distortion. This approach is also computationally inexpensive\nwhich makes it feasible for real-time and practical implementations. We\nvalidate our proposed metric by testing it on a variety of distorted sequences\nfrom PVQA databases. The results show that our metric estimates the perceptual\nquality at the sequence level accurately. We report the correlation\ncoefficients with the differential mean opinion scores (DMOS) reported in the\ndatabases. The results show high and competitive correlations compared with the\nstate of the art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 22:22:21 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Aabed", "Mohammed A.", ""], ["Kwon", "Gukyeong", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1812.05206", "submitter": "Ye Wang", "authors": "Ye Wang, Jongmoo Choi, Yueru Chen, Qin Huang, Siyang Li, Ming-Sui Lee,\n  C.-C. Jay Kuo", "title": "Design Pseudo Ground Truth with Motion Cue for Unsupervised Video Object\n  Segmentation", "comments": "16 pages, 7 figures, 6 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major technique debt in video object segmentation is to label the object\nmasks for training instances. As a result, we propose to prepare inexpensive,\nyet high quality pseudo ground truth corrected with motion cue for video object\nsegmentation training. Our method conducts semantic segmentation using instance\nsegmentation networks and, then, selects the segmented object of interest as\nthe pseudo ground truth based on the motion information. Afterwards, the pseudo\nground truth is exploited to finetune the pretrained objectness network to\nfacilitate object segmentation in the remaining frames of the video. We show\nthat the pseudo ground truth could effectively improve the segmentation\nperformance. This straightforward unsupervised video object segmentation method\nis more efficient than existing methods. Experimental results on DAVIS and FBMS\nshow that the proposed method outperforms state-of-the-art unsupervised\nsegmentation methods on various benchmark datasets. And the category-agnostic\npseudo ground truth has great potential to extend to multiple arbitrary object\ntracking.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 00:13:29 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Wang", "Ye", ""], ["Choi", "Jongmoo", ""], ["Chen", "Yueru", ""], ["Huang", "Qin", ""], ["Li", "Siyang", ""], ["Lee", "Ming-Sui", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1812.05214", "submitter": "Junnan Li Mr", "authors": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan Kankanhalli", "title": "Learning to Learn from Noisy Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of deep neural networks (DNNs) in image classification\ntasks, the human-level performance relies on massive training data with\nhigh-quality manual annotations, which are expensive and time-consuming to\ncollect. There exist many inexpensive data sources on the web, but they tend to\ncontain inaccurate labels. Training on noisy labeled datasets causes\nperformance degradation because DNNs can easily overfit to the label noise. To\novercome this problem, we propose a noise-tolerant training algorithm, where a\nmeta-learning update is performed prior to conventional gradient update. The\nproposed meta-learning method simulates actual training by generating synthetic\nnoisy labels, and train the model such that after one gradient update using\neach set of synthetic noisy labels, the model does not overfit to the specific\nnoise. We conduct extensive experiments on the noisy CIFAR-10 dataset and the\nClothing1M dataset. The results demonstrate the advantageous performance of the\nproposed method compared to several state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 00:58:05 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 12:28:54 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Li", "Junnan", ""], ["Wong", "Yongkang", ""], ["Zhao", "Qi", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1812.05219", "submitter": "Masakazu Iwamura", "authors": "Masakazu Iwamura", "title": "Advances of Scene Text Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces publicly available datasets in scene text detection\nand recognition. The information is as of 2017.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 01:36:36 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Iwamura", "Masakazu", ""]]}, {"id": "1812.05231", "submitter": "Prerana Mukherjee", "authors": "Vinay Kaushik, Prerana Mukherjee, Brejesh Lall", "title": "Nrityantar: Pose oblivious Indian classical dance sequence\n  classification system", "comments": "Eleventh Indian Conference on Computer Vision, Graphics and Image\n  Processing 2018", "journal-ref": null, "doi": "10.1145/3293353.3293419", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attempt to advance the research work done in human action\nrecognition to a rather specialized application namely Indian Classical Dance\n(ICD) classification. The variation in such dance forms in terms of hand and\nbody postures, facial expressions or emotions and head orientation makes pose\nestimation an extremely challenging task. To circumvent this problem, we\nconstruct a pose-oblivious shape signature which is fed to a sequence learning\nframework. The pose signature representation is done in two-fold process.\nFirst, we represent person-pose in first frame of a dance video using symmetric\nSpatial Transformer Networks (STN) to extract good person object proposals and\nCNN-based parallel single person pose estimator (SPPE). Next, the pose basis\nare converted to pose flows by assigning a similarity score between successive\nposes followed by non-maximal suppression. Instead of feeding a simple chain of\njoints in the sequence learner which generally hinders the network performance\nwe constitute a feature vector of the normalized distance vectors, flow, angles\nbetween anchor joints which captures the adjacency configuration in the\nskeletal pattern. Thus, the kinematic relationship amongst the body joints\nacross the frames using pose estimation helps in better establishing the\nspatio-temporal dependencies. We present an exhaustive empirical evaluation of\nstate-of-the-art deep network based methods for dance classification on ICD\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 02:16:38 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Kaushik", "Vinay", ""], ["Mukherjee", "Prerana", ""], ["Lall", "Brejesh", ""]]}, {"id": "1812.05233", "submitter": "Chi Zhang", "authors": "Chi Zhang, Yixin Zhu, Song-Chun Zhu", "title": "MetaStyle: Three-Way Trade-Off Among Speed, Flexibility, and Quality in\n  Neural Style Transfer", "comments": "AAAI 2019 spotlight. Supplementary:\n  http://wellyzhang.github.io/attach/aaai19zhang_supp.pdf GitHub:\n  https://github.com/WellyZhang/MetaStyle Project:\n  http://wellyzhang.github.io/project/metastyle.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unprecedented booming has been witnessed in the research area of artistic\nstyle transfer ever since Gatys et al. introduced the neural method. One of the\nremaining challenges is to balance a trade-off among three critical\naspects---speed, flexibility, and quality: (i) the vanilla optimization-based\nalgorithm produces impressive results for arbitrary styles, but is\nunsatisfyingly slow due to its iterative nature, (ii) the fast approximation\nmethods based on feed-forward neural networks generate satisfactory artistic\neffects but bound to only a limited number of styles, and (iii)\nfeature-matching methods like AdaIN achieve arbitrary style transfer in a\nreal-time manner but at a cost of the compromised quality. We find it\nconsiderably difficult to balance the trade-off well merely using a single\nfeed-forward step and ask, instead, whether there exists an algorithm that\ncould adapt quickly to any style, while the adapted model maintains high\nefficiency and good image quality. Motivated by this idea, we propose a novel\nmethod, coined MetaStyle, which formulates the neural style transfer as a\nbilevel optimization problem and combines learning with only a few\npost-processing update steps to adapt to a fast approximation model with\nsatisfying artistic effects, comparable to the optimization-based methods for\nan arbitrary style. The qualitative and quantitative analysis in the\nexperiments demonstrates that the proposed approach achieves high-quality\narbitrary artistic style transfer effectively, with a good trade-off among\nspeed, flexibility, and quality.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 02:25:10 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 12:47:31 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 03:44:16 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Zhang", "Chi", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1812.05252", "submitter": "Peng Gao", "authors": "Gao Peng, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven Hoi, Xiaogang\n  Wang, Hongsheng Li", "title": "Dynamic Fusion with Intra- and Inter- Modality Attention Flow for Visual\n  Question Answering", "comments": "CVPR 2019 ORAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning effective fusion of multi-modality features is at the heart of\nvisual question answering. We propose a novel method of dynamically fusing\nmulti-modal features with intra- and inter-modality information flow, which\nalternatively pass dynamic information between and across the visual and\nlanguage modalities. It can robustly capture the high-level interactions\nbetween language and vision domains, thus significantly improves the\nperformance of visual question answering. We also show that the proposed\ndynamic intra-modality attention flow conditioned on the other modality can\ndynamically modulate the intra-modality attention of the target modality, which\nis vital for multimodality feature fusion. Experimental evaluations on the VQA\n2.0 dataset show that the proposed method achieves state-of-the-art VQA\nperformance. Extensive ablation studies are carried out for the comprehensive\nanalysis of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 03:41:18 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 11:36:51 GMT"}, {"version": "v3", "created": "Sat, 10 Aug 2019 05:41:36 GMT"}, {"version": "v4", "created": "Fri, 23 Aug 2019 19:25:25 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Peng", "Gao", ""], ["Jiang", "Zhengkai", ""], ["You", "Haoxuan", ""], ["Lu", "Pan", ""], ["Hoi", "Steven", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1812.05262", "submitter": "Huiyu Wang", "authors": "Huiyu Wang, Aniruddha Kembhavi, Ali Farhadi, Alan Yuille, Mohammad\n  Rastegari", "title": "ELASTIC: Improving CNNs with Dynamic Scaling Policies", "comments": "CVPR 2019 oral, code available https://github.com/allenai/elastic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale variation has been a challenge from traditional to modern approaches in\ncomputer vision. Most solutions to scale issues have a similar theme: a set of\nintuitive and manually designed policies that are generic and fixed (e.g. SIFT\nor feature pyramid). We argue that the scaling policy should be learned from\ndata. In this paper, we introduce ELASTIC, a simple, efficient and yet very\neffective approach to learn a dynamic scale policy from data. We formulate the\nscaling policy as a non-linear function inside the network's structure that (a)\nis learned from data, (b) is instance specific, (c) does not add extra\ncomputation, and (d) can be applied on any network architecture. We applied\nELASTIC to several state-of-the-art network architectures and showed consistent\nimprovement without extra (sometimes even lower) computation on ImageNet\nclassification, MSCOCO multi-label classification, and PASCAL VOC semantic\nsegmentation. Our results show major improvement for images with scale\nchallenges. Our code is available here: https://github.com/allenai/elastic\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 05:00:31 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 18:29:53 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Wang", "Huiyu", ""], ["Kembhavi", "Aniruddha", ""], ["Farhadi", "Ali", ""], ["Yuille", "Alan", ""], ["Rastegari", "Mohammad", ""]]}, {"id": "1812.05276", "submitter": "Zetong Yang", "authors": "Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, Jiaya Jia", "title": "IPOD: Intensive Point-based Object Detector for Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel 3D object detection framework, named IPOD, based on raw\npoint cloud. It seeds object proposal for each point, which is the basic\nelement. This paradigm provides us with high recall and high fidelity of\ninformation, leading to a suitable way to process point cloud data. We design\nan end-to-end trainable architecture, where features of all points within a\nproposal are extracted from the backbone network and achieve a proposal feature\nfor final bounding inference. These features with both context information and\nprecise point cloud coordinates yield improved performance. We conduct\nexperiments on KITTI dataset, evaluating our performance in terms of 3D object\ndetection, Bird's Eye View (BEV) detection and 2D object detection. Our method\naccomplishes new state-of-the-art , showing great advantage on the hard set.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 05:48:49 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Yang", "Zetong", ""], ["Sun", "Yanan", ""], ["Liu", "Shu", ""], ["Shen", "Xiaoyong", ""], ["Jia", "Jiaya", ""]]}, {"id": "1812.05285", "submitter": "Minghao Guo", "authors": "Minghao Guo, Zhao Zhong, Wei Wu, Dahua Lin, and Junjie Yan", "title": "IRLAS: Inverse Reinforcement Learning for Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an inverse reinforcement learning method for\narchitecture search (IRLAS), which trains an agent to learn to search network\nstructures that are topologically inspired by human-designed network. Most\nexisting architecture search approaches totally neglect the topological\ncharacteristics of architectures, which results in complicated architecture\nwith a high inference latency. Motivated by the fact that human-designed\nnetworks are elegant in topology with a fast inference speed, we propose a\nmirror stimuli function inspired by biological cognition theory to extract the\nabstract topological knowledge of an expert human-design network (ResNeXt). To\navoid raising a too strong prior over the search space, we introduce inverse\nreinforcement learning to train the mirror stimuli function and exploit it as a\nheuristic guidance for architecture search, easily generalized to different\narchitecture search algorithms. On CIFAR-10, the best architecture searched by\nour proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our\nmodel achieves a state-of-the-art top-1 accuracy 75.28%, while being 2~4x\nfaster than most auto-generated architectures. A fast version of this model\nachieves 10% faster than MobileNetV2, while maintaining a higher accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 06:53:36 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 05:27:07 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 05:26:56 GMT"}, {"version": "v4", "created": "Mon, 19 Aug 2019 06:31:43 GMT"}, {"version": "v5", "created": "Wed, 6 Nov 2019 02:30:08 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Guo", "Minghao", ""], ["Zhong", "Zhao", ""], ["Wu", "Wei", ""], ["Lin", "Dahua", ""], ["Yan", "Junjie", ""]]}, {"id": "1812.05308", "submitter": "Avantika Singh Ms", "authors": "Avantika Singh, Ashish Arora, Shreya Hasmukh Patel, Gaurav Jaswal,\n  Aditya Nigam", "title": "FDFNet : A Secure Cancelable Deep Finger Dorsal Template Generation\n  Network Secured via. Bio-Hashing", "comments": "Accepted in ISBA 2019: International Conference on Identity, Security\n  and Behavior Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Present world has already been consistently exploring the fine edges of\nonline and digital world by imposing multiple challenging problems/scenarios.\nSimilar to physical world, personal identity management is very crucial\nin-order to provide any secure online system. Last decade has seen a lot of\nwork in this area using biometrics such as face, fingerprint, iris etc. Still\nthere exist several vulnerabilities and one should have to address the problem\nof compromised biometrics much more seriously, since they cannot be modified\neasily once compromised. In this work, we have proposed a secure cancelable\nfinger dorsal template generation network (learning domain specific features)\nsecured via. Bio-Hashing. Proposed system effectively protects the original\nfinger dorsal images by withdrawing compromised template and reassigning the\nnew one. A novel Finger-Dorsal Feature Extraction Net (FDFNet) has been\nproposed for extracting the discriminative features. This network is\nexclusively trained on trait specific features without using any kind of\npre-trained architecture. Later Bio-Hashing, a technique based on assigning a\ntokenized random number to each user, has been used to hash the features\nextracted from FDFNet. To test the performance of the proposed architecture, we\nhave tested it over two benchmark public finger knuckle datasets: PolyU FKP and\nPolyU Contactless FKI. The experimental results shows the effectiveness of the\nproposed system in terms of security and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 08:10:56 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Singh", "Avantika", ""], ["Arora", "Ashish", ""], ["Patel", "Shreya Hasmukh", ""], ["Jaswal", "Gaurav", ""], ["Nigam", "Aditya", ""]]}, {"id": "1812.05313", "submitter": "Hong-Yu Zhou", "authors": "Hong-Yu Zhou, Avital Oliver, Jianxin Wu, Yefeng Zheng", "title": "When Semi-Supervised Learning Meets Transfer Learning: Training\n  Strategies, Models and Datasets", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Supervised Learning (SSL) has been proved to be an effective way to\nleverage both labeled and unlabeled data at the same time. Recent\nsemi-supervised approaches focus on deep neural networks and have achieved\npromising results on several benchmarks: CIFAR10, CIFAR100 and SVHN. However,\nmost of their experiments are based on models trained from scratch instead of\npre-trained models. On the other hand, transfer learning has demonstrated its\nvalue when the target domain has limited labeled data. Here comes the intuitive\nquestion: is it possible to incorporate SSL when fine-tuning a pre-trained\nmodel? We comprehensively study how SSL methods starting from pretrained models\nperform under varying conditions, including training strategies, architecture\nchoice and datasets. From this study, we obtain several interesting and useful\nobservations.\n  While practitioners have had an intuitive understanding of these\nobservations, we do a comprehensive emperical analysis and demonstrate that:\n(1) the gains from SSL techniques over a fully-supervised baseline are smaller\nwhen trained from a pre-trained model than when trained from random\ninitialization, (2) when the domain of the source data used to train the\npre-trained model differs significantly from the domain of the target task, the\ngains from SSL are significantly higher and (3) some SSL methods are able to\nadvance fully-supervised baselines (like Pseudo-Label).\n  We hope our studies can deepen the understanding of SSL research and\nfacilitate the process of developing more effective SSL methods to utilize\npre-trained models. Code is now available at github.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 08:46:42 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Zhou", "Hong-Yu", ""], ["Oliver", "Avital", ""], ["Wu", "Jianxin", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1812.05319", "submitter": "Di Wu", "authors": "Di Wu, Hong-Wei Yang and De-Shuang Huang", "title": "Omni-directional Feature Learning for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (PReID) has received increasing attention due to it\nis an important part in intelligent surveillance. Recently, many\nstate-of-the-art methods on PReID are part-based deep models. Most of them\nfocus on learning the part feature representation of person body in horizontal\ndirection. However, the feature representation of body in vertical direction is\nusually ignored. Besides, the spatial information between these part features\nand the different feature channels is not considered. In this study, we\nintroduce a multi-branches deep model for PReID. Specifically, the model\nconsists of five branches. Among the five branches, two of them learn the local\nfeature with spatial information from horizontal or vertical orientations,\nrespectively. The other one aims to learn interdependencies knowledge between\ndifferent feature channels generated by the last convolution layer. The remains\nof two other branches are identification and triplet sub-networks, in which the\ndiscriminative global feature and a corresponding measurement can be learned\nsimultaneously. All the five branches can improve the representation learning.\nWe conduct extensive comparative experiments on three PReID benchmarks\nincluding CUHK03, Market-1501 and DukeMTMC-reID. The proposed deep framework\noutperforms many state-of-the-art in most cases.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 09:05:11 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Wu", "Di", ""], ["Yang", "Hong-Wei", ""], ["Huang", "De-Shuang", ""]]}, {"id": "1812.05329", "submitter": "Gu Jun", "authors": "Jun Gu, Guangluan Xu, Yue Zhang, Xian Sun, Ran Wen and Lei Wang", "title": "Wider Channel Attention Network for Remote Sensing Image\n  Super-resolution", "comments": "This work is proposed for remote sensing images, but the idea of the\n  whole paper do not foucs on the characteristics of remote sensing images. The\n  content of the article does not match the title. In this case, we want to do\n  some experiments on the natural images to verify the three tricks in our work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural networks (CNNs) have obtained promising\nresults in image processing tasks including super-resolution (SR). However,\nmost CNN-based SR methods treat low-resolution (LR) inputs and features equally\nacross channels, rarely notice the loss of information flow caused by the\nactivation function and fail to leverage the representation ability of CNNs. In\nthis letter, we propose a novel single-image super-resolution (SISR) algorithm\nnamed Wider Channel Attention Network (WCAN) for remote sensing images.\nFirstly, the channel attention mechanism is used to adaptively recalibrate the\nimportance of each channel at the middle of the wider attention block (WAB).\nSecondly, we propose the Local Memory Connection (LMC) to enhance the\ninformation flow. Finally, the features within each WAB are fused to take\nadvantage of the network's representation capability and further improve\ninformation and gradient flow. Analytic experiments on a public remote sensing\ndata set (UC Merced) show that our WCAN achieves better accuracy and visual\nimprovements against most state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 09:27:12 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 08:51:09 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Gu", "Jun", ""], ["Xu", "Guangluan", ""], ["Zhang", "Yue", ""], ["Sun", "Xian", ""], ["Wen", "Ran", ""], ["Wang", "Lei", ""]]}, {"id": "1812.05415", "submitter": "Andres Milioto", "authors": "F. Langer and L. Mandtler and A. Milioto and E. Palazzolo and C.\n  Stachniss", "title": "Geometrical Stem Detection from Image Data for Precision Agriculture", "comments": "Note that this work has been published without author's consent by\n  WSEAS TRANSACTIONS on SYSTEMS, so please cite this arxiv paper if you want to\n  reference to our work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High efficiency in precision farming depends on accurate tools to perform\nweed detection and mapping of crops. This allows for precise removal of harmful\nweeds with a lower amount of pesticides, as well as increase of the harvest's\nyield by providing the farmer with valuable information. In this paper, we\naddress the problem of fully automatic stem detection from image data for this\npurpose. Our approach runs on mobile agricultural robots taking RGB images.\nAfter processing the images to obtain a vegetation mask, our approach separates\neach plant into its individual leaves and later estimates a precise stem\nposition. This allows an upstream mapping algorithm to add the high-resolution\nstem positions as a semantic aggregate to the global map of the robot, which\ncan be used for weeding and for analyzing crop statistics. We implemented our\napproach and thoroughly tested it on three different datasets with vegetation\nmasks and stem position ground truth. The experiments presented in this paper\nconclude that our module is able to detect leaves and estimate the stem's\nposition at a rate of 56 Hz on a single CPU. We furthermore provide the\nsoftware to the community.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 13:28:27 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Langer", "F.", ""], ["Mandtler", "L.", ""], ["Milioto", "A.", ""], ["Palazzolo", "E.", ""], ["Stachniss", "C.", ""]]}, {"id": "1812.05418", "submitter": "Rui Gong", "authors": "Rui Gong, Wen Li, Yuhua Chen, Luc Van Gool", "title": "DLOW: Domain Flow for Adaptation and Generalization", "comments": "Accepted to CVPR 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a domain flow generation(DLOW) model to bridge two\ndifferent domains by generating a continuous sequence of intermediate domains\nflowing from one domain to the other. The benefits of our DLOW model are\ntwo-fold. First, it is able to transfer source images into different styles in\nthe intermediate domains. The transferred images smoothly bridge the gap\nbetween source and target domains, thus easing the domain adaptation task.\nSecond, when multiple target domains are provided for training, our DLOW model\nis also able to generate new styles of images that are unseen in the training\ndata. We implement our DLOW model based on CycleGAN. A domainness variable is\nintroduced to guide the model to generate the desired intermediate domain\nimages. In the inference phase, a flow of various styles of images can be\nobtained by varying the domainness variable. We demonstrate the effectiveness\nof our model for both cross-domain semantic segmentation and the style\ngeneralization tasks on benchmark datasets. Our implementation is available at\nhttps://github.com/ETHRuiGong/DLOW.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 13:31:59 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 14:04:36 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Gong", "Rui", ""], ["Li", "Wen", ""], ["Chen", "Yuhua", ""], ["Van Gool", "Luc", ""]]}, {"id": "1812.05447", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee, Heesung Kwon, Wonkook Kim", "title": "Discovering and Generating Hard Examples for Training a Red Tide\n  Detector", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, accurate detection of natural phenomena, such as red tide, that\nadversely affect wildlife and human, using satellite images has been\nincreasingly utilized. However, red tide detection on satellite images still\nremains a very hard task due to unpredictable nature of red tide occurrence,\nextreme sparsity of red tide samples, difficulties in accurate groundtruthing,\netc. In this paper, we aim to tackle both the data sparsity and groundtruthing\nissues by primarily addressing two challenges: i) significant lack of hard\nexamples of non-red tide that can enhance detection performance and ii) extreme\ndata imbalance between red tide and non-red tide examples. In the proposed\nwork, we devise a 9-layer fully convolutional network jointly optimized with\ntwo plug-in modules tailored to overcoming the two challenges: i) a hard\nnegative example generator (HNG) to supplement the hard negative (non-red tide)\nexamples and ii) cascaded online hard example mining (cOHEM) to ease the data\nimbalance. Our proposed network jointly trained with HNG and cOHEM provides\nstate-of-the-art red tide detection accuracy on GOCI satellite images.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 14:29:25 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 13:16:02 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Lee", "Hyungtae", ""], ["Kwon", "Heesung", ""], ["Kim", "Wonkook", ""]]}, {"id": "1812.05455", "submitter": "Daniel Harari", "authors": "Daniel Harari", "title": "Using Motion and Internal Supervision in Object Recognition", "comments": "PhD dissertation, 87 pages, 51 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis we address two related aspects of visual object recognition:\nthe use of motion information, and the use of internal supervision, to help\nunsupervised learning. These two aspects are inter-related in the current\nstudy, since image motion is used for internal supervision, via the detection\nof spatiotemporal events of active-motion and the use of tracking. Most current\nwork in object recognition deals with static images during both learning and\nrecognition. In contrast, we are interested in a dynamic scene where visual\nprocesses, such as detecting motion events and tracking, contribute\nspatiotemporal information, which is useful for object attention, motion\nsegmentation, 3-D understanding and object interactions. We explore the use of\nthese sources of information in both learning and recognition processes. In the\nfirst part of the work, we demonstrate how motion can be used for adaptive\ndetection of object-parts in dynamic environments, while automatically learning\nnew object appearances and poses. In the second and main part of the study we\ndevelop methods for using specific types of visual motion to solve two\ndifficult problems in unsupervised visual learning: learning to recognize hands\nby their appearance and by context, and learning to extract direction of gaze.\nWe use our conclusions in this part to propose a model for several aspects of\nlearning by human infants from their visual environment.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 14:33:31 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Harari", "Daniel", ""]]}, {"id": "1812.05477", "submitter": "Alessandro Di Martino", "authors": "Alessandro Di Martino and Erik Bodin and Carl Henrik Ek and Neill D.F.\n  Campbell", "title": "Gaussian Process Deep Belief Networks: A Smooth Generative Model of\n  Shape with Uncertainty Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shape of an object is an important characteristic for many vision\nproblems such as segmentation, detection and tracking. Being independent of\nappearance, it is possible to generalize to a large range of objects from only\nsmall amounts of data. However, shapes represented as silhouette images are\nchallenging to model due to complicated likelihood functions leading to\nintractable posteriors. In this paper we present a generative model of shapes\nwhich provides a low dimensional latent encoding which importantly resides on a\nsmooth manifold with respect to the silhouette images. The proposed model\npropagates uncertainty in a principled manner allowing it to learn from small\namounts of data and providing predictions with associated uncertainty. We\nprovide experiments that show how our proposed model provides favorable\nquantitative results compared with the state-of-the-art while simultaneously\nproviding a representation that resides on a low-dimensional interpretable\nmanifold.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 15:25:40 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Di Martino", "Alessandro", ""], ["Bodin", "Erik", ""], ["Ek", "Carl Henrik", ""], ["Campbell", "Neill D. F.", ""]]}, {"id": "1812.05478", "submitter": "Alejandro Hernandez Ruiz", "authors": "Alejandro Hernandez Ruiz, Juergen Gall, Francesc Moreno-Noguer", "title": "Human Motion Prediction via Spatio-Temporal Inpainting", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Generative Adversarial Network (GAN) to forecast 3D human motion\ngiven a sequence of past 3D skeleton poses. While recent GANs have shown\npromising results, they can only forecast plausible motion over relatively\nshort periods of time (few hundred milliseconds) and typically ignore the\nabsolute position of the skeleton w.r.t. the camera. Our scheme provides long\nterm predictions (two seconds or more) for both the body pose and its absolute\nposition. Our approach builds upon three main contributions. First, we\nrepresent the data using a spatio-temporal tensor of 3D skeleton coordinates\nwhich allows formulating the prediction problem as an inpainting one, for which\nGANs work particularly well. Secondly, we design an architecture to learn the\njoint distribution of body poses and global motion, capable to hypothesize\nlarge chunks of the input 3D tensor with missing data. And finally, we argue\nthat the L2 metric, considered so far by most approaches, fails to capture the\nactual distribution of long-term human motion. We propose two alternative\nmetrics, based on the distribution of frequencies, that are able to capture\nmore realistic motion patterns. Extensive experiments demonstrate our approach\nto significantly improve the state of the art, while also handling situations\nin which past observations are corrupted by occlusions, noise and missing\nframes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 15:27:15 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 03:41:27 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ruiz", "Alejandro Hernandez", ""], ["Gall", "Juergen", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "1812.05484", "submitter": "Othman Sbai", "authors": "Othman Sbai, Camille Couprie, Mathieu Aubry", "title": "Unsupervised Image Decomposition in Vector Layers", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image generation is becoming a tool to enhance artists and designers\ncreativity potential. In this paper, we aim at making the generation process\nmore structured and easier to interact with. Inspired by vector graphics\nsystems, we propose a new deep image reconstruction paradigm where the outputs\nare composed from simple layers, defined by their color and a vector\ntransparency mask. This presents a number of advantages compared to the\ncommonly used convolutional network architectures. In particular, our layered\ndecomposition allows simple user interaction, for example to update a given\nmask, or change the color of a selected layer. From a compact code, our\narchitecture also generates vector images with a virtually infinite resolution,\nthe color at each point in an image being a parametric function of its\ncoordinates. We validate the efficiency of our approach by comparing\nreconstructions with state-of-the-art baselines given similar memory resources\non CelebA and ImageNet datasets. Most importantly, we demonstrate several\napplications of our new image representation obtained in an unsupervised\nmanner, including editing, vectorization and image search.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 15:41:19 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 22:11:18 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Sbai", "Othman", ""], ["Couprie", "Camille", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1812.05490", "submitter": "Ahmad S. Tarawneh", "authors": "Ahmad S. Tarawneh, Ahmad B. A. Hassanat, Ceyhun Celik, Dmitry\n  Chetverikov, M. Sohel Rahman, Chaman Verma", "title": "Deep Face Image Retrieval: a Comparative Study with Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial image retrieval is a challenging task since faces have many similar\nfeatures (areas), which makes it difficult for the retrieval systems to\ndistinguish faces of different people. With the advent of deep learning, deep\nnetworks are often applied to extract powerful features that are used in many\nareas of computer vision. This paper investigates the application of different\ndeep learning models for face image retrieval, namely, Alexlayer6, Alexlayer7,\nVGG16layer6, VGG16layer7, VGG19layer6, and VGG19layer7, with two types of\ndictionary learning techniques, namely $K$-means and $K$-SVD. We also\ninvestigate some coefficient learning techniques such as the Homotopy, Lasso,\nElastic Net and SSF and their effect on the face retrieval system. The\ncomparative results of the experiments conducted on three standard face image\ndatasets show that the best performers for face image retrieval are Alexlayer7\nwith $K$-means and SSF, Alexlayer6 with $K$-SVD and SSF, and Alexlayer6 with\n$K$-means and SSF. The APR and ARR of these methods were further compared to\nsome of the state of the art methods based on local descriptors. The\nexperimental results show that deep learning outperforms most of those methods\nand therefore can be recommended for use in practice of face image retrieval\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 15:58:02 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Tarawneh", "Ahmad S.", ""], ["Hassanat", "Ahmad B. A.", ""], ["Celik", "Ceyhun", ""], ["Chetverikov", "Dmitry", ""], ["Rahman", "M. Sohel", ""], ["Verma", "Chaman", ""]]}, {"id": "1812.05530", "submitter": "Dino Ienco", "authors": "Dino Ienco, Raffaele Gaetano, Roberto Interdonato, Kenji Ose and Dinh\n  Ho Tong Minh", "title": "Combining Sentinel-1 and Sentinel-2 Time Series via RNN for object-based\n  land cover classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar and Optical Satellite Image Time Series (SITS) are sources of\ninformation that are commonly employed to monitor earth surfaces for tasks\nrelated to ecology, agriculture, mobility, land management planning and land\ncover monitoring. Many studies have been conducted using one of the two\nsources, but how to smartly combine the complementary information provided by\nradar and optical SITS is still an open challenge. In this context, we propose\na new neural architecture for the combination of Sentinel-1 (S1) and Sentinel-2\n(S2) imagery at object level, applied to a real-world land cover classification\ntask. Experiments carried out on the Reunion Island, a overseas department of\nFrance in the Indian Ocean, demonstrate the significance of our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 17:24:58 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Ienco", "Dino", ""], ["Gaetano", "Raffaele", ""], ["Interdonato", "Roberto", ""], ["Ose", "Kenji", ""], ["Minh", "Dinh Ho Tong", ""]]}, {"id": "1812.05537", "submitter": "Line K\\\"uhnel", "authors": "Line K\\\"uhnel, Alexis Arnaudon, Tom Fletcher, and Stefan Sommer", "title": "Stochastic Image Deformation in Frequency Domain and Parameter\n  Estimation using Moment Evolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling deformation of anatomical objects observed in medical images can\nhelp describe disease progression patterns and variations in anatomy across\npopulations. We apply a stochastic generalisation of the Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) framework to model differences in the\nevolution of anatomical objects detected in populations of image data. The\ncomputational challenges that are prevalent even in the deterministic LDDMM\nsetting are handled by extending the FLASH LDDMM representation to the\nstochastic setting keeping a finite discretisation of the infinite dimensional\nspace of image deformations. In this computationally efficient setting, we\nperform estimation to infer parameters for noise correlations and local\nvariability in datasets of images. Fundamental for the optimisation procedure\nis using the finite dimensional Fourier representation to derive approximations\nof the evolution of moments for the stochastic warps. Particularly, the first\nmoment allows us to infer deformation mean trajectories. The second moment\nencodes variation around the mean, and thus provides information on the noise\ncorrelation. We show on simulated datasets of 2D MR brain images that the\nestimation algorithm can successfully recover parameters of the stochastic\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 17:46:06 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["K\u00fchnel", "Line", ""], ["Arnaudon", "Alexis", ""], ["Fletcher", "Tom", ""], ["Sommer", "Stefan", ""]]}, {"id": "1812.05538", "submitter": "Hazel Doughty", "authors": "Hazel Doughty, Walterio Mayol-Cuevas and Dima Damen", "title": "The Pros and Cons: Rank-aware Temporal Attention for Skill Determination\n  in Long Videos", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model to determine relative skill from long videos, through\nlearnable temporal attention modules. Skill determination is formulated as a\nranking problem, making it suitable for common and generic tasks. However, for\nlong videos, parts of the video are irrelevant for assessing skill, and there\nmay be variability in the skill exhibited throughout a video. We therefore\npropose a method which assesses the relative overall level of skill in a long\nvideo by attending to its skill-relevant parts. Our approach trains temporal\nattention modules, learned with only video-level supervision, using a novel\nrank-aware loss function. In addition to attending to task relevant video\nparts, our proposed loss jointly trains two attention modules to separately\nattend to video parts which are indicative of higher (pros) and lower (cons)\nskill. We evaluate our approach on the EPIC-Skills dataset and additionally\nannotate a larger dataset from YouTube videos for skill determination with five\npreviously unexplored tasks. Our method outperforms previous approaches and\nclassic softmax attention on both datasets by over 4% pairwise accuracy, and as\nmuch as 12% on individual tasks. We also demonstrate our model's ability to\nattend to rank-aware parts of the video.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 17:46:23 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 15:17:19 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Doughty", "Hazel", ""], ["Mayol-Cuevas", "Walterio", ""], ["Damen", "Dima", ""]]}, {"id": "1812.05581", "submitter": "Youngjun Choe", "authors": "Sean Andrew Chen, Andrew Escay, Christopher Haberland, Tessa\n  Schneider, Valentina Staneva, Youngjun Choe", "title": "Benchmark Dataset for Automatic Damaged Building Detection from\n  Post-Hurricane Remotely Sensed Imagery", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid damage assessment is of crucial importance to emergency responders\nduring hurricane events, however, the evaluation process is often slow,\nlabor-intensive, costly, and error-prone. New advances in computer vision and\nremote sensing open possibilities to observe the Earth at a different scale.\nHowever, substantial pre-processing work is still required in order to apply\nstate-of-the-art methodology for emergency response. To enable the comparison\nof methods for automatic detection of damaged buildings from post-hurricane\nremote sensing imagery taken from both airborne and satellite sensors, this\npaper presents the development of benchmark datasets from publicly available\ndata. The major contributions of this work include (1) a scalable framework for\ncreating benchmark datasets of hurricane-damaged buildings and (2) public\nsharing of the resulting benchmark datasets for Greater Houston area after\nHurricane Harvey in 2017. The proposed approach can be used to build other\nhurricane-damaged building datasets on which researchers can train and test\nobject detection models to automatically identify damaged buildings.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 18:52:40 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Chen", "Sean Andrew", ""], ["Escay", "Andrew", ""], ["Haberland", "Christopher", ""], ["Schneider", "Tessa", ""], ["Staneva", "Valentina", ""], ["Choe", "Youngjun", ""]]}, {"id": "1812.05583", "submitter": "Hamid Izadinia", "authors": "Hamid Izadinia, Steven M. Seitz", "title": "Scene Recomposition by Learning-based ICP", "comments": "To appear at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By moving a depth sensor around a room, we compute a 3D CAD model of the\nenvironment, capturing the room shape and contents such as chairs, desks,\nsofas, and tables. Rather than reconstructing geometry, we match, place, and\nalign each object in the scene to thousands of CAD models of objects. In\naddition to the fully automatic system, the key technical contribution is a\nnovel approach for aligning CAD models to 3D scans, based on deep reinforcement\nlearning. This approach, which we call Learning-based ICP, outperforms prior\nICP methods in the literature, by learning the best points to match and\nconditioning on object viewpoint. LICP learns to align using only synthetic\ndata and does not require ground truth annotation of object pose or keypoint\npair matching in real scene scans. While LICP is trained on synthetic data and\nwithout 3D real scene annotations, it outperforms both learned local deep\nfeature matching and geometric based alignment methods in real scenes. The\nproposed method is evaluated on real scenes datasets of SceneNN and ScanNet as\nwell as synthetic scenes of SUNCG. High quality results are demonstrated on a\nrange of real world scenes, with robustness to clutter, viewpoint, and\nocclusion.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 18:54:14 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 08:09:26 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Izadinia", "Hamid", ""], ["Seitz", "Steven M.", ""]]}, {"id": "1812.05586", "submitter": "Mahyar Najibi", "authors": "Mahyar Najibi, Bharat Singh, Larry S. Davis", "title": "FA-RPN: Floating Region Proposals for Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for generating region proposals for performing\nface-detection. Instead of classifying anchor boxes using features from a pixel\nin the convolutional feature map, we adopt a pooling-based approach for\ngenerating region proposals. However, pooling hundreds of thousands of anchors\nwhich are evaluated for generating proposals becomes a computational bottleneck\nduring inference. To this end, an efficient anchor placement strategy for\nreducing the number of anchor-boxes is proposed. We then show that proposals\ngenerated by our network (Floating Anchor Region Proposal Network, FA-RPN) are\nbetter than RPN for generating region proposals for face detection. We discuss\nseveral beneficial features of FA-RPN proposals like iterative refinement,\nplacement of fractional anchors and changing anchors which can be enabled\nwithout making any changes to the trained model. Our face detector based on\nFA-RPN obtains 89.4% mAP with a ResNet-50 backbone on the WIDER dataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 18:55:17 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Najibi", "Mahyar", ""], ["Singh", "Bharat", ""], ["Davis", "Larry S.", ""]]}, {"id": "1812.05634", "submitter": "Anna Rohrbach", "authors": "Jae Sung Park, Marcus Rohrbach, Trevor Darrell, Anna Rohrbach", "title": "Adversarial Inference for Multi-Sentence Video Description", "comments": "Accepted to Computer Vision and Pattern Recognition (CVPR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While significant progress has been made in the image captioning task, video\ndescription is still in its infancy due to the complex nature of video data.\nGenerating multi-sentence descriptions for long videos is even more\nchallenging. Among the main issues are the fluency and coherence of the\ngenerated descriptions, and their relevance to the video. Recently,\nreinforcement and adversarial learning based methods have been explored to\nimprove the image captioning models; however, both types of methods suffer from\na number of issues, e.g. poor readability and high redundancy for RL and\nstability issues for GANs. In this work, we instead propose to apply\nadversarial techniques during inference, designing a discriminator which\nencourages better multi-sentence video description. In addition, we find that a\nmulti-discriminator \"hybrid\" design, where each discriminator targets one\naspect of a description, leads to the best results. Specifically, we decouple\nthe discriminator to evaluate on three criteria: 1) visual relevance to the\nvideo, 2) language diversity and fluency, and 3) coherence across sentences.\nOur approach results in more accurate, diverse, and coherent multi-sentence\nvideo descriptions, as shown by automatic as well as human evaluation on the\npopular ActivityNet Captions dataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 19:07:17 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 02:04:44 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Park", "Jae Sung", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Anna", ""]]}, {"id": "1812.05637", "submitter": "Hao Huang", "authors": "Hao Huang, Luowei Zhou, Wei Zhang, Jason J. Corso, Chenliang Xu", "title": "Dynamic Graph Modules for Modeling Object-Object Interactions in\n  Activity Recognition", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video action recognition, a critical problem in video understanding, has been\ngaining increasing attention. To identify actions induced by complex\nobject-object interactions, we need to consider not only spatial relations\namong objects in a single frame, but also temporal relations among different or\nthe same objects across multiple frames. However, existing approaches that\nmodel video representations and non-local features are either incapable of\nexplicitly modeling relations at the object-object level or unable to handle\nstreaming videos. In this paper, we propose a novel dynamic hidden graph module\nto model complex object-object interactions in videos, of which two\ninstantiations are considered: a visual graph that captures appearance/motion\nchanges among objects and a location graph that captures relative\nspatiotemporal position changes among objects. Additionally, the proposed graph\nmodule allows us to process streaming videos, setting it apart from existing\nmethods. Experimental results on benchmark datasets, Something-Something and\nActivityNet, show the competitive performance of our method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 19:11:55 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 17:51:56 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 17:02:47 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Huang", "Hao", ""], ["Zhou", "Luowei", ""], ["Zhang", "Wei", ""], ["Corso", "Jason J.", ""], ["Xu", "Chenliang", ""]]}, {"id": "1812.05642", "submitter": "Yue Meng", "authors": "Yue Meng, Yongxi Lu, Aman Raj, Samuel Sunarjo, Rui Guo, Tara Javidi,\n  Gaurav Bansal, Dinesh Bharadia", "title": "SIGNet: Semantic Instance Aided Unsupervised 3D Geometry Perception", "comments": "To appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning for geometric perception (depth, optical flow, etc.) is\nof great interest to autonomous systems. Recent works on unsupervised learning\nhave made considerable progress on perceiving geometry; however, they usually\nignore the coherence of objects and perform poorly under scenarios with dark\nand noisy environments. In contrast, supervised learning algorithms, which are\nrobust, require large labeled geometric dataset. This paper introduces SIGNet,\na novel framework that provides robust geometry perception without requiring\ngeometrically informative labels. Specifically, SIGNet integrates semantic\ninformation to make depth and flow predictions consistent with objects and\nrobust to low lighting conditions. SIGNet is shown to improve upon the\nstate-of-the-art unsupervised learning for depth prediction by 30% (in squared\nrelative error). In particular, SIGNet improves the dynamic object class\nperformance by 39% in depth prediction and 29% in flow prediction. Our code\nwill be made available at https://github.com/mengyuest/SIGNet\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 19:21:44 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 02:58:11 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Meng", "Yue", ""], ["Lu", "Yongxi", ""], ["Raj", "Aman", ""], ["Sunarjo", "Samuel", ""], ["Guo", "Rui", ""], ["Javidi", "Tara", ""], ["Bansal", "Gaurav", ""], ["Bharadia", "Dinesh", ""]]}, {"id": "1812.05659", "submitter": "Enes Karaaslan", "authors": "Enes Karaaslan, Ulas Bagci, F. Necati Catbas", "title": "Artificial Intelligence Assisted Infrastructure Assessment Using Mixed\n  Reality Systems", "comments": "5,240 word texts, 3 tables, 14 figures. Transportation Research\n  Record: Journal of the Transportation Research Board, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional methods for visual assessment of civil infrastructures have\ncertain limitations, such as subjectivity of the collected data, long\ninspection time, and high cost of labor. Although some new technologies i.e.\nrobotic techniques that are currently in practice can collect objective,\nquantified data, the inspectors own expertise is still critical in many\ninstances since these technologies are not designed to work interactively with\nhuman inspector. This study aims to create a smart, human centered method that\noffers significant contributions to infrastructure inspection, maintenance,\nmanagement practice, and safety for the bridge owners. By developing a smart\nMixed Reality framework, which can be integrated into a wearable holographic\nheadset device, a bridge inspector, for example, can automatically analyze a\ncertain defect such as a crack that he or she sees on an element, display its\ndimension information in real-time along with the condition state. Such systems\ncan potentially decrease the time and cost of infrastructure inspections by\naccelerating essential tasks of the inspector such as defect measurement,\ncondition assessment and data processing to management systems. The human\ncentered artificial intelligence will help the inspector collect more\nquantified and objective data while incorporating inspectors professional\njudgement. This study explains in detail the described system and related\nmethodologies of implementing attention guided semi supervised deep learning\ninto mixed reality technology, which interacts with the human inspector during\nassessment. Thereby, the inspector and the AI will collaborate or communicate\nfor improved visual inspection.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 19:46:00 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Karaaslan", "Enes", ""], ["Bagci", "Ulas", ""], ["Catbas", "F. Necati", ""]]}, {"id": "1812.05720", "submitter": "Maksym Andriushchenko", "authors": "Matthias Hein, Maksym Andriushchenko, Julian Bitterwolf", "title": "Why ReLU networks yield high-confidence predictions far away from the\n  training data and how to mitigate the problem", "comments": "Slight update of the CVPR 2019 final version [accepted with an oral\n  presentation]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers used in the wild, in particular for safety-critical systems,\nshould not only have good generalization properties but also should know when\nthey don't know, in particular make low confidence predictions far away from\nthe training data. We show that ReLU type neural networks which yield a\npiecewise linear classifier function fail in this regard as they produce almost\nalways high confidence predictions far away from the training data. For bounded\ndomains like images we propose a new robust optimization technique similar to\nadversarial training which enforces low confidence predictions far away from\nthe training data. We show that this technique is surprisingly effective in\nreducing the confidence of predictions far away from the training data while\nmaintaining high confidence predictions and test error on the original\nclassification task compared to standard training.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 22:52:42 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 06:57:43 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Hein", "Matthias", ""], ["Andriushchenko", "Maksym", ""], ["Bitterwolf", "Julian", ""]]}, {"id": "1812.05736", "submitter": "Julia Peyre", "authors": "Julia Peyre, Ivan Laptev, Cordelia Schmid, Josef Sivic", "title": "Detecting unseen visual relations using analogies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to detect visual relations in images of the form of triplets t =\n(subject, predicate, object), such as \"person riding dog\", where training\nexamples of the individual entities are available but their combinations are\nunseen at training. This is an important set-up due to the combinatorial nature\nof visual relations : collecting sufficient training data for all possible\ntriplets would be very hard. The contributions of this work are three-fold.\nFirst, we learn a representation of visual relations that combines (i)\nindividual embeddings for subject, object and predicate together with (ii) a\nvisual phrase embedding that represents the relation triplet. Second, we learn\nhow to transfer visual phrase embeddings from existing training triplets to\nunseen test triplets using analogies between relations that involve similar\nobjects. Third, we demonstrate the benefits of our approach on three\nchallenging datasets : on HICO-DET, our model achieves significant improvement\nover a strong baseline for both frequent and unseen triplets, and we observe\nsimilar improvement for the retrieval of unseen triplets with out-of-vocabulary\npredicates on the COCO-a dataset as well as the challenging unusual triplets in\nthe UnRel dataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 23:56:24 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 07:37:30 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 18:09:10 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Peyre", "Julia", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""], ["Sivic", "Josef", ""]]}, {"id": "1812.05740", "submitter": "Alan Godoy", "authors": "Guilherme Folego, Filipe Costa, Bruno Costa, Alan Godoy and Luiz Pita", "title": "Pay Voice: Point of Sale Recognition for Visually Impaired People", "comments": null, "journal-ref": "XIV Workshop de Visao Computacional (WVC 2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of visually impaired people depend on relatives and friends to\nperform their everyday tasks. One relevant step towards self-sufficiency is to\nprovide them with means to verify the value and operation presented in payment\nmachines. In this work, we developed and released a smartphone application,\nnamed Pay Voice, that uses image processing, optical character recognition\n(OCR) and voice synthesis to recognize the value and operation presented in POS\nand PIN pad machines, and thus informing the user with auditive and visual\nfeedback. The proposed approach presented significant results for value and\noperation recognition, especially for POS, due to the higher display quality.\nImportantly, we achieved the key performance indicators, namely, more than 80%\nof accuracy in a real-world scenario, and less than $5$ seconds of processing\ntime for recognition. Pay Voice is publicly available on Google Play and App\nStore for free.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 00:04:32 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Folego", "Guilherme", ""], ["Costa", "Filipe", ""], ["Costa", "Bruno", ""], ["Godoy", "Alan", ""], ["Pita", "Luiz", ""]]}, {"id": "1812.05770", "submitter": "Jiagang Zhu", "authors": "Jiagang Zhu, Wei Zou, Liang Xu, Yiming Hu, Zheng Zhu, Manyu Chang,\n  Junjie Huang, Guan Huang, Dalong Du", "title": "Action Machine: Rethinking Action Recognition in Trimmed Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods in video action recognition mostly do not distinguish human\nbody from the environment and easily overfit the scenes and objects. In this\nwork, we present a conceptually simple, general and high-performance framework\nfor action recognition in trimmed videos, aiming at person-centric modeling.\nThe method, called Action Machine, takes as inputs the videos cropped by person\nbounding boxes. It extends the Inflated 3D ConvNet (I3D) by adding a branch for\nhuman pose estimation and a 2D CNN for pose-based action recognition, being\nfast to train and test. Action Machine can benefit from the multi-task training\nof action recognition and pose estimation, the fusion of predictions from RGB\nimages and poses. On NTU RGB-D, Action Machine achieves the state-of-the-art\nperformance with top-1 accuracies of 97.2% and 94.3% on cross-view and\ncross-subject respectively. Action Machine also achieves competitive\nperformance on another three smaller action recognition datasets: Northwestern\nUCLA Multiview Action3D, MSR Daily Activity3D and UTD-MHAD. Code will be made\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 03:43:54 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 08:12:06 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Zhu", "Jiagang", ""], ["Zou", "Wei", ""], ["Xu", "Liang", ""], ["Hu", "Yiming", ""], ["Zhu", "Zheng", ""], ["Chang", "Manyu", ""], ["Huang", "Junjie", ""], ["Huang", "Guan", ""], ["Du", "Dalong", ""]]}, {"id": "1812.05784", "submitter": "Alex Lang", "authors": "Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang,\n  Oscar Beijbom", "title": "PointPillars: Fast Encoders for Object Detection from Point Clouds", "comments": "9 pages. v1 is initial submission to CVPR 2019. v2 is final version\n  accepted for publication at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in point clouds is an important aspect of many robotics\napplications such as autonomous driving. In this paper we consider the problem\nof encoding a point cloud into a format appropriate for a downstream detection\npipeline. Recent literature suggests two types of encoders; fixed encoders tend\nto be fast but sacrifice accuracy, while encoders that are learned from data\nare more accurate, but slower. In this work we propose PointPillars, a novel\nencoder which utilizes PointNets to learn a representation of point clouds\norganized in vertical columns (pillars). While the encoded features can be used\nwith any standard 2D convolutional detection architecture, we further propose a\nlean downstream network. Extensive experimentation shows that PointPillars\noutperforms previous encoders with respect to both speed and accuracy by a\nlarge margin. Despite only using lidar, our full detection pipeline\nsignificantly outperforms the state of the art, even among fusion methods, with\nrespect to both the 3D and bird's eye view KITTI benchmarks. This detection\nperformance is achieved while running at 62 Hz: a 2 - 4 fold runtime\nimprovement. A faster version of our method matches the state of the art at 105\nHz. These benchmarks suggest that PointPillars is an appropriate encoding for\nobject detection in point clouds.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 05:15:08 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 02:00:24 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Lang", "Alex H.", ""], ["Vora", "Sourabh", ""], ["Caesar", "Holger", ""], ["Zhou", "Lubing", ""], ["Yang", "Jiong", ""], ["Beijbom", "Oscar", ""]]}, {"id": "1812.05785", "submitter": "Menglin Wang", "authors": "Menglin Wang, Baisheng Lai, Zhongming Jin, Xiaojin Gong, Jianqiang\n  Huang, Xiansheng Hua", "title": "Deep Active Learning for Video-based Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is prohibitively expensive to annotate a large-scale video-based person\nre-identification (re-ID) dataset, which makes fully supervised methods\ninapplicable to real-world deployment. How to maximally reduce the annotation\ncost while retaining the re-ID performance becomes an interesting problem. In\nthis paper, we address this problem by integrating an active learning scheme\ninto a deep learning framework. Noticing that the truly matched tracklet-pairs,\nalso denoted as true positives (TP), are the most informative samples for our\nre-ID model, we propose a sampling criterion to choose the most TP-likely\ntracklet-pairs for annotation. A view-aware sampling strategy considering\nview-specific biases is designed to facilitate candidate selection, followed by\nan adaptive resampling step to leave out the selected candidates that are\nunnecessary to annotate. Our method learns the re-ID model and updates the\nannotation set iteratively. The re-ID model is supervised by the tracklets'\npesudo labels that are initialized by treating each tracklet as a distinct\nclass. With the gained annotations of the actively selected candidates, the\ntracklets' pesudo labels are updated by label merging and further used to\nre-train our re-ID model. While being simple, the proposed method demonstrates\nits effectiveness on three video-based person re-ID datasets. Experimental\nresults show that less than 3\\% pairwise annotations are needed for our method\nto reach comparable performance with the fully-supervised setting.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 05:16:03 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Wang", "Menglin", ""], ["Lai", "Baisheng", ""], ["Jin", "Zhongming", ""], ["Gong", "Xiaojin", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xiansheng", ""]]}, {"id": "1812.05788", "submitter": "Chen Ma", "authors": "Chen Ma, Li Chen, Junhai Yong", "title": "AU R-CNN: Encoding Expert Prior Knowledge into R-CNN for Action Unit\n  Detection", "comments": "14 pages,10 figures, published on Neurocomputing", "journal-ref": "Neurocomputing 355 (2019) 35-47", "doi": "10.1016/j.neucom.2019.03.082", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting action units (AUs) on human faces is challenging because various\nAUs make subtle facial appearance change over various regions at different\nscales. Current works have attempted to recognize AUs by emphasizing important\nregions. However, the incorporation of expert prior knowledge into region\ndefinition remains under-exploited, and current AU detection approaches do not\nuse regional convolutional neural networks (R-CNN) with expert prior knowledge\nto directly focus on AU-related regions adaptively. By incorporating expert\nprior knowledge, we propose a novel R-CNN based model named AU R-CNN. The\nproposed solution offers two main contributions: (1) AU R-CNN directly observes\ndifferent facial regions, where various AUs are located. Specifically, we\ndefine an AU partition rule which encodes the expert prior knowledge into the\nregion definition and RoI-level label definition. This design produces\nconsiderably better detection performance than existing approaches. (2) We\nintegrate various dynamic models (including convolutional long short-term\nmemory, two stream network, conditional random field, and temporal action\nlocalization network) into AU R-CNN and then investigate and analyze the reason\nbehind the performance of dynamic models. Experiment results demonstrate that\n\\textit{only} static RGB image information and no optical flow-based AU R-CNN\nsurpasses the one fused with dynamic models. AU R-CNN is also superior to\ntraditional CNNs that use the same backbone on varying image resolutions.\nState-of-the-art recognition performance of AU detection is achieved. The\ncomplete network is end-to-end trainable. Experiments on BP4D and DISFA\ndatasets show the effectiveness of our approach. The implementation code is\navailable online.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 05:23:49 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 11:56:47 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ma", "Chen", ""], ["Chen", "Li", ""], ["Yong", "Junhai", ""]]}, {"id": "1812.05802", "submitter": "Xin Yang", "authors": "Cheng Bian, Xin Yang, Jianqiang Ma, Shen Zheng, Yu-An Liu, Reza\n  Nezafat, Pheng-Ann Heng, and Yefeng Zheng", "title": "Pyramid Network with Online Hard Example Mining for Accurate Left Atrium\n  Segmentation", "comments": "9 pages, 4 figures. MICCAI Workshop on STACOM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately segmenting left atrium in MR volume can benefit the ablation\nprocedure of atrial fibrillation. Traditional automated solutions often fail in\nrelieving experts from the labor-intensive manual labeling. In this paper, we\npropose a deep neural network based solution for automated left atrium\nsegmentation in gadolinium-enhanced MR volumes with promising performance. We\nfirstly argue that, for this volumetric segmentation task, networks in 2D\nfashion can present great superiorities in time efficiency and segmentation\naccuracy than networks with 3D fashion. Considering the highly varying shape of\natrium and the branchy structure of associated pulmonary veins, we propose to\nadopt a pyramid module to collect semantic cues in feature maps from multiple\nscales for fine-grained segmentation. Also, to promote our network in\nclassifying the hard examples, we propose an Online Hard Negative Example\nMining strategy to identify voxels in slices with low classification\ncertainties and penalize the wrong predictions on them. Finally, we devise a\ncompetitive training scheme to further boost the generalization ability of\nnetworks. Extensively verified on 20 testing volumes, our proposed framework\nachieves an average Dice of 92.83% in segmenting the left atria and pulmonary\nveins.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 07:28:07 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Bian", "Cheng", ""], ["Yang", "Xin", ""], ["Ma", "Jianqiang", ""], ["Zheng", "Shen", ""], ["Liu", "Yu-An", ""], ["Nezafat", "Reza", ""], ["Heng", "Pheng-Ann", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1812.05806", "submitter": "Yifan Xing", "authors": "Yifan Xing, Rahul Tewari, Paulo R. S. Mendonca", "title": "A Self-Supervised Bootstrap Method for Single-Image 3D Face\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for 3D reconstruction of faces from a single image\nrequire 2D-3D pairs of ground-truth data for supervision. Such data is costly\nto acquire, and most datasets available in the literature are restricted to\npairs for which the input 2D images depict faces in a near fronto-parallel\npose. Therefore, many data-driven methods for single-image 3D facial\nreconstruction perform poorly on profile and near-profile faces. We propose a\nmethod to improve the performance of single-image 3D facial reconstruction\nnetworks by utilizing the network to synthesize its own training data for\nfine-tuning, comprising: (i) single-image 3D reconstruction of faces in\nnear-frontal images without ground-truth 3D shape; (ii) application of a\nrigid-body transformation to the reconstructed face model; (iii) rendering of\nthe face model from new viewpoints; and (iv) use of the rendered image and\ncorresponding 3D reconstruction as additional data for supervised fine-tuning.\nThe new 2D-3D pairs thus produced have the same high-quality observed for near\nfronto-parallel reconstructions, thereby nudging the network towards more\nuniform performance as a function of the viewing angle of input faces.\nApplication of the proposed technique to the fine-tuning of a state-of-the-art\nsingle-image 3D-reconstruction network for faces demonstrates the usefulness of\nthe method, with particularly significant gains for profile or near-profile\nviews.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 07:46:02 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 19:22:22 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Xing", "Yifan", ""], ["Tewari", "Rahul", ""], ["Mendonca", "Paulo R. S.", ""]]}, {"id": "1812.05807", "submitter": "Xin Yang", "authors": "Xin Yang, Na Wang, Yi Wang, Xu Wang, Reza Nezafat, Dong Ni, Pheng-Ann\n  Heng", "title": "Combating Uncertainty with Novel Losses for Automatic Left Atrium\n  Segmentation", "comments": "9 pages, 4 figures. MICCAI Workshop on STACOM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting left atrium in MR volume holds great potentials in promoting the\ntreatment of atrial fibrillation. However, the varying anatomies, artifacts and\nlow contrasts among tissues hinder the advance of both manual and automated\nsolutions. In this paper, we propose a fully-automated framework to segment\nleft atrium in gadolinium-enhanced MR volumes. The region of left atrium is\nfirstly automatically localized by a detection module. Our framework then\noriginates with a customized 3D deep neural network to fully explore the\nspatial dependency in the region for segmentation. To alleviate the risk of low\ntraining efficiency and potential overfitting, we enhance our deep network with\nthe transfer learning and deep supervision strategy. Main contribution of our\nnetwork design lies in the composite loss function to combat the boundary\nambiguity and hard examples. We firstly adopt the Overlap loss to encourage\nnetwork reduce the overlap between the foreground and background and thus\nsharpen the predictions on boundary. We then propose a novel Focal Positive\nloss to guide the learning of voxel-specific threshold and emphasize the\nforeground to improve classification sensitivity. Further improvement is\nobtained with an recursive training scheme. With ablation studies, all the\nintroduced modules prove to be effective. The proposed framework achieves an\naverage Dice of 92.24 in segmenting left atrium with pulmonary veins on 20\ntesting volumes.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 07:47:10 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Yang", "Xin", ""], ["Wang", "Na", ""], ["Wang", "Yi", ""], ["Wang", "Xu", ""], ["Nezafat", "Reza", ""], ["Ni", "Dong", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1812.05824", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan and Shijian Lu", "title": "ESIR: End-to-end Scene Text Recognition via Iterative Image\n  Rectification", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated recognition of texts in scenes has been a research challenge for\nyears, largely due to the arbitrary variation of text appearances in\nperspective distortion, text line curvature, text styles and different types of\nimaging artifacts. The recent deep networks are capable of learning robust\nrepresentations with respect to imaging artifacts and text style changes, but\nstill face various problems while dealing with scene texts with perspective and\ncurvature distortions. This paper presents an end-to-end trainable scene text\nrecognition system (ESIR) that iteratively removes perspective distortion and\ntext line curvature as driven by better scene text recognition performance. An\ninnovative rectification network is developed which employs a novel\nline-fitting transformation to estimate the pose of text lines in scenes. In\naddition, an iterative rectification pipeline is developed where scene text\ndistortions are corrected iteratively towards a fronto-parallel view. The ESIR\nis also robust to parameter initialization and the training needs only scene\ntext images and word-level annotations as required by most scene text\nrecognition systems. Extensive experiments over a number of public datasets\nshow that the proposed ESIR is capable of rectifying scene text distortions\naccurately, achieving superior recognition performance for both normal scene\ntext images and those suffering from perspective and curvature distortions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 08:32:36 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 12:33:00 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 09:13:15 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhan", "Fangneng", ""], ["Lu", "Shijian", ""]]}, {"id": "1812.05831", "submitter": "Guido Borghi", "authors": "Guido Borghi", "title": "Combining Deep and Depth: Deep Learning and Face Depth Maps for Driver\n  Attention Monitoring", "comments": "White paper about my research activity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approaches have achieved promising results in various\nfields of computer vision. In this paper, we investigate the combination of\ndeep learning based methods and depth maps as input images to tackle the\nproblem of driver attention monitoring. Moreover, we assume the concept of\nattention as Head Pose Estimation and Facial Landmark Detection tasks.\nDifferently from other proposals in the literature, the proposed systems are\nable to work directly and based only on raw depth data. All presented methods\nare trained and tested on two new public datasets, namely Pandora and\nMotorMark, achieving state-of-art results and running with real time\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 09:10:17 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Borghi", "Guido", ""]]}, {"id": "1812.05836", "submitter": "Martin Mundt", "authors": "Martin Mundt, Sagnik Majumder, Tobias Weis, Visvanathan Ramesh", "title": "Rethinking Layer-wise Feature Amounts in Convolutional Neural Network\n  Architectures", "comments": "Accepted at the Critiquing and Correcting Trends in Machine Learning\n  (CRACT) Workshop at the 32nd Conference on Neural Information Processing\n  Systems (NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize convolutional neural networks with respect to the relative\namount of features per layer. Using a skew normal distribution as a\nparametrized framework, we investigate the common assumption of monotonously\nincreasing feature-counts with higher layers of architecture designs. Our\nevaluation on models with VGG-type layers on the MNIST, Fashion-MNIST and\nCIFAR-10 image classification benchmarks provides evidence that motivates\nrethinking of our common assumption: architectures that favor larger early\nlayers seem to yield better accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 09:28:05 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Mundt", "Martin", ""], ["Majumder", "Sagnik", ""], ["Weis", "Tobias", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "1812.05840", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Hongyuan Zhu and Shijian Lu", "title": "Spatial Fusion GAN for Image Synthesis", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in generative adversarial networks (GANs) have shown great\npotentials in realistic image synthesis whereas most existing works address\nsynthesis realism in either appearance space or geometry space but few in both.\nThis paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a\ngeometry synthesizer and an appearance synthesizer to achieve synthesis realism\nin both geometry and appearance spaces. The geometry synthesizer learns\ncontextual geometries of background images and transforms and places foreground\nobjects into the background images unanimously. The appearance synthesizer\nadjusts the color, brightness and styles of the foreground objects and embeds\nthem into background images harmoniously, where a guided filter is introduced\nfor detail preserving. The two synthesizers are inter-connected as mutual\nreferences which can be trained end-to-end without supervision. The SF-GAN has\nbeen evaluated in two tasks: (1) realistic scene text image synthesis for\ntraining better recognition models; (2) glass and hat wearing for realistic\nmatching glasses and hats with real portraits. Qualitative and quantitative\ncomparisons with the state-of-the-art demonstrate the superiority of the\nproposed SF-GAN.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 09:38:07 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 12:19:22 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 08:58:47 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhan", "Fangneng", ""], ["Zhu", "Hongyuan", ""], ["Lu", "Shijian", ""]]}, {"id": "1812.05841", "submitter": "Emilie Wirbel", "authors": "Laurent George, Thibault Buhet, Emilie Wirbel, Gaetan Le-Gall, Xavier\n  Perrotton", "title": "Imitation Learning for End to End Vehicle Longitudinal Control with\n  Forward Camera", "comments": "NeurIps 2018 Imitation Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a complete study of an end-to-end imitation learning\nsystem for speed control of a real car, based on a neural network with a Long\nShort Term Memory (LSTM). To achieve robustness and generalization from expert\ndemonstrations, we propose data augmentation and label augmentation that are\nrelevant for imitation learning in longitudinal control context. Based on front\ncamera image only, our system is able to correctly control the speed of a car\nin simulation environment, and in a real car on a challenging test track. The\nsystem also shows promising results in open road context.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 09:45:34 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["George", "Laurent", ""], ["Buhet", "Thibault", ""], ["Wirbel", "Emilie", ""], ["Le-Gall", "Gaetan", ""], ["Perrotton", "Xavier", ""]]}, {"id": "1812.05850", "submitter": "Hasan Ates", "authors": "Hasan F. Ates, Sercan Sunetci", "title": "Multi-hypothesis contextual modeling for semantic segmentation", "comments": "8 pages and 3 figure, accepted to Pattern Recognition Letters,\n  Elsevier", "journal-ref": "Pattern Recog. Letters, 117 (2019) 104-110", "doi": "10.1016/j.patrec.2018.12.011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation (i.e. image parsing) aims to annotate each image pixel\nwith its corresponding semantic class label. Spatially consistent labeling of\nthe image requires an accurate description and modeling of the local contextual\ninformation. Segmentation result is typically improved by Markov Random Field\n(MRF) optimization on the initial labels. However this improvement is limited\nby the accuracy of initial result and how the contextual neighborhood is\ndefined. In this paper, we develop generalized and flexible contextual models\nfor segmentation neighborhoods in order to improve parsing accuracy. Instead of\nusing a fixed segmentation and neighborhood definition, we explore various\ncontextual models for fusion of complementary information available in\nalternative segmentations of the same image. In other words, we propose a novel\nMRF framework that describes and optimizes the contextual dependencies between\nmultiple segmentations. Simulation results on two common datasets demonstrate\nsignificant improvement in parsing accuracy over the baseline approaches.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 10:33:36 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Ates", "Hasan F.", ""], ["Sunetci", "Sercan", ""]]}, {"id": "1812.05869", "submitter": "Dmitrii Lachinov", "authors": "Dmitry Lachinov and Vadim Turlapov", "title": "The Coherent Point Drift for Clustered Point Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of non-rigid point set registration is a key problem for many\ncomputer vision tasks. In many cases the nature of the data or capabilities of\nthe point detection algorithms can give us some prior information on point sets\ndistribution. In non-rigid case this information is able to drastically improve\nregistration results by limiting number of possible solutions. In this paper we\nexplore use of prior information about point sets clustering, such information\ncan be obtained with preliminary segmentation. We extend existing probabilistic\nframework for fitting two level Gaussian mixture model and derive closed form\nsolution for maximization step of the EM algorithm. This enables us to improve\nmethod accuracy with almost no performance loss. We evaluate our approach and\ncompare the Cluster Coherent Point Drift with other existing non-rigid point\nset registration methods and show it's advantages for digital medicine tasks,\nespecially for heart template model personalization using patient's medical\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 11:52:21 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Lachinov", "Dmitry", ""], ["Turlapov", "Vadim", ""]]}, {"id": "1812.05914", "submitter": "Wenhui Zhang", "authors": "Wenhui Zhang, Tejas Mahale", "title": "End to End Video Segmentation for Driving : Lane Detection For\n  Autonomous Car", "comments": "arXiv admin note: text overlap with arXiv:1806.07226 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Safety and decline of road traffic accidents remain important issues of\nautonomous driving. Statistics show that unintended lane departure is a leading\ncause of worldwide motor vehicle collisions, making lane detection the most\npromising and challenge task for self-driving. Today, numerous groups are\ncombining deep learning techniques with computer vision problems to solve\nself-driving problems. In this paper, a Global Convolution Networks (GCN) model\nis used to address both classification and localization issues for semantic\nsegmentation of lane. We are using color-based segmentation is presented and\nthe usability of the model is evaluated. A residual-based boundary refinement\nand Adam optimization is also used to achieve state-of-art performance. As\nnormal cars could not afford GPUs on the car, and training session for a\nparticular road could be shared by several cars. We propose a framework to get\nit work in real world. We build a real time video transfer system to get video\nfrom the car, get the model trained in edge server (which is equipped with\nGPUs), and send the trained model back to the car.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 01:17:57 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Zhang", "Wenhui", ""], ["Mahale", "Tejas", ""]]}, {"id": "1812.05917", "submitter": "Junnan Li Mr", "authors": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli", "title": "Visual Social Relationship Recognition", "comments": "arXiv admin note: text overlap with arXiv:1708.00634", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social relationships form the basis of social structure of humans. Developing\ncomputational models to understand social relationships from visual data is\nessential for building intelligent machines that can better interact with\nhumans in a social environment. In this work, we study the problem of visual\nsocial relationship recognition in images. We propose a Dual-Glance model for\nsocial relationship recognition, where the first glance fixates at the person\nof interest and the second glance deploys attention mechanism to exploit\ncontextual cues. To enable this study, we curated a large scale People in\nSocial Context (PISC) dataset, which comprises of 23,311 images and 79,244\nperson pairs with annotated social relationships. Since visually identifying\nsocial relationship bears certain degree of uncertainty, we further propose an\nAdaptive Focal Loss to leverage the ambiguous annotations for more effective\nlearning. We conduct extensive experiments to quantitatively and qualitatively\ndemonstrate the efficacy of our proposed method, which yields state-of-the-art\nperformance on social relationship recognition.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 08:18:52 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Li", "Junnan", ""], ["Wong", "Yongkang", ""], ["Zhao", "Qi", ""], ["Kankanhalli", "Mohan S.", ""]]}, {"id": "1812.06023", "submitter": "Farzad Toutounchi", "authors": "Farzad Toutounchi, Ebroul Izquierdo", "title": "Advanced Super-Resolution using Lossless Pooling Convolutional Networks", "comments": "Accepted paper: 2019 IEEE Winter Conference on Applications of\n  Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel deep learning-based approach for still\nimage super-resolution, that unlike the mainstream models does not rely solely\non the input low resolution image for high quality upsampling, and takes\nadvantage of a set of artificially created auxiliary self-replicas of the input\nimage that are incorporated in the neural network to create an enhanced and\naccurate upscaling scheme. Inclusion of the proposed lossless pooling layers,\nand the fusion of the input self-replicas enable the model to exploit the high\ncorrelation between multiple instances of the same content, and eventually\nresult in significant improvements in the quality of the super-resolution,\nwhich is confirmed by extensive evaluations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 16:58:02 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Toutounchi", "Farzad", ""], ["Izquierdo", "Ebroul", ""]]}, {"id": "1812.06024", "submitter": "Vincent Casser", "authors": "Vincent Casser, Kai Kang, Hanspeter Pfister, Daniel Haehn", "title": "Fast Mitochondria Detection for Connectomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution connectomics data allows for the identification of\ndysfunctional mitochondria which are linked to a variety of diseases such as\nautism or bipolar. However, manual analysis is not feasible since datasets can\nbe petabytes in size. We present a fully automatic mitochondria detector based\non a modified U-Net architecture that yields high accuracy and fast processing\ntimes. We evaluate our method on multiple real-world connectomics datasets,\nincluding an improved version of the EPFL mitochondria benchmark. Our results\nshow an Jaccard index of up to 0.90 with inference times lower than 16ms for a\n512x512px image tile. This speed is faster than the acquisition speed of modern\nelectron microscopes, enabling mitochondria detection in real-time. Our\ndetector ranks first for real-time detection when compared to previous works\nand data, results, and code are openly available.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 16:58:05 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 04:25:23 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Casser", "Vincent", ""], ["Kang", "Kai", ""], ["Pfister", "Hanspeter", ""], ["Haehn", "Daniel", ""]]}, {"id": "1812.06061", "submitter": "Ariel Curiale A.H.C", "authors": "Ariel H. Curiale, Flavio D. Colavecchia, German Mato", "title": "Automatic quantification of the LV function and mass: a deep learning\n  approach for cardiovascular MRI", "comments": "Accepted in Computer Methods and Programs in Biomedicine.\n  https://www.sciencedirect.com/science/article/pii/S0169260718311696?via%3Dihub", "journal-ref": null, "doi": "10.1016/j.cmpb.2018.12.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: This paper proposes a novel approach for automatic left ventricle\n(LV) quantification using convolutional neural networks (CNN).\n  Methods: The general framework consists of one CNN for detecting the LV, and\nanother for tissue classification. Also, three new deep learning architectures\nwere proposed for LV quantification. These new CNNs introduce the ideas of\nsparsity and depthwise separable convolution into the U-net architecture, as\nwell as, a residual learning strategy level-to-level. To this end, we extend\nthe classical U-net architecture and use the generalized Jaccard distance as\noptimization objective function.\n  Results: The CNNs were trained and evaluated with 140 patients from two\npublic cardiovascular magnetic resonance datasets (Sunnybrook and Cardiac Atlas\nProject) by using a 5-fold cross-validation strategy. Our results demonstrate a\nsuitable accuracy for myocardial segmentation ($\\sim$0.9 Dice's coefficient),\nand a strong correlation with the most relevant physiological measures: 0.99\nfor end-diastolic and end-systolic volume, 0.97 for the left myocardial mass,\n0.95 for the ejection fraction and 0.93 for the stroke volume and cardiac\noutput.\n  Conclusion: Our simulation and clinical evaluation results demonstrate the\ncapability and merits of the proposed CNN to estimate different structural and\nfunctional features such as LV mass and EF which are commonly used for both\ndiagnosis and treatment of different pathologies.\n  Significance: This paper suggests a new approach for automatic LV\nquantification based on deep learning where errors are comparable to the inter-\nand intra-operator ranges for manual contouring. Also, this approach may have\nimportant applications on motion quantification.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 18:20:39 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Curiale", "Ariel H.", ""], ["Colavecchia", "Flavio D.", ""], ["Mato", "German", ""]]}, {"id": "1812.06071", "submitter": "Naji Khosravan", "authors": "Naji Khosravan, Shervin Ardeshir, Rohit Puri", "title": "On Attention Modules for Audio-Visual Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of media and networking technologies, multimedia\napplications ranging from feature presentation in a cinema setting to video on\ndemand to interactive video conferencing are in great demand. Good\nsynchronization between audio and video modalities is a key factor towards\ndefining the quality of a multimedia presentation. The audio and visual signals\nof a multimedia presentation are commonly managed by independent workflows -\nthey are often separately authored, processed, stored and even delivered to the\nplayback system. This opens up the possibility of temporal misalignment between\nthe two modalities - such a tendency is often more pronounced in the case of\nproduced content (such as movies).\n  To judge whether audio and video signals of a multimedia presentation are\nsynchronized, we as humans often pay close attention to discriminative\nspatio-temporal blocks of the video (e.g. synchronizing the lip movement with\nthe utterance of words, or the sound of a bouncing ball at the moment it hits\nthe ground). At the same time, we ignore large portions of the video in which\nno discriminative sounds exist (e.g. background music playing in a movie).\nInspired by this observation, we study leveraging attention modules for\nautomatically detecting audio-visual synchronization. We propose neural network\nbased attention modules, capable of weighting different portions\n(spatio-temporal blocks) of the video based on their respective discriminative\npower. Our experiments indicate that incorporating attention modules yields\nstate-of-the-art results for the audio-visual synchronization classification\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 18:37:12 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Khosravan", "Naji", ""], ["Ardeshir", "Shervin", ""], ["Puri", "Rohit", ""]]}, {"id": "1812.06125", "submitter": "Guoan Zheng", "authors": "Shaowei Jiang, Jun Liao, Zichao Bian, Pengming Song, Garrett Soler,\n  Kazunori Hoshino, and Guoan Zheng", "title": "Axially-shifted pattern illumination for macroscale turbidity\n  suppression and virtual volumetric confocal imaging without axial scanning", "comments": null, "journal-ref": "Optics Letters, 44(4), (2019)", "doi": "10.1364/OL.44.000811", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured illumination has been widely used for optical sectioning and 3D\nsurface recovery. In a typical implementation, multiple images under\nnon-uniform pattern illumination are used to recover a single object section.\nAxial scanning of the sample or the objective lens is needed for acquiring the\n3D volumetric data. Here we demonstrate the use of axially-shifted pattern\nillumination (asPI) for virtual volumetric confocal imaging without axial\nscanning. In the reported approach, we project illumination patterns at a\ntilted angle with respect to the detection optics. As such, the illumination\npatterns shift laterally at different z sections and the sample information at\ndifferent z-sections can be recovered based on the captured 2D images. We\ndemonstrate the reported approach for virtual confocal imaging through a\ndiffusing layer and underwater 3D imaging through diluted milk. We show that we\ncan acquire the entire confocal volume in ~1s with a throughput of 420\nmegapixels per second. Our approach may provide new insights for developing\nconfocal light ranging and detection systems in degraded visual environments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 19:25:40 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Jiang", "Shaowei", ""], ["Liao", "Jun", ""], ["Bian", "Zichao", ""], ["Song", "Pengming", ""], ["Soler", "Garrett", ""], ["Hoshino", "Kazunori", ""], ["Zheng", "Guoan", ""]]}, {"id": "1812.06145", "submitter": "Mahdi Abavisani", "authors": "Mahdi Abavisani, Hamid Reza Vaezi Joze, Vishal M. Patel", "title": "Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition\n  with Multimodal Training", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019, pp. 1165-1174", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an efficient approach for leveraging the knowledge from multiple\nmodalities in training unimodal 3D convolutional neural networks (3D-CNNs) for\nthe task of dynamic hand gesture recognition. Instead of explicitly combining\nmultimodal information, which is commonplace in many state-of-the-art methods,\nwe propose a different framework in which we embed the knowledge of multiple\nmodalities in individual networks so that each unimodal network can achieve an\nimproved performance. In particular, we dedicate separate networks per\navailable modality and enforce them to collaborate and learn to develop\nnetworks with common semantics and better representations. We introduce a\n\"spatiotemporal semantic alignment\" loss (SSA) to align the content of the\nfeatures from different networks. In addition, we regularize this loss with our\nproposed \"focal regularization parameter\" to avoid negative knowledge transfer.\nExperimental results show that our framework improves the test time recognition\naccuracy of unimodal networks, and provides the state-of-the-art performance on\nvarious dynamic hand gesture recognition datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 20:08:24 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 09:51:14 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Abavisani", "Mahdi", ""], ["Joze", "Hamid Reza Vaezi", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1812.06148", "submitter": "Heng Fan", "authors": "Heng Fan and Haibin Ling", "title": "Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region proposal networks (RPN) have been recently combined with the Siamese\nnetwork for tracking, and shown excellent accuracy with high efficiency.\nNevertheless, previously proposed one-stage Siamese-RPN trackers degenerate in\npresence of similar distractors and large scale variation. Addressing these\nissues, we propose a multi-stage tracking framework, Siamese Cascaded RPN\n(C-RPN), which consists of a sequence of RPNs cascaded from deep high-level to\nshallow low-level layers in a Siamese network. Compared to previous solutions,\nC-RPN has several advantages: (1) Each RPN is trained using the outputs of RPN\nin the previous stage. Such process stimulates hard negative sampling,\nresulting in more balanced training samples. Consequently, the RPNs are\nsequentially more discriminative in distinguishing difficult background (i.e.,\nsimilar distractors). (2) Multi-level features are fully leveraged through a\nnovel feature transfer block (FTB) for each RPN, further improving the\ndiscriminability of C-RPN using both high-level semantic and low-level spatial\ninformation. (3) With multiple steps of regressions, C-RPN progressively\nrefines the location and shape of the target in each RPN with adjusted anchor\nboxes in the previous stage, which makes localization more accurate. C-RPN is\ntrained end-to-end with the multi-task loss function. In inference, C-RPN is\ndeployed as it is, without any temporal adaption, for real-time tracking. In\nextensive experiments on OTB-2013, OTB-2015, VOT-2016, VOT-2017, LaSOT and\nTrackingNet, C-RPN consistently achieves state-of-the-art results and runs in\nreal-time.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 20:10:36 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Fan", "Heng", ""], ["Ling", "Haibin", ""]]}, {"id": "1812.06152", "submitter": "Buyu Liu", "authors": "Ziyan Wang, Buyu Liu, Samuel Schulter, Manmohan Chandraker", "title": "A Parametric Top-View Representation of Complex Road Scenes", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of inferring the layout of complex road\nscenes given a single camera as input. To achieve that, we first propose a\nnovel parameterized model of road layouts in a top-view representation, which\nis not only intuitive for human visualization but also provides an\ninterpretable interface for higher-level decision making. Moreover, the design\nof our top-view scene model allows for efficient sampling and thus generation\nof large-scale simulated data, which we leverage to train a deep neural network\nto infer our scene model's parameters. Specifically, our proposed training\nprocedure uses supervised domain-adaptation techniques to incorporate both\nsimulated as well as manually annotated data. Finally, we design a Conditional\nRandom Field (CRF) that enforces coherent predictions for a single frame and\nencourages temporal smoothness among video frames. Experiments on two public\ndata sets show that: (1) Our parametric top-view model is representative enough\nto describe complex road scenes; (2) The proposed method outperforms baselines\ntrained on manually-annotated or simulated data only, thus getting the best of\nboth; (3) Our CRF is able to generate temporally smoothed while semantically\nmeaningful results.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 20:18:38 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 19:25:38 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Wang", "Ziyan", ""], ["Liu", "Buyu", ""], ["Schulter", "Samuel", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1812.06164", "submitter": "Amaia Salvador", "authors": "Amaia Salvador, Michal Drozdzal, Xavier Giro-i-Nieto, Adriana Romero", "title": "Inverse Cooking: Recipe Generation from Food Images", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People enjoy food photography because they appreciate food. Behind each meal\nthere is a story described in a complex recipe and, unfortunately, by simply\nlooking at a food image we do not have access to its preparation process.\nTherefore, in this paper we introduce an inverse cooking system that recreates\ncooking recipes given food images. Our system predicts ingredients as sets by\nmeans of a novel architecture, modeling their dependencies without imposing any\norder, and then generates cooking instructions by attending to both image and\nits inferred ingredients simultaneously. We extensively evaluate the whole\nsystem on the large-scale Recipe1M dataset and show that (1) we improve\nperformance w.r.t. previous baselines for ingredient prediction; (2) we are\nable to obtain high quality recipes by leveraging both image and ingredients;\n(3) our system is able to produce more compelling recipes than retrieval-based\napproaches according to human judgment. We make code and models publicly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 20:59:33 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 10:56:03 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Salvador", "Amaia", ""], ["Drozdzal", "Michal", ""], ["Giro-i-Nieto", "Xavier", ""], ["Romero", "Adriana", ""]]}, {"id": "1812.06181", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li and Nicha C. Dvornek and Yuan Zhou and Juntang Zhuang and\n  Pamela Ventola and James S. Duncan", "title": "Efficient Interpretation of Deep Learning Models Using Graph Structure\n  and Cooperative Game Theory: Application to ASD Biomarker Discovery", "comments": "12 pages, 7 figures, accpeted as a full paper in IPMI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering imaging biomarkers for autism spectrum disorder (ASD) is critical\nto help explain ASD and predict or monitor treatment outcomes. Toward this end,\ndeep learning classifiers have recently been used for identifying ASD from\nfunctional magnetic resonance imaging (fMRI) with higher accuracy than\ntraditional learning strategies. However, a key challenge with deep learning\nmodels is understanding just what image features the network is using, which\ncan in turn be used to define the biomarkers. Current methods extract\nbiomarkers, i.e., important features, by looking at how the prediction changes\nif \"ignoring\" one feature at a time. In this work, we go beyond looking at only\nindividual features by using Shapley value explanation (SVE) from cooperative\ngame theory. Cooperative game theory is advantageous here because it directly\nconsiders the interaction between features and can be applied to any machine\nlearning method, making it a novel, more accurate way of determining\ninstance-wise biomarker importance from deep learning models. A barrier to\nusing SVE is its computational complexity: $2^N$ given $N$ features. We\nexplicitly reduce the complexity of SVE computation by two approaches based on\nthe underlying graph structure of the input data: 1) only consider the\ncentralized coalition of each feature; 2) a hierarchical pipeline which first\nclusters features into small communities, then applies SVE in each community.\nMonte Carlo approximation can be used for large permutation sets. We first\nvalidate our methods on the MNIST dataset and compare to human perception.\nNext, to insure plausibility of our biomarker results, we train a Random Forest\n(RF) to classify ASD/control subjects from fMRI and compare SVE results to\nstandard RF-based feature importance. Finally, we show initial results on\nranked fMRI biomarkers using SVE on a deep learning classifier for the\nASD/control dataset.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 21:50:02 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 18:38:11 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Dvornek", "Nicha C.", ""], ["Zhou", "Yuan", ""], ["Zhuang", "Juntang", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S.", ""]]}, {"id": "1812.06190", "submitter": "Jack Klys", "authors": "Jack Klys, Jake Snell, Richard Zemel", "title": "Learning Latent Subspaces in Variational Autoencoders", "comments": "Published as a conference paper at NeurIPS 2018. 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) are widely used deep generative models\ncapable of learning unsupervised latent representations of data. Such\nrepresentations are often difficult to interpret or control. We consider the\nproblem of unsupervised learning of features correlated to specific labels in a\ndataset. We propose a VAE-based generative model which we show is capable of\nextracting features correlated to binary labels in the data and structuring it\nin a latent subspace which is easy to interpret. Our model, the Conditional\nSubspace VAE (CSVAE), uses mutual information minimization to learn a\nlow-dimensional latent subspace associated with each label that can easily be\ninspected and independently manipulated. We demonstrate the utility of the\nlearned representations for attribute manipulation tasks on both the Toronto\nFace and CelebA datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 22:10:50 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Klys", "Jack", ""], ["Snell", "Jake", ""], ["Zemel", "Richard", ""]]}, {"id": "1812.06203", "submitter": "Xiyang Dai", "authors": "Xiyang Dai, Bharat Singh, Joe Yue-Hei Ng, Larry S. Davis", "title": "TAN: Temporal Aggregation Network for Dense Multi-label Action\n  Recognition", "comments": "WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Temporal Aggregation Network (TAN) which decomposes 3D\nconvolutions into spatial and temporal aggregation blocks. By stacking spatial\nand temporal convolutions repeatedly, TAN forms a deep hierarchical\nrepresentation for capturing spatio-temporal information in videos. Since we do\nnot apply 3D convolutions in each layer but only apply temporal aggregation\nblocks once after each spatial downsampling layer in the network, we\nsignificantly reduce the model complexity. The use of dilated convolutions at\ndifferent resolutions of the network helps in aggregating multi-scale\nspatio-temporal information efficiently. Experiments show that our model is\nwell suited for dense multi-label action recognition, which is a challenging\nsub-topic of action recognition that requires predicting multiple action labels\nin each frame. We outperform state-of-the-art methods by 5% and 3% on the\nCharades and Multi-THUMOS dataset respectively.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 23:28:39 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Dai", "Xiyang", ""], ["Singh", "Bharat", ""], ["Ng", "Joe Yue-Hei", ""], ["Davis", "Larry S.", ""]]}, {"id": "1812.06216", "submitter": "Sebastian Koch", "authors": "Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams,\n  Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo", "title": "ABC: A Big CAD Model Dataset For Geometric Deep Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ABC-Dataset, a collection of one million Computer-Aided Design\n(CAD) models for research of geometric deep learning methods and applications.\nEach model is a collection of explicitly parametrized curves and surfaces,\nproviding ground truth for differential quantities, patch segmentation,\ngeometric feature detection, and shape reconstruction. Sampling the parametric\ndescriptions of surfaces and curves allows generating data in different formats\nand resolutions, enabling fair comparisons for a wide range of geometric\nlearning algorithms. As a use case for our dataset, we perform a large-scale\nbenchmark for estimation of surface normals, comparing existing data driven\nmethods and evaluating their performance against both the ground truth and\ntraditional normal estimation methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 01:21:48 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 07:18:44 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Koch", "Sebastian", ""], ["Matveev", "Albert", ""], ["Jiang", "Zhongshi", ""], ["Williams", "Francis", ""], ["Artemov", "Alexey", ""], ["Burnaev", "Evgeny", ""], ["Alexa", "Marc", ""], ["Zorin", "Denis", ""], ["Panozzo", "Daniele", ""]]}, {"id": "1812.06220", "submitter": "Haiyong Chen", "authors": "Haiyong Chen, Yue Pang, Qidi Hu, Kun Liu", "title": "Solar Cell Surface Defect Inspection Based on Multispectral\n  Convolutional Neural Network", "comments": "14 pages, 7 figures,14 tables", "journal-ref": null, "doi": "10.1007/s10845-018-1458-z", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similar and indeterminate defect detection of solar cell surface with\nheterogeneous texture and complex background is a challenge of solar cell\nmanufacturing. The traditional manufacturing process relies on human eye\ndetection which requires a large number of workers without a stable and good\ndetection effect. In order to solve the problem, a visual defect detection\nmethod based on multi-spectral deep convolutional neural network (CNN) is\ndesigned in this paper. Firstly, a selected CNN model is established. By\nadjusting the depth and width of the model, the influence of model depth and\nkernel size on the recognition result is evaluated. The optimal CNN model\nstructure is selected. Secondly, the light spectrum features of solar cell\ncolor image are analyzed. It is found that a variety of defects exhibited\ndifferent distinguishable characteristics in different spectral bands. Thus, a\nmulti-spectral CNN model is constructed to enhance the discrimination ability\nof the model to distinguish between complex texture background features and\ndefect features. Finally, some experimental results and K-fold cross validation\nshow that the multi-spectral deep CNN model can effectively detect the solar\ncell surface defects with higher accuracy and greater adaptability. The\naccuracy of defect recognition reaches 94.30%. Applying such an algorithm can\nincrease the efficiency of solar cell manufacturing and make the manufacturing\nprocess smarter.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 02:14:18 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Chen", "Haiyong", ""], ["Pang", "Yue", ""], ["Hu", "Qidi", ""], ["Liu", "Kun", ""]]}, {"id": "1812.06224", "submitter": "Isha Garg", "authors": "Isha Garg, Priyadarshini Panda and Kaushik Roy", "title": "A Low Effort Approach to Structured CNN Design Using PCA", "comments": "To be Published in IEEE Access, Volume 8, 2020", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2961960", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models hold state of the art performance in many fields, yet\ntheir design is still based on heuristics or grid search methods that often\nresult in overparametrized networks. This work proposes a method to analyze a\ntrained network and deduce an optimized, compressed architecture that preserves\naccuracy while keeping computational costs tractable. Model compression is an\nactive field of research that targets the problem of realizing deep learning\nmodels in hardware. However, most pruning methodologies tend to be\nexperimental, requiring large compute and time intensive iterations of\nretraining the entire network. We introduce structure into model design by\nproposing a single shot analysis of a trained network that serves as a first\norder, low effort approach to dimensionality reduction, by using PCA (Principal\nComponent Analysis). The proposed method simultaneously analyzes the\nactivations of each layer and considers the dimensionality of the space\ndescribed by the filters generating these activations. It optimizes the\narchitecture in terms of number of layers, and number of filters per layer\nwithout any iterative retraining procedures, making it a viable, low effort\ntechnique to design efficient networks. We demonstrate the proposed methodology\non AlexNet and VGG style networks on the CIFAR-10, CIFAR-100 and ImageNet\ndatasets, and successfully achieve an optimized architecture with a reduction\nof up to 3.8X and 9X in the number of operations and parameters respectively,\nwhile trading off less than 1% accuracy. We also apply the method to MobileNet,\nand achieve 1.7X and 3.9X reduction in the number of operations and parameters\nrespectively, while improving accuracy by almost one percentage point.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 02:41:09 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 02:07:16 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 19:36:39 GMT"}, {"version": "v4", "created": "Fri, 10 Jan 2020 09:36:18 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Garg", "Isha", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1812.06228", "submitter": "Liantao Wang", "authors": "Liantao Wang, Qingwu Li, Jianfeng Lu", "title": "Weakly supervised segment annotation via expectation kernel density\n  estimation", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the labelling for the positive images/videos is ambiguous in weakly\nsupervised segment annotation, negative mining based methods that only use the\nintra-class information emerge. In these methods, negative instances are\nutilized to penalize unknown instances to rank their likelihood of being an\nobject, which can be considered as a voting in terms of similarity. However,\nthese methods 1) ignore the information contained in positive bags, 2) only\nrank the likelihood but cannot generate an explicit decision function. In this\npaper, we propose a voting scheme involving not only the definite negative\ninstances but also the ambiguous positive instances to make use of the extra\nuseful information in the weakly labelled positive bags. In the scheme, each\ninstance votes for its label with a magnitude arising from the similarity, and\nthe ambiguous positive instances are assigned soft labels that are iteratively\nupdated during the voting. It overcomes the limitations of voting using only\nthe negative bags. We also propose an expectation kernel density estimation\n(eKDE) algorithm to gain further insight into the voting mechanism.\nExperimental results demonstrate the superiority of our scheme beyond the\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 03:31:32 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Wang", "Liantao", ""], ["Li", "Qingwu", ""], ["Lu", "Jianfeng", ""]]}, {"id": "1812.06264", "submitter": "Zhichao Yin", "authors": "Zhichao Yin, Trevor Darrell, Fisher Yu", "title": "Hierarchical Discrete Distribution Decomposition for Match Density\n  Estimation", "comments": "To appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicit representations of the global match distributions of pixel-wise\ncorrespondences between pairs of images are desirable for uncertainty\nestimation and downstream applications. However, the computation of the match\ndensity for each pixel may be prohibitively expensive due to the large number\nof candidates. In this paper, we propose Hierarchical Discrete Distribution\nDecomposition (HD^3), a framework suitable for learning probabilistic pixel\ncorrespondences in both optical flow and stereo matching. We decompose the full\nmatch density into multiple scales hierarchically, and estimate the local\nmatching distributions at each scale conditioned on the matching and warping at\ncoarser scales. The local distributions can then be composed together to form\nthe global match density. Despite its simplicity, our probabilistic method\nachieves state-of-the-art results for both optical flow and stereo matching on\nestablished benchmarks. We also find the estimated uncertainty is a good\nindication of the reliability of the predicted correspondences.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 09:37:58 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 07:27:28 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 05:11:00 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yin", "Zhichao", ""], ["Darrell", "Trevor", ""], ["Yu", "Fisher", ""]]}, {"id": "1812.06271", "submitter": "Daksh Thapar", "authors": "Daksh Thapar, Gaurav Jaswal, Aditya Nigam, Vivek Kanhangad", "title": "PVSNet: Palm Vein Authentication Siamese Network Trained using Triplet\n  Loss and Adaptive Hard Mining by Learning Enforced Domain Specific Features", "comments": "Accepted in 5th IEEE International Conference on Identity, Security\n  and Behavior Analysis (ISBA), 2019, Hyderabad, India", "journal-ref": null, "doi": "10.1109/ISBA.2019.8778623", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing an end-to-end deep learning network to match the biometric features\nwith limited training samples is an extremely challenging task. To address this\nproblem, we propose a new way to design an end-to-end deep CNN framework i.e.,\nPVSNet that works in two major steps: first, an encoder-decoder network is used\nto learn generative domain-specific features followed by a Siamese network in\nwhich convolutional layers are pre-trained in an unsupervised fashion as an\nautoencoder. The proposed model is trained via triplet loss function that is\nadjusted for learning feature embeddings in a way that minimizes the distance\nbetween embedding-pairs from the same subject and maximizes the distance with\nthose from different subjects, with a margin. In particular, a triplet Siamese\nmatching network using an adaptive margin based hard negative mining has been\nsuggested. The hyper-parameters associated with the training strategy, like the\nadaptive margin, have been tuned to make the learning more effective on\nbiometric datasets. In extensive experimentation, the proposed network\noutperforms most of the existing deep learning solutions on three type of\ntypical vein datasets which clearly demonstrates the effectiveness of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 11:16:44 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Thapar", "Daksh", ""], ["Jaswal", "Gaurav", ""], ["Nigam", "Aditya", ""], ["Kanhangad", "Vivek", ""]]}, {"id": "1812.06297", "submitter": "Anqi Xu", "authors": "Joel Lamy-Poirier, Anqi Xu", "title": "Hinted Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Hinted Networks: a collection of architectural transformations for\nimproving the accuracies of neural network models for regression tasks, through\nthe injection of a prior for the output prediction (i.e. a hint). We ground our\ninvestigations within the camera relocalization domain, and propose two\nvariants, namely the Hinted Embedding and Hinted Residual networks, both\napplied to the PoseNet base model for regressing camera pose from an image. Our\nevaluations show practical improvements in localization accuracy for standard\noutdoor and indoor localization datasets, without using additional information.\nWe further assess the range of accuracy gains within an aerial-view\nlocalization setup, simulated across vast areas at different times of the year.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 14:44:00 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Lamy-Poirier", "Joel", ""], ["Xu", "Anqi", ""]]}, {"id": "1812.06314", "submitter": "Nian Liu", "authors": "Nian Liu, Junwei Han, Ming-Hsuan Yang", "title": "PiCANet: Pixel-wise Contextual Attention Learning for Accurate Saliency\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In saliency detection, every pixel needs contextual information to make\nsaliency prediction. Previous models usually incorporate contexts holistically.\nHowever, for each pixel, usually only part of its context region is useful and\ncontributes to its prediction, while some other part may serve as noises and\ndistractions. In this paper, we propose a novel pixel-wise contextual attention\nnetwork, \\ie PiCANet, to learn to selectively attend to informative context\nlocations at each pixel. Specifically, PiCANet generates an attention map over\nthe context region of each pixel, where each attention weight corresponds to\nthe relevance of a context location w.r.t the referred pixel. Then, attentive\ncontextual features can be constructed via selectively incorporating the\nfeatures of useful context locations with the learned attention. We propose\nthree specific formulations of the PiCANet via embedding the pixel-wise\ncontextual attention mechanism into the pooling and convolution operations with\nattending to global or local contexts. All the three models are fully\ndifferentiable and can be integrated with CNNs with joint training. We\nintroduce the proposed PiCANets into a U-Net architecture for salient object\ndetection. Experimental results indicate that the proposed PiCANets can\nsignificantly improve the saliency detection performance. The generated global\nand local attention can learn to incorporate global contrast and smoothness,\nrespectively, which help localize salient objects more accurately and highlight\nthem more uniformly. Consequently, our saliency model performs favorably\nagainst other state-of-the-art methods. Moreover, we also validate that\nPiCANets can also improve semantic segmentation and object detection\nperformances, which further demonstrates their effectiveness and generalization\nability.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 16:20:33 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Liu", "Nian", ""], ["Han", "Junwei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1812.06367", "submitter": "Paritosh Parmar", "authors": "Paritosh Parmar and Brendan Tran Morris", "title": "Action Quality Assessment Across Multiple Actions", "comments": "WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can learning to measure the quality of an action help in measuring the\nquality of other actions? If so, can consolidated samples from multiple actions\nhelp improve the performance of current approaches? In this paper, we carry out\nexperiments to see if knowledge transfer is possible in the action quality\nassessment (AQA) setting. Experiments are carried out on our newly released AQA\ndataset (http://rtis.oit.unlv.edu/datasets.html) consisting of 1106 action\nsamples from seven actions with quality scores as measured by expert human\njudges. Our experimental results show that there is utility in learning a\nsingle model across multiple actions.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 23:37:39 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 00:18:38 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Parmar", "Paritosh", ""], ["Morris", "Brendan Tran", ""]]}, {"id": "1812.06378", "submitter": "Hongyu Xiong", "authors": "Yinglan Ma, Hongyu Xiong, Zhe Hu and Lizhuang Ma", "title": "Efficient Super Resolution Using Binarized Neural Network", "comments": "10 Pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have recently demonstrated\nhigh-quality results in single-image super-resolution (SR). DCNNs often suffer\nfrom over-parametrization and large amounts of redundancy, which results in\ninefficient inference and high memory usage, preventing massive applications on\nmobile devices. As a way to significantly reduce model size and computation\ntime, binarized neural network has only been shown to excel on semantic-level\ntasks such as image classification and recognition. However, little effort of\nnetwork quantization has been spent on image enhancement tasks like SR, as\nnetwork quantization is usually assumed to sacrifice pixel-level accuracy. In\nthis work, we explore an network-binarization approach for SR tasks without\nsacrificing much reconstruction accuracy. To achieve this, we binarize the\nconvolutional filters in only residual blocks, and adopt a learnable weight for\neach binary filter. We evaluate this idea on several state-of-the-art\nDCNN-based architectures, and show that binarized SR networks achieve\ncomparable qualitative and quantitative results as their real-weight\ncounterparts. Moreover, the proposed binarized strategy could help reduce model\nsize by 80% when applying on SRResNet, and could potentially speed up inference\nby 5 times.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 02:44:20 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Ma", "Yinglan", ""], ["Xiong", "Hongyu", ""], ["Hu", "Zhe", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1812.06384", "submitter": "Shuai Yang", "authors": "Shuai Yang, Jiaying Liu, Wenjing Wang, Zongming Guo", "title": "TET-GAN: Text Effects Transfer via Stylization and Destylization", "comments": "Accepted by AAAI 2019. Code and dataset will be available at\n  http://www.icst.pku.edu.cn/struct/Projects/TETGAN.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text effects transfer technology automatically makes the text dramatically\nmore impressive. However, previous style transfer methods either study the\nmodel for general style, which cannot handle the highly-structured text effects\nalong the glyph, or require manual design of subtle matching criteria for text\neffects. In this paper, we focus on the use of the powerful representation\nabilities of deep neural features for text effects transfer. For this purpose,\nwe propose a novel Texture Effects Transfer GAN (TET-GAN), which consists of a\nstylization subnetwork and a destylization subnetwork. The key idea is to train\nour network to accomplish both the objective of style transfer and style\nremoval, so that it can learn to disentangle and recombine the content and\nstyle features of text effects images. To support the training of our network,\nwe propose a new text effects dataset with as much as 64 professionally\ndesigned styles on 837 characters. We show that the disentangled feature\nrepresentations enable us to transfer or remove all these styles on arbitrary\nglyphs using one network. Furthermore, the flexible network design empowers\nTET-GAN to efficiently extend to a new text style via one-shot learning where\nonly one example is required. We demonstrate the superiority of the proposed\nmethod in generating high-quality stylized text over the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 03:19:08 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 04:08:18 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Yang", "Shuai", ""], ["Liu", "Jiaying", ""], ["Wang", "Wenjing", ""], ["Guo", "Zongming", ""]]}, {"id": "1812.06387", "submitter": "Aravind Ravi", "authors": "Aravind Ravi", "title": "Pre-Trained Convolutional Neural Network Features for Facial Expression\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Facial expression recognition has been an active area in computer vision with\napplication areas including animation, social robots, personalized banking,\netc. In this study, we explore the problem of image classification for\ndetecting facial expressions based on features extracted from pre-trained\nconvolutional neural networks trained on ImageNet database. Features are\nextracted and transferred to a Linear Support Vector Machine for\nclassification. All experiments are performed on two publicly available\ndatasets such as JAFFE and CK+ database. The results show that representations\nlearned from pre-trained networks for a task such as object recognition can be\ntransferred, and used for facial expression recognition. Furthermore, for a\nsmall dataset, using features from earlier layers of the VGG19 network provides\nbetter classification accuracy. Accuracies of 92.26% and 92.86% were achieved\nfor the CK+ and JAFFE datasets respectively.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 04:23:59 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Ravi", "Aravind", ""]]}, {"id": "1812.06407", "submitter": "Kapil Sharma Prof.", "authors": "Kapil Sharma, Himanshu Ahuja, Ashish Kumar, Nipun Bansal, Gurjit Singh\n  Walia", "title": "Unified Graph based Multi-Cue Feature Fusion for Robust Visual Tracking", "comments": "The information on this paper is not complete in entirety and may be\n  misinterpreted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Tracking is a complex problem due to unconstrained appearance\nvariations and dynamic environment. Extraction of complementary information\nfrom the object environment via multiple features and adaption to the target's\nappearance variations are the key problems of this work. To this end, we\npropose a robust object tracking framework based on Unified Graph Fusion (UGF)\nof multi-cue to adapt to the object's appearance. The proposed cross-diffusion\nof sparse and dense features not only suppresses the individual feature\ndeficiencies but also extracts the complementary information from multi-cue.\nThis iterative process builds robust unified features which are invariant to\nobject deformations, fast motion, and occlusion. Robustness of the unified\nfeature also enables the random forest classifier to precisely distinguish the\nforeground from the background, adding resilience to background clutter. In\naddition, we present a novel kernel-based adaptation strategy using outlier\ndetection and a transductive reliability metric.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 07:08:41 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 03:51:07 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 22:46:02 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Sharma", "Kapil", ""], ["Ahuja", "Himanshu", ""], ["Kumar", "Ashish", ""], ["Bansal", "Nipun", ""], ["Walia", "Gurjit Singh", ""]]}, {"id": "1812.06408", "submitter": "Asanka G. Perera", "authors": "Asanka G Perera, Yee Wei Law and Javaan Chahl", "title": "Human Pose and Path Estimation from Aerial Video using Dynamic\n  Classifier Selection", "comments": "For associated dataset, see\n  https://asankagp.github.io/aerialgaitdataset/", "journal-ref": "Cogn Comput 10 (2018) 1019-1041", "doi": "10.1007/s12559-018-9577-6", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating human pose and trajectory by an aerial\nrobot with a monocular camera in near real time. We present a preliminary\nsolution whose distinguishing feature is a dynamic classifier selection\narchitecture. In our solution, each video frame is corrected for perspective\nusing projective transformation. Then, two alternative feature sets are used:\n(i) Histogram of Oriented Gradients (HOG) of the silhouette, (ii) Convolutional\nNeural Network (CNN) features of the RGB image. The features (HOG or CNN) are\nclassified using a dynamic classifier. A class is defined as a pose-viewpoint\npair, and a total of 64 classes are defined to represent a forward walking and\nturning gait sequence. Our solution provides three main advantages: (i)\nClassification is efficient due to dynamic selection (4-class vs. 64-class\nclassification). (ii) Classification errors are confined to neighbors of the\ntrue view-points. (iii) The robust temporal relationship between poses is used\nto resolve the left-right ambiguities of human silhouettes. Experiments\nconducted on both fronto-parallel videos and aerial videos confirm our solution\ncan achieve accurate pose and trajectory estimation for both scenarios. We\nfound using HOG features provides higher accuracy than using CNN features. For\nexample, applying the HOG-based variant of our scheme to the 'walking on a\nfigure 8-shaped path' dataset (1652 frames) achieved estimation accuracies of\n99.6% for viewpoints and 96.2% for number of poses.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 07:27:26 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Perera", "Asanka G", ""], ["Law", "Yee Wei", ""], ["Chahl", "Javaan", ""]]}, {"id": "1812.06417", "submitter": "Daniela Massiceti", "authors": "Daniela Massiceti, Puneet K. Dokania, N. Siddharth, Philip H.S. Torr", "title": "Visual Dialogue without Vision or Dialogue", "comments": "2018 NeurIPS Workshop on Critiquing and Correcting Trends in Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterise some of the quirks and shortcomings in the exploration of\nVisual Dialogue - a sequential question-answering task where the questions and\ncorresponding answers are related through given visual stimuli. To do so, we\ndevelop an embarrassingly simple method based on Canonical Correlation Analysis\n(CCA) that, on the standard dataset, achieves near state-of-the-art performance\non mean rank (MR). In direct contrast to current complex and over-parametrised\narchitectures that are both compute and time intensive, our method ignores the\nvisual stimuli, ignores the sequencing of dialogue, does not need gradients,\nuses off-the-shelf feature extractors, has at least an order of magnitude fewer\nparameters, and learns in practically no time. We argue that these results are\nindicative of issues in current approaches to Visual Dialogue and conduct\nanalyses to highlight implicit dataset biases and effects of over-constrained\nevaluation metrics. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 08:18:37 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 18:20:00 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 10:09:41 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Massiceti", "Daniela", ""], ["Dokania", "Puneet K.", ""], ["Siddharth", "N.", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1812.06418", "submitter": "Peizhao Li", "authors": "Xiaolong Jiang, Peizhao Li, Xiantong Zhen, Xianbin Cao", "title": "Model-free Tracking with Deep Appearance and Motion Features Integration", "comments": "10 pages, 10 figures. Accepted in IEEE Winter Conference on\n  Applications of Computer Vision (WACV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to track an anonymous object, a model-free tracker is\ncomprehensively applicable regardless of the target type. However, designing\nsuch a generalized framework is challenged by the lack of object-oriented prior\ninformation. As one solution, a real-time model-free object tracking approach\nis designed in this work relying on Convolutional Neural Networks (CNNs). To\novercome the object-centric information scarcity, both appearance and motion\nfeatures are deeply integrated by the proposed AMNet, which is an end-to-end\noffline trained two-stream network. Between the two parallel streams, the ANet\ninvestigates appearance features with a multi-scale Siamese atrous CNN,\nenabling the tracking-by-matching strategy. The MNet achieves deep motion\ndetection to localize anonymous moving objects by processing generic motion\nfeatures. The final tracking result at each frame is generated by fusing the\noutput response maps from both sub-networks. The proposed AMNet reports leading\nperformance on both OTB and VOT benchmark datasets with favorable real-time\nprocessing speed.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 08:24:57 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Jiang", "Xiaolong", ""], ["Li", "Peizhao", ""], ["Zhen", "Xiantong", ""], ["Cao", "Xianbin", ""]]}, {"id": "1812.06423", "submitter": "Wei-Lun Chao", "authors": "Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, Fei Sha", "title": "Classifier and Exemplar Synthesis for Zero-Shot Learning", "comments": "Extended version of arXiv:1603.00550 (CVPR 2016) and arXiv:1605.08151\n  (ICCV 2017); Accepted for publication in International Journal of Computer\n  Vision (IJCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) enables solving a task without the need to see its\nexamples. In this paper, we propose two ZSL frameworks that learn to synthesize\nparameters for novel unseen classes. First, we propose to cast the problem of\nZSL as learning manifold embeddings from graphs composed of object classes,\nleading to a flexible approach that synthesizes \"classifiers\" for the unseen\nclasses. Then, we define an auxiliary task of synthesizing \"exemplars\" for the\nunseen classes to be used as an automatic denoising mechanism for any existing\nZSL approaches or as an effective ZSL model by itself. On five visual\nrecognition benchmark datasets, we demonstrate the superior performances of our\nproposed frameworks in various scenarios of both conventional and generalized\nZSL. Finally, we provide valuable insights through a series of empirical\nanalyses, among which are a comparison of semantic representations on the full\nImageNet benchmark as well as a comparison of metrics used in generalized ZSL.\nOur code and data are publicly available at\nhttps://github.com/pujols/Zero-shot-learning-journal\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 08:44:11 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 04:12:19 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Changpinyo", "Soravit", ""], ["Chao", "Wei-Lun", ""], ["Gong", "Boqing", ""], ["Sha", "Fei", ""]]}, {"id": "1812.06426", "submitter": "Guangli Li", "authors": "Guangli Li, Lei Liu, Xueying Wang, Xiao Dong, Peng Zhao, Xiaobing Feng", "title": "Auto-tuning Neural Network Quantization Framework for Collaborative\n  Inference Between the Cloud and Edge", "comments": "Published at ICANN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks (DNNs) have been widely applied in mobile\nintelligent applications. The inference for the DNNs is usually performed in\nthe cloud. However, it leads to a large overhead of transmitting data via\nwireless network. In this paper, we demonstrate the advantages of the\ncloud-edge collaborative inference with quantization. By analyzing the\ncharacteristics of layers in DNNs, an auto-tuning neural network quantization\nframework for collaborative inference is proposed. We study the effectiveness\nof mixed-precision collaborative inference of state-of-the-art DNNs by using\nImageNet dataset. The experimental results show that our framework can generate\nreasonable network partitions and reduce the storage on mobile devices with\ntrivial loss of accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 09:05:44 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Li", "Guangli", ""], ["Liu", "Lei", ""], ["Wang", "Xueying", ""], ["Dong", "Xiao", ""], ["Zhao", "Peng", ""], ["Feng", "Xiaobing", ""]]}, {"id": "1812.06499", "submitter": "Simon Graham Mr", "authors": "Simon Graham, Quoc Dang Vu, Shan E Ahmed Raza, Ayesha Azam, Yee Wah\n  Tsang, Jin Tae Kwak and Nasir Rajpoot", "title": "HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in\n  Multi-Tissue Histology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear segmentation and classification within Haematoxylin & Eosin stained\nhistology images is a fundamental prerequisite in the digital pathology\nwork-flow. The development of automated methods for nuclear segmentation and\nclassification enables the quantitative analysis of tens of thousands of nuclei\nwithin a whole-slide pathology image, opening up possibilities of further\nanalysis of large-scale nuclear morphometry. However, automated nuclear\nsegmentation and classification is faced with a major challenge in that there\nare several different types of nuclei, some of them exhibiting large\nintra-class variability such as the tumour cells. Additionally, some of the\nnuclei are often clustered together. To address these challenges, we present a\nnovel convolutional neural network for simultaneous nuclear segmentation and\nclassification that leverages the instance-rich information encoded within the\nvertical and horizontal distances of nuclear pixels to their centres of mass.\nThese distances are then utilised to separate clustered nuclei, resulting in an\naccurate segmentation, particularly in areas with overlapping instances. Then\nfor each segmented instance, the network predicts the type of nucleus via a\ndevoted up-sampling branch. We demonstrate state-of-the-art performance\ncompared to other methods on multiple independent multi-tissue histology image\ndatasets. As part of this work, we introduce a new dataset of Haematoxylin &\nEosin stained colorectal adenocarcinoma image tiles, containing 24,319\nexhaustively annotated nuclei with associated class labels.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 16:53:41 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 15:25:40 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 11:12:06 GMT"}, {"version": "v4", "created": "Tue, 25 Jun 2019 10:52:10 GMT"}, {"version": "v5", "created": "Wed, 13 Nov 2019 17:25:43 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Graham", "Simon", ""], ["Vu", "Quoc Dang", ""], ["Raza", "Shan E Ahmed", ""], ["Azam", "Ayesha", ""], ["Tsang", "Yee Wah", ""], ["Kwak", "Jin Tae", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1812.06509", "submitter": "Xiaogang Cheng", "authors": "Xiaogang Cheng, Bin Yang, Kaige Tan, Erik Isaksson, Liren Li, Anders\n  Hedman, Thomas Olofsson and Haibo Li", "title": "Non-invasive measuring method of skin temperature based on skin\n  sensitivity index and deep learning", "comments": "13 pages, 5 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human-centered intelligent building, real-time measurements of human\nthermal comfort play critical roles and supply feedback control signals for\nbuilding heating, ventilation, and air conditioning (HVAC) systems. Due to the\nchallenges of intra- and inter-individual differences and skin subtleness\nvariations, there is no satisfactory solution for thermal comfort measurements\nuntil now. In this paper, a non-invasive measuring method based on skin\nsensitivity index and deep learning (NISDL) was proposed to measure real-time\nskin temperature. A new evaluating index, named skin sensitivity index (SSI),\nwas defined to overcome individual differences and skin subtleness variations.\nTo illustrate the effectiveness of SSI proposed, two multi-layers deep learning\nframework (NISDL method I and II) was designed and the DenseNet201 was used for\nextracting features from skin images. The partly personal saturation\ntemperature (NIPST) algorithm was use for algorithm comparisons. Another deep\nlearning algorithm without SSI (DL) was also generated for algorithm\ncomparisons. Finally, a total of 1.44 million image data was used for algorithm\nvalidation. The results show that 55.6180% and 52.2472% error values (NISDL\nmethod I, II) are scattered at [0, 0.25), and the same error intervals\ndistribution of NIPST is 35.3933%.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 17:56:16 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Cheng", "Xiaogang", ""], ["Yang", "Bin", ""], ["Tan", "Kaige", ""], ["Isaksson", "Erik", ""], ["Li", "Liren", ""], ["Hedman", "Anders", ""], ["Olofsson", "Thomas", ""], ["Li", "Haibo", ""]]}, {"id": "1812.06544", "submitter": "Krishanu Sarker", "authors": "Krishanu Sarker, Mohamed Masoud, Saeid Belkasim, Shihao Ji", "title": "Towards Robust Human Activity Recognition from RGB Video Stream with\n  Limited Labeled Data", "comments": "To appear in ICMLA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition based on video streams has received numerous\nattentions in recent years. Due to lack of depth information, RGB video based\nactivity recognition performs poorly compared to RGB-D video based solutions.\nOn the other hand, acquiring depth information, inertia etc. is costly and\nrequires special equipment, whereas RGB video streams are available in ordinary\ncameras. Hence, our goal is to investigate whether similar or even higher\naccuracy can be achieved with RGB-only modality. In this regard, we propose a\nnovel framework that couples skeleton data extracted from RGB video and deep\nBidirectional Long Short Term Memory (BLSTM) model for activity recognition. A\nbig challenge of training such a deep network is the limited training data, and\nexploring RGB-only stream significantly exaggerates the difficulty. We\ntherefore propose a set of algorithmic techniques to train this model\neffectively, e.g., data augmentation, dynamic frame dropout and gradient\ninjection. The experiments demonstrate that our RGB-only solution surpasses the\nstate-of-the-art approaches that all exploit RGB-D video streams by a notable\nmargin. This makes our solution widely deployable with ordinary cameras.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 22:19:43 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Sarker", "Krishanu", ""], ["Masoud", "Mohamed", ""], ["Belkasim", "Saeid", ""], ["Ji", "Shihao", ""]]}, {"id": "1812.06570", "submitter": "Xiang Li", "authors": "Xiang Li and Shihao Ji", "title": "Defense-VAE: A Fast and Accurate Defense against Adversarial Attacks", "comments": "Published as a workshop paper at MLCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been enormously successful across a variety\nof prediction tasks. However, recent research shows that DNNs are particularly\nvulnerable to adversarial attacks, which poses a serious threat to their\napplications in security-sensitive systems. In this paper, we propose a simple\nyet effective defense algorithm Defense-VAE that uses variational autoencoder\n(VAE) to purge adversarial perturbations from contaminated images. The proposed\nmethod is generic and can defend white-box and black-box attacks without the\nneed of retraining the original CNN classifiers, and can further strengthen the\ndefense by retraining CNN or end-to-end finetuning the whole pipeline. In\naddition, the proposed method is very efficient compared to the\noptimization-based alternatives, such as Defense-GAN, since no iterative\noptimization is needed for online prediction. Extensive experiments on MNIST,\nFashion-MNIST, CelebA and CIFAR-10 demonstrate the superior defense accuracy of\nDefense-VAE compared to Defense-GAN, while being 50x faster than the latter.\nThis makes Defense-VAE widely deployable in real-time security-sensitive\nsystems. Our source code can be found at\nhttps://github.com/lxuniverse/defense-vae.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 01:13:44 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 21:31:54 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 03:11:26 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Li", "Xiang", ""], ["Ji", "Shihao", ""]]}, {"id": "1812.06571", "submitter": "Lili Pan", "authors": "Lili Pan, Shen Cheng, Jian Liu, Yazhou Ren, Zenglin Xu", "title": "Latent Dirichlet Allocation in Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of multimodal generative modelling of images based on\ngenerative adversarial networks (GANs). Despite the success of existing\nmethods, they often ignore the underlying structure of vision data or its\nmultimodal generation characteristics. To address this problem, we introduce\nthe Dirichlet prior for multimodal image generation, which leads to a new\nLatent Dirichlet Allocation based GAN (LDAGAN). In detail, for the generative\nprocess modelling, LDAGAN defines a generative mode for each sample,\ndetermining which generative sub-process it belongs to. For the adversarial\ntraining, LDAGAN derives a variational expectation-maximization (VEM) algorithm\nto estimate model parameters. Experimental results on real-world datasets have\ndemonstrated the outstanding performance of LDAGAN over other existing GANs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 01:21:20 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 10:06:57 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 10:28:54 GMT"}, {"version": "v4", "created": "Tue, 16 Jul 2019 08:58:45 GMT"}, {"version": "v5", "created": "Wed, 6 Nov 2019 14:56:54 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Pan", "Lili", ""], ["Cheng", "Shen", ""], ["Liu", "Jian", ""], ["Ren", "Yazhou", ""], ["Xu", "Zenglin", ""]]}, {"id": "1812.06576", "submitter": "Di Xie", "authors": "Yingying Zhang and Qiaoyong Zhong and Liang Ma and Di Xie and Shiliang\n  Pu", "title": "Learning Incremental Triplet Margin for Person Re-identification", "comments": "accepted by AAAI19 as spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) aims to match people across multiple\nnon-overlapping video cameras deployed at different locations. To address this\nchallenging problem, many metric learning approaches have been proposed, among\nwhich triplet loss is one of the state-of-the-arts. In this work, we explore\nthe margin between positive and negative pairs of triplets and prove that large\nmargin is beneficial. In particular, we propose a novel multi-stage training\nstrategy which learns incremental triplet margin and improves triplet loss\neffectively. Multiple levels of feature maps are exploited to make the learned\nfeatures more discriminative. Besides, we introduce global hard identity\nsearching method to sample hard identities when generating a training batch.\nExtensive experiments on Market-1501, CUHK03, and DukeMTMCreID show that our\napproach yields a performance boost and outperforms most existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 01:48:06 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Zhang", "Yingying", ""], ["Zhong", "Qiaoyong", ""], ["Ma", "Liang", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""]]}, {"id": "1812.06580", "submitter": "Maria Brbi\\'c", "authors": "Maria Brbi\\'c and Ivica Kopriva", "title": "$\\ell_0$-Motivated Low-Rank Sparse Subspace Clustering", "comments": null, "journal-ref": null, "doi": "10.1109/TCYB.2018.2883566", "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, high-dimensional data points can be well represented by\nlow-dimensional subspaces. To identify the subspaces, it is important to\ncapture a global and local structure of the data which is achieved by imposing\nlow-rank and sparseness constraints on the data representation matrix. In\nlow-rank sparse subspace clustering (LRSSC), nuclear and $\\ell_1$ norms are\nused to measure rank and sparsity. However, the use of nuclear and $\\ell_1$\nnorms leads to an overpenalized problem and only approximates the original\nproblem. In this paper, we propose two $\\ell_0$ quasi-norm based\nregularizations. First, the paper presents regularization based on multivariate\ngeneralization of minimax-concave penalty (GMC-LRSSC), which contains the\nglobal minimizers of $\\ell_0$ quasi-norm regularized objective. Afterward, we\nintroduce the Schatten-0 ($S_0$) and $\\ell_0$ regularized objective and\napproximate the proximal map of the joint solution using a proximal average\nmethod ($S_0/\\ell_0$-LRSSC). The resulting nonconvex optimization problems are\nsolved using alternating direction method of multipliers with established\nconvergence conditions of both algorithms. Results obtained on synthetic and\nfour real-world datasets show the effectiveness of GMC-LRSSC and\n$S_0/\\ell_0$-LRSSC when compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 02:06:16 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Brbi\u0107", "Maria", ""], ["Kopriva", "Ivica", ""]]}, {"id": "1812.06587", "submitter": "Luowei Zhou", "authors": "Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J. Corso, Marcus\n  Rohrbach", "title": "Grounded Video Description", "comments": "CVPR 2019 oral, camera-ready version including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video description is one of the most challenging problems in vision and\nlanguage understanding due to the large variability both on the video and\nlanguage side. Models, hence, typically shortcut the difficulty in recognition\nand generate plausible sentences that are based on priors but are not\nnecessarily grounded in the video. In this work, we explicitly link the\nsentence to the evidence in the video by annotating each noun phrase in a\nsentence with the corresponding bounding box in one of the frames of a video.\nOur dataset, ActivityNet-Entities, augments the challenging ActivityNet\nCaptions dataset with 158k bounding box annotations, each grounding a noun\nphrase. This allows training video description models with this data, and\nimportantly, evaluate how grounded or \"true\" such model are to the video they\ndescribe. To generate grounded captions, we propose a novel video description\nmodel which is able to exploit these bounding box annotations. We demonstrate\nthe effectiveness of our model on our dataset, but also show how it can be\napplied to image description on the Flickr30k Entities dataset. We achieve\nstate-of-the-art performance on video description, video paragraph description,\nand image description and demonstrate our generated sentences are better\ngrounded in the video.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 02:46:17 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 19:52:55 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Zhou", "Luowei", ""], ["Kalantidis", "Yannis", ""], ["Chen", "Xinlei", ""], ["Corso", "Jason J.", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1812.06589", "submitter": "Hao Zhu", "authors": "Hao Zhu, Huaibo Huang, Yi Li, Aihua Zheng, Ran He", "title": "Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence\n  Learning", "comments": "IJCAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Talking face generation aims to synthesize a face video with precise lip\nsynchronization as well as a smooth transition of facial motion over the entire\nvideo via the given speech clip and facial image. Most existing methods mainly\nfocus on either disentangling the information in a single image or learning\ntemporal information between frames. However, cross-modality coherence between\naudio and video information has not been well addressed during synthesis. In\nthis paper, we propose a novel arbitrary talking face generation framework by\ndiscovering the audio-visual coherence via the proposed Asymmetric Mutual\nInformation Estimator (AMIE). In addition, we propose a Dynamic Attention (DA)\nblock by selectively focusing the lip area of the input image during the\ntraining stage, to further enhance lip synchronization. Experimental results on\nbenchmark LRW dataset and GRID dataset transcend the state-of-the-art methods\non prevalent metrics with robust high-resolution synthesizing on gender and\npose variations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 02:56:09 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 13:22:47 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Zhu", "Hao", ""], ["Huang", "Huaibo", ""], ["Li", "Yi", ""], ["Zheng", "Aihua", ""], ["He", "Ran", ""]]}, {"id": "1812.06597", "submitter": "Hanting Chen", "authors": "Hanting Chen, Yunhe Wang, Chang Xu, Chao Xu, Dacheng Tao", "title": "Learning Student Networks via Feature Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have been widely used in numerous\napplications, but their demanding storage and computational resource\nrequirements prevent their applications on mobile devices. Knowledge\ndistillation aims to optimize a portable student network by taking the\nknowledge from a well-trained heavy teacher network. Traditional\nteacher-student based methods used to rely on additional fully-connected layers\nto bridge intermediate layers of teacher and student networks, which brings in\na large number of auxiliary parameters. In contrast, this paper aims to\npropagate information from teacher to student without introducing new variables\nwhich need to be optimized. We regard the teacher-student paradigm from a new\nperspective of feature embedding. By introducing the locality preserving loss,\nthe student network is encouraged to generate the low-dimensional features\nwhich could inherit intrinsic properties of their corresponding\nhigh-dimensional features from teacher network. The resulting portable network\nthus can naturally maintain the performance as that of the teacher network.\nTheoretical analysis is provided to justify the lower computation complexity of\nthe proposed method. Experiments on benchmark datasets and well-trained\nnetworks suggest that the proposed algorithm is superior to state-of-the-art\nteacher-student learning methods in terms of computational and storage\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 03:21:20 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Chen", "Hanting", ""], ["Wang", "Yunhe", ""], ["Xu", "Chang", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1812.06611", "submitter": "Di Xie", "authors": "Weijie Chen and Yuan Zhang and Di Xie and Shiliang Pu", "title": "A Layer Decomposition-Recomposition Framework for Neuron Pruning towards\n  Accurate Lightweight Networks", "comments": "accepted by AAAI19 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuron pruning is an efficient method to compress the network into a slimmer\none for reducing the computational cost and storage overhead. Most of\nstate-of-the-art results are obtained in a layer-by-layer optimization mode. It\ndiscards the unimportant input neurons and uses the survived ones to\nreconstruct the output neurons approaching to the original ones in a\nlayer-by-layer manner. However, an unnoticed problem arises that the\ninformation loss is accumulated as layer increases since the survived neurons\nstill do not encode the entire information as before. A better alternative is\nto propagate the entire useful information to reconstruct the pruned layer\ninstead of directly discarding the less important neurons. To this end, we\npropose a novel Layer Decomposition-Recomposition Framework (LDRF) for neuron\npruning, by which each layer's output information is recovered in an embedding\nspace and then propagated to reconstruct the following pruned layers with\nuseful information preserved. We mainly conduct our experiments on ILSVRC-12\nbenchmark with VGG-16 and ResNet-50. What should be emphasized is that our\nresults before end-to-end fine-tuning are significantly superior owing to the\ninformation-preserving property of our proposed framework.With end-to-end\nfine-tuning, we achieve state-of-the-art results of 5.13x and 3x speed-up with\nonly 0.5% and 0.65% top-5 accuracy drop respectively, which outperform the\nexisting neuron pruning methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 04:15:41 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Chen", "Weijie", ""], ["Zhang", "Yuan", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""]]}, {"id": "1812.06613", "submitter": "Juan Wang", "authors": "Zhijing Xu, Juan Wang, Ying Zhang, Xiangjian He", "title": "Voiceprint recognition of Parkinson patients based on deep learning", "comments": "10 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than 90% of the Parkinson Disease (PD) patients suffer from vocal\ndisorders. Speech impairment is already indicator of PD. This study focuses on\nPD diagnosis through voiceprint features. In this paper, a method based on Deep\nNeural Network (DNN) recognition and classification combined with Mini-Batch\nGradient Descent (MBGD) is proposed to distinguish PD patients from healthy\npeople using voiceprint features. In order to exact the voiceprint features\nfrom patients, Weighted Mel Frequency Cepstrum Coefficients (WMFCC) is applied.\nThe proposed method is tested on experimental data obtained by the voice\nrecordings of three sustained vowels /a/, /o/ and /u/ from participants (48 PD\nand 20 healthy people). The results show that the proposed method achieves a\nhigh accuracy of diagnosis of PD patients from healthy people, than the\nconventional methods like Support Vector Machine (SVM) and other mentioned in\nthis paper. The accuracy achieved is 89.5%. WMFCC approach can solve the\nproblem that the high-order cepstrum coefficients are small and the features\ncomponent's representation ability to the audio is weak. MBGD reduces the\ncomputational loads of the loss function, and increases the training speed of\nthe system. DNN classifier enhances the classification ability of voiceprint\nfeatures. Therefore, the above approaches can provide a solid solution for the\nquick auxiliary diagnosis of PD in early stage.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 04:24:46 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Xu", "Zhijing", ""], ["Wang", "Juan", ""], ["Zhang", "Ying", ""], ["He", "Xiangjian", ""]]}, {"id": "1812.06624", "submitter": "Chiranjib Sur", "authors": "Chiranjib Sur", "title": "Feature Fusion Effects of Tensor Product Representation on\n  (De)Compositional Network for Caption Generation for Images", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Progress in image captioning is gradually getting complex as researchers try\nto generalized the model and define the representation between visual features\nand natural language processing. This work tried to define such kind of\nrelationship in the form of representation called Tensor Product Representation\n(TPR) which generalized the scheme of language modeling and structuring the\nlinguistic attributes (related to grammar and parts of speech of language)\nwhich will provide a much better structure and grammatically correct sentence.\nTPR enables better and unique representation and structuring of the feature\nspace and will enable better sentence composition from these representations. A\nlarge part of the different ways of defining and improving these TPR are\ndiscussed and their performance with respect to the traditional procedures and\nfeature representations are evaluated for image captioning application. The new\nmodels achieved considerable improvement than the corresponding previous\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 06:24:03 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Sur", "Chiranjib", ""]]}, {"id": "1812.06625", "submitter": "Zhiwei Wang", "authors": "Zhiwei Wang, Yi Lin, Kwang-Ting Cheng and Xin Yang", "title": "Semi-supervised mp-MRI Data Synthesis with StitchLayer and Auxiliary\n  Distance Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of synthesizing multi-parameter\nmagnetic resonance imaging (mp-MRI) data, i.e. Apparent Diffusion Coefficients\n(ADC) and T2-weighted (T2w), containing clinically significant (CS) prostate\ncancer (PCa) via semi-supervised adversarial learning. Specifically, our\nsynthesizer generates mp-MRI data in a sequential manner: first generating ADC\nmaps from 128-d latent vectors, followed by translating them to the T2w images.\nThe synthesizer is trained in a semisupervised manner. In the supervised\ntraining process, a limited amount of paired ADC-T2w images and the\ncorresponding ADC encodings are provided and the synthesizer learns the paired\nrelationship by explicitly minimizing the reconstruction losses between\nsynthetic and real images. To avoid overfitting limited ADC encodings, an\nunlimited amount of random latent vectors and unpaired ADC-T2w Images are\nutilized in the unsupervised training process for learning the marginal image\ndistributions of real images. To improve the robustness of synthesizing, we\ndecompose the difficult task of generating full-size images into several\nsimpler tasks which generate sub-images only. A StitchLayer is then employed to\nfuse sub-images together in an interlaced manner into a full-size image. To\nenforce the synthetic images to indeed contain distinguishable CS PCa lesions,\nwe propose to also maximize an auxiliary distance of Jensen-Shannon divergence\n(JSD) between CS and nonCS images. Experimental results show that our method\ncan effectively synthesize a large variety of mpMRI images which contain\nmeaningful CS PCa lesions, display a good visual quality and have the correct\npaired relationship. Compared to the state-of-the-art synthesis methods, our\nmethod achieves a significant improvement in terms of both visual and\nquantitative evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 06:27:51 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Wang", "Zhiwei", ""], ["Lin", "Yi", ""], ["Cheng", "Kwang-Ting", ""], ["Yang", "Xin", ""]]}, {"id": "1812.06663", "submitter": "Keke Tang", "authors": "Keke Tang, Guodong Wei, Runnan Chen, Jie Zhu, Zhaoquan Gu, and Wenping\n  Wang", "title": "Attending Category Disentangled Global Context for Image Classification", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework for image classification using\nthe attention mechanism and global context, which could incorporate with\nvarious network architectures to improve their performance. To investigate the\ncapability of the global context, we compare four mathematical models and\nobserve the global context encoded in the category disentangled conditional\ngenerative model could give more guidance as \"know what is task irrelevant will\nalso know what is relevant\". Based on this observation, we define a novel\nCategory Disentangled Global Context (CDGC) and devise a deep network to obtain\nit. By attending CDGC, the baseline networks could identify the objects of\ninterest more accurately, thus improving the performance. We apply the\nframework to many different network architectures and compare with the\nstate-of-the-art on four publicly available datasets. Extensive results\nvalidate the effectiveness and superiority of our approach. Code will be made\npublic upon paper acceptance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 09:17:45 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 06:59:51 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 11:31:53 GMT"}, {"version": "v4", "created": "Thu, 18 Jul 2019 14:41:27 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tang", "Keke", ""], ["Wei", "Guodong", ""], ["Chen", "Runnan", ""], ["Zhu", "Jie", ""], ["Gu", "Zhaoquan", ""], ["Wang", "Wenping", ""]]}, {"id": "1812.06673", "submitter": "Zhao Kang", "authors": "Zhao Kang, Haiqi Pan, Steven C.H. Hoi, Zenglin Xu", "title": "Robust Graph Learning from Noisy Data", "comments": null, "journal-ref": null, "doi": "10.1109/TCYB.2018.2887094", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning graphs from data automatically has shown encouraging performance on\nclustering and semisupervised learning tasks. However, real data are often\ncorrupted, which may cause the learned graph to be inexact or unreliable. In\nthis paper, we propose a novel robust graph learning scheme to learn reliable\ngraphs from real-world noisy data by adaptively removing noise and errors in\nthe raw data. We show that our proposed model can also be viewed as a robust\nversion of manifold regularized robust PCA, where the quality of the graph\nplays a critical role. The proposed model is able to boost the performance of\ndata clustering, semisupervised classification, and data recovery\nsignificantly, primarily due to two key factors: 1) enhanced low-rank recovery\nby exploiting the graph smoothness assumption, 2) improved graph construction\nby exploiting clean data recovered by robust PCA. Thus, it boosts the\nclustering, semi-supervised classification, and data recovery performance\noverall. Extensive experiments on image/document clustering, object\nrecognition, image shadow removal, and video background subtraction reveal that\nour model outperforms the previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 10:01:59 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Kang", "Zhao", ""], ["Pan", "Haiqi", ""], ["Hoi", "Steven C. H.", ""], ["Xu", "Zenglin", ""]]}, {"id": "1812.06677", "submitter": "Cheng Lin", "authors": "Cheng Lin, Changjian Li, Wenping Wang", "title": "Floorplan-Jigsaw: Jointly Estimating Scene Layout and Aligning Partial\n  Scans", "comments": "Published at ICCV 2019. Previously on arxiv as \"Floorplan Priors for\n  Joint Camera Pose and Room Layout Estimation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to align partial 3D reconstructions which may not\nhave substantial overlap. Using floorplan priors, our method jointly predicts a\nroom layout and estimates the transformations from a set of partial 3D data.\nUnlike the existing methods relying on feature descriptors to establish\ncorrespondences, we exploit the 3D \"box\" structure of a typical room layout\nthat meets the Manhattan World property. We first estimate a local layout for\neach partial scan separately and then combine these local layouts to form a\nglobally aligned layout with loop closure. Without the requirement of feature\nmatching, the proposed method enables some novel applications ranging from\nlarge or featureless scene reconstruction and modeling from sparse input. We\nvalidate our method quantitatively and qualitatively on real and synthetic\nscenes of various sizes and complexities. The evaluations and comparisons show\nsuperior effectiveness and accuracy of our method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 10:24:46 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 02:31:28 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 11:37:29 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lin", "Cheng", ""], ["Li", "Changjian", ""], ["Wang", "Wenping", ""]]}, {"id": "1812.06707", "submitter": "Rakshith Shetty", "authors": "Rakshith Shetty and Bernt Schiele and Mario Fritz", "title": "Not Using the Car to See the Sidewalk: Quantifying and Controlling the\n  Effects of Context in Classification and Segmentation", "comments": "14 pages (12 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance of visual context in scene understanding tasks is well recognized\nin the computer vision community. However, to what extent the computer vision\nmodels for image classification and semantic segmentation are dependent on the\ncontext to make their predictions is unclear. A model overly relying on context\nwill fail when encountering objects in context distributions different from\ntraining data and hence it is important to identify these dependencies before\nwe can deploy the models in the real-world. We propose a method to quantify the\nsensitivity of black-box vision models to visual context by editing images to\nremove selected objects and measuring the response of the target models. We\napply this methodology on two tasks, image classification and semantic\nsegmentation, and discover undesirable dependency between objects and context,\nfor example that \"sidewalk\" segmentation relies heavily on \"cars\" being present\nin the image. We propose an object removal based data augmentation solution to\nmitigate this dependency and increase the robustness of classification and\nsegmentation models to contextual variations. Our experiments show that the\nproposed data augmentation helps these models improve the performance in\nout-of-context scenarios, while preserving the performance on regular data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 11:28:05 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Shetty", "Rakshith", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1812.06765", "submitter": "Daniel Budelmann", "authors": "Daniel Budelmann, Lars K\\\"onig, Nils Papenberg, Jan Lellmann", "title": "Fully-deformable 3D image registration in two seconds", "comments": "Accepted for publication in Bildverarbeitung f\\\"ur die Medizin (BVM)\n  Proceedings 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a highly parallel method for accurate and efficient variational\ndeformable 3D image registration on a consumer-grade graphics processing unit\n(GPU). We build on recent matrix-free variational approaches and specialize the\nconcepts to the massively-parallel manycore architecture provided by the GPU.\nCompared to a parallel and optimized CPU implementation, this allows us to\nachieve an average speedup of 32.53 on 986 real-world CT thorax-abdomen\nfollow-up scans. At a resolution of approximately $256^3$ voxels, the average\nruntime is 1.99 seconds for the full registration. On the publicly available\nDIR-lab benchmark, our method ranks third with respect to average landmark\nerror at an average runtime of 0.32 seconds.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 13:52:09 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Budelmann", "Daniel", ""], ["K\u00f6nig", "Lars", ""], ["Papenberg", "Nils", ""], ["Lellmann", "Jan", ""]]}, {"id": "1812.06775", "submitter": "Dominik Zietlow", "authors": "Michal Rolinek, Dominik Zietlow, Georg Martius", "title": "Variational Autoencoders Pursue PCA Directions (by Accident)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Variational Autoencoder (VAE) is a powerful architecture capable of\nrepresentation learning and generative modeling. When it comes to learning\ninterpretable (disentangled) representations, VAE and its variants show\nunparalleled performance. However, the reasons for this are unclear, since a\nvery particular alignment of the latent embedding is needed but the design of\nthe VAE does not encourage it in any explicit way. We address this matter and\noffer the following explanation: the diagonal approximation in the encoder\ntogether with the inherent stochasticity force local orthogonality of the\ndecoder. The local behavior of promoting both reconstruction and orthogonality\nmatches closely how the PCA embedding is chosen. Alongside providing an\nintuitive understanding, we justify the statement with full theoretical\nanalysis as well as with experiments.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 14:06:18 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 12:20:39 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Rolinek", "Michal", ""], ["Zietlow", "Dominik", ""], ["Martius", "Georg", ""]]}, {"id": "1812.06829", "submitter": "Achraf Ben-Hamadou", "authors": "Nesrine Grati, Achraf Ben-Hamadou, and Mohamed Hammami", "title": "Discriminant Patch Representation for RGB-D Face Recognition Using\n  Convolutional Neural Networks", "comments": "Preprint accepted for publication in Proceedings of the 14th\n  International Joint Conference on Computer Vision, Imaging and Computer\n  Graphics Theory and Applications - 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on designing data-driven models to learn a discriminant\nrepresentation space for face recognition using RGB-D data. Unlike hand-crafted\nrepresentations, learned models can extract and organize the discriminant\ninformation from the data, and can automatically adapt to build new compute\nvision applications faster. We proposed an effective way to train Convolutional\nNeural Networks to learn face patch discriminant features. The proposed\nsolution was tested and validated on state-of-the-art RGB-D datasets and showed\ncompetitive and promising results relatively to standard hand-crafted feature\nextractors.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 15:19:21 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Grati", "Nesrine", ""], ["Ben-Hamadou", "Achraf", ""], ["Hammami", "Mohamed", ""]]}, {"id": "1812.06847", "submitter": "Huiqiang Liao", "authors": "Huiqiang Liao, Guihua Wen, Yang Hu, Changjun Wang", "title": "Convolutional herbal prescription building method from multi-scale\n  facial features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Traditional Chinese Medicine (TCM), facial features are important basis\nfor diagnosis and treatment. A doctor of TCM can prescribe according to a\npatient's physical indicators such as face, tongue, voice, symptoms, pulse.\nPrevious works analyze and generate prescription according to symptoms.\nHowever, research work to mine the association between facial features and\nprescriptions has not been found for the time being. In this work, we try to\nuse deep learning methods to mine the relationship between the patient's face\nand herbal prescriptions (TCM prescriptions), and propose to construct\nconvolutional neural networks that generate TCM prescriptions according to the\npatient's face image. It is a novel and challenging job. In order to mine\nfeatures from different granularities of faces, we design a multi-scale\nconvolutional neural network based on three-grained face, which mines the\npatient's face information from the organs, local regions, and the entire face.\nOur experiments show that convolutional neural networks can learn relevant\ninformation from face to prescribe, and the multi-scale convolutional neural\nnetworks based on three-grained face perform better.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 15:47:00 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Liao", "Huiqiang", ""], ["Wen", "Guihua", ""], ["Hu", "Yang", ""], ["Wang", "Changjun", ""]]}, {"id": "1812.06858", "submitter": "Guangyuan Pan", "authors": "Guangyuan Pan, Liping Fu, Ruifan Yu, Matthew Muresan", "title": "Winter Road Surface Condition Recognition Using A Pretrained Deep\n  Convolutional Network", "comments": null, "journal-ref": "Transportation Research Board 97th Annual Meeting, 2018", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the application of the latest machine learning\ntechnique deep neural networks for classifying road surface conditions (RSC)\nbased on images from smartphones. Traditional machine learning techniques such\nas support vector machine (SVM) and random forests (RF) have been attempted in\nliterature; however, their classification performance has been less than\ndesirable due to challenges associated with image noises caused by sunlight\nglare and residual salts. A deep learning model based on convolutional neural\nnetwork (CNN) is proposed and evaluated for its potential to address these\nchallenges for improved classification accuracy. In the proposed approach we\nintroduce the idea of applying an existing CNN model that has been pre-trained\nusing millions of images with proven high recognition accuracy. The model is\nextended with two additional fully-connected layers of neurons for learning the\nspecific features of the RSC images. The whole model is then trained with a low\nlearning rate for fine-tuning by using a small set of RSC images. Results show\nthat the proposed model has the highest classification performance in\ncomparison to the traditional machine learning techniques. The testing accuracy\nwith different training dataset sizes is also analyzed, showing the potential\nof achieving much higher accuracy with a larger training dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 15:55:18 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Pan", "Guangyuan", ""], ["Fu", "Liping", ""], ["Yu", "Ruifan", ""], ["Muresan", "Matthew", ""]]}, {"id": "1812.06861", "submitter": "Zhaoyang Lv", "authors": "Zhaoyang Lv, Frank Dellaert, James M. Rehg, Andreas Geiger", "title": "Taking a Deeper Look at the Inverse Compositional Algorithm", "comments": "Paper accepted at CVPR 2019, oral presentation. Code is available at\n  https://github.com/lvzhaoyang/DeeperInverseCompositionalAlgorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a modern synthesis of the classic inverse\ncompositional algorithm for dense image alignment. We first discuss the\nassumptions made by this well-established technique, and subsequently propose\nto relax these assumptions by incorporating data-driven priors into this model.\nMore specifically, we unroll a robust version of the inverse compositional\nalgorithm and replace multiple components of this algorithm using more\nexpressive models whose parameters we train in an end-to-end fashion from data.\nOur experiments on several challenging 3D rigid motion estimation tasks\ndemonstrate the advantages of combining optimization with learning-based\ntechniques, outperforming the classic inverse compositional algorithm as well\nas data-driven image-to-pose regression approaches.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 15:58:10 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 04:53:10 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Lv", "Zhaoyang", ""], ["Dellaert", "Frank", ""], ["Rehg", "James M.", ""], ["Geiger", "Andreas", ""]]}, {"id": "1812.06869", "submitter": "Alexey Gritsenko", "authors": "Alexey A. Gritsenko, Alex D'Amour, James Atwood, Yoni Halpern, D.\n  Sculley", "title": "BriarPatches: Pixel-Space Interventions for Inducing Demographic Parity", "comments": "6 pages, 5 figures, NeurIPS Workshop on Ethical, Social and\n  Governance Issues in AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the BriarPatch, a pixel-space intervention that obscures\nsensitive attributes from representations encoded in pre-trained classifiers.\nThe patches encourage internal model representations not to encode sensitive\ninformation, which has the effect of pushing downstream predictors towards\nexhibiting demographic parity with respect to the sensitive information. The\nnet result is that these BriarPatches provide an intervention mechanism\navailable at user level, and complements prior research on fair representations\nthat were previously only applicable by model developers and ML experts.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 16:13:42 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Gritsenko", "Alexey A.", ""], ["D'Amour", "Alex", ""], ["Atwood", "James", ""], ["Halpern", "Yoni", ""], ["Sculley", "D.", ""]]}, {"id": "1812.06932", "submitter": "Kathleen M Lewis", "authors": "Kathleen M. Lewis and Natalia S. Rost and John Guttag and Adrian V.\n  Dalca", "title": "Fast Learning-based Registration of Sparse 3D Clinical Images", "comments": "This version was accepted to CHIL. It builds on the previous version\n  of the paper and includes more experimental results", "journal-ref": null, "doi": "10.1145/3368555.3384462", "report-no": null, "categories": "cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SparseVM, a method that registers clinical-quality 3D MR scans\nboth faster and more accurately than previously possible. Deformable alignment,\nor registration, of clinical scans is a fundamental task for many clinical\nneuroscience studies. However, most registration algorithms are designed for\nhigh-resolution research-quality scans. In contrast to research-quality scans,\nclinical scans are often sparse, missing up to 86% of the slices available in\nresearch-quality scans. Existing methods for registering these sparse images\nare either inaccurate or extremely slow. We present a learning-based\nregistration method, SparseVM, that is more accurate and orders of magnitude\nfaster than the most accurate clinical registration methods. To our knowledge,\nit is the first method to use deep learning specifically tailored to\nregistering clinical images. We demonstrate our method on a clinically-acquired\nMRI dataset of stroke patients and on a simulated sparse MRI dataset. Our code\nis available as part of the VoxelMorph package at http://voxelmorph.mit.edu/.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 18:14:24 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 15:43:29 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 13:45:39 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Lewis", "Kathleen M.", ""], ["Rost", "Natalia S.", ""], ["Guttag", "John", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "1812.06934", "submitter": "Ana Maria Barragan Montero Dr", "authors": "Ana M. Barragan-Montero, Dan Nguyen, Weiguo Lu, Mu-Han Lin, Xavier\n  Geets, Edmond Sterpin, and Steve Jiang", "title": "Three-Dimensional Dose Prediction for Lung IMRT Patients with Deep\n  Neural Networks: Robust Learning from Heterogeneous Beam Configurations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of neural networks to directly predict three-dimensional dose\ndistributions for automatic planning is becoming popular. However, the existing\nmethods only use patient anatomy as input and assume consistent beam\nconfiguration for all patients in the training database. The purpose of this\nwork is to develop a more general model that, in addition to patient anatomy,\nalso considers variable beam configurations, to achieve a more comprehensive\nautomatic planning with a potentially easier clinical implementation, without\nthe need of training specific models for different beam settings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 18:17:12 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 16:57:31 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Barragan-Montero", "Ana M.", ""], ["Nguyen", "Dan", ""], ["Lu", "Weiguo", ""], ["Lin", "Mu-Han", ""], ["Geets", "Xavier", ""], ["Sterpin", "Edmond", ""], ["Jiang", "Steve", ""]]}, {"id": "1812.06968", "submitter": "Michael Perlmutter", "authors": "Michael Perlmutter, Guy Wolf, and Matthew Hirn", "title": "Geometric Scattering on Manifolds", "comments": "A shorter version of this paper appeared in the NeurIPS 2018\n  Integration of Deep Learning Theories Workshop, Montr\\'{e}al, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Euclidean scattering transform was introduced nearly a decade ago to\nimprove the mathematical understanding of the success of convolutional neural\nnetworks (ConvNets) in image data analysis and other tasks. Inspired by recent\ninterest in geometric deep learning, which aims to generalize ConvNets to\nmanifold and graph-structured domains, we generalize the scattering transform\nto compact manifolds. Similar to the Euclidean scattering transform, our\ngeometric scattering transform is based on a cascade of designed filters and\npointwise nonlinearities, which enables rigorous analysis of the feature\nextraction provided by scattering layers. Our main focus here is on theoretical\nunderstanding of this geometric scattering network, while setting aside\nimplementation aspects, although we remark that application of similar\ntransforms to graph data analysis has been studied recently in related work.\nOur results establish conditions under which geometric scattering provides\nlocalized isometry invariant descriptions of manifold signals, which are also\nstable to families of diffeomorphisms formulated in intrinsic manifolds terms.\nThese results not only generalize the deformation stability and local\nroto-translation invariance of Euclidean scattering, but also demonstrate the\nimportance of linking the used filter structures (e.g., in geometric deep\nlearning) to the underlying manifold geometry, or the data geometry it\nrepresents.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 23:13:59 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 15:00:00 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 04:41:39 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 01:37:56 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Perlmutter", "Michael", ""], ["Wolf", "Guy", ""], ["Hirn", "Matthew", ""]]}, {"id": "1812.07003", "submitter": "Ji Hou", "authors": "Ji Hou, Angela Dai, Matthias Nie{\\ss}ner", "title": "3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans", "comments": "video: https://youtu.be/IH9rNLD1-JE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce 3D-SIS, a novel neural network architecture for 3D semantic\ninstance segmentation in commodity RGB-D scans. The core idea of our method is\nto jointly learn from both geometric and color signal, thus enabling accurate\ninstance predictions. Rather than operate solely on 2D frames, we observe that\nmost computer vision applications have multi-view RGB-D input available, which\nwe leverage to construct an approach for 3D instance segmentation that\neffectively fuses together these multi-modal inputs. Our network leverages\nhigh-resolution RGB input by associating 2D images with the volumetric grid\nbased on the pose alignment of the 3D reconstruction. For each image, we first\nextract 2D features for each pixel with a series of 2D convolutions; we then\nbackproject the resulting feature vector to the associated voxel in the 3D\ngrid. This combination of 2D and 3D feature learning allows significantly\nhigher accuracy object detection and instance segmentation than\nstate-of-the-art alternatives. We show results on both synthetic and real-world\npublic benchmarks, achieving an improvement in mAP of over 13 on real-world\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 19:04:31 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 09:44:43 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 14:18:55 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Hou", "Ji", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1812.07010", "submitter": "Mark Kozdoba", "authors": "Mark Kozdoba, Edward Moroshko, Lior Shani, Takuya Takagi, Takashi\n  Katoh, Shie Mannor, Koby Crammer", "title": "Multi Instance Learning For Unbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of Multi Instance Learning, we analyze the Single Instance\n(SI) learning objective. We show that when the data is unbalanced and the\nfamily of classifiers is sufficiently rich, the SI method is a useful learning\nalgorithm. In particular, we show that larger data imbalance, a quality that is\ntypically perceived as negative, in fact implies a better resilience of the\nalgorithm to the statistical dependencies of the objects in bags. In addition,\nour results shed new light on some known issues with the SI method in the\nsetting of linear classifiers, and we show that these issues are significantly\nless likely to occur in the setting of neural networks. We demonstrate our\nresults on a synthetic dataset, and on the COCO dataset for the problem of\npatch classification with weak image level labels derived from captions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 19:28:30 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Kozdoba", "Mark", ""], ["Moroshko", "Edward", ""], ["Shani", "Lior", ""], ["Takagi", "Takuya", ""], ["Katoh", "Takashi", ""], ["Mannor", "Shie", ""], ["Crammer", "Koby", ""]]}, {"id": "1812.07023", "submitter": "Shikhar Sharma", "authors": "Dat Tien Nguyen, Shikhar Sharma, Hannes Schulz, Layla El Asri", "title": "From FiLM to Video: Multi-turn Question Answering with Multi-modal\n  Context", "comments": "Accepted for an Oral presentation at the DSTC7 workshop at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding audio-visual content and the ability to have an informative\nconversation about it have both been challenging areas for intelligent systems.\nThe Audio Visual Scene-aware Dialog (AVSD) challenge, organized as a track of\nthe Dialog System Technology Challenge 7 (DSTC7), proposes a combined task,\nwhere a system has to answer questions pertaining to a video given a dialogue\nwith previous question-answer pairs and the video itself. We propose for this\ntask a hierarchical encoder-decoder model which computes a multi-modal\nembedding of the dialogue context. It first embeds the dialogue history using\ntwo LSTMs. We extract video and audio frames at regular intervals and compute\nsemantic features using pre-trained I3D and VGGish models, respectively. Before\nsummarizing both modalities into fixed-length vectors using LSTMs, we use FiLM\nblocks to condition them on the embeddings of the current question, which\nallows us to reduce the dimensionality considerably. Finally, we use an LSTM\ndecoder that we train with scheduled sampling and evaluate using beam search.\nCompared to the modality-fusing baseline model released by the AVSD challenge\norganizers, our model achieves a relative improvements of more than 16%,\nscoring 0.36 BLEU-4 and more than 33%, scoring 0.997 CIDEr.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 19:41:37 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Nguyen", "Dat Tien", ""], ["Sharma", "Shikhar", ""], ["Schulz", "Hannes", ""], ["Asri", "Layla El", ""]]}, {"id": "1812.07028", "submitter": "Saber Malekzadeh", "authors": "Saber Malekzadeh", "title": "Fuzzy Controller of Reward of Reinforcement Learning For Handwritten\n  Digit Recognition", "comments": "Submitted to the 3rd International Conference on applied research in\n  Computer Science and Information Technology. in Farsi", "journal-ref": null, "doi": "10.13140/RG.2.2.20222.79688", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of human environment with computer systems always was a big deal\nin artificial intelligence. In this area handwriting recognition and\nconceptualization of it to computer is an important area in it. In the past\nyears with growth of machine learning in artificial intelligence, efforts to\nusing this technique increased. In this paper is tried to using fuzzy\ncontroller, to optimizing amount of reward of reinforcement learning for\nrecognition of handwritten digits. For this aim first a sample of every digit\nwith 10 standard computer fonts, given to actor and then actor is trained. In\nthe next level is tried to test the actor with dataset and then results show\nimprovement of recognition when using fuzzy controller of reinforcement\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 19:57:43 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Malekzadeh", "Saber", ""]]}, {"id": "1812.07032", "submitter": "Hoel Kervadec", "authors": "Hoel Kervadec, Jihene Bouchtiba, Christian Desrosiers, Eric Granger,\n  Jose Dolz, Ismail Ben Ayed", "title": "Boundary loss for highly unbalanced segmentation", "comments": "Runner-up for best paper award at MIDL 2019 [PMLR 102:285-296],\n  invited for MedIA deep learning special issue (Volume 67, January 2021)", "journal-ref": "MIDL 2019, PMLR 102:285-296 -- MedIA Volume 67, January 2021,\n  101851", "doi": "10.1016/j.media.2020.101851", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widely used loss functions for CNN segmentation, e.g., Dice or cross-entropy,\nare based on integrals over the segmentation regions. Unfortunately, for highly\nunbalanced segmentations, such regional summations have values that differ by\nseveral orders of magnitude across classes, which affects training performance\nand stability. We propose a boundary loss, which takes the form of a distance\nmetric on the space of contours, not regions. This can mitigate the\ndifficulties of highly unbalanced problems because it uses integrals over the\ninterface between regions instead of unbalanced integrals over the regions.\nFurthermore, a boundary loss complements regional information. Inspired by\ngraph-based optimization techniques for computing active-contour flows, we\nexpress a non-symmetric $L_2$ distance on the space of contours as a regional\nintegral, which avoids completely local differential computations involving\ncontour points. This yields a boundary loss expressed with the regional softmax\nprobability outputs of the network, which can be easily combined with standard\nregional losses and implemented with any existing deep network architecture for\nN-D segmentation. We report comprehensive evaluations and comparisons on\ndifferent unbalanced problems, showing that our boundary loss can yield\nsignificant increases in performances while improving training stability. Our\ncode is publicly available: https://github.com/LIVIAETS/surface-loss .\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 20:06:50 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 13:08:27 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 17:45:06 GMT"}, {"version": "v4", "created": "Sat, 17 Oct 2020 13:38:05 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kervadec", "Hoel", ""], ["Bouchtiba", "Jihene", ""], ["Desrosiers", "Christian", ""], ["Granger", "Eric", ""], ["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1812.07033", "submitter": "Jigar Doshi", "authors": "Jigar Doshi, Saikat Basu, Guan Pang", "title": "From Satellite Imagery to Disaster Insights", "comments": "NeurIPS 2018 Camera-Ready version; AI for Social Good Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of satellite imagery has become increasingly popular for disaster\nmonitoring and response. After a disaster, it is important to prioritize rescue\noperations, disaster response and coordinate relief efforts. These have to be\ncarried out in a fast and efficient manner since resources are often limited in\ndisaster-affected areas and it's extremely important to identify the areas of\nmaximum damage. However, most of the existing disaster mapping efforts are\nmanual which is time-consuming and often leads to erroneous results. In order\nto address these issues, we propose a framework for change detection using\nConvolutional Neural Networks (CNN) on satellite images which can then be\nthresholded and clustered together into grids to find areas which have been\nmost severely affected by a disaster. We also present a novel metric called\nDisaster Impact Index (DII) and use it to quantify the impact of two natural\ndisasters - the Hurricane Harvey flood and the Santa Rosa fire. Our framework\nachieves a top F1 score of 81.2% on the gridded flood dataset and 83.5% on the\ngridded fire dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 20:08:23 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Doshi", "Jigar", ""], ["Basu", "Saikat", ""], ["Pang", "Guan", ""]]}, {"id": "1812.07045", "submitter": "Yusuke Sekikawa", "authors": "Yusuke Sekikawa, Kosuke Hara, and Hideo Saito", "title": "EventNet: Asynchronous Recursive Event Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired vision sensors that mimic retinas to\nasynchronously report per-pixel intensity changes rather than outputting an\nactual intensity image at regular intervals. This new paradigm of image sensor\noffers significant potential advantages; namely, sparse and non-redundant data\nrepresentation. Unfortunately, however, most of the existing artificial neural\nnetwork architectures, such as a CNN, require dense synchronous input data, and\ntherefore, cannot make use of the sparseness of the data. We propose EventNet,\na neural network designed for real-time processing of asynchronous event\nstreams in a recursive and event-wise manner. EventNet models dependence of the\noutput on tens of thousands of causal events recursively using a novel temporal\ncoding scheme. As a result, at inference time, our network operates in an\nevent-wise manner that is realized with very few sum-of-the-product\noperations---look-up table and temporal feature aggregation---which enables\nprocessing of 1 mega or more events per second on standard CPU. In experiments\nusing real data, we demonstrated the real-time performance and robustness of\nour framework.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 09:47:35 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 07:08:14 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Sekikawa", "Yusuke", ""], ["Hara", "Kosuke", ""], ["Saito", "Hideo", ""]]}, {"id": "1812.07049", "submitter": "Wenfu Wang", "authors": "Wenfu Wang, Zhijie Pan", "title": "DSNet for Real-Time Driving Scene Semantic Segmentation", "comments": "We have discovered some reported numbers unreproducible, and decided\n  to redesign the methods, and rewrite most of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the very challenging task of semantic segmentation for autonomous\ndriving system. It must deliver decent semantic segmentation result for traffic\ncritical objects real-time. In this paper, we propose a very efficient yet\npowerful deep neural network for driving scene semantic segmentation termed as\nDriving Segmentation Network (DSNet). DSNet achieves state-of-the-art balance\nbetween accuracy and inference speed through efficient units and architecture\ndesign inspired by ShuffleNet V2 and ENet. More importantly, DSNet highlights\nclasses most critical with driving decision making through our novel Driving\nImportance-weighted Loss. We evaluate DSNet on Cityscapes dataset, our DSNet\nachieves 71.8% mean Intersection-over-Union (IoU) on validation set and 69.3%\non test set. Class-wise IoU scores show that Driving Importance-weighted Loss\ncould improve most driving critical classes by a large margin. Compared with\nENet, DSNet is 18.9% more accurate and 1.1+ times faster which implies great\npotential for autonomous driving application.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 07:59:02 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 01:57:07 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Wang", "Wenfu", ""], ["Pan", "Zhijie", ""]]}, {"id": "1812.07050", "submitter": "Zhe Liu", "authors": "Zhe Liu and Shunbo Zhou and Chuanzhe Suo and Yingtian Liu and Peng Yin\n  and Hesheng Wang and Yun-Hui Liu", "title": "LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and\n  Environment Analysis", "comments": "This paper has been accepted by ICCV-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud based place recognition is still an open issue due to the\ndifficulty in extracting local features from the raw 3D point cloud and\ngenerating the global descriptor, and it's even harder in the large-scale\ndynamic environments. In this paper, we develop a novel deep neural network,\nnamed LPD-Net (Large-scale Place Description Network), which can extract\ndiscriminative and generalizable global descriptors from the raw 3D point\ncloud. Two modules, the adaptive local feature extraction module and the\ngraph-based neighborhood aggregation module, are proposed, which contribute to\nextract the local structures and reveal the spatial distribution of local\nfeatures in the large-scale point cloud, with an end-to-end manner. We\nimplement the proposed global descriptor in solving point cloud based retrieval\ntasks to achieve the large-scale place recognition. Comparison results show\nthat our LPD-Net is much better than PointNetVLAD and reaches the\nstate-of-the-art. We also compare our LPD-Net with the vision-based solutions\nto show the robustness of our approach to different weather and light\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 04:42:24 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 08:02:47 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Liu", "Zhe", ""], ["Zhou", "Shunbo", ""], ["Suo", "Chuanzhe", ""], ["Liu", "Yingtian", ""], ["Yin", "Peng", ""], ["Wang", "Hesheng", ""], ["Liu", "Yun-Hui", ""]]}, {"id": "1812.07051", "submitter": "Alona Golts", "authors": "Alona Golts, Daniel Freedman, Michael Elad", "title": "Unsupervised Single Image Dehazing Using Dark Channel Prior Loss", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2952032", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image dehazing is a critical stage in many modern-day autonomous\nvision applications. Early prior-based methods often involved a time-consuming\nminimization of a hand-crafted energy function. Recent learning-based\napproaches utilize the representational power of deep neural networks (DNNs) to\nlearn the underlying transformation between hazy and clear images. Due to\ninherent limitations in collecting matching clear and hazy images, these\nmethods resort to training on synthetic data; constructed from indoor images\nand corresponding depth information. This may result in a possible domain shift\nwhen treating outdoor scenes. We propose a completely unsupervised method of\ntraining via minimization of the well-known, Dark Channel Prior (DCP) energy\nfunction. Instead of feeding the network with synthetic data, we solely use\nreal-world outdoor images and tune the network's parameters by directly\nminimizing the DCP. Although our \"Deep DCP\" technique can be regarded as a fast\napproximator of DCP, it actually improves its results significantly. This\nsuggests an additional regularization obtained via the network and learning\nprocess. Experiments show that our method performs on par with large-scale\nsupervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 11:29:38 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 11:40:23 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Golts", "Alona", ""], ["Freedman", "Daniel", ""], ["Elad", "Michael", ""]]}, {"id": "1812.07056", "submitter": "Peyman Gholami", "authors": "Peyman Gholami, Priyanka Roy, Mohana Kuppuswamy Parthasarathy,\n  Vasudevan Lakshminarayanan", "title": "OCTID: Optical Coherence Tomography Image Database", "comments": "This paper is linked to an open access Optical Coherence Tomography\n  (OCT) image database which is avaiable at:\n  https://dataverse.scholarsportal.info/dataverse/OCTID", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) is a non-invasive imaging modality which\nis widely used in clinical ophthalmology. OCT images are capable of visualizing\ndeep retinal layers which is crucial for early diagnosis of retinal diseases.\nIn this paper, we describe a comprehensive open-access database containing more\nthan 500 highresolution images categorized into different pathological\nconditions. The image classes include Normal (NO), Macular Hole (MH),\nAge-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), and\nDiabetic Retinopathy (DR). The images were obtained from a raster scan protocol\nwith a 2mm scan length and 512x1024 pixel resolution. We have also included 25\nnormal OCT images with their corresponding ground truth delineations which can\nbe used for an accurate evaluation of OCT image segmentation. In addition, we\nhave provided a user-friendly GUI which can be used by clinicians for manual\n(and semi-automated) segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 23:26:39 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 15:31:35 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Gholami", "Peyman", ""], ["Roy", "Priyanka", ""], ["Parthasarathy", "Mohana Kuppuswamy", ""], ["Lakshminarayanan", "Vasudevan", ""]]}, {"id": "1812.07059", "submitter": "Chankyu Choi", "authors": "Chankyu Choi, Youngmin Yoon, Junsu Lee, Junseok Kim", "title": "Simultaneous Recognition of Horizontal and Vertical Text in Natural\n  Images", "comments": "Presented at the IWRR workshop at ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art scene text recognition methods have primarily focused\non horizontal text in images. However, in several Asian countries, including\nChina, large amounts of text in signs, books, and TV commercials are vertically\ndirected. Because the horizontal and vertical texts exhibit different\ncharacteristics, developing an algorithm that can simultaneously recognize both\ntypes of text in real environments is necessary. To address this problem, we\nadopted the direction encoding mask (DEM) and selective attention network (SAN)\nmethods based on supervised learning. DEM contains directional information to\ncompensate in cases that lack text direction; therefore, our network is trained\nusing this information to handle the vertical text. The SAN method is designed\nto work individually for both types of text. To train the network to recognize\nboth types of text and to evaluate the effectiveness of the designed model, we\nprepared a new synthetic vertical text dataset and collected an actual vertical\ntext dataset (VTD142) from the Web. Using these datasets, we proved that our\nproposed model can accurately recognize both vertical and horizontal text and\ncan achieve state-of-the-art results in experiments using benchmark datasets,\nincluding the street view test (SVT), IIIT-5k, and ICDAR. Although our model is\nrelatively simple as compared to its predecessors, it maintains the accuracy\nand is trained in an end-to-end manner.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 01:16:08 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Choi", "Chankyu", ""], ["Yoon", "Youngmin", ""], ["Lee", "Junsu", ""], ["Kim", "Junseok", ""]]}, {"id": "1812.07060", "submitter": "Alexey Kruglov", "authors": "Alexey Kruglov", "title": "Channel-wise pruning of neural networks with tapering resource\n  constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network pruning is an important step in design process of efficient\nneural networks for edge devices with limited computational power. Pruning is a\nform of knowledge transfer from the weights of the original network to a\nsmaller target subnetwork. We propose a new method for compute-constrained\nstructured channel-wise pruning of convolutional neural networks. The method\niteratively fine-tunes the network, while gradually tapering the computation\nresources available to the pruned network via a holonomic constraint in the\nmethod of Lagrangian multipliers framework. An explicit and adaptive automatic\ncontrol over the rate of tapering is provided. The trainable parameters of our\npruning method are separate from the weights of the neural network, which\nallows us to avoid the interference with the neural network solver (e.g. avoid\nthe direct dependence of pruning speed on neural network learning rates). Our\nmethod combines the `rigoristic' approach by the direct application of\nconstrained optimization, avoiding the pitfalls of ADMM-based methods, like\ntheir need to define the target amount of resources for each pruning run, and\ndirect dependence of pruning speed and priority of pruning on the relative\nscale of weights between layers. For VGG-16 @ ILSVRC-2012, we achieve reduction\nof 15.47 -> 3.87 GMAC with only 1% top-1 accuracy reduction (68.4% -> 67.4%).\nFor AlexNet @ ILSVRC-2012, we achieve 0.724 -> 0.411 GMAC with 1% top-1\naccuracy reduction (56.8% -> 55.8%).\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 10:41:09 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Kruglov", "Alexey", ""]]}, {"id": "1812.07067", "submitter": "Zibo Meng", "authors": "Jie Cai, Zibo Meng, Ahmed Shehab Khan, Zhiyuan Li, James O'Reilly, and\n  Yan Tong", "title": "Probabilistic Attribute Tree in Convolutional Neural Networks for Facial\n  Expression Recognition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a novel Probabilistic Attribute Tree-CNN (PAT-CNN)\nto explicitly deal with the large intra-class variations caused by\nidentity-related attributes, e.g., age, race, and gender. Specifically, a novel\nPAT module with an associated PAT loss was proposed to learn features in a\nhierarchical tree structure organized according to attributes, where the final\nfeatures are less affected by the attributes. Then, expression-related features\nare extracted from leaf nodes. Samples are probabilistically assigned to tree\nnodes at different levels such that expression-related features can be learned\nfrom all samples weighted by probabilities. We further proposed a\nsemi-supervised strategy to learn the PAT-CNN from limited attribute-annotated\nsamples to make the best use of available data. Experimental results on five\nfacial expression datasets have demonstrated that the proposed PAT-CNN\noutperforms the baseline models by explicitly modeling attributes. More\nimpressively, the PAT-CNN using a single model achieves the best performance\nfor faces in the wild on the SFEW dataset, compared with the state-of-the-art\nmethods using an ensemble of hundreds of CNNs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 21:48:27 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Cai", "Jie", ""], ["Meng", "Zibo", ""], ["Khan", "Ahmed Shehab", ""], ["Li", "Zhiyuan", ""], ["O'Reilly", "James", ""], ["Tong", "Yan", ""]]}, {"id": "1812.07098", "submitter": "Yosr Ghozzi", "authors": "Yosr Ghozzi, Nesrine Baklouti, Hani Hagras, Mounir Ben Ayed, and Adel\n  M. Alimi", "title": "Interval type-2 Beta Fuzzy Near set based approach to content based\n  image retrieval", "comments": "10 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an automated search system, similarity is a key concept in solving a human\ntask. Indeed, human process is usually a natural categorization that underlies\nmany natural abilities such as image recovery, language comprehension, decision\nmaking, or pattern recognition. In the image search axis, there are several\nways to measure the similarity between images in an image database, to a query\nimage. Image search by content is based on the similarity of the visual\ncharacteristics of the images. The distance function used to evaluate the\nsimilarity between images depends on the criteria of the search but also on the\nrepresentation of the characteristics of the image; this is the main idea of\nthe near and fuzzy sets approaches. In this article, we introduce a new\ncategory of beta type-2 fuzzy sets for the description of image characteristics\nas well as the near sets approach for image recovery. Finally, we illustrate\nour work with examples of image recovery problems used in the real world.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 11:34:23 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Ghozzi", "Yosr", ""], ["Baklouti", "Nesrine", ""], ["Hagras", "Hani", ""], ["Ayed", "Mounir Ben", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1812.07099", "submitter": "Hanqing Guo", "authors": "Hanqing Guo, Nan Zhang, Wenjun Shi, Saeed AlQarni, Shaoen Wu", "title": "Real Time 3D Indoor Human Image Capturing Based on FMCW Radar", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most smart systems such as smart home and smart health response to human's\nlocations and activities. However, traditional solutions are either require\nwearable sensors or lead to leaking privacy. This work proposes an ambient\nradar solution which is a real-time, privacy secure and dark surroundings\nresistant system. In this solution, we use a low power, Frequency-Modulated\nContinuous Wave (FMCW) radar array to capture the reflected signals and then\nconstruct to 3D image frames. This solution designs $1)$a data preprocessing\nmechanism to remove background static reflection, $2)$a signal processing\nmechanism to transfer received complex radar signals to a matrix contains\nspacial information, and $3)$ a Deep Learning scheme to filter broken frame\nwhich caused by the rough surface of human's body. This solution has been\nextensively evaluated in a research area and captures real-time human images\nthat are recognizable for specific activities. Our results show that the indoor\ncapturing is clear to be recognized frame by frame compares to camera recorded\nvideo.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 05:28:24 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Guo", "Hanqing", ""], ["Zhang", "Nan", ""], ["Shi", "Wenjun", ""], ["AlQarni", "Saeed", ""], ["Wu", "Shaoen", ""]]}, {"id": "1812.07100", "submitter": "Avinash Kumar Singh", "authors": "Avinash Kumar Singh", "title": "On effective human robot interaction based on recognition and\n  association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faces play a magnificent role in human robot interaction, as they do in our\ndaily life. The inherent ability of the human mind facilitates us to recognize\na person by exploiting various challenges such as bad illumination, occlusions,\npose variation etc. which are involved in face recognition. But it is a very\ncomplex task in nature to identify a human face by humanoid robots. The recent\nliteratures on face biometric recognition are extremely rich in its application\non structured environment for solving human identification problem. But the\napplication of face biometric on mobile robotics is limited for its inability\nto produce accurate identification in uneven circumstances. The existing face\nrecognition problem has been tackled with our proposed component based\nfragmented face recognition framework. The proposed framework uses only a\nsubset of the full face such as eyes, nose and mouth to recognize a person.\nIt's less searching cost, encouraging accuracy and ability to handle various\nchallenges of face recognition offers its applicability on humanoid robots. The\nsecond problem in face recognition is the face spoofing, in which a face\nrecognition system is not able to distinguish between a person and an imposter\n(photo/video of the genuine user). The problem will become more detrimental\nwhen robots are used as an authenticator. A depth analysis method has been\ninvestigated in our research work to test the liveness of imposters to\ndiscriminate them from the legitimate users. The implication of the previous\nearned techniques has been used with respect to criminal identification with\nNAO robot. An eyewitness can interact with NAO through a user interface. NAO\nasks several questions about the suspect, such as age, height, her/his facial\nshape and size etc., and then making a guess about her/his face.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 11:00:13 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Singh", "Avinash Kumar", ""]]}, {"id": "1812.07101", "submitter": "Sourya Sengupta", "authors": "Sourya Sengupta, Amitojdeep Singh, Henry A.Leopold, Tanmay Gulati,\n  Vasudevan Lakshminarayanan", "title": "Application of Deep Learning in Fundus Image Processing for Ophthalmic\n  Diagnosis -- A Review", "comments": null, "journal-ref": "Artificial Intelligence in Medicine Volume 102, January 2020,\n  101758", "doi": "10.1016/j.artmed.2019.101758", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An overview of the applications of deep learning in ophthalmic diagnosis\nusing retinal fundus images is presented. We also review various retinal image\ndatasets that can be used for deep learning purposes. Applications of deep\nlearning for segmentation of optic disk, blood vessels and retinal layer as\nwell as detection of lesions are reviewed. Recent deep learning models for\nclassification of diseases such as age-related macular degeneration,\nglaucoma,diabetic macular edema and diabetic retinopathy are also reported.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 05:57:17 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 01:59:58 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 19:57:45 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Sengupta", "Sourya", ""], ["Singh", "Amitojdeep", ""], ["Leopold", "Henry A.", ""], ["Gulati", "Tanmay", ""], ["Lakshminarayanan", "Vasudevan", ""]]}, {"id": "1812.07102", "submitter": "Edward Lee", "authors": "Liyue Shen, Katie Shpanskaya, Edward Lee, Emily McKenna, Maryam\n  Maleki, Quin Lu, Safwan Halabi, John Pauly, and Kristen Yeom", "title": "Deep Learning with Attention to Predict Gestational Age of the Fetal\n  Brain", "comments": "NIPS Machine Learning for Health Workshop 2018, spotlight\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fetal brain imaging is a cornerstone of prenatal screening and early\ndiagnosis of congenital anomalies. Knowledge of fetal gestational age is the\nkey to the accurate assessment of brain development. This study develops an\nattention-based deep learning model to predict gestational age of the fetal\nbrain. The proposed model is an end-to-end framework that combines key insights\nfrom multi-view MRI including axial, coronal, and sagittal views. The model\nalso uses age-activated weakly-supervised attention maps to enable\nrotation-invariant localization of the fetal brain among background noise. We\nevaluate our methods on the collected fetal brain MRI cohort with a large age\ndistribution from 125 to 273 days. Our extensive experiments show age\nprediction performance with R2 = 0.94 using multi-view MRI and attention.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 09:30:11 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Shen", "Liyue", ""], ["Shpanskaya", "Katie", ""], ["Lee", "Edward", ""], ["McKenna", "Emily", ""], ["Maleki", "Maryam", ""], ["Lu", "Quin", ""], ["Halabi", "Safwan", ""], ["Pauly", "John", ""], ["Yeom", "Kristen", ""]]}, {"id": "1812.07103", "submitter": "Omar Mohammed", "authors": "Omar Mohammed, Gerard Bailly, Damien Pellier", "title": "Style Transfer and Extraction for the Handwritten Letters Using Deep\n  Learning", "comments": "Accepted in ICAART 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we learn, transfer and extract handwriting styles using deep neural\nnetworks? This paper explores these questions using a deep conditioned\nautoencoder on the IRON-OFF handwriting data-set. We perform three experiments\nthat systematically explore the quality of our style extraction procedure.\nFirst, We compare our model to handwriting benchmarks using multidimensional\nperformance metrics. Second, we explore the quality of style transfer, i.e. how\nthe model performs on new, unseen writers. In both experiments, we improve the\nmetrics of state of the art methods by a large margin. Lastly, we analyze the\nlatent space of our model, and we see that it separates consistently writing\nstyles.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 13:38:46 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Mohammed", "Omar", ""], ["Bailly", "Gerard", ""], ["Pellier", "Damien", ""]]}, {"id": "1812.07104", "submitter": "Arindam Chowdhury", "authors": "Rohit Rahul, Arindam Chowdhury, Animesh, Samarth Mittal, Lovekesh Vig", "title": "Reading Industrial Inspection Sheets by Inferring Visual Relations", "comments": "Published in 3rd International Workshop on Robust Reading at Asian\n  Conference on Computer Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional mode of recording faults in heavy factory equipment has been\nvia hand marked inspection sheets, wherein a machine engineer manually marks\nthe faulty machine regions on a paper outline of the machine. Over the years,\nmillions of such inspection sheets have been recorded and the data within these\nsheets has remained inaccessible. However, with industries going digital and\nwaking up to the potential value of fault data for machine health monitoring,\nthere is an increased impetus towards digitization of these hand marked\ninspection records. To target this digitization, we propose a novel visual\npipeline combining state of the art deep learning models, with domain knowledge\nand low level vision techniques, followed by inference of visual relationships.\nOur framework is robust to the presence of both static and non-static\nbackground in the document, variability in the machine template diagrams,\nunstructured shape of graphical objects to be identified and variability in the\nstrokes of handwritten text. The proposed pipeline incorporates a capsule and\nspatial transformer network based classifier for accurate text reading, and a\ncustomized CTPN network for text detection in addition to hybrid techniques for\narrow detection and dialogue cloud removal. We have tested our approach on a\nreal world dataset of 50 inspection sheets for large containers and boilers.\nThe results are visually appealing and the pipeline achieved an accuracy of\n87.1% for text detection and 94.6% for text reading.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 13:10:14 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Rahul", "Rohit", ""], ["Chowdhury", "Arindam", ""], ["Animesh", "", ""], ["Mittal", "Samarth", ""], ["Vig", "Lovekesh", ""]]}, {"id": "1812.07105", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi", "title": "Towards Ophthalmologist Level Accurate Deep Learning System for OCT\n  Screening and Diagnosis", "comments": "JAMA submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an advanced AI based grading system for OCT images.\nThe proposed system is a very deep fully convolutional attentive classification\nnetwork trained with end to end advanced transfer learning with online random\naugmentation. It uses quasi random augmentation that outputs confidence values\nfor diseases prevalence during inference. Its a fully automated retinal OCT\nanalysis AI system capable of pathological lesions understanding without any\noffline preprocessing/postprocessing step or manual feature extraction. We\npresent a state of the art performance on the publicly available Mendeley OCT\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 14:42:10 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Haloi", "Mrinal", ""]]}, {"id": "1812.07106", "submitter": "Zhe Li", "authors": "Zhe Li, Caiwen Ding, Siyue Wang, Wujie Wen, Youwei Zhuo, Chang Liu,\n  Qinru Qiu, Wenyao Xu, Xue Lin, Xuehai Qian, Yanzhi Wang", "title": "E-RNN: Design Optimization for Efficient Recurrent Neural Networks in\n  FPGAs", "comments": "In The 25th International Symposium on High-Performance Computer\n  Architecture (HPCA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are becoming increasingly important for time\nseries-related applications which require efficient and real-time\nimplementations. The two major types are Long Short-Term Memory (LSTM) and\nGated Recurrent Unit (GRU) networks. It is a challenging task to have\nreal-time, efficient, and accurate hardware RNN implementations because of the\nhigh sensitivity to imprecision accumulation and the requirement of special\nactivation function implementations.\n  A key limitation of the prior works is the lack of a systematic design\noptimization framework of RNN model and hardware implementations, especially\nwhen the block size (or compression ratio) should be jointly optimized with RNN\ntype, layer size, etc. In this paper, we adopt the block-circulant matrix-based\nframework, and present the Efficient RNN (E-RNN) framework for FPGA\nimplementations of the Automatic Speech Recognition (ASR) application. The\noverall goal is to improve performance/energy efficiency under accuracy\nrequirement. We use the alternating direction method of multipliers (ADMM)\ntechnique for more accurate block-circulant training, and present two design\nexplorations providing guidance on block size and reducing RNN training trials.\nBased on the two observations, we decompose E-RNN in two phases: Phase I on\ndetermining RNN model to reduce computation and storage subject to accuracy\nrequirement, and Phase II on hardware implementations given RNN model,\nincluding processing element design/optimization, quantization, activation\nimplementation, etc. Experimental results on actual FPGA deployments show that\nE-RNN achieves a maximum energy efficiency improvement of 37.4$\\times$ compared\nwith ESE, and more than 2$\\times$ compared with C-LSTM, under the same\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 22:22:16 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Li", "Zhe", ""], ["Ding", "Caiwen", ""], ["Wang", "Siyue", ""], ["Wen", "Wujie", ""], ["Zhuo", "Youwei", ""], ["Liu", "Chang", ""], ["Qiu", "Qinru", ""], ["Xu", "Wenyao", ""], ["Lin", "Xue", ""], ["Qian", "Xuehai", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1812.07119", "submitter": "Nam Vo", "authors": "Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James\n  Hays", "title": "Composing Text and Image for Image Retrieval - An Empirical Odyssey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the task of image retrieval, where the input query is\nspecified in the form of an image plus some text that describes desired\nmodifications to the input image. For example, we may present an image of the\nEiffel tower, and ask the system to find images which are visually similar but\nare modified in small ways, such as being taken at nighttime instead of during\nthe day. To tackle this task, we learn a similarity metric between a target\nimage and a source image plus source text, an embedding and composing function\nsuch that target image feature is close to the source image plus text\ncomposition feature. We propose a new way to combine image and text using such\nfunction that is designed for the retrieval task. We show this outperforms\nexisting approaches on 3 different datasets, namely Fashion-200k, MIT-States\nand a new synthetic dataset we create based on CLEVR. We also show that our\napproach can be used to classify input queries, in addition to image retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 00:57:03 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Vo", "Nam", ""], ["Jiang", "Lu", ""], ["Sun", "Chen", ""], ["Murphy", "Kevin", ""], ["Li", "Li-Jia", ""], ["Fei-Fei", "Li", ""], ["Hays", "James", ""]]}, {"id": "1812.07124", "submitter": "Harshala Gammulle", "authors": "Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Multi-Level Sequence GAN for Group Activity Recognition", "comments": "Published in ACCV 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel semi-supervised, Multi-Level Sequential Generative\nAdversarial Network (MLS-GAN) architecture for group activity recognition. In\ncontrast to previous works which utilise manually annotated individual human\naction predictions, we allow the models to learn it's own internal\nrepresentations to discover pertinent sub-activities that aid the final group\nactivity recognition task. The generator is fed with person-level and\nscene-level features that are mapped temporally through LSTM networks.\nAction-based feature fusion is performed through novel gated fusion units that\nare able to consider long-term dependencies, exploring the relationships among\nall individual actions, to learn an intermediate representation or `action\ncode' for the current group activity. The network achieves its semi-supervised\nbehaviour by allowing it to perform group action classification together with\nthe adversarial real/fake validation. We perform extensive evaluations on\ndifferent architectural variants to demonstrate the importance of the proposed\narchitecture. Furthermore, we show that utilising both person-level and\nscene-level features facilitates the group activity prediction better than\nusing only person-level features. Our proposed architecture outperforms current\nstate-of-the-art results for sports and pedestrian based classification tasks\non Volleyball and Collective Activity datasets, showing it's flexible nature\nfor effective learning of group activities.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 01:21:36 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Gammulle", "Harshala", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1812.07134", "submitter": "Kenta Moriwaki", "authors": "Kenta Moriwaki, Ryota Yoshihashi, Rei Kawakami, Shaodi You, Takeshi\n  Naemura", "title": "Hybrid Loss for Learning Single-Image-based HDR Reconstruction", "comments": "20 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles high-dynamic-range (HDR) image reconstruction given only a\nsingle low-dynamic-range (LDR) image as input. While the existing methods focus\non minimizing the mean-squared-error (MSE) between the target and reconstructed\nimages, we minimize a hybrid loss that consists of perceptual and adversarial\nlosses in addition to HDR-reconstruction loss. The reconstruction loss instead\nof MSE is more suitable for HDR since it puts more weight on both over- and\nunder- exposed areas. It makes the reconstruction faithful to the input.\nPerceptual loss enables the networks to utilize knowledge about objects and\nimage structure for recovering the intensity gradients of saturated and grossly\nquantized areas. Adversarial loss helps to select the most plausible appearance\nfrom multiple solutions. The hybrid loss that combines all the three losses is\ncalculated in logarithmic space of image intensity so that the outputs retain a\nlarge dynamic range and meanwhile the learning becomes tractable. Comparative\nexperiments conducted with other state-of-the-art methods demonstrated that our\nmethod produces a leap in image quality.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 01:57:27 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Moriwaki", "Kenta", ""], ["Yoshihashi", "Ryota", ""], ["Kawakami", "Rei", ""], ["You", "Shaodi", ""], ["Naemura", "Takeshi", ""]]}, {"id": "1812.07145", "submitter": "Yunze Gao", "authors": "Yunze Gao, Yingying Chen, Jinqiao Wang, Zhen Lei, Xiao-Yu Zhang,\n  Hanqing Lu", "title": "Recurrent Calibration Network for Irregular Text Recognition", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition has received increased attention in the research\ncommunity. Text in the wild often possesses irregular arrangements, typically\nincluding perspective text, curved text, oriented text. Most existing methods\nare hard to work well for irregular text, especially for severely distorted\ntext. In this paper, we propose a novel Recurrent Calibration Network (RCN) for\nirregular scene text recognition. The RCN progressively calibrates the\nirregular text to boost the recognition performance. By decomposing the\ncalibration process into multiple steps, the irregular text can be calibrated\nto normal one step by step. Besides, in order to avoid the accumulation of lost\ninformation caused by inaccurate transformation, we further design a\nfiducial-point refinement structure to keep the integrity of text during the\nrecurrent process. Instead of the calibrated images, the coordinates of\nfiducial points are tracked and refined, which implicitly models the\ntransformation information. Based on the refined fiducial points, we estimate\nthe transformation parameters and sample from the original image at each step.\nIn this way, the original character information is preserved until the final\ntransformation. Such designs lead to optimal calibration results to boost the\nperformance of succeeding recognition. Extensive experiments on challenging\ndatasets demonstrate the superiority of our method, especially on irregular\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 02:56:17 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Gao", "Yunze", ""], ["Chen", "Yingying", ""], ["Wang", "Jinqiao", ""], ["Lei", "Zhen", ""], ["Zhang", "Xiao-Yu", ""], ["Lu", "Hanqing", ""]]}, {"id": "1812.07150", "submitter": "Mandana Hamidi-Haines", "authors": "Mandana Hamidi-Haines, Zhongang Qi, Alan Fern, Fuxin Li, Prasad\n  Tadepalli", "title": "Interactive Naming for Explaining Deep Neural Networks: A Formative\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of explaining the decisions of deep neural networks\nfor image recognition in terms of human-recognizable visual concepts. In\nparticular, given a test set of images, we aim to explain each classification\nin terms of a small number of image regions, or activation maps, which have\nbeen associated with semantic concepts by a human annotator. This allows for\ngenerating summary views of the typical reasons for classifications, which can\nhelp build trust in a classifier and/or identify example types for which the\nclassifier may not be trusted. For this purpose, we developed a user interface\nfor \"interactive naming,\" which allows a human annotator to manually cluster\nsignificant activation maps in a test set into meaningful groups called \"visual\nconcepts\". The main contribution of this paper is a systematic study of the\nvisual concepts produced by five human annotators using the interactive naming\ninterface. In particular, we consider the adequacy of the concepts for\nexplaining the classification of test-set images, correspondence of the\nconcepts to activations of individual neurons, and the inter-annotator\nagreement of visual concepts. We find that a large fraction of the activation\nmaps have recognizable visual concepts, and that there is significant agreement\nbetween the different annotators about their denotations. Our work is an\nexploratory study of the interplay between machine learning and human\nrecognition mediated by visualizations of the results of learning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 03:31:09 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 05:30:17 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Hamidi-Haines", "Mandana", ""], ["Qi", "Zhongang", ""], ["Fern", "Alan", ""], ["Li", "Fuxin", ""], ["Tadepalli", "Prasad", ""]]}, {"id": "1812.07166", "submitter": "Jiechao Ma", "authors": "Jiechao Ma, Xiang Li, Hongwei Li, Bjoern H Menze, Sen Liang, Rongguo\n  Zhang, Wei-Shi Zheng", "title": "Group-Attention Single-Shot Detector (GA-SSD): Finding Pulmonary Nodules\n  in Large-Scale CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of pulmonary nodules (PNs) can improve the survival rate of\npatients and yet is a challenging task for radiologists due to the image noise\nand artifacts in computed tomography (CT) images. In this paper, we propose a\nnovel and effective abnormality detector implementing the attention mechanism\nand group convolution on 3D single-shot detector (SSD) called group-attention\nSSD (GA-SSD). We find that group convolution is effective in extracting rich\ncontext information between continuous slices, and attention network can learn\nthe target features automatically. We collected a large-scale dataset that\ncontained 4146 CT scans with annotations of varying types and sizes of PNs\n(even PNs smaller than 3mm were annotated). To the best of our knowledge, this\ndataset is the largest cohort with relatively complete annotations for PNs\ndetection. Our experimental results show that the proposed group-attention SSD\noutperforms the classic SSD framework as well as the state-of-the-art 3DCNN,\nespecially on some challenging lesion types.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 04:41:16 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 02:54:18 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Ma", "Jiechao", ""], ["Li", "Xiang", ""], ["Li", "Hongwei", ""], ["Menze", "Bjoern H", ""], ["Liang", "Sen", ""], ["Zhang", "Rongguo", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1812.07169", "submitter": "Quanshi Zhang", "authors": "Runjin Chen, Hao Chen, Ge Huang, Jie Ren, and Quanshi Zhang", "title": "Explaining Neural Networks Semantically and Quantitatively", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to explain the knowledge encoded in a\nconvolutional neural network (CNN) quantitatively and semantically. The\nanalysis of the specific rationale of each prediction made by the CNN presents\na key issue of understanding neural networks, but it is also of significant\npractical values in certain applications. In this study, we propose to distill\nknowledge from the CNN into an explainable additive model, so that we can use\nthe explainable model to provide a quantitative explanation for the CNN\nprediction. We analyze the typical bias-interpreting problem of the explainable\nmodel and develop prior losses to guide the learning of the explainable\nadditive model. Experimental results have demonstrated the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 04:52:43 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Chen", "Runjin", ""], ["Chen", "Hao", ""], ["Huang", "Ge", ""], ["Ren", "Jie", ""], ["Zhang", "Quanshi", ""]]}, {"id": "1812.07174", "submitter": "Se Young Chun", "authors": "Kwanyoung Kim and Se Young Chun", "title": "SREdgeNet: Edge Enhanced Single Image Super Resolution using Dense Edge\n  Detection Network and Feature Merge Network", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based single image super-resolution (SR) methods have been\nrapidly evolved over the past few years and have yielded state-of-the-art\nperformances over conventional methods. Since these methods usually minimized\nl1 loss between the output SR image and the ground truth image, they yielded\nvery high peak signal-to-noise ratio (PSNR) that is inversely proportional to\nthese losses. Unfortunately, minimizing these losses inevitably lead to blurred\nedges due to averaging of plausible solutions. Recently, SRGAN was proposed to\navoid this average effect by minimizing perceptual losses instead of l1 loss\nand it yielded perceptually better SR images (or images with sharp edges) at\nthe price of lowering PSNR. In this paper, we propose SREdgeNet, edge enhanced\nsingle image SR network, that was inspired by conventional SR theories so that\naverage effect could be avoided not by changing the loss, but by changing the\nSR network property with the same l1 loss. Our SREdgeNet consists of 3\nsequential deep neural network modules: the first module is any\nstate-of-the-art SR network and we selected a variant of EDSR. The second\nmodule is any edge detection network taking the output of the first SR module\nas an input and we propose DenseEdgeNet for this module. Lastly, the third\nmodule is merging the outputs of the first and second modules to yield edge\nenhanced SR image and we propose MergeNet for this module. Qualitatively, our\nproposed method yielded images with sharp edges compared to other\nstate-of-the-art SR methods. Quantitatively, our SREdgeNet yielded\nstate-of-the-art performance in terms of structural similarity (SSIM) while\nmaintained comparable PSNR for x8 enlargement.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 05:13:41 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Kim", "Kwanyoung", ""], ["Chun", "Se Young", ""]]}, {"id": "1812.07179", "submitter": "Wei-Lun Chao", "authors": "Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark\n  Campbell, Kilian Q. Weinberger", "title": "Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object\n  Detection for Autonomous Driving", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D object detection is an essential task in autonomous driving. Recent\ntechniques excel with highly accurate detection rates, provided the 3D input\ndata is obtained from precise but expensive LiDAR technology. Approaches based\non cheaper monocular or stereo imagery data have, until now, resulted in\ndrastically lower accuracies --- a gap that is commonly attributed to poor\nimage-based depth estimation. However, in this paper we argue that it is not\nthe quality of the data but its representation that accounts for the majority\nof the difference. Taking the inner workings of convolutional neural networks\ninto consideration, we propose to convert image-based depth maps to\npseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With\nthis representation we can apply different existing LiDAR-based detection\nalgorithms. On the popular KITTI benchmark, our approach achieves impressive\nimprovements over the existing state-of-the-art in image-based performance ---\nraising the detection accuracy of objects within the 30m range from the\nprevious state-of-the-art of 22% to an unprecedented 74%. At the time of\nsubmission our algorithm holds the highest entry on the KITTI 3D object\ndetection leaderboard for stereo-image-based approaches. Our code is publicly\navailable at https://github.com/mileyan/pseudo_lidar.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 05:37:04 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 06:46:57 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 05:43:47 GMT"}, {"version": "v4", "created": "Thu, 25 Apr 2019 14:10:23 GMT"}, {"version": "v5", "created": "Fri, 14 Jun 2019 09:03:45 GMT"}, {"version": "v6", "created": "Sat, 22 Feb 2020 18:09:09 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Wang", "Yan", ""], ["Chao", "Wei-Lun", ""], ["Garg", "Divyansh", ""], ["Hariharan", "Bharath", ""], ["Campbell", "Mark", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1812.07203", "submitter": "Santhosh Kelathodi", "authors": "Santhosh Kelathodi Kumaran, Debi Prosad Dogra, Partha Pratim Roy and\n  Adway Mitra", "title": "Video Trajectory Classification and Anomaly Detection Using Hybrid\n  CNN-VAE", "comments": "First version submitted in an Journal on 8-10-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying time series data using neural networks is a challenging problem\nwhen the length of the data varies. Video object trajectories, which are key to\nmany of the visual surveillance applications, are often found to be of varying\nlength. If such trajectories are used to understand the behavior (normal or\nanomalous) of moving objects, they need to be represented correctly. In this\npaper, we propose video object trajectory classification and anomaly detection\nusing a hybrid Convolutional Neural Network (CNN) and Variational Autoencoder\n(VAE) architecture. First, we introduce a high level representation of object\ntrajectories using color gradient form. In the next stage, a semi-supervised\nway to annotate moving object trajectories extracted using Temporal Unknown\nIncremental Clustering (TUIC), has been applied for trajectory class labeling.\nAnomalous trajectories are separated using t-Distributed Stochastic Neighbor\nEmbedding (t-SNE). Finally, a hybrid CNN-VAE architecture has been used for\ntrajectory classification and anomaly detection. The results obtained using\npublicly available surveillance video datasets reveal that the proposed method\ncan successfully identify some of the important traffic anomalies such as\nvehicles not following lane driving, sudden speed variations, abrupt\ntermination of vehicle movement, and vehicles moving in wrong directions. The\nproposed method is able to detect above anomalies at higher accuracy as\ncompared to existing anomaly detection methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:19:24 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Kumaran", "Santhosh Kelathodi", ""], ["Dogra", "Debi Prosad", ""], ["Roy", "Partha Pratim", ""], ["Mitra", "Adway", ""]]}, {"id": "1812.07252", "submitter": "Stephen James", "authors": "Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov,\n  Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, Konstantinos Bousmalis", "title": "Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via\n  Randomized-to-Canonical Adaptation Networks", "comments": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world data, especially in the domain of robotics, is notoriously costly\nto collect. One way to circumvent this can be to leverage the power of\nsimulation to produce large amounts of labelled data. However, training models\non simulated images does not readily transfer to real-world ones. Using domain\nadaptation methods to cross this \"reality gap\" requires a large amount of\nunlabelled real-world data, whilst domain randomization alone can waste\nmodeling power. In this paper, we present Randomized-to-Canonical Adaptation\nNetworks (RCANs), a novel approach to crossing the visual reality gap that uses\nno real-world data. Our method learns to translate randomized rendered images\ninto their equivalent non-randomized, canonical versions. This in turn allows\nfor real images to also be translated into canonical sim images. We demonstrate\nthe effectiveness of this sim-to-real approach by training a vision-based\nclosed-loop grasping reinforcement learning agent in simulation, and then\ntransferring it to the real world to attain 70% zero-shot grasp success on\nunseen objects, a result that almost doubles the success of learning the same\ntask directly on domain randomization alone. Additionally, by joint finetuning\nin the real-world with only 5,000 real-world grasps, our method achieves 91%,\nattaining comparable performance to a state-of-the-art system trained with\n580,000 real-world grasps, resulting in a reduction of real-world data by more\nthan 99%.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 09:11:02 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 10:53:54 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 11:18:03 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["James", "Stephen", ""], ["Wohlhart", "Paul", ""], ["Kalakrishnan", "Mrinal", ""], ["Kalashnikov", "Dmitry", ""], ["Irpan", "Alex", ""], ["Ibarz", "Julian", ""], ["Levine", "Sergey", ""], ["Hadsell", "Raia", ""], ["Bousmalis", "Konstantinos", ""]]}, {"id": "1812.07260", "submitter": "Hwann-Tzong Chen", "authors": "Ding-Jie Chen, Hwann-Tzong Chen, Long-Wen Chang", "title": "SwipeCut: Interactive Segmentation with Diversified Seed Proposals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive image segmentation algorithms rely on the user to provide\nannotations as the guidance. When the task of interactive segmentation is\nperformed on a small touchscreen device, the requirement of providing precise\nannotations could be cumbersome to the user. We design an efficient seed\nproposal method that actively proposes annotation seeds for the user to label.\nThe user only needs to check which ones of the query seeds are inside the\nregion of interest (ROI). We enforce the sparsity and diversity criteria on the\nselection of the query seeds. At each round of interaction the user is only\npresented with a small number of informative query seeds that are far apart\nfrom each other. As a result, we are able to derive a user friendly interaction\nmechanism for annotation on small touchscreen devices. The user merely has to\nswipe through on the ROI-relevant query seeds, which should be easy since those\ngestures are commonly used on a touchscreen. The performance of our algorithm\nis evaluated on six publicly available datasets. The evaluation results show\nthat our algorithm achieves high segmentation accuracy, with short response\ntime and less user feedback.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 09:37:11 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Chen", "Ding-Jie", ""], ["Chen", "Hwann-Tzong", ""], ["Chang", "Long-Wen", ""]]}, {"id": "1812.07363", "submitter": "Jian Han", "authors": "Jian Han, Sezer Karaoglu, Hoang-An Le, Theo Gevers", "title": "Improving Face Detection Performance with 3D-Rendered Synthetic Data", "comments": "11 pages. Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a synthetic data generator methodology with fully\ncontrolled, multifaceted variations based on a new 3D face dataset (3DU-Face).\nWe customized synthetic datasets to address specific types of variations\n(scale, pose, occlusion, blur, etc.), and systematically investigate the\ninfluence of different variations on face detection performances. We examine\nwhether and how these factors contribute to better face detection performances.\nWe validate our synthetic data augmentation for different face detectors\n(Faster RCNN, SSH and HR) on various face datasets (MAFA, UFDD and Wider Face).\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 13:46:58 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 21:12:04 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 15:24:37 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Han", "Jian", ""], ["Karaoglu", "Sezer", ""], ["Le", "Hoang-An", ""], ["Gevers", "Theo", ""]]}, {"id": "1812.07368", "submitter": "Mustansar Fiaz", "authors": "Mustansar Fiaz, Arif Mahmood, Sajid Javed, Soon Ki Jung", "title": "Handcrafted and Deep Trackers: Recent Visual Object Tracking Approaches\n  and Trends", "comments": "27pages, 26 figures. arXiv admin note: substantial text overlap with\n  arXiv:1802.03098", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years visual object tracking has become a very active research\narea. An increasing number of tracking algorithms are being proposed each year.\nIt is because tracking has wide applications in various real world problems\nsuch as human-computer interaction, autonomous vehicles, robotics, surveillance\nand security just to name a few. In the current study, we review latest trends\nand advances in the tracking area and evaluate the robustness of different\ntrackers based on the feature extraction methods. The first part of this work\ncomprises a comprehensive survey of the recently proposed trackers. We broadly\ncategorize trackers into Correlation Filter based Trackers (CFTs) and Non-CFTs.\nEach category is further classified into various types based on the\narchitecture and the tracking mechanism. In the second part, we experimentally\nevaluated 24 recent trackers for robustness, and compared handcrafted and deep\nfeature based trackers. We observe that trackers using deep features performed\nbetter, though in some cases a fusion of both increased performance\nsignificantly. In order to overcome the drawbacks of the existing benchmarks, a\nnew benchmark Object Tracking and Temple Color (OTTC) has also been proposed\nand used in the evaluation of different algorithms. We analyze the performance\nof trackers over eleven different challenges in OTTC, and three other\nbenchmarks. Our study concludes that Discriminative Correlation Filter (DCF)\nbased trackers perform better than the others. Our study also reveals that\ninclusion of different types of regularizations over DCF often results in\nboosted tracking performance. Finally, we sum up our study by pointing out some\ninsights and indicating future trends in visual object tracking field.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 06:51:58 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 01:27:06 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Fiaz", "Mustansar", ""], ["Mahmood", "Arif", ""], ["Javed", "Sajid", ""], ["Jung", "Soon Ki", ""]]}, {"id": "1812.07390", "submitter": "Mohammad Motamedi", "authors": "Mohammad Motamedi, Felix Portillo, Daniel Fong, and Soheil Ghiasi", "title": "Distill-Net: Application-Specific Distillation of Deep Convolutional\n  Neural Networks for Resource-Constrained IoT Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Internet-of-Things (IoT) applications demand fast and accurate\nunderstanding of a few key events in their surrounding environment. Deep\nConvolutional Neural Networks (CNNs) have emerged as an effective approach to\nunderstand speech, images, and similar high dimensional data types. Algorithmic\nperformance of modern CNNs, however, fundamentally relies on learning\nclass-agnostic hierarchical features that only exist in comprehensive training\ndatasets with many classes. As a result, fast inference using CNNs trained on\nsuch datasets is prohibitive for most resource-constrained IoT platforms. To\nbridge this gap, we present a principled and practical methodology for\ndistilling a complex modern CNN that is trained to effectively recognize many\ndifferent classes of input data into an application-dependent essential core\nthat not only recognizes the few classes of interest to the application\naccurately, but also runs efficiently on platforms with limited resources.\nExperimental results confirm that our approach strikes a favorable balance\nbetween classification accuracy (application constraint), inference efficiency\n(platform constraint), and productive development of new applications (business\nconstraint).\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 02:37:03 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Motamedi", "Mohammad", ""], ["Portillo", "Felix", ""], ["Fong", "Daniel", ""], ["Ghiasi", "Soheil", ""]]}, {"id": "1812.07425", "submitter": "Luca Calatroni", "authors": "Marcelo Bertalm\\'io, Luca Calatroni, Valentina Franceschi, Benedetta\n  Franceschiello, Dario Prandi", "title": "A cortical-inspired model for orientation-dependent contrast perception:\n  a link with Wilson-Cowan equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a differential model describing neuro-physiological contrast\nperception phenomena induced by surrounding orientations. The mathematical\nformulation relies on a cortical-inspired modelling [10] largely used over the\nlast years to describe neuron interactions in the primary visual cortex (V1)\nand applied to several image processing problems [12,19,13]. Our model connects\nto Wilson-Cowan-type equations [23] and it is analogous to the one used in\n[3,2,14] to describe assimilation and contrast phenomena, the main novelty\nbeing its explicit dependence on local image orientation. To confirm the\nvalidity of the model, we report some numerical tests showing its ability to\nexplain orientation-dependent phenomena (such as grating induction) and\ngeometric-optical illusions [21,16] classically explained only by\nfiltering-based techniques [6,18].\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 15:11:52 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Bertalm\u00edo", "Marcelo", ""], ["Calatroni", "Luca", ""], ["Franceschi", "Valentina", ""], ["Franceschiello", "Benedetta", ""], ["Prandi", "Dario", ""]]}, {"id": "1812.07444", "submitter": "Gaurav Jaswal", "authors": "Avantika Singh, Gaurav Jaswal, Aditya Nigam", "title": "FDSNet: Finger dorsal image spoof detection network using light field\n  camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present spoofing attacks via which biometric system is potentially\nvulnerable against a fake biometric characteristic, introduces a great\nchallenge to recognition performance. Despite the availability of a broad range\nof presentation attack detection (PAD) or liveness detection algorithms,\nfingerprint sensors are vulnerable to spoofing via fake fingers. In such\nsituations, finger dorsal images can be thought of as an alternative which can\nbe captured without much user cooperation and are more appropriate for outdoor\nsecurity applications. In this paper, we present a first feasibility study of\nspoofing attack scenarios on finger dorsal authentication system, which include\nfour types of presentation attacks such as printed paper, wrapped printed\npaper, scan and mobile. This study also presents a CNN based spoofing attack\ndetection method which employ state-of-the-art deep learning techniques along\nwith transfer learning mechanism. We have collected 196 finger dorsal real\nimages from 33 subjects, captured with a Lytro camera and also created a set of\n784 finger dorsal spoofing images. Extensive experimental results have been\nperformed that demonstrates the superiority of the proposed approach for\nvarious spoofing attacks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 15:49:45 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Singh", "Avantika", ""], ["Jaswal", "Gaurav", ""], ["Nigam", "Aditya", ""]]}, {"id": "1812.07460", "submitter": "Julian Krebs", "authors": "Julian Krebs, Herv\\'e Delingette, Boris Mailh\\'e, Nicholas Ayache and\n  Tommaso Mansi", "title": "Learning a Probabilistic Model for Diffeomorphic Registration", "comments": "Accepted at (c) IEEE TMI and featured on https://ieee-tmi.org/", "journal-ref": null, "doi": "10.1109/TMI.2019.2897112", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn a low-dimensional probabilistic deformation model from\ndata which can be used for registration and the analysis of deformations. The\nlatent variable model maps similar deformations close to each other in an\nencoding space. It enables to compare deformations, generate normal or\npathological deformations for any new image or to transport deformations from\none image pair to any other image. Our unsupervised method is based on\nvariational inference. In particular, we use a conditional variational\nautoencoder (CVAE) network and constrain transformations to be symmetric and\ndiffeomorphic by applying a differentiable exponentiation layer with a\nsymmetric loss function. We also present a formulation that includes spatial\nregularization such as diffusion-based filters. Additionally, our framework\nprovides multi-scale velocity field estimations. We evaluated our method on 3-D\nintra-subject registration using 334 cardiac cine-MRIs. On this dataset, our\nmethod showed state-of-the-art performance with a mean DICE score of 81.2% and\na mean Hausdorff distance of 7.3mm using 32 latent dimensions compared to three\nstate-of-the-art methods while also demonstrating more regular deformation\nfields. The average time per registration was 0.32s. Besides, we visualized the\nlearned latent space and show that the encoded deformations can be used to\ntransport deformations and to cluster diseases with a classification accuracy\nof 83% after applying a linear projection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 16:22:41 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 16:11:20 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Krebs", "Julian", ""], ["Delingette", "Herv\u00e9", ""], ["Mailh\u00e9", "Boris", ""], ["Ayache", "Nicholas", ""], ["Mansi", "Tommaso", ""]]}, {"id": "1812.07509", "submitter": "Brendon Lutnick", "authors": "Brendon Lutnick, Brandon Ginley, Darshana Govind, Sean D. McGarry,\n  Peter S. LaViolette, Rabi Yacoub, Sanjay Jain, John E. Tomaszewski, Kuang-Yu\n  Jen, and Pinaki Sarder", "title": "Iterative annotation to ease neural network training: Specialized\n  machine learning in medical image analysis", "comments": "15 pages, 7 figures, 2 supplemental figures (on the last page)", "journal-ref": "Nature Machine Intelligence 1.2 (2019): 112", "doi": "10.1038/s42256-019-0018-3", "report-no": null, "categories": "eess.IV cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks promise to bring robust, quantitative analysis to medical\nfields, but adoption is limited by the technicalities of training these\nnetworks. To address this translation gap between medical researchers and\nneural networks in the field of pathology, we have created an intuitive\ninterface which utilizes the commonly used whole slide image (WSI) viewer,\nAperio ImageScope (Leica Biosystems Imaging, Inc.), for the annotation and\ndisplay of neural network predictions on WSIs. Leveraging this, we propose the\nuse of a human-in-the-loop strategy to reduce the burden of WSI annotation. We\ntrack network performance improvements as a function of iteration and quantify\nthe use of this pipeline for the segmentation of renal histologic findings on\nWSIs. More specifically, we present network performance when applied to\nsegmentation of renal micro compartments, and demonstrate multi-class\nsegmentation in human and mouse renal tissue slides. Finally, to show the\nadaptability of this technique to other medical imaging fields, we demonstrate\nits ability to iteratively segment human prostate glands from radiology imaging\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 17:29:23 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Lutnick", "Brendon", ""], ["Ginley", "Brandon", ""], ["Govind", "Darshana", ""], ["McGarry", "Sean D.", ""], ["LaViolette", "Peter S.", ""], ["Yacoub", "Rabi", ""], ["Jain", "Sanjay", ""], ["Tomaszewski", "John E.", ""], ["Jen", "Kuang-Yu", ""], ["Sarder", "Pinaki", ""]]}, {"id": "1812.07524", "submitter": "Lukasz Romaszko", "authors": "Lukasz Romaszko, Christopher K.I. Williams, John Winn", "title": "Learning Direct Optimization for Scene Understanding", "comments": null, "journal-ref": "Pattern Recognition, Volume 105, 2020, 107369", "doi": "10.1016/j.patcog.2020.107369", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Learning Direct Optimization (LiDO) method for the refinement of\na latent variable model that describes input image x. Our goal is to explain a\nsingle image x with an interpretable 3D computer graphics model having scene\ngraph latent variables z (such as object appearance, camera position). Given a\ncurrent estimate of z we can render a prediction of the image g(z), which can\nbe compared to the image x. The standard way to proceed is then to measure the\nerror E(x, g(z)) between the two, and use an optimizer to minimize the error.\nHowever, it is unknown which error measure E would be most effective for\nsimultaneously addressing issues such as misaligned objects, occlusions,\ntextures, etc. In contrast, the LiDO approach trains a Prediction Network to\npredict an update directly to correct z, rather than minimizing the error with\nrespect to z. Experiments show that our LiDO method converges rapidly as it\ndoes not need to perform a search on the error landscape, produces better\nsolutions than error-based competitors, and is able to handle the mismatch\nbetween the data and the fitted scene model. We apply LiDO to a realistic\nsynthetic dataset, and show that the method also transfers to work well with\nreal images.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 17:46:13 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 13:43:49 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Romaszko", "Lukasz", ""], ["Williams", "Christopher K. I.", ""], ["Winn", "John", ""]]}, {"id": "1812.07567", "submitter": "Sorin Grigorescu", "authors": "Sorin Grigorescu", "title": "Generative One-Shot Learning (GOL): A Semi-Parametric Approach to\n  One-Shot Learning in Autonomous Vision", "comments": "Web-site: http://rovislab.com/gol.html", "journal-ref": "Int. Conf. on Robotics and Automation ICRA 2018", "doi": "10.1109/ICRA.2018.8461174", "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly Autonomous Driving (HAD) systems rely on deep neural networks for the\nvisual perception of the driving environment. Such networks are trained on\nlarge manually annotated databases. In this work, a semi-parametric approach to\none-shot learning is proposed, with the aim of bypassing the manual annotation\nstep required for training perceptions systems used in autonomous driving. The\nproposed generative framework, coined Generative One-Shot Learning (GOL), takes\nas input single one-shot objects, or generic patterns, and a small set of\nso-called regularization samples used to drive the generative process. New\nsynthetic data is generated as Pareto optimal solutions from one-shot objects\nusing a set of generalization functions built into a generalization generator.\nGOL has been evaluated on environment perception challenges encountered in\nautonomous vision.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 04:22:15 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Grigorescu", "Sorin", ""]]}, {"id": "1812.07603", "submitter": "Ayush Tewari", "authors": "Ayush Tewari, Florian Bernard, Pablo Garrido, Gaurav Bharaj, Mohamed\n  Elgharib, Hans-Peter Seidel, Patrick P\\'erez, Michael Zollh\\\"ofer, Christian\n  Theobalt", "title": "FML: Face Model Learning from Videos", "comments": "CVPR 2019 (Oral). Video: https://www.youtube.com/watch?v=SG2BwxCw0lQ,\n  Project Page: https://gvv.mpi-inf.mpg.de/projects/FML19/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular image-based 3D reconstruction of faces is a long-standing problem\nin computer vision. Since image data is a 2D projection of a 3D face, the\nresulting depth ambiguity makes the problem ill-posed. Most existing methods\nrely on data-driven priors that are built from limited 3D face scans. In\ncontrast, we propose multi-frame video-based self-supervised training of a deep\nnetwork that (i) learns a face identity model both in shape and appearance\nwhile (ii) jointly learning to reconstruct 3D faces. Our face model is learned\nusing only corpora of in-the-wild video clips collected from the Internet. This\nvirtually endless source of training data enables learning of a highly general\n3D face model. In order to achieve this, we propose a novel multi-frame\nconsistency loss that ensures consistent shape and appearance across multiple\nframes of a subject's face, thus minimizing depth ambiguity. At test time we\ncan use an arbitrary number of frames, so that we can perform both monocular as\nwell as multi-frame reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 19:15:23 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 13:36:39 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Tewari", "Ayush", ""], ["Bernard", "Florian", ""], ["Garrido", "Pablo", ""], ["Bharaj", "Gaurav", ""], ["Elgharib", "Mohamed", ""], ["Seidel", "Hans-Peter", ""], ["P\u00e9rez", "Patrick", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""]]}, {"id": "1812.07660", "submitter": "Jun Yu", "authors": "Jun Yu, Xiao-Jun Wu, Josef Kittler", "title": "Discriminative Supervised Hashing for Cross-Modal similarity Search", "comments": "7 pages,3 figures,4 tables;The paper is under consideration at Image\n  and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advantage of low storage cost and high retrieval efficiency, hashing\ntechniques have recently been an emerging topic in cross-modal similarity\nsearch. As multiple modal data reflect similar semantic content, many\nresearches aim at learning unified binary codes. However, discriminative\nhashing features learned by these methods are not adequate. This results in\nlower accuracy and robustness. We propose a novel hashing learning framework\nwhich jointly performs classifier learning, subspace learning and matrix\nfactorization to preserve class-specific semantic content, termed\nDiscriminative Supervised Hashing (DSH), to learn the discrimative unified\nbinary codes for multi-modal data. Besides, reducing the loss of information\nand preserving the non-linear structure of data, DSH non-linearly projects\ndifferent modalities into the common space in which the similarity among\nheterogeneous data points can be measured. Extensive experiments conducted on\nthe three publicly available datasets demonstrate that the framework proposed\nin this paper outperforms several state-of -the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 04:28:31 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 08:53:49 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 02:54:33 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Yu", "Jun", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1812.07667", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "GD-GAN: Generative Adversarial Networks for Trajectory Prediction and\n  Group Detection in Crowds", "comments": "Appeared in ACCV 2108", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning framework for human trajectory\nprediction and detecting social group membership in crowds. We introduce a\ngenerative adversarial pipeline which preserves the spatio-temporal structure\nof the pedestrian's neighbourhood, enabling us to extract relevant attributes\ndescribing their social identity. We formulate the group detection task as an\nunsupervised learning problem, obviating the need for supervised learning of\ngroup memberships via hand labeled databases, allowing us to directly employ\nthe proposed framework in different surveillance settings. We evaluate the\nproposed trajectory prediction and group detection frameworks on multiple\npublic benchmarks, and for both tasks the proposed method demonstrates its\ncapability to better anticipate human sociological behaviour compared to the\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 22:20:37 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1812.07697", "submitter": "Hamad Ahmed", "authors": "Ren Li, Jared S. Johansen, Hamad Ahmed, Thomas V. Ilyevsky, Ronnie B\n  Wilbur, Hari M Bharadwaj, and Jeffrey Mark Siskind", "title": "Training on the test set? An analysis of Spampinato et al. [31]", "comments": "18 Pages, 4 Figures, 10 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent paper [31] claims to classify brain processing evoked in subjects\nwatching ImageNet stimuli as measured with EEG and to use a representation\nderived from this processing to create a novel object classifier. That paper,\ntogether with a series of subsequent papers [8, 15, 17, 20, 21, 30, 35], claims\nto revolutionize the field by achieving extremely successful results on several\ncomputer-vision tasks, including object classification, transfer learning, and\ngeneration of images depicting human perception and thought using brain-derived\nrepresentations measured through EEG. Our novel experiments and analyses\ndemonstrate that their results crucially depend on the block design that they\nuse, where all stimuli of a given class are presented together, and fail with a\nrapid-event design, where stimuli of different classes are randomly intermixed.\nThe block design leads to classification of arbitrary brain states based on\nblock-level temporal correlations that tend to exist in all EEG data, rather\nthan stimulus-related activity. Because every trial in their test sets comes\nfrom the same block as many trials in the corresponding training sets, their\nblock design thus leads to surreptitiously training on the test set. This\ninvalidates all subsequent analyses performed on this data in multiple\npublished papers and calls into question all of the purported results. We\nfurther show that a novel object classifier constructed with a random codebook\nperforms as well as or better than a novel object classifier constructed with\nthe representation extracted from EEG data, suggesting that the performance of\ntheir classifier constructed with a representation extracted from EEG data does\nnot benefit at all from the brain-derived representation. Our results calibrate\nthe underlying difficulty of the tasks involved and caution against sensational\nand overly optimistic, but false, claims to the contrary.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:38:28 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Li", "Ren", ""], ["Johansen", "Jared S.", ""], ["Ahmed", "Hamad", ""], ["Ilyevsky", "Thomas V.", ""], ["Wilbur", "Ronnie B", ""], ["Bharadwaj", "Hari M", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1812.07712", "submitter": "Ye Wang", "authors": "Ye Wang, Jongmoo Choi, Yueru Chen, Siyang Li, Qin Huang, Kaitai Zhang,\n  Ming-Sui Lee and C.-C. Jay Kuo", "title": "Unsupervised Video Object Segmentation with Distractor-Aware Online\n  Adaptation", "comments": "11 pages, 6 figures, 4 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised video object segmentation is a crucial application in video\nanalysis without knowing any prior information about the objects. It becomes\ntremendously challenging when multiple objects occur and interact in a given\nvideo clip. In this paper, a novel unsupervised video object segmentation\napproach via distractor-aware online adaptation (DOA) is proposed. DOA models\nspatial-temporal consistency in video sequences by capturing background\ndependencies from adjacent frames. Instance proposals are generated by the\ninstance segmentation network for each frame and then selected by motion\ninformation as hard negatives if they exist and positives. To adopt\nhigh-quality hard negatives, the block matching algorithm is then applied to\npreceding frames to track the associated hard negatives. General negatives are\nalso introduced in case that there are no hard negatives in the sequence and\nexperiments demonstrate both kinds of negatives (distractors) are\ncomplementary. Finally, we conduct DOA using the positive, negative, and hard\nnegative masks to update the foreground/background segmentation. The proposed\napproach achieves state-of-the-art results on two benchmark datasets, DAVIS\n2016 and FBMS-59 datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 00:45:10 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Wang", "Ye", ""], ["Choi", "Jongmoo", ""], ["Chen", "Yueru", ""], ["Li", "Siyang", ""], ["Huang", "Qin", ""], ["Zhang", "Kaitai", ""], ["Lee", "Ming-Sui", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1812.07715", "submitter": "Khalid Raza", "authors": "Khalid Raza and Nripendra Kumar Singh", "title": "A Tour of Unsupervised Deep Learning for Medical Image Analysis", "comments": "29 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation of medical images for diagnosis and treatment of complex\ndisease from high-dimensional and heterogeneous data remains a key challenge in\ntransforming healthcare. In the last few years, both supervised and\nunsupervised deep learning achieved promising results in the area of medical\nimaging and image analysis. Unlike supervised learning which is biased towards\nhow it is being supervised and manual efforts to create class label for the\nalgorithm, unsupervised learning derive insights directly from the data itself,\ngroup the data and help to make data driven decisions without any external\nbias. This review systematically presents various unsupervised models applied\nto medical image analysis, including autoencoders and its several variants,\nRestricted Boltzmann machines, Deep belief networks, Deep Boltzmann machine and\nGenerative adversarial network. Future research opportunities and challenges of\nunsupervised techniques for medical image analysis have also been discussed.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 18:42:05 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Raza", "Khalid", ""], ["Singh", "Nripendra Kumar", ""]]}, {"id": "1812.07741", "submitter": "Xiaoming Li", "authors": "Xiaoming Li, Ming Liu, Jieru Zhu, Wangmeng Zuo, Meng Wang, Guosheng\n  Hu, Lei Zhang", "title": "Learning Symmetry Consistent Deep CNNs for Face Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks (CNNs) have achieved great success in face\ncompletion to generate plausible facial structures. These methods, however, are\nlimited in maintaining global consistency among face components and recovering\nfine facial details. On the other hand, reflectional symmetry is a prominent\nproperty of face image and benefits face recognition and consistency modeling,\nyet remaining uninvestigated in deep face completion. In this work, we leverage\ntwo kinds of symmetry-enforcing subnets to form a symmetry-consistent CNN model\n(i.e., SymmFCNet) for effective face completion. For missing pixels on only one\nof the half-faces, an illumination-reweighted warping subnet is developed to\nguide the warping and illumination reweighting of the other half-face. As for\nmissing pixels on both of half-faces, we present a generative reconstruction\nsubnet together with a perceptual symmetry loss to enforce symmetry consistency\nof recovered structures. The SymmFCNet is constructed by stacking generative\nreconstruction subnet upon illumination-reweighted warping subnet, and can be\nend-to-end learned from training set of unaligned face images. Experiments show\nthat SymmFCNet can generate high quality results on images with synthetic and\nreal occlusion, and performs favorably against state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 03:25:45 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Li", "Xiaoming", ""], ["Liu", "Ming", ""], ["Zhu", "Jieru", ""], ["Zuo", "Wangmeng", ""], ["Wang", "Meng", ""], ["Hu", "Guosheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1812.07742", "submitter": "Yuan Zong", "authors": "Yuan Zong, Tong Zhang, Wenming Zheng, Xiaopeng Hong, Chuangao Tang,\n  Zhen Cui, Guoying Zhao", "title": "Cross-Database Micro-Expression Recognition: A Benchmark", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-database micro-expression recognition (CDMER) is one of recently\nemerging and interesting problem in micro-expression analysis. CDMER is more\nchallenging than the conventional micro-expression recognition (MER), because\nthe training and testing samples in CDMER come from different micro-expression\ndatabases, resulting in the inconsistency of the feature distributions between\nthe training and testing sets. In this paper, we contribute to this topic from\nthree aspects. First, we establish a CDMER experimental evaluation protocol\naiming to allow the researchers to conveniently work on this topic and provide\na standard platform for evaluating their proposed methods. Second, we conduct\nbenchmark experiments by using NINE state-of-the-art domain adaptation (DA)\nmethods and SIX popular spatiotemporal descriptors for respectively\ninvestigating CDMER problem from two different perspectives. Third, we propose\na novel DA method called region selective transfer regression (RSTR) to deal\nwith the CDMER task. Our RSTR takes advantage of one important cue for\nrecognizing micro-expressions, i.e., the different contributions of the facial\nlocal regions in MER. The overall superior performance of RSTR demonstrates\nthat taking into consideration the important cues benefiting MER, e.g., the\nfacial local region information, contributes to develop effective DA methods\nfor dealing with CDMER problem.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 03:26:44 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 10:38:43 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Zong", "Yuan", ""], ["Zhang", "Tong", ""], ["Zheng", "Wenming", ""], ["Hong", "Xiaopeng", ""], ["Tang", "Chuangao", ""], ["Cui", "Zhen", ""], ["Zhao", "Guoying", ""]]}, {"id": "1812.07749", "submitter": "Jie Yang", "authors": "Xinyang Feng, Jie Yang, Andrew F. Laine, Elsa D. Angelini", "title": "Discriminative analysis of the human cortex using spherical CNNs - a\n  study on Alzheimer's disease diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroimaging studies, the human cortex is commonly modeled as a sphere to\npreserve the topological structure of the cortical surface. Cortical\nneuroimaging measures hence can be modeled in spherical representation. In this\nwork, we explore analyzing the human cortex using spherical CNNs in an\nAlzheimer's disease (AD) classification task using cortical morphometric\nmeasures derived from structural MRI. Our results show superior performance in\nclassifying AD versus cognitively normal and in predicting MCI progression\nwithin two years, using structural MRI information only. This work demonstrates\nfor the first time the potential of the spherical CNNs framework in the\ndiscriminative analysis of the human cortex and could be extended to other\nmodalities and other neurological diseases.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 04:16:18 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Feng", "Xinyang", ""], ["Yang", "Jie", ""], ["Laine", "Andrew F.", ""], ["Angelini", "Elsa D.", ""]]}, {"id": "1812.07760", "submitter": "Yilun Chen", "authors": "Yilun Chen, Praveen Palanisamy, Priyantha Mudalige, Katharina\n  Muelling, John M. Dolan", "title": "Learning On-Road Visual Control for Self-Driving Vehicles with Auxiliary\n  Tasks", "comments": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A safe and robust on-road navigation system is a crucial component of\nachieving fully automated vehicles. NVIDIA recently proposed an End-to-End\nalgorithm that can directly learn steering commands from raw pixels of a front\ncamera by using one convolutional neural network. In this paper, we leverage\nauxiliary information aside from raw images and design a novel network\nstructure, called Auxiliary Task Network (ATN), to help boost the driving\nperformance while maintaining the advantage of minimal training data and an\nEnd-to-End training method. In this network, we introduce human prior knowledge\ninto vehicle navigation by transferring features from image recognition tasks.\nImage semantic segmentation is applied as an auxiliary task for navigation. We\nconsider temporal information by introducing an LSTM module and optical flow to\nthe network. Finally, we combine vehicle kinematics with a sensor fusion step.\nWe discuss the benefits of our method over state-of-the-art visual navigation\nmethods both in the Udacity simulation environment and on the real-world\nComma.ai dataset.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 05:29:53 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Chen", "Yilun", ""], ["Palanisamy", "Praveen", ""], ["Mudalige", "Priyantha", ""], ["Muelling", "Katharina", ""], ["Dolan", "John M.", ""]]}, {"id": "1812.07762", "submitter": "Se Young Chun", "authors": "Dongwon Park, Yonghyeok Seo, Se Young Chun", "title": "Real-Time, Highly Accurate Robotic Grasp Detection using Fully\n  Convolutional Neural Network with Rotation Ensemble Module", "comments": "7 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation invariance has been an important topic in computer vision tasks.\nIdeally, robot grasp detection should be rotation-invariant. However,\nrotation-invariance in robotic grasp detection has been only recently studied\nby using rotation anchor box that are often time-consuming and unreliable for\nmultiple objects. In this paper, we propose a rotation ensemble module (REM)\nfor robotic grasp detection using convolutions that rotates network weights.\nOur proposed REM was able to outperform current state-of-the-art methods by\nachieving up to 99.2% (image-wise), 98.6% (object-wise) accuracies on the\nCornell dataset with real-time computation (50 frames per second). Our proposed\nmethod was also able to yield reliable grasps for multiple objects and up to\n93.8% success rate for the real-time robotic grasping task with a 4-axis robot\narm for small novel objects that was significantly higher than the baseline\nmethods by 11-56%.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 05:38:47 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 07:47:52 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 04:50:27 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Park", "Dongwon", ""], ["Seo", "Yonghyeok", ""], ["Chun", "Se Young", ""]]}, {"id": "1812.07763", "submitter": "Chuangye Zhang", "authors": "Chuangye Zhang, Yan Niu, Tieru Wu and Ximing Li", "title": "Light Weight Color Image Warping with Inter-Channel Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Image warping is a necessary step in many multimedia applications such as\ntexture mapping, image-based rendering, panorama stitching, image resizing and\noptical flow computation etc. Traditionally, color image warping interpolation\nis performed in each color channel independently. In this paper, we show that\nthe warping quality can be significantly enhanced by exploiting the\ncross-channel correlation. We design a warping scheme that integrates\nintra-channel interpolation with cross-channel variation at very low\ncomputational cost, which is required for interactive multimedia applications\non mobile devices. The effectiveness and efficiency of our method are validated\nby extensive experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 05:43:49 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Zhang", "Chuangye", ""], ["Niu", "Yan", ""], ["Wu", "Tieru", ""], ["Li", "Ximing", ""]]}, {"id": "1812.07770", "submitter": "Tian-Zhu Xiang", "authors": "Tian-Zhu Xiang, Gui-Song Xia, Liangpei Zhang", "title": "Mini-Unmanned Aerial Vehicle-Based Remote Sensing: Techniques,\n  Applications, and Prospects", "comments": null, "journal-ref": "IEEE Geoscience and Remote Sensing Magazine, 2019, Vol. 7, No. 3,\n  pp. 29-63", "doi": "10.1109/MGRS.2019.2918840", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few decades have witnessed the great progress of unmanned aircraft\nvehicles (UAVs) in civilian fields, especially in photogrammetry and remote\nsensing. In contrast with the platforms of manned aircraft and satellite, the\nUAV platform holds many promising characteristics: flexibility, efficiency,\nhigh-spatial/temporal resolution, low cost, easy operation, etc., which make it\nan effective complement to other remote-sensing platforms and a cost-effective\nmeans for remote sensing. Considering the popularity and expansion of UAV-based\nremote sensing in recent years, this paper provides a systematic survey on the\nrecent advances and future prospectives of UAVs in the remote-sensing\ncommunity. Specifically, the main challenges and key technologies of\nremote-sensing data processing based on UAVs are discussed and summarized\nfirstly. Then, we provide an overview of the widespread applications of UAVs in\nremote sensing. Finally, some prospects for future work are discussed. We hope\nthis paper will provide remote-sensing researchers an overall picture of recent\nUAV-based remote sensing developments and help guide the further research on\nthis topic.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 06:12:09 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 03:49:24 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 13:08:45 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Xiang", "Tian-Zhu", ""], ["Xia", "Gui-Song", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1812.07809", "submitter": "Paul Pu Liang", "authors": "Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe Morency,\n  Barnabas Poczos", "title": "Found in Translation: Learning Robust Joint Representations by Cyclic\n  Translations Between Modalities", "comments": "AAAI 2019, code available at https://github.com/hainow/MCTN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal sentiment analysis is a core research area that studies speaker\nsentiment expressed from the language, visual, and acoustic modalities. The\ncentral challenge in multimodal learning involves inferring joint\nrepresentations that can process and relate information from these modalities.\nHowever, existing work learns joint representations by requiring all modalities\nas input and as a result, the learned representations may be sensitive to noisy\nor missing modalities at test time. With the recent success of sequence to\nsequence (Seq2Seq) models in machine translation, there is an opportunity to\nexplore new ways of learning joint representations that may not require all\ninput modalities at test time. In this paper, we propose a method to learn\nrobust joint representations by translating between modalities. Our method is\nbased on the key insight that translation from a source to a target modality\nprovides a method of learning joint representations using only the source\nmodality as input. We augment modality translations with a cycle consistency\nloss to ensure that our joint representations retain maximal information from\nall modalities. Once our translation model is trained with paired multimodal\ndata, we only need data from the source modality at test time for final\nsentiment prediction. This ensures that our model remains robust from\nperturbations or missing information in the other modalities. We train our\nmodel with a coupled translation-prediction objective and it achieves new\nstate-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI,\nICT-MMMO, and YouTube. Additional experiments show that our model learns\nincreasingly discriminative joint representations with more input modalities\nwhile maintaining robustness to missing or perturbed modalities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:38:21 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 09:20:33 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Pham", "Hai", ""], ["Liang", "Paul Pu", ""], ["Manzini", "Thomas", ""], ["Morency", "Louis-Philippe", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1812.07816", "submitter": "Haruki Imai", "authors": "Haruki Imai, Samuel Matzek, Tung D. Le, Yasushi Negishi, Kiyokuni\n  Kawachiya", "title": "Fast and Accurate 3D Medical Image Segmentation with Data-swapping\n  Method", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models used for medical image segmentation are large\nbecause they are trained with high-resolution three-dimensional (3D) images.\nGraphics processing units (GPUs) are widely used to accelerate the trainings.\nHowever, the memory on a GPU is not large enough to train the models. A popular\napproach to tackling this problem is patch-based method, which divides a large\nimage into small patches and trains the models with these small patches.\nHowever, this method would degrade the segmentation quality if a target object\nspans multiple patches. In this paper, we propose a novel approach for 3D\nmedical image segmentation that utilizes the data-swapping, which swaps out\nintermediate data from GPU memory to CPU memory to enlarge the effective GPU\nmemory size, for training high-resolution 3D medical images without patching.\nWe carefully tuned parameters in the data-swapping method to obtain the best\ntraining performance for 3D U-Net, a widely used deep neural network model for\nmedical image segmentation. We applied our tuning to train 3D U-Net with\nfull-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result,\ncommunication overhead, which is the most important issue, was reduced by\n17.1%. Compared with the patch-based method for patches of 128 x 128 x 128\nvoxels, our training for full-size images achieved improvement on the mean Dice\nscore by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core\nsub-region, respectively. The total training time was reduced from 164 hours to\n47 hours, resulting in 3.53 times of acceleration.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:49:50 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Imai", "Haruki", ""], ["Matzek", "Samuel", ""], ["Le", "Tung D.", ""], ["Negishi", "Yasushi", ""], ["Kawachiya", "Kiyokuni", ""]]}, {"id": "1812.07832", "submitter": "Bruno Lecouat", "authors": "Bruno Lecouat, Ken Chang, Chuan-Sheng Foo, Balagopal Unnikrishnan,\n  James M. Brown, Houssam Zenati, Andrew Beers, Vijay Chandrasekhar, Jayashree\n  Kalpathy-Cramer, Pavitra Krishnaswamy", "title": "Semi-Supervised Deep Learning for Abnormality Classification in Retinal\n  Images", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/227", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning algorithms have enabled significant performance\ngains in medical image classification tasks. But these methods rely on large\nlabeled datasets that require resource-intensive expert annotation.\nSemi-supervised generative adversarial network (GAN) approaches offer a means\nto learn from limited labeled data alongside larger unlabeled datasets, but\nhave not been applied to discern fine-scale, sparse or localized features that\ndefine medical abnormalities. To overcome these limitations, we propose a\npatch-based semi-supervised learning approach and evaluate performance on\nclassification of diabetic retinopathy from funduscopic images. Our\nsemi-supervised approach achieves high AUC with just 10-20 labeled training\nimages, and outperforms the supervised baselines by upto 15% when less than 30%\nof the training dataset is labeled. Further, our method implicitly enables\ninterpretation of the SSL predictions. As this approach enables good accuracy,\nresolution and interpretability with lower annotation burden, it sets the\npathway for scalable applications of deep learning in clinical imaging.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 09:18:56 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Lecouat", "Bruno", ""], ["Chang", "Ken", ""], ["Foo", "Chuan-Sheng", ""], ["Unnikrishnan", "Balagopal", ""], ["Brown", "James M.", ""], ["Zenati", "Houssam", ""], ["Beers", "Andrew", ""], ["Chandrasekhar", "Vijay", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Krishnaswamy", "Pavitra", ""]]}, {"id": "1812.07857", "submitter": "Rashidedin Jahandideh", "authors": "Rashidedin Jahandideh, Alireza Tavakoli Targhi, Maryam Tahmasbi", "title": "Physical Attribute Prediction Using Deep Residual Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images taken from the Internet have been used alongside Deep Learning for\nmany different tasks such as: smile detection, ethnicity, hair style, hair\ncolour, gender and age prediction. After witnessing these usages, we were\nwondering what other attributes can be predicted from facial images available\non the Internet. In this paper we tackle the prediction of physical attributes\nfrom face images using Convolutional Neural Networks trained on our dataset\nnamed FIRW. We crawled around 61, 000 images from the web, then use face\ndetection to crop faces from these real world images. We choose ResNet-50 as\nour base network architecture. This network was pretrained for the task of face\nrecognition by using the VGG-Face dataset, and we finetune it by using our own\ndataset to predict physical attributes. Separate networks are trained for the\nprediction of body type, ethnicity, gender, height and weight; our models\nachieve the following accuracies for theses tasks, respectively: 84.58%,\n87.34%, 97.97%, 70.51%, 63.99%. To validate our choice of ResNet-50 as the base\narchitecture, we also tackle the famous CelebA dataset. Our models achieve an\naveragy accuracy of 91.19% on CelebA, which is comparable to state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 10:19:19 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Jahandideh", "Rashidedin", ""], ["Targhi", "Alireza Tavakoli", ""], ["Tahmasbi", "Maryam", ""]]}, {"id": "1812.07868", "submitter": "Manh Duong Phung", "authors": "Q. Zhu, T. H. Dinh, V. T. Hoang, M. D. Phung, Q. P. Ha", "title": "Crack Detection Using Enhanced Thresholding on UAV based Collected\n  Images", "comments": "In Proceedings of Australian Conference on Robotics and Automation\n  2018 (ACRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a thresholding approach for crack detection in an\nunmanned aerial vehicle (UAV) based infrastructure inspection system. The\nproposed algorithm performs recursively on the intensity histogram of UAV-taken\nimages to exploit their crack-pixels appearing at the low intensity interval. A\nquantified criterion of interclass contrast is proposed and employed as an\nobject cost and stop condition for the recursive process. Experiments on\ndifferent datasets show that our algorithm outperforms different segmentation\napproaches to accurately extract crack features of some commercial buildings.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 10:49:15 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Zhu", "Q.", ""], ["Dinh", "T. H.", ""], ["Hoang", "V. T.", ""], ["Phung", "M. D.", ""], ["Ha", "Q. P.", ""]]}, {"id": "1812.07869", "submitter": "Jinqiang Bai", "authors": "Yimin Lin, Zhaoxiang Liu, Jianfeng Huang, Chaopeng Wang, Guoguang Du,\n  Jinqiang Bai, Shiguo Lian, Bill Huang", "title": "Deep Global-Relative Networks for End-to-End 6-DoF Visual Localization\n  and Odometry", "comments": "7 pages, 6 figures", "journal-ref": "2019 The Pacific Rim International Conferences on Artificial\n  Intelligence (PRICAI)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a wide variety of deep neural networks for robust Visual Odometry\n(VO) can be found in the literature, they are still unable to solve the drift\nproblem in long-term robot navigation. Thus, this paper aims to propose novel\ndeep end-to-end networks for long-term 6-DoF VO task. It mainly fuses relative\nand global networks based on Recurrent Convolutional Neural Networks (RCNNs) to\nimprove the monocular localization accuracy. Indeed, the relative sub-networks\nare implemented to smooth the VO trajectory, while global subnetworks are\ndesigned to avoid drift problem. All the parameters are jointly optimized using\nCross Transformation Constraints (CTC), which represents temporal geometric\nconsistency of the consecutive frames, and Mean Square Error (MSE) between the\npredicted pose and ground truth. The experimental results on both indoor and\noutdoor datasets show that our method outperforms other state-of-the-art\nlearning-based VO methods in terms of pose accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 10:51:09 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 08:11:42 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Lin", "Yimin", ""], ["Liu", "Zhaoxiang", ""], ["Huang", "Jianfeng", ""], ["Wang", "Chaopeng", ""], ["Du", "Guoguang", ""], ["Bai", "Jinqiang", ""], ["Lian", "Shiguo", ""], ["Huang", "Bill", ""]]}, {"id": "1812.07870", "submitter": "Yinglong Wang", "authors": "Yinglong Wang, Shuaicheng Liu, Bing Zeng", "title": "Removing rain streaks by a linear model", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing rain streaks from a single image continues to draw attentions today\nin outdoor vision systems. In this paper, we present an efficient method to\nremove rain streaks. First, the location map of rain pixels needs to be known\nas precisely as possible, to which we implement a relatively accurate detection\nof rain streaks by utilizing two characteristics of rain streaks.The key\ncomponent of our method is to represent the intensity of each detected rain\npixel using a linear model: $p=\\alpha s + \\beta$, where $p$ is the observed\nintensity of a rain pixel and $s$ represents the intensity of the background\n(i.e., before rain-affected). To solve $\\alpha$ and $\\beta$ for each detected\nrain pixel, we concentrate on a window centered around it and form an\n$L_2$-norm cost function by considering all detected rain pixels within the\nwindow, where the corresponding rain-removed intensity of each detected rain\npixel is estimated by some neighboring non-rain pixels. By minimizing this cost\nfunction, we determine $\\alpha$ and $\\beta$ so as to construct the final\nrain-removed pixel intensity. Compared with several state-of-the-art works, our\nproposed method can remove rain streaks from a single color image much more\nefficiently - it offers not only a better visual quality but also a speed-up of\nseveral times to one degree of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 10:51:50 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Wang", "Yinglong", ""], ["Liu", "Shuaicheng", ""], ["Zeng", "Bing", ""]]}, {"id": "1812.07907", "submitter": "Qi Dou", "authors": "Qi Dou, Cheng Ouyang, Cheng Chen, Hao Chen, Ben Glocker, Xiahai\n  Zhuang, and Pheng-Ann Heng", "title": "PnP-AdaNet: Plug-and-Play Adversarial Domain Adaptation Network with a\n  Benchmark at Cross-modality Cardiac Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have demonstrated the state-of-the-art\nperformance on various medical image computing tasks. Leveraging images from\ndifferent modalities for the same analysis task holds clinical benefits.\nHowever, the generalization capability of deep models on test data with\ndifferent distributions remain as a major challenge. In this paper, we propose\nthe PnPAdaNet (plug-and-play adversarial domain adaptation network) for\nadapting segmentation networks between different modalities of medical images,\ne.g., MRI and CT. We propose to tackle the significant domain shift by aligning\nthe feature spaces of source and target domains in an unsupervised manner.\nSpecifically, a domain adaptation module flexibly replaces the early encoder\nlayers of the source network, and the higher layers are shared between domains.\nWith adversarial learning, we build two discriminators whose inputs are\nrespectively multi-level features and predicted segmentation masks. We have\nvalidated our domain adaptation method on cardiac structure segmentation in\nunpaired MRI and CT. The experimental results with comprehensive ablation\nstudies demonstrate the excellent efficacy of our proposed PnP-AdaNet.\nMoreover, we introduce a novel benchmark on the cardiac dataset for the task of\nunsupervised cross-modality domain adaptation. We will make our code and\ndatabase publicly available, aiming to promote future studies on this\nchallenging yet important research topic in medical imaging.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 12:21:36 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Dou", "Qi", ""], ["Ouyang", "Cheng", ""], ["Chen", "Cheng", ""], ["Chen", "Hao", ""], ["Glocker", "Ben", ""], ["Zhuang", "Xiahai", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1812.07933", "submitter": "Mikhail Povolotskiy", "authors": "M.A. Povolotskiy and D.V. Tropin", "title": "Dynamic Programming Approach to Template-based OCR", "comments": "8 pages, 5 figures, 1 table", "journal-ref": null, "doi": "10.1117/12.2522974", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a dynamic programming solution to the template-based\nrecognition task in OCR case. We formulate a problem of optimal position search\nfor complex objects consisting of parts forming a sequence. We limit the\ndistance between every two adjacent elements with predefined upper and lower\nthresholds. We choose the sum of penalties for each part in given position as a\nfunction to be minimized. We show that such a choice of restrictions allows a\nfaster algorithm to be used than the one for the general form of deformation\npenalties. We named this algorithm Dynamic Squeezeboxes Packing (DSP) and\napplied it to solve the two OCR problems: text fields extraction from an image\nof document Visual Inspection Zone (VIZ) and license plate segmentation. The\nquality and the performance of resulting solutions were experimentally proved\nto meet the requirements of the state-of-the-art industrial recognition\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 13:22:00 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Povolotskiy", "M. A.", ""], ["Tropin", "D. V.", ""]]}, {"id": "1812.07971", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Rigid Body Structure and Motion From Two-Frame Point-Correspondences\n  Under Perspective Projection", "comments": "arXiv admin note: text overlap with arXiv:1705.03986", "journal-ref": "M.A. K{\\l}opotek: Rigid Body Structure and Motion From Two-Frame\n  Point-Correspondences Under Perspective Projection. Machine Graphics & Vision\n  4 (1995)3-4, pp. 187-202", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with possibility of recovery of motion and structure\nparameters from multiframes under perspective projection when only points on a\nrigid body are traced. Free (unrestricted and uncontrolled) pattern of motion\nbetween frames is assumed. The major question is how many points and/or how\nmany frames are necessary for the task. It has been shown in an earlier paper\n{Klopotek:95b} that for orthogonal projection two frames are insufficient for\nthe task. The paper demonstrates that, under perspective projection, that total\nuncertainty about relative position of focal point versus projection plane\nmakes the recovery of structure and motion from two frames impossible.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 08:22:20 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1812.07976", "submitter": "Binbin Xu", "authors": "Binbin Xu, Wenbin Li, Dimos Tzoumanikas, Michael Bloesch, Andrew\n  Davison, Stefan Leutenegger", "title": "MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic SLAM", "comments": "Accepted to International Conference on Robotics and Automation\n  (ICRA) 2019. 7 (6 + 1) pages. Please also see video Link:\n  https://youtu.be/gturboNl9gg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new multi-instance dynamic RGB-D SLAM system using an\nobject-level octree-based volumetric representation. It can provide robust\ncamera tracking in dynamic environments and at the same time, continuously\nestimate geometric, semantic, and motion properties for arbitrary objects in\nthe scene. For each incoming frame, we perform instance segmentation to detect\nobjects and refine mask boundaries using geometric and motion information.\nMeanwhile, we estimate the pose of each existing moving object using an\nobject-oriented tracking method and robustly track the camera pose against the\nstatic scene. Based on the estimated camera pose and object poses, we associate\nsegmented masks with existing models and incrementally fuse corresponding\ncolour, depth, semantic, and foreground object probabilities into each object\nmodel. In contrast to existing approaches, our system is the first system to\ngenerate an object-level dynamic volumetric map from a single RGB-D camera,\nwhich can be used directly for robotic tasks. Our method can run at 2-3 Hz on a\nCPU, excluding the instance segmentation part. We demonstrate its effectiveness\nby quantitatively and qualitatively testing it on both synthetic and real-world\nsequences.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 14:43:05 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 17:13:15 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 16:52:32 GMT"}, {"version": "v4", "created": "Thu, 21 Mar 2019 23:41:56 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Xu", "Binbin", ""], ["Li", "Wenbin", ""], ["Tzoumanikas", "Dimos", ""], ["Bloesch", "Michael", ""], ["Davison", "Andrew", ""], ["Leutenegger", "Stefan", ""]]}, {"id": "1812.07989", "submitter": "Xiaodan Zhang", "authors": "Xiaodan Zhang, Xinbo Gao, Wen Lu, and Lihuo He", "title": "A Gated Peripheral-Foveal Convolutional Neural Network for Unified Image\n  Aesthetic Prediction", "comments": "Add more experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning fine-grained details is a key issue in image aesthetic assessment.\nMost of the previous methods extract the fine-grained details via random\ncropping strategy, which may undermine the integrity of semantic information.\nExtensive studies show that humans perceive fine-grained details with a mixture\nof foveal vision and peripheral vision. Fovea has the highest possible visual\nacuity and is responsible for seeing the details. The peripheral vision is used\nfor perceiving the broad spatial scene and selecting the attended regions for\nthe fovea. Inspired by these observations, we propose a Gated Peripheral-Foveal\nConvolutional Neural Network (GPF-CNN). It is a dedicated double-subnet neural\nnetwork, i.e. a peripheral subnet and a foveal subnet. The former aims to mimic\nthe functions of peripheral vision to encode the holistic information and\nprovide the attended regions. The latter aims to extract fine-grained features\non these key regions. Considering that the peripheral vision and foveal vision\nplay different roles in processing different visual stimuli, we further employ\na gated information fusion (GIF) network to weight their contributions. The\nweights are determined through the fully connected layers followed by a sigmoid\nfunction. We conduct comprehensive experiments on the standard AVA and\nPhoto.net datasets for unified aesthetic prediction tasks: (i) aesthetic\nquality classification; (ii) aesthetic score regression; and (iii) aesthetic\nscore distribution prediction. The experimental results demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 14:57:06 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 08:04:30 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Zhang", "Xiaodan", ""], ["Gao", "Xinbo", ""], ["Lu", "Wen", ""], ["He", "Lihuo", ""]]}, {"id": "1812.07996", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu", "title": "Mining Interpretable AOG Representations from Convolutional Networks via\n  Active Question Answering", "comments": "arXiv admin note: text overlap with arXiv:1704.03173", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method to mine object-part patterns from\nconv-layers of a pre-trained convolutional neural network (CNN). The mined\nobject-part patterns are organized by an And-Or graph (AOG). This interpretable\nAOG representation consists of a four-layer semantic hierarchy, i.e., semantic\nparts, part templates, latent patterns, and neural units. The AOG associates\neach object part with certain neural units in feature maps of conv-layers. The\nAOG is constructed in a weakly-supervised manner, i.e., very few annotations\n(e.g., 3-20) of object parts are used to guide the learning of AOGs. We develop\na question-answering (QA) method that uses active human-computer communications\nto mine patterns from a pre-trained CNN, in order to incrementally explain more\nfeatures in conv-layers. During the learning process, our QA method uses the\ncurrent AOG for part localization. The QA method actively identifies objects,\nwhose feature maps cannot be explained by the AOG. Then, our method asks people\nto annotate parts on the unexplained objects, and uses answers to discover CNN\npatterns corresponding to the newly labeled parts. In this way, our method\ngradually grows new branches and refines existing branches on the AOG to\nsemanticize CNN representations. In experiments, our method exhibited a high\nlearning efficiency. Our method used about 1/6-1/3 of the part annotations for\ntraining, but achieved similar or better part-localization performance than\nfast-RCNN methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 05:49:36 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Zhang", "Quanshi", ""], ["Cao", "Ruiming", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1812.07997", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Xin Wang, Ruiming Cao, Ying Nian Wu, Feng Shi,\n  Song-Chun Zhu", "title": "Explanatory Graphs for CNNs", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.01785", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a graphical model, namely an explanatory graph, which\nreveals the knowledge hierarchy hidden inside conv-layers of a pre-trained CNN.\nEach filter in a conv-layer of a CNN for object classification usually\nrepresents a mixture of object parts. We develop a simple yet effective method\nto disentangle object-part pattern components from each filter. We construct an\nexplanatory graph to organize the mined part patterns, where a node represents\na part pattern, and each edge encodes co-activation relationships and spatial\nrelationships between patterns. More crucially, given a pre-trained CNN, the\nexplanatory graph is learned without a need of annotating object parts.\nExperiments show that each graph node consistently represented the same object\npart through different images, which boosted the transferability of CNN\nfeatures. We transferred part patterns in the explanatory graph to the task of\npart localization, and our method significantly outperformed other approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 06:33:49 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Zhang", "Quanshi", ""], ["Wang", "Xin", ""], ["Cao", "Ruiming", ""], ["Wu", "Ying Nian", ""], ["Shi", "Feng", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1812.08008", "submitter": "Zhe Cao", "authors": "Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, Yaser Sheikh", "title": "OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity\n  Fields", "comments": "Journal version of arXiv:1611.08050, with better accuracy and faster\n  speed, release a new foot keypoint dataset:\n  https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realtime multi-person 2D pose estimation is a key component in enabling\nmachines to have an understanding of people in images and videos. In this work,\nwe present a realtime approach to detect the 2D pose of multiple people in an\nimage. The proposed method uses a nonparametric representation, which we refer\nto as Part Affinity Fields (PAFs), to learn to associate body parts with\nindividuals in the image. This bottom-up system achieves high accuracy and\nrealtime performance, regardless of the number of people in the image. In\nprevious work, PAFs and body part location estimation were refined\nsimultaneously across training stages. We demonstrate that a PAF-only\nrefinement rather than both PAF and body part location refinement results in a\nsubstantial increase in both runtime performance and accuracy. We also present\nthe first combined body and foot keypoint detector, based on an internal\nannotated foot dataset that we have publicly released. We show that the\ncombined detector not only reduces the inference time compared to running them\nsequentially, but also maintains the accuracy of each component individually.\nThis work has culminated in the release of OpenPose, the first open-source\nrealtime system for multi-person 2D pose detection, including body, foot, hand,\nand facial keypoints.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 18:50:33 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 23:46:18 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Cao", "Zhe", ""], ["Hidalgo", "Gines", ""], ["Simon", "Tomas", ""], ["Wei", "Shih-En", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1812.08028", "submitter": "Filippos Gouidis Mr.", "authors": "Filippos Gouidis, Paschalis Panteleris, Iason Oikonomidis, Antonis\n  Argyros", "title": "Accurate Hand Keypoint Localization on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for 2D hand keypoint localization from regular\ncolor input. The proposed approach relies on an appropriately designed\nConvolutional Neural Network (CNN) that computes a set of heatmaps, one per\nhand keypoint of interest. Extensive experiments with the proposed method\ncompare it against state of the art approaches and demonstrate its accuracy and\ncomputational performance on standard, publicly available datasets. The\nobtained results demonstrate that the proposed method matches or outperforms\nthe competing methods in accuracy, but clearly outperforms them in\ncomputational efficiency, making it a suitable building block for applications\nthat require hand keypoint estimation on mobile devices.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 15:39:13 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Gouidis", "Filippos", ""], ["Panteleris", "Paschalis", ""], ["Oikonomidis", "Iason", ""], ["Argyros", "Antonis", ""]]}, {"id": "1812.08043", "submitter": "Sanketh Vedula", "authors": "Sanketh Vedula, Ortal Senouf, Grigoriy Zurakhov, Alex Bronstein, Oleg\n  Michailovich, Michael Zibulevsky", "title": "Learning beamforming in ultrasound imaging", "comments": null, "journal-ref": "Proceedings of The 2nd International Conference on Medical Imaging\n  with Deep Learning, PMLR 102:493-511, 2019", "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical ultrasound (US) is a widespread imaging modality owing its popularity\nto cost efficiency, portability, speed, and lack of harmful ionizing radiation.\nIn this paper, we demonstrate that replacing the traditional ultrasound\nprocessing pipeline with a data-driven, learnable counterpart leads to\nsignificant improvement in image quality. Moreover, we demonstrate that greater\nimprovement can be achieved through a learning-based design of the transmitted\nbeam patterns simultaneously with learning an image reconstruction pipeline. We\nevaluate our method on an in-vivo first-harmonic cardiac ultrasound dataset\nacquired from volunteers and demonstrate the significance of the learned\npipeline and transmit beam patterns on the image quality when compared to\nstandard transmit and receive beamformers used in high frame-rate US imaging.\nWe believe that the presented methodology provides a fundamentally different\nperspective on the classical problem of ultrasound beam pattern design.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 15:59:19 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 06:43:29 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Vedula", "Sanketh", ""], ["Senouf", "Ortal", ""], ["Zurakhov", "Grigoriy", ""], ["Bronstein", "Alex", ""], ["Michailovich", "Oleg", ""], ["Zibulevsky", "Michael", ""]]}, {"id": "1812.08047", "submitter": "Ramanarayan Mohanty", "authors": "Ramanarayan Mohanty, S L Happy, Aurobinda Routray", "title": "Spatial-Spectral Regularized Local Scaling Cut for Dimensionality\n  Reduction in Hyperspectral Image Classification", "comments": "arXiv admin note: text overlap with arXiv:1811.08223", "journal-ref": "IEEE Geoscience and Remote Sensing Letters, 2018", "doi": "10.1109/LGRS.2018.2885809", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction (DR) methods have attracted extensive attention to\nprovide discriminative information and reduce the computational burden of the\nhyperspectral image (HSI) classification. However, the DR methods face many\nchallenges due to limited training samples with high dimensional spectra. To\naddress this issue, a graph-based spatial and spectral regularized local\nscaling cut (SSRLSC) for DR of HSI data is proposed. The underlying idea of the\nproposed method is to utilize the information from both the spectral and\nspatial domains to achieve better classification accuracy than its spectral\ndomain counterpart. In SSRLSC, a guided filter is initially used to smoothen\nand homogenize the pixels of the HSI data in order to preserve the pixel\nconsistency. This is followed by generation of between-class and within-class\ndissimilarity matrices in both spectral and spatial domains by regularized\nlocal scaling cut (RLSC) and neighboring pixel local scaling cut (NPLSC)\nrespectively. Finally, we obtain the projection matrix by optimizing the\nupdated spatial-spectral between-class and total-class dissimilarity. The\neffectiveness of the proposed DR algorithm is illustrated with two popular\nreal-world HSI datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 04:13:31 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Mohanty", "Ramanarayan", ""], ["Happy", "S L", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1812.08052", "submitter": "Davide Mazzini", "authors": "Simone Bianco, Davide Mazzini, Paolo Napoletano, Raimondo Schettini", "title": "Multitask Painting Categorization by Deep Multibranch Neural Network", "comments": "11 pages, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new deep multibranch neural network to solve the\ntasks of artist, style, and genre categorization in a multitask formulation. In\norder to gather clues from low-level texture details and, at the same time,\nexploit the coarse layout of the painting, the branches of the proposed\nnetworks are fed with crops at different resolutions. We propose and compare\ntwo different crop strategies: the first one is a random-crop strategy that\npermits to manage the tradeoff between accuracy and speed; the second one is a\nsmart extractor based on Spatial Transformer Networks trained to extract the\nmost representative subregions. Furthermore, inspired by the results obtained\nin other domains, we experiment the joint use of hand-crafted features directly\ncomputed on the input images along with neural ones. Experiments are performed\non a new dataset originally sourced from wikiart.org and hosted by Kaggle, and\nmade suitable for artist, style and genre multitask learning. The dataset here\nproposed, named MultitaskPainting100k, is composed by 100K paintings, 1508\nartists, 125 styles and 41 genres. Our best method, tested on the\nMultitaskPainting100k dataset, achieves accuracy levels of 56.5%, 57.2%, and\n63.6% on the tasks of artist, style and genre prediction respectively.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 16:12:29 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Bianco", "Simone", ""], ["Mazzini", "Davide", ""], ["Napoletano", "Paolo", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1812.08094", "submitter": "Fangwen Tu", "authors": "Fangwen Tu, Shuzhi Sam Ge and Chang Chieh Hang", "title": "Shallow Cue Guided Deep Visual Tracking via Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, a robust visual tracking approach via mixed model based\nconvolutional neural networks (SDT) is developed. In order to handle abrupt or\nfast motion, a prior map is generated to facilitate the localization of region\nof interest (ROI) before the deep tracker is performed. A top-down saliency\nmodel with nineteen shallow cues are employed to construct the prior map with\nonline learnt combination weights. Moreover, apart from a holistic deep\nlearner, four local networks are also trained to learn different components of\nthe target. The generated four local heat maps will facilitate to rectify the\nholistic map by eliminating the distracters to avoid drifting. Furthermore, to\nguarantee the instance for online update of high quality, a prioritised update\nstrategy is implemented by casting the problem into a label noise problem. The\nselection probability is designed by considering both confidence values and\nbio-inspired memory for temporal information integration. Experiments are\nconducted qualitatively and quantitatively on a set of challenging image\nsequences. Comparative study demonstrates that the proposed algorithm\noutperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:13:20 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Tu", "Fangwen", ""], ["Ge", "Shuzhi Sam", ""], ["Hang", "Chang Chieh", ""]]}, {"id": "1812.08095", "submitter": "Franziska Lippoldt", "authors": "Franziska Lippoldt, Marius Erdt", "title": "Window detection in aerial texture images of the Berlin 3D CityGML Model", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article explores the usage of the state-of-art neural network Mask R-CNN\nto be used for window detection of texture files from the CityGML model of\nBerlin. As texture files are very irregular in terms of size, exposure settings\nand orientation, we use several parameter optimisation methods to improve the\nprecision. Those textures are cropped from aerial photos, which implies that\nthe angle of the facade, the exposure as well as contrast are calibrated\ntowards the mean and not towards the single facade. The analysis of a single\ntexture image with the human eye itself is challenging: A combination of window\nand facade estimation and perspective analysis is necessary in order to\ndetermine the facades and windows. We train and detect bounding boxes and masks\nfrom two data sets with image size 128 and 256. We explore various\nconfiguration optimisation methods and the relation of the Region Proposal\nNetwork, detected ROIs and the mask output. Our final results shows that the we\ncan improve the average precision scores for both data set sizes, yet the\ninitial AP score varies and leads to different resulting scores.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:15:02 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Lippoldt", "Franziska", ""], ["Erdt", "Marius", ""]]}, {"id": "1812.08102", "submitter": "Mario Amrehn", "authors": "Mario Amrehn, Firas Mualla, Elli Angelopoulou, Stefan Steidl, Andreas\n  Maier", "title": "The Random Forest Classifier in WEKA: Discussion and New Developments\n  for Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis and machine learning have become an integrative part of the\nmodern scientific methodology, providing automated techniques to predict\nfurther information based on observations. One of these classification and\nregression techniques is the random forest approach. Those decision tree based\npredictors are best known for their good computational performance and\nscalability. However, in case of severely imbalanced training data, as often\nseen in medical studies' data with large control groups, the training algorithm\nor the sampling process has to be altered in order to improve the prediction\nquality for minority classes. In this work, a balanced random forest approach\nfor WEKA is proposed. Furthermore, the prediction quality of the unmodified\nrandom forest implementation and the new balanced random forest version for\nWEKA are evaluated against reference implementations in R. Two-class problems\non balanced data sets and imbalanced medical studies' data are investigated. A\nsuperior prediction quality using the proposed method for imbalanced data is\nshown compared to the other three techniques.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:27:04 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 09:45:48 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Amrehn", "Mario", ""], ["Mualla", "Firas", ""], ["Angelopoulou", "Elli", ""], ["Steidl", "Stefan", ""], ["Maier", "Andreas", ""]]}, {"id": "1812.08115", "submitter": "Hemant Kumar Aggarwal", "authors": "Hemant Kumar Aggarwal, Merry P. Mani, Mathews Jacob", "title": "MoDL-MUSSELS: Model-Based Deep Learning for Multi-Shot Sensitivity\n  Encoded Diffusion MRI", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, 2019", "doi": "10.1109/TMI.2019.2946501", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model-based deep learning architecture termed MoDL-MUSSELS for\nthe correction of phase errors in multishot diffusion-weighted echo-planar MRI\nimages. The proposed algorithm is a generalization of existing MUSSELS\nalgorithm with similar performance but with significantly reduced computational\ncomplexity. In this work, we show that an iterative re-weighted least-squares\nimplementation of MUSSELS alternates between a multichannel filter bank and the\nenforcement of data consistency. The multichannel filter bank projects the data\nto the signal subspace thus exploiting the phase relations between shots. Due\nto the high computational complexity of self-learned filter bank, we propose to\nreplace it with a convolutional neural network (CNN) whose parameters are\nlearned from exemplary data. The proposed CNN is a hybrid model involving a\nmultichannel CNN in the k-space and another CNN in the image space. The k-space\nCNN exploits the phase relations between the shot images, while the image\ndomain network is used to project the data to an image manifold. The\nexperiments show that the proposed scheme can yield reconstructions that are\ncomparable to state of the art methods while offering several orders of\nmagnitude reduction in run-time.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:46:43 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 18:42:16 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 17:32:33 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Aggarwal", "Hemant Kumar", ""], ["Mani", "Merry P.", ""], ["Jacob", "Mathews", ""]]}, {"id": "1812.08119", "submitter": "Atsushi Yaguchi", "authors": "Atsushi Yaguchi, Taiji Suzuki, Wataru Asano, Shuhei Nitta, Yukinobu\n  Sakata, Akiyuki Tanizawa", "title": "Adam Induces Implicit Weight Sparsity in Rectifier Neural Networks", "comments": "8 pages, 7 figures, 6 tables, 2018 17th IEEE International Conference\n  on Machine Learning and Applications (ICMLA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks (DNNs) have been applied to various\nmachine leaning tasks, including image recognition, speech recognition, and\nmachine translation. However, large DNN models are needed to achieve\nstate-of-the-art performance, exceeding the capabilities of edge devices. Model\nreduction is thus needed for practical use. In this paper, we point out that\ndeep learning automatically induces group sparsity of weights, in which all\nweights connected to an output channel (node) are zero, when training DNNs\nunder the following three conditions: (1) rectified-linear-unit (ReLU)\nactivations, (2) an $L_2$-regularized objective function, and (3) the Adam\noptimizer. Next, we analyze this behavior both theoretically and\nexperimentally, and propose a simple model reduction method: eliminate the zero\nweights after training the DNN. In experiments on MNIST and CIFAR-10 datasets,\nwe demonstrate the sparsity with various training setups. Finally, we show that\nour method can efficiently reduce the model size and performs well relative to\nmethods that use a sparsity-inducing regularizer.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:59:08 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Yaguchi", "Atsushi", ""], ["Suzuki", "Taiji", ""], ["Asano", "Wataru", ""], ["Nitta", "Shuhei", ""], ["Sakata", "Yukinobu", ""], ["Tanizawa", "Akiyuki", ""]]}, {"id": "1812.08125", "submitter": "Jimmy Ren", "authors": "Yan Chen, Jimmy Ren, Xuanye Cheng, Keyuan Qian, Jinwei Gu", "title": "Very Power Efficient Neural Time-of-Flight", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-of-Flight (ToF) cameras require active illumination to obtain depth\ninformation thus the power of illumination directly affects the performance of\nToF cameras. Traditional ToF imaging algorithms is very sensitive to\nillumination and the depth accuracy degenerates rapidly with the power of it.\nTherefore, the design of a power efficient ToF camera always creates a painful\ndilemma for the illumination and the performance trade-off. In this paper, we\nshow that despite the weak signals in many areas under extreme short exposure\nsetting, these signals as a whole can be well utilized through a learning\nprocess which directly translates the weak and noisy ToF camera raw to depth\nmap. This creates an opportunity to tackle the aforementioned dilemma and make\na very power efficient ToF camera possible. To enable the learning, we collect\na comprehensive dataset under a variety of scenes and photographic conditions\nby a specialized ToF camera. Experiments show that our method is able to\nrobustly process ToF camera raw with the exposure time of one order of\nmagnitude shorter than that used in conventional ToF cameras. In addition to\nevaluating our approach both quantitatively and qualitatively, we also discuss\nits implication to designing the next generation power efficient ToF cameras.\nWe will make our dataset and code publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 18:08:48 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Chen", "Yan", ""], ["Ren", "Jimmy", ""], ["Cheng", "Xuanye", ""], ["Qian", "Keyuan", ""], ["Gu", "Jinwei", ""]]}, {"id": "1812.08126", "submitter": "Annika Lindh", "authors": "Annika Lindh, Robert J. Ross, Abhijit Mahalunkar, Giancarlo Salton,\n  John D. Kelleher", "title": "Generating Diverse and Meaningful Captions", "comments": "Accepted for presentation at The 27th International Conference on\n  Artificial Neural Networks (ICANN 2018)", "journal-ref": "Artificial Neural Networks and Machine Learning - ICANN 2018 (pp.\n  176-187). Springer International Publishing", "doi": "10.1007/978-3-030-01418-6_18", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Captioning is a task that requires models to acquire a multi-modal\nunderstanding of the world and to express this understanding in natural\nlanguage text. While the state-of-the-art for this task has rapidly improved in\nterms of n-gram metrics, these models tend to output the same generic captions\nfor similar images. In this work, we address this limitation and train a model\nthat generates more diverse and specific captions through an unsupervised\ntraining approach that incorporates a learning signal from an Image Retrieval\nmodel. We summarize previous results and improve the state-of-the-art on\ncaption diversity and novelty. We make our source code publicly available\nonline.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 18:10:18 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Lindh", "Annika", ""], ["Ross", "Robert J.", ""], ["Mahalunkar", "Abhijit", ""], ["Salton", "Giancarlo", ""], ["Kelleher", "John D.", ""]]}, {"id": "1812.08155", "submitter": "Ilkay Oksuz", "authors": "Ilkay Oksuz, Gastao Cruz, James Clough, Aurelien Bustin, Nicolo Fuin,\n  Rene M. Botnar, Claudia Prieto, Andrew P. King, Julia A. Schnabel", "title": "Magnetic Resonance Fingerprinting using Recurrent Neural Networks", "comments": "Accepted for ISBI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative\nmagnetic resonance imaging that allows simultaneous measurement of multiple\ntissue properties in a single, time-efficient acquisition. Standard MRF\nreconstructs parametric maps using dictionary matching and lacks scalability\ndue to computational inefficiency. We propose to perform MRF map reconstruction\nusing a recurrent neural network, which exploits the time-dependent information\nof the MRF signal evolution. We evaluate our method on multiparametric\nsynthetic signals and compare it to existing MRF map reconstruction approaches,\nincluding those based on neural networks. Our method achieves state-of-the-art\nestimates of T1 and T2 values. In addition, the reconstruction time is\nsignificantly reduced compared to dictionary-matching based approaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 18:50:15 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Oksuz", "Ilkay", ""], ["Cruz", "Gastao", ""], ["Clough", "James", ""], ["Bustin", "Aurelien", ""], ["Fuin", "Nicolo", ""], ["Botnar", "Rene M.", ""], ["Prieto", "Claudia", ""], ["King", "Andrew P.", ""], ["Schnabel", "Julia A.", ""]]}, {"id": "1812.08156", "submitter": "Alex Zihao Zhu", "authors": "Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, Kostas Daniilidis", "title": "Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel framework for unsupervised learning for\nevent cameras that learns motion information from only the event stream. In\nparticular, we propose an input representation of the events in the form of a\ndiscretized volume that maintains the temporal distribution of the events,\nwhich we pass through a neural network to predict the motion of the events.\nThis motion is used to attempt to remove any motion blur in the event image. We\nthen propose a loss function applied to the motion compensated event image that\nmeasures the motion blur in this image. We train two networks with this\nframework, one to predict optical flow, and one to predict egomotion and\ndepths, and evaluate these networks on the Multi Vehicle Stereo Event Camera\ndataset, along with qualitative results from a variety of different scenes.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 18:50:54 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Zhu", "Alex Zihao", ""], ["Yuan", "Liangzhe", ""], ["Chaney", "Kenneth", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1812.08196", "submitter": "Rahul Dey", "authors": "Rahul Dey, Felix Juefei-Xu, Vishnu Naresh Boddeti and Marios Savvides", "title": "RankGAN: A Maximum Margin Ranking GAN for Generating Faces", "comments": "Best Student Paper Award at Asian Conference on Computer Vision\n  (ACCV), 2018 at Perth, Australia. Includes main paper and supplementary\n  material. Total 32 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new stage-wise learning paradigm for training generative\nadversarial networks (GANs). The goal of our work is to progressively\nstrengthen the discriminator and thus, the generators, with each subsequent\nstage without changing the network architecture. We call this proposed method\nthe RankGAN. We first propose a margin-based loss for the GAN discriminator. We\nthen extend it to a margin-based ranking loss to train the multiple stages of\nRankGAN. We focus on face images from the CelebA dataset in our work and show\nvisual as well as quantitative improvements in face generation and completion\ntasks over other GAN approaches, including WGAN and LSGAN.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 19:09:32 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Dey", "Rahul", ""], ["Juefei-Xu", "Felix", ""], ["Boddeti", "Vishnu Naresh", ""], ["Savvides", "Marios", ""]]}, {"id": "1812.08245", "submitter": "Sohaib Ahmad", "authors": "Sohaib Ahmad, Benjamin Fuller", "title": "Unconstrained Iris Segmentation using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of consistent and identifiable features from an image of the\nhuman iris is known as iris recognition. Identifying which pixels belong to the\niris, known as segmentation, is the first stage of iris recognition. Errors in\nsegmentation propagate to later stages. Current segmentation approaches are\ntuned to specific environments. We propose using a convolution neural network\nfor iris segmentation. Our algorithm is accurate when trained in a single\nenvironment and tested in multiple environments. Our network builds on the Mask\nR-CNN framework (He et al., ICCV 2017). Our approach segments faster than\nprevious approaches including the Mask R-CNN network. Our network is accurate\nwhen trained on a single environment and tested with a different sensors\n(either visible light or near-infrared). Its accuracy degrades when trained\nwith a visible light sensor and tested with a near-infrared sensor (and vice\nversa). A small amount of retraining of the visible light model (using a few\nsamples from a near-infrared dataset) yields a tuned network accurate in both\nsettings. For training and testing, this work uses the Casia v4 Interval, Notre\nDame 0405, Ubiris v2, and IITD datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 21:10:08 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Ahmad", "Sohaib", ""], ["Fuller", "Benjamin", ""]]}, {"id": "1812.08247", "submitter": "Scott McCloskey", "authors": "Scott McCloskey and Michael Albright", "title": "Detecting GAN-generated Imagery using Color Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image forensics is an increasingly relevant problem, as it can potentially\naddress online disinformation campaigns and mitigate problematic aspects of\nsocial media. Of particular interest, given its recent successes, is the\ndetection of imagery produced by Generative Adversarial Networks (GANs), e.g.\n`deepfakes'. Leveraging large training sets and extensive computing resources,\nrecent work has shown that GANs can be trained to generate synthetic imagery\nwhich is (in some ways) indistinguishable from real imagery. We analyze the\nstructure of the generating network of a popular GAN implementation, and show\nthat the network's treatment of color is markedly different from a real camera\nin two ways. We further show that these two cues can be used to distinguish\nGAN-generated imagery from camera imagery, demonstrating effective\ndiscrimination between GAN imagery and real camera images used to train the\nGAN.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 21:12:00 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["McCloskey", "Scott", ""], ["Albright", "Michael", ""]]}, {"id": "1812.08249", "submitter": "Jonathan Stroud", "authors": "Jonathan C. Stroud, David A. Ross, Chen Sun, Jia Deng, and Rahul\n  Sukthankar", "title": "D3D: Distilled 3D Networks for Video Action Recognition", "comments": "Added link to code and models at https://www.jonathancstroud.com/d3d", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for video action recognition commonly use an\nensemble of two networks: the spatial stream, which takes RGB frames as input,\nand the temporal stream, which takes optical flow as input. In recent work,\nboth of these streams consist of 3D Convolutional Neural Networks, which apply\nspatiotemporal filters to the video clip before performing classification.\nConceptually, the temporal filters should allow the spatial stream to learn\nmotion representations, making the temporal stream redundant. However, we still\nsee significant benefits in action recognition performance by including an\nentirely separate temporal stream, indicating that the spatial stream is\n\"missing\" some of the signal captured by the temporal stream. In this work, we\nfirst investigate whether motion representations are indeed missing in the\nspatial stream of 3D CNNs. Second, we demonstrate that these motion\nrepresentations can be improved by distillation, by tuning the spatial stream\nto predict the outputs of the temporal stream, effectively combining both\nmodels into a single stream. Finally, we show that our Distilled 3D Network\n(D3D) achieves performance on par with two-stream approaches, using only a\nsingle model and with no need to compute optical flow.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 21:19:41 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 19:01:06 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Stroud", "Jonathan C.", ""], ["Ross", "David A.", ""], ["Sun", "Chen", ""], ["Deng", "Jia", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1812.08263", "submitter": "Zhiling Long", "authors": "Zhiling Long, Yazeed Alaudah, Muhammad Ali Qureshi, Yuting Hu, Zhen\n  Wang, Motaz Alfarraj, Ghassan AlRegib, Asjad Amin, Mohamed Deriche, Suhail\n  Al-Dharrab, and Haibin Di", "title": "A comparative study of texture attributes for characterizing subsurface\n  structures in seismic volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore how to computationally characterize subsurface\ngeological structures presented in seismic volumes using texture attributes.\nFor this purpose, we conduct a comparative study of typical texture attributes\npresented in the image processing literature. We focus on spatial attributes in\nthis study and examine them in a new application for seismic interpretation,\ni.e., seismic volume labeling. For this application, a data volume is\nautomatically segmented into various structures, each assigned with its\ncorresponding label. If the labels are assigned with reasonable accuracy, such\nvolume labeling will help initiate an interpretation process in a more\neffective manner. Our investigation proves the feasibility of accomplishing\nthis task using texture attributes. Through the study, we also identify\nadvantages and disadvantages associated with each attribute.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 21:55:59 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Long", "Zhiling", ""], ["Alaudah", "Yazeed", ""], ["Qureshi", "Muhammad Ali", ""], ["Hu", "Yuting", ""], ["Wang", "Zhen", ""], ["Alfarraj", "Motaz", ""], ["AlRegib", "Ghassan", ""], ["Amin", "Asjad", ""], ["Deriche", "Mohamed", ""], ["Al-Dharrab", "Suhail", ""], ["Di", "Haibin", ""]]}, {"id": "1812.08301", "submitter": "Xiaofan Xu", "authors": "Mi Sun Park, Xiaofan Xu, Cormac Brick", "title": "SQuantizer: Simultaneous Learning for Both Sparse and Low-precision\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved state-of-the-art accuracies in a wide\nrange of computer vision, speech recognition, and machine translation tasks.\nHowever the limits of memory bandwidth and computational power constrain the\nrange of devices capable of deploying these modern networks. To address this\nproblem, we propose SQuantizer, a new training method that jointly optimizes\nfor both sparse and low-precision neural networks while maintaining high\naccuracy and providing a high compression rate. This approach brings\nsparsification and low-bit quantization into a single training pass, employing\nthese techniques in an order demonstrated to be optimal. Our method achieves\nstate-of-the-art accuracies using 4-bit and 2-bit precision for ResNet18,\nMobileNet-v2 and ResNet50, even with high degree of sparsity. The compression\nrates of 18x for ResNet18 and 17x for ResNet50, and 9x for MobileNet-v2 are\nobtained when SQuantizing both weights and activations within 1% and 2% loss in\naccuracy for ResNets and MobileNet-v2 respectively. An extension of these\ntechniques to object detection also demonstrates high accuracy on YOLO-v3.\nAdditionally, our method allows for fast single pass training, which is\nimportant for rapid prototyping and neural architecture search techniques.\nFinally extensive results from this simultaneous training approach allows us to\ndraw some useful insights into the relative merits of sparsity and\nquantization.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 00:55:55 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 20:47:47 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Park", "Mi Sun", ""], ["Xu", "Xiaofan", ""], ["Brick", "Cormac", ""]]}, {"id": "1812.08333", "submitter": "Ye Wang", "authors": "Ye Wang, Yueru Chen, Jongmoo Choi and C.-C. Jay Kuo", "title": "Towards Visible and Thermal Drone Monitoring with Convolutional Neural\n  Networks", "comments": "12 pages, 18 figures, journal. arXiv admin note: substantial text\n  overlap with arXiv:1712.00863", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports a visible and thermal drone monitoring system that\nintegrates deep-learning-based detection and tracking modules. The biggest\nchallenge in adopting deep learning methods for drone detection is the paucity\nof training drone images especially thermal drone images. To address this\nissue, we develop two data augmentation techniques. One is a model-based drone\naugmentation technique that automatically generates visible drone images with a\nbounding box label on the drone's location. The other is exploiting an\nadversarial data augmentation methodology to create thermal drone images. To\ntrack a small flying drone, we utilize the residual information between\nconsecutive image frames. Finally, we present an integrated detection and\ntracking system that outperforms the performance of each individual module\ncontaining detection or tracking only. The experiments show that even being\ntrained on synthetic data, the proposed system performs well on real-world\ndrone images with complex background. The USC drone detection and tracking\ndataset with user labeled bounding boxes is available to the public.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 06:26:19 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Wang", "Ye", ""], ["Chen", "Yueru", ""], ["Choi", "Jongmoo", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1812.08348", "submitter": "Yinglong Wang", "authors": "Yinglong Wang, Shuaicheng Liu, Chen Chen, Dehua Xie, Bing Zeng", "title": "Rain Removal By Image Quasi-Sparsity Priors", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streaks will inevitably be captured by some outdoor vision systems,\nwhich lowers the image visual quality and also interferes various computer\nvision applications. We present a novel rain removal method in this paper,\nwhich consists of two steps, i.e., detection of rain streaks and reconstruction\nof the rain-removed image. An accurate detection of rain streaks determines the\nquality of the overall performance. To this end, we first detect rain streaks\naccording to pixel intensities, motivated by the observation that rain streaks\noften possess higher intensities compared to other neighboring image\nstructures. Some mis-detected locations are then refined through a\nmorphological processing and the principal component analysis (PCA) such that\nonly locations corresponding to real rain streaks are retained. In the second\nstep, we separate image gradients into a background layer and a rain streak\nlayer, thanks to the image quasi-sparsity prior, so that a rain image can be\ndecomposed into a background layer and a rain layer. We validate the\neffectiveness of our method through quantitative and qualitative evaluations.\nWe show that our method can remove rain (even for some relatively bright rain)\nfrom images robustly and outperforms some state-of-the-art rain removal\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 03:39:39 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Wang", "Yinglong", ""], ["Liu", "Shuaicheng", ""], ["Chen", "Chen", ""], ["Xie", "Dehua", ""], ["Zeng", "Bing", ""]]}, {"id": "1812.08350", "submitter": "Tsun-Hsuan Wang", "authors": "Tsun-Hsuan Wang, Fu-En Wang, Juan-Ting Lin, Yi-Hsuan Tsai, Wei-Chen\n  Chiu, Min Sun", "title": "Plug-and-Play: Improve Depth Estimation via Sparse Data Propagation", "comments": "7 pages. 7 figures. ver.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel plug-and-play (PnP) module for improving depth prediction\nwith taking arbitrary patterns of sparse depths as input. Given any pre-trained\ndepth prediction model, our PnP module updates the intermediate feature map\nsuch that the model outputs new depths consistent with the given sparse depths.\nOur method requires no additional training and can be applied to practical\napplications such as leveraging both RGB and sparse LiDAR points to robustly\nestimate dense depth map. Our approach achieves consistent improvements on\nvarious state-of-the-art methods on indoor (i.e., NYU-v2) and outdoor (i.e.,\nKITTI) datasets. Various types of LiDARs are also synthesized in our\nexperiments to verify the general applicability of our PnP module in practice.\nFor project page, see https://zswang666.github.io/PnP-Depth-Project-Page/\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 03:49:49 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 14:32:53 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Wang", "Tsun-Hsuan", ""], ["Wang", "Fu-En", ""], ["Lin", "Juan-Ting", ""], ["Tsai", "Yi-Hsuan", ""], ["Chiu", "Wei-Chen", ""], ["Sun", "Min", ""]]}, {"id": "1812.08351", "submitter": "Alex Zihao Zhu", "authors": "Alex Zihao Zhu, Wenxin Liu, Ziyun Wang, Vijay Kumar, Kostas Daniilidis", "title": "Robustness Meets Deep Learning: An End-to-End Hybrid Pipeline for\n  Unsupervised Learning of Egomotion", "comments": "10 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a method that combines unsupervised deep learning\npredictions for optical flow and monocular disparity with a model based\noptimization procedure for instantaneous camera pose. Given the flow and\ndisparity predictions from the network, we apply a RANSAC outlier rejection\nscheme to find an inlier set of flows and disparities, which we use to solve\nfor the relative camera pose in a least squares fashion. We show that this\npipeline is fully differentiable, allowing us to combine the pose with the\nnetwork outputs as an additional unsupervised training loss to further refine\nthe predicted flows and disparities. This method not only allows us to directly\nregress relative pose from the network outputs, but also automatically segments\naway pixels that do not fit the rigid scene assumptions that many unsupervised\nstructure from motion methods apply, such as on independently moving objects.\nWe evaluate our method on the KITTI dataset, and demonstrate state of the art\nresults, even in the presence of challenging independently moving objects.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 03:51:47 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 20:38:01 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2019 01:31:15 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Zhu", "Alex Zihao", ""], ["Liu", "Wenxin", ""], ["Wang", "Ziyun", ""], ["Kumar", "Vijay", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1812.08352", "submitter": "Yu Cheng", "authors": "Yu Cheng, Zhe Gan, Yitong Li, Jingjing Liu, Jianfeng Gao", "title": "Sequential Attention GAN for Interactive Image Editing", "comments": "ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing text-to-image synthesis tasks are static single-turn\ngeneration, based on pre-defined textual descriptions of images. To explore\nmore practical and interactive real-life applications, we introduce a new task\n- Interactive Image Editing, where users can guide an agent to edit images via\nmulti-turn textual commands on-the-fly. In each session, the agent takes a\nnatural language description from the user as the input and modifies the image\ngenerated in the previous turn to a new design, following the user description.\nThe main challenges in this sequential and interactive image generation task\nare two-fold: 1) contextual consistency between a generated image and the\nprovided textual description; 2) step-by-step region-level modification to\nmaintain visual consistency across the generated image sequence in each\nsession. To address these challenges, we propose a novel Sequential Attention\nGenerative Adversarial Net-work (SeqAttnGAN), which applies a neural state\ntracker to encode the previous image and the textual description in each turn\nof the sequence, and uses a GAN framework to generate a modified version of the\nimage that is consistent with the preceding images and coherent with the\ndescription. To achieve better region-specific refinement, we also introduce a\nsequential attention mechanism into the model. To benchmark on the new task, we\nintroduce two new datasets, Zap-Seq and DeepFashion-Seq, which contain\nmulti-turn sessions with image-description sequences in the fashion domain.\nExperiments on both datasets show that the proposed SeqAttnGANmodel outperforms\nstate-of-the-art approaches on the interactive image editing task across all\nevaluation metrics including visual quality, image sequence coherence, and\ntext-image consistency.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 03:55:33 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 00:32:27 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2019 19:06:18 GMT"}, {"version": "v4", "created": "Wed, 5 Aug 2020 22:13:20 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Li", "Yitong", ""], ["Liu", "Jingjing", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1812.08370", "submitter": "Vignesh Prasad", "authors": "Vignesh Prasad, Brojeshwar Bhowmick", "title": "SfMLearner++: Learning Monocular Depth & Ego-Motion using Meaningful\n  Geometric Constraints", "comments": "Accepted at WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most geometric approaches to monocular Visual Odometry (VO) provide robust\npose estimates, but sparse or semi-dense depth estimates. Off late, deep\nmethods have shown good performance in generating dense depths and VO from\nmonocular images by optimizing the photometric consistency between images.\nDespite being intuitive, a naive photometric loss does not ensure proper pixel\ncorrespondences between two views, which is the key factor for accurate depth\nand relative pose estimations. It is a well known fact that simply minimizing\nsuch an error is prone to failures.\n  We propose a method using Epipolar constraints to make the learning more\ngeometrically sound. We use the Essential matrix, obtained using Nister's Five\nPoint Algorithm, for enforcing meaningful geometric constraints on the loss,\nrather than using it as labels for training. Our method, although simplistic\nbut more geometrically meaningful, using lesser number of parameters, gives a\ncomparable performance to state-of-the-art methods which use complex losses and\nlarge networks showing the effectiveness of using epipolar constraints. Such a\ngeometrically constrained learning method performs successfully even in cases\nwhere simply minimizing the photometric error would fail.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 06:17:37 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Prasad", "Vignesh", ""], ["Bhowmick", "Brojeshwar", ""]]}, {"id": "1812.08374", "submitter": "Shuai Zhang", "authors": "Xin Li, Shuai Zhang, Bolan Jiang, Yingyong Qi, Mooi Choo Chuah and\n  Ning Bi", "title": "DAC: Data-free Automatic Acceleration of Convolutional Networks", "comments": "Accepted by IEEE Winter Conference on Applications of Computer Vision\n  (WACV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying a deep learning model on mobile/IoT devices is a challenging task.\nThe difficulty lies in the trade-off between computation speed and accuracy. A\ncomplex deep learning model with high accuracy runs slowly on resource-limited\ndevices, while a light-weight model that runs much faster loses accuracy. In\nthis paper, we propose a novel decomposition method, namely DAC, that is\ncapable of factorizing an ordinary convolutional layer into two layers with\nmuch fewer parameters. DAC computes the corresponding weights for the newly\ngenerated layers directly from the weights of the original convolutional layer.\nThus, no training (or fine-tuning) or any data is needed. The experimental\nresults show that DAC reduces a large number of floating-point operations\n(FLOPs) while maintaining high accuracy of a pre-trained model. If 2% accuracy\ndrop is acceptable, DAC saves 53% FLOPs of VGG16 image classification model on\nImageNet dataset, 29% FLOPS of SSD300 object detection model on PASCAL VOC2007\ndataset, and 46% FLOPS of a multi-person pose estimation model on Microsoft\nCOCO dataset. Compared to other existing decomposition methods, DAC achieves\nbetter performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 06:26:08 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 22:55:58 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "Xin", ""], ["Zhang", "Shuai", ""], ["Jiang", "Bolan", ""], ["Qi", "Yingyong", ""], ["Chuah", "Mooi Choo", ""], ["Bi", "Ning", ""]]}, {"id": "1812.08402", "submitter": "Shi Luo", "authors": "Shi Luo, Xiongfei Li, Rui Zhu, Xiaoli Zhang", "title": "SFA: Small Faces Attention Face Detector", "comments": "10 pages, 0 figures, 0 tables, 41 references", "journal-ref": "IEEE Access 2019", "doi": "10.1109/ACCESS.2019.2955757", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent year, tremendous strides have been made in face detection thanks to\ndeep learning. However, most published face detectors deteriorate dramatically\nas the faces become smaller. In this paper, we present the Small Faces\nAttention (SFA) face detector to better detect faces with small scale. First,\nwe propose a new scale-invariant face detection architecture which pays more\nattention to small faces, including 4-branch detection architecture and small\nfaces sensitive anchor design. Second, feature maps fusion strategy is applied\nin SFA by partially combining high-level features into low-level features to\nfurther improve the ability of finding hard faces. Third, we use multi-scale\ntraining and testing strategy to enhance face detection performance in\npractice. Comprehensive experiments show that SFA significantly improves face\ndetection performance, especially on small faces. Our real-time SFA face\ndetector can run at 5 FPS on a single GPU as well as maintain high performance.\nBesides, our final SFA face detector achieves state-of-the-art detection\nperformance on challenging face detection benchmarks, including WIDER FACE and\nFDDB datasets, with competitive runtime speed. Both our code and models will be\navailable to the research community.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 07:46:23 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Luo", "Shi", ""], ["Li", "Xiongfei", ""], ["Zhu", "Rui", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "1812.08442", "submitter": "Hwann-Tzong Chen", "authors": "Ding-Jie Chen, Jui-Ting Chien, Hwann-Tzong Chen, Tyng-Luh Liu", "title": "Unsupervised Meta-learning of Figure-Ground Segmentation via Imitating\n  Visual Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a \"learning to learn\" approach to figure-ground image\nsegmentation. By exploring webly-abundant images of specific visual effects,\nour method can effectively learn the visual-effect internal representations in\nan unsupervised manner and uses this knowledge to differentiate the figure from\nthe ground in an image. Specifically, we formulate the meta-learning process as\na compositional image editing task that learns to imitate a certain visual\neffect and derive the corresponding internal representation. Such a generative\nprocess can help instantiate the underlying figure-ground notion and enables\nthe system to accomplish the intended image segmentation. Whereas existing\ngenerative methods are mostly tailored to image synthesis or style transfer,\nour approach offers a flexible learning mechanism to model a general concept of\nfigure-ground segmentation from unorganized images that have no explicit\npixel-level annotations. We validate our approach via extensive experiments on\nsix datasets to demonstrate that the proposed model can be end-to-end trained\nwithout ground-truth pixel labeling yet outperforms the existing methods of\nunsupervised segmentation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 09:39:47 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Chen", "Ding-Jie", ""], ["Chien", "Jui-Ting", ""], ["Chen", "Hwann-Tzong", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1812.08468", "submitter": "Patrick Schlachter", "authors": "Patrick Schlachter, Yiwen Liao and Bin Yang", "title": "One-Class Feature Learning Using Intra-Class Splitting", "comments": "IEEE European Signal Processing Conference 2019 (EUSIPCO 2019)", "journal-ref": null, "doi": "10.23919/EUSIPCO.2019.8902848", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel generic one-class feature learning method based\non intra-class splitting. In one-class classification, feature learning is\nchallenging, because only samples of one class are available during training.\nHence, state-of-the-art methods require reference multi-class datasets to\npretrain feature extractors. In contrast, the proposed method realizes feature\nlearning by splitting the given normal class into typical and atypical normal\nsamples. By introducing closeness loss and dispersion loss, an intra-class\njoint training procedure between the two subsets after splitting enables the\nextraction of valuable features for one-class classification. Various\nexperiments on three well-known image classification datasets demonstrate the\neffectiveness of our method which outperformed other baseline models in\naverage.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 10:32:46 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 08:12:38 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 08:35:08 GMT"}, {"version": "v4", "created": "Wed, 19 Jun 2019 14:38:47 GMT"}, {"version": "v5", "created": "Wed, 20 Nov 2019 13:51:36 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Schlachter", "Patrick", ""], ["Liao", "Yiwen", ""], ["Yang", "Bin", ""]]}, {"id": "1812.08577", "submitter": "Juan Cerrolaza", "authors": "Juan J. Cerrolaza, Mirella Lopez-Picazo, Ludovic Humbert, Yoshinobu\n  Sato, Daniel Rueckert, Miguel Angel Gonzalez Ballester, Marius George\n  Linguraru", "title": "Computational Anatomy for Multi-Organ Analysis in Medical Imaging: A\n  Review", "comments": "Paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The medical image analysis field has traditionally been focused on the\ndevelopment of organ-, and disease-specific methods. Recently, the interest in\nthe development of more 20 comprehensive computational anatomical models has\ngrown, leading to the creation of multi-organ models. Multi-organ approaches,\nunlike traditional organ-specific strategies, incorporate inter-organ relations\ninto the model, thus leading to a more accurate representation of the complex\nhuman anatomy. Inter-organ relations are not only spatial, but also functional\nand physiological. Over the years, the strategies 25 proposed to efficiently\nmodel multi-organ structures have evolved from the simple global modeling, to\nmore sophisticated approaches such as sequential, hierarchical, or machine\nlearning-based models. In this paper, we present a review of the state of the\nart on multi-organ analysis and associated computation anatomy methodology. The\nmanuscript follows a methodology-based classification of the different\ntechniques 30 available for the analysis of multi-organs and multi-anatomical\nstructures, from techniques using point distribution models to the most recent\ndeep learning-based approaches. With more than 300 papers included in this\nreview, we reflect on the trends and challenges of the field of computational\nanatomy, the particularities of each anatomical region, and the potential of\nmulti-organ analysis to increase the impact of 35 medical imaging applications\non the future of healthcare.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 14:05:15 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Cerrolaza", "Juan J.", ""], ["Lopez-Picazo", "Mirella", ""], ["Humbert", "Ludovic", ""], ["Sato", "Yoshinobu", ""], ["Rueckert", "Daniel", ""], ["Ballester", "Miguel Angel Gonzalez", ""], ["Linguraru", "Marius George", ""]]}, {"id": "1812.08624", "submitter": "Lida Fanara", "authors": "L. Fanara, K. Gwinner, E. Hauber and J. Oberst", "title": "Automated detection of block falls in the north polar region of Mars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a change detection method for the identification of ice block\nfalls using NASA's HiRISE images of the north polar scarps on Mars. Our method\nis based on a Support Vector Machine (SVM), trained using Histograms of\nOriented Gradients (HOG), and on blob detection. The SVM detects potential new\nblocks between a set of images; the blob detection, then, confirms the\nidentification of a block inside the area indicated by the SVM and derives the\nshape of the block. The results from the automatic analysis were compared with\nblock statistics from visual inspection. We tested our method in 6 areas\nconsisting of 1000x1000 pixels, where several hundreds of blocks were\nidentified. The results for the given test areas produced a true positive rate\nof ~75% for blocks with sizes larger than 0.7 m (i.e., approx. 3 times the\navailable ground pixel size) and a false discovery rate of ~8.5%. Using blob\ndetection we also recover the size of each block within 3 pixels of their\nactual size.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 15:05:21 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Fanara", "L.", ""], ["Gwinner", "K.", ""], ["Hauber", "E.", ""], ["Oberst", "J.", ""]]}, {"id": "1812.08658", "submitter": "Harsh Agrawal", "authors": "Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain,\n  Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson", "title": "nocaps: novel object captioning at scale", "comments": null, "journal-ref": "IEEE International Conference on Computer Vision (ICCV) 2019", "doi": "10.1109/ICCV.2019.00904", "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning models have achieved impressive results on datasets\ncontaining limited visual concepts and large amounts of paired image-caption\ntraining data. However, if these models are to ever function in the wild, a\nmuch larger variety of visual concepts must be learned, ideally from less\nsupervision. To encourage the development of image captioning models that can\nlearn visual concepts from alternative data sources, such as object detection\ndatasets, we present the first large-scale benchmark for this task. Dubbed\n'nocaps', for novel object captioning at scale, our benchmark consists of\n166,100 human-generated captions describing 15,100 images from the OpenImages\nvalidation and test sets. The associated training data consists of COCO\nimage-caption pairs, plus OpenImages image-level labels and object bounding\nboxes. Since OpenImages contains many more classes than COCO, nearly 400 object\nclasses seen in test images have no or very few associated training captions\n(hence, nocaps). We extend existing novel object captioning models to establish\nstrong baselines for this benchmark and provide analysis to guide future work\non this task.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 16:04:05 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 23:13:30 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 20:10:33 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Agrawal", "Harsh", ""], ["Desai", "Karan", ""], ["Wang", "Yufei", ""], ["Chen", "Xinlei", ""], ["Jain", "Rishabh", ""], ["Johnson", "Mark", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Lee", "Stefan", ""], ["Anderson", "Peter", ""]]}, {"id": "1812.08685", "submitter": "Pavel Korshunov", "authors": "Pavel Korshunov and Sebastien Marcel", "title": "DeepFakes: a New Threat to Face Recognition? Assessment and Detection", "comments": "http://publications.idiap.ch/index.php/publications/show/3988", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is becoming increasingly easy to automatically replace a face of one\nperson in a video with the face of another person by using a pre-trained\ngenerative adversarial network (GAN). Recent public scandals, e.g., the faces\nof celebrities being swapped onto pornographic videos, call for automated ways\nto detect these Deepfake videos. To help developing such methods, in this\npaper, we present the first publicly available set of Deepfake videos generated\nfrom videos of VidTIMIT database. We used open source software based on GANs to\ncreate the Deepfakes, and we emphasize that training and blending parameters\ncan significantly impact the quality of the resulted videos. To demonstrate\nthis impact, we generated videos with low and high visual quality (320 videos\neach) using differently tuned parameter sets. We showed that the state of the\nart face recognition systems based on VGG and Facenet neural networks are\nvulnerable to Deepfake videos, with 85.62% and 95.00% false acceptance rates\nrespectively, which means methods for detecting Deepfake videos are necessary.\nBy considering several baseline approaches, we found that audio-visual approach\nbased on lip-sync inconsistency detection was not able to distinguish Deepfake\nvideos. The best performing method, which is based on visual quality metrics\nand is often used in presentation attack detection domain, resulted in 8.97%\nequal error rate on high quality Deepfakes. Our experiments demonstrate that\nGAN-generated Deepfake videos are challenging for both face recognition systems\nand existing detection methods, and the further development of face swapping\ntechnology will make it even more so.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 16:36:39 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Korshunov", "Pavel", ""], ["Marcel", "Sebastien", ""]]}, {"id": "1812.08756", "submitter": "Zhiling Long", "authors": "G. AlRegib, M. Deriche, Z. Long, H. Di, Z. Wang, Y. Alaudah, M.\n  Shafiq, and M. Alfarraj", "title": "Subsurface structure analysis using computational interpretation and\n  learning: A visual signal processing perspective", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2017.2785979", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding Earth's subsurface structures has been and continues to be an\nessential component of various applications such as environmental monitoring,\ncarbon sequestration, and oil and gas exploration. By viewing the seismic\nvolumes that are generated through the processing of recorded seismic traces,\nresearchers were able to learn from applying advanced image processing and\ncomputer vision algorithms to effectively analyze and understand Earth's\nsubsurface structures. In this paper, first, we summarize the recent advances\nin this direction that relied heavily on the fields of image processing and\ncomputer vision. Second, we discuss the challenges in seismic interpretation\nand provide insights and some directions to address such challenges using\nemerging machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 18:38:11 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["AlRegib", "G.", ""], ["Deriche", "M.", ""], ["Long", "Z.", ""], ["Di", "H.", ""], ["Wang", "Z.", ""], ["Alaudah", "Y.", ""], ["Shafiq", "M.", ""], ["Alfarraj", "M.", ""]]}, {"id": "1812.08775", "submitter": "Jia-Bin Huang", "authors": "Jia-Bin Huang", "title": "Deep Paper Gestalt", "comments": "Project page: https://github.com/vt-vl-lab/paper-gestalt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a significant increase in the number of paper\nsubmissions to computer vision conferences. The sheer volume of paper\nsubmissions and the insufficient number of competent reviewers cause a\nconsiderable burden for the current peer review system. In this paper, we learn\na classifier to predict whether a paper should be accepted or rejected based\nsolely on the visual appearance of the paper (i.e., the gestalt of a paper).\nExperimental results show that our classifier can safely reject 50% of the bad\npapers while wrongly reject only 0.4% of the good papers, and thus dramatically\nreduce the workload of the reviewers. We also provide tools for providing\nsuggestions to authors so that they can improve the gestalt of their papers.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 18:55:26 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Huang", "Jia-Bin", ""]]}, {"id": "1812.08781", "submitter": "Han Hu", "authors": "Bin Liu, Zhirong Wu, Han Hu and Stephen Lin", "title": "Deep Metric Transfer for Label Propagation with Limited Annotated Data", "comments": "Code is availble at\n  http://github.com/Microsoft/metric-transfer.pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study object recognition under the constraint that each object class is\nonly represented by very few observations. Semi-supervised learning, transfer\nlearning, and few-shot recognition all concern with achieving fast\ngeneralization with few labeled data. In this paper, we propose a generic\nframework that utilizes unlabeled data to aid generalization for all three\ntasks. Our approach is to create much more training data through label\npropagation from the few labeled examples to a vast collection of unannotated\nimages. The main contribution of the paper is that we show such a label\npropagation scheme can be highly effective when the similarity metric used for\npropagation is transferred from other related domains. We test various\ncombinations of supervised and unsupervised metric learning methods with\nvarious label propagation algorithms. We find that our framework is very\ngeneric without being sensitive to any specific techniques. By taking advantage\nof unlabeled data in this way, we achieve significant improvements on all three\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 18:57:28 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 06:46:33 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Liu", "Bin", ""], ["Wu", "Zhirong", ""], ["Hu", "Han", ""], ["Lin", "Stephen", ""]]}, {"id": "1812.08789", "submitter": "Zhizhen Zhao", "authors": "Zhizhen Zhao, Lydia T. Liu, Amit Singer", "title": "Steerable $e$PCA: Rotationally Invariant Exponential Family PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In photon-limited imaging, the pixel intensities are affected by photon count\nnoise. Many applications, such as 3-D reconstruction using correlation analysis\nin X-ray free electron laser (XFEL) single molecule imaging, require an\naccurate estimation of the covariance of the underlying 2-D clean images.\nAccurate estimation of the covariance from low-photon count images must take\ninto account that pixel intensities are Poisson distributed, hence the\nclassical sample covariance estimator is sub-optimal. Moreover, in single\nmolecule imaging, including in-plane rotated copies of all images could further\nimprove the accuracy of covariance estimation. In this paper we introduce an\nefficient and accurate algorithm for covariance matrix estimation of count\nnoise 2-D images, including their uniform planar rotations and possibly\nreflections. Our procedure, steerable $e$PCA, combines in a novel way two\nrecently introduced innovations. The first is a methodology for principal\ncomponent analysis (PCA) for Poisson distributions, and more generally,\nexponential family distributions, called $e$PCA. The second is steerable PCA, a\nfast and accurate procedure for including all planar rotations for PCA. The\nresulting principal components are invariant to the rotation and reflection of\nthe input images. We demonstrate the efficiency and accuracy of steerable\n$e$PCA in numerical experiments involving simulated XFEL datasets and rotated\nYale B face data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 18:59:58 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 15:25:35 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 17:27:54 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Zhao", "Zhizhen", ""], ["Liu", "Lydia T.", ""], ["Singer", "Amit", ""]]}, {"id": "1812.08848", "submitter": "Calden Wloka", "authors": "Calden Wloka (1), Toni Kuni\\'c (1), Iuliia Kotseruba (1), Ramin Fahimi\n  (2), Nicholas Frosst (3), Neil D. B. Bruce (4) and John K. Tsotsos (1) ((1)\n  Department of Electrical Engineering and Computer Science York University,\n  (2) Department of Computer Science University of Manitoba, (3) Google Brain\n  Toronto, (4) Department of Computer Science Ryerson University)", "title": "SMILER: Saliency Model Implementation Library for Experimental Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Saliency Model Implementation Library for Experimental Research (SMILER)\nis a new software package which provides an open, standardized, and extensible\nframework for maintaining and executing computational saliency models. This\nwork drastically reduces the human effort required to apply saliency algorithms\nto new tasks and datasets, while also ensuring consistency and procedural\ncorrectness for results and conclusions produced by different parties. At its\nlaunch SMILER already includes twenty three saliency models (fourteen models\nbased in MATLAB and nine supported through containerization), and the open\ndesign of SMILER encourages this number to grow with future contributions from\nthe community. The project may be downloaded and contributed to through its\nGitHub page: https://github.com/tsotsoslab/smiler\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 21:20:11 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Wloka", "Calden", ""], ["Kuni\u0107", "Toni", ""], ["Kotseruba", "Iuliia", ""], ["Fahimi", "Ramin", ""], ["Frosst", "Nicholas", ""], ["Bruce", "Neil D. B.", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1812.08849", "submitter": "Ed Quigley", "authors": "Ed Quigley, Winnie Lin, Yilin Zhu, Ronald Fedkiw", "title": "Three Dimensional Reconstruction of Botanical Trees with Simulatable\n  Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the challenging problem of creating full and accurate three\ndimensional reconstructions of botanical trees with the topological and\ngeometric accuracy required for subsequent physical simulation, e.g. in\nresponse to wind forces. Although certain aspects of our approach would benefit\nfrom various improvements, our results exceed the state of the art especially\nin geometric and topological complexity and accuracy. Starting with two\ndimensional RGB image data acquired from cameras attached to drones, we create\npoint clouds, textured triangle meshes, and a simulatable and skinned\ncylindrical articulated rigid body model. We discuss the pros and cons of each\nstep of our pipeline, and in order to stimulate future research we make the raw\nand processed data from every step of the pipeline as well as the final\ngeometric reconstructions publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 21:22:43 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Quigley", "Ed", ""], ["Lin", "Winnie", ""], ["Zhu", "Yilin", ""], ["Fedkiw", "Ronald", ""]]}, {"id": "1812.08852", "submitter": "Chao Wang", "authors": "Yaghoub Rahimi, Chao Wang, Hongbo Dong, Yifei Lou", "title": "A Scale Invariant Approach for Sparse Signal Recovery", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the ratio of the $L_1 $ and $L_2 $ norms, denoted as\n$L_1/L_2$, to promote sparsity. Due to the non-convexity and non-linearity,\nthere has been little attention to this scale-invariant model. Compared to\npopular models in the literature such as the $L_p$ model for $p\\in(0,1)$ and\nthe transformed $L_1$ (TL1), this ratio model is parameter free. Theoretically,\nwe present a strong null space property (sNSP) and prove that any sparse vector\nis a local minimizer of the $L_1 /L_2 $ model provided with this sNSP\ncondition. Computationally, we focus on a constrained formulation that can be\nsolved via the alternating direction method of multipliers (ADMM). Experiments\nshow that the proposed approach is comparable to the state-of-the-art methods\nin sparse recovery. In addition, a variant of the $L_1/L_2$ model to apply on\nthe gradient is also discussed with a proof-of-concept example of the MRI\nreconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 21:29:44 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 06:10:53 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 19:39:54 GMT"}, {"version": "v4", "created": "Fri, 16 Aug 2019 14:34:47 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Rahimi", "Yaghoub", ""], ["Wang", "Chao", ""], ["Dong", "Hongbo", ""], ["Lou", "Yifei", ""]]}, {"id": "1812.08861", "submitter": "Aliaksandr Siarohin", "authors": "Aliaksandr Siarohin, St\\'ephane Lathuili\\`ere, Sergey Tulyakov, Elisa\n  Ricci and Nicu Sebe", "title": "Animating Arbitrary Objects via Deep Motion Transfer", "comments": "CVPR-2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel deep learning framework for image animation.\nGiven an input image with a target object and a driving video sequence\ndepicting a moving object, our framework generates a video in which the target\nobject is animated according to the driving sequence. This is achieved through\na deep architecture that decouples appearance and motion information. Our\nframework consists of three main modules: (i) a Keypoint Detector unsupervisely\ntrained to extract object keypoints, (ii) a Dense Motion prediction network for\ngenerating dense heatmaps from sparse keypoints, in order to better encode\nmotion information and (iii) a Motion Transfer Network, which uses the motion\nheatmaps and appearance information extracted from the input image to\nsynthesize the output frames. We demonstrate the effectiveness of our method on\nseveral benchmark datasets, spanning a wide variety of object appearances, and\nshow that our approach outperforms state-of-the-art image animation and video\ngeneration methods. Our source code is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 21:45:56 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 08:01:58 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 23:48:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Siarohin", "Aliaksandr", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Tulyakov", "Sergey", ""], ["Ricci", "Elisa", ""], ["Sebe", "Nicu", ""]]}, {"id": "1812.08882", "submitter": "Berkay Kanberoglu", "authors": "Berkay Kanberoglu and Dhritiman Das and Priya Nair and Pavan Turaga\n  and David Frakes", "title": "An Optical Flow-Based Approach for Minimally-Divergent Velocimetry Data\n  Interpolation", "comments": "24 pages, 10 figures, International Journal of Biomedical Imaging,\n  accepted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional (3D) biomedical image sets are often acquired with in-plane\npixel spacings that are far less than the out-of-plane spacings between images.\nThe resultant anisotropy, which can be detrimental in many applications, can be\ndecreased using image interpolation. Optical flow and/or other\nregistration-based interpolators have proven useful in such interpolation roles\nin the past. When acquired images are comprised of signals that describe the\nflow velocity of fluids, additional information is available to guide the\ninterpolation process. In this paper, we present an optical-flow based\nframework for image interpolation that also minimizes resultant divergence in\nthe interpolated data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 23:01:30 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Kanberoglu", "Berkay", ""], ["Das", "Dhritiman", ""], ["Nair", "Priya", ""], ["Turaga", "Pavan", ""], ["Frakes", "David", ""]]}, {"id": "1812.08891", "submitter": "Ahmed BenSaid", "authors": "Ahmed Ben Said and Rachid Hadjidj and Sebti Foufou", "title": "Cluster validity index based on Jeffrey divergence", "comments": null, "journal-ref": "PATTERN.ANAL.APPL. 20 (2015) 21-31", "doi": "10.1007/s10044-015-0453-7", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster validity indexes are very important tools designed for two purposes:\ncomparing the performance of clustering algorithms and determining the number\nof clusters that best fits the data. These indexes are in general constructed\nby combining a measure of compactness and a measure of separation. A classical\nmeasure of compactness is the variance. As for separation, the distance between\ncluster centers is used. However, such a distance does not always reflect the\nquality of the partition between clusters and sometimes gives misleading\nresults. In this paper, we propose a new cluster validity index for which\nJeffrey divergence is used to measure separation between clusters. Experimental\nresults are conducted using different types of data and comparison with widely\nused cluster validity indexes demonstrates the outperformance of the proposed\nindex.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 23:41:14 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Said", "Ahmed Ben", ""], ["Hadjidj", "Rachid", ""], ["Foufou", "Sebti", ""]]}, {"id": "1812.08911", "submitter": "Sonia Phene", "authors": "Sonia Phene, R. Carter Dunn, Naama Hammel, Yun Liu, Jonathan Krause,\n  Naho Kitade, Mike Schaekermann, Rory Sayres, Derek J. Wu, Ashish Bora,\n  Christopher Semturs, Anita Misra, Abigail E. Huang, Arielle Spitze, Felipe A.\n  Medeiros, April Y. Maa, Monica Gandhi, Greg S. Corrado, Lily Peng, Dale R.\n  Webster", "title": "Deep Learning and Glaucoma Specialists: The Relative Importance of Optic\n  Disc Features to Predict Glaucoma Referral in Fundus Photos", "comments": null, "journal-ref": "Ophthalmology (2019)", "doi": "10.1016/j.ophtha.2019.07.024", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is the leading cause of preventable, irreversible blindness\nworld-wide. The disease can remain asymptomatic until severe, and an estimated\n50%-90% of people with glaucoma remain undiagnosed. Glaucoma screening is\nrecommended for early detection and treatment. A cost-effective tool to detect\nglaucoma could expand screening access to a much larger patient population, but\nsuch a tool is currently unavailable. We trained a deep learning algorithm\nusing a retrospective dataset of 86,618 images, assessed for glaucomatous optic\nnerve head features and referable glaucomatous optic neuropathy (GON). The\nalgorithm was validated using 3 datasets. For referable GON, the algorithm had\nan AUC of 0.945 (95% CI, 0.929-0.960) in dataset A (1205 images, 1\nimage/patient; 18.1% referable), images adjudicated by panels of Glaucoma\nSpecialists (GSs); 0.855 (95% CI, 0.841-0.870) in dataset B (9642 images, 1\nimage/patient; 9.2% referable), images from Atlanta Veterans Affairs Eye Clinic\ndiabetic teleretinal screening program; and 0.881 (95% CI, 0.838-0.918) in\ndataset C (346 images, 1 image/patient; 81.7% referable), images from Dr.\nShroff's Charity Eye Hospital's glaucoma clinic. The algorithm showed\nsignificantly higher sensitivity than 7 of 10 graders not involved in\ndetermining the reference standard, including 2 of 3 GSs, and showed higher\nspecificity than 3 graders, while remaining comparable to others. For both GSs\nand the algorithm, the most crucial features related to referable GON were:\npresence of vertical cup-to-disc ratio of 0.7 or more, neuroretinal rim\nnotching, retinal nerve fiber layer defect, and bared circumlinear vessels. An\nalgorithm trained on fundus images alone can detect referable GON with higher\nsensitivity than and comparable specificity to eye care providers. The\nalgorithm maintained good performance on an independent dataset with diagnoses\nbased on a full glaucoma workup.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 02:05:29 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 00:46:03 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Phene", "Sonia", ""], ["Dunn", "R. Carter", ""], ["Hammel", "Naama", ""], ["Liu", "Yun", ""], ["Krause", "Jonathan", ""], ["Kitade", "Naho", ""], ["Schaekermann", "Mike", ""], ["Sayres", "Rory", ""], ["Wu", "Derek J.", ""], ["Bora", "Ashish", ""], ["Semturs", "Christopher", ""], ["Misra", "Anita", ""], ["Huang", "Abigail E.", ""], ["Spitze", "Arielle", ""], ["Medeiros", "Felipe A.", ""], ["Maa", "April Y.", ""], ["Gandhi", "Monica", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily", ""], ["Webster", "Dale R.", ""]]}, {"id": "1812.08915", "submitter": "Yuan Mao", "authors": "Yixiong Liang, Yuan Mao, Zhihong Tang, Meng Yan, Yuqian Zhao, Jianfeng\n  Liu", "title": "Efficient Misalignment-Robust Multi-Focus Microscopical Images Fusion", "comments": "14 pages,11 figures", "journal-ref": null, "doi": "10.1016/j.sigpro.2019.03.020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a very efficient method to fuse the unregistered\nmulti-focus microscopical images based on the speed-up robust features (SURF).\nOur method follows the pipeline of first registration and then fusion. However,\ninstead of treating the registration and fusion as two completely independent\nstage, we propose to reuse the determinant of the approximate Hessian generated\nin SURF detection stage as the corresponding salient response for the final\nimage fusion, thus it enables nearly cost-free saliency map generation. In\naddition, due to the adoption of SURF scale space representation, our method\ncan generate scale-invariant saliency map which is desired for scale-invariant\nimage fusion. We present an extensive evaluation on the dataset consisting of\nseveral groups of unregistered multi-focus 4K ultra HD microscopic images with\nsize of 4112 x 3008. Compared with the state-of-the-art multi-focus image\nfusion methods, our method is much faster and achieve better results in the\nvisual performance. Our method provides a flexible and efficient way to\nintegrate complementary and redundant information from multiple multi-focus\nultra HD unregistered images into a fused image that contains better\ndescription than any of the individual input images. Code is available at\nhttps://github.com/yiqingmy/JointRF.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 02:17:35 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Liang", "Yixiong", ""], ["Mao", "Yuan", ""], ["Tang", "Zhihong", ""], ["Yan", "Meng", ""], ["Zhao", "Yuqian", ""], ["Liu", "Jianfeng", ""]]}, {"id": "1812.08928", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, Thomas Huang", "title": "Slimmable Neural Networks", "comments": "Accepted in ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and general method to train a single neural network\nexecutable at different widths (number of channels in a layer), permitting\ninstant and adaptive accuracy-efficiency trade-offs at runtime. Instead of\ntraining individual networks with different width configurations, we train a\nshared network with switchable batch normalization. At runtime, the network can\nadjust its width on the fly according to on-device benchmarks and resource\nconstraints, rather than downloading and offloading different models. Our\ntrained networks, named slimmable neural networks, achieve similar (and in many\ncases better) ImageNet classification accuracy than individually trained models\nof MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths\nrespectively. We also demonstrate better performance of slimmable models\ncompared with individual ones across a wide range of applications including\nCOCO bounding-box object detection, instance segmentation and person keypoint\ndetection without tuning hyper-parameters. Lastly we visualize and discuss the\nlearned features of slimmable networks. Code and models are available at:\nhttps://github.com/JiahuiYu/slimmable_networks\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 03:36:48 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Yu", "Jiahui", ""], ["Yang", "Linjie", ""], ["Xu", "Ning", ""], ["Yang", "Jianchao", ""], ["Huang", "Thomas", ""]]}, {"id": "1812.08934", "submitter": "Xiaoliang Dai", "authors": "Xiaoliang Dai, Peizhao Zhang, Bichen Wu, Hongxu Yin, Fei Sun, Yanghan\n  Wang, Marat Dukhan, Yunqing Hu, Yiming Wu, Yangqing Jia, Peter Vajda, Matt\n  Uyttendaele, Niraj K. Jha", "title": "ChamNet: Towards Efficient Network Design through Platform-Aware Model\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient neural network (NN) architecture design\nmethodology called Chameleon that honors given resource constraints. Instead of\ndeveloping new building blocks or using computationally-intensive reinforcement\nlearning algorithms, our approach leverages existing efficient network building\nblocks and focuses on exploiting hardware traits and adapting computation\nresources to fit target latency and/or energy constraints. We formulate\nplatform-aware NN architecture search in an optimization framework and propose\na novel algorithm to search for optimal architectures aided by efficient\naccuracy and resource (latency and/or energy) predictors. At the core of our\nalgorithm lies an accuracy predictor built atop Gaussian Process with Bayesian\noptimization for iterative sampling. With a one-time building cost for the\npredictors, our algorithm produces state-of-the-art model architectures on\ndifferent platforms under given constraints in just minutes. Our results show\nthat adapting computation resources to building blocks is critical to model\nperformance. Without the addition of any bells and whistles, our models achieve\nsignificant accuracy improvements against state-of-the-art hand-crafted and\nautomatically designed architectures. We achieve 73.8% and 75.3% top-1 accuracy\non ImageNet at 20ms latency on a mobile CPU and DSP. At reduced latency, our\nmodels achieve up to 8.5% (4.8%) and 6.6% (9.3%) absolute top-1 accuracy\nimprovements compared to MobileNetV2 and MnasNet, respectively, on a mobile CPU\n(DSP), and 2.7% (4.6%) and 5.6% (2.6%) accuracy gains over ResNet-101 and\nResNet-152, respectively, on an Nvidia GPU (Intel CPU).\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 03:59:34 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Dai", "Xiaoliang", ""], ["Zhang", "Peizhao", ""], ["Wu", "Bichen", ""], ["Yin", "Hongxu", ""], ["Sun", "Fei", ""], ["Wang", "Yanghan", ""], ["Dukhan", "Marat", ""], ["Hu", "Yunqing", ""], ["Wu", "Yiming", ""], ["Jia", "Yangqing", ""], ["Vajda", "Peter", ""], ["Uyttendaele", "Matt", ""], ["Jha", "Niraj K.", ""]]}, {"id": "1812.08967", "submitter": "Zhizheng Zhang", "authors": "Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen", "title": "Densely Semantically Aligned Person Re-Identification", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a densely semantically aligned person re-identification framework.\nIt fundamentally addresses the body misalignment problem caused by\npose/viewpoint variations, imperfect person detection, occlusion, etc. By\nleveraging the estimation of the dense semantics of a person image, we\nconstruct a set of densely semantically aligned part images (DSAP-images),\nwhere the same spatial positions have the same semantics across different\nimages. We design a two-stream network that consists of a main full image\nstream (MF-Stream) and a densely semantically-aligned guiding stream\n(DSAG-Stream). The DSAG-Stream, with the DSAP-images as input, acts as a\nregulator to guide the MF-Stream to learn densely semantically aligned features\nfrom the original image. In the inference, the DSAG-Stream is discarded and\nonly the MF-Stream is needed, which makes the inference system computationally\nefficient and robust. To the best of our knowledge, we are the first to make\nuse of fine grained semantics to address the misalignment problems for re-ID.\nOur method achieves rank-1 accuracy of 78.9% (new protocol) on the CUHK03\ndataset, 90.4% on the CUHK01 dataset, and 95.7% on the Market1501 dataset,\noutperforming state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 06:23:34 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 15:01:04 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zhang", "Zhizheng", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""]]}, {"id": "1812.08973", "submitter": "Fangwen Tu", "authors": "Fangwen Tu, Shuzhi Sam Ge, Yazhe Tang and Chang Chieh Hang", "title": "Saliency Guided Hierarchical Robust Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A saliency guided hierarchical visual tracking (SHT) algorithm containing\nglobal and local search phases is proposed in this paper. In global search, a\ntop-down saliency model is novelly developed to handle abrupt motion and\nappearance variation problems. Nineteen feature maps are extracted first and\ncombined with online learnt weights to produce the final saliency map and\nestimated target locations. After the evaluation of integration mechanism, the\noptimum candidate patch is passed to the local search. In local search, a\nsuperpixel based HSV histogram matching is performed jointly with an L2-RLS\ntracker to take both color distribution and holistic appearance feature of the\nobject into consideration. Furthermore, a linear refinement search process with\nfast iterative solver is implemented to attenuate the possible negative\ninfluence of dominant particles. Both qualitative and quantitative experiments\nare conducted on a series of challenging image sequences. The superior\nperformance of the proposed method over other state-of-the-art algorithms is\ndemonstrated by comparative study.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 06:56:56 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Tu", "Fangwen", ""], ["Ge", "Shuzhi Sam", ""], ["Tang", "Yazhe", ""], ["Hang", "Chang Chieh", ""]]}, {"id": "1812.08974", "submitter": "Mohammad Mahfujur Rahman", "authors": "Mohammad Mahfujur Rahman, Clinton Fookes, Mahsa Baktashmotlagh, Sridha\n  Sridharan", "title": "Multi-component Image Translation for Deep Domain Generalization", "comments": "Accepted in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaption (DA) and domain generalization (DG) are two closely related\nmethods which are both concerned with the task of assigning labels to an\nunlabeled data set. The only dissimilarity between these approaches is that DA\ncan access the target data during the training phase, while the target data is\ntotally unseen during the training phase in DG. The task of DG is challenging\nas we have no earlier knowledge of the target samples. If DA methods are\napplied directly to DG by a simple exclusion of the target data from training,\npoor performance will result for a given task. In this paper, we tackle the\ndomain generalization challenge in two ways. In our first approach, we propose\na novel deep domain generalization architecture utilizing synthetic data\ngenerated by a Generative Adversarial Network (GAN). The discrepancy between\nthe generated images and synthetic images is minimized using existing domain\ndiscrepancy metrics such as maximum mean discrepancy or correlation alignment.\nIn our second approach, we introduce a protocol for applying DA methods to a DG\nscenario by excluding the target data from the training phase, splitting the\nsource data to training and validation parts, and treating the validation data\nas target data for DA. We conduct extensive experiments on four cross-domain\nbenchmark datasets. Experimental results signify our proposed model outperforms\nthe current state-of-the-art methods for DG.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 07:03:31 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Rahman", "Mohammad Mahfujur", ""], ["Fookes", "Clinton", ""], ["Baktashmotlagh", "Mahsa", ""], ["Sridharan", "Sridha", ""]]}, {"id": "1812.08983", "submitter": "Amena Khatun", "authors": "Amena Khatun, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "A Deep Four-Stream Siamese Convolutional Neural Network with Joint\n  Verification and Identification Loss for Person Re-detection", "comments": "Published in WACV 2018", "journal-ref": null, "doi": "10.1109/WACV.2018.00146", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art person re-identification systems that employ a triplet based\ndeep network suffer from a poor generalization capability. In this paper, we\npropose a four stream Siamese deep convolutional neural network for person\nredetection that jointly optimises verification and identification losses over\na four image input group. Specifically, the proposed method overcomes the\nweakness of the typical triplet formulation by using groups of four images\nfeaturing two matched (i.e. the same identity) and two mismatched images. This\nallows us to jointly increase the interclass variations and reduce the\nintra-class variations in the learned feature space. The proposed approach also\noptimises over both the identification and verification losses, further\nminimising intra-class variation and maximising inter-class variation,\nimproving overall performance. Extensive experiments on four challenging\ndatasets, VIPeR, CUHK01, CUHK03 and PRID2011, demonstrates that the proposed\napproach achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 07:50:09 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Khatun", "Amena", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1812.08985", "submitter": "Yedid Hoshen", "authors": "Yedid Hoshen and Jitendra Malik", "title": "Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconditional image generation has recently been dominated by generative\nadversarial networks (GANs). GAN methods train a generator which regresses\nimages from random noise vectors, as well as a discriminator that attempts to\ndifferentiate between the generated images and a training set of real images.\nGANs have shown amazing results at generating realistic looking images. Despite\ntheir success, GANs suffer from critical drawbacks including: unstable training\nand mode-dropping. The weaknesses in GANs have motivated research into\nalternatives including: variational auto-encoders (VAEs), latent embedding\nlearning methods (e.g. GLO) and nearest-neighbor based implicit maximum\nlikelihood estimation (IMLE). Unfortunately at the moment, GANs still\nsignificantly outperform the alternative methods for image generation. In this\nwork, we present a novel method - Generative Latent Nearest Neighbors (GLANN) -\nfor training generative models without adversarial training. GLANN combines the\nstrengths of IMLE and GLO in a way that overcomes the main drawbacks of each\nmethod. Consequently, GLANN generates images that are far better than GLO and\nIMLE. Our method does not suffer from mode collapse which plagues GAN training\nand is much more stable. Qualitative results show that GLANN outperforms a\nbaseline consisting of 800 GANs and VAEs on commonly used datasets. Our models\nare also shown to be effective for training truly non-adversarial unsupervised\nimage translation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 07:54:02 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Hoshen", "Yedid", ""], ["Malik", "Jitendra", ""]]}, {"id": "1812.09010", "submitter": "Klemen Grm", "authors": "Klemen Grm, Martin Pernu\\v{s}, Leo Cluzel, Walter Scheirer, Simon\n  Dobri\\v{s}ek, Vitomir \\v{S}truc", "title": "Face Hallucination Revisited: An Exploratory Study on Dataset Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary face hallucination (FH) models exhibit considerable ability to\nreconstruct high-resolution (HR) details from low-resolution (LR) face images.\nThis ability is commonly learned from examples of corresponding HR-LR image\npairs, created by artificially down-sampling the HR ground truth data. This\ndown-sampling (or degradation) procedure not only defines the characteristics\nof the LR training data, but also determines the type of image degradations the\nlearned FH models are eventually able to handle. If the image characteristics\nencountered with real-world LR images differ from the ones seen during\ntraining, FH models are still expected to perform well, but in practice may not\nproduce the desired results. In this paper we study this problem and explore\nthe bias introduced into FH models by the characteristics of the training data.\nWe systematically analyze the generalization capabilities of several FH models\nin various scenarios, where the image the degradation function does not match\nthe training setup and conduct experiments with synthetically downgraded as\nwell as real-life low-quality images. We make several interesting findings that\nprovide insight into existing problems with FH models and point to future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 09:25:55 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Grm", "Klemen", ""], ["Pernu\u0161", "Martin", ""], ["Cluzel", "Leo", ""], ["Scheirer", "Walter", ""], ["Dobri\u0161ek", "Simon", ""], ["\u0160truc", "Vitomir", ""]]}, {"id": "1812.09025", "submitter": "Michael Werman", "authors": "Erez Yahalomi, Michael Chernofsky and Michael Werman", "title": "Detection of distal radius fractures trained by a small set of X-ray\n  images and Faster R-CNN", "comments": null, "journal-ref": "Computing Conference 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distal radius fractures are the most common fractures of the upper extremity\nin humans. As such, they account for a significant portion of the injuries that\npresent to emergency rooms and clinics throughout the world. We trained a\nFaster R-CNN, a machine vision neural network for object detection, to identify\nand locate distal radius fractures in anteroposterior X-ray images. We achieved\nan accuracy of 96\\% in identifying fractures and mean Average Precision, mAP,\nof 0.866. This is significantly more accurate than the detection achieved by\nphysicians and radiologists. These results were obtained by training the deep\nlearning network with only 38 original images of anteroposterior hands X-ray\nimages with fractures. This opens the possibility to detect with this type of\nneural network rare diseases or rare symptoms of common diseases , where only a\nsmall set of diagnosed X-ray images could be collected for each disease.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 10:13:14 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Yahalomi", "Erez", ""], ["Chernofsky", "Michael", ""], ["Werman", "Michael", ""]]}, {"id": "1812.09041", "submitter": "Boyang Li", "authors": "Guoyun Tu, Yanwei Fu, Boyang Li, Jiarui Gao, Yu-Gang Jiang, Xiangyang\n  Xue", "title": "A Multi-task Neural Approach for Emotion Attribution, Classification and\n  Summarization", "comments": "Authors' manuscript; published at the IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional content is a crucial ingredient in user-generated videos. However,\nthe sparsity of emotional expressions in the videos poses an obstacle to visual\nemotion analysis. In this paper, we propose a new neural approach, Bi-stream\nEmotion Attribution-Classification Network (BEAC-Net), to solve three related\nemotion analysis tasks: emotion recognition, emotion attribution, and\nemotion-oriented summarization, in a single integrated framework. BEAC-Net has\ntwo major constituents, an attribution network and a classification network.\nThe attribution network extracts the main emotional segment that classification\nshould focus on in order to mitigate the sparsity issue. The classification\nnetwork utilizes both the extracted segment and the original video in a\nbi-stream architecture. We contribute a new dataset for the emotion attribution\ntask with human-annotated ground-truth labels for emotion segments. Experiments\non two video datasets demonstrate superior performance of the proposed\nframework and the complementary nature of the dual classification streams.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 10:53:44 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 04:21:23 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Tu", "Guoyun", ""], ["Fu", "Yanwei", ""], ["Li", "Boyang", ""], ["Gao", "Jiarui", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1812.09046", "submitter": "Carole Sudre", "authors": "Carole H. Sudre, Beatriz Gomez Anson, Silvia Ingala, Chris D. Lane,\n  Daniel Jimenez, Lukas Haider, Thomas Varsavsky, Lorna Smith, H. Rolf J\\\"ager,\n  M. Jorge Cardoso", "title": "3D multirater RCNN for multimodal multiclass detection and\n  characterisation of extremely small objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extremely small objects (ESO) have become observable on clinical routine\nmagnetic resonance imaging acquisitions, thanks to a reduction in acquisition\ntime at higher resolution. Despite their small size (usually $<$10 voxels per\nobject for an image of more than $10^6$ voxels), these markers reflect tissue\ndamage and need to be accounted for to investigate the complete phenotype of\ncomplex pathological pathways. In addition to their very small size,\nvariability in shape and appearance leads to high labelling variability across\nhuman raters, resulting in a very noisy gold standard. Such objects are notably\npresent in the context of cerebral small vessel disease where enlarged\nperivascular spaces and lacunes, commonly observed in the ageing population,\nare thought to be associated with acceleration of cognitive decline and risk of\ndementia onset. In this work, we redesign the RCNN model to scale to 3D data,\nand to jointly detect and characterise these important markers of age-related\nneurovascular changes. We also propose training strategies enforcing the\ndetection of extremely small objects, ensuring a tractable and stable training\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 11:03:14 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Sudre", "Carole H.", ""], ["Anson", "Beatriz Gomez", ""], ["Ingala", "Silvia", ""], ["Lane", "Chris D.", ""], ["Jimenez", "Daniel", ""], ["Haider", "Lukas", ""], ["Varsavsky", "Thomas", ""], ["Smith", "Lorna", ""], ["J\u00e4ger", "H. Rolf", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1812.09079", "submitter": "Soo Ye Kim", "authors": "Soo Ye Kim, Jeongyeon Lim, Taeyoung Na, Munchurl Kim", "title": "3DSRnet: Video Super-resolution using 3D Convolutional Neural Networks", "comments": "Extension of our paper accepted at ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video super-resolution, the spatio-temporal coherence between, and among\nthe frames must be exploited appropriately for accurate prediction of the high\nresolution frames. Although 2D convolutional neural networks (CNNs) are\npowerful in modelling images, 3D-CNNs are more suitable for spatio-temporal\nfeature extraction as they can preserve temporal information. To this end, we\npropose an effective 3D-CNN for video super-resolution, called the 3DSRnet that\ndoes not require motion alignment as preprocessing. Our 3DSRnet maintains the\ntemporal depth of spatio-temporal feature maps to maximally capture the\ntemporally nonlinear characteristics between low and high resolution frames,\nand adopts residual learning in conjunction with the sub-pixel outputs. It\noutperforms the most state-of-the-art method with average 0.45 and 0.36 dB\nhigher in PSNR for scales 3 and 4, respectively, in the Vidset4 benchmark. Our\n3DSRnet first deals with the performance drop due to scene change, which is\nimportant in practice but has not been previously considered.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 12:31:42 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 07:18:50 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Kim", "Soo Ye", ""], ["Lim", "Jeongyeon", ""], ["Na", "Taeyoung", ""], ["Kim", "Munchurl", ""]]}, {"id": "1812.09111", "submitter": "Timoth\\'ee Lesort", "authors": "Timoth\\'ee Lesort, Hugo Caselles-Dupr\\'e, Michael Garcia-Ortiz, Andrei\n  Stoian, David Filliat", "title": "Generative Models from the perspective of Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which generative model is the most suitable for Continual Learning? This\npaper aims at evaluating and comparing generative models on disjoint sequential\nimage generation tasks. We investigate how several models learn and forget,\nconsidering various strategies: rehearsal, regularization, generative replay\nand fine-tuning. We used two quantitative metrics to estimate the generation\nquality and memory ability. We experiment with sequential tasks on three\ncommonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and\nCIFAR10). We found that among all models, the original GAN performs best and\namong Continual Learning strategies, generative replay outperforms all other\nmethods. Even if we found satisfactory combinations on MNIST and Fashion MNIST,\ntraining generative models sequentially on CIFAR10 is particularly instable,\nand remains a challenge. Our code is available online\n\\footnote{\\url{https://github.com/TLESORT/Generative\\_Continual\\_Learning}}.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 13:35:32 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Lesort", "Timoth\u00e9e", ""], ["Caselles-Dupr\u00e9", "Hugo", ""], ["Garcia-Ortiz", "Michael", ""], ["Stoian", "Andrei", ""], ["Filliat", "David", ""]]}, {"id": "1812.09119", "submitter": "Hichem Sahbi", "authors": "Hichem Sahbi", "title": "Cascaded Coarse-to-Fine Deep Kernel Networks for Efficient Satellite\n  Image Change Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are nowadays becoming popular in many computer vision and\npattern recognition tasks. Among these networks, deep kernels are particularly\ninteresting and effective, however, their computational complexity is a major\nissue especially on cheap hardware resources. In this paper, we address the\nissue of efficient computation in deep kernel networks. We propose a novel\nframework that reduces dramatically the complexity of evaluating these deep\nkernels. Our method is based on a coarse-to-fine cascade of networks designed\nfor efficient computation; early stages of the cascade are cheap and reject\nmany patterns efficiently while deep stages are more expensive and accurate.\nThe design principle of these reduced complexity networks is based on a variant\nof the cross-entropy criterion that reduces the complexity of the networks in\nthe cascade while preserving all the positive responses of the original kernel\nnetwork. Experiments conducted - on the challenging and time demanding change\ndetection task, on very large satellite images - show that our proposed\ncoarse-to-fine approach is effective and highly efficient.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 13:52:17 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Sahbi", "Hichem", ""]]}, {"id": "1812.09127", "submitter": "Trung Nguyen", "authors": "Trung Nguyen, Barth Lakshmanan, Weihua Sheng", "title": "A Smart Security System with Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-based technology has improved drastically in the past decade. As a\nresult, security technology has become a major help to protect our daily life.\nIn this paper, we propose a robust security based on face recognition system\n(SoF). In particular, we develop this system to giving access into a home for\nauthenticated users. The classifier is trained by using a new adaptive learning\nmethod. The training data are initially collected from social networks. The\naccuracy of the classifier is incrementally improved as the user starts using\nthe system. A novel method has been introduced to improve the classifier model\nby human interaction and social media. By using a deep learning framework -\nTensorFlow, it will be easy to reuse the framework to adopt with many devices\nand applications.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 07:57:14 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Nguyen", "Trung", ""], ["Lakshmanan", "Barth", ""], ["Sheng", "Weihua", ""]]}, {"id": "1812.09131", "submitter": "Chang Liu", "authors": "Chang Liu, Zhaowei Shang, Anyong Qin", "title": "A Multiscale Image Denoising Algorithm Based On Dilated Residual\n  Convolution Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is a classical problem in low level computer vision.\nModel-based optimization methods and deep learning approaches have been the two\nmain strategies for solving the problem. Model-based optimization methods are\nflexible for handling different inverse problems but are usually\ntime-consuming. In contrast, deep learning methods have fast testing speed but\nthe performance of these CNNs is still inferior. To address this issue, here we\npropose a novel deep residual learning model that combines the dilated residual\nconvolution and multi-scale convolution groups. Due to the complex patterns and\nstructures of inside an image, the multiscale convolution group is utilized to\nlearn those patterns and enlarge the receptive field. Specifically, the\nresidual connection and batch normalization are utilized to speed up the\ntraining process and maintain the denoising performance. In order to decrease\nthe gridding artifacts, we integrate the hybrid dilated convolution design into\nour model. To this end, this paper aims to train a lightweight and effective\ndenoiser based on multiscale convolution group. Experimental results have\ndemonstrated that the enhanced denoiser can not only achieve promising\ndenoising results, but also become a strong competitor in practical\napplication.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:13:07 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Liu", "Chang", ""], ["Shang", "Zhaowei", ""], ["Qin", "Anyong", ""]]}, {"id": "1812.09162", "submitter": "Nicolas Le Scouarnec", "authors": "Fabien Andr\\'e, Anne-Marie Kermarrec and Nicolas Le Scouarnec", "title": "Quicker ADC : Unlocking the hidden potential of Product Quantization\n  with SIMD", "comments": "Open-source implementation at\n  http://github.com/nlescoua/faiss-quickeradc", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019 Early Access", "doi": "10.1109/TPAMI.2019.2952606", "report-no": null, "categories": "cs.CV cs.AI cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. A common approach is to rely\non Product Quantization, which allows the storage of large vector databases in\nmemory and efficient distance computations. Yet, implementations of nearest\nneighbor search with Product Quantization have their performance limited by the\nmany memory accesses they perform. Following this observation, Andr\\'e et al.\nproposed Quick ADC with up to $6\\times$ faster implementations of $m\\times{}4$\nproduct quantizers (PQ) leveraging specific SIMD instructions. Quicker ADC is a\ngeneralization of Quick ADC not limited to $m\\times{}4$ codes and supporting\nAVX-512, the latest revision of SIMD instruction set. In doing so, Quicker ADC\nfaces the challenge of using efficiently 5,6 and 7-bit shuffles that do not\nalign to computer bytes or words. To this end, we introduce (i) irregular\nproduct quantizers combining sub-quantizers of different granularity and (ii)\nsplit tables allowing lookup tables larger than registers. We evaluate Quicker\nADC with multiple indexes including Inverted Multi-Indexes and IVF HNSW and\nshow that it outperforms the reference optimized implementations (i.e., FAISS\nand polysemous codes) for numerous configurations. Finally, we release an\nopen-source fork of FAISS enhanced with Quicker ADC at\nhttp://github.com/nlescoua/faiss-quickeradc.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:51:27 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 21:39:48 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Andr\u00e9", "Fabien", ""], ["Kermarrec", "Anne-Marie", ""], ["Scouarnec", "Nicolas Le", ""]]}, {"id": "1812.09213", "submitter": "Pavel Tokmakov", "authors": "Pavel Tokmakov, Yu-Xiong Wang, Martial Hebert", "title": "Learning Compositional Representations for Few-Shot Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key limitations of modern deep learning approaches lies in the\namount of data required to train them. Humans, by contrast, can learn to\nrecognize novel categories from just a few examples. Instrumental to this rapid\nlearning ability is the compositional structure of concept representations in\nthe human brain --- something that deep learning models are lacking. In this\nwork, we make a step towards bridging this gap between human and machine\nlearning by introducing a simple regularization technique that allows the\nlearned representation to be decomposable into parts. Our method uses\ncategory-level attribute annotations to disentangle the feature space of a\nnetwork into subspaces corresponding to the attributes. These attributes can be\neither purely visual, like object parts, or more abstract, like openness and\nsymmetry. We demonstrate the value of compositional representations on three\ndatasets: CUB-200-2011, SUN397, and ImageNet, and show that they require fewer\nexamples to learn classifiers for novel categories.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 15:58:02 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 16:33:21 GMT"}, {"version": "v3", "created": "Sat, 17 Aug 2019 14:38:02 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Tokmakov", "Pavel", ""], ["Wang", "Yu-Xiong", ""], ["Hebert", "Martial", ""]]}, {"id": "1812.09232", "submitter": "Xiaoxiao Sun", "authors": "Xiaoxiao Sun, Liang Zheng, Yu-Kun Lai, Jufeng Yang", "title": "Learning from Web Data: the Benefit of Unsupervised Object Localization", "comments": "13 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating a large number of training images is very time-consuming. In this\nbackground, this paper focuses on learning from easy-to-acquire web data and\nutilizes the learned model for fine-grained image classification in labeled\ndatasets. Currently, the performance gain from training with web data is\nincremental, like a common saying \"better than nothing, but not by much\".\nConventionally, the community looks to correcting the noisy web labels to\nselect informative samples. In this work, we first systematically study the\nbuilt-in gap between the web and standard datasets, i.e. different data\ndistributions between the two kinds of data. Then, in addition to using web\nlabels, we present an unsupervised object localization method, which provides\ncritical insights into the object density and scale in web images.\nSpecifically, we design two constraints on web data to substantially reduce the\ndifference of data distributions for the web and standard datasets. First, we\npresent a method to control the scale, localization and number of objects in\nthe detected region. Second, we propose to select the regions containing\nobjects that are consistent with the web tag. Based on the two constraints, we\nare able to process web images to reduce the gap, and the processed web data is\nused to better assist the standard dataset to train CNNs. Experiments on\nseveral fine-grained image classification datasets confirm that our method\nperforms favorably against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 16:25:20 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Sun", "Xiaoxiao", ""], ["Zheng", "Liang", ""], ["Lai", "Yu-Kun", ""], ["Yang", "Jufeng", ""]]}, {"id": "1812.09271", "submitter": "Mangayarkarasi Ramaiah", "authors": "Mangayarkarasi Ramaiah, Dilip K. Prasad", "title": "Polygonal approximation of digital planar curve using novel significant\n  measure", "comments": "17 pages,15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an iterative smoothing technique for polygonal\napproximation of digital image boundary. The technique starts with finest\ninitial segmentation points of a curve. The contribution of initially segmented\npoints towards preserving the original shape of the image boundary is\ndetermined by computing the significant measure of every initial segmentation\npoints which is sensitive to sharp turns, which may be missed easily when\nconventional significant measures are used for detecting dominant points. The\nproposed method differentiates between the situations when a point on the curve\nbetween two points on a curve projects directly upon the line segment or beyond\nthis line segment. It not only identifies these situations, but also computes\nits significant contribution for these situations differently. This\nsituation-specific treatment allows preservation of points with high curvature\neven as revised set of dominant points are derived. The experimental results\nshow that the proposed technique competes well with the state of the art\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 17:25:58 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Ramaiah", "Mangayarkarasi", ""], ["Prasad", "Dilip K.", ""]]}, {"id": "1812.09276", "submitter": "Feras Almasri", "authors": "Feras Almasri, Olivier Debeir", "title": "Multimodal Sensor Fusion In Single Thermal image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast growth in the visual surveillance and security sectors, thermal\ninfrared images have become increasingly necessary ina large variety of\nindustrial applications. This is true even though IR sensors are still more\nexpensive than their RGB counterpart having the same resolution. In this paper,\nwe propose a deep learning solution to enhance the thermal image resolution.\nThe following results are given:(I) Introduction of a multimodal,\nvisual-thermal fusion model that ad-dresses thermal image super-resolution, via\nintegrating high-frequency information from the visual image. (II)\nInvestigation of different net-work architecture schemes in the literature,\ntheir up-sampling methods,learning procedures, and their optimization functions\nby showing their beneficial contribution to the super-resolution problem. (III)\nA bench-mark ULB17-VT dataset that contains thermal images and their visual\nimages counterpart is presented. (IV) Presentation of a qualitative evaluation\nof a large test set with 58 samples and 22 raters which shows that our proposed\nmodel performs better against state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 17:36:35 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Almasri", "Feras", ""], ["Debeir", "Olivier", ""]]}, {"id": "1812.09280", "submitter": "Hichem Sahbi", "authors": "Hichem Sahbi", "title": "Canonical Correlation Analysis for Misaligned Satellite Image Change\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a statistical learning method that\nseeks to build view-independent latent representations from multi-view data.\nThis method has been successfully applied to several pattern analysis tasks\nsuch as image-to-text mapping and view-invariant object/action recognition.\nHowever, this success is highly dependent on the quality of data pairing (i.e.,\nalignments) and mispairing adversely affects the generalization ability of the\nlearned CCA representations. In this paper, we address the issue of alignment\nerrors using a new variant of canonical correlation analysis referred to as\nalignment-agnostic (AA) CCA. Starting from erroneously paired data taken from\ndifferent views, this CCA finds transformation matrices by optimizing a\nconstrained maximization problem that mixes a data correlation term with\ncontext regularization; the particular design of these two terms mitigates the\neffect of alignment errors when learning the CCA transformations. Experiments\nconducted on multi-view tasks, including multi-temporal satellite image change\ndetection, show that our AA CCA method is highly effective and resilient to\nmispairing errors.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 17:43:16 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Sahbi", "Hichem", ""]]}, {"id": "1812.09336", "submitter": "Yihui He", "authors": "Devesh Walawalkar, Yihui He, Rohit Pillai", "title": "An Empirical Analysis of Deep Audio-Visual Models for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we worked on speech recognition, specifically predicting\nindividual words based on both the video frames and audio. Empowered by\nconvolutional neural networks, the recent speech recognition and lip reading\nmodels are comparable to human level performance. We re-implemented and made\nderivations of the state-of-the-art model. Then, we conducted rich experiments\nincluding the effectiveness of attention mechanism, more accurate residual\nnetwork as the backbone with pre-trained weights and the sensitivity of our\nmodel with respect to audio input with/without noise.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 19:02:52 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Walawalkar", "Devesh", ""], ["He", "Yihui", ""], ["Pillai", "Rohit", ""]]}, {"id": "1812.09366", "submitter": "Sameer Ansari", "authors": "Sameer Ansari, Neal Wadhwa, Rahul Garg, Jiawen Chen", "title": "Wireless Software Synchronization of Multiple Distributed Cameras", "comments": "Main: 9 pages, 10 figures. Supplemental: 3 pages, 5 figures", "journal-ref": null, "doi": "10.1109/ICCPHOT.2019.8747340", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for precisely time-synchronizing the capture of image\nsequences from a collection of smartphone cameras connected over WiFi. Our\nmethod is entirely software-based, has only modest hardware requirements, and\nachieves an accuracy of less than 250 microseconds on unmodified commodity\nhardware. It does not use image content and synchronizes cameras prior to\ncapture. The algorithm operates in two stages. In the first stage, we designate\none device as the leader and synchronize each client device's clock to it by\nestimating network delay. Once clocks are synchronized, the second stage\ninitiates continuous image streaming, estimates the relative phase of image\ntimestamps between each client and the leader, and shifts the streams into\nalignment. We quantitatively validate our results on a multi-camera rig imaging\na high-precision LED array and qualitatively demonstrate significant\nimprovements to multi-view stereo depth estimation and stitching of dynamic\nscenes. We release as open source 'libsoftwaresync', an Android implementation\nof our system, to inspire new types of collective capture applications.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 20:48:11 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 21:21:24 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Ansari", "Sameer", ""], ["Wadhwa", "Neal", ""], ["Garg", "Rahul", ""], ["Chen", "Jiawen", ""]]}, {"id": "1812.09375", "submitter": "Thomas K\\\"ohler", "authors": "Thomas K\\\"ohler", "title": "Multi-Frame Super-Resolution Reconstruction with Applications to Medical\n  Imaging", "comments": "Ph.D. thesis at the Friedrich-Alexander-Universit\\\"at (FAU)\n  Erlangen-N\\\"urnberg; source code is available at\n  https://www5.cs.fau.de/de/forschung/software/multi-frame-super-resolution-toolbox/\n  . https://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/9145", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optical resolution of a digital camera is one of its most crucial\nparameters with broad relevance for consumer electronics, surveillance systems,\nremote sensing, or medical imaging. However, resolution is physically limited\nby the optics and sensor characteristics. In addition, practical and economic\nreasons often stipulate the use of out-dated or low-cost hardware.\nSuper-resolution is a class of retrospective techniques that aims at\nhigh-resolution imagery by means of software. Multi-frame algorithms approach\nthis task by fusing multiple low-resolution frames to reconstruct\nhigh-resolution images. This work covers novel super-resolution methods along\nwith new applications in medical imaging.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 21:21:11 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["K\u00f6hler", "Thomas", ""]]}, {"id": "1812.09395", "submitter": "Nima Mohajerin", "authors": "Nima Mohajerin and Mohsen Rohani", "title": "Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the multi-step prediction of the drivable space, represented\nby Occupancy Grid Maps (OGMs), for autonomous vehicles. Our motivation is that\naccurate multi-step prediction of the drivable space can efficiently improve\npath planning and navigation resulting in safe, comfortable and optimum paths\nin autonomous driving. We train a variety of Recurrent Neural Network (RNN)\nbased architectures on the OGM sequences from the KITTI dataset. The results\ndemonstrate significant improvement of the prediction accuracy using our\nproposed difference learning method, incorporating motion related features,\nover the state of the art. We remove the egomotion from the OGM sequences by\ntransforming them into a common frame. Although in the transformed sequences\nthe KITTI dataset is heavily biased toward static objects, by learning the\ndifference between subsequent OGMs, our proposed method provides accurate\nprediction over both the static and moving objects.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 22:27:10 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 19:02:46 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 20:29:59 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Mohajerin", "Nima", ""], ["Rohani", "Mohsen", ""]]}, {"id": "1812.09427", "submitter": "Yongli Zhu", "authors": "Yongli Zhu, Chengxi Liu, Kai Sun", "title": "Image Embedding of PMU Data for Deep Learning towards Transient\n  Disturbance Classification", "comments": "An updated version of this manuscript has been accepted by the 2018\n  IEEE International Conference on Energy Internet (ICEI), Beijing, China", "journal-ref": null, "doi": "10.1109/ICEI.2018.00038", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study on power grid disturbance classification by Deep\nLearning (DL). A real synchrophasor set composing of three different types of\ndisturbance events from the Frequency Monitoring Network (FNET) is used. An\nimage embedding technique called Gramian Angular Field is applied to transform\neach time series of event data to a two-dimensional image for learning. Two\nmain DL algorithms, i.e. CNN (Convolutional Neural Network) and RNN (Recurrent\nNeural Network) are tested and compared with two widely used data mining tools,\nthe Support Vector Machine and Decision Tree. The test results demonstrate the\nsuperiority of the both DL algorithms over other methods in the application of\npower system transient disturbance classification.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 01:08:47 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zhu", "Yongli", ""], ["Liu", "Chengxi", ""], ["Sun", "Kai", ""]]}, {"id": "1812.09431", "submitter": "Chi Zhang", "authors": "Chi Zhang, Xiaohan Duan, Linyuan Wang, Yongli Li, Bin Yan, Guoen Hu,\n  Ruyuan Zhang, Li Tong", "title": "Dissociable neural representations of adversarially perturbed images in\n  convolutional neural networks and the human brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable similarities between convolutional neural networks\n(CNN) and the human brain, CNNs still fall behind humans in many visual tasks,\nindicating that there still exist considerable differences between the two\nsystems. Here, we leverage adversarial noise (AN) and adversarial interference\n(AI) images to quantify the consistency between neural representations and\nperceptual outcomes in the two systems. Humans can successfully recognize AI\nimages as corresponding categories but perceive AN images as meaningless noise.\nIn contrast, CNNs can correctly recognize AN images but mistakenly classify AI\nimages into wrong categories with surprisingly high confidence. We use\nfunctional magnetic resonance imaging to measure brain activity evoked by\nregular and adversarial images in the human brain, and compare it to the\nactivity of artificial neurons in a prototypical CNN-AlexNet. In the human\nbrain, we find that the representational similarity between regular and\nadversarial images largely echoes their perceptual similarity in all early\nvisual areas. In AlexNet, however, the neural representations of adversarial\nimages are inconsistent with network outputs in all intermediate processing\nlayers, providing no neural foundations for perceptual similarity. Furthermore,\nwe show that voxel-encoding models trained on regular images can successfully\ngeneralize to the neural responses to AI images but not AN images. These\nremarkable differences between the human brain and AlexNet in the\nrepresentation-perception relation suggest that future CNNs should emulate both\nbehavior and the internal neural presentations of the human brain.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 01:56:04 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 04:47:10 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 01:28:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Chi", ""], ["Duan", "Xiaohan", ""], ["Wang", "Linyuan", ""], ["Li", "Yongli", ""], ["Yan", "Bin", ""], ["Hu", "Guoen", ""], ["Zhang", "Ruyuan", ""], ["Tong", "Li", ""]]}, {"id": "1812.09477", "submitter": "Tingxiao Yang", "authors": "Tingxiao Yang, Yuichiro Yoshimura, Akira Morita, Takao Namiki, Toshiya\n  Nakaguchi", "title": "Fully Automatic Segmentation of Sublingual Veins from Retrained U-Net\n  Model for Few Near Infrared Images", "comments": "IMQA 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sublingual vein is commonly used to diagnose the health status. The width of\nmain sublingual veins gives information of the blood circulation. Therefore, it\nis necessary to segment the main sublingual veins from the tongue\nautomatically. In general, the dataset in the medical field is small, which is\na challenge for training the deep learning model. In order to train the model\nwith a small data set, the proposed method for automatically segmenting the\nsublingual veins is to re-train U-net model with different sets of the limited\nnumber of labels for the same training images. With pre-knowledge of the\nsegmentation, the loss of the trained model will be convergence easier. To\nimprove the performance of the segmentation further, a novel strategy of data\naugmentation was utilized. The operation for masking output of the model with\nthe input was randomly switched on or switched off in each training step. This\napproach will force the model to learn the contrast invariance and avoid\noverfitting. Images of dataset were taken with the developed device using eight\nnear infrared LEDs. The final segmentation results were evaluated on the\nvalidation dataset by the IoU metric.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 08:27:45 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Yang", "Tingxiao", ""], ["Yoshimura", "Yuichiro", ""], ["Morita", "Akira", ""], ["Namiki", "Takao", ""], ["Nakaguchi", "Toshiya", ""]]}, {"id": "1812.09502", "submitter": "Zhilin Zheng", "authors": "Zhilin Zheng, Li Sun", "title": "Disentangling Latent Space for VAE by Label Relevant/Irrelevant\n  Dimensions", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VAE requires the standard Gaussian distribution as a prior in the latent\nspace. Since all codes tend to follow the same prior, it often suffers the\nso-called \"posterior collapse\". To avoid this, this paper introduces the class\nspecific distribution for the latent code. But different from CVAE, we present\na method for disentangling the latent space into the label relevant and\nirrelevant dimensions, $\\bm{\\mathrm{z}}_s$ and $\\bm{\\mathrm{z}}_u$, for a\nsingle input. We apply two separated encoders to map the input into\n$\\bm{\\mathrm{z}}_s$ and $\\bm{\\mathrm{z}}_u$ respectively, and then give the\nconcatenated code to the decoder to reconstruct the input. The label irrelevant\ncode $\\bm{\\mathrm{z}}_u$ represent the common characteristics of all inputs,\nhence they are constrained by the standard Gaussian, and their encoder is\ntrained in amortized variational inference way, like VAE. While\n$\\bm{\\mathrm{z}}_s$ is assumed to follow the Gaussian mixture distribution in\nwhich each component corresponds to a particular class. The parameters for the\nGaussian components in $\\bm{\\mathrm{z}}_s$ encoder are optimized by the label\nsupervision in a global stochastic way. In theory, we show that our method is\nactually equivalent to adding a KL divergence term on the joint distribution of\n$\\bm{\\mathrm{z}}_s$ and the class label $c$, and it can directly increase the\nmutual information between $\\bm{\\mathrm{z}}_s$ and the label $c$. Our model can\nalso be extended to GAN by adding a discriminator in the pixel domain so that\nit produces high quality and diverse images.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 11:09:50 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 05:19:34 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 13:36:24 GMT"}, {"version": "v4", "created": "Fri, 15 Mar 2019 12:01:42 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Zheng", "Zhilin", ""], ["Sun", "Li", ""]]}, {"id": "1812.09530", "submitter": "Guangyao Shi", "authors": "Hong Huang, Guangyao Shi, Haibo He, Yule Duan, Fulin Luo", "title": "Dimensionality Reduction of Hyperspectral Imagery Based on\n  Spatial-spectral Manifold Learning", "comments": "under review in IEEE Transactions On Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph embedding (GE) methods have been widely applied for dimensionality\nreduction of hyperspectral imagery (HSI). However, a major challenge of GE is\nhow to choose proper neighbors for graph construction and explore the spatial\ninformation of HSI data. In this paper, we proposed an unsupervised\ndimensionality reduction algorithm termed spatial-spectral manifold\nreconstruction preserving embedding (SSMRPE) for HSI classification. At first,\na weighted mean filter (WMF) is employed to preprocess the image, which aims to\nreduce the influence of background noise. According to the spatial consistency\nproperty of HSI, the SSMRPE method utilizes a new spatial-spectral combined\ndistance (SSCD) to fuse the spatial structure and spectral information for\nselecting effective spatial-spectral neighbors of HSI pixels. Then, it explores\nthe spatial relationship between each point and its neighbors to adjusts the\nreconstruction weights for improving the efficiency of manifold reconstruction.\nAs a result, the proposed method can extract the discriminant features and\nsubsequently improve the classification performance of HSI. The experimental\nresults on PaviaU and Salinas hyperspectral datasets indicate that SSMRPE can\nachieve better classification accuracies in comparison with some\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 14:06:21 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Huang", "Hong", ""], ["Shi", "Guangyao", ""], ["He", "Haibo", ""], ["Duan", "Yule", ""], ["Luo", "Fulin", ""]]}, {"id": "1812.09533", "submitter": "Zixi Cai", "authors": "Zixi Cai, Helmut Neher, Kanav Vats, David Clausi, John Zelek", "title": "Temporal Hockey Action Recognition via Pose and Optical Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing actions in ice hockey using computer vision poses challenges due\nto bulky equipment and inadequate image quality. A novel two-stream framework\nhas been designed to improve action recognition accuracy for hockey using three\nmain components. First, pose is estimated via the Part Affinity Fields model to\nextract meaningful cues from the player. Second, optical flow (using\nLiteFlowNet) is used to extract temporal features. Third, pose and optical flow\nstreams are fused and passed to fully-connected layers to estimate the hockey\nplayer's action. A novel publicly available dataset named HARPET (Hockey Action\nRecognition Pose Estimation, Temporal) was created, composed of sequences of\nannotated actions and pose of hockey players including their hockey sticks as\nan extension of human body pose. Three contributions are recognized. (1) The\nnovel two-stream architecture achieves 85% action recognition accuracy, with\nthe inclusion of optical flows increasing accuracy by about 10%. (2) The unique\nlocalization of hand-held objects (e.g., hockey sticks) as part of pose\nincreases accuracy by about 13%. (3) For pose estimation, a bigger and more\ngeneral dataset, MSCOCO, is successfully used for transfer learning to a\nsmaller and more specific dataset, HARPET, achieving a PCKh of 87%.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 14:15:15 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Cai", "Zixi", ""], ["Neher", "Helmut", ""], ["Vats", "Kanav", ""], ["Clausi", "David", ""], ["Zelek", "John", ""]]}, {"id": "1812.09569", "submitter": "Sergey Belim", "authors": "S.V. Belim, S.B. Larionov", "title": "The algorithm of formation of a training set for an artificial neural\n  network for image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article suggests an algorithm of formation a training set for artificial\nneural network in case of image segmentation. The distinctive feature of this\nalgorithm is that it using only one image for segmentation. The segmentation\nperforms using three-layer perceptron. The main method of the segmentation is a\nmethod of region growing. Neural network is using for get a decision to include\npixel into an area or not. Impulse noise is using for generation of a training\nset. Pixels damaged by noise are not related to the same region. Suggested\nmethod has been tested with help of computer experiment in automatic and\ninteractive modes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 17:21:59 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Belim", "S. V.", ""], ["Larionov", "S. B.", ""]]}, {"id": "1812.09570", "submitter": "Emrah Basaran", "authors": "Emrah Basaran, Yonatan Tariku Tesfaye, Mubarak Shah", "title": "EgoReID Dataset: Person Re-identification in Videos Acquired by Mobile\n  Devices with First-Person Point-of-View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have seen the performance of video-based person\nRe-Identification (ReID) methods have improved considerably. However, most of\nthe work in this area has dealt with videos acquired by fixed cameras with\nwider field of view. Recently, widespread use of wearable cameras and recording\ndevices such as cellphones have opened the door to interesting research in\nfirst-person Point-of-view (POV) videos (egocentric videos). Nonetheless,\nanalysis of such videos is challenging due to factors such as poor video\nquality due to ego-motion, blurriness, severe changes in lighting conditions\nand perspective distortions. To facilitate the research towards conquering\nthese challenges, this paper contributes a new dataset called EgoReID. The\ndataset is captured using 3 mobile cellphones with non-overlapping\nfield-of-view. It contains 900 IDs and around 10,200 tracks with a total of\n176,000 detections. The dataset also contains 12-sensor meta data e.g. camera\norientation pitch and rotation for each video.\n  In addition, we propose a new framework which takes advantage of both visual\nand sensor meta data to successfully perform Person ReID. We extend image-based\nre-ID method employing human body parsing trained on ten datasets to\nvideo-based re-ID. In our method, first frame level local features are\nextracted for each semantic region, then 3D convolutions are applied to encode\nthe temporal information in each sequence of semantic regions. Additionally, we\nemploy sensor meta data to predict targets' next camera and their estimated\ntime of arrival, which considerably improves our ReID performance as it\nsignificantly reduces our search space.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 17:32:36 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 10:46:30 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 03:36:28 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 21:28:51 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Basaran", "Emrah", ""], ["Tesfaye", "Yonatan Tariku", ""], ["Shah", "Mubarak", ""]]}, {"id": "1812.09629", "submitter": "Kazutaka Uchida", "authors": "Kazutaka Uchida, Masayuki Tanaka, and Masatoshi Okutomi", "title": "Estimation and Restoration of Compositional Degradation Using\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration from a single image degradation type, such as blurring,\nhazing, random noise, and compression has been investigated for decades.\nHowever, image degradations in practice are often a mixture of several types of\ndegradation. Such compositional degradations complicate restoration because\nthey require the differentiation of different degradation types and levels. In\nthis paper, we propose a convolutional neural network (CNN) model for\nestimating the degradation properties of a given degraded image. Furthermore,\nwe introduce an image restoration CNN model that adopts the estimated\ndegradation properties as its input. Experimental results show that the\nproposed degradation estimation model can successfully infer the degradation\nproperties of compositionally degraded images. The proposed restoration model\ncan restore degraded images by exploiting the estimated degradation properties\nand can achieve both blind and nonblind image restorations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 00:51:10 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Uchida", "Kazutaka", ""], ["Tanaka", "Masayuki", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "1812.09648", "submitter": "Yang Hu Dr.", "authors": "Yingxue Xu, Guihua Wen, Yang Hu, Mingnan Luo, Dan Dai, Yishan Zhuang", "title": "Chinese Herbal Recognition based on Competitive Attentional Fusion of\n  Multi-hierarchies Pyramid Features", "comments": "New Datasets for Chinese Herbs Recognition", "journal-ref": "[J]. Pattern Recognition, 2021, 110: 107558", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution neural netwotks (CNNs) are successfully applied in image\nrecognition task. In this study, we explore the approach of automatic herbal\nrecognition with CNNs and build the standard Chinese herbs datasets firstly.\nAccording to the characteristics of herbal images, we proposed the competitive\nattentional fusion pyramid networks to model the features of herbal image,\nwhich mdoels the relationship of feature maps from different levels, and\nre-weights multi-level channels with channel-wise attention mechanism. In this\nway, we can dynamically adjust the weight of feature maps from various layers,\naccording to the visual characteristics of each herbal image. Moreover, we also\nintroduce the spatial attention to recalibrate the misaligned features caused\nby sampling in features amalgamation. Extensive experiments are conducted on\nour proposed datasets and validate the superior performance of our proposed\nmodels. The Chinese herbs datasets will be released upon acceptance to\nfacilitate the research of Chinese herbal recognition.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 03:31:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Xu", "Yingxue", ""], ["Wen", "Guihua", ""], ["Hu", "Yang", ""], ["Luo", "Mingnan", ""], ["Dai", "Dan", ""], ["Zhuang", "Yishan", ""]]}, {"id": "1812.09681", "submitter": "Zhuoqian Yang", "authors": "Zhuoqian Yang, Zengchang Qin, Jing Yu, Yue Hu", "title": "Scene Graph Reasoning with Prior Visual Relationship for Visual Question\n  Answering", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key issues of Visual Question Answering (VQA) is to reason with\nsemantic clues in the visual content under the guidance of the question, how to\nmodel relational semantics still remains as a great challenge. To fully capture\nvisual semantics, we propose to reason over a structured visual representation\n- scene graph, with embedded objects and inter-object relationships. This shows\ngreat benefit over vanilla vector representations and implicit visual\nrelationship learning. Based on existing visual relationship models, we propose\na visual relationship encoder that projects visual relationships into a learned\ndeep semantic space constrained by visual context and language priors. Upon the\nconstructed graph, we propose a Scene Graph Convolutional Network (SceneGCN) to\njointly reason the object properties and relational semantics for the correct\nanswer. We demonstrate the model's effectiveness and interpretability on the\nchallenging GQA dataset and the classical VQA 2.0 dataset, remarkably achieving\nstate-of-the-art 54.56% accuracy on GQA compared to the existing best model.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 09:59:49 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 16:42:04 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Yang", "Zhuoqian", ""], ["Qin", "Zengchang", ""], ["Yu", "Jing", ""], ["Hu", "Yue", ""]]}, {"id": "1812.09693", "submitter": "Diganta Misra", "authors": "Diganta Misra and Vanshika Arora", "title": "Image Processing on IOPA Radiographs: A comprehensive case study on\n  Apical Periodontitis", "comments": "15 pages, 42 figures and Submitted at ICIAP 2019: 21st International\n  Conference on Image Analysis and Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advancements in Image Processing Techniques and development\nof new robust computer vision algorithms, new areas of research within Medical\nDiagnosis and Biomedical Engineering are picking up pace. This paper provides a\ncomprehensive in-depth case study of Image Processing, Feature Extraction and\nAnalysis of Apical Periodontitis diagnostic cases in IOPA (Intra Oral\nPeri-Apical) Radiographs, a common case in oral diagnostic pipeline. This paper\nprovides a detailed analytical approach towards improving the diagnostic\nprocedure with improved and faster results with higher accuracy targeting to\neliminate True Negative and False Positive cases.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 11:38:35 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 14:02:33 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Misra", "Diganta", ""], ["Arora", "Vanshika", ""]]}, {"id": "1812.09702", "submitter": "Sparsha Mishra", "authors": "Diganta Misra, Sparsha Mishra and Bhargav Appasani", "title": "Advanced Image Processing for Astronomical Images", "comments": "7 pages, 13 figures, accepted at IEEE International Conference on\n  Electrical, Communication, Electronics, Instrumentation and Computing\n  (ICECEIC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Processing in Astronomy is a major field of research and involves a lot\nof techniques pertaining to improve analyzing the properties of the celestial\nobjects or obtaining preliminary inference from the image data. In this paper,\nwe provide a comprehensive case study of advanced image processing techniques\napplied to Astronomical Galaxy Images for improved analysis, accurate\ninferences and faster analysis.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 13:18:26 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Misra", "Diganta", ""], ["Mishra", "Sparsha", ""], ["Appasani", "Bhargav", ""]]}, {"id": "1812.09737", "submitter": "Siyu Tang", "authors": "Jie Song and Bjoern Andres and Michael Black and Otmar Hilliges and\n  Siyu Tang", "title": "End-to-end Learning for Graph Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel end-to-end trainable framework for the graph decomposition\nproblem. The minimum cost multicut problem is first converted to an\nunconstrained binary cubic formulation where cycle consistency constraints are\nincorporated into the objective function. The new optimization problem can be\nviewed as a Conditional Random Field (CRF) in which the random variables are\nassociated with the binary edge labels of the initial graph and the hard\nconstraints are introduced in the CRF as high-order potentials. The parameters\nof a standard Neural Network and the fully differentiable CRF are optimized in\nan end-to-end manner. Furthermore, our method utilizes the cycle constraints as\nmeta-supervisory signals during the learning of the deep feature\nrepresentations by taking the dependencies between the output random variables\ninto account. We present analyses of the end-to-end learned representations,\nshowing the impact of the joint training, on the task of clustering images of\nMNIST. We also validate the effectiveness of our approach both for the feature\nlearning and the final clustering on the challenging task of real-world\nmulti-person pose estimation.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 16:21:08 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Song", "Jie", ""], ["Andres", "Bjoern", ""], ["Black", "Michael", ""], ["Hilliges", "Otmar", ""], ["Tang", "Siyu", ""]]}, {"id": "1812.09744", "submitter": "Pooran Singh Negi", "authors": "Pooran Singh Negi, David chan, Mohammad Mahoor", "title": "Leveraging Class Similarity to Improve Deep Neural Network Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally artificial neural networks (ANNs) are trained by minimizing the\ncross-entropy between a provided groundtruth delta distribution (encoded as\none-hot vector) and the ANN's predictive softmax distribution. It seems,\nhowever, unacceptable to penalize networks equally for missclassification\nbetween classes. Confusing the class \"Automobile\" with the class \"Truck\" should\nbe penalized less than confusing the class \"Automobile\" with the class\n\"Donkey\". To avoid such representation issues and learn cleaner classification\nboundaries in the network, this paper presents a variation of cross-entropy\nloss which depends not only on the sample class but also on a data-driven prior\n\"class-similarity distribution\" across the classes encoded in a matrix form. We\nexplore learning the class-similarity distribution using a datadriven method\nand then show that by training with our modified similarity-driven loss, we\nobtain slightly better generalization performance over multiple architectures\nand datasets as well as improved performance on noisy testing scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 17:33:29 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 05:40:47 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Negi", "Pooran Singh", ""], ["chan", "David", ""], ["Mahoor", "Mohammad", ""]]}, {"id": "1812.09793", "submitter": "Mehdi Zakroum", "authors": "Mehdi Zakroum, Mounir Ghogho, Mustapha Faqir and Mohamed Aymane\n  Ahajjam", "title": "Deep Learning for Inferring the Surface Solar Irradiance from Sky\n  Imagery", "comments": null, "journal-ref": null, "doi": "10.1109/IRSEC.2017.8477236", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to perform ground-based estimation and prediction\nof the surface solar irradiance with the view to predicting photovoltaic energy\nproduction. We propose the use of mini-batch k-means clustering to extract\nfeatures, referred to as per cluster number of pixels (PCNP), from sky images\ntaken by a low-cost fish eye camera. These features are first used to classify\nthe sky as clear or cloudy using a single hidden layer neural network; the\nclassification accuracy achieves 99.7%. If the sky is classified as cloudy, we\npropose to use a deep neural network having as input features the PCNP to\npredict intra-hour variability of the solar irradiance. Toward this objective,\nin this paper, we focus on estimating the deep neural network model relating\nthe PCNP features and the solar irradiance, which is an important step before\nperforming the prediction task. The proposed deep learning-based estimation\napproach is shown to have an accuracy of 95%.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 23:16:54 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zakroum", "Mehdi", ""], ["Ghogho", "Mounir", ""], ["Faqir", "Mustapha", ""], ["Ahajjam", "Mohamed Aymane", ""]]}, {"id": "1812.09803", "submitter": "Thomas Brunner", "authors": "Thomas Brunner, Frederik Diehl, Michael Truong Le, Alois Knoll", "title": "Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial\n  Attacks", "comments": "For source code and videos, see\n  https://github.com/ttbrunner/biased_boundary_attack", "journal-ref": null, "doi": "10.1109/ICCV.2019.00506", "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider adversarial examples for image classification in the black-box\ndecision-based setting. Here, an attacker cannot access confidence scores, but\nonly the final label. Most attacks for this scenario are either unreliable or\ninefficient. Focusing on the latter, we show that a specific class of attacks,\nBoundary Attacks, can be reinterpreted as a biased sampling framework that\ngains efficiency from domain knowledge. We identify three such biases, image\nfrequency, regional masks and surrogate gradients, and evaluate their\nperformance against an ImageNet classifier. We show that the combination of\nthese biases outperforms the state of the art by a wide margin. We also\nshowcase an efficient way to attack the Google Cloud Vision API, where we craft\nconvincing perturbations with just a few hundred queries. Finally, the methods\nwe propose have also been found to work very well against strong defenses: Our\ntargeted attack won second place in the NeurIPS 2018 Adversarial Vision\nChallenge.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 00:48:31 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 13:39:59 GMT"}, {"version": "v3", "created": "Sun, 5 May 2019 13:05:16 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Brunner", "Thomas", ""], ["Diehl", "Frederik", ""], ["Le", "Michael Truong", ""], ["Knoll", "Alois", ""]]}, {"id": "1812.09809", "submitter": "Zi-Rui Wang", "authors": "Zi-Rui Wang, Jun Du, Jia-Ming Wang", "title": "Writer-Aware CNN for Parsimonious HMM-Based Offline Handwritten Chinese\n  Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the hybrid convolutional neural network hidden Markov model\n(CNN-HMM) has been introduced for offline handwritten Chinese text recognition\n(HCTR) and has achieved state-of-the-art performance. However, modeling each of\nthe large vocabulary of Chinese characters with a uniform and fixed number of\nhidden states requires high memory and computational costs and makes the tens\nof thousands of HMM state classes confusing. Another key issue of CNN-HMM for\nHCTR is the diversified writing style, which leads to model strain and a\nsignificant performance decline for specific writers. To address these issues,\nwe propose a writer-aware CNN based on parsimonious HMM (WCNN-PHMM). First,\nPHMM is designed using a data-driven state-tying algorithm to greatly reduce\nthe total number of HMM states, which not only yields a compact CNN by state\nsharing of the same or similar radicals among different Chinese characters but\nalso improves the recognition accuracy due to the more accurate modeling of\ntied states and the lower confusion among them. Second, WCNN integrates each\nconvolutional layer with one adaptive layer fed by a writer-dependent vector,\nnamely, the writer code, to extract the irrelevant variability in writer\ninformation to improve recognition performance. The parameters of\nwriter-adaptive layers are jointly optimized with other network parameters in\nthe training stage, while a multiple-pass decoding strategy is adopted to learn\nthe writer code and generate recognition results. Validated on the ICDAR 2013\ncompetition of CASIA-HWDB database, the more compact WCNN-PHMM of a 7360-class\nvocabulary can achieve a relative character error rate (CER) reduction of 16.6%\nover the conventional CNN-HMM without considering language modeling. By\nadopting a powerful hybrid language model (N-gram language model and recurrent\nneural network language model), the CER of WCNN-PHMM is reduced to 3.17%.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 02:38:08 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 02:50:28 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Wang", "Zi-Rui", ""], ["Du", "Jun", ""], ["Wang", "Jia-Ming", ""]]}, {"id": "1812.09818", "submitter": "Eunhyeok Park", "authors": "Eunhyeok Park, Dongyoung Kim, Sungjoo Yoo and Peter Vajda", "title": "Precision Highway for Ultra Low-Precision Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network quantization has an inherent problem called accumulated\nquantization error, which is the key obstacle towards ultra-low precision,\ne.g., 2- or 3-bit precision. To resolve this problem, we propose precision\nhighway, which forms an end-to-end high-precision information flow while\nperforming the ultra low-precision computation. First, we describe how the\nprecision highway reduce the accumulated quantization error in both\nconvolutional and recurrent neural networks. We also provide the quantitative\nanalysis of the benefit of precision highway and evaluate the overhead on the\nstate-of-the-art hardware accelerator. In the experiments, our proposed method\noutperforms the best existing quantization methods while offering 3-bit\nweight/activation quantization with no accuracy loss and 2-bit quantization\nwith a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the\nproposed method significantly outperforms the existing method in the 2-bit\nquantization of an LSTM for language modeling.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 03:29:36 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Park", "Eunhyeok", ""], ["Kim", "Dongyoung", ""], ["Yoo", "Sungjoo", ""], ["Vajda", "Peter", ""]]}, {"id": "1812.09832", "submitter": "Xi Jia", "authors": "WenTing Chen, Xinpeng Xie, Xi Jia, Linlin Shen", "title": "Texture Deformation Based Generative Adversarial Networks for Face\n  Editing", "comments": "10 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant success in image-to-image translation and latent\nrepresentation based facial attribute editing and expression synthesis, the\nexisting approaches still have limitations in the sharpness of details,\ndistinct image translation and identity preservation. To address these issues,\nwe propose a Texture Deformation Based GAN, namely TDB-GAN, to disentangle\ntexture from original image and transfers domains based on the extracted\ntexture. The approach utilizes the texture to transfer facial attributes and\nexpressions without the consideration of the object pose. This leads to shaper\ndetails and more distinct visual effect of the synthesized faces. In addition,\nit brings the faster convergence during training. The effectiveness of the\nproposed method is validated through extensive ablation studies. We also\nevaluate our approach qualitatively and quantitatively on facial attribute and\nfacial expression synthesis. The results on both the CelebA and RaFD datasets\nsuggest that Texture Deformation Based GAN achieves better performance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 05:03:58 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Chen", "WenTing", ""], ["Xie", "Xinpeng", ""], ["Jia", "Xi", ""], ["Shen", "Linlin", ""]]}, {"id": "1812.09834", "submitter": "Guoyan Zheng", "authors": "Guodong Zeng and Guoyan Zheng", "title": "Holistic Decomposition Convolution for Effective Semantic Segmentation\n  of 3D MR Images", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have achieved state-of-the-art\nperformance in many different 2D medical image analysis tasks. In clinical\npractice, however, a large part of the medical imaging data available is in 3D.\nThis has motivated the development of 3D CNNs for volumetric image segmentation\nin order to benefit from more spatial context. Due to GPU memory restrictions\ncaused by moving to fully 3D, state-of-the-art methods depend on\nsubvolume/patch processing and the size of the input patch is usually small,\nlimiting the incorporation of larger context information for a better\nperformance. In this paper, we propose a novel Holistic Decomposition\nConvolution (HDC), for an effective and efficient semantic segmentation of\nvolumetric images. HDC consists of a periodic down-shuffling operation followed\nby a conventional 3D convolution. HDC has the advantage of significantly\nreducing the size of the data for sub-sequential processing while using all the\ninformation available in the input irrespective of the down-shuffling factors.\nResults obtained from comprehensive experiments conducted on hip T1 MR images\nand intervertebral disc T2 MR images demonstrate the efficacy of the present\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 05:18:18 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zeng", "Guodong", ""], ["Zheng", "Guoyan", ""]]}, {"id": "1812.09874", "submitter": "Oleg Voinov", "authors": "Oleg Voynov, Alexey Artemov, Vage Egiazarian, Alexander Notchenko,\n  Gleb Bobrovskikh, Denis Zorin, Evgeny Burnaev", "title": "Perceptual deep depth super-resolution", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGBD images, combining high-resolution color and lower-resolution depth from\nvarious types of depth sensors, are increasingly common. One can significantly\nimprove the resolution of depth maps by taking advantage of color information;\ndeep learning methods make combining color and depth information particularly\neasy. However, fusing these two sources of data may lead to a variety of\nartifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual\nreality applications, the visual quality of upsampled images is particularly\nimportant. The main idea of our approach is to measure the quality of depth map\nupsampling using renderings of resulting 3D surfaces. We demonstrate that a\nsimple visual appearance-based loss, when used with either a trained CNN or\nsimply a deep prior, yields significantly improved 3D shapes, as measured by a\nnumber of existing perceptual metrics. We compare this approach with a number\nof existing optimization and learning-based techniques.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 09:43:25 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 02:20:13 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 14:12:39 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Voynov", "Oleg", ""], ["Artemov", "Alexey", ""], ["Egiazarian", "Vage", ""], ["Notchenko", "Alexander", ""], ["Bobrovskikh", "Gleb", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1812.09877", "submitter": "Yazeed Alharbi", "authors": "Yazeed Alharbi, Neil Smith, Peter Wonka", "title": "Latent Filter Scaling for Multimodal Unsupervised Image-to-Image\n  Translation", "comments": "Accepted to CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multimodal unsupervised image-to-image translation tasks, the goal is to\ntranslate an image from the source domain to many images in the target domain.\nWe present a simple method that produces higher quality images than current\nstate-of-the-art while maintaining the same amount of multimodal diversity.\nPrevious methods follow the unconditional approach of trying to map the latent\ncode directly to a full-size image. This leads to complicated network\narchitectures with several introduced hyperparameters to tune. By treating the\nlatent code as a modifier of the convolutional filters, we produce multimodal\noutput while maintaining the traditional Generative Adversarial Network (GAN)\nloss and without additional hyperparameters. The only tuning required by our\nmethod controls the tradeoff between variability and quality of generated\nimages. Furthermore, we achieve disentanglement between source domain content\nand target domain style for free as a by-product of our formulation. We perform\nqualitative and quantitative experiments showing the advantages of our method\ncompared with the state-of-the art on multiple benchmark image-to-image\ntranslation datasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 10:07:50 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 11:54:38 GMT"}, {"version": "v3", "created": "Sun, 7 Apr 2019 07:47:18 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Alharbi", "Yazeed", ""], ["Smith", "Neil", ""], ["Wonka", "Peter", ""]]}, {"id": "1812.09878", "submitter": "Kavya Gupta", "authors": "Kavya Gupta, Brojeshwar Bhowmick and Angshul Majumdar", "title": "Coupled Analysis Dictionary Learning to inductively learn inversion:\n  Application to real-time reconstruction of Biomedical signals", "comments": "2018 International Joint Conference on Neural Networks (IJCNN)(July\n  2018)", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489148", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work addresses the problem of reconstructing biomedical signals from\ntheir lower dimensional projections. Traditionally Compressed Sensing (CS)\nbased techniques have been employed for this task. These are transductive\ninversion processes, the problem with these approaches is that the inversion is\ntime-consuming and hence not suitable for real-time applications. With the\nrecent advent of deep learning, Stacked Sparse Denoising Autoencoder (SSDAE)\nhas been used for learning inversion in an inductive setup. The training period\nfor inductive learning is large but is very fast during application -- capable\nof real-time speed. This work proposes a new approach for inductive learning of\nthe inversion process. It is based on Coupled Analysis Dictionary Learning.\nResults on Biomedical signal reconstruction show that our proposed approach is\nvery fast and yields result far better than CS and SSDAE.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 10:17:20 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Gupta", "Kavya", ""], ["Bhowmick", "Brojeshwar", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1812.09888", "submitter": "Kavya Gupta", "authors": "Kavya Gupta, Brojeshwar Bhowmick and Angshul Majumdar", "title": "Motion Blur removal via Coupled Autoencoder", "comments": null, "journal-ref": "2017 IEEE International Conference on Image Processing\n  (ICIP)(pages - 480-484, September 2017)", "doi": "10.1109/ICIP.2017.8296327", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper a joint optimization technique has been proposed for coupled\nautoencoder which learns the autoencoder weights and coupling map (between\nsource and target) simultaneously. The technique is applicable to any transfer\nlearning problem. In this work, we propose a new formulation that recasts\ndeblurring as a transfer learning problem, it is solved using the proposed\ncoupled autoencoder. The proposed technique can operate on-the-fly, since it\ndoes not require solving any costly inverse problem. Experiments have been\ncarried out on state-of-the-art techniques, our method yields better quality\nimages in shorter operating times.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 10:58:30 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Gupta", "Kavya", ""], ["Bhowmick", "Brojeshwar", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1812.09899", "submitter": "Kyaw Zaw Lin", "authors": "Kyaw Zaw Lin, Weipeng Xu, Qianru Sun, Christian Theobalt, Tat-Seng\n  Chua", "title": "Learning a Disentangled Embedding for Monocular 3D Shape Retrieval and\n  Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to jointly perform 3D shape retrieval and pose\nestimation from monocular images.In order to make the method robust to\nreal-world image variations, e.g. complex textures and backgrounds, we learn an\nembedding space from 3D data that only includes the relevant information,\nnamely the shape and pose. Our approach explicitly disentangles a shape vector\nand a pose vector, which alleviates both pose bias for 3D shape retrieval and\ncategorical bias for pose estimation. We then train a CNN to map the images to\nthis embedding space, and then retrieve the closest 3D shape from the database\nand estimate the 6D pose of the object. Our method achieves 10.3 median error\nfor pose estimation and 0.592 top-1-accuracy for category agnostic 3D object\nretrieval on the Pascal3D+ dataset, outperforming the previous state-of-the-art\nmethods on both tasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 11:44:36 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 18:19:43 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Lin", "Kyaw Zaw", ""], ["Xu", "Weipeng", ""], ["Sun", "Qianru", ""], ["Theobalt", "Christian", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1812.09900", "submitter": "Yipeng Sun", "authors": "Yipeng Sun, Chengquan Zhang, Zuming Huang, Jiaming Liu, Junyu Han, and\n  Errui Ding", "title": "TextNet: Irregular Text Reading from Images with an End-to-End Trainable\n  Network", "comments": "Asian conference on computer vision, 2018, oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading text from images remains challenging due to multi-orientation,\nperspective distortion and especially the curved nature of irregular text. Most\nof existing approaches attempt to solve the problem in two or multiple stages,\nwhich is considered to be the bottleneck to optimize the overall performance.\nTo address this issue, we propose an end-to-end trainable network architecture,\nnamed TextNet, which is able to simultaneously localize and recognize irregular\ntext from images. Specifically, we develop a scale-aware attention mechanism to\nlearn multi-scale image features as a backbone network, sharing fully\nconvolutional features and computation for localization and recognition. In\ntext detection branch, we directly generate text proposals in quadrangles,\ncovering oriented, perspective and curved text regions. To preserve text\nfeatures for recognition, we introduce a perspective RoI transform layer, which\ncan align quadrangle proposals into small feature maps. Furthermore, in order\nto extract effective features for recognition, we propose to encode the aligned\nRoI features by RNN into context information, combining spatial attention\nmechanism to generate text sequences. This overall pipeline is capable of\nhandling both regular and irregular cases. Finally, text localization and\nrecognition tasks can be jointly trained in an end-to-end fashion with designed\nmulti-task loss. Experiments on standard benchmarks show that the proposed\nTextNet can achieve state-of-the-art performance, and outperform existing\napproaches on irregular datasets by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 11:44:55 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sun", "Yipeng", ""], ["Zhang", "Chengquan", ""], ["Huang", "Zuming", ""], ["Liu", "Jiaming", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""]]}, {"id": "1812.09903", "submitter": "Yuval Atzmon", "authors": "Yuval Atzmon, Gal Chechik", "title": "Adaptive Confidence Smoothing for Generalized Zero-Shot Learning", "comments": "(1) Accepted to CVPR 2019. (2) Previous title was \"Domain-Aware\n  Generalized Zero-Shot Learning\". (3) This arxiv version is as the CVPR final\n  version with the following modifications: (a) corrected typos found in Table\n  3 (b) updated \"Related Work\" with [52, 10, 20] (c) add a paragraph to the\n  abstract (d) add a probabilistic explanation for the smoothing term", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot learning (GZSL) is the problem of learning a classifier\nwhere some classes have samples and others are learned from side information,\nlike semantic attributes or text description, in a zero-shot learning fashion\n(ZSL). Training a single model that operates in these two regimes\nsimultaneously is challenging. Here we describe a probabilistic approach that\nbreaks the model into three modular components, and then combines them in a\nconsistent way. Specifically, our model consists of three classifiers: A\n\"gating\" model that makes soft decisions if a sample is from a \"seen\" class,\nand two experts: a ZSL expert, and an expert model for seen classes.\n  We address two main difficulties in this approach: How to provide an accurate\nestimate of the gating probability without any training samples for unseen\nclasses; and how to use expert predictions when it observes samples outside of\nits domain. The key insight to our approach is to pass information between the\nthree models to improve each one's accuracy, while maintaining the modular\nstructure. We test our approach, adaptive confidence smoothing (COSMO), on four\nstandard GZSL benchmark datasets and find that it largely outperforms\nstate-of-the-art GZSL models. COSMO is also the first model that closes the gap\nand surpasses the performance of generative models for GZSL, even-though it is\na light-weight model that is much easier to train and tune.\n  Notably, COSMO offers a new view for developing zero-shot models. Thanks to\nCOSMO's modular structure, instead of trying to perform well both on seen and\non unseen classes, models can focus on accurate classification of unseen\nclasses, and later consider seen class models.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 11:54:41 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 10:25:53 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 16:01:33 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Atzmon", "Yuval", ""], ["Chechik", "Gal", ""]]}, {"id": "1812.09912", "submitter": "Wonwoong Cho", "authors": "Wonwoong Cho, Sungha Choi, David Keetae Park, Inkyu Shin, Jaegul Choo", "title": "Image-to-Image Translation via Group-wise Deep Whitening-and-Coloring\n  Transformation", "comments": "CVPR 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, unsupervised exemplar-based image-to-image translation, conditioned\non a given exemplar without the paired data, has accomplished substantial\nadvancements. In order to transfer the information from an exemplar to an input\nimage, existing methods often use a normalization technique, e.g., adaptive\ninstance normalization, that controls the channel-wise statistics of an input\nactivation map at a particular layer, such as the mean and the variance.\nMeanwhile, style transfer approaches similar task to image translation by\nnature, demonstrated superior performance by using the higher-order statistics\nsuch as covariance among channels in representing a style. In detail, it works\nvia whitening (given a zero-mean input feature, transforming its covariance\nmatrix into the identity). followed by coloring (changing the covariance matrix\nof the whitened feature to those of the style feature). However, applying this\napproach in image translation is computationally intensive and error-prone due\nto the expensive time complexity and its non-trivial backpropagation. In\nresponse, this paper proposes an end-to-end approach tailored for image\ntranslation that efficiently approximates this transformation with our novel\nregularization methods. We further extend our approach to a group-wise form for\nmemory and time efficiency as well as image quality. Extensive qualitative and\nquantitative experiments demonstrate that our proposed method is fast, both in\ntraining and inference, and highly effective in reflecting the style of an\nexemplar. Finally, our code is available at\nhttps://github.com/WonwoongCho/GDWCT.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 13:03:24 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 08:58:05 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Cho", "Wonwoong", ""], ["Choi", "Sungha", ""], ["Park", "David Keetae", ""], ["Shin", "Inkyu", ""], ["Choo", "Jaegul", ""]]}, {"id": "1812.09916", "submitter": "Wei Wang", "authors": "Wei Wang, Yuan Sun, Saman Halgamuge", "title": "Improving MMD-GAN Training with Repulsive Loss Function", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial nets (GANs) are widely used to learn the data sampling\nprocess and their performance may heavily depend on the loss functions, given a\nlimited computational budget. This study revisits MMD-GAN that uses the maximum\nmean discrepancy (MMD) as the loss function for GAN and makes two\ncontributions. First, we argue that the existing MMD loss function may\ndiscourage the learning of fine details in data as it attempts to contract the\ndiscriminator outputs of real data. To address this issue, we propose a\nrepulsive loss function to actively learn the difference among the real data by\nsimply rearranging the terms in MMD. Second, inspired by the hinge loss, we\npropose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the\nrepulsive loss function. The proposed methods are applied to the unsupervised\nimage generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets.\nResults show that the repulsive loss function significantly improves over the\nMMD loss at no additional computational cost and outperforms other\nrepresentative loss functions. The proposed methods achieve an FID score of\n16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral\nnormalization.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 13:23:18 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 03:06:33 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 11:04:01 GMT"}, {"version": "v4", "created": "Fri, 8 Feb 2019 06:28:35 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Wang", "Wei", ""], ["Sun", "Yuan", ""], ["Halgamuge", "Saman", ""]]}, {"id": "1812.09922", "submitter": "Tailin Liang", "authors": "Tailin Liang and Lei Wang and Shaobo Shi and John Glossner", "title": "Dynamic Runtime Feature Map Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High bandwidth requirements are an obstacle for accelerating the training and\ninference of deep neural networks. Most previous research focuses on reducing\nthe size of kernel maps for inference. We analyze parameter sparsity of six\npopular convolutional neural networks - AlexNet, MobileNet, ResNet-50,\nSqueezeNet, TinyNet, and VGG16. Of the networks considered, those using ReLU\n(AlexNet, SqueezeNet, VGG16) contain a high percentage of 0-valued parameters\nand can be statically pruned. Networks with Non-ReLU activation functions in\nsome cases may not contain any 0-valued parameters (ResNet-50, TinyNet). We\nalso investigate runtime feature map usage and find that input feature maps\ncomprise the majority of bandwidth requirements when depth-wise convolution and\npoint-wise convolutions used. We introduce dynamic runtime pruning of feature\nmaps and show that 10% of dynamic feature map execution can be removed without\nloss of accuracy. We then extend dynamic pruning to allow for values within an\nepsilon of zero and show a further 5% reduction of feature map loading with a\n1% loss of accuracy in top-1.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 13:54:50 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 02:22:22 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Liang", "Tailin", ""], ["Wang", "Lei", ""], ["Shi", "Shaobo", ""], ["Glossner", "John", ""]]}, {"id": "1812.09924", "submitter": "Zhihui Zhu", "authors": "Zhihui Zhu, Yifan Wang, Daniel P. Robinson, Daniel Q. Naiman, Rene\n  Vidal, Manolis C. Tsakiris", "title": "Dual Principal Component Pursuit: Probability Analysis and Efficient\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods for learning a linear subspace from data corrupted by outliers\nare based on convex $\\ell_1$ and nuclear norm optimization and require the\ndimension of the subspace and the number of outliers to be sufficiently small.\nIn sharp contrast, the recently proposed Dual Principal Component Pursuit\n(DPCP) method can provably handle subspaces of high dimension by solving a\nnon-convex $\\ell_1$ optimization problem on the sphere. However, its geometric\nanalysis is based on quantities that are difficult to interpret and are not\namenable to statistical analysis. In this paper we provide a refined geometric\nanalysis and a new statistical analysis that show that DPCP can tolerate as\nmany outliers as the square of the number of inliers, thus improving upon other\nprovably correct robust PCA methods. We also propose a scalable Projected\nSub-Gradient Method method (DPCP-PSGM) for solving the DPCP problem and show it\nadmits linear convergence even though the underlying optimization problem is\nnon-convex and non-smooth. Experiments on road plane detection from 3D point\ncloud data demonstrate that DPCP-PSGM can be more efficient than the\ntraditional RANSAC algorithm, which is one of the most popular methods for such\ncomputer vision applications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 14:07:18 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zhu", "Zhihui", ""], ["Wang", "Yifan", ""], ["Robinson", "Daniel P.", ""], ["Naiman", "Daniel Q.", ""], ["Vidal", "Rene", ""], ["Tsakiris", "Manolis C.", ""]]}, {"id": "1812.09930", "submitter": "Qi Mu", "authors": "Qi Mu, Yanyan Wei, Zhanli Li", "title": "Color Image Enhancement Method Based on Weighted Image Guided Filtering", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel color image enhancement method is proposed based on Retinex to\nenhance color images under non-uniform illumination or poor visibility\nconditions. Different from the conventional Retinex algorithms, the Weighted\nGuided Image Filter is used as a surround function instead of the Gaussian\nfilter to estimate the background illumination, which can overcome the\ndrawbacks of local blur and halo artifact that may appear by Gaussian filter.\nTo avoid color distortion, the image is converted to the HSI color model, and\nonly the intensity channel is enhanced. Then a linear color restoration\nalgorithm is adopted to convert the enhanced intensity image back to the RGB\ncolor model, which ensures the hue is constant and undistorted. Experimental\nresults show that the proposed method is effective to enhance both color and\ngray images with low exposure and non-uniform illumination, resulting in better\nvisual quality than traditional method. At the same time, the objective\nevaluation indicators are also superior to the conventional methods. In\naddition, the efficiency of the proposed method is also improved thanks to the\nlinear color restoration algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 14:46:08 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Mu", "Qi", ""], ["Wei", "Yanyan", ""], ["Li", "Zhanli", ""]]}, {"id": "1812.09953", "submitter": "Yang Zhang", "authors": "Yang Zhang, Philip David, Hassan Foroosh, Boqing Gong", "title": "A Curriculum Domain Adaptation Approach to the Semantic Segmentation of\n  Urban Scenes", "comments": "This is the journal version of arXiv:1707.09465", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last half decade, convolutional neural networks (CNNs) have\ntriumphed over semantic segmentation, which is one of the core tasks in many\napplications such as autonomous driving and augmented reality. However, to\ntrain CNNs requires a considerable amount of data, which is difficult to\ncollect and laborious to annotate. Recent advances in computer graphics make it\npossible to train CNNs on photo-realistic synthetic imagery with\ncomputer-generated annotations. Despite this, the domain mismatch between the\nreal images and the synthetic data hinders the models' performance. Hence, we\npropose a curriculum-style learning approach to minimizing the domain gap in\nurban scene semantic segmentation. The curriculum domain adaptation solves easy\ntasks first to infer necessary properties about the target domain; in\nparticular, the first task is to learn global label distributions over images\nand local distributions over landmark superpixels. These are easy to estimate\nbecause images of urban scenes have strong idiosyncrasies (e.g., the size and\nspatial relations of buildings, streets, cars, etc.). We then train a\nsegmentation network, while regularizing its predictions in the target domain\nto follow those inferred properties. In experiments, our method outperforms the\nbaselines on two datasets and two backbone networks. We also report extensive\nablation studies about our approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 16:50:49 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 21:54:02 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 01:11:19 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Zhang", "Yang", ""], ["David", "Philip", ""], ["Foroosh", "Hassan", ""], ["Gong", "Boqing", ""]]}, {"id": "1812.10000", "submitter": "Huijuan Xu", "authors": "Huijuan Xu, Bingyi Kang, Ximeng Sun, Jiashi Feng, Kate Saenko, Trevor\n  Darrell", "title": "Similarity R-C3D for Few-shot Temporal Activity Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many activities of interest are rare events, with only a few labeled examples\navailable. Therefore models for temporal activity detection which are able to\nlearn from a few examples are desirable. In this paper, we present a\nconceptually simple and general yet novel framework for few-shot temporal\nactivity detection which detects the start and end time of the few-shot input\nactivities in an untrimmed video. Our model is end-to-end trainable and can\nbenefit from more few-shot examples. At test time, each proposal is assigned\nthe label of the few-shot activity class corresponding to the maximum\nsimilarity score. Our Similarity R-C3D method outperforms previous work on\nthree large-scale benchmarks for temporal activity detection (THUMOS14,\nActivityNet1.2, and ActivityNet1.3 datasets) in the few-shot setting. Our code\nwill be made available.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 00:35:31 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Xu", "Huijuan", ""], ["Kang", "Bingyi", ""], ["Sun", "Ximeng", ""], ["Feng", "Jiashi", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1812.10016", "submitter": "Kai Wang", "authors": "Kai Wang and Yimin Lin and Luowei Wang and Liming Han and Minjie Hua\n  and Xiang Wang and Shiguo Lian and Bill Huang", "title": "A Unified Framework for Mutual Improvement of SLAM and Semantic\n  Segmentation", "comments": "7 pages, 5 figures.This work has been accepted by ICRA 2019. The demo\n  video can be found at https://youtu.be/Bkt53dAehjY", "journal-ref": null, "doi": "10.1109/ICRA.2019.8793499", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for simultaneously implementing\nlocalization and segmentation, which are two of the most important vision-based\ntasks for robotics. While the goals and techniques used for them were\nconsidered to be different previously, we show that by making use of the\nintermediate results of the two modules, their performance can be enhanced at\nthe same time. Our framework is able to handle both the instantaneous motion\nand long-term changes of instances in localization with the help of the\nsegmentation result, which also benefits from the refined 3D pose information.\nWe conduct experiments on various datasets, and prove that our framework works\neffectively on improving the precision and robustness of the two tasks and\noutperforms existing localization and segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 02:46:22 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 10:06:50 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Wang", "Kai", ""], ["Lin", "Yimin", ""], ["Wang", "Luowei", ""], ["Han", "Liming", ""], ["Hua", "Minjie", ""], ["Wang", "Xiang", ""], ["Lian", "Shiguo", ""], ["Huang", "Bill", ""]]}, {"id": "1812.10025", "submitter": "Hiroshi Fukui", "authors": "Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu\n  Fujiyoshi", "title": "Attention Branch Network: Learning of Attention Mechanism for Visual\n  Explanation", "comments": "Accepted to CVPR2019 10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual explanation enables human to understand the decision making of Deep\nConvolutional Neural Network (CNN), but it is insufficient to contribute the\nperformance improvement. In this paper, we focus on the attention map for\nvisual explanation, which represents high response value as the important\nregion in image recognition. This region significantly improves the performance\nof CNN by introducing an attention mechanism that focuses on a specific region\nin an image. In this work, we propose Attention Branch Network (ABN), which\nextends the top-down visual explanation model by introducing a branch structure\nwith an attention mechanism. ABN can be applicable to several image recognition\ntasks by introducing a branch for attention mechanism and is trainable for the\nvisual explanation and image recognition in end-to-end manner. We evaluate ABN\non several image recognition tasks such as image classification, fine-grained\nrecognition, and multiple facial attributes recognition. Experimental results\nshow that ABN can outperform the accuracy of baseline models on these image\nrecognition tasks while generating an attention map for visual explanation. Our\ncode is available at\nhttps://github.com/machine-perception-robotics-group/attention_branch_network.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 04:25:54 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 11:18:02 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Fukui", "Hiroshi", ""], ["Hirakawa", "Tsubasa", ""], ["Yamashita", "Takayoshi", ""], ["Fujiyoshi", "Hironobu", ""]]}, {"id": "1812.10033", "submitter": "Huai Chen", "authors": "Huai Chen, Yuxiao Qi, Yong Yin, Tengxiang Li, Xiaoqing Liu, Xiuli Li,\n  Guanzhong Gong, Lisheng Wang", "title": "MMFNet: A Multi-modality MRI Fusion Network for Segmentation of\n  Nasopharyngeal Carcinoma", "comments": "34 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of nasopharyngeal carcinoma (NPC) from Magnetic Resonance Images\n(MRI) is a crucial prerequisite for NPC radiotherapy. However, manually\nsegmenting of NPC is time-consuming and labor-intensive. Additionally,\nsingle-modality MRI generally cannot provide enough information for its\naccurate delineation. Therefore, a multi-modality MRI fusion network (MMFNet)\nbased on three modalities of MRI (T1, T2 and contrast-enhanced T1) is proposed\nto complete accurate segmentation of NPC. The backbone of MMFNet is designed as\na multi-encoder-based network, consisting of several encoders to capture\nmodality-specific features and one single decoder to fuse them and obtain\nhigh-level features for NPC segmentation. A fusion block is presented to\neffectively fuse features from multi-modality MRI. It firstly recalibrates\nlow-level features captured from modality-specific encoders to highlight both\ninformative features and regions of interest, then fuses weighted features by a\nresidual fusion block to keep balance between fused ones and high-level\nfeatures from decoder. Moreover, a training strategy named self-transfer, which\nutilizes pre-trained modality-specific encoders to initialize\nmulti-encoder-based network, is proposed to make full mining of information\nfrom different modalities of MRI. The proposed method based on multi-modality\nMRI can effectively segment NPC and its advantages are validated by extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 05:19:00 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 10:23:54 GMT"}, {"version": "v3", "created": "Sun, 5 May 2019 12:39:40 GMT"}, {"version": "v4", "created": "Thu, 9 May 2019 03:06:22 GMT"}, {"version": "v5", "created": "Mon, 27 May 2019 05:48:07 GMT"}, {"version": "v6", "created": "Tue, 22 Oct 2019 08:53:21 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Chen", "Huai", ""], ["Qi", "Yuxiao", ""], ["Yin", "Yong", ""], ["Li", "Tengxiang", ""], ["Liu", "Xiaoqing", ""], ["Li", "Xiuli", ""], ["Gong", "Guanzhong", ""], ["Wang", "Lisheng", ""]]}, {"id": "1812.10066", "submitter": "Jia Li", "authors": "Jinming Su, Jia Li, Yu Zhang, Changqun Xia, Yonghong Tian", "title": "Selectivity or Invariance: Boundary-aware Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, a salient object detection (SOD) model faces opposite requirements\nin processing object interiors and boundaries. The features of interiors should\nbe invariant to strong appearance change so as to pop-out the salient object as\na whole, while the features of boundaries should be selective to slight\nappearance change to distinguish salient objects and background. To address\nthis selectivity-invariance dilemma, we propose a novel boundary-aware network\nwith successive dilation for image-based SOD. In this network, the feature\nselectivity at boundaries is enhanced by incorporating a boundary localization\nstream, while the feature invariance at interiors is guaranteed with a complex\ninterior perception stream. Moreover, a transition compensation stream is\nadopted to amend the probable failures in transitional regions between\ninteriors and boundaries. In particular, an integrated successive dilation\nmodule is proposed to enhance the feature invariance at interiors and\ntransitional regions. Extensive experiments on six datasets show that the\nproposed approach outperforms 16 state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 08:31:47 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 01:02:22 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 08:08:17 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Su", "Jinming", ""], ["Li", "Jia", ""], ["Zhang", "Yu", ""], ["Xia", "Changqun", ""], ["Tian", "Yonghong", ""]]}, {"id": "1812.10067", "submitter": "Tianyu He", "authors": "Zhibo Chen, Tianyu He", "title": "Learning based Facial Image Compression with Semantic Fidelity Metric", "comments": "Accepted by Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2019.01.086", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance and security scenarios usually require high efficient facial\nimage compression scheme for face recognition and identification. While either\ntraditional general image codecs or special facial image compression schemes\nonly heuristically refine codec separately according to face verification\naccuracy metric. We propose a Learning based Facial Image Compression (LFIC)\nframework with a novel Regionally Adaptive Pooling (RAP) module whose\nparameters can be automatically optimized according to gradient feedback from\nan integrated hybrid semantic fidelity metric, including a successfully\nexploration to apply Generative Adversarial Network (GAN) as metric directly in\nimage compression scheme. The experimental results verify the framework's\nefficiency by demonstrating performance improvement of 71.41%, 48.28% and\n52.67% bitrate saving separately over JPEG2000, WebP and neural network-based\ncodecs under the same face verification accuracy distortion metric. We also\nevaluate LFIC's superior performance gain compared with latest specific facial\nimage codecs. Visual experiments also show some interesting insight on how LFIC\ncan automatically capture the information in critical areas based on semantic\ndistortion metrics for optimized compression, which is quite different from the\nheuristic way of optimization in traditional image compression algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 08:42:55 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 11:54:39 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Chen", "Zhibo", ""], ["He", "Tianyu", ""]]}, {"id": "1812.10071", "submitter": "Lin Sun", "authors": "Lin Sun, Kui Jia, Yuejia Shen, Silvio Savarese, Dit Yan Yeung, and\n  Bertram E. Shi", "title": "Coupled Recurrent Network (CRN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many semantic video analysis tasks can benefit from multiple, heterogenous\nsignals. For example, in addition to the original RGB input sequences,\nsequences of optical flow are usually used to boost the performance of human\naction recognition in videos. To learn from these heterogenous input sources,\nexisting methods reply on two-stream architectural designs that contain\nindependent, parallel streams of Recurrent Neural Networks (RNNs). However,\ntwo-stream RNNs do not fully exploit the reciprocal information contained in\nthe multiple signals, let alone exploit it in a recurrent manner. To this end,\nwe propose in this paper a novel recurrent architecture, termed Coupled\nRecurrent Network (CRN), to deal with multiple input sources. In CRN, the\nparallel streams of RNNs are coupled together. Key design of CRN is a Recurrent\nInterpretation Block (RIB) that supports learning of reciprocal feature\nrepresentations from multiple signals in a recurrent manner. Different from\nRNNs which stack the training loss at each time step or the last time step, we\npropose an effective and efficient training strategy for CRN. Experiments show\nthe efficacy of the proposed CRN. In particular, we achieve the new state of\nthe art on the benchmark datasets of human action recognition and multi-person\npose estimation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 09:04:24 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 19:11:57 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Sun", "Lin", ""], ["Jia", "Kui", ""], ["Shen", "Yuejia", ""], ["Savarese", "Silvio", ""], ["Yeung", "Dit Yan", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1812.10085", "submitter": "Haifeng Li", "authors": "Li Chen, Hailun Ding, Qi Li, Jiawei Zhu, Jian Peng, Haifeng Li", "title": "A Data-driven Adversarial Examples Recognition Framework via Adversarial\n  Feature Genome", "comments": "10 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are easily spoofed by adversarial\nexamples which lead to wrong classification results. Most of the defense\nmethods focus only on how to improve the robustness of CNNs or to detect\nadversarial examples. They are incapable of detecting and correctly classifying\nadversarial examples simultaneously. We find that adversarial examples and\noriginal images have diverse representations in the feature space, and this\ndifference grows as layers go deeper, which we call Adversarial Feature\nSeparability (AFS). Inspired by AFS, we propose a defense framework based on\nAdversarial Feature Genome (AFG), which can detect and correctly classify\nadversarial examples into original classes simultaneously. AFG is an innovative\nencoding for both image and adversarial example. It consists of group features\nand a mixed label. With group features which are visual representations of\nadversarial and original images via group visualization method, one can detect\nadversarial examples because of ASF of group features. With a mixed label, one\ncan trace back to the original label of an adversarial example. Then, the\nclassification of adversarial example is modeled as a multi-label\nclassification trained on the AFG dataset, which can get the original class of\nadversarial example. Experiments show that the proposed framework not only\neffectively detects adversarial examples from different attack algorithms, but\nalso correctly classifies adversarial examples. Our framework potentially gives\na new perspective, i.e., a data-driven way, to improve the robustness of a CNN\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 10:31:36 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 10:54:52 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Chen", "Li", ""], ["Ding", "Hailun", ""], ["Li", "Qi", ""], ["Zhu", "Jiawei", ""], ["Peng", "Jian", ""], ["Li", "Haifeng", ""]]}, {"id": "1812.10087", "submitter": "Claus Aranha", "authors": "Yusei Miura, Tetsuya Sakurai, Claus Aranha, Toshiya Senda, Ryuichi\n  Kato, Yusuke Yamada", "title": "Classification of X-Ray Protein Crystallization Using Deep Convolutional\n  Neural Networks with a Finder Module", "comments": "7 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural networks have shown good results for\nimage recognition. In this paper, we use convolutional neural networks with a\nfinder module, which discovers the important region for recognition and\nextracts that region. We propose applying our method to the recognition of\nprotein crystals for X-ray structural analysis. In this analysis, it is\nnecessary to recognize states of protein crystallization from a large number of\nimages. There are several methods that realize protein crystallization\nrecognition by using convolutional neural networks. In each method, large-scale\ndata sets are required to recognize with high accuracy. In our data set, the\nnumber of images is not good enough for training CNN. The amount of data for\nCNN is a serious issue in various fields. Our method realizes high accuracy\nrecognition with few images by discovering the region where the crystallization\ndrop exists. We compared our crystallization image recognition method with a\nhigh precision method using Inception-V3. We demonstrate that our method is\neffective for crystallization images using several experiments. Our method\ngained the AUC value that is about 5% higher than the compared method.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 10:48:26 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Miura", "Yusei", ""], ["Sakurai", "Tetsuya", ""], ["Aranha", "Claus", ""], ["Senda", "Toshiya", ""], ["Kato", "Ryuichi", ""], ["Yamada", "Yusuke", ""]]}, {"id": "1812.10098", "submitter": "Sergey Belim", "authors": "S.V. Belim, S.B. Larionov", "title": "The algorithm of the impulse noise filtration in images based on an\n  algorithm of community detection in graphs", "comments": null, "journal-ref": "2017 Dynamics of Systems, Mechanisms and Machines (Dynamics),\n  Omsk, Russia, pp. 1-5", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article suggests an algorithm of impulse noise filtration, based on the\ncommunity detection in graphs. The image is representing as non-oriented\nweighted graph. Each pixel of an image is corresponding to a vertex of the\ngraph. Community detection algorithm is running on the given graph. Assumed\nthat communities that contain only one pixel are corresponding to noised pixels\nof an image. Suggested method was tested with help of computer experiment. This\nexperiment was conducted on grayscale, and on colored images, on artificial\nimages and on photos. It is shown that the suggested method is better than\nmedian filter by 20% regardless of noise percent. Higher efficiency is\njustified by the fact that most of filters are changing all of image pixels,\nbut suggested method is finding and restoring only noised pixels. The\ndependence of the effectiveness of the proposed method on the percentage of\nnoise in the image is shown.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 12:40:12 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Belim", "S. V.", ""], ["Larionov", "S. B.", ""]]}, {"id": "1812.10111", "submitter": "Hamid Laga", "authors": "Hamid Laga", "title": "A Survey on Non-rigid 3D Shape Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape is an important physical property of natural and manmade 3D objects\nthat characterizes their external appearances. Understanding differences\nbetween shapes and modeling the variability within and across shape classes,\nhereinafter referred to as \\emph{shape analysis}, are fundamental problems to\nmany applications, ranging from computer vision and computer graphics to\nbiology and medicine. This chapter provides an overview of some of the recent\ntechniques that studied the shape of 3D objects that undergo non-rigid\ndeformations including bending and stretching. Recent surveys that covered some\naspects such classification, retrieval, recognition, and rigid or nonrigid\nregistration, focused on methods that use shape descriptors. Descriptors,\nhowever, provide abstract representations that do not enable the exploration of\nshape variability. In this chapter, we focus on recent techniques that treated\nthe shape of 3D objects as points in some high dimensional space where paths\ndescribe deformations. Equipping the space with a suitable metric enables the\nquantification of the range of deformations of a given shape, which in turn\nenables (1) comparing and classifying 3D objects based on their shape, (2)\ncomputing smooth deformations, i.e. geodesics, between pairs of objects, and\n(3) modeling and exploring continuous shape variability in a collection of 3D\nmodels. This article surveys and classifies recent developments in this field,\noutlines fundamental issues, discusses their potential applications in computer\nvision and graphics, and highlights opportunities for future research. Our\nprimary goal is to bridge the gap between various techniques that have been\noften independently proposed by different communities including mathematics and\nstatistics, computer vision and graphics, and medical image analysis.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 14:33:42 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Laga", "Hamid", ""]]}, {"id": "1812.10157", "submitter": "Veronique Prinet", "authors": "Veronique Prinet", "title": "Motion Selective Prediction for Video Frame Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing conditional video prediction approaches train a network from large\ndatabases and generalize to previously unseen data. We take the opposite\nstance, and introduce a model that learns from the first frames of a given\nvideo and extends its content and motion, to, eg, double its length. To this\nend, we propose a dual network that can use in a flexible way both dynamic and\nstatic convolutional motion kernels, to predict future frames. The construct of\nour model gives us the the means to efficiently analyze its functioning and\ninterpret its output. We demonstrate experimentally the robustness of our\napproach on challenging videos in-the-wild and show that it is competitive wrt\nrelated baselines.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 19:15:29 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Prinet", "Veronique", ""]]}, {"id": "1812.10179", "submitter": "Bappaditya Mandal Dr.", "authors": "Bappaditya Mandal, N. B. Puhan and Avijit Verma", "title": "Deep Convolutional Generative Adversarial Network Based Food Recognition\n  Using Partially Labeled Data", "comments": "5 pages, 2 figures, 2 tables, submitted to IEEE Sensors Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional machine learning algorithms using hand-crafted feature extraction\ntechniques (such as local binary pattern) have limited accuracy because of high\nvariation in images of the same class (or intra-class variation) for food\nrecognition task. In recent works, convolutional neural networks (CNN) have\nbeen applied to this task with better results than all previously reported\nmethods. However, they perform best when trained with large amount of annotated\n(labeled) food images. This is problematic when obtained in large volume,\nbecause they are expensive, laborious and impractical. Our work aims at\ndeveloping an efficient deep CNN learning-based method for food recognition\nalleviating these limitations by using partially labeled training data on\ngenerative adversarial networks (GANs). We make new enhancements to the\nunsupervised training architecture introduced by Goodfellow et al. (2014),\nwhich was originally aimed at generating new data by sampling a dataset. In\nthis work, we make modifications to deep convolutional GANs to make them robust\nand efficient for classifying food images. Experimental results on benchmarking\ndatasets show the superiority of our proposed method as compared to the\ncurrent-state-of-the-art methodologies even when trained with partially labeled\ntraining data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 00:10:26 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Mandal", "Bappaditya", ""], ["Puhan", "N. B.", ""], ["Verma", "Avijit", ""]]}, {"id": "1812.10191", "submitter": "Sukesh Adiga V", "authors": "Sukesh Adiga V, Jayanthi Sivaswamy", "title": "FPD-M-net: Fingerprint Image Denoising and Inpainting Using M-Net Based\n  Convolutional Neural Networks", "comments": "11 pages, Accepted in CiML; 3rd Rank in ECCV 2018 Satellite Event by\n  Chalearn LAP In-painting Competition Track-3;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint is a common biometric used for authentication and verification of\nan individual. These images are degraded when fingers are wet, dirty, dry or\nwounded and due to the failure of the sensors, etc. The extraction of the\nfingerprint from a degraded image requires denoising and inpainting. We propose\nto address these problems with an end-to-end trainable Convolutional Neural\nNetwork based architecture called FPD-M-net, by posing the fingerprint\ndenoising and inpainting problem as a segmentation (foreground) task. Our\narchitecture is based on the M-net with a change: structure similarity loss\nfunction, used for better extraction of the fingerprint from the noisy\nbackground. Our method outperforms the baseline method and achieves an overall\n3rd rank in the Chalearn LAP Inpainting Competition Track 3 - Fingerprint\nDenoising and Inpainting, ECCV 2018\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 00:42:47 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 11:53:36 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Adiga", "Sukesh", "V"], ["Sivaswamy", "Jayanthi", ""]]}, {"id": "1812.10212", "submitter": "Lei Han", "authors": "Lei Han, Mengqi Ji, Lu Fang, Matthias Nie{\\ss}ner", "title": "RegNet: Learning the Optimization of Direct Image-to-Image Pose\n  Registration", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct image-to-image alignment that relies on the optimization of\nphotometric error metrics suffers from limited convergence range and\nsensitivity to lighting conditions. Deep learning approaches has been applied\nto address this problem by learning better feature representations using\nconvolutional neural networks, yet still require a good initialization. In this\npaper, we demonstrate that the inaccurate numerical Jacobian limits the\nconvergence range which could be improved greatly using learned approaches.\nBased on this observation, we propose a novel end-to-end network, RegNet, to\nlearn the optimization of image-to-image pose registration. By jointly learning\nfeature representation for each pixel and partial derivatives that replace\nhandcrafted ones (e.g., numerical differentiation) in the optimization step,\nthe neural network facilitates end-to-end optimization. The energy landscape is\nconstrained on both the feature representation and the learned Jacobian, hence\nproviding more flexibility for the optimization as a consequence leads to more\nrobust and faster convergence. In a series of experiments, including a broad\nablation study, we demonstrate that RegNet is able to converge for\nlarge-baseline image pairs with fewer iterations.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 03:47:31 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Han", "Lei", ""], ["Ji", "Mengqi", ""], ["Fang", "Lu", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1812.10213", "submitter": "Kai Cao", "authors": "Kai Cao, Dinh-Luan Nguyen, Cori Tymoszek and A.K. Jain", "title": "End-to-End Latent Fingerprint Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent fingerprints are one of the most important and widely used sources of\nevidence in law enforcement and forensic agencies. Yet the performance of the\nstate-of-the-art latent recognition systems is far from satisfactory, and they\noften require manual markups to boost the latent search performance. Further,\nthe COTS systems are proprietary and do not output the true comparison scores\nbetween a latent and reference prints to conduct quantitative evidential\nanalysis. We present an end-to-end latent fingerprint search system, including\nautomated region of interest (ROI) cropping, latent image preprocessing,\nfeature extraction, feature comparison , and outputs a candidate list. Two\nseparate minutiae extraction models provide complementary minutiae templates.\nTo compensate for the small number of minutiae in small area and poor quality\nlatents, a virtual minutiae set is generated to construct a texture template. A\n96-dimensional descriptor is extracted for each minutia from its neighborhood.\nFor computational efficiency, the descriptor length for virtual minutiae is\nfurther reduced to 16 using product quantization. Our end-to-end system is\nevaluated on three latent databases: NIST SD27 (258 latents); MSP (1,200\nlatents), WVU (449 latents) and N2N (10,000 latents) against a background set\nof 100K rolled prints, which includes the true rolled mates of the latents with\nrank-1 retrieval rates of 65.7%, 69.4%, 65.5%, and 7.6% respectively. A\nmulti-core solution implemented on 24 cores obtains 1ms per latent to rolled\ncomparison.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 03:48:12 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Cao", "Kai", ""], ["Nguyen", "Dinh-Luan", ""], ["Tymoszek", "Cori", ""], ["Jain", "A. K.", ""]]}, {"id": "1812.10217", "submitter": "Yue Zhao", "authors": "Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi Zhang, Kai\n  Chen", "title": "Seeing isn't Believing: Practical Adversarial Attack Against Object\n  Detectors", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we presented systematic solutions to build robust and\npractical AEs against real world object detectors. Particularly, for Hiding\nAttack (HA), we proposed the feature-interference reinforcement (FIR) method\nand the enhanced realistic constraints generation (ERG) to enhance robustness,\nand for Appearing Attack (AA), we proposed the nested-AE, which combines two\nAEs together to attack object detectors in both long and short distance. We\nalso designed diverse styles of AEs to make AA more surreptitious. Evaluation\nresults show that our AEs can attack the state-of-the-art real-time object\ndetectors (i.e., YOLO V3 and faster-RCNN) at the success rate up to 92.4% with\nvarying distance from 1m to 25m and angles from -60{\\deg} to 60{\\deg}. Our AEs\nare also demonstrated to be highly transferable, capable of attacking another\nthree state-of-the-art black-box models with high success rate.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 04:14:08 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 01:26:10 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 08:54:39 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Zhao", "Yue", ""], ["Zhu", "Hong", ""], ["Liang", "Ruigang", ""], ["Shen", "Qintao", ""], ["Zhang", "Shengzhi", ""], ["Chen", "Kai", ""]]}, {"id": "1812.10222", "submitter": "Yang Wang", "authors": "Lin Wu, Yang Wang, Ling Shao, Meng Wang", "title": "3D PersonVLAD: Learning Deep Global Representations for Video-based\n  Person Re-identification", "comments": "Accepted to appear at IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2891244", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a global video representation to video-based\nperson re-identification (re-ID) that aggregates local 3D features across the\nentire video extent. Most of the existing methods rely on 2D convolutional\nnetworks (ConvNets) to extract frame-wise deep features which are pooled\ntemporally to generate the video-level representations. However, 2D ConvNets\nlose temporal input information immediately after the convolution, and a\nseparate temporal pooling is limited in capturing human motion in shorter\nsequences. To this end, we present a \\textit{global} video representation (3D\nPersonVLAD), complementary to 3D ConvNets as a novel layer to capture the\nappearance and motion dynamics in full-length videos. However, encoding each\nvideo frame in its entirety and computing an aggregate global representation\nacross all frames is tremendously challenging due to occlusions and\nmisalignments. To resolve this, our proposed network is further augmented with\n3D part alignment module to learn local features through soft-attention module.\nThese attended features are statistically aggregated to yield\nidentity-discriminative representations. Our global 3D features are\ndemonstrated to achieve state-of-the-art results on three benchmark datasets:\nMARS \\cite{MARS}, iLIDS-VID \\cite{VideoRanking}, and PRID 2011\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 04:51:55 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 06:43:52 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 02:51:47 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Wu", "Lin", ""], ["Wang", "Yang", ""], ["Shao", "Ling", ""], ["Wang", "Meng", ""]]}, {"id": "1812.10240", "submitter": "Deepak Mittal", "authors": "Deepak Mittal, Shweta Bhardwaj, Mitesh M. Khapra, Balaraman Ravindran", "title": "Studying the Plasticity in Deep Convolutional Neural Networks using\n  Random Pruning", "comments": "To appear in the Journal of Machine Vision and Applications,\n  Springer. This work is an extended version of our previous work\n  arXiv:1801.10447, \"Recovering from Random Pruning: On the Plasticity of Deep\n  Convolutional Neural Networks\", accepted at WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been a lot of work on pruning filters from deep\nconvolutional neural networks (CNNs) with the intention of reducing\ncomputations.The key idea is to rank the filters based on a certain criterion\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\nfilters are pruned away the remainder of the network is fine tuned and is shown\nto give performance comparable to the original unpruned network. In this work,\nwe report experiments which suggest that the comparable performance of the\npruned network is not due to the specific criterion chosen but due to the\ninherent plasticity of deep neural networks which allows them to recover from\nthe loss of pruned filters once the rest of the filters are fine-tuned.\nSpecifically we show counter-intuitive results wherein by randomly pruning\n25-50% filters from deep CNNs we are able to obtain the same performance as\nobtained by using state-of-the-art pruning methods. We empirically validate our\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\nneeds to be tested on only a small set of classes at test time (say, only\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\nclass specific pruning and show that even here a random pruning strategy gives\nclose to state-of-the-art performance. Unlike existing approaches which mainly\nfocus on the task of image classification, in this work we also report results\non object detection and image segmentation. We show that using a simple random\npruning strategy we can achieve significant speed up in object detection (74%\nimprovement in fps) while retaining the same accuracy as that of the original\nFaster RCNN model. Similarly we show that the performance of a pruned\nSegmentation Network (SegNet) is actually very similar to that of the original\nunpruned SegNet.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 06:26:06 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Mittal", "Deepak", ""], ["Bhardwaj", "Shweta", ""], ["Khapra", "Mitesh M.", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1812.10256", "submitter": "Abdulkadir Albayrak", "authors": "Abdulkadir Albayrak, Asli Unlu, Nurullah Calik, Abdulkerim Capar,\n  Gokhan Bilgin, Behcet Ugur Toreyin, Bahar Muezzinoglu, Ilknur Turkmen,\n  Lutfiye Durak-Ata", "title": "A Whole Slide Image Grading Benchmark and Tissue Classification for\n  Cervical Cancer Precursor Lesions with Inter-Observer Variability", "comments": null, "journal-ref": null, "doi": "10.1007/s11517-021-02388-w", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cervical cancer developing from the precancerous lesions caused by the\nHuman Papilloma Virus (HPV) has been one of the preventable cancers with the\nhelp of periodic screening. There are two types of grading conventions widely\naccepted among pathologists. On the other hand, inter-observer variability is\nan important issue for final diagnosis. In this paper, a whole-slide image\ngrading benchmark for cervical cancer precursor lesions is introduced. The\npapillae of the cervical epithelium and overlapping cell problems are handled\nand a tissue classification method with a novel morphological feature\nexploiting the relative orientation between the BM and the major axis of all\nnuclei is developed and its performance is evaluated. Besides, the\ninter-observer variability is also revealed by a thorough comparison among\npathologists' decisions, as well as, the final diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 07:40:37 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Albayrak", "Abdulkadir", ""], ["Unlu", "Asli", ""], ["Calik", "Nurullah", ""], ["Capar", "Abdulkerim", ""], ["Bilgin", "Gokhan", ""], ["Toreyin", "Behcet Ugur", ""], ["Muezzinoglu", "Bahar", ""], ["Turkmen", "Ilknur", ""], ["Durak-Ata", "Lutfiye", ""]]}, {"id": "1812.10265", "submitter": "Zheng Xin", "authors": "Xin Zheng, Yanqing Guo, Huaibo Huang, Yi Li, Ran He", "title": "A Survey of Deep Facial Attribute Analysis", "comments": "submitted to International Journal of Computer Vision (IJCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attribute analysis has received considerable attention when deep\nlearning techniques made remarkable breakthroughs in this field over the past\nfew years. Deep learning based facial attribute analysis consists of two basic\nsub-issues: facial attribute estimation (FAE), which recognizes whether facial\nattributes are present in given images, and facial attribute manipulation\n(FAM), which synthesizes or removes desired facial attributes. In this paper,\nwe provide a comprehensive survey of deep facial attribute analysis from the\nperspectives of both estimation and manipulation. First, we summarize a general\npipeline that deep facial attribute analysis follows, which comprises two\nstages: data preprocessing and model construction. Additionally, we introduce\nthe underlying theories of this two-stage pipeline for both FAE and FAM.\nSecond, the datasets and performance metrics commonly used in facial attribute\nanalysis are presented. Third, we create a taxonomy of state-of-the-art methods\nand review deep FAE and FAM algorithms in detail. Furthermore, several\nadditional facial attribute related issues are introduced, as well as relevant\nreal-world applications. Finally, we discuss possible challenges and promising\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 09:24:07 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 06:58:40 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 03:13:51 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zheng", "Xin", ""], ["Guo", "Yanqing", ""], ["Huang", "Huaibo", ""], ["Li", "Yi", ""], ["He", "Ran", ""]]}, {"id": "1812.10305", "submitter": "Yiheng Liu", "authors": "Yiheng Liu, Zhenxun Yuan, Wengang Zhou, Houqiang Li", "title": "Spatial and Temporal Mutual Promotion for Video-based Person\n  Re-identification", "comments": "Accepted by AAAI19 as spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification is a crucial task of matching video\nsequences of a person across multiple camera views. Generally, features\ndirectly extracted from a single frame suffer from occlusion, blur,\nillumination and posture changes. This leads to false activation or missing\nactivation in some regions, which corrupts the appearance and motion\nrepresentation. How to explore the abundant spatial-temporal information in\nvideo sequences is the key to solve this problem. To this end, we propose a\nRefining Recurrent Unit (RRU) that recovers the missing parts and suppresses\nnoisy parts of the current frame's features by referring historical frames.\nWith RRU, the quality of each frame's appearance representation is improved.\nThen we use the Spatial-Temporal clues Integration Module (STIM) to mine the\nspatial-temporal information from those upgraded features. Meanwhile, the\nmulti-level training objective is used to enhance the capability of RRU and\nSTIM. Through the cooperation of those modules, the spatial and temporal\nfeatures mutually promote each other and the final spatial-temporal feature\nrepresentation is more discriminative and robust. Extensive experiments are\nconducted on three challenging datasets, i.e., iLIDS-VID, PRID-2011 and MARS.\nThe experimental results demonstrate that our approach outperforms existing\nstate-of-the-art methods of video-based person re-identification on iLIDS-VID\nand MARS and achieves favorable results on PRID-2011.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 13:24:31 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Liu", "Yiheng", ""], ["Yuan", "Zhenxun", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""]]}, {"id": "1812.10306", "submitter": "Moi Hoon Yap", "authors": "Jingting Li and Catherine Soladie and Renaud Sguier and Sujing Wang\n  and Moi Hoon Yap", "title": "Spotting Micro-Expressions on Long Videos Sequences", "comments": "4 pages, 3 figures and 3 tables", "journal-ref": "The 14th IEEE International Conference on Automatic Face & Gesture\n  Recognition (FG 2019)", "doi": "10.1109/FG.2019.8756626", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents baseline results for the first Micro-Expression Spotting\nChallenge 2019 by evaluating local temporal pattern (LTP) on SAMM and CAS(ME)2.\nThe proposed LTP patterns are extracted by applying PCA in a temporal window on\nseveral facial local regions. The micro-expression sequences are then spotted\nby a local classification of LTP and a global fusion. The performance is\nevaluated by Leave-One-Subject-Out cross validation. Furthermore, we define the\ncriteria of determining true positives in one video by overlap rate and set the\nmetric F1-score for spotting performance of the whole database. The F1-score of\nbaseline results for SAMM and CAS(ME)2 are 0.0316 and 0.0179, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 13:25:46 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 09:13:34 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Li", "Jingting", ""], ["Soladie", "Catherine", ""], ["Sguier", "Renaud", ""], ["Wang", "Sujing", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1812.10320", "submitter": "Fuyang Huang", "authors": "Fuyang Huang, Ailing Zeng, Minhao Liu, Jing Qin, Qiang Xu", "title": "Structure-Aware 3D Hourglass Network for Hand Pose Estimation from\n  Single Depth Image", "comments": "BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel structure-aware 3D hourglass network for\nhand pose estimation from a single depth image, which achieves state-of-the-art\nresults on MSRA and NYU datasets. Compared to existing works that perform\nimage-to-coordination regression, our network takes 3D voxel as input and\ndirectly regresses 3D heatmap for each joint. To be specific, we use hourglass\nnetwork as our backbone network and modify it into 3D form. We explicitly model\ntree-like finger bone into the network as well as in the loss function in an\nend-to-end manner, in order to take the skeleton constraints into\nconsideration. Final estimation can then be easily obtained from voxel density\nmap with simple post-processing. Experimental results show that the proposed\nstructure-aware 3D hourglass network is able to achieve a mean joint error of\n7.4 mm in MSRA and 8.9 mm in NYU datasets, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 14:08:08 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Huang", "Fuyang", ""], ["Zeng", "Ailing", ""], ["Liu", "Minhao", ""], ["Qin", "Jing", ""], ["Xu", "Qiang", ""]]}, {"id": "1812.10325", "submitter": "Zishan Sami", "authors": "Doney Alex, Zishan Sami, Sumandeep Banerjee and Subrat Panda", "title": "Cluster Loss for Person Re-Identification", "comments": "Published in 11th Indian Conference on Computer Vision, Graphics and\n  Image Processing (ICVGIP 2018). arXiv admin note: text overlap with\n  arXiv:1610.02984, arXiv:1703.07737, arXiv:1704.01719, arXiv:1511.09123 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) is an important problem in computer vision,\nespecially for video surveillance applications. The problem focuses on\nidentifying people across different cameras or across different frames of the\nsame camera. The main challenge lies in identifying the similarity of the same\nperson against large appearance and structure variations, while differentiating\nbetween individuals. Recently, deep learning networks with triplet loss have\nbecome a common framework for person ReID. However, triplet loss focuses on\nobtaining correct orders on the training set. We demonstrate that it performs\ninferior in a clustering task. In this paper, we design a cluster loss, which\ncan lead to the model output with a larger inter-class variation and a smaller\nintra-class variation compared to the triplet loss. As a result, our model has\na better generalization ability and can achieve higher accuracy on the test set\nespecially for a clustering task. We also introduce a batch hard training\nmechanism for improving the results and faster convergence of training.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 18:14:19 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Alex", "Doney", ""], ["Sami", "Zishan", ""], ["Banerjee", "Sumandeep", ""], ["Panda", "Subrat", ""]]}, {"id": "1812.10328", "submitter": "Sina Mokhtarzadeh Azar", "authors": "Sina Mokhtarzadeh Azar, Mina Ghadimi Atigh, Ahmad Nickabadi", "title": "A Multi-Stream Convolutional Neural Network Framework for Group Activity\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a framework based on multi-stream convolutional\nneural networks (CNNs) for group activity recognition. Streams of CNNs are\nseparately trained on different modalities and their predictions are fused at\nthe end. Each stream has two branches to predict the group activity based on\nperson and scene level representations. A new modality based on the human pose\nestimation is presented to add extra information to the model. We evaluate our\nmethod on the Volleyball and Collective Activity datasets. Experimental results\nshow that the proposed framework is able to achieve state-of-the-art results\nwhen multiple or single frames are given as input to the model with 90.50% and\n86.61% accuracy on Volleyball dataset, respectively, and 87.01% accuracy of\nmultiple frames group activity on Collective Activity dataset.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 14:30:46 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Azar", "Sina Mokhtarzadeh", ""], ["Atigh", "Mina Ghadimi", ""], ["Nickabadi", "Ahmad", ""]]}, {"id": "1812.10330", "submitter": "Awais Mansoor", "authors": "Awais Mansoor, Antonio R. Porras, Marius George Linguraru", "title": "Region Proposal Networks with Contextual Selective Attention for\n  Real-Time Organ Detection", "comments": "Accepted at the IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2019, Venice-Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for object detection use region proposal networks\n(RPN) to hypothesize object location. These networks simultaneously predicts\nobject bounding boxes and \\emph{objectness} scores at each location in the\nimage. Unlike natural images for which RPN algorithms were originally designed,\nmost medical images are acquired following standard protocols, thus organs in\nthe image are typically at a similar location and possess similar geometrical\ncharacteristics (e.g. scale, aspect-ratio, etc.). Therefore, medical image\nacquisition protocols hold critical localization and geometric information that\ncan be incorporated for faster and more accurate detection. This paper presents\na novel attention mechanism for the detection of organs by incorporating\nimaging protocol information. Our novel selective attention approach (i)\neffectively shrinks the search space inside the feature map, (ii) appends\nuseful localization information to the hypothesized proposal for the detection\narchitecture to learn where to look for each organ, and (iii) modifies the\npyramid of regression references in the RPN by incorporating organ- and\nmodality-specific information, which results in additional time reduction. We\nevaluated the proposed framework on a dataset of 768 chest X-ray images\nobtained from a diverse set of sources. Our results demonstrate superior\nperformance for the detection of the lung field compared to the\nstate-of-the-art, both in terms of detection accuracy, demonstrating an\nimprovement of $>7\\%$ in Dice score, and reduced processing time by $27.53\\%$\ndue to fewer hypotheses.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 14:33:59 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Mansoor", "Awais", ""], ["Porras", "Antonio R.", ""], ["Linguraru", "Marius George", ""]]}, {"id": "1812.10352", "submitter": "Byungju Kim", "authors": "Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim and Junmo Kim", "title": "Learning Not to Learn: Training Deep Neural Networks with Biased Data", "comments": "CVPR 2019, Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel regularization algorithm to train deep neural networks, in\nwhich data at training time is severely biased. Since a neural network\nefficiently learns data distribution, a network is likely to learn the bias\ninformation to categorize input data. It leads to poor performance at test\ntime, if the bias is, in fact, irrelevant to the categorization. In this paper,\nwe formulate a regularization loss based on mutual information between feature\nembedding and bias. Based on the idea of minimizing this mutual information, we\npropose an iterative algorithm to unlearn the bias information. We employ an\nadditional network to predict the bias distribution and train the network\nadversarially against the feature embedding network. At the end of learning,\nthe bias prediction network is not able to predict the bias not because it is\npoorly trained, but because the feature embedding network successfully unlearns\nthe bias information. We also demonstrate quantitative and qualitative\nexperimental results which show that our algorithm effectively removes the bias\ninformation from feature embedding.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 16:01:29 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 08:42:54 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kim", "Byungju", ""], ["Kim", "Hyunwoo", ""], ["Kim", "Kyungsu", ""], ["Kim", "Sungjin", ""], ["Kim", "Junmo", ""]]}, {"id": "1812.10358", "submitter": "Lior Bracha", "authors": "Lior Bracha, Gal Chechik", "title": "Informative Object Annotations: Tell Me Something I Don't Know", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the interesting components of an image is a key aspect of image\nunderstanding. When a speaker annotates an image, selecting labels that are\ninformative greatly depends on the prior knowledge of a prospective listener.\nMotivated by cognitive theories of categorization and communication, we present\na new unsupervised approach to model this prior knowledge and quantify the\ninformativeness of a description. Specifically, we compute how knowledge of a\nlabel reduces uncertainty over the space of labels and utilize this to rank\ncandidate labels for describing an image. While the full estimation problem is\nintractable, we describe an efficient algorithm to approximate entropy\nreduction using a tree-structured graphical model. We evaluate our approach on\nthe open-images dataset using a new evaluation set of 10K ground-truth ratings\nand find that it achieves ~65% agreement with human raters, largely\noutperforming other unsupervised baseline approaches.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 16:12:30 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Bracha", "Lior", ""], ["Chechik", "Gal", ""]]}, {"id": "1812.10366", "submitter": "Yinhao Zhu", "authors": "Yide Zhang, Yinhao Zhu, Evan Nichols, Qingfei Wang, Siyuan Zhang, Cody\n  Smith, Scott Howard", "title": "A Poisson-Gaussian Denoising Dataset with Real Fluorescence Microscopy\n  Images", "comments": "Camera-ready version for CVPR 2019. The Fluorescence Microscopy\n  Denoising (FMD) dataset is available at\n  https://drive.google.com/drive/folders/1aygMzSDdoq63IqSk-ly8cMq0_owup8UM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence microscopy has enabled a dramatic development in modern biology.\nDue to its inherently weak signal, fluorescence microscopy is not only much\nnoisier than photography, but also presented with Poisson-Gaussian noise where\nPoisson noise, or shot noise, is the dominating noise source. To get clean\nfluorescence microscopy images, it is highly desirable to have effective\ndenoising algorithms and datasets that are specifically designed to denoise\nfluorescence microscopy images. While such algorithms exist, no such datasets\nare available. In this paper, we fill this gap by constructing a dataset - the\nFluorescence Microscopy Denoising (FMD) dataset - that is dedicated to\nPoisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence\nmicroscopy images obtained with commercial confocal, two-photon, and wide-field\nmicroscopes and representative biological samples such as cells, zebrafish, and\nmouse brain tissues. We use image averaging to effectively obtain ground truth\nimages and 60,000 noisy images with different noise levels. We use this dataset\nto benchmark 10 representative denoising algorithms and find that deep learning\nmethods have the best performance. To our knowledge, this is the first real\nmicroscopy image dataset for Poisson-Gaussian denoising purposes and it could\nbe an important tool for high-quality, real-time denoising applications in\nbiomedical research.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 16:42:02 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 22:26:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Yide", ""], ["Zhu", "Yinhao", ""], ["Nichols", "Evan", ""], ["Wang", "Qingfei", ""], ["Zhang", "Siyuan", ""], ["Smith", "Cody", ""], ["Howard", "Scott", ""]]}, {"id": "1812.10378", "submitter": "Polina Lemenkova", "authors": "Polina Lemenkova", "title": "Urban-Rural Environmental Gradient in a Developing City: Testing ENVI\n  GIS Functionality", "comments": "5 pages, 2 figures, 1 table", "journal-ref": "Conference Proceedings 'Abishevskie Readings. Innovation in the\n  Complex Processing of Mineral Raw Materials', 21-22 Jan 2016", "doi": "10.6084/m9.figshare.7210286", "report-no": null, "categories": "cs.CY cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The research performs urban ecosystem analysis supported by ENVI GIS by\nintegrated studies on land cover types and geospatial modeling of Taipei city.\nThe paper deals with the role of anthropogenic pressure on the structure of the\nlandscape and change of land cover types. Methods included assessment of the\nimpact from anthropogenic activities on the natural ecosystems, evaluation of\nthe rate and scale of landscape dynamics using remote sensing data and GIS. The\nresearch aims to assist environmentalists and city planners to evaluate\nstrategies for specific objectives of urban development in Taiwan, China.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 02:10:53 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Lemenkova", "Polina", ""]]}, {"id": "1812.10411", "submitter": "Siddique Latif", "authors": "Siddique Latif, Adnan Qayyum, Muhammad Usman, and Junaid Qadir", "title": "Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages", "comments": "IEEE International Conference on Frontiers of Information Technology\n  (FIT), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual speech emotion recognition is an important task for practical\napplications. The performance of automatic speech emotion recognition systems\ndegrades in cross-corpus scenarios, particularly in scenarios involving\nmultiple languages or a previously unseen language such as Urdu for which\nlimited or no data is available. In this study, we investigate the problem of\ncross-lingual emotion recognition for Urdu language and contribute URDU---the\nfirst ever spontaneous Urdu-language speech emotion database. Evaluations are\nperformed using three different Western languages against Urdu and experimental\nresults on different possible scenarios suggest various interesting aspects for\ndesigning more adaptive emotion recognition system for such limited languages.\nIn results, selecting training instances of multiple languages can deliver\ncomparable results to baseline and augmentation a fraction of testing language\ndata while training can help to boost accuracy for speech emotion recognition.\nURDU data is publicly available for further research.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 01:04:18 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 01:42:46 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Latif", "Siddique", ""], ["Qayyum", "Adnan", ""], ["Usman", "Muhammad", ""], ["Qadir", "Junaid", ""]]}, {"id": "1812.10477", "submitter": "Yulun Zhang", "authors": "Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu", "title": "Residual Dense Network for Image Restoration", "comments": "To appear in TPAMI. arXiv admin note: substantial text overlap with\n  arXiv:1802.08797", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network has recently achieved great success for image\nrestoration (IR) and also offered hierarchical features. However, most deep CNN\nbased IR models do not make full use of the hierarchical features from the\noriginal low-quality images, thereby achieving relatively-low performance. In\nthis paper, we propose a novel residual dense network (RDN) to address this\nproblem in IR. We fully exploit the hierarchical features from all the\nconvolutional layers. Specifically, we propose residual dense block (RDB) to\nextract abundant local features via densely connected convolutional layers. RDB\nfurther allows direct connections from the state of preceding RDB to all the\nlayers of current RDB, leading to a contiguous memory mechanism. To adaptively\nlearn more effective features from preceding and current local features and\nstabilize the training of wider network, we proposed local feature fusion in\nRDB. After fully obtaining dense local features, we use global feature fusion\nto jointly and adaptively learn global hierarchical features in a holistic way.\nWe demonstrate the effectiveness of RDN with several representative IR\napplications, single image super-resolution, Gaussian image denoising, image\ncompression artifact reduction, and image deblurring. Experiments on benchmark\nand real-world datasets show that our RDN achieves favorable performance\nagainst state-of-the-art methods for each IR task quantitatively and visually.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 03:45:44 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 01:10:07 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Zhang", "Yulun", ""], ["Tian", "Yapeng", ""], ["Kong", "Yu", ""], ["Zhong", "Bineng", ""], ["Fu", "Yun", ""]]}, {"id": "1812.10482", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Amirali Abdolrashidi", "title": "Finger-GAN: Generating Realistic Fingerprint Images Using Connectivity\n  Imposed GAN", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.04822", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating realistic biometric images has been an interesting and, at the\nsame time, challenging problem. Classical statistical models fail to generate\nrealistic-looking fingerprint images, as they are not powerful enough to\ncapture the complicated texture representation in fingerprint images. In this\nwork, we present a machine learning framework based on generative adversarial\nnetworks (GAN), which is able to generate fingerprint images sampled from a\nprior distribution (learned from a set of training images). We also add a\nsuitable regularization term to the loss function, to impose the connectivity\nof generated fingerprint images. This is highly desirable for fingerprints, as\nthe lines in each finger are usually connected. We apply this framework to two\npopular fingerprint databases, and generate images which look very realistic,\nand similar to the samples in those databases. Through experimental results, we\nshow that the generated fingerprint images have a good diversity, and are able\nto capture different parts of the prior distribution. We also evaluate the\nFrechet Inception distance (FID) of our proposed model, and show that our model\nis able to achieve good quantitative performance in terms of this score.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 20:41:35 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "Amirali", ""]]}, {"id": "1812.10524", "submitter": "Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Francesca Babiloni, Rahaf Aljundi, Marcus Rohrbach,\n  Manohar Paluri, Tinne Tuytelaars", "title": "Exploring the Challenges towards Lifelong Fact Learning", "comments": "This work got published at ACCV 2018 as a main conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far life-long learning (LLL) has been studied in relatively small-scale\nand relatively artificial setups. Here, we introduce a new large-scale\nalternative. What makes the proposed setup more natural and closer to\nhuman-like visual systems is threefold: First, we focus on concepts (or facts,\nas we call them) of varying complexity, ranging from single objects to more\ncomplex structures such as objects performing actions, and objects interacting\nwith other objects. Second, as in real-world settings, our setup has a\nlong-tail distribution, an aspect which has mostly been ignored in the LLL\ncontext. Third, facts across tasks may share structure (e.g., <person, riding,\nwave> and <dog, riding, wave>). Facts can also be semantically related (e.g.,\n\"liger\" relates to seen categories like \"tiger\" and \"lion\"). Given the large\nnumber of possible facts, a LLL setup seems a natural choice. To avoid model\nsize growing over time and to optimally exploit the semantic relations and\nstructure, we combine it with a visual semantic embedding instead of discrete\nclass labels. We adapt existing datasets with the properties mentioned above\ninto new benchmarks, by dividing them semantically or randomly into disjoint\ntasks. This leads to two large-scale benchmarks with 906,232 images and 165,150\nunique facts, on which we evaluate and analyze state-of-the-art LLL methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 20:09:49 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Babiloni", "Francesca", ""], ["Aljundi", "Rahaf", ""], ["Rohrbach", "Marcus", ""], ["Paluri", "Manohar", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1812.10532", "submitter": "Sharath Girish", "authors": "Anil Kumar Vadathya, Sharath Girish and Kaushik Mitra", "title": "A Unified Learning Based Framework for Light Field Reconstruction from\n  Coded Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field presents a rich way to represent the 3D world by capturing the\nspatio-angular dimensions of the visual signal. However, the popular way of\ncapturing light field (LF) via a plenoptic camera presents spatio-angular\nresolution trade-off. Computational imaging techniques such as compressive\nlight field and programmable coded aperture reconstruct full sensor resolution\nLF from coded projections obtained by multiplexing the incoming spatio-angular\nlight field. Here, we present a unified learning framework that can reconstruct\nLF from a variety of multiplexing schemes with minimal number of coded images\nas input. We consider three light field capture schemes: heterodyne capture\nscheme with code placed near the sensor, coded aperture scheme with code at the\ncamera aperture and finally the dual exposure scheme of capturing a\nfocus-defocus pair where there is no explicit coding. Our algorithm consists of\nthree stages 1) we recover the all-in-focus image from the coded image 2) we\nestimate the disparity maps for all the LF views from the coded image and the\nall-in-focus image, 3) we then render the LF by warping the all-in-focus image\nusing disparity maps and refine it. For these three stages we propose three\ndeep neural networks - ViewNet, DispairtyNet and RefineNet. Our reconstructions\nshow that our learning algorithm achieves state-of-the-art results for all the\nthree multiplexing schemes. Especially, our LF reconstructions from\nfocus-defocus pair is comparable to other learning-based view synthesis\napproaches from multiple images. Thus, our work paves the way for capturing\nhigh-resolution LF (~ a megapixel) using conventional cameras such as DSLRs.\nPlease check our supplementary materials\n$\\href{https://docs.google.com/presentation/d/1Vr-F8ZskrSd63tvnLfJ2xmEXY6OBc1Rll3XeOAtc11I/}{online}$\nto better appreciate the reconstructed light fields.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 20:57:43 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 21:35:45 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Vadathya", "Anil Kumar", ""], ["Girish", "Sharath", ""], ["Mitra", "Kaushik", ""]]}, {"id": "1812.10550", "submitter": "Huy-Hieu Pham", "authors": "Huy-Hieu Pham and Louahdi Khoudour and Alain Crouzil and Pablo Zegers\n  and Sergio A. Velastin", "title": "Learning to Recognize 3D Human Action from A New Skeleton-based\n  Representation Using Deep Convolutional Neural Networks", "comments": "This paper is a preprint of a paper published to IET Computer Vision.\n  The copy of the record will be available at the IET Digital Library", "journal-ref": null, "doi": "10.1049/iet-cvi.2018.5014", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing human actions in untrimmed videos is an important challenging\ntask. An effective 3D motion representation and a powerful learning model are\ntwo key factors influencing recognition performance. In this paper we introduce\na new skeleton-based representation for 3D action recognition in videos. The\nkey idea of the proposed representation is to transform 3D joint coordinates of\nthe human body carried in skeleton sequences into RGB images via a color\nencoding process. By normalizing the 3D joint coordinates and dividing each\nskeleton frame into five parts, where the joints are concatenated according to\nthe order of their physical connections, the color-coded representation is able\nto represent spatio-temporal evolutions of complex 3D motions, independently of\nthe length of each sequence. We then design and train different Deep\nConvolutional Neural Networks (D-CNNs) based on the Residual Network\narchitecture (ResNet) on the obtained image-based representations to learn 3D\nmotion features and classify them into classes. Our method is evaluated on two\nwidely used action recognition benchmarks: MSR Action3D and NTU-RGB+D, a very\nlarge-scale dataset for 3D human action recognition. The experimental results\ndemonstrate that the proposed method outperforms previous state-of-the-art\napproaches whilst requiring less computation for training and prediction.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 21:47:08 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Pham", "Huy-Hieu", ""], ["Khoudour", "Louahdi", ""], ["Crouzil", "Alain", ""], ["Zegers", "Pablo", ""], ["Velastin", "Sergio A.", ""]]}, {"id": "1812.10553", "submitter": "Niv Derech", "authors": "Niv Derech, Ayellet Tal, Ilan Shimshoni", "title": "Solving Archaeological Puzzles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Puzzle solving is a difficult problem in its own right, even when the pieces\nare all square and build up a natural image. But what if these ideal conditions\ndo not hold? One such application domain is archaeology, where restoring an\nartifact from its fragments is highly important. From the point of view of\ncomputer vision, archaeological puzzle solving is very challenging, due to\nthree additional difficulties: the fragments are of general shape; they are\nabraded, especially at the boundaries (where the strongest cues for matching\nshould exist); and the domain of valid transformations between the pieces is\ncontinuous. The key contribution of this paper is a fully-automatic and general\nalgorithm that addresses puzzle solving in this intriguing domain. We show that\nour state-of-the-art approach manages to correctly reassemble dozens of broken\nartifacts and frescoes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 21:57:55 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Derech", "Niv", ""], ["Tal", "Ayellet", ""], ["Shimshoni", "Ilan", ""]]}, {"id": "1812.10558", "submitter": "Minh Ngo", "authors": "Minh Ng\\^o, Burak Mandira, Selim F{\\i}rat Y{\\i}lmaz, Ward Heij, Sezer\n  Karaoglu, Henri Bouma, Hamdi Dibeklioglu, Theo Gevers", "title": "Deception Detection by 2D-to-3D Face Reconstruction from Videos", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lies and deception are common phenomena in society, both in our private and\nprofessional lives. However, humans are notoriously bad at accurate deception\ndetection. Based on the literature, human accuracy of distinguishing between\nlies and truthful statements is 54% on average, in other words it is slightly\nbetter than a random guess. While people do not much care about this issue, in\nhigh-stakes situations such as interrogations for series crimes and for\nevaluating the testimonies in court cases, accurate deception detection methods\nare highly desirable. To achieve a reliable, covert, and non-invasive deception\ndetection, we propose a novel method that jointly extracts reliable low- and\nhigh-level facial features namely, 3D facial geometry, skin reflectance,\nexpression, head pose, and scene illumination in a video sequence. Then these\nfeatures are modeled using a Recurrent Neural Network to learn temporal\ncharacteristics of deceptive and honest behavior. We evaluate the proposed\nmethod on the Real-Life Trial (RLT) dataset that contains high-stake deceptive\nand honest videos recorded in courtrooms. Our results show that the proposed\nmethod (with an accuracy of 72.8%) improves the state of the art as well as\noutperforming the use of manually coded facial attributes 67.6%) in deception\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 22:11:52 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Ng\u00f4", "Minh", ""], ["Mandira", "Burak", ""], ["Y\u0131lmaz", "Selim F\u0131rat", ""], ["Heij", "Ward", ""], ["Karaoglu", "Sezer", ""], ["Bouma", "Henri", ""], ["Dibeklioglu", "Hamdi", ""], ["Gevers", "Theo", ""]]}, {"id": "1812.10587", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, Ying Nian Wu", "title": "Learning Dynamic Generator Model by Alternating Back-Propagation Through\n  Time", "comments": "10 pages", "journal-ref": "The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)\n  2019", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the dynamic generator model for spatial-temporal processes\nsuch as dynamic textures and action sequences in video data. In this model,\neach time frame of the video sequence is generated by a generator model, which\nis a non-linear transformation of a latent state vector, where the non-linear\ntransformation is parametrized by a top-down neural network. The sequence of\nlatent state vectors follows a non-linear auto-regressive model, where the\nstate vector of the next frame is a non-linear transformation of the state\nvector of the current frame as well as an independent noise vector that\nprovides randomness in the transition. The non-linear transformation of this\ntransition model can be parametrized by a feedforward neural network. We show\nthat this model can be learned by an alternating back-propagation through time\nalgorithm that iteratively samples the noise vectors and updates the parameters\nin the transition model and the generator model. We show that our training\nmethod can learn realistic models for dynamic textures and action patterns.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 01:34:08 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Xie", "Jianwen", ""], ["Gao", "Ruiqi", ""], ["Zheng", "Zilong", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1812.10592", "submitter": "Robert Ravier", "authors": "Robert J. Ravier", "title": "Eyes on the Prize: Improved Biological Surface Registration via Forward\n  Propagation", "comments": "20 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms for surface registration risk producing significant errors if\nsurfaces are significantly nonisometric. Manifold learning has been shown to be\neffective at improving registration quality, using information from an entire\ncollection of surfaces to correct issues present in pairwise registrations.\nThese methods, however, are not robust to changes in the collection of\nsurfaces, or do not produce accurate registrations at a resolution high enough\nfor subsequent downstream analysis. We propose a novel algorithm for\nefficiently registering such collections given initial correspondences with\nvarying degrees of accuracy. By combining the initial information with recent\ndevelopments in manifold learning, we employ a simple metric condition to\nconstruct a measure on the space of correspondences between any pair of shapes\nin our collection, which we then use to distill soft correspondences. We\ndemonstrate that this measure can improve correspondence accuracy between\nfeature points compared to currently employed, less robust methods on a diverse\ndataset of surfaces from evolutionary biology. We then show how our methods can\nbe used, in combination with recent sampling and interpolation methods, to\ncompute accurate and consistent homeomorphisms between surfaces.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 02:11:36 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 17:55:18 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Ravier", "Robert J.", ""]]}, {"id": "1812.10595", "submitter": "Sheikh Muhammad Saiful Islam", "authors": "Sheikh Muhammad Saiful Islam, Md Mahedi Hasan, Sohaib Abdullah", "title": "Deep Learning based Early Detection and Grading of Diabetic Retinopathy\n  Using Retinal Fundus Images", "comments": "Accepted in MIND 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy (DR) is a constantly deteriorating disease, being one of\nthe leading causes of vision impairment and blindness. Subtle distinction among\ndifferent grades and existence of many significant small features make the task\nof recognition very challenging. In addition, the present approach of\nretinopathy detection is a very laborious and time-intensive task, which\nheavily relies on the skill of a physician. Automated detection of diabetic\nretinopathy is essential to tackle these problems. Early-stage detection of\ndiabetic retinopathy is also very important for diagnosis, which can prevent\nblindness with proper treatment. In this paper, we developed a novel deep\nconvolutional neural network, which performs the early-stage detection by\nidentifying all microaneurysms (MAs), the first signs of DR, along with\ncorrectly assigning labels to retinal fundus images which are graded into five\ncategories. We have tested our network on the largest publicly available Kaggle\ndiabetic retinopathy dataset, and achieved 0.851 quadratic weighted kappa score\nand 0.844 AUC score, which achieves the state-of-the-art performance on\nseverity grading. In the early-stage detection, we have achieved a sensitivity\nof 98% and specificity of above 94%, which demonstrates the effectiveness of\nour proposed method. Our proposed architecture is at the same time very simple\nand efficient with respect to computational time and space are concerned.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 02:24:24 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Islam", "Sheikh Muhammad Saiful", ""], ["Hasan", "Md Mahedi", ""], ["Abdullah", "Sohaib", ""]]}, {"id": "1812.10636", "submitter": "Abhijit Balaji", "authors": "Abhijit Balaji, Thuvaarakkesh Ramanathan and Venkateshwarlu Sonathi", "title": "Chart-Text: A Fully Automated Chart Image Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images greatly help in understanding, interpreting and visualizing data.\nAdding textual description to images is the first and foremost principle of web\naccessibility. Visually impaired users using screen readers will use these\ntextual descriptions to get better understanding of images present in digital\ncontents. In this paper, we propose Chart-Text a novel fully automated system\nthat creates textual description of chart images. Given a PNG image of a chart,\nour Chart-Text system creates a complete textual description of it. First, the\nsystem classifies the type of chart and then it detects and classifies the\nlabels and texts in the charts. Finally, it uses specific image processing\nalgorithms to extract relevant information from the chart images. Our proposed\nsystem achieves an accuracy of 99.72% in classifying the charts and an accuracy\nof 78.9% in extracting the data and creating the corresponding textual\ndescription.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 05:52:06 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Balaji", "Abhijit", ""], ["Ramanathan", "Thuvaarakkesh", ""], ["Sonathi", "Venkateshwarlu", ""]]}, {"id": "1812.10695", "submitter": "Guangyi Yang", "authors": "Xiaoqiao Chen, Qingyi Zhang, Manhui Lin, Guangyi Yang, Chu He", "title": "No-Reference Color Image Quality Assessment: From Entropy to Perceptual\n  Quality", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a high-performance general-purpose no-reference (NR)\nimage quality assessment (IQA) method based on image entropy. The image\nfeatures are extracted from two domains. In the spatial domain, the mutual\ninformation between the color channels and the two-dimensional entropy are\ncalculated. In the frequency domain, the two-dimensional entropy and the mutual\ninformation of the filtered sub-band images are computed as the feature set of\nthe input color image. Then, with all the extracted features, the support\nvector classifier (SVC) for distortion classification and support vector\nregression (SVR) are utilized for the quality prediction, to obtain the final\nquality assessment score. The proposed method, which we call entropy-based\nno-reference image quality assessment (ENIQA), can assess the quality of\ndifferent categories of distorted images, and has a low complexity. The\nproposed ENIQA method was assessed on the LIVE and TID2013 databases and showed\na superior performance. The experimental results confirmed that the proposed\nENIQA method has a high consistency of objective and subjective assessment on\ncolor images, which indicates the good overall performance and generalization\nability of ENIQA. The source code is available on github\nhttps://github.com/jacob6/ENIQA.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 11:01:29 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Chen", "Xiaoqiao", ""], ["Zhang", "Qingyi", ""], ["Lin", "Manhui", ""], ["Yang", "Guangyi", ""], ["He", "Chu", ""]]}, {"id": "1812.10705", "submitter": "Niv Haim", "authors": "Niv Haim, Nimrod Segol, Heli Ben-Hamu, Haggai Maron, Yaron Lipman", "title": "Surface Networks via General Covers", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing deep learning techniques for geometric data is an active and\nfruitful research area. This paper tackles the problem of sphere-type surface\nlearning by developing a novel surface-to-image representation. Using this\nrepresentation we are able to quickly adapt successful CNN models to the\nsurface setting.\n  The surface-image representation is based on a covering map from the image\ndomain to the surface. Namely, the map wraps around the surface several times,\nmaking sure that every part of the surface is well represented in the image.\nDifferently from previous surface-to-image representations, we provide a low\ndistortion coverage of all surface parts in a single image. Specifically, for\nthe use case of learning spherical signals, our representation provides a low\ndistortion alternative to several popular spherical parameterizations used in\ndeep learning.\n  We have used the surface-to-image representation to apply standard CNN\narchitectures to 3D models as well as spherical signals. We show that our\nmethod achieves state of the art or comparable results on the tasks of shape\nretrieval, shape classification and semantic shape segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 12:18:59 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 16:34:06 GMT"}, {"version": "v3", "created": "Sun, 18 Aug 2019 12:52:09 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Haim", "Niv", ""], ["Segol", "Nimrod", ""], ["Ben-Hamu", "Heli", ""], ["Maron", "Haggai", ""], ["Lipman", "Yaron", ""]]}, {"id": "1812.10717", "submitter": "Sinisa Stekovic", "authors": "Sinisa Stekovic, Friedrich Fraundorfer, Vincent Lepetit", "title": "S4-Net: Geometry-Consistent Semi-Supervised Semantic Segmentation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that it is possible to learn semantic segmentation from very limited\namounts of manual annotations, by enforcing geometric 3D constraints between\nmultiple views. More exactly, image locations corresponding to the same\nphysical 3D point should all have the same label. We show that introducing such\nconstraints during learning is very effective, even when no manual label is\navailable for a 3D point, and can be done simply by employing techniques from\n'general' semi-supervised learning to the context of semantic segmentation. To\ndemonstrate this idea, we use RGB-D image sequences of rigid scenes, for a\n4-class segmentation problem derived from the ScanNet dataset. Starting from\nRGB-D sequences with a few annotated frames, we show that we can incorporate\nRGB-D sequences without any manual annotations to improve the performance,\nwhich makes our approach very convenient. Furthermore, we demonstrate our\napproach for semantic segmentation of objects on the LabelFusion dataset, where\nwe show that one manually labeled image in a scene is sufficient for high\nperformance on the whole scene.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 13:24:56 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 13:48:44 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Stekovic", "Sinisa", ""], ["Fraundorfer", "Friedrich", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1812.10745", "submitter": "Hichem Sahbi", "authors": "Hichem Sahbi", "title": "Finite State Machines for Semantic Scene Parsing and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in this work a novel stochastic inference process, for scene\nannotation and object class segmentation, based on finite state machines\n(FSMs). The design principle of our framework is generative and based on\nbuilding, for a given scene, finite state machines that encode annotation\nlattices, and inference consists in finding and scoring the best configurations\nin these lattices. Different novel operations are defined using our FSM\nframework including reordering, segmentation, visual transduction, and label\ndependency modeling. All these operations are combined together in order to\nachieve annotation as well as object class segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 15:41:22 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Sahbi", "Hichem", ""]]}, {"id": "1812.10747", "submitter": "Aniket Pramanik", "authors": "Aniket Pramanik, Hemant Kumar Aggarwal, Mathews Jacob", "title": "Off-the-grid model based deep learning (O-MODL)", "comments": "ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model based off-the-grid image reconstruction algorithm using\ndeep learned priors. The main difference of the proposed scheme with current\ndeep learning strategies is the learning of non-linear annihilation relations\nin Fourier space. We rely on a model based framework, which allows us to use a\nsignificantly smaller deep network, compared to direct approaches that also\nlearn how to invert the forward model. Preliminary comparisons against image\ndomain MoDL approach demonstrates the potential of the off-the-grid\nformulation. The main benefit of the proposed scheme compared to structured\nlow-rank methods is the quite significant reduction in computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 15:48:10 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Pramanik", "Aniket", ""], ["Aggarwal", "Hemant Kumar", ""], ["Jacob", "Mathews", ""]]}, {"id": "1812.10766", "submitter": "Meysam Madadi", "authors": "Meysam Madadi, Hugo Bertiche and Sergio Escalera", "title": "SMPLR: Deep SMPL reverse for 3D human pose and shape recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current state-of-the-art in 3D human pose and shape recovery relies on deep\nneural networks and statistical morphable body models, such as the Skinned\nMulti-Person Linear model (SMPL). However, regardless of the advantages of\nhaving both body pose and shape, SMPL-based solutions have shown difficulties\nto predict 3D bodies accurately. This is mainly due to the unconstrained nature\nof SMPL, which may generate unrealistic body meshes. Because of this,\nregression of SMPL parameters is a difficult task, often addressed with complex\nregularization terms. In this paper we propose to embed SMPL within a deep\nmodel to accurately estimate 3D pose and shape from a still RGB image. We use\nCNN-based 3D joint predictions as an intermediate representation to regress\nSMPL pose and shape parameters. Later, 3D joints are reconstructed again in the\nSMPL output. This module can be seen as an autoencoder where the encoder is a\ndeep neural network and the decoder is SMPL model. We refer to this as SMPL\nreverse (SMPLR). By implementing SMPLR as an encoder-decoder we avoid the need\nof complex constraints on pose and shape. Furthermore, given that in-the-wild\ndatasets usually lack accurate 3D annotations, it is desirable to lift 2D\njoints to 3D without pairing 3D annotations with RGB images. Therefore, we also\npropose a denoising autoencoder (DAE) module between CNN and SMPLR, able to\nlift 2D joints to 3D and partially recover from structured error. We evaluate\nour method on SURREAL and Human3.6M datasets, showing improvement over\nSMPL-based state-of-the-art alternatives by about 4 and 25 millimeters,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 16:47:11 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 10:56:36 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Madadi", "Meysam", ""], ["Bertiche", "Hugo", ""], ["Escalera", "Sergio", ""]]}, {"id": "1812.10775", "submitter": "Tolga Birdal", "authors": "Yongheng Zhao and Tolga Birdal and Haowen Deng and Federico Tombari", "title": "3D Point Capsule Networks", "comments": "As published in CVPR 2019 (camera ready version), with supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose 3D point-capsule networks, an auto-encoder designed\nto process sparse 3D point clouds while preserving spatial arrangements of the\ninput data. 3D capsule networks arise as a direct consequence of our novel\nunified 3D auto-encoder formulation. Their dynamic routing scheme and the\npeculiar 2D latent space deployed by our approach bring in improvements for\nseveral common point cloud-related tasks, such as object classification, object\nreconstruction and part segmentation as substantiated by our extensive\nevaluations. Moreover, it enables new applications such as part interpolation\nand replacement.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 17:16:48 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 23:39:55 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Zhao", "Yongheng", ""], ["Birdal", "Tolga", ""], ["Deng", "Haowen", ""], ["Tombari", "Federico", ""]]}, {"id": "1812.10779", "submitter": "Alejandro Lopez-Cifuentes", "authors": "Alejandro L\\'opez-Cifuentes, Marcos Escudero-Vi\\~nolo, Jes\\'us\n  Besc\\'os and Pablo Carballeira", "title": "Semantic Driven Multi-Camera Pedestrian Detection", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current worldwide situation, pedestrian detection has reemerged as a\npivotal tool for intelligent video-based systems aiming to solve tasks such as\npedestrian tracking, social distancing monitoring or pedestrian mass counting.\nPedestrian detection methods, even the top performing ones, are highly\nsensitive to occlusions among pedestrians, which dramatically degrades their\nperformance in crowded scenarios. The generalization of multi-camera set-ups\npermits to better confront occlusions by combining information from different\nviewpoints. In this paper, we present a multi-camera approach to globally\ncombine pedestrian detections leveraging automatically extracted scene context.\nContrarily to the majority of the methods of the state-of-the-art, the proposed\napproach is scene-agnostic, not requiring a tailored adaptation to the target\nscenario\\textemdash e.g., via fine-tunning. This noteworthy attribute does not\nrequire \\textit{ad hoc} training with labelled data, expediting the deployment\nof the proposed method in real-world situations. Context information, obtained\nvia semantic segmentation, is used 1) to automatically generate a common Area\nof Interest for the scene and all the cameras, avoiding the usual need of\nmanually defining it; and 2) to obtain detections for each camera by solving a\nglobal optimization problem that maximizes coherence of detections both in each\n2D image and in the 3D scene. This process yields tightly-fitted bounding boxes\nthat circumvent occlusions or miss-detections. Experimental results on five\npublicly available datasets show that the proposed approach outperforms\nstate-of-the-art multi-camera pedestrian detectors, even some specifically\ntrained on the target scenario, signifying the versatility and robustness of\nthe proposed method without requiring ad-hoc annotations nor human-guided\nconfiguration.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 17:36:37 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 09:49:31 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["L\u00f3pez-Cifuentes", "Alejandro", ""], ["Escudero-Vi\u00f1olo", "Marcos", ""], ["Besc\u00f3s", "Jes\u00fas", ""], ["Carballeira", "Pablo", ""]]}, {"id": "1812.10784", "submitter": "Vivek Sharma", "authors": "Congcong Wang, Vivek Sharma, Yu Fan, Faouzi Alaya Cheikh, Azeddine\n  Beghdadi, Ole Jacob Elle, and Rainer Stiefelhagen", "title": "Can Image Enhancement be Beneficial to Find Smoke Images in Laparoscopic\n  Surgery?", "comments": "In proceedings of IST, Color and Imaging Conference (CIC 26).\n  Congcong Wang and Vivek Sharma contributed equally to this work and listed in\n  alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Laparoscopic surgery has a limited field of view. Laser ablation in a\nlaproscopic surgery causes smoke, which inevitably influences the surgeon's\nvisibility. Therefore, it is of vital importance to remove the smoke, such that\na clear visualization is possible. In order to employ a desmoking technique,\none needs to know beforehand if the image contains smoke or not, to this date,\nthere exists no accurate method that could classify the smoke/non-smoke images\ncompletely. In this work, we propose a new enhancement method which enhances\nthe informative details in the RGB images for discrimination of smoke/non-smoke\nimages. Our proposed method utilizes weighted least squares optimization\nframework~(WLS). For feature extraction, we use statistical features based on\nbivariate histogram distribution of gradient magnitude~(GM) and Laplacian of\nGaussian~(LoG). We then train a SVM classifier with binary smoke/non-smoke\nclassification task. We demonstrate the effectiveness of our method on Cholec80\ndataset. Experiments using our proposed enhancement method show promising\nresults with improvements of 4\\% in accuracy and 4\\% in F1-Score over the\nbaseline performance of RGB images. In addition, our approach improves over the\nsaturation histogram based classification methodologies Saturation\nAnalysis~(SAN) and Saturation Peak Analysis~(SPA) by 1/5\\% and 1/6\\% in\naccuracy/F1-Score metrics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 18:07:05 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Wang", "Congcong", ""], ["Sharma", "Vivek", ""], ["Fan", "Yu", ""], ["Cheikh", "Faouzi Alaya", ""], ["Beghdadi", "Azeddine", ""], ["Elle", "Ole Jacob", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1812.10786", "submitter": "Samarth Bharadwaj", "authors": "Talha Siddiqui and Samarth Bharadwaj", "title": "Future semantic segmentation of time-lapsed videos with large temporal\n  displacement", "comments": "12 pages, 7 figures, https://bit.ly/2Bw7HGP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of video understanding is the ability to predict the\nevolution of its content in the future. This paper presents a future frame\nsemantic segmentation technique for predicting semantic masks of the current\nand future frames in a time-lapsed video. We specifically focus on time-lapsed\nvideos with large temporal displacement to highlight the model's ability to\ncapture large motions in time. We first introduce a unique semantic\nsegmentation prediction dataset with over 120,000 time-lapsed sky-video frames\nand all corresponding semantic masks captured over a span of five years in\nNorth America region. The dataset has immense practical value for cloud cover\nanalysis, which are treated as non-rigid objects of interest. %Here the model\nprovides both semantic segmentation of cloud region and solar irradiance\nemitted from a region from the sky-videos. Next, our proposed recurrent network\narchitecture departs from existing trend of using temporal convolutional\nnetworks (TCN) (or feed-forward networks), by explicitly learning an internal\nrepresentations for the evolution of video content with time. Experimental\nevaluation shows an improvement of mean IoU over TCNs in the segmentation task\nby 10.8% for 10 mins (21% over 60 mins) ahead of time predictions. Further, our\nmodel simultaneously measures both the current and future solar irradiance from\nthe same video frames with a normalized-MAE of 10.5% over two years. These\nresults indicate that recurrent memory networks with attention mechanism are\nable to capture complex advective and diffused flow characteristic of dense\nfluids even with sparse temporal sampling and are more suitable for future\nframe prediction tasks for longer duration videos.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 18:17:28 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Siddiqui", "Talha", ""], ["Bharadwaj", "Samarth", ""]]}, {"id": "1812.10788", "submitter": "Roozbeh Rajabi", "authors": "Sara Khoshsokhan, Roozbeh Rajabi, Hadi Zayyani", "title": "Hyperspectral Unmixing Based on Clustered Multitask Networks", "comments": "4 pages, ICSPIS 2018 Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral remote sensing is a prominent research topic in data\nprocessing. Most of the spectral unmixing algorithms are developed by adopting\nthe linear mixing models. Nonnegative matrix factorization (NMF) and its\ndevelopments are used widely for estimation of signatures and fractional\nabundances in the SU problem. Sparsity constraints was added to NMF, and was\nregularized by $ L_ {q} $ norm. In this paper, at first hyperspectral images\nare clustered by fuzzy c- means method, and then a new algorithm based on\nsparsity constrained distributed optimization is used for spectral unmixing. In\nthe proposed algorithm, a network including clusters is employed. Each pixel in\nthe hyperspectral images considered as a node in this network. The proposed\nalgorithm is optimized with diffusion LMS strategy, and then the update\nequations for fractional abundance and signature matrices are obtained.\nSimulation results based on defined performance metrics illustrate advantage of\nthe proposed algorithm in spectral unmixing of hyperspectral data compared with\nother methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 18:31:25 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Khoshsokhan", "Sara", ""], ["Rajabi", "Roozbeh", ""], ["Zayyani", "Hadi", ""]]}, {"id": "1812.10812", "submitter": "Husheng Zhou", "authors": "Husheng Zhou, Wei Li, Yuankun Zhu, Yuqun Zhang, Bei Yu, Lingming\n  Zhang, Cong Liu", "title": "DeepBillboard: Systematic Physical-World Testing of Autonomous Driving\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been widely applied in many autonomous\nsystems such as autonomous driving. Recently, DNN testing has been intensively\nstudied to automatically generate adversarial examples, which inject\nsmall-magnitude perturbations into inputs to test DNNs under extreme\nsituations. While existing testing techniques prove to be effective, they\nmostly focus on generating digital adversarial perturbations (particularly for\nautonomous driving), e.g., changing image pixels, which may never happen in\nphysical world. There is a critical missing piece in the literature on\nautonomous driving testing: understanding and exploiting both digital and\nphysical adversarial perturbation generation for impacting steering decisions.\nIn this paper, we present DeepBillboard, a systematic physical-world testing\napproach targeting at a common and practical driving scenario: drive-by\nbillboards. DeepBillboard is capable of generating a robust and resilient\nprintable adversarial billboard, which works under dynamic changing driving\nconditions including viewing angle, distance, and lighting. The objective is to\nmaximize the possibility, degree, and duration of the steering-angle errors of\nan autonomous vehicle driving by the generated adversarial billboard. We have\nextensively evaluated the efficacy and robustness of DeepBillboard through\nconducting both digital and physical-world experiments. Results show that\nDeepBillboard is effective for various steering models and scenes. Furthermore,\nDeepBillboard is sufficiently robust and resilient for generating\nphysical-world adversarial billboard tests for real-world driving under various\nweather conditions. To the best of our knowledge, this is the first study\ndemonstrating the possibility of generating realistic and continuous\nphysical-world tests for practical autonomous driving systems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 19:55:54 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Zhou", "Husheng", ""], ["Li", "Wei", ""], ["Zhu", "Yuankun", ""], ["Zhang", "Yuqun", ""], ["Yu", "Bei", ""], ["Zhang", "Lingming", ""], ["Liu", "Cong", ""]]}, {"id": "1812.10818", "submitter": "Marina Bendersky", "authors": "Marina Bendersky and Joy Wu and Tanveer Syeda-Mahmood", "title": "Classification of radiology reports by modality and anatomy: A\n  comparative study", "comments": "8 pages, 4 figures, BIBM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data labeling is currently a time-consuming task that often requires expert\nknowledge. In research settings, the availability of correctly labeled data is\ncrucial to ensure that model predictions are accurate and useful. We propose\nrelatively simple machine learning-based models that achieve high performance\nmetrics in the binary and multiclass classification of radiology reports. We\ncompare the performance of these algorithms to that of a data-driven approach\nbased on NLP, and find that the logistic regression classifier outperforms all\nother models, in both the binary and multiclass classification tasks. We then\nchoose the logistic regression binary classifier to predict chest X-ray (CXR)/\nnon-chest X-ray (non-CXR) labels in reports from different datasets, unseen\nduring any training phase of any of the models. Even in unseen report\ncollections, the binary logistic regression classifier achieves average\nprecision values of above 0.9. Based on the regression coefficient values, we\nalso identify frequent tokens in CXR and non-CXR reports that are features with\npossibly high predictive power.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 20:21:36 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Bendersky", "Marina", ""], ["Wu", "Joy", ""], ["Syeda-Mahmood", "Tanveer", ""]]}, {"id": "1812.10836", "submitter": "Qiqin Dai", "authors": "Qiqin Dai, Henry Chopp, Emeline Pouyet, Oliver Cossairt, Marc Walton,\n  Aggelos K. Katsaggelos", "title": "Adaptive Image Sampling using Deep Learning and its Application on X-Ray\n  Fluorescence Image Reconstruction", "comments": "journal preprint v3", "journal-ref": null, "doi": "10.1109/TMM.2019.2958760", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an adaptive image sampling algorithm based on Deep\nLearning (DL). The adaptive sampling mask generation network is jointly trained\nwith an image inpainting network. The sampling rate is controlled in the mask\ngeneration network, and a binarization strategy is investigated to make the\nsampling mask binary. Besides the image sampling and reconstruction\napplication, we show that the proposed adaptive sampling algorithm is able to\nspeed up raster scan processes such as the X-Ray fluorescence (XRF) image\nscanning process. Recently XRF laboratory-based systems have evolved to\nlightweight and portable instruments thanks to technological advancements in\nboth X-Ray generation and detection. However, the scanning time of an XRF image\nis usually long due to the long exposures requires (e.g., $100 \\mu s-1ms$ per\npoint). We propose an XRF image inpainting approach to address the issue of\nlong scanning time, thus speeding up the scanning process while still\nmaintaining the possibility to reconstruct a high quality XRF image. The\nproposed adaptive image sampling algorithm is applied to the RGB image of the\nscanning target to generate the sampling mask. The XRF scanner is then driven\naccording to the sampling mask to scan a subset of the total image pixels.\nFinally, we inpaint the scanned XRF image by fusing the RGB image to\nreconstruct the full scan XRF image. The experiments show that the proposed\nadaptive sampling algorithm is able to effectively sample the image and achieve\na better reconstruction accuracy than that of the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 21:53:14 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 18:03:31 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 23:53:17 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Dai", "Qiqin", ""], ["Chopp", "Henry", ""], ["Pouyet", "Emeline", ""], ["Cossairt", "Oliver", ""], ["Walton", "Marc", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1812.10859", "submitter": "Tiep H. Vu", "authors": "Tiep Huu Vu", "title": "Signal Classification under structure sparsity constraints", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Classification is a key direction of research in signal and image\nprocessing, computer vision and artificial intelligence. The goal is to come up\nwith algorithms that automatically analyze images and put them in predefined\ncategories. This dissertation focuses on the theory and application of sparse\nsignal processing and learning algorithms for image processing and computer\nvision, especially object classification problems. A key emphasis of this work\nis to formulate novel optimization problems for learning dictionary and\nstructured sparse representations. Tractable solutions are proposed\nsubsequently for the corresponding optimization problems.\n  An important goal of this dissertation is to demonstrate the wide\napplications of these algorithmic tools for real-world applications. To that\nend, we explored important problems in the areas of:\n  1. Medical imaging: histopathological images acquired from mammalian tissues,\nhuman breast tissues, and human brain tissues.\n  2. Low-frequency (UHF to L-band) ultra-wideband (UWB) synthetic aperture\nradar: detecting bombs and mines buried under rough surfaces.\n  3. General object classification: face, flowers, objects, dogs, indoor\nscenes, etc.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 01:20:43 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Vu", "Tiep Huu", ""]]}, {"id": "1812.10885", "submitter": "Longlong Jing", "authors": "Longlong Jing, Yucheng Chen, Yingli Tian", "title": "Coarse-to-fine Semantic Segmentation from Image-level Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network-based semantic segmentation generally requires\nlarge-scale cost extensive annotations for training to obtain better\nperformance. To avoid pixel-wise segmentation annotations which are needed for\nmost methods, recently some researchers attempted to use object-level labels\n(e.g. bounding boxes) or image-level labels (e.g. image categories). In this\npaper, we propose a novel recursive coarse-to-fine semantic segmentation\nframework based on only image-level category labels. For each image, an initial\ncoarse mask is first generated by a convolutional neural network-based\nunsupervised foreground segmentation model and then is enhanced by a graph\nmodel. The enhanced coarse mask is fed to a fully convolutional neural network\nto be recursively refined. Unlike existing image-level label-based semantic\nsegmentation methods which require to label all categories for images contain\nmultiple types of objects, our framework only needs one label for each image\nand can handle images contains multi-category objects. With only trained on\nImageNet, our framework achieves comparable performance on PASCAL VOC dataset\nas other image-level label-based state-of-the-arts of semantic segmentation.\nFurthermore, our framework can be easily extended to foreground object\nsegmentation task and achieves comparable performance with the state-of-the-art\nsupervised methods on the Internet Object dataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 04:04:06 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Jing", "Longlong", ""], ["Chen", "Yucheng", ""], ["Tian", "Yingli", ""]]}, {"id": "1812.10889", "submitter": "Sangwoo Mo", "authors": "Sangwoo Mo, Minsu Cho, Jinwoo Shin", "title": "InstaGAN: Instance-aware Image-to-Image Translation", "comments": "Accepted to ICLR 2019. High resolution images are available in\n  https://github.com/sangwoomo/instagan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation has gained considerable attention due\nto the recent impressive progress based on generative adversarial networks\n(GANs). However, previous methods often fail in challenging cases, in\nparticular, when an image has multiple target instances and a translation task\ninvolves significant changes in shape, e.g., translating pants to skirts in\nfashion images. To tackle the issues, we propose a novel method, coined\ninstance-aware GAN (InstaGAN), that incorporates the instance information\n(e.g., object segmentation masks) and improves multi-instance transfiguration.\nThe proposed method translates both an image and the corresponding set of\ninstance attributes while maintaining the permutation invariance property of\nthe instances. To this end, we introduce a context preserving loss that\nencourages the network to learn the identity function outside of target\ninstances. We also propose a sequential mini-batch inference/training technique\nthat handles multiple instances with a limited GPU memory and enhances the\nnetwork to generalize better for multiple instances. Our comparative evaluation\ndemonstrates the effectiveness of the proposed method on different image\ndatasets, in particular, in the aforementioned challenging cases. Code and\nresults are available in https://github.com/sangwoomo/instagan\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 04:30:47 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 09:29:21 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Mo", "Sangwoo", ""], ["Cho", "Minsu", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1812.10902", "submitter": "Matthew Hill", "authors": "Matthew Q. Hill (1), Connor J. Parde (1), Carlos D. Castillo (2), Y.\n  Ivette Colon (1), Rajeev Ranjan (2), Jun-Cheng Chen (2), Volker Blanz (3),\n  Alice J. O'Toole (1) ((1) The University of Texas at Dallas, (2) University\n  of Maryland, (3) University of Siegen)", "title": "Deep Convolutional Neural Networks in the Face of Caricature: Identity\n  and Image Revealed", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": "10.1038/s42256-019-0111-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world face recognition requires an ability to perceive the unique\nfeatures of an individual face across multiple, variable images. The primate\nvisual system solves the problem of image invariance using cascades of neurons\nthat convert images of faces into categorical representations of facial\nidentity. Deep convolutional neural networks (DCNNs) also create generalizable\nface representations, but with cascades of simulated neurons. DCNN\nrepresentations can be examined in a multidimensional \"face space\", with\nidentities and image parameters quantified via their projections onto the axes\nthat define the space. We examined the organization of viewpoint, illumination,\ngender, and identity in this space. We show that the network creates a highly\norganized, hierarchically nested, face similarity structure in which\ninformation about face identity and imaging characteristics coexist. Natural\nimage variation is accommodated in this hierarchy, with face identity nested\nunder gender, illumination nested under identity, and viewpoint nested under\nillumination. To examine identity, we caricatured faces and found that network\nidentification accuracy increased with caricature level, and--mimicking human\nperception--a caricatured distortion of a face \"resembled\" its veridical\ncounterpart. Caricatures improved performance by moving the identity away from\nother identities in the face space and minimizing the effects of illumination\nand viewpoint. Deep networks produce face representations that solve\nlong-standing computational problems in generalized face recognition. They also\nprovide a unitary theoretical framework for reconciling decades of behavioral\nand neural results that emphasized either the image or the object/face in\nrepresentations, without understanding how a neural code could seamlessly\naccommodate both.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 06:16:23 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Hill", "Matthew Q.", ""], ["Parde", "Connor J.", ""], ["Castillo", "Carlos D.", ""], ["Colon", "Y. Ivette", ""], ["Ranjan", "Rajeev", ""], ["Chen", "Jun-Cheng", ""], ["Blanz", "Volker", ""], ["O'Toole", "Alice J.", ""]]}, {"id": "1812.10907", "submitter": "Erik Nijkamp", "authors": "Tian Han, Erik Nijkamp, Xiaolin Fang, Mitch Hill, Song-Chun Zhu, Ying\n  Nian Wu", "title": "Divergence Triangle for Joint Training of Generator Model, Energy-based\n  Model, and Inference Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the divergence triangle as a framework for joint training\nof generator model, energy-based model and inference model. The divergence\ntriangle is a compact and symmetric (anti-symmetric) objective function that\nseamlessly integrates variational learning, adversarial learning, wake-sleep\nalgorithm, and contrastive divergence in a unified probabilistic formulation.\nThis unification makes the processes of sampling, inference, energy evaluation\nreadily available without the need for costly Markov chain Monte Carlo methods.\nOur experiments demonstrate that the divergence triangle is capable of learning\n(1) an energy-based model with well-formed energy landscape, (2) direct\nsampling in the form of a generator network, and (3) feed-forward inference\nthat faithfully reconstructs observed as well as synthesized data. The\ndivergence triangle is a robust training method that can learn from incomplete\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 06:35:39 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 09:35:03 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Han", "Tian", ""], ["Nijkamp", "Erik", ""], ["Fang", "Xiaolin", ""], ["Hill", "Mitch", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1812.10915", "submitter": "Vladimir Ivashkin", "authors": "Vladimir Ivashkin, Vadim Lebedev", "title": "Spatiotemporal Data Fusion for Precipitation Nowcasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precipitation nowcasting using neural networks and ground-based radars has\nbecome one of the key components of modern weather prediction services, but it\nis limited to the regions covered by ground-based radars. Truly global\nprecipitation nowcasting requires fusion of radar and satellite observations.\nWe propose the data fusion pipeline based on computer vision techniques,\nincluding novel inpainting algorithm with soft masking.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 07:51:08 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Ivashkin", "Vladimir", ""], ["Lebedev", "Vadim", ""]]}, {"id": "1812.10956", "submitter": "Yun Liu", "authors": "Yun Liu, Yu Qiu, Le Zhang, JiaWang Bian, Guang-Yu Nie, Ming-Ming Cheng", "title": "Salient Object Detection via High-to-Low Hierarchical Context\n  Aggregation", "comments": "We made a significant change and re-submitted as \"DNA:\n  Deeply-supervised Nonlinear Aggregation for Salient Object Detection\",\n  arXiv:1903.12476", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on salient object detection mainly aims at exploiting how to\neffectively integrate convolutional side-output features in convolutional\nneural networks (CNN). Based on this, most of the existing state-of-the-art\nsaliency detectors design complex network structures to fuse the side-output\nfeatures of the backbone feature extraction networks. However, should the\nfusion strategies be more and more complex for accurate salient object\ndetection? In this paper, we observe that the contexts of a natural image can\nbe well expressed by a high-to-low self-learning of side-output convolutional\nfeatures. As we know, the contexts of an image usually refer to the global\nstructures, and the top layers of CNN usually learn to convey global\ninformation. On the other hand, it is difficult for the intermediate\nside-output features to express contextual information. Here, we design an\nhourglass network with intermediate supervision to learn contextual features in\na high-to-low manner. The learned hierarchical contexts are aggregated to\ngenerate the hybrid contextual expression for an input image. At last, the\nhybrid contextual features can be used for accurate saliency estimation. We\nextensively evaluate our method on six challenging saliency datasets, and our\nsimple method achieves state-of-the-art performance under various evaluation\nmetrics. Code will be released upon paper acceptance.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 11:34:12 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 01:29:52 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Liu", "Yun", ""], ["Qiu", "Yu", ""], ["Zhang", "Le", ""], ["Bian", "JiaWang", ""], ["Nie", "Guang-Yu", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "1812.10967", "submitter": "Yuance Liu", "authors": "Yuance Liu and Michael Z. Q. Chen", "title": "TROVE Feature Detection for Online Pose Recovery by Binocular Cameras", "comments": "18 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new and efficient method to estimate 6-DoF ego-states:\nattitudes and positions in real time. The proposed method extract information\nof ego-states by observing a feature called \"TROVE\" (Three Rays and One\nVErtex). TROVE features are projected from structures that are ubiquitous on\nman-made constructions and objects. The proposed method does not search for\nconventional corner-type features nor use Perspective-n-Point (PnP) methods,\nand it achieves a real-time estimation of attitudes and positions up to 60 Hz.\nThe accuracy of attitude estimates can reach 0.3 degrees and that of position\nestimates can reach 2 cm in an indoor environment. The result shows a promising\napproach for unmanned robots to localize in an environment that is rich in\nman-made structures.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 12:07:37 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Liu", "Yuance", ""], ["Chen", "Michael Z. Q.", ""]]}, {"id": "1812.10968", "submitter": "Anis Koubaa", "authors": "Bilel Benjdira, Taha Khursheed, Anis Koubaa, Adel Ammar, Kais Ouni", "title": "Car Detection using Unmanned Aerial Vehicles: Comparison between Faster\n  R-CNN and YOLOv3", "comments": "This paper is accepted in The 1st Unmanned Vehicle Systems conference\n  in Oman, Feb 2019", "journal-ref": "The 1st Unmanned Vehicle Systems conference in Oman, Feb 2019", "doi": null, "report-no": "RIOTU-01", "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmanned Aerial Vehicles are increasingly being used in surveillance and\ntraffic monitoring thanks to their high mobility and ability to cover areas at\ndifferent altitudes and locations. One of the major challenges is to use aerial\nimages to accurately detect cars and count them in real-time for traffic\nmonitoring purposes. Several deep learning techniques were recently proposed\nbased on convolution neural network (CNN) for real-time classification and\nrecognition in computer vision. However, their performance depends on the\nscenarios where they are used. In this paper, we investigate the performance of\ntwo state-of-the-art CNN algorithms, namely Faster R-CNN and YOLOv3, in the\ncontext of car detection from aerial images. We trained and tested these two\nmodels on a large car dataset taken from UAVs. We demonstrated in this paper\nthat YOLOv3 outperforms Faster R-CNN in sensitivity and processing time,\nalthough they are comparable in the precision metric.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 12:08:55 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Benjdira", "Bilel", ""], ["Khursheed", "Taha", ""], ["Koubaa", "Anis", ""], ["Ammar", "Adel", ""], ["Ouni", "Kais", ""]]}, {"id": "1812.10972", "submitter": "Michael Janner", "authors": "Michael Janner, Sergey Levine, William T. Freeman, Joshua B.\n  Tenenbaum, Chelsea Finn, Jiajun Wu", "title": "Reasoning About Physical Interactions with Object-Oriented Prediction\n  and Planning", "comments": "ICLR 2019, project page:\n  https://people.eecs.berkeley.edu/~janner/o2p2/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-based factorizations provide a useful level of abstraction for\ninteracting with the world. Building explicit object representations, however,\noften requires supervisory signals that are difficult to obtain in practice. We\npresent a paradigm for learning object-centric representations for physical\nscene understanding without direct supervision of object properties. Our model,\nObject-Oriented Prediction and Planning (O2P2), jointly learns a perception\nfunction to map from image observations to object representations, a pairwise\nphysics interaction function to predict the time evolution of a collection of\nobjects, and a rendering function to map objects back to pixels. For\nevaluation, we consider not only the accuracy of the physical predictions of\nthe model, but also its utility for downstream tasks that require an actionable\nrepresentation of intuitive physics. After training our model on an image\nprediction task, we can use its learned representations to build block towers\nmore complicated than those observed during training.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 12:18:23 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 08:27:03 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Janner", "Michael", ""], ["Levine", "Sergey", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Finn", "Chelsea", ""], ["Wu", "Jiajun", ""]]}, {"id": "1812.10998", "submitter": "Preeti Gopal Ms.", "authors": "Preeti Gopal and Sharat Chandran and Imants Svalbe and Ajit Rajwade", "title": "Learning from past scans: Tomographic reconstruction to detect new\n  structures", "comments": "5 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for tomographic reconstruction from sparse measurements arises when\nthe measurement process is potentially harmful, needs to be rapid, or is\nuneconomical. In such cases, prior information from previous longitudinal scans\nof the same or similar objects helps to reconstruct the current object whilst\nrequiring significantly fewer `updating' measurements. However, a significant\nlimitation of all prior-based methods is the possible dominance of the prior\nover the reconstruction of new localised information that has evolved within\nthe test object. In this paper, we improve the state of the art by (1)\ndetecting potential regions where new changes may have occurred, and (2)\neffectively reconstructing both the old and new structures by computing\nregional weights that moderate the local influence of the priors. We have\ntested the efficacy of our method on synthetic as well as real volume data. The\nresults demonstrate that using weighted priors significantly improves the\noverall quality of the reconstructed data whilst minimising their impact on\nregions that contain new information.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 09:45:15 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Gopal", "Preeti", ""], ["Chandran", "Sharat", ""], ["Svalbe", "Imants", ""], ["Rajwade", "Ajit", ""]]}, {"id": "1812.11004", "submitter": "Jingkuan Song Dr.", "authors": "Jingkuan Song, Xiangpeng Li, Lianli Gao, Heng Tao Shen", "title": "Hierarchical LSTMs with Adaptive Attention for Visual Captioning", "comments": "arXiv admin note: substantial text overlap with arXiv:1706.01231", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress has been made in using attention based encoder-decoder\nframework for image and video captioning. Most existing decoders apply the\nattention mechanism to every generated word including both visual words (e.g.,\n\"gun\" and \"shooting\") and non-visual words (e.g. \"the\", \"a\"). However, these\nnon-visual words can be easily predicted using natural language model without\nconsidering visual signals or attention. Imposing attention mechanism on\nnon-visual words could mislead and decrease the overall performance of visual\ncaptioning. Furthermore, the hierarchy of LSTMs enables more complex\nrepresentation of visual data, capturing information at different scales. To\naddress these issues, we propose a hierarchical LSTM with adaptive attention\n(hLSTMat) approach for image and video captioning. Specifically, the proposed\nframework utilizes the spatial or temporal attention for selecting specific\nregions or frames to predict the related words, while the adaptive attention is\nfor deciding whether to depend on the visual information or the language\ncontext information. Also, a hierarchical LSTMs is designed to simultaneously\nconsider both low-level visual information and high-level language context\ninformation to support the caption generation. We initially design our hLSTMat\nfor video captioning task. Then, we further refine it and apply it to image\ncaptioning task. To demonstrate the effectiveness of our proposed framework, we\ntest our method on both video and image captioning tasks. Experimental results\nshow that our approach achieves the state-of-the-art performance for most of\nthe evaluation metrics on both tasks. The effect of important components is\nalso well exploited in the ablation study.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 09:51:58 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Song", "Jingkuan", ""], ["Li", "Xiangpeng", ""], ["Gao", "Lianli", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1812.11006", "submitter": "Moran Rubin", "authors": "Moran Rubin, Omer Stein, Nir A. Turko, Yoav Nygate, Darina Roitshtain,\n  Lidor Karako, Itay Barnea, Raja Giryes, and Natan T. Shaked", "title": "TOP-GAN: Label-Free Cancer Cell Classification Using Deep Learning with\n  a Small Training Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep learning approach for medical imaging that copes with\nthe problem of a small training set, the main bottleneck of deep learning, and\napply it for classification of healthy and cancer cells acquired by\nquantitative phase imaging. The proposed method, called transferring of\npre-trained generative adversarial network (TOP-GAN), is a hybridization\nbetween transfer learning and generative adversarial networks (GANs). Healthy\ncells and cancer cells of different metastatic potential have been imaged by\nlow-coherence off-axis holography. After the acquisition, the optical path\ndelay maps of the cells have been extracted and directly used as an input to\nthe deep networks. In order to cope with the small number of classified images,\nwe have used GANs to train a large number of unclassified images from another\ncell type (sperm cells). After this preliminary training, and after\ntransforming the last layer of the network with new ones, we have designed an\nautomatic classifier for the correct cell type (healthy/primary\ncancer/metastatic cancer) with 90-99% accuracy, although small training sets of\ndown to several images have been used. These results are better in comparison\nto other classic methods that aim at coping with the same problem of a small\ntraining set. We believe that our approach makes the combination of holographic\nmicroscopy and deep learning networks more accessible to the medical field by\nenabling a rapid, automatic and accurate classification in stain-free imaging\nflow cytometry. Furthermore, our approach is expected to be applicable to many\nother medical image classification tasks, suffering from a small training set.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 17:02:58 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Rubin", "Moran", ""], ["Stein", "Omer", ""], ["Turko", "Nir A.", ""], ["Nygate", "Yoav", ""], ["Roitshtain", "Darina", ""], ["Karako", "Lidor", ""], ["Barnea", "Itay", ""], ["Giryes", "Raja", ""], ["Shaked", "Natan T.", ""]]}, {"id": "1812.11017", "submitter": "Hang Zhou", "authors": "Hang Zhou, Kejiang Chen, Weiming Zhang, Han Fang, Wenbo Zhou and\n  Nenghai Yu", "title": "DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds\n  Defense", "comments": "Published in IEEE ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to adversarial examples, which poses a threat\nto their application in security sensitive systems. We propose a Denoiser and\nUPsampler Network (DUP-Net) structure as defenses for 3D adversarial point\ncloud classification, where the two modules reconstruct surface smoothness by\ndropping or adding points. In this paper, statistical outlier removal (SOR) and\na data-driven upsampling network are considered as denoiser and upsampler\nrespectively. Compared with baseline defenses, DUP-Net has three advantages.\nFirst, with DUP-Net as a defense, the target model is more robust to white-box\nadversarial attacks. Second, the statistical outlier removal provides added\nrobustness since it is a non-differentiable denoising operation. Third, the\nupsampler network can be trained on a small dataset and defends well against\nadversarial attacks generated from other point cloud datasets. We conduct\nvarious experiments to validate that DUP-Net is very effective as defense in\npractice. Our best defense eliminates 83.8% of C&W and l_2 loss based attack\n(point shifting), 50.0% of C&W and Hausdorff distance loss based attack (point\nadding) and 9.0% of saliency map based attack (point dropping) under 200\ndropped points on PointNet.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 00:38:20 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 15:12:54 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhou", "Hang", ""], ["Chen", "Kejiang", ""], ["Zhang", "Weiming", ""], ["Fang", "Han", ""], ["Zhou", "Wenbo", ""], ["Yu", "Nenghai", ""]]}, {"id": "1812.11042", "submitter": "Aditya Dendukuri", "authors": "Aditya Dendukuri and Khoa Luu", "title": "Image Processing in Quantum Computers", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum Image Processing (QIP)is an exciting new field showing a lot of\npromise as a powerful addition to the arsenal of Image Processing techniques.\nRepresenting image pixel by pixel using classical information requires an\nenormous amount of computational resources. Hence, exploring methods to\nrepresent images in a different paradigm of information is important. In this\nwork, we study the representation of images in Quantum Information. The main\nmotivation for this pursuit is the ability of storing N bits of classical\ninformation in only log(2N) quantum bits (qubits). The promising first step was\nthe exponentially efficient implementation of the Fourier transform in quantum\ncomputers as compared to Fast Fourier Transform in classical computers. In\naddition, images encoded in quantum information could obey unique quantum\nproperties like superposition or entanglement.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 15:29:06 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 23:04:47 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 19:03:12 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Dendukuri", "Aditya", ""], ["Luu", "Khoa", ""]]}, {"id": "1812.11092", "submitter": "Bas Peters", "authors": "Bas Peters, Justin Granek, Eldad Haber", "title": "Multi-resolution neural networks for tracking seismic horizons from few\n  training images", "comments": "24 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting a specific horizon in seismic images is a valuable tool for\ngeological interpretation. Because hand-picking the locations of the horizon is\na time-consuming process, automated computational methods were developed\nstarting three decades ago. Older techniques for such picking include\ninterpolation of control points however, in recent years neural networks have\nbeen used for this task. Until now, most networks trained on small patches from\nlarger images. This limits the networks ability to learn from large-scale\ngeologic structures. Moreover, currently available networks and training\nstrategies require label patches that have full and continuous annotations,\nwhich are also time-consuming to generate.\n  We propose a projected loss-function for training convolutional networks with\na multi-resolution structure, including variants of the U-net. Our networks\nlearn from a small number of large seismic images without creating patches. The\nprojected loss-function enables training on labels with just a few annotated\npixels and has no issue with the other unknown label pixels. Training uses all\ndata without reserving some for validation. Only the labels are split into\ntraining/testing. Contrary to other work on horizon tracking, we train the\nnetwork to perform non-linear regression, and not classification. As such, we\npropose labels as the convolution of a Gaussian kernel and the known horizon\nlocations that indicate uncertainty in the labels. The network output is the\nprobability of the horizon location. We demonstrate the proposed computational\ningredients on two different datasets, for horizon extrapolation and\ninterpolation. We show that the predictions of our methodology are accurate\neven in areas far from known horizon locations because our learning strategy\nexploits all data in large seismic images.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 07:35:24 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Peters", "Bas", ""], ["Granek", "Justin", ""], ["Haber", "Eldad", ""]]}, {"id": "1812.11139", "submitter": "Chris Thomas", "authors": "Christopher Thomas and Adriana Kovashka", "title": "Artistic Object Recognition by Unsupervised Style Adaptation", "comments": null, "journal-ref": "Asian Conference on Computer Vision 2018 (ACCV)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision systems currently lack the ability to reliably recognize\nartistically rendered objects, especially when such data is limited. In this\npaper, we propose a method for recognizing objects in artistic modalities (such\nas paintings, cartoons, or sketches), without requiring any labeled data from\nthose modalities. Our method explicitly accounts for stylistic domain shifts\nbetween and within domains. To do so, we introduce a complementary training\nmodality constructed to be similar in artistic style to the target domain, and\nenforce that the network learns features that are invariant between the two\ntraining modalities. We show how such artificial labeled source domains can be\ngenerated automatically through the use of style transfer techniques, using\ndiverse target images to represent the style in the target domain. Unlike\nexisting methods which require a large amount of unlabeled target data, our\nmethod can work with as few as ten unlabeled images. We evaluate it on a number\nof cross-domain object and scene classification tasks and on a new dataset we\nrelease. Our experiments show that our approach, though conceptually simple,\nsignificantly improves the accuracy that existing domain adaptation techniques\nobtain for artistic object recognition.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 18:08:23 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Thomas", "Christopher", ""], ["Kovashka", "Adriana", ""]]}, {"id": "1812.11163", "submitter": "Md Abu Layek", "authors": "Md Abu Layek, Sanjida Afroz, TaeChoong Chung and Eui-Nam Huh", "title": "Center Emphasized Visual Saliency and a Contrast-based Full Reference\n  Image Quality Index", "comments": "This work was supported by Institute for Information & communications\n  Technology Promotion(IITP) grant funded by the Korea government(MSIT)\n  (No.2017-0-00294, Service mobility support distributed cloud technology)", "journal-ref": "Symmetry 2019, 11, 296. Online:\n  https://www.mdpi.com/2073-8994/11/3/296", "doi": "10.3390/sym11030296", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective image quality assessment (IQA) is imperative in the current\nmultimedia-intensive world, in order to assess the visual quality of an image\nat close to a human level of ability. Many~parameters such as color intensity,\nstructure, sharpness, contrast, presence of an object, etc., draw human\nattention to an image. Psychological vision research suggests that human vision\nis biased to the center area of an image and display screen. As a result, if\nthe center part contains any visually salient information, it draws human\nattention even more and any distortion in that part will be better perceived\nthan other parts. To the best of our knowledge, previous IQA methods have not\nconsidered this fact. In this paper, we propose a full reference image quality\nassessment (FR-IQA) approach using visual saliency and contrast; however, we\ngive extra attention to the center by increasing the sensitivity of the\nsimilarity maps in that region. We evaluated our method on three large-scale\npopular benchmark databases used by most of the current IQA researchers\n(TID2008, CSIQ~and LIVE), having a total of 3345 distorted images with\n28~different kinds of distortions. Our~method is compared with 13\nstate-of-the-art approaches. This comparison reveals the stronger correlation\nof our method with human-evaluated values. The prediction-of-quality score is\nconsistent for distortion specific as well as distortion independent cases.\nMoreover, faster processing makes it applicable to any real-time application.\nThe MATLAB code is publicly available to test the algorithm and can be found\nonline at http://layek.khu.ac.kr/CEQI.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 18:50:40 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 12:38:52 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 09:24:50 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Layek", "Md Abu", ""], ["Afroz", "Sanjida", ""], ["Chung", "TaeChoong", ""], ["Huh", "Eui-Nam", ""]]}, {"id": "1812.11166", "submitter": "Xiuming Zhang", "authors": "Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Joshua B. Tenenbaum,\n  William T. Freeman, Jiajun Wu", "title": "Learning to Reconstruct Shapes from Unseen Classes", "comments": "NeurIPS 2018 (Oral). The first two authors contributed equally to\n  this paper. Project page: http://genre.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a single image, humans are able to perceive the full 3D shape of an\nobject by exploiting learned shape priors from everyday life. Contemporary\nsingle-image 3D reconstruction algorithms aim to solve this task in a similar\nfashion, but often end up with priors that are highly biased by training\nclasses. Here we present an algorithm, Generalizable Reconstruction (GenRe),\ndesigned to capture more generic, class-agnostic shape priors. We achieve this\nwith an inference network and training procedure that combine 2.5D\nrepresentations of visible surfaces (depth and silhouette), spherical shape\nrepresentations of both visible and non-visible surfaces, and 3D voxel-based\nrepresentations, in a principled manner that exploits the causal structure of\nhow 3D shapes give rise to 2D images. Experiments demonstrate that GenRe\nperforms well on single-view shape reconstruction, and generalizes to diverse\nnovel objects from categories not seen during training.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 18:52:50 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Zhang", "Xiuming", ""], ["Zhang", "Zhoutong", ""], ["Zhang", "Chengkai", ""], ["Tenenbaum", "Joshua B.", ""], ["Freeman", "William T.", ""], ["Wu", "Jiajun", ""]]}, {"id": "1812.11204", "submitter": "Siqi Liu", "authors": "Jie Yang, Siqi Liu, Sasa Grbic, Arnaud Arindra Adiyoso Setio, Zhoubing\n  Xu, Eli Gibson, Guillaume Chabin, Bogdan Georgescu, Andrew F. Laine, Dorin\n  Comaniciu", "title": "Class-Aware Adversarial Lung Nodule Synthesis in CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though large-scale datasets are essential for training deep learning systems,\nit is expensive to scale up the collection of medical imaging datasets.\nSynthesizing the objects of interests, such as lung nodules, in medical images\nbased on the distribution of annotated datasets can be helpful for improving\nthe supervised learning tasks, especially when the datasets are limited by size\nand class balance. In this paper, we propose the class-aware adversarial\nsynthesis framework to synthesize lung nodules in CT images. The framework is\nbuilt with a coarse-to-fine patch in-painter (generator) and two class-aware\ndiscriminators. By conditioning on the random latent variables and the target\nnodule labels, the trained networks are able to generate diverse nodules given\nthe same context. By evaluating on the public LIDC-IDRI dataset, we demonstrate\nan example application of the proposed framework for improving the accuracy of\nthe lung nodule malignancy estimation as a binary classification problem, which\nis important in the lung screening scenario. We show that combining the real\nimage patches and the synthetic lung nodules in the training set can improve\nthe mean AUC classification score across different network architectures by 2%.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 19:33:38 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Yang", "Jie", ""], ["Liu", "Siqi", ""], ["Grbic", "Sasa", ""], ["Setio", "Arnaud Arindra Adiyoso", ""], ["Xu", "Zhoubing", ""], ["Gibson", "Eli", ""], ["Chabin", "Guillaume", ""], ["Georgescu", "Bogdan", ""], ["Laine", "Andrew F.", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "1812.11207", "submitter": "Joan Duran", "authors": "Antoni Buades and Joan Duran", "title": "CFA Bayer image sequence denoising and demosaicking chain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demosaicking provokes the spatial and color correlation of noise, which\nis afterwards enhanced by the imaging pipeline. The correct removal previous or\nsimultaneously with the demosaicking process is not usually considered in the\nliterature. We present a novel imaging chain including a denoising of the Bayer\nCFA and a demosaicking method for image sequences. The proposed algorithm uses\na spatio-temporal patch method for the noise removal and demosaicking of the\nCFA. The experimentation, including real examples, illustrates the superior\nperformance of the proposed chain, avoiding the creation of artifacts and\ncolored spots in the final image.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 19:51:39 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Buades", "Antoni", ""], ["Duran", "Joan", ""]]}, {"id": "1812.11209", "submitter": "Valentin Radu", "authors": "Adrian Cosma, Ion Emilian Radoi, Valentin Radu", "title": "CamLoc: Pedestrian Location Detection from Pose Estimation on\n  Resource-constrained Smart-cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in energy-efficient hardware technology is driving the\nexponential growth we are experiencing in the Internet of Things (IoT) space,\nwith more pervasive computations being performed near to data generation\nsources. A range of intelligent devices and applications performing local\ndetection is emerging (activity recognition, fitness monitoring, etc.) bringing\nwith them obvious advantages such as reducing detection latency for improved\ninteraction with devices and safeguarding user data by not leaving the device.\nVideo processing holds utility for many emerging applications and data\nlabelling in the IoT space. However, performing this video processing with deep\nneural networks at the edge of the Internet is not trivial. In this paper we\nshow that pedestrian location estimation using deep neural networks is\nachievable on fixed cameras with limited compute resources. Our approach uses\npose estimation from key body points detection to extend pedestrian skeleton\nwhen whole body not in image (occluded by obstacles or partially outside of\nframe), which achieves better location estimation performance (infrence time\nand memory footprint) compared to fitting a bounding box over pedestrian and\nscaling. We collect a sizable dataset comprising of over 2100 frames in videos\nfrom one and two surveillance cameras pointing from different angles at the\nscene, and annotate each frame with the exact position of person in image, in\n42 different scenarios of activity and occlusion. We compare our pose\nestimation based location detection with a popular detection algorithm, YOLOv2,\nfor overlapping bounding-box generation, our solution achieving faster\ninference time (15x speedup) at half the memory footprint, within resource\ncapabilities on embedded devices, which demonstrate that CamLoc is an efficient\nsolution for location estimation in videos on smart-cameras.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 19:57:48 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Cosma", "Adrian", ""], ["Radoi", "Ion Emilian", ""], ["Radu", "Valentin", ""]]}, {"id": "1812.11214", "submitter": "Eugene Belilovsky", "authors": "Mathieu Andreux, Tom\\'as Angles, Georgios Exarchakis, Roberto\n  Leonarduzzi, Gaspar Rochette, Louis Thiry, John Zarka, St\\'ephane Mallat,\n  Joakim And\\'en, Eugene Belilovsky, Joan Bruna, Vincent Lostanlen, Matthew J.\n  Hirn, Edouard Oyallon, Sixin Zhang, Carmine Cella, Michael Eickenberg", "title": "Kymatio: Scattering Transforms in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The wavelet scattering transform is an invariant signal representation\nsuitable for many signal processing and machine learning applications. We\npresent the Kymatio software package, an easy-to-use, high-performance Python\nimplementation of the scattering transform in 1D, 2D, and 3D that is compatible\nwith modern deep learning frameworks. All transforms may be executed on a GPU\n(in addition to CPU), offering a considerable speed up over CPU\nimplementations. The package also has a small memory footprint, resulting\ninefficient memory usage. The source code, documentation, and examples are\navailable undera BSD license at https://www.kymat.io/\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 20:53:29 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 06:00:28 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Andreux", "Mathieu", ""], ["Angles", "Tom\u00e1s", ""], ["Exarchakis", "Georgios", ""], ["Leonarduzzi", "Roberto", ""], ["Rochette", "Gaspar", ""], ["Thiry", "Louis", ""], ["Zarka", "John", ""], ["Mallat", "St\u00e9phane", ""], ["And\u00e9n", "Joakim", ""], ["Belilovsky", "Eugene", ""], ["Bruna", "Joan", ""], ["Lostanlen", "Vincent", ""], ["Hirn", "Matthew J.", ""], ["Oyallon", "Edouard", ""], ["Zhang", "Sixin", ""], ["Cella", "Carmine", ""], ["Eickenberg", "Michael", ""]]}, {"id": "1812.11284", "submitter": "Zhongang Cai", "authors": "Zhongang Cai, Cunjun Yu, Quang-Cuong Pham", "title": "3D Convolution on RGB-D Point Clouds for Accurate Model-free Object Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional pose estimation of a 3D object usually requires the\nknowledge of the 3D model of the object. Even with the recent development in\nconvolutional neural networks (CNNs), a 3D model is often necessary in the\nfinal estimation. In this paper, we propose a two-stage pipeline that takes in\nraw colored point cloud data and estimates an object's translation and rotation\nby running 3D convolutions on voxels. The pipeline is simple yet highly\naccurate: translation error is reduced to the voxel resolution (around 1 cm)\nand rotation error is around 5 degrees. The pipeline is also put to actual\nrobotic grasping tests where it achieves above 90% success rate for test\nobjects. Another innovation is that a motion capture system is used to\nautomatically label the point cloud samples which makes it possible to rapidly\ncollect a large amount of highly accurate real data for training the neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 04:46:51 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Cai", "Zhongang", ""], ["Yu", "Cunjun", ""], ["Pham", "Quang-Cuong", ""]]}, {"id": "1812.11295", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni, Dahua Lin, Ji Liu, Kostas Daniilidis, Jianbo Shi", "title": "Monocular 3D Pose Recovery via Nonconvex Sparsity with Theoretical\n  Analysis", "comments": "Partially overlap with arXiv:1711.02857, which targeted different\n  applications and will not be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For recovering 3D object poses from 2D images, a prevalent method is to\npre-train an over-complete dictionary $\\mathcal D=\\{B_i\\}_i^D$ of 3D basis\nposes. During testing, the detected 2D pose $Y$ is matched to dictionary by $Y\n\\approx \\sum_i M_i B_i$ where $\\{M_i\\}_i^D=\\{c_i \\Pi R_i\\}$, by estimating the\nrotation $R_i$, projection $\\Pi$ and sparse combination coefficients $c \\in\n\\mathbb R_{+}^D$. In this paper, we propose non-convex regularization $H(c)$ to\nlearn coefficients $c$, including novel leaky capped $\\ell_1$-norm\nregularization (LCNR), \\begin{align*} H(c)=\\alpha \\sum_{i } \\min(|c_i|,\\tau)+\n\\beta \\sum_{i } \\max(| c_i|,\\tau), \\end{align*} where $0\\leq \\beta \\leq \\alpha$\nand $0<\\tau$ is a certain threshold, so the invalid components smaller than\n$\\tau$ are composed with larger regularization and other valid components with\nsmaller regularization. We propose a multi-stage optimizer with convex\nrelaxation and ADMM. We prove that the estimation error $\\mathcal L(l)$ decays\nw.r.t. the stages $l$, \\begin{align*} Pr\\left(\\mathcal L(l) < \\rho^{l-1}\n\\mathcal L(0) + \\delta \\right) \\geq 1- \\epsilon, \\end{align*} where $0< \\rho\n<1, 0<\\delta, 0<\\epsilon \\ll 1$. Experiments on large 3D human datasets like\nH36M are conducted to support our improvement upon previous approaches. To the\nbest of our knowledge, this is the first theoretical analysis in this line of\nresearch, to understand how the recovery error is affected by fundamental\nfactors, e.g. dictionary size, observation noises, optimization times. We\ncharacterize the trade-off between speed and accuracy towards real-time\ninference in applications.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 06:23:11 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Wangni", "Jianqiao", ""], ["Lin", "Dahua", ""], ["Liu", "Ji", ""], ["Daniilidis", "Kostas", ""], ["Shi", "Jianbo", ""]]}, {"id": "1812.11302", "submitter": "Yash Bhalgat", "authors": "Yash Bhalgat, Meet Shah, Suyash Awate", "title": "Annotation-cost Minimization for Medical Image Segmentation using\n  Suggestive Mixed Supervision Fully Convolutional Networks", "comments": "Medical Imaging meets NeurIPS 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For medical image segmentation, most fully convolutional networks (FCNs) need\nstrong supervision through a large sample of high-quality dense segmentations,\nwhich is taxing in terms of costs, time and logistics involved. This burden of\nannotation can be alleviated by exploiting weak inexpensive annotations such as\nbounding boxes and anatomical landmarks. However, it is very difficult to\n\\textit{a priori} estimate the optimal balance between the number of\nannotations needed for each supervision type that leads to maximum performance\nwith the least annotation cost. To optimize this cost-performance trade off, we\npresent a budget-based cost-minimization framework in a mixed-supervision\nsetting via dense segmentations, bounding boxes, and landmarks. We propose a\nlinear programming (LP) formulation combined with uncertainty and similarity\nbased ranking strategy to judiciously select samples to be annotated next for\noptimal performance. In the results section, we show that our proposed method\nachieves comparable performance to state-of-the-art approaches with\nsignificantly reduced cost of annotations.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 07:22:57 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Bhalgat", "Yash", ""], ["Shah", "Meet", ""], ["Awate", "Suyash", ""]]}, {"id": "1812.11307", "submitter": "Xuechen Li", "authors": "Xuechen Li, Yinlong Liu, Yiru Wang, Chen Wang, Manning Wang, Zhijian\n  Song", "title": "Fast and Globally Optimal Rigid Registration of 3D Point Sets by\n  Transformation Decomposition", "comments": "17pages, 16 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rigid registration of two 3D point sets is a fundamental problem in\ncomputer vision. The current trend is to solve this problem globally using the\nBnB optimization framework. However, the existing global methods are slow for\ntwo main reasons: the computational complexity of BnB is exponential to the\nproblem dimensionality (which is six for 3D rigid registration), and the bound\nevaluation used in BnB is inefficient. In this paper, we propose two techniques\nto address these problems. First, we introduce the idea of translation\ninvariant vectors, which allows us to decompose the search of a 6D rigid\ntransformation into a search of 3D rotation followed by a search of 3D\ntranslation, each of which is solved by a separate BnB algorithm. This\ntransformation decomposition reduces the problem dimensionality of BnB\nalgorithms and substantially improves its efficiency. Then, we propose a new\ndata structure, named 3D Integral Volume, to accelerate the bound evaluation in\nboth BnB algorithms. By combining these two techniques, we implement an\nefficient algorithm for rigid registration of 3D point sets. Extensive\nexperiments on both synthetic and real data show that the proposed algorithm is\nthree orders of magnitude faster than the existing state-of-the-art global\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 07:46:48 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 11:21:46 GMT"}, {"version": "v3", "created": "Sat, 5 Jan 2019 08:50:55 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Li", "Xuechen", ""], ["Liu", "Yinlong", ""], ["Wang", "Yiru", ""], ["Wang", "Chen", ""], ["Wang", "Manning", ""], ["Song", "Zhijian", ""]]}, {"id": "1812.11317", "submitter": "Xiaobo Wang", "authors": "Xiaobo Wang, Shuo Wang, Shifeng Zhang, Tianyu Fu, Hailin Shi, Tao Mei", "title": "Support Vector Guided Softmax Loss for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has witnessed significant progresses due to the advances of\ndeep convolutional neural networks (CNNs), the central challenge of which, is\nfeature discrimination. To address it, one group tries to exploit mining-based\nstrategies (\\textit{e.g.}, hard example mining and focal loss) to focus on the\ninformative examples. The other group devotes to designing margin-based loss\nfunctions (\\textit{e.g.}, angular, additive and additive angular margins) to\nincrease the feature margin from the perspective of ground truth class. Both of\nthem have been well-verified to learn discriminative features. However, they\nsuffer from either the ambiguity of hard examples or the lack of discriminative\npower of other classes. In this paper, we design a novel loss function, namely\nsupport vector guided softmax loss (SV-Softmax), which adaptively emphasizes\nthe mis-classified points (support vectors) to guide the discriminative\nfeatures learning. So the developed SV-Softmax loss is able to eliminate the\nambiguity of hard examples as well as absorb the discriminative power of other\nclasses, and thus results in more discrimiantive features. To the best of our\nknowledge, this is the first attempt to inherit the advantages of mining-based\nand margin-based losses into one framework. Experimental results on several\nbenchmarks have demonstrated the effectiveness of our approach over\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 09:16:21 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Wang", "Xiaobo", ""], ["Wang", "Shuo", ""], ["Zhang", "Shifeng", ""], ["Fu", "Tianyu", ""], ["Shi", "Hailin", ""], ["Mei", "Tao", ""]]}, {"id": "1812.11319", "submitter": "Ajay Kumar", "authors": "Yang Liu, Ajay Kumar", "title": "A Deep Learning based Framework to Detect and Recognize Humans using\n  Contactless Palmprints in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report No. COMP-K-24, March 2018", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contactless and online palmprint identfication offers improved user\nconvenience, hygiene, user-security and is highly desirable in a range of\napplications. This technical report details an accurate and generalizable deep\nlearning-based framework to detect and recognize humans using contactless\npalmprint images in the wild. Our network is based on fully convolutional\nnetwork that generates deeply learned residual features. We design a\nsoft-shifted triplet loss function to more effectively learn discriminative\npalmprint features. Online palmprint identification also requires a contactless\npalm detector, which is adapted and trained from faster-R-CNN architecture, to\ndetect palmprint region under varying backgrounds. Our reproducible\nexperimental results on publicly available contactless palmprint databases\nsuggest that the proposed framework consistently outperforms several classical\nand state-of-the-art palmprint recognition methods. More importantly, the model\npresented in this report offers superior generalization capability, unlike\nother popular methods in the literature, as it does not essentially require\ndatabase-specific parameter tuning, which is another key advantage over other\nmethods in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 09:24:06 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Liu", "Yang", ""], ["Kumar", "Ajay", ""]]}, {"id": "1812.11328", "submitter": "Yusuke Yoshiyasu", "authors": "Yusuke Yoshiyasu, Ryusuke Sagawa, Ko Ayusawa, Akihiko Murai", "title": "Skeleton Transformer Networks: 3D Human Pose and Skinned Mesh from\n  Single RGB Image", "comments": "ACCV conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Skeleton Transformer Networks (SkeletonNet), an\nend-to-end framework that can predict not only 3D joint positions but also 3D\nangular pose (bone rotations) of a human skeleton from a single color image.\nThis in turn allows us to generate skinned mesh animations. Here, we propose a\ntwo-step regression approach. The first step regresses bone rotations in order\nto obtain an initial solution by considering skeleton structure. The second\nstep performs refinement based on heatmap regressor using a 3D pose\nrepresentation called cross heatmap which stacks heatmaps of xy and zy\ncoordinates. By training the network using the proposed 3D human pose dataset\nthat is comprised of images annotated with 3D skeletal angular poses, we showed\nthat SkeletonNet can predict a full 3D human pose (joint positions and bone\nrotations) from a single image in-the-wild.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 10:22:14 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Yoshiyasu", "Yusuke", ""], ["Sagawa", "Ryusuke", ""], ["Ayusawa", "Ko", ""], ["Murai", "Akihiko", ""]]}, {"id": "1812.11337", "submitter": "Ghouthi Boukli Hacene", "authors": "Ghouthi Boukli Hacene (ELEC), Vincent Gripon, Matthieu Arzel (ELEC),\n  Nicolas Farrugia (ELEC), Yoshua Bengio (DIRO)", "title": "Quantized Guided Pruning for Efficient Hardware Implementations of\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are state-of-the-art in numerous\ncomputer vision tasks such as object classification and detection. However, the\nlarge amount of parameters they contain leads to a high computational\ncomplexity and strongly limits their usability in budget-constrained devices\nsuch as embedded devices. In this paper, we propose a combination of a new\npruning technique and a quantization scheme that effectively reduce the\ncomplexity and memory usage of convolutional layers of CNNs, and replace the\ncomplex convolutional operation by a low-cost multiplexer. We perform\nexperiments on the CIFAR10, CIFAR100 and SVHN and show that the proposed method\nachieves almost state-of-the-art accuracy, while drastically reducing the\ncomputational and memory footprints. We also propose an efficient hardware\narchitecture to accelerate CNN operations. The proposed hardware architecture\nis a pipeline and accommodates multiple layers working at the same time to\nspeed up the inference process.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 11:06:39 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Hacene", "Ghouthi Boukli", "", "ELEC"], ["Gripon", "Vincent", "", "ELEC"], ["Arzel", "Matthieu", "", "ELEC"], ["Farrugia", "Nicolas", "", "ELEC"], ["Bengio", "Yoshua", "", "DIRO"]]}, {"id": "1812.11339", "submitter": "Nieto Gregoire", "authors": "Gr\\'egoire Nieto (LJK), Fr\\'ed\\'eric Devernay (PRIMA), James Crowley\n  (PERVASIVE)", "title": "Rendu bas\\'e image avec contraintes sur les gradients", "comments": "in French. Traitement du Signal, Lavoisier, A para\\^itre", "journal-ref": null, "doi": "10.3166/HSP.x.1-26", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view image-based rendering consists in generating a novel view of a\nscene from a set of source views. In general, this works by first doing a\ncoarse 3D reconstruction of the scene, and then using this reconstruction to\nestablish correspondences between source and target views, followed by blending\nthe warped views to get the final image. Unfortunately, discontinuities in the\nblending weights, due to scene geometry or camera placement, result in\nartifacts in the target view. In this paper, we show how to avoid these\nartifacts by imposing additional constraints on the image gradients of the\nnovel view. We propose a variational framework in which an energy functional is\nderived and optimized by iteratively solving a linear system. We demonstrate\nthis method on several structured and unstructured multi-view datasets, and\nshow that it numerically outperforms state-of-the-art methods, and eliminates\nartifacts that result from visibility discontinuities\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 11:16:39 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Nieto", "Gr\u00e9goire", "", "LJK"], ["Devernay", "Fr\u00e9d\u00e9ric", "", "PRIMA"], ["Crowley", "James", "", "PERVASIVE"]]}, {"id": "1812.11369", "submitter": "Houjing Huang", "authors": "Houjing Huang, Wenjie Yang, Xiaotang Chen, Xin Zhao, Kaiqi Huang,\n  Jinbin Lin, Guan Huang, Dalong Du", "title": "EANet: Enhancing Alignment for Cross-Domain Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) has achieved significant improvement under\nthe single-domain setting. However, directly exploiting a model to new domains\nis always faced with huge performance drop, and adapting the model to new\ndomains without target-domain identity labels is still challenging. In this\npaper, we address cross-domain ReID and make contributions for both model\ngeneralization and adaptation. First, we propose Part Aligned Pooling (PAP)\nthat brings significant improvement for cross-domain testing. Second, we design\na Part Segmentation (PS) constraint over ReID feature to enhance alignment and\nimprove model generalization. Finally, we show that applying our PS constraint\nto unlabeled target domain images serves as effective domain adaptation. We\nconduct extensive experiments between three large datasets, Market1501, CUHK03\nand DukeMTMC-reID. Our model achieves state-of-the-art performance under both\nsource-domain and cross-domain settings. For completeness, we also demonstrate\nthe complementarity of our model to existing domain adaptation methods. The\ncode is available at https://github.com/huanghoujing/EANet.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 14:12:32 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Huang", "Houjing", ""], ["Yang", "Wenjie", ""], ["Chen", "Xiaotang", ""], ["Zhao", "Xin", ""], ["Huang", "Kaiqi", ""], ["Lin", "Jinbin", ""], ["Huang", "Guan", ""], ["Du", "Dalong", ""]]}, {"id": "1812.11383", "submitter": "Junkun Qi", "authors": "Junkun Qi, Wei Hu, Zongming Guo", "title": "Feature Preserving and Uniformity-controllable Point Cloud\n  Simplification on Graph", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of 3D sensing technologies, point clouds have attracted\nincreasing attention in a variety of applications for 3D object representation,\nsuch as autonomous driving, 3D immersive tele-presence and heritage\nreconstruction. However, it is challenging to process large-scale point clouds\nin terms of both computation time and storage due to the tremendous amounts of\ndata. Hence, we propose a point cloud simplification algorithm, aiming to\nstrike a balance between preserving sharp features and keeping uniform density\nduring resampling. In particular, leveraging on graph spectral processing, we\nrepresent irregular point clouds naturally on graphs, and propose concise\nformulations of feature preservation and density uniformity based on graph\nfilters. The problem of point cloud simplification is finally formulated as a\ntrade-off between the two factors and efficiently solved by our proposed\nalgorithm. Experimental results demonstrate the superiority of our method, as\nwell as its efficient application in point cloud registration.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 15:35:06 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Qi", "Junkun", ""], ["Hu", "Wei", ""], ["Guo", "Zongming", ""]]}, {"id": "1812.11440", "submitter": "Veronica Vilaplana", "authors": "Irina Sanchez and Veronica Vilaplana", "title": "Brain MRI super-resolution using 3D generative adversarial networks", "comments": "First International Conference on Medical Imaging with Deep Learning,\n  Amsterdam, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose an adversarial learning approach to generate high\nresolution MRI scans from low resolution images. The architecture, based on the\nSRGAN model, adopts 3D convolutions to exploit volumetric information. For the\ndiscriminator, the adversarial loss uses least squares in order to stabilize\nthe training. For the generator, the loss function is a combination of a least\nsquares adversarial loss and a content term based on mean square error and\nimage gradients in order to improve the quality of the generated images. We\nexplore different solutions for the upsampling phase. We present promising\nresults that improve classical interpolation, showing the potential of the\napproach for 3D medical imaging super-resolution. Source code available at\nhttps://github.com/imatge-upc/3D-GAN-superresolution\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 22:19:00 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Sanchez", "Irina", ""], ["Vilaplana", "Veronica", ""]]}, {"id": "1812.11477", "submitter": "Meenakshi Khosla", "authors": "Meenakshi Khosla, Keith Jamison, Gia H. Ngo, Amy Kuceyeski and Mert R.\n  Sabuncu", "title": "Machine learning in resting-state fMRI analysis", "comments": "51 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have gained prominence for the analysis of\nresting-state functional Magnetic Resonance Imaging (rs-fMRI) data. Here, we\npresent an overview of various unsupervised and supervised machine learning\napplications to rs-fMRI. We present a methodical taxonomy of machine learning\nmethods in resting-state fMRI. We identify three major divisions of\nunsupervised learning methods with regard to their applications to rs-fMRI,\nbased on whether they discover principal modes of variation across space, time\nor population. Next, we survey the algorithms and rs-fMRI feature\nrepresentations that have driven the success of supervised subject-level\npredictions. The goal is to provide a high-level overview of the burgeoning\nfield of rs-fMRI from the perspective of machine learning applications.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 06:37:46 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Khosla", "Meenakshi", ""], ["Jamison", "Keith", ""], ["Ngo", "Gia H.", ""], ["Kuceyeski", "Amy", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1812.11478", "submitter": "Zenglin Xu", "authors": "Xianghong Fang, Haoli Bai, Ziyi Guo, Bin Shen, Steven Hoi, Zenglin Xu", "title": "DART: Domain-Adversarial Residual-Transfer Networks for Unsupervised\n  Cross-Domain Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of deep learning (e.g., convolutional neural networks) for an\nimage classification task critically relies on the amount of labeled training\ndata. Aiming to solve an image classification task on a new domain that lacks\nlabeled data but gains access to cheaply available unlabeled data, unsupervised\ndomain adaptation is a promising technique to boost the performance without\nincurring extra labeling cost, by assuming images from different domains share\nsome invariant characteristics. In this paper, we propose a new unsupervised\ndomain adaptation method named Domain-Adversarial Residual-Transfer (DART)\nlearning of Deep Neural Networks to tackle cross-domain image classification\ntasks. In contrast to the existing unsupervised domain adaption approaches, the\nproposed DART not only learns domain-invariant features via adversarial\ntraining, but also achieves robust domain-adaptive classification via a\nresidual-transfer strategy, all in an end-to-end training framework. We\nevaluate the performance of the proposed method for cross-domain image\nclassification tasks on several well-known benchmark data sets, in which our\nmethod clearly outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 06:49:08 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Fang", "Xianghong", ""], ["Bai", "Haoli", ""], ["Guo", "Ziyi", ""], ["Shen", "Bin", ""], ["Hoi", "Steven", ""], ["Xu", "Zenglin", ""]]}, {"id": "1812.11489", "submitter": "Pavlo Melnyk", "authors": "Pavlo Melnyk, Zhiqiang You, Keqin Li", "title": "A High-Performance CNN Method for Offline Handwritten Chinese Character\n  Recognition and Visualization", "comments": "11 pages, 4 figures; corrected typos; added figures; added section\n  4.6; added details in section 3.3, 4.4", "journal-ref": null, "doi": "10.1007/s00500-019-04083-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches introduced fast, compact and efficient convolutional neural\nnetworks (CNNs) for offline handwritten Chinese character recognition (HCCR).\nHowever, many of them did not address the problem of network interpretability.\nWe propose a new architecture of a deep CNN with high recognition performance\nwhich is capable of learning deep features for visualization. A special\ncharacteristic of our model is the bottleneck layers which enable us to retain\nits expressiveness while reducing the number of multiply-accumulate operations\nand the required storage. We introduce a modification of global weighted\naverage pooling (GWAP) - global weighted output average pooling (GWOAP). This\npaper demonstrates how they allow us to calculate class activation maps (CAMs)\nin order to indicate the most relevant input character image regions used by\nour CNN to identify a certain class. Evaluating on the ICDAR-2013 offline HCCR\ncompetition dataset, we show that our model enables a relative 0.83% error\nreduction while having 49% fewer parameters and the same computational cost\ncompared to the current state-of-the-art single-network method trained only on\nhandwritten data. Our solution outperforms even recent residual learning\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 08:23:34 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 07:33:45 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Melnyk", "Pavlo", ""], ["You", "Zhiqiang", ""], ["Li", "Keqin", ""]]}, {"id": "1812.11501", "submitter": "Danfeng Hong", "authors": "Danfeng Hong, Naoto Yokoya, Jocelyn Chanussot, Xiao Xiang Zhu", "title": "CoSpace: Common Subspace Learning from Hyperspectral-Multispectral\n  Correspondences", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 2019", "doi": "10.1109/TGRS.2018.2890705", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a large amount of open satellite multispectral imagery (e.g., Sentinel-2\nand Landsat-8), considerable attention has been paid to global multispectral\nland cover classification. However, its limited spectral information hinders\nfurther improving the classification performance. Hyperspectral imaging enables\ndiscrimination between spectrally similar classes but its swath width from\nspace is narrow compared to multispectral ones. To achieve accurate land cover\nclassification over a large coverage, we propose a cross-modality feature\nlearning framework, called common subspace learning (CoSpace), by jointly\nconsidering subspace learning and supervised classification. By locally\naligning the manifold structure of the two modalities, CoSpace linearly learns\na shared latent subspace from hyperspectral-multispectral(HS-MS)\ncorrespondences. The multispectral out-of-samples can be then projected into\nthe subspace, which are expected to take advantages of rich spectral\ninformation of the corresponding hyperspectral data used for learning, and thus\nleads to a better classification. Extensive experiments on two simulated HSMS\ndatasets (University of Houston and Chikusei), where HS-MS data sets have\ntrade-offs between coverage and spectral resolution, are performed to\ndemonstrate the superiority and effectiveness of the proposed method in\ncomparison with previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 10:03:08 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 20:47:39 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Hong", "Danfeng", ""], ["Yokoya", "Naoto", ""], ["Chanussot", "Jocelyn", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1812.11532", "submitter": "Zuzana Kukelova", "authors": "Zuzana Kukelova, Cenek Albl, Akihiro Sugimoto, Tomas Pajdla", "title": "Linear solution to the minimal absolute pose rolling shutter problem", "comments": "14th Asian Conference on Computer Vision (ACCV 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new efficient solutions to the rolling shutter camera\nabsolute pose problem. Unlike the state-of-the-art polynomial solvers, we\napproach the problem using simple and fast linear solvers in an iterative\nscheme. We present several solutions based on fixing different sets of\nvariables and investigate the performance of them thoroughly. We design a new\nalternation strategy that estimates all parameters in each iteration linearly\nby fixing just the non-linear terms. Our best 6-point solver, based on the new\nalternation technique, shows an identical or even better performance than the\nstate-of-the-art R6P solver and is two orders of magnitude faster. In addition,\na linear non-iterative solver is presented that requires a non-minimal number\nof 9 correspondences but provides even better results than the state-of-the-art\nR6P. Moreover, all proposed linear solvers provide a single solution while the\nstate-of-the-art R6P provides up to 20 solutions which have to be pruned by\nexpensive verification.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 13:43:59 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Kukelova", "Zuzana", ""], ["Albl", "Cenek", ""], ["Sugimoto", "Akihiro", ""], ["Pajdla", "Tomas", ""]]}, {"id": "1812.11560", "submitter": "Veronica Vilaplana", "authors": "Marc Combalia and Veronica Vilaplana", "title": "Monte-Carlo Sampling applied to Multiple Instance Learning for\n  Histological Image Classification", "comments": "accepted at 4th International Workshop on Deep Learning for Medical\n  Image Analysis (DLMIA), MICCAI 2018, Deep Learning in Medical Image Analysis\n  and Multimodal Learning for Clinical Decision Support, Springer International\n  Publishing, 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00889-5", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a patch sampling strategy based on a sequential Monte-Carlo method\nfor high resolution image classification in the context of Multiple Instance\nLearning. When compared with grid sampling and uniform sampling techniques, it\nachieves higher generalization performance. We validate the strategy on two\nartificial datasets and two histological datasets for breast cancer and sun\nexposure classification.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 15:38:25 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Combalia", "Marc", ""], ["Vilaplana", "Veronica", ""]]}, {"id": "1812.11574", "submitter": "Tarang Chugh", "authors": "Tarang Chugh and Anil K. Jain", "title": "Fingerprint Presentation Attack Detection: Generalization and Efficiency", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of fingerprint presentation attack detection (PAD) under\nunknown PA materials not seen during PAD training. A dataset of 5,743 bonafide\nand 4,912 PA images of 12 different materials is used to evaluate a\nstate-of-the-art PAD, namely Fingerprint Spoof Buster. We utilize 3D t-SNE\nvisualization and clustering of material characteristics to identify a\nrepresentative set of PA materials that cover most of PA feature space. We\nobserve that a set of six PA materials, namely Silicone, 2D Paper, Play Doh,\nGelatin, Latex Body Paint and Monster Liquid Latex provide a good\nrepresentative set that should be included in training to achieve\ngeneralization of PAD. We also propose an optimized Android app of Fingerprint\nSpoof Buster that can run on a commodity smartphone (Xiaomi Redmi Note 4)\nwithout a significant drop in PAD performance (from TDR = 95.7% to 95.3% @ FDR\n= 0.2%) which can make a PA prediction in less than 300ms.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 17:23:53 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Chugh", "Tarang", ""], ["Jain", "Anil K.", ""]]}, {"id": "1812.11586", "submitter": "Veronica Vilaplana", "authors": "Marc G\\'orriz, Albert Aparicio, Berta Ravent\\'os, Ver\\'onica\n  Vilaplana, Elisa Sayrol and Daniel L\\'opez-Codina", "title": "Leishmaniasis Parasite Segmentation and Classification using Deep\n  Learning", "comments": "10th International Conference, AMDO 2018, Palma de Mallorca, Spain,\n  July 12-13, 2018, Proceedings", "journal-ref": "Articulated Motion and Deformable Objects, Series volume 10945 ,\n  2018, Springer International Publishing AG, part of Springer Nature", "doi": "10.1007/978-3-319-94544-6", "report-no": null, "categories": "cs.CV cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leishmaniasis is considered a neglected disease that causes thousands of\ndeaths annually in some tropical and subtropical countries. There are various\ntechniques to diagnose leishmaniasis of which manual microscopy is considered\nto be the gold standard. There is a need for the development of automatic\ntechniques that are able to detect parasites in a robust and unsupervised\nmanner. In this paper we present a procedure for automatizing the detection\nprocess based on a deep learning approach. We train a U-net model that\nsuccessfully segments leismania parasites and classifies them into\npromastigotes, amastigotes and adhered parasites.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 18:42:08 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["G\u00f3rriz", "Marc", ""], ["Aparicio", "Albert", ""], ["Ravent\u00f3s", "Berta", ""], ["Vilaplana", "Ver\u00f3nica", ""], ["Sayrol", "Elisa", ""], ["L\u00f3pez-Codina", "Daniel", ""]]}, {"id": "1812.11588", "submitter": "Veronica Vilaplana", "authors": "Adri\\`a Casamitjana, Marcel Cat\\`a, Irina S\\'anchez, Marc Combalia and\n  Ver\\'onica Vilaplana", "title": "Cascaded V-Net using ROI masks for brain tumor segmentation", "comments": "Third International Workshop, BrainLes 2017, Held in Conjunction with\n  MICCAI 2017, Quebec City, QC, Canada, September 14, 2017, Revised Selected\n  Papers", "journal-ref": "Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic\n  Brain Injuries, Series volume 10670, 2018, Springer International Publishing\n  AG, part of Springer Nature", "doi": "10.1007/978-3-319-75238-9", "report-no": null, "categories": "cs.CV cs.AI cs.CY cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we approach the brain tumor segmentation problem with a cascade\nof two CNNs inspired in the V-Net architecture \\cite{VNet}, reformulating\nresidual connections and making use of ROI masks to constrain the networks to\ntrain only on relevant voxels. This architecture allows dense training on\nproblems with highly skewed class distributions, such as brain tumor\nsegmentation, by focusing training only on the vecinity of the tumor area. We\nreport results on BraTS2017 Training and Validation sets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 18:51:37 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Casamitjana", "Adri\u00e0", ""], ["Cat\u00e0", "Marcel", ""], ["S\u00e1nchez", "Irina", ""], ["Combalia", "Marc", ""], ["Vilaplana", "Ver\u00f3nica", ""]]}, {"id": "1812.11606", "submitter": "Akash Kumar", "authors": "Akash Kumar", "title": "Solar Potential Analysis of Rooftops Using Satellite Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solar energy is one of the most important sources of renewable energy and the\ncleanest form of energy. In India, where solar energy could produce power\naround trillion kilowatt-hours in a year, our country is only able to produce\npower of around in gigawatts only. Many people are not aware of the solar\npotential of their rooftop, and hence they always think that installing solar\npanels is very much expensive. In this work, we introduce an approach through\nwhich we can generate a report remotely that provides the amount of solar\npotential of a building using only its latitude and longitude. We further\nevaluated various types of rooftops to make our solution more robust. We also\nprovide an approximate area of rooftop that can be used for solar panels\nplacement and a visual analysis of how solar panels can be placed to maximize\nthe output of solar power at a location.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 20:36:36 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 11:03:51 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Kumar", "Akash", ""]]}, {"id": "1812.11631", "submitter": "Oytun Ulutan", "authors": "Oytun Ulutan, Swati Rallapalli, Mudhakar Srivatsa, Carlos Torres, B.S.\n  Manjunath", "title": "Actor Conditioned Attention Maps for Video Action Detection", "comments": "WACV2020 Paper", "journal-ref": "In The IEEE Winter Conference on Applications of Computer Vision\n  (pp. 527-536) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While observing complex events with multiple actors, humans do not assess\neach actor separately, but infer from the context. The surrounding context\nprovides essential information for understanding actions. To this end, we\npropose to replace region of interest(RoI) pooling with an attention module,\nwhich ranks each spatio-temporal region's relevance to a detected actor instead\nof cropping. We refer to these as Actor-Conditioned Attention Maps (ACAM),\nwhich amplify/dampen the features extracted from the entire scene. The\nresulting actor-conditioned features focus the model on regions that are\nrelevant to the conditioned actor. For actor localization, we leverage\npre-trained object detectors, which transfer better. The proposed model is\nefficient and our action detection pipeline achieves near real-time\nperformance. Experimental results on AVA 2.1 and JHMDB demonstrate the\neffectiveness of attention maps, with improvements of 7 mAP on AVA and 4 mAP on\nJHMDB.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 23:12:27 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 01:17:59 GMT"}, {"version": "v3", "created": "Sun, 10 May 2020 23:01:24 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ulutan", "Oytun", ""], ["Rallapalli", "Swati", ""], ["Srivatsa", "Mudhakar", ""], ["Torres", "Carlos", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1812.11647", "submitter": "Zaiwei Zhang", "authors": "Zaiwei Zhang, Zhenxiao Liang, Lemeng Wu, Xiaowei Zhou, Qixing Huang", "title": "Path-Invariant Map Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing a network of maps among a collection of objects/domains (or map\nsynchronization) is a central problem across computer vision and many other\nrelevant fields. Compared to optimizing pairwise maps in isolation, the benefit\nof map synchronization is that there are natural constraints among a map\nnetwork that can improve the quality of individual maps. While such\nself-supervision constraints are well-understood for undirected map networks\n(e.g., the cycle-consistency constraint), they are under-explored for directed\nmap networks, which naturally arise when maps are given by parametric maps\n(e.g., a feed-forward neural network). In this paper, we study a natural\nself-supervision constraint for directed map networks called path-invariance,\nwhich enforces that composite maps along different paths between a fixed pair\nof source and target domains are identical. We introduce path-invariance bases\nfor efficient encoding of the path-invariance constraint and present an\nalgorithm that outputs a path-variance basis with polynomial time and space\ncomplexities. We demonstrate the effectiveness of our approach on optimizing\nobject correspondences, estimating dense image maps via neural networks, and\nsemantic segmentation of 3D scenes via map networks of diverse 3D\nrepresentations. In particular, for 3D semantic segmentation, our approach only\nrequires 8% labeled data from ScanNet to achieve the same performance as\ntraining a single 3D segmentation network with 30% to 100% labeled data.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 00:38:26 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 13:39:13 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 08:14:47 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Zhang", "Zaiwei", ""], ["Liang", "Zhenxiao", ""], ["Wu", "Lemeng", ""], ["Zhou", "Xiaowei", ""], ["Huang", "Qixing", ""]]}, {"id": "1812.11671", "submitter": "Zhimin Zhang", "authors": "Zhimin Zhang, Jianzhong Qiao, Shukuan Lin", "title": "Unsupervised monocular stereo matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, deep learning has been applied more and more in monocular image\ndepth estimation and has shown promising results. The current more ideal method\nfor monocular depth estimation is the supervised learning based on ground truth\ndepth, but this method requires an abundance of expensive ground truth depth as\nthe supervised labels. Therefore, researchers began to work on unsupervised\ndepth estimation methods. Although the accuracy of unsupervised depth\nestimation method is still lower than that of supervised method, it is a\npromising research direction.\n  In this paper, Based on the experimental results that the stereo matching\nmodels outperforms monocular depth estimation models under the same\nunsupervised depth estimation model, we proposed an unsupervised monocular\nvision stereo matching method. In order to achieve the monocular stereo\nmatching, we constructed two unsupervised deep convolution network models, one\nwas to reconstruct the right view from the left view, and the other was to\nestimate the depth map using the reconstructed right view and the original left\nview. The two network models are piped together during the test phase. The\noutput results of this method outperforms the current mainstream unsupervised\ndepth estimation method in the challenging KITTI dataset.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 02:13:55 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Zhang", "Zhimin", ""], ["Qiao", "Jianzhong", ""], ["Lin", "Shukuan", ""]]}, {"id": "1812.11677", "submitter": "Ao Ren", "authors": "Ao Ren, Tianyun Zhang, Shaokai Ye, Jiayu Li, Wenyao Xu, Xuehai Qian,\n  Xue Lin, Yanzhi Wang", "title": "ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using\n  Alternating Direction Method of Multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate efficient embedded and hardware implementations of deep neural\nnetworks (DNNs), two important categories of DNN model compression techniques:\nweight pruning and weight quantization are investigated. The former leverages\nthe redundancy in the number of weights, whereas the latter leverages the\nredundancy in bit representation of weights. However, there lacks a systematic\nframework of joint weight pruning and quantization of DNNs, thereby limiting\nthe available model compression ratio. Moreover, the computation reduction,\nenergy efficiency improvement, and hardware performance overhead need to be\naccounted for besides simply model size reduction.\n  To address these limitations, we present ADMM-NN, the first\nalgorithm-hardware co-optimization framework of DNNs using Alternating\nDirection Method of Multipliers (ADMM), a powerful technique to deal with\nnon-convex optimization problems with possibly combinatorial constraints. The\nfirst part of ADMM-NN is a systematic, joint framework of DNN weight pruning\nand quantization using ADMM. It can be understood as a smart regularization\ntechnique with regularization target dynamically updated in each ADMM\niteration, thereby resulting in higher performance in model compression than\nprior work. The second part is hardware-aware DNN optimizations to facilitate\nhardware-level implementations.\n  Without accuracy loss, we can achieve 85$\\times$ and 24$\\times$ pruning on\nLeNet-5 and AlexNet models, respectively, significantly higher than prior work.\nThe improvement becomes more significant when focusing on computation\nreductions. Combining weight pruning and quantization, we achieve 1,910$\\times$\nand 231$\\times$ reductions in overall model size on these two benchmarks, when\nfocusing on data storage. Highly promising results are also observed on other\nrepresentative DNNs such as VGGNet and ResNet-50.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 02:26:48 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Ren", "Ao", ""], ["Zhang", "Tianyun", ""], ["Ye", "Shaokai", ""], ["Li", "Jiayu", ""], ["Xu", "Wenyao", ""], ["Qian", "Xuehai", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1812.11690", "submitter": "Max Ehrlich", "authors": "Max Ehrlich and Larry Davis", "title": "Deep Residual Learning in the JPEG Transform Domain", "comments": "Published in ICCV 2019. Code and notes are available on our website\n  at https://maxehr.umiacs.io/jpeg_domain_resnet/jpeg_domain_resnet_html.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general method of performing Residual Network inference and\nlearning in the JPEG transform domain that allows the network to consume\ncompressed images as input. Our formulation leverages the linearity of the JPEG\ntransform to redefine convolution and batch normalization with a tune-able\nnumerical approximation for ReLu. The result is mathematically equivalent to\nthe spatial domain network up to the ReLu approximation accuracy. A formulation\nfor image classification and a model conversion algorithm for spatial domain\nnetworks are given as examples of the method. We show that the sparsity of the\nJPEG format allows for faster processing of images with little to no penalty in\nthe network accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 03:55:09 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 20:40:37 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 14:41:31 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Ehrlich", "Max", ""], ["Davis", "Larry", ""]]}, {"id": "1812.11702", "submitter": "Ignacio Viedma", "authors": "Juan Tapia, Claudia Arellano, Ignacio Viedma", "title": "Sex-Classification from Cell-Phones Periocular Iris Images", "comments": "Pre-print version accepted to be published On Selfie Biometrics\n  Book-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selfie soft biometrics has great potential for various applications ranging\nfrom marketing, security and online banking. However, it faces many challenges\nsince there is limited control in data acquisition conditions. This chapter\npresents a Super-Resolution-Convolutional Neural Networks (SRCNNs) approach\nthat increases the resolution of low quality periocular iris images cropped\nfrom selfie images of subject's faces. This work shows that increasing image\nresolution (2x and 3x) can improve the sex-classification rate when using a\nRandom Forest classifier. The best sex-classification rate was 90.15% for the\nright and 87.15% for the left eye. This was achieved when images were upscaled\nfrom 150x150 to 450x450 pixels. These results compare well with the state of\nthe art and show that when improving image resolution with the SRCNN the\nsex-classification rate increases. Additionally, a novel selfie database\ncaptured from 150 subjects with an iPhone X was created (available upon\nrequest).\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 06:08:07 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Tapia", "Juan", ""], ["Arellano", "Claudia", ""], ["Viedma", "Ignacio", ""]]}, {"id": "1812.11703", "submitter": "Bo Li", "authors": "Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan", "title": "SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese network based trackers formulate tracking as convolutional feature\ncross-correlation between target template and searching region. However,\nSiamese trackers still have accuracy gap compared with state-of-the-art\nalgorithms and they cannot take advantage of feature from deep networks, such\nas ResNet-50 or deeper. In this work we prove the core reason comes from the\nlack of strict translation invariance. By comprehensive theoretical analysis\nand experimental validations, we break this restriction through a simple yet\neffective spatial aware sampling strategy and successfully train a\nResNet-driven Siamese tracker with significant performance gain. Moreover, we\npropose a new model architecture to perform depth-wise and layer-wise\naggregations, which not only further improves the accuracy but also reduces the\nmodel size. We conduct extensive ablation studies to demonstrate the\neffectiveness of the proposed tracker, which obtains currently the best results\non four large tracking benchmarks, including OTB2015, VOT2018, UAV123, and\nLaSOT. Our model will be released to facilitate further studies based on this\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 06:14:11 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Li", "Bo", ""], ["Wu", "Wei", ""], ["Wang", "Qiang", ""], ["Zhang", "Fangyi", ""], ["Xing", "Junliang", ""], ["Yan", "Junjie", ""]]}, {"id": "1812.11725", "submitter": "Xingguo Liu", "authors": "Xingguo Liu, Yinping Chen, Zhenming Peng, Juan Wu", "title": "Total Variation with Overlapping Group Sparsity and Lp Quasinorm for\n  Infrared Image Deblurring under Salt-and-Pepper Noise", "comments": null, "journal-ref": null, "doi": "10.1117/1.JEI.28.4.043031", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the limitations of the infrared imaging principle and the\nproperties of infrared imaging systems, infrared images have some drawbacks,\nincluding a lack of details, indistinct edges, and a large amount of\nsalt-andpepper noise. To improve the sparse characteristics of the image while\nmaintaining the image edges and weakening staircase artifacts, this paper\nproposes a method that uses the Lp quasinorm instead of the L1 norm and for\ninfrared image deblurring with an overlapping group sparse total variation\nmethod. The Lp quasinorm introduces another degree of freedom, better describes\nimage sparsity characteristics, and improves image restoration. Furthermore, we\nadopt the accelerated alternating direction method of multipliers and fast\nFourier transform theory in the proposed method to improve the efficiency and\nrobustness of our algorithm. Experiments show that under different conditions\nfor blur and salt-and-pepper noise, the proposed method leads to excellent\nperformance in terms of objective evaluation and subjective visual results.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 08:54:33 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 12:36:36 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Liu", "Xingguo", ""], ["Chen", "Yinping", ""], ["Peng", "Zhenming", ""], ["Wu", "Juan", ""]]}, {"id": "1812.11737", "submitter": "Alexander Kuhnle", "authors": "Alexander Kuhnle and Ann Copestake", "title": "The meaning of \"most\" for visual question answering models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correct interpretation of quantifier statements in the context of a\nvisual scene requires non-trivial inference mechanisms. For the example of\n\"most\", we discuss two strategies which rely on fundamentally different\ncognitive concepts. Our aim is to identify what strategy deep learning models\nfor visual question answering learn when trained on such questions. To this\nend, we carefully design data to replicate experiments from psycholinguistics\nwhere the same question was investigated for humans. Focusing on the FiLM\nvisual question answering model, our experiments indicate that a form of\napproximate number system emerges whose performance declines with more\ndifficult scenes as predicted by Weber's law. Moreover, we identify confounding\nfactors, like spatial arrangement of the scene, which impede the effectiveness\nof this system.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 09:41:04 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 08:22:29 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Kuhnle", "Alexander", ""], ["Copestake", "Ann", ""]]}, {"id": "1812.11771", "submitter": "Shreya Ghosh", "authors": "Shreya Ghosh, Abhinav Dhall, Nicu Sebe, Tom Gedeon", "title": "Predicting Group Cohesiveness in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cohesiveness of a group is an essential indicator of the emotional state,\nstructure and success of a group of people. We study the factors that influence\nthe perception of group-level cohesion and propose methods for estimating the\nhuman-perceived cohesion on the group cohesiveness scale. In order to identify\nthe visual cues (attributes) for cohesion, we conducted a user survey. Image\nanalysis is performed at a group-level via a multi-task convolutional neural\nnetwork. For analyzing the contribution of facial expressions of the group\nmembers for predicting the Group Cohesion Score (GCS), a capsule network is\nexplored. We add GCS to the Group Affect database and propose the `GAF-Cohesion\ndatabase'. The proposed model performs well on the database and is able to\nachieve near human-level performance in predicting a group's cohesion score. It\nis interesting to note that group cohesion as an attribute, when jointly\ntrained for group-level emotion prediction, helps in increasing the performance\nfor the later task. This suggests that group-level emotion and cohesion are\ncorrelated.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 12:01:19 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 11:52:56 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 04:50:06 GMT"}, {"version": "v4", "created": "Sun, 7 Apr 2019 06:16:27 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ghosh", "Shreya", ""], ["Dhall", "Abhinav", ""], ["Sebe", "Nicu", ""], ["Gedeon", "Tom", ""]]}, {"id": "1812.11780", "submitter": "F\\'abio Vin\\'icius Moreira Perez", "authors": "F\\'abio Perez, R\\'emi Lebret, Karl Aberer", "title": "Weakly Supervised Active Learning with Cluster Annotation", "comments": "Poster session at the Bayesian Deep Learning Workshop - NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel framework that employs cluster annotation\nto boost active learning by reducing the number of human interactions required\nto train deep neural networks. Instead of annotating single samples\nindividually, humans can also label clusters, producing a higher number of\nannotated samples with the cost of a small label error. Our experiments show\nthat the proposed framework requires 82% and 87% less human interactions for\nCIFAR-10 and EuroSAT datasets respectively when compared with the\nfully-supervised training while maintaining similar performance on the test\nset.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 13:06:09 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 11:37:04 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Perez", "F\u00e1bio", ""], ["Lebret", "R\u00e9mi", ""], ["Aberer", "Karl", ""]]}, {"id": "1812.11788", "submitter": "Sida Peng", "authors": "Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, Xiaowei Zhou", "title": "PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation", "comments": "The first two authors contributed equally to this paper. Project\n  page: https://zju-3dv.github.io/pvnet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenge of 6DoF pose estimation from a single RGB\nimage under severe occlusion or truncation. Many recent works have shown that a\ntwo-stage approach, which first detects keypoints and then solves a\nPerspective-n-Point (PnP) problem for pose estimation, achieves remarkable\nperformance. However, most of these methods only localize a set of sparse\nkeypoints by regressing their image coordinates or heatmaps, which are\nsensitive to occlusion and truncation. Instead, we introduce a Pixel-wise\nVoting Network (PVNet) to regress pixel-wise unit vectors pointing to the\nkeypoints and use these vectors to vote for keypoint locations using RANSAC.\nThis creates a flexible representation for localizing occluded or truncated\nkeypoints. Another important feature of this representation is that it provides\nuncertainties of keypoint locations that can be further leveraged by the PnP\nsolver. Experiments show that the proposed approach outperforms the state of\nthe art on the LINEMOD, Occlusion LINEMOD and YCB-Video datasets by a large\nmargin, while being efficient for real-time pose estimation. We further create\na Truncation LINEMOD dataset to validate the robustness of our approach against\ntruncation. The code will be avaliable at https://zju-3dv.github.io/pvnet/.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 13:24:10 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Peng", "Sida", ""], ["Liu", "Yuan", ""], ["Huang", "Qixing", ""], ["Bao", "Hujun", ""], ["Zhou", "Xiaowei", ""]]}, {"id": "1812.11797", "submitter": "Greg Stephens", "authors": "Katarzyna Bozek, Laetitia Hebert, Alexander S Mikheyev and Greg J\n  Stephens", "title": "Pixel personality for dense object tracking in a 2D honeybee hive", "comments": "13 pages, 4 main and 9 supplementary figures as well as a link to\n  supplementary movies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking large numbers of densely-arranged, interacting objects is\nchallenging due to occlusions and the resulting complexity of possible\ntrajectory combinations, as well as the sparsity of relevant, labeled datasets.\nHere we describe a novel technique of collective tracking in the model\nenvironment of a 2D honeybee hive in which sample colonies consist of\n$N\\sim10^3$ highly similar individuals, tightly packed, and in rapid, irregular\nmotion. Such a system offers universal challenges for multi-object tracking,\nwhile being conveniently accessible for image recording. We first apply an\naccurate, segmentation-based object detection method to build initial short\ntrajectory segments by matching object configurations based on class, position\nand orientation. We then join these tracks into full single object trajectories\nby creating an object recognition model which is adaptively trained to\nrecognize honeybee individuals through their visual appearance across multiple\nframes, an attribute we denote as pixel personality. Overall, we reconstruct\n~46% of the trajectories in 5 min recordings from two different hives and over\n71% of the tracks for at least 2 min. We provide validated trajectories\nspanning 3000 video frames of 876 unmarked moving bees in two distinct colonies\nin different locations and filmed with different pixel resolutions, which we\nexpect to be useful in the further development of general-purpose tracking\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 13:46:21 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Bozek", "Katarzyna", ""], ["Hebert", "Laetitia", ""], ["Mikheyev", "Alexander S", ""], ["Stephens", "Greg J", ""]]}, {"id": "1812.11800", "submitter": "Sajad Darabi", "authors": "Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, Vahid Partovi\n  Nia", "title": "Regularized Binary Network Training", "comments": "NeurIPS19 Workshop on Energy Efficient Machine Learning and Cognitive\n  Computing (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a significant performance gap between Binary Neural Networks (BNNs)\nand floating point Deep Neural Networks (DNNs). We propose to improve the\nbinary training method, by introducing a new regularization function that\nencourages training weights around binary values. In addition, we add trainable\nscaling factors to our regularization functions. Additionally, an improved\napproximation of the derivative of the sign activation function in the backward\ncomputation. These modifications are based on linear operations that are easily\nimplementable into the binary training framework. Experimental results on\nImageNet shows our method outperforms the traditional BNN method and XNOR-net.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 14:07:08 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 00:12:28 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 04:57:46 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Darabi", "Sajad", ""], ["Belbahri", "Mouloud", ""], ["Courbariaux", "Matthieu", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "1812.11806", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw, Marco Loog", "title": "An introduction to domain adaptation and transfer learning", "comments": "Technical Report. 41 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, if the training data is an unbiased sample of an\nunderlying distribution, then the learned classification function will make\naccurate predictions for new samples. However, if the training data is not an\nunbiased sample, then there will be differences between how the training data\nis distributed and how the test data is distributed. Standard classifiers\ncannot cope with changes in data distributions between training and test\nphases, and will not perform well. Domain adaptation and transfer learning are\nsub-fields within machine learning that are concerned with accounting for these\ntypes of changes. Here, we present an introduction to these fields, guided by\nthe question: when and how can a classifier generalize from a source to a\ntarget domain? We will start with a brief introduction into risk minimization,\nand how transfer learning and domain adaptation expand upon this framework.\nFollowing that, we discuss three special cases of data set shift, namely prior,\ncovariate and concept shift. For more complex domain shifts, there are a wide\nvariety of approaches. These are categorized into: importance-weighting,\nsubspace mapping, domain-invariant spaces, feature augmentation, minimax\nestimators and robust algorithms. A number of points will arise, which we will\ndiscuss in the last section. We conclude with the remark that many open\nquestions will have to be addressed before transfer learners and\ndomain-adaptive classifiers become practical.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 14:19:20 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 13:06:25 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Kouw", "Wouter M.", ""], ["Loog", "Marco", ""]]}, {"id": "1812.11832", "submitter": "Patrizio Frosini", "authors": "Mattia G. Bergomi, Patrizio Frosini, Daniela Giorgi, Nicola Quercioli", "title": "Towards a topological-geometrical theory of group equivariant\n  non-expansive operators for data analysis and machine learning", "comments": "Added references. Extended Section 7. Added 3 figures. Corrected\n  typos. 42 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.AT math.OA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to provide a general mathematical framework for\ngroup equivariance in the machine learning context. The framework builds on a\nsynergy between persistent homology and the theory of group actions. We define\ngroup-equivariant non-expansive operators (GENEOs), which are maps between\nfunction spaces associated with groups of transformations. We study the\ntopological and metric properties of the space of GENEOs to evaluate their\napproximating power and set the basis for general strategies to initialise and\ncompose operators. We begin by defining suitable pseudo-metrics for the\nfunction spaces, the equivariance groups, and the set of non-expansive\noperators. Basing on these pseudo-metrics, we prove that the space of GENEOs is\ncompact and convex, under the assumption that the function spaces are compact\nand convex. These results provide fundamental guarantees in a machine learning\nperspective. We show examples on the MNIST and fashion-MNIST datasets. By\nconsidering isometry-equivariant non-expansive operators, we describe a simple\nstrategy to select and sample operators, and show how the selected and sampled\noperators can be used to perform both classical metric learning and an\neffective initialisation of the kernels of a convolutional neural network.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 15:17:07 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 22:13:41 GMT"}, {"version": "v3", "created": "Sun, 3 Mar 2019 19:18:26 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Bergomi", "Mattia G.", ""], ["Frosini", "Patrizio", ""], ["Giorgi", "Daniela", ""], ["Quercioli", "Nicola", ""]]}, {"id": "1812.11834", "submitter": "Jianxin Lin", "authors": "Zhibo Chen, Jianxin Lin, Tiankuang Zhou, Feng Wu", "title": "Sequential Gating Ensemble Network for Noise Robust Multi-Scale Face\n  Restoration", "comments": "11 pages, 15 figures. arXiv admin note: substantial text overlap with\n  arXiv:1805.02164", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face restoration from low resolution and noise is important for applications\nof face analysis recognition. However, most existing face restoration models\nomit the multiple scale issues in face restoration problem, which is still not\nwell-solved in research area. In this paper, we propose a Sequential Gating\nEnsemble Network (SGEN) for multi-scale noise robust face restoration issue. To\nendow the network with multi-scale representation ability, we first employ the\nprinciple of ensemble learning for SGEN network architecture designing. The\nSGEN aggregates multi-level base-encoders and base-decoders into the network,\nwhich enables the network to contain multiple scales of receptive field.\nInstead of combining these base-en/decoders directly with non-sequential\noperations, the SGEN takes base-en/decoders from different levels as sequential\ndata. Specifically, it is visualized that SGEN learns to sequentially extract\nhigh level information from base-encoders in bottom-up manner and restore low\nlevel information from base-decoders in top-down manner. Besides, we propose to\nrealize bottom-up and top-down information combination and selection with\nSequential Gating Unit (SGU). The SGU sequentially takes information from two\ndifferent levels as inputs and decides the output based on one active input.\nExperiment results on benchmark dataset demonstrate that our SGEN is more\neffective at multi-scale human face restoration with more image details and\nless noise than state-of-the-art image restoration models. Further utilizing\nadversarial training scheme, SGEN also produces more visually preferred results\nthan other models under subjective evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 11:53:40 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Chen", "Zhibo", ""], ["Lin", "Jianxin", ""], ["Zhou", "Tiankuang", ""], ["Wu", "Feng", ""]]}, {"id": "1812.11842", "submitter": "Giovanni Poggi", "authors": "Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, Giovanni Poggi", "title": "Do GANs leave artificial fingerprints?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, generative adversarial networks (GAN) have shown\ntremendous potential for a number of applications in computer vision and\nrelated fields. With the current pace of progress, it is a sure bet they will\nsoon be able to generate high-quality images and videos, virtually\nindistinguishable from real ones. Unfortunately, realistic GAN-generated images\npose serious threats to security, to begin with a possible flood of fake\nmultimedia, and multimedia forensic countermeasures are in urgent need. In this\nwork, we show that each GAN leaves its specific fingerprint in the images it\ngenerates, just like real-world cameras mark acquired images with traces of\ntheir photo-response non-uniformity pattern. Source identification experiments\nwith several popular GANs show such fingerprints to represent a precious asset\nfor forensic analyses.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 15:32:33 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Marra", "Francesco", ""], ["Gragnaniello", "Diego", ""], ["Verdoliva", "Luisa", ""], ["Poggi", "Giovanni", ""]]}, {"id": "1812.11852", "submitter": "Etienne de Stoutz", "authors": "Etienne de Stoutz, Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Luc\n  Van Gool", "title": "Fast Perceptual Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of photos taken today are by mobile phones. While their\nquality is rapidly growing, due to physical limitations and cost constraints,\nmobile phone cameras struggle to compare in quality with DSLR cameras. This\nmotivates us to computationally enhance these images. We extend upon the\nresults of Ignatov et al., where they are able to translate images from compact\nmobile cameras into images with comparable quality to high-resolution photos\ntaken by DSLR cameras. However, the neural models employed require large\namounts of computational resources and are not lightweight enough to run on\nmobile devices. We build upon the prior work and explore different network\narchitectures targeting an increase in image quality and speed. With an\nefficient network architecture which does most of its processing in a lower\nspatial resolution, we achieve a significantly higher mean opinion score (MOS)\nthan the baseline while speeding up the computation by 6.3 times on a\nconsumer-grade CPU. This suggests a promising direction for\nneural-network-based photo enhancement using the phone hardware of the future.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 15:52:29 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["de Stoutz", "Etienne", ""], ["Ignatov", "Andrey", ""], ["Kobyshev", "Nikolay", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1812.11894", "submitter": "Mohamed Yousef", "authors": "Mohamed Yousef, Khaled F. Hussain, and Usama S. Mohammed", "title": "Accurate, Data-Efficient, Unconstrained Text Recognition with\n  Convolutional Neural Networks", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained text recognition is an important computer vision task,\nfeaturing a wide variety of different sub-tasks, each with its own set of\nchallenges. One of the biggest promises of deep neural networks has been the\nconvergence and automation of feature extractors from input raw signals,\nallowing for the highest possible performance with minimum required domain\nknowledge. To this end, we propose a data-efficient, end-to-end neural network\nmodel for generic, unconstrained text recognition. In our proposed architecture\nwe strive for simplicity and efficiency without sacrificing recognition\naccuracy. Our proposed architecture is a fully convolutional network without\nany recurrent connections trained with the CTC loss function. Thus it operates\non arbitrary input sizes and produces strings of arbitrary length in a very\nefficient and parallelizable manner. We show the generality and superiority of\nour proposed text recognition architecture by achieving state of the art\nresults on seven public benchmark datasets, covering a wide spectrum of text\nrecognition tasks, namely: Handwriting Recognition, CAPTCHA recognition, OCR,\nLicense Plate Recognition, and Scene Text Recognition. Our proposed\narchitecture has won the ICFHR2018 Competition on Automated Text Recognition on\na READ Dataset.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 16:53:21 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Yousef", "Mohamed", ""], ["Hussain", "Khaled F.", ""], ["Mohammed", "Usama S.", ""]]}, {"id": "1812.11901", "submitter": "George K. Thiruvathukal", "authors": "Caleb Tung, Matthew R. Kelleher, Ryan J. Schlueter, Binhan Xu,\n  Yung-Hsiang Lu, George K. Thiruvathukal, Yen-Kuang Chen, Yang Lu", "title": "Large-Scale Object Detection of Images from Network Cameras in Variable\n  Ambient Lighting Conditions", "comments": "Submitted to MIPR 2019 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Computer vision relies on labeled datasets for training and evaluation in\ndetecting and recognizing objects. The popular computer vision program, YOLO\n(\"You Only Look Once\"), has been shown to accurately detect objects in many\nmajor image datasets. However, the images found in those datasets, are\nindependent of one another and cannot be used to test YOLO's consistency at\ndetecting the same object as its environment (e.g. ambient lighting) changes.\nThis paper describes a novel effort to evaluate YOLO's consistency for\nlarge-scale applications. It does so by working (a) at large scale and (b) by\nusing consecutive images from a curated network of public video cameras\ndeployed in a variety of real-world situations, including traffic\nintersections, national parks, shopping malls, university campuses, etc. We\nspecifically examine YOLO's ability to detect objects in different scenarios\n(e.g., daytime vs. night), leveraging the cameras' ability to rapidly retrieve\nmany successive images for evaluating detection consistency. Using our camera\nnetwork and advanced computing resources (supercomputers), we analyzed more\nthan 5 million images captured by 140 network cameras in 24 hours. Compared\nwith labels marked by humans (considered as \"ground truth\"), YOLO struggles to\nconsistently detect the same humans and cars as their positions change from one\nframe to the next; it also struggles to detect objects at night time. Our\nfindings suggest that state-of-the art vision solutions should be trained by\ndata from network camera with contextual information before they can be\ndeployed in applications that demand high consistency on object detection.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 17:06:44 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Tung", "Caleb", ""], ["Kelleher", "Matthew R.", ""], ["Schlueter", "Ryan J.", ""], ["Xu", "Binhan", ""], ["Lu", "Yung-Hsiang", ""], ["Thiruvathukal", "George K.", ""], ["Chen", "Yen-Kuang", ""], ["Lu", "Yang", ""]]}, {"id": "1812.11922", "submitter": "Vignesh Prasad", "authors": "Vignesh Prasad, Dipanjan Das, Brojeshwar Bhowmick", "title": "Epipolar Geometry based Learning of Multi-view Depth and Ego-Motion from\n  Monocular Sequences", "comments": "ICVGIP 2018 Best Paper Award. Extension of our work accepted at WACV\n  2019, available at arXiv:1812.08370", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep approaches to predict monocular depth and ego-motion have grown in\nrecent years due to their ability to produce dense depth from monocular images.\nThe main idea behind them is to optimize the photometric consistency over image\nsequences by warping one view into another, similar to direct visual odometry\nmethods. One major drawback is that these methods infer depth from a single\nview, which might not effectively capture the relation between pixels.\nMoreover, simply minimizing the photometric loss does not ensure proper pixel\ncorrespondences, which is a key factor for accurate depth and pose estimations.\n  In contrast, we propose a 2-view depth network to infer the scene depth from\nconsecutive frames, thereby learning inter-pixel relationships. To ensure\nbetter correspondences, thereby better geometric understanding, we propose\nincorporating epipolar constraints to make the learning more geometrically\nsound. We use the Essential matrix obtained using Nist'er's Five Point\nAlgorithm, to enforce meaningful geometric constraints, rather than using it as\ntraining labels. This allows us to use lesser no. of trainable parameters\ncompared to state-of-the-art methods. The proposed method results in better\ndepth images and pose estimates, which capture the scene structure and motion\nin a better way. Such a geometrically constrained learning performs\nsuccessfully even in cases where simply minimizing the photometric error would\nfail.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 09:26:49 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 19:10:10 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2019 12:00:24 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Prasad", "Vignesh", ""], ["Das", "Dipanjan", ""], ["Bhowmick", "Brojeshwar", ""]]}, {"id": "1812.11941", "submitter": "Ibraheem Alhashim", "authors": "Ibraheem Alhashim and Peter Wonka", "title": "High Quality Monocular Depth Estimation via Transfer Learning", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate depth estimation from images is a fundamental task in many\napplications including scene understanding and reconstruction. Existing\nsolutions for depth estimation often produce blurry approximations of low\nresolution. This paper presents a convolutional neural network for computing a\nhigh-resolution depth map given a single RGB image with the help of transfer\nlearning. Following a standard encoder-decoder architecture, we leverage\nfeatures extracted using high performing pre-trained networks when initializing\nour encoder along with augmentation and training strategies that lead to more\naccurate results. We show how, even for a very simple decoder, our method is\nable to achieve detailed high-resolution depth maps. Our network, with fewer\nparameters and training iterations, outperforms state-of-the-art on two\ndatasets and also produces qualitatively better results that capture object\nboundaries more faithfully. Code and corresponding pre-trained weights are made\npublicly available.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 18:25:21 GMT"}, {"version": "v2", "created": "Sun, 10 Mar 2019 07:46:03 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Alhashim", "Ibraheem", ""], ["Wonka", "Peter", ""]]}, {"id": "1812.11950", "submitter": "Menglei Zhang", "authors": "Menglei Zhang, Zhou Liu, Lei Yu", "title": "Image Super-Resolution via RL-CSC: When Residual Learning Meets\n  Convolutional Sparse Coding", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective model for Single Image Super-Resolution\n(SISR), by combining the merits of Residual Learning and Convolutional Sparse\nCoding (RL-CSC). Our model is inspired by the Learned Iterative\nShrinkage-Threshold Algorithm (LISTA). We extend LISTA to its convolutional\nversion and build the main part of our model by strictly following the\nconvolutional form, which improves the network's interpretability.\nSpecifically, the convolutional sparse codings of input feature maps are\nlearned in a recursive manner, and high-frequency information can be recovered\nfrom these CSCs. More importantly, residual learning is applied to alleviate\nthe training difficulty when the network goes deeper. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of our method. RL-CSC (30\nlayers) outperforms several recent state-of-the-arts, e.g., DRRN (52 layers)\nand MemNet (80 layers) in both accuracy and visual qualities. Codes and more\nresults are available at https://github.com/axzml/RL-CSC.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 18:44:26 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Zhang", "Menglei", ""], ["Liu", "Zhou", ""], ["Yu", "Lei", ""]]}, {"id": "1812.11960", "submitter": "Zhiling Long", "authors": "Muhammad Amir Shafiq, Tariq Alshawi, Zhiling Long and Ghassan AlRegib", "title": "The role of visual saliency in the automation of seismic interpretation", "comments": null, "journal-ref": "Geophysical Prospecting, vol. 66, issue S1, pp. 132-143, Mar. 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a workflow based on SalSi for the detection and\ndelineation of geological structures such as salt domes. SalSi is a seismic\nattribute designed based on the modeling of human visual system that detects\nthe salient features and captures the spatial correlation within seismic\nvolumes for delineating seismic structures. Using SalSi, we can not only\nhighlight the neighboring regions of salt domes to assist a seismic interpreter\nbut also delineate such structures using a region growing method and\npost-processing. The proposed delineation workflow detects the salt-dome\nboundary with very good precision and accuracy. Experimental results show the\neffectiveness of the proposed workflow on a real seismic dataset acquired from\nthe North Sea, F3 block. For the subjective evaluation of the results of\ndifferent salt-dome delineation algorithms, we have used a reference salt-dome\nboundary interpreted by a geophysicist. For the objective evaluation of\nresults, we have used five different metrics based on pixels, shape, and\ncurvedness to establish the effectiveness of the proposed workflow. The\nproposed workflow is not only fast but also yields better results as compared\nto other salt-dome delineation algorithms and shows a promising potential in\nseismic interpretation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 18:55:26 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Shafiq", "Muhammad Amir", ""], ["Alshawi", "Tariq", ""], ["Long", "Zhiling", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1812.11971", "submitter": "Alexander Sax", "authors": "Alexander Sax, Bradley Emi, Amir R. Zamir, Leonidas Guibas, Silvio\n  Savarese, Jitendra Malik", "title": "Mid-Level Visual Representations Improve Generalization and Sample\n  Efficiency for Learning Visuomotor Policies", "comments": "See project website, demos, and code at http://perceptual.actor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much does having visual priors about the world (e.g. the fact that the\nworld is 3D) assist in learning to perform downstream motor tasks (e.g.\ndelivering a package)? We study this question by integrating a generic\nperceptual skill set (e.g. a distance estimator, an edge detector, etc.) within\na reinforcement learning framework--see Figure 1. This skill set (hereafter\nmid-level perception) provides the policy with a more processed state of the\nworld compared to raw images.\n  We find that using a mid-level perception confers significant advantages over\ntraining end-to-end from scratch (i.e. not leveraging priors) in\nnavigation-oriented tasks. Agents are able to generalize to situations where\nthe from-scratch approach fails and training becomes significantly more sample\nefficient. However, we show that realizing these gains requires careful\nselection of the mid-level perceptual skills. Therefore, we refine our findings\ninto an efficient max-coverage feature set that can be adopted in lieu of raw\nimages. We perform our study in completely separate buildings for training and\ntesting and compare against visually blind baseline policies and\nstate-of-the-art feature learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 18:59:25 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 17:58:50 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 07:12:34 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Sax", "Alexander", ""], ["Emi", "Bradley", ""], ["Zamir", "Amir R.", ""], ["Guibas", "Leonidas", ""], ["Savarese", "Silvio", ""], ["Malik", "Jitendra", ""]]}]