[{"id": "1307.0036", "submitter": "Firas Ajil Jassim", "authors": "Firas A. Jassim", "title": "Increasing Compression Ratio in PNG Images by k-Modulus Method for Image\n  Transformation", "comments": "10 pages, 7 figures, 2 tables", "journal-ref": "International Journal of Advanced Research in Computer Science and\n  Software Engineering, Vol. 3, issue 6, pp. 45-52,June 2013", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Image compression is an important filed in image processing. The science\nwelcomes any tinny contribution that may increase the compression ratio by\nwhichever insignificant percentage. Therefore, the essential contribution in\nthis paper is to increase the compression ratio for the well known Portable\nNetwork Graphics (PNG) image file format. The contribution starts with\nconverting the original PNG image into k-Modulus Method (k-MM). Practically,\ntaking k equals to ten, and then the pixels in the constructed image will be\nintegers divisible by ten. Since PNG uses Lempel-Ziv compression algorithm,\nthen the ability to reduce file size will increase according to the repetition\nin pixels in each k-by-k window according to the transformation done by k-MM.\nExperimental results show that the proposed technique (k-PNG) produces high\ncompression ratio with smaller file size in comparison to the original PNG\nfile.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 22:01:48 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Jassim", "Firas A.", ""]]}, {"id": "1307.0060", "submitter": "Tejas Kulkarni", "authors": "Vikash K. Mansinghka, Tejas D. Kulkarni, Yura N. Perov, Joshua B.\n  Tenenbaum", "title": "Approximate Bayesian Image Interpretation using Generative Probabilistic\n  Graphics Programs", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of computer vision as the Bayesian inverse problem to computer\ngraphics has a long history and an appealing elegance, but it has proved\ndifficult to directly implement. Instead, most vision tasks are approached via\ncomplex bottom-up processing pipelines. Here we show that it is possible to\nwrite short, simple probabilistic graphics programs that define flexible\ngenerative models and to automatically invert them to interpret real-world\nimages. Generative probabilistic graphics programs consist of a stochastic\nscene generator, a renderer based on graphics software, a stochastic likelihood\nmodel linking the renderer's output and the data, and latent variables that\nadjust the fidelity of the renderer and the tolerance of the likelihood model.\nRepresentations and algorithms from computer graphics, originally designed to\nproduce high-quality images, are instead used as the deterministic backbone for\nhighly approximate and stochastic generative models. This formulation combines\nprobabilistic programming, computer graphics, and approximate Bayesian\ncomputation, and depends only on general-purpose, automatic inference\ntechniques. We describe two applications: reading sequences of degraded and\nadversarially obscured alphanumeric characters, and inferring 3D road models\nfrom vehicle-mounted camera images. Each of the probabilistic graphics programs\nwe present relies on under 20 lines of probabilistic code, and supports\naccurate, approximately Bayesian inferences about ambiguous real-world images.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 02:36:45 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Mansinghka", "Vikash K.", ""], ["Kulkarni", "Tejas D.", ""], ["Perov", "Yura N.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1307.0129", "submitter": "Roozbeh Rajabi", "authors": "Roozbeh Rajabi, Hassan Ghassemian", "title": "Hyperspectral Data Unmixing Using GNMF Method and Sparseness Constraint", "comments": "4 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images contain mixed pixels due to low spatial resolution of\nhyperspectral sensors. Mixed pixels are pixels containing more than one\ndistinct material called endmembers. The presence percentages of endmembers in\nmixed pixels are called abundance fractions. Spectral unmixing problem refers\nto decomposing these pixels into a set of endmembers and abundance fractions.\nDue to nonnegativity constraint on abundance fractions, nonnegative matrix\nfactorization methods (NMF) have been widely used for solving spectral unmixing\nproblem. In this paper we have used graph regularized (GNMF) method with\nsparseness constraint to unmix hyperspectral data. This method applied on\nsimulated data using AVIRIS Indian Pines dataset and USGS library and results\nare quantified based on AAD and SAD measures. Results in comparison with other\nmethods show that the proposed method can unmix data more effectively.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 16:57:44 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Rajabi", "Roozbeh", ""], ["Ghassemian", "Hassan", ""]]}, {"id": "1307.0277", "submitter": "Nilanjan  Dey", "authors": "Sourav Samantaa, Nilanjan Dey, Poulami Das, Suvojit Acharjee, Sheli\n  Sinha Chaudhuri", "title": "Multilevel Threshold Based Gray Scale Image Segmentation using Cuckoo\n  Search", "comments": "8 Pages,7 figures,ICECIT2012,Anatapur,India. arXiv admin note: text\n  overlap with arXiv:1003.1594, arXiv:1005.2908 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Segmentation is a technique of partitioning the original image into\nsome distinct classes. Many possible solutions may be available for segmenting\nan image into a certain number of classes, each one having different quality of\nsegmentation. In our proposed method, multilevel thresholding technique has\nbeen used for image segmentation. A new approach of Cuckoo Search (CS) is used\nfor selection of optimal threshold value. In other words, the algorithm is used\nto achieve the best solution from the initial random threshold values or\nsolutions and to evaluate the quality of a solution correlation function is\nused. Finally, MSE and PSNR are measured to understand the segmentation\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 06:50:23 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Samantaa", "Sourav", ""], ["Dey", "Nilanjan", ""], ["Das", "Poulami", ""], ["Acharjee", "Suvojit", ""], ["Chaudhuri", "Sheli Sinha", ""]]}, {"id": "1307.0426", "submitter": "Thomas Lampert", "authors": "Thomas A. Lampert, Andr\\'e Stumpf, Pierre Gan\\c{c}arski", "title": "An Empirical Study into Annotator Agreement, Ground Truth Estimation,\n  and Algorithm Evaluation", "comments": "16 pages", "journal-ref": "IEEE Transactions on Image Processing 25(6), 2557-2572, 2016", "doi": "10.1109/TIP.2016.2544703", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although agreement between annotators has been studied in the past from a\nstatistical viewpoint, little work has attempted to quantify the extent to\nwhich this phenomenon affects the evaluation of computer vision (CV) object\ndetection algorithms. Many researchers utilise ground truth (GT) in experiments\nand more often than not this GT is derived from one annotator's opinion. How\ndoes the difference in opinion affect an algorithm's evaluation? Four examples\nof typical CV problems are chosen, and a methodology is applied to each to\nquantify the inter-annotator variance and to offer insight into the mechanisms\nbehind agreement and the use of GT. It is found that when detecting linear\nobjects annotator agreement is very low. The agreement in object position,\nlinear or otherwise, can be partially explained through basic image properties.\nAutomatic object detectors are compared to annotator agreement and it is found\nthat a clear relationship exists. Several methods for calculating GTs from a\nnumber of annotations are applied and the resulting differences in the\nperformance of the object detectors are quantified. It is found that the rank\nof a detector is highly dependent upon the method used to form the GT. It is\nalso found that although the STAPLE and LSML GT estimation methods appear to\nrepresent the mean of the performance measured using the individual\nannotations, when there are few annotations, or there is a large variance in\nthem, these estimates tend to degrade. Furthermore, one of the most commonly\nadopted annotation combination methods--consensus voting--accentuates more\nobvious features, which results in an overestimation of the algorithm's\nperformance. Finally, it is concluded that in some datasets it may not be\npossible to state with any confidence that one algorithm outperforms another\nwhen evaluating upon one GT and a method for calculating confidence bounds is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 16:16:40 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2013 14:39:01 GMT"}, {"version": "v3", "created": "Tue, 26 Apr 2016 11:05:18 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Lampert", "Thomas A.", ""], ["Stumpf", "Andr\u00e9", ""], ["Gan\u00e7arski", "Pierre", ""]]}, {"id": "1307.0776", "submitter": "Jian Cheng", "authors": "Jian Cheng, Tianzi Jiang, Rachid Deriche, Dinggang Shen, Pew-Thian Yap", "title": "Regularized Spherical Polar Fourier Diffusion MRI with Optimal\n  Dictionary Learning", "comments": "Accepted by MICCAI 2013. Abstract shortened to respect the arXiv\n  limit of 1920 characters", "journal-ref": null, "doi": "10.1007/978-3-642-40811-3_80", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed Sensing (CS) takes advantage of signal sparsity or compressibility\nand allows superb signal reconstruction from relatively few measurements. Based\non CS theory, a suitable dictionary for sparse representation of the signal is\nrequired. In diffusion MRI (dMRI), CS methods were proposed to reconstruct\ndiffusion-weighted signal and the Ensemble Average Propagator (EAP), and there\nare two kinds of Dictionary Learning (DL) methods: 1) Discrete Representation\nDL (DR-DL), and 2) Continuous Representation DL (CR-DL). DR-DL is susceptible\nto numerical inaccuracy owing to interpolation and regridding errors in a\ndiscretized q-space. In this paper, we propose a novel CR-DL approach, called\nDictionary Learning - Spherical Polar Fourier Imaging (DL-SPFI) for effective\ncompressed-sensing reconstruction of the q-space diffusion-weighted signal and\nthe EAP. In DL-SPFI, an dictionary that sparsifies the signal is learned from\nthe space of continuous Gaussian diffusion signals. The learned dictionary is\nthen adaptively applied to different voxels using a weighted LASSO framework\nfor robust signal reconstruction. The adaptive dictionary is proved to be\noptimal. Compared with the start-of-the-art CR-DL and DR-DL methods proposed by\nMerlet et al. and Bilgic et al., espectively, our work offers the following\nadvantages. First, the learned dictionary is proved to be optimal for Gaussian\ndiffusion signals. Second, to our knowledge, this is the first work to learn a\nvoxel-adaptive dictionary. The importance of the adaptive dictionary in EAP\nreconstruction will be demonstrated theoretically and empirically. Third,\noptimization in DL-SPFI is only performed in a small subspace resided by the\nSPF coefficients, as opposed to the q-space approach utilized by Merlet et al.\nThe experiment results demonstrate the advantages of DL-SPFI over the original\nSPF basis and Bilgic et al.'s method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 17:47:32 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Cheng", "Jian", ""], ["Jiang", "Tianzi", ""], ["Deriche", "Rachid", ""], ["Shen", "Dinggang", ""], ["Yap", "Pew-Thian", ""]]}, {"id": "1307.0805", "submitter": "Zemin Zhang", "authors": "Zemin Zhang, Gregory Ely, Shuchin Aeron, Ning Hao, Misha Kilmer", "title": "Novel Factorization Strategies for Higher Order Tensors: Implications\n  for Compression and Recovery of Multi-linear Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose novel methods for compression and recovery of\nmultilinear data under limited sampling. We exploit the recently proposed\ntensor- Singular Value Decomposition (t-SVD)[1], which is a group theoretic\nframework for tensor decomposition. In contrast to popular existing tensor\ndecomposition techniques such as higher-order SVD (HOSVD), t-SVD has optimality\nproperties similar to the truncated SVD for matrices. Based on t-SVD, we first\nconstruct novel tensor-rank like measures to characterize informational and\nstructural complexity of multilinear data. Following that we outline a\ncomplexity penalized algorithm for tensor completion from missing entries. As\nan application, 3-D and 4-D (color) video data compression and recovery are\nconsidered. We show that videos with linear camera motion can be represented\nmore efficiently using t-SVD compared to traditional approaches based on\nvectorizing or flattening of the tensors. Application of the proposed tensor\ncompletion algorithm for video recovery from missing entries is shown to yield\na superior performance over existing methods. In conclusion we point out\nseveral research directions and implications to online prediction of\nmultilinear data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 19:39:51 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2013 16:58:28 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2013 02:06:16 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Zhang", "Zemin", ""], ["Ely", "Gregory", ""], ["Aeron", "Shuchin", ""], ["Hao", "Ning", ""], ["Kilmer", "Misha", ""]]}, {"id": "1307.0937", "submitter": "Riadh Bouslimi", "authors": "Mouhamed Gaith Ayadi, Riadh Bouslimi, Jalel Akaichi", "title": "Extending UML for Conceptual Modeling of Annotation of Medical Images", "comments": "9 pages, 6 figures. In International Journal of Computer\n  Applications, June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging has occupied a huge role in the management of patients, whether\nhospitalized or not. Depending on the patients clinical problem, a variety of\nimaging modalities were available for use. This gave birth of the annotation of\nmedical image process. The annotation is intended to image analysis and solve\nthe problem of semantic gap. The reason for image annotation is due to increase\nin acquisition of images. Physicians and radiologists feel better while using\nannotation techniques for faster remedy in surgery and medicine due to the\nfollowing reasons: giving details to the patients, searching the present and\npast records from the larger databases, and giving solutions to them in a\nfaster and more accurate way. However, classical conceptual modeling does not\nincorporate the specificity of medical domain specially the annotation of\nmedical image. The design phase is the most important activity in the\nsuccessful building of annotation process. For this reason, we focus in this\npaper on presenting the conceptual modeling of the annotation of medical image\nby defining a new profile using the StarUML extensibility mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 08:45:37 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Ayadi", "Mouhamed Gaith", ""], ["Bouslimi", "Riadh", ""], ["Akaichi", "Jalel", ""]]}, {"id": "1307.0998", "submitter": "Ziqiang Chen", "authors": "F. Lu and Z. Chen", "title": "A Unified Framework of Elementary Geometric Transformation\n  Representation", "comments": "26 pages, 11 figures, 1 table, 21 referneces", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an extension of projective homology, stereohomology is proposed via an\nextension of Desargues theorem and the extended Desargues configuration.\nGeometric transformations such as reflection, translation, central symmetry,\ncentral projection, parallel projection, shearing, central dilation, scaling,\nand so on are all included in stereohomology and represented as\nHouseholder-Chen elementary matrices. Hence all these geometric transformations\nare called elementary. This makes it possible to represent these elementary\ngeometric transformations in homogeneous square matrices independent of a\nparticular choice of coordinate system.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 12:59:53 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 04:28:51 GMT"}, {"version": "v3", "created": "Tue, 1 Jul 2014 00:24:20 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Lu", "F.", ""], ["Chen", "Z.", ""]]}, {"id": "1307.1166", "submitter": "Firas Ajil Jassim", "authors": "Firas A. Jassim", "title": "A Novel Robust Method to Add Watermarks to Bitmap Images by Fading\n  Technique", "comments": "5 pages, 6 figures, 1 table", "journal-ref": "International Journal of Multidisciplinary Sciences and\n  Engineering, Vol. 4, No. 5, pp. 43-47, June 2013", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Digital water marking is one of the essential fields in image security and\ncopyright protection. The proposed technique in this paper was based on the\nprinciple of protecting images by hide an invisible watermark in the image. The\ntechnique starts with merging the cover image and the watermark image with\nsuitable ratios, i.e., 99% from the cover image will be merged with 1% from the\nwatermark image. Technically, the fading process is irreversible but with the\nproposed technique, the probability to reconstruct the original watermark image\nis great. There is no perceptible difference between the original and\nwatermarked image by human eye. The experimental results show that the proposed\ntechnique proven its ability to hide images that have the same size of the\ncover image. Three performance measures were implemented to support the\nproposed techniques which are MSE, PSNR, and SSIM. Fortunately, all the three\nmeasures have excellent values.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 21:35:17 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Jassim", "Firas A.", ""]]}, {"id": "1307.1289", "submitter": "Miguel Angel Veganzones", "authors": "Miguel Angel Veganzones (GIPSA), Mihai Datcu (DLR), Manuel Gra\\~na\n  (GIC)", "title": "Further results on dissimilarity spaces for hyperspectral images RF-CBIR", "comments": "In Pattern Recognition Letters (2013)", "journal-ref": "Pattern Recognition Letters 34, 14 (2013) 1659-1668", "doi": "10.1016/j.patrec.2013.05.025", "report-no": "veganzones_PRL2013", "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-Based Image Retrieval (CBIR) systems are powerful search tools in\nimage databases that have been little applied to hyperspectral images.\nRelevance feedback (RF) is an iterative process that uses machine learning\ntechniques and user's feedback to improve the CBIR systems performance. We\npursued to expand previous research in hyperspectral CBIR systems built on\ndissimilarity functions defined either on spectral and spatial features\nextracted by spectral unmixing techniques, or on dictionaries extracted by\ndictionary-based compressors. These dissimilarity functions were not suitable\nfor direct application in common machine learning techniques. We propose to use\na RF general approach based on dissimilarity spaces which is more appropriate\nfor the application of machine learning algorithms to the hyperspectral\nRF-CBIR. We validate the proposed RF method for hyperspectral CBIR systems over\na real hyperspectral dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 11:58:04 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Veganzones", "Miguel Angel", "", "GIPSA"], ["Datcu", "Mihai", "", "DLR"], ["Gra\u00f1a", "Manuel", "", "GIC"]]}, {"id": "1307.1303", "submitter": "Toufiq Parag", "authors": "Toufiq Parag (Janelia Farm Research Campus-HHMI)", "title": "Submodularity of a Set Label Disagreement Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set label disagreement function is defined over the number of variables\nthat deviates from the dominant label. The dominant label is the value assumed\nby the largest number of variables within a set of binary variables. The\nsubmodularity of a certain family of set label disagreement function is\ndiscussed in this manuscript. Such disagreement function could be utilized as a\ncost function in combinatorial optimization approaches for problems defined\nover hypergraphs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 15:25:09 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Parag", "Toufiq", "", "Janelia Farm Research Campus-HHMI"]]}, {"id": "1307.1437", "submitter": "Yuqian Zhang", "authors": "Yuqian Zhang, Cun Mu, Han-wen Kuo and John Wright", "title": "Toward Guaranteed Illumination Models for Non-Convex Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illumination variation remains a central challenge in object detection and\nrecognition. Existing analyses of illumination variation typically pertain to\nconvex, Lambertian objects, and guarantee quality of approximation in an\naverage case sense. We show that it is possible to build V(vertex)-description\nconvex cone models with worst-case performance guarantees, for non-convex\nLambertian objects. Namely, a natural verification test based on the angle to\nthe constructed cone guarantees to accept any image which is sufficiently\nwell-approximated by an image of the object under some admissible lighting\ncondition, and guarantees to reject any image that does not have a sufficiently\ngood approximation. The cone models are generated by sampling point\nilluminations with sufficient density, which follows from a new perturbation\nbound for point images in the Lambertian model. As the number of point images\nrequired for guaranteed verification may be large, we introduce a new\nformulation for cone preserving dimensionality reduction, which leverages tools\nfrom sparse and low-rank decomposition to reduce the complexity, while\ncontrolling the approximation error with respect to the original cone.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 18:08:19 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Zhang", "Yuqian", ""], ["Mu", "Cun", ""], ["Kuo", "Han-wen", ""], ["Wright", "John", ""]]}, {"id": "1307.1561", "submitter": "Vimina E R", "authors": "E. R. Vimina, K. Poulose Jacob", "title": "A Sub-block Based Image Retrieval Using Modified Integrated Region\n  Matching", "comments": "7 pages", "journal-ref": "International Journal of Computer Science Issues, Vol.10, Issue\n  1,No 2, January 2013, pp. 686-692", "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a content based image retrieval (CBIR) system using the\nlocal colour and texture features of selected image sub-blocks and global\ncolour and shape features of the image. The image sub-blocks are roughly\nidentified by segmenting the image into partitions of different configuration,\nfinding the edge density in each partition using edge thresholding followed by\nmorphological dilation. The colour and texture features of the identified\nregions are computed from the histograms of the quantized HSV colour space and\nGray Level Co- occurrence Matrix (GLCM) respectively. The colour and texture\nfeature vectors is computed for each region. The shape features are computed\nfrom the Edge Histogram Descriptor (EHD). A modified Integrated Region Matching\n(IRM) algorithm is used for finding the minimum distance between the sub-blocks\nof the query and target image. Experimental results show that the proposed\nmethod provides better retrieving result than retrieval using some of the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 09:29:46 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Vimina", "E. R.", ""], ["Jacob", "K. Poulose", ""]]}, {"id": "1307.1739", "submitter": "Xin Zhao", "authors": "Xin Zhao and Arie Kaufman", "title": "Anatomical Feature-guided Volumeric Registration of Multimodal Prostate\n  MRI", "comments": "This paper has been withdrawn by the author due to publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiological imaging of prostate is becoming more popular among researchers\nand clinicians in searching for diseases, primarily cancer. Scans might be\nacquired at different times, with patient movement between scans, or with\ndifferent equipment, resulting in multiple datasets that need to be registered.\nFor this issue, we introduce a registration method using anatomical\nfeature-guided mutual information. Prostate scans of the same patient taken in\nthree different orientations are first aligned for the accurate detection of\nanatomical features in 3D. Then, our pipeline allows for multiple modalities\nregistration through the use of anatomical features, such as the interior\nurethra of prostate and gland utricle, in a bijective way. The novelty of this\napproach is the application of anatomical features as the pre-specified\ncorresponding landmarks for prostate registration. We evaluate the registration\nresults through both artificial and clinical datasets. Registration accuracy is\nevaluated by performing statistical analysis of local intensity differences or\nspatial differences of anatomical landmarks between various MR datasets.\nEvaluation results demonstrate that our method statistics-significantly\nimproves the quality of registration. Although this strategy is tested for\nMRI-guided brachytherapy, the preliminary results from these experiments\nsuggest that it can be also applied to other settings such as transrectal\nultrasound-guided or CT-guided therapy, where the integration of preoperative\nMRI may have a significant impact upon treatment planning and guidance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 00:30:40 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2013 21:38:08 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Zhao", "Xin", ""], ["Kaufman", "Arie", ""]]}, {"id": "1307.2434", "submitter": "Firouz AL-Wassai", "authors": "Firouz A. Al-Wassai, N.V. Kalyankar", "title": "Major Limitations of Satellite images", "comments": null, "journal-ref": "Journal of Global Research in Computer Science, 4 (5), May 2013,\n  51-59", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing has proven to be a powerful tool for the monitoring of the\nEarth surface to improve our perception of our surroundings has led to\nunprecedented developments in sensor and information technologies. However,\ntechnologies for effective use of the data and for extracting useful\ninformation from the data of Remote sensing are still very limited since no\nsingle sensor combines the optimal spectral, spatial and temporal resolution.\nThis paper briefly reviews the limitations of satellite remote sensing. Also,\nreviews on the problems of image fusion techniques. The conclusion of this,\nAccording to literature, the remote sensing is still the lack of software tools\nfor effective information extraction from remote sensing data. The trade-off in\nspectral and spatial resolution will remain and new advanced data fusion\napproaches are needed to make optimal use of remote sensors for extract the\nmost useful information.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 13:01:46 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Al-Wassai", "Firouz A.", ""], ["Kalyankar", "N. V.", ""]]}, {"id": "1307.2440", "submitter": "Firouz AL-Wassai", "authors": "Firouz Abdullah Al-Wassai, N.V. Kalyankar", "title": "Image Fusion Technologies In Commercial Remote Sensing Packages", "comments": "Keywords: Commercial Processing Systems, Image Fusion, quality\n  evaluation", "journal-ref": "Journal of Global Research in Computer Science, 4 (5), May 2013,\n  44-50", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several remote sensing software packages are used to the explicit purpose of\nanalyzing and visualizing remotely sensed data, with the developing of remote\nsensing sensor technologies from last ten years. Accord-ing to literature, the\nremote sensing is still the lack of software tools for effective information\nextraction from remote sensing data. So, this paper provides a state-of-art of\nmulti-sensor image fusion technologies as well as review on the quality\nevaluation of the single image or fused images in the commercial remote sensing\npack-ages. It also introduces program (ALwassaiProcess) developed for image\nfusion and classification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 13:14:11 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Al-Wassai", "Firouz Abdullah", ""], ["Kalyankar", "N. V.", ""]]}, {"id": "1307.2457", "submitter": "Roxana Bujack", "authors": "Roxana Bujack and Gerik Scheuermann and Eckhard Hitzer", "title": "Detection of Outer Rotations on 3D-Vector Fields with Iterative\n  Geometric Correlation and its Efficiency", "comments": "accepted for publication in Advances in Applied Clifford Algebras,\n  (2013). arXiv admin note: substantial text overlap with arXiv:1306.2195", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation is a common technique for the detection of shifts. Its\ngeneralization to the multidimensional geometric correlation in Clifford\nalgebras has been proven a useful tool for color image processing, because it\nadditionally contains information about a rotational misalignment. But so far\nthe exact correction of a three-dimensional outer rotation could only be\nachieved in certain special cases. In this paper we prove that applying the\ngeometric correlation iteratively has the potential to detect the outer\nrotational misalignment for arbitrary three-dimensional vector fields. We\nfurther present the explicit iterative algorithm, analyze its efficiency\ndetecting the rotational misalignment in the color space of a color image. The\nexperiments suggest a method for the acceleration of the algorithm, which is\npractically tested with great success.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 14:18:38 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Bujack", "Roxana", ""], ["Scheuermann", "Gerik", ""], ["Hitzer", "Eckhard", ""]]}, {"id": "1307.2560", "submitter": "Tejaswi Agarwal", "authors": "Saurabh Jha, Tejaswi Agarwal and B. Rajesh Kanna", "title": "Exploiting Data Parallelism in the yConvex Hypergraph Algorithm for\n  Image Representation using GPGPUs", "comments": "1 page, 1 figure published in Proceedings of the 27th ACM\n  International Conference on Supercomputing, ICS 2013, Eugene, Oregon, USA", "journal-ref": "ACM 978-1-4503-2130-3/13/06 2013", "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To define and identify a region-of-interest (ROI) in a digital image, the\nshape descriptor of the ROI has to be described in terms of its boundary\ncharacteristics. To address the generic issues of contour tracking, the yConvex\nHypergraph (yCHG) model was proposed by Kanna et al [1]. In this work, we\npropose a parallel approach to implement the yCHG model by exploiting massively\nparallel cores of NVIDIA's Compute Unified Device Architecture (CUDA). We\nperform our experiments on the MODIS satellite image database by NASA, and\nbased on our analysis we observe that the performance of the serial\nimplementation is better on smaller images, but once the threshold is achieved\nin terms of image resolution, the parallel implementation outperforms its\nsequential counterpart by 2 to 10 times (2x-10x). We also conclude that an\nincrease in the number of hyperedges in the ROI of a given size does not impact\nthe performance of the overall algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 22:31:49 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Jha", "Saurabh", ""], ["Agarwal", "Tejaswi", ""], ["Kanna", "B. Rajesh", ""]]}, {"id": "1307.2818", "submitter": "Vinay Kumar Dr.", "authors": "Harbinder Singh, Vinay Kumar and Sunil Bhooshan", "title": "Anisotropic Diffusion for Details Enhancement in Multi-Exposure Image\n  Fusion", "comments": "30 pages", "journal-ref": "ISRN Signal Processing, vol. 2013, Article ID 928971, 18 pages,\n  2013", "doi": "10.1155/2013/928971", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a multiexposure image fusion method based on texture features,\nwhich exploits the edge preserving and intraregion smoothing property of\nnonlinear diffusion filters based on partial differential equations (PDE). With\nthe captured multiexposure image series, we first decompose images into base\nlayers and detail layers to extract sharp details and fine details,\nrespectively. The magnitude of the gradient of the image intensity is utilized\nto encourage smoothness at homogeneous regions in preference to inhomogeneous\nregions. Then, we have considered texture features of the base layer to\ngenerate a mask (i.e., decision mask) that guides the fusion of base layers in\nmultiresolution fashion. Finally, well-exposed fused image is obtained that\ncombines fused base layer and the detail layers at each scale across all the\ninput exposures. Proposed algorithm skipping complex High Dynamic Range Image\n(HDRI) generation and tone mapping steps to produce detail preserving image for\ndisplay on standard dynamic range display devices. Moreover, our technique is\neffective for blending flash/no-flash image pair and multifocus images, that\nis, images focused on different targets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 15:07:26 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Singh", "Harbinder", ""], ["Kumar", "Vinay", ""], ["Bhooshan", "Sunil", ""]]}, {"id": "1307.2965", "submitter": "Quan Wang", "authors": "Quan Wang, Dijia Wu, Le Lu, Meizhu Liu, Kim L. Boyer and Shaohua Kevin\n  Zhou", "title": "Semantic Context Forests for Learning-Based Knee Cartilage Segmentation\n  in 3D MR Images", "comments": "MICCAI 2013: Workshop on Medical Computer Vision", "journal-ref": null, "doi": "10.1007/978-3-319-05530-5_11", "report-no": null, "categories": "cs.CV cs.LG q-bio.TO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic segmentation of human knee cartilage from 3D MR images is a\nuseful yet challenging task due to the thin sheet structure of the cartilage\nwith diffuse boundaries and inhomogeneous intensities. In this paper, we\npresent an iterative multi-class learning method to segment the femoral, tibial\nand patellar cartilage simultaneously, which effectively exploits the spatial\ncontextual constraints between bone and cartilage, and also between different\ncartilages. First, based on the fact that the cartilage grows in only certain\narea of the corresponding bone surface, we extract the distance features of not\nonly to the surface of the bone, but more informatively, to the densely\nregistered anatomical landmarks on the bone surface. Second, we introduce a set\nof iterative discriminative classifiers that at each iteration, probability\ncomparison features are constructed from the class confidence maps derived by\npreviously learned classifiers. These features automatically embed the semantic\ncontext information between different cartilages of interest. Validated on a\ntotal of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, the\nproposed approach demonstrates high robustness and accuracy of segmentation in\ncomparison with existing state-of-the-art MR cartilage segmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 03:29:51 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 16:01:12 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Wang", "Quan", ""], ["Wu", "Dijia", ""], ["Lu", "Le", ""], ["Liu", "Meizhu", ""], ["Boyer", "Kim L.", ""], ["Zhou", "Shaohua Kevin", ""]]}, {"id": "1307.2971", "submitter": "Ana Georgina Flesia MS", "authors": "Ana Georgina Flesia, Josef Baumgartner, Javier Gimenez, Jorge Martinez", "title": "Accuracy of MAP segmentation with hidden Potts and Markov mesh prior\n  models via Path Constrained Viterbi Training, Iterated Conditional Modes and\n  Graph Cut based algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study statistical classification accuracy of two different\nMarkov field environments for pixelwise image segmentation, considering the\nlabels of the image as hidden states and solving the estimation of such labels\nas a solution of the MAP equation. The emission distribution is assumed the\nsame in all models, and the difference lays in the Markovian prior hypothesis\nmade over the labeling random field. The a priori labeling knowledge will be\nmodeled with a) a second order anisotropic Markov Mesh and b) a classical\nisotropic Potts model. Under such models, we will consider three different\nsegmentation procedures, 2D Path Constrained Viterbi training for the Hidden\nMarkov Mesh, a Graph Cut based segmentation for the first order isotropic Potts\nmodel, and ICM (Iterated Conditional Modes) for the second order isotropic\nPotts model.\n  We provide a unified view of all three methods, and investigate goodness of\nfit for classification, studying the influence of parameter estimation,\ncomputational gain, and extent of automation in the statistical measures\nOverall Accuracy, Relative Improvement and Kappa coefficient, allowing robust\nand accurate statistical analysis on synthetic and real-life experimental data\ncoming from the field of Dental Diagnostic Radiography. All algorithms, using\nthe learned parameters, generate good segmentations with little interaction\nwhen the images have a clear multimodal histogram. Suboptimal learning proves\nto be frail in the case of non-distinctive modes, which limits the complexity\nof usable models, and hence the achievable error rate as well.\n  All Matlab code written is provided in a toolbox available for download from\nour website, following the Reproducible Research Paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 04:49:11 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Flesia", "Ana Georgina", ""], ["Baumgartner", "Josef", ""], ["Gimenez", "Javier", ""], ["Martinez", "Jorge", ""]]}, {"id": "1307.2982", "submitter": "Mohammad Norouzi", "authors": "Mohammad Norouzi, Ali Punjani, David J. Fleet", "title": "Fast Exact Search in Hamming Space with Multi-Index Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in representing image data and feature descriptors\nusing compact binary codes for fast near neighbor search. Although binary codes\nare motivated by their use as direct indices (addresses) into a hash table,\ncodes longer than 32 bits are not being used as such, as it was thought to be\nineffective. We introduce a rigorous way to build multiple hash tables on\nbinary code substrings that enables exact k-nearest neighbor search in Hamming\nspace. The approach is storage efficient and straightforward to implement.\nTheoretical analysis shows that the algorithm exhibits sub-linear run-time\nbehavior for uniformly distributed codes. Empirical results show dramatic\nspeedups over a linear scan baseline for datasets of up to one billion codes of\n64, 128, or 256 bits.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 05:52:21 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2013 02:36:21 GMT"}, {"version": "v3", "created": "Fri, 25 Apr 2014 01:31:55 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Norouzi", "Mohammad", ""], ["Punjani", "Ali", ""], ["Fleet", "David J.", ""]]}, {"id": "1307.2997", "submitter": "Padmavathi S", "authors": "S. Padmavathi, Manojna K.S.S, S. Sphoorthy Reddy, D. Meenakshy", "title": "Conversion of Braille to Text in English, Hindi and Tamil Languages", "comments": "14 pages, 20 figures, 4 tables", "journal-ref": "International Journal of Computer Science, Engineering and\n  Applications (IJCSEA) Vol.3, No.3, June 2013", "doi": "10.5121/ijcsea.2013.3303", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Braille system has been used by the visually impaired for reading and\nwriting. Due to limited availability of the Braille text books an efficient\nusage of the books becomes a necessity. This paper proposes a method to convert\na scanned Braille document to text which can be read out to many through the\ncomputer. The Braille documents are pre processed to enhance the dots and\nreduce the noise. The Braille cells are segmented and the dots from each cell\nis extracted and converted in to a number sequence. These are mapped to the\nappropriate alphabets of the language. The converted text is spoken out through\na speech synthesizer. The paper also provides a mechanism to type the Braille\ncharacters through the number pad of the keyboard. The typed Braille character\nis mapped to the alphabet and spoken out. The Braille cell has a standard\nrepresentation but the mapping differs for each language. In this paper mapping\nof English, Hindi and Tamil are considered.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 07:24:16 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Padmavathi", "S.", ""], ["S", "Manojna K. S.", ""], ["Reddy", "S. Sphoorthy", ""], ["Meenakshy", "D.", ""]]}, {"id": "1307.3040", "submitter": "Mehul Bhatt", "authors": "Mehul Bhatt", "title": "Between Sense and Sensibility: Declarative narrativisation of mental\n  models as a basis and benchmark for visuo-spatial cognition and computation\n  focussed collaborative cognitive systems", "comments": "5 pages, research statement summarising recent publications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What lies between `\\emph{sensing}' and `\\emph{sensibility}'? In other words,\nwhat kind of cognitive processes mediate sensing capability, and the formation\nof sensible impressions ---e.g., abstractions, analogies, hypotheses and theory\nformation, beliefs and their revision, argument formation--- in domain-specific\nproblem solving, or in regular activities of everyday living, working and\nsimply going around in the environment? How can knowledge and reasoning about\nsuch capabilities, as exhibited by humans in particular problem contexts, be\nused as a model and benchmark for the development of collaborative cognitive\n(interaction) systems concerned with human assistance, assurance, and\nempowerment?\n  We pose these questions in the context of a range of assistive technologies\nconcerned with \\emph{visuo-spatial perception and cognition} tasks encompassing\naspects such as commonsense, creativity, and the application of specialist\ndomain knowledge and problem-solving thought processes. Assistive technologies\nbeing considered include: (a) human activity interpretation; (b) high-level\ncognitive rovotics; (c) people-centred creative design in domains such as\narchitecture & digital media creation, and (d) qualitative analyses geographic\ninformation systems. Computational narratives not only provide a rich cognitive\nbasis, but they also serve as a benchmark of functional performance in our\ndevelopment of computational cognitive assistance systems. We posit that\ncomputational narrativisation pertaining to space, actions, and change provides\na useful model of \\emph{visual} and \\emph{spatio-temporal thinking} within a\nwide-range of problem-solving tasks and application areas where collaborative\ncognitive systems could serve an assistive and empowering function.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 10:01:29 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 10:22:29 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Bhatt", "Mehul", ""]]}, {"id": "1307.3043", "submitter": "Sergey Kosov", "authors": "Sergey Kosov and Pushmeet Kohli and Franz Rottensteiner and Christian\n  Heipke", "title": "A two-layer Conditional Random Field for the classification of partially\n  occluded objects", "comments": "Conference Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Random Fields (CRF) are among the most popular techniques for\nimage labelling because of their flexibility in modelling dependencies between\nthe labels and the image features. This paper proposes a novel CRF-framework\nfor image labeling problems which is capable to classify partially occluded\nobjects. Our approach is evaluated on aerial near-vertical images as well as on\nurban street-view images and compared with another methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 10:07:19 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2013 15:48:23 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Kosov", "Sergey", ""], ["Kohli", "Pushmeet", ""], ["Rottensteiner", "Franz", ""], ["Heipke", "Christian", ""]]}, {"id": "1307.3054", "submitter": "Sucheta Shrivastava", "authors": "Sayali Nimkar, Sanal Varghese, Sucheta Shrivastava", "title": "Contrast Enhancement And Brightness Preservation Using Multi-\n  Decomposition Histogram Equalization", "comments": "9 pages,13 figures", "journal-ref": "SIPIJ, Vol.4, Issue.3, pp. 85-93", "doi": "10.5121/sipij.2013.4308", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histogram Equalization (HE) has been an essential addition to the Image\nEnhancement world. Enhancement techniques like Classical Histogram Equalization\n(CHE), Adaptive Histogram Equalization (ADHE), Bi-Histogram Equalization (BHE)\nand Recursive Mean Separate Histogram Equalization (RMSHE) methods enhance\ncontrast, however, brightness is not well preserved with these methods, which\ngives an unpleasant look to the final image obtained. Thus, we introduce a\nnovel technique Multi-Decomposition Histogram Equalization (MDHE) to eliminate\nthe drawbacks of the earlier methods. In MDHE, we have decomposed the input\nsixty-four parts, applied CHE in each of the sub-images and then finally\ninterpolated them in correct order. The final image after MDHE results in\ncontrast enhanced and brightness preserved image compared to all other\ntechniques mentioned above. We have calculated the various parameters like\nPSNR, SNR, RMSE, MSE, etc. for every technique. Our results are well supported\nby bar graphs, histograms and the parameter calculations at the end.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 11:02:57 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Nimkar", "Sayali", ""], ["Varghese", "Sanal", ""], ["Shrivastava", "Sucheta", ""]]}, {"id": "1307.3271", "submitter": "Thomas Schultz", "authors": "Thomas Schultz and Anna Vilanova and Ralph Brecheisen and Gordon\n  Kindlmann", "title": "Fuzzy Fibers: Uncertainty in dMRI Tractography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiber tracking based on diffusion weighted Magnetic Resonance Imaging (dMRI)\nallows for noninvasive reconstruction of fiber bundles in the human brain. In\nthis chapter, we discuss sources of error and uncertainty in this technique,\nand review strategies that afford a more reliable interpretation of the\nresults. This includes methods for computing and rendering probabilistic\ntractograms, which estimate precision in the face of measurement noise and\nartifacts. However, we also address aspects that have received less attention\nso far, such as model selection, partial voluming, and the impact of\nparameters, both in preprocessing and in fiber tracking itself. We conclude by\ngiving impulses for future research.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 21:01:23 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Schultz", "Thomas", ""], ["Vilanova", "Anna", ""], ["Brecheisen", "Ralph", ""], ["Kindlmann", "Gordon", ""]]}, {"id": "1307.3439", "submitter": "Y Jayanta Singh", "authors": "Y. Jayanta Singh, Shalu Gupta", "title": "Speedy Object Detection based on Shape", "comments": "arXiv admin note: text overlap with arXiv:1210.7038 by other authors", "journal-ref": "The International Journal of Multimedia & Its Applications (IJMA)\n  Vol.5, No.3, June 2013", "doi": "10.5121/ijma.2013.5302", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This study is a part of design of an audio system for in-house object\ndetection system for visually impaired, low vision personnel by birth or by an\naccident or due to old age. The input of the system will be scene and output as\naudio. Alert facility is provided based on severity levels of the objects\n(snake, broke glass etc) and also during difficulties. The study proposed\ntechniques to provide speedy detection of objects based on shapes and its\nscale. Features are extraction to have minimum spaces using dynamic scaling.\nFrom a scene, clusters of objects are formed based on the scale and shape.\nSearching is performed among the clusters initially based on the shape, scale,\nmean cluster value and index of object(s). The minimum operation to detect the\npossible shape of the object is performed. In case the object does not have a\nlikely matching shape, scale etc, then the several operations required for an\nobject detection will not perform; instead, it will declared as a new object.\nIn such way, this study finds a speedy way of detecting objects.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 12:37:06 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Singh", "Y. Jayanta", ""], ["Gupta", "Shalu", ""]]}, {"id": "1307.3581", "submitter": "Li He", "authors": "Li He, Hairong Qi, Russell Zaretzki", "title": "Image color transfer to evoke different emotions based on color\n  combinations", "comments": "Signal, Image and Video Processing, September 2014", "journal-ref": null, "doi": "10.1007/s11760-014-0691-y", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a color transfer framework to evoke different emotions for\nimages based on color combinations is proposed. The purpose of this color\ntransfer is to change the \"look and feel\" of images, i.e., evoking different\nemotions. Colors are confirmed as the most attractive factor in images. In\naddition, various studies in both art and science areas have concluded that\nother than single color, color combinations are necessary to evoke specific\nemotions. Therefore, we propose a novel framework to transfer color of images\nbased on color combinations, using a predefined color emotion model. The\ncontribution of this new framework is three-fold. First, users do not need to\nprovide reference images as used in traditional color transfer algorithms. In\nmost situations, users may not have enough aesthetic knowledge or path to\nchoose desired reference images. Second, because of the usage of color\ncombinations instead of single color for emotions, a new color transfer\nalgorithm that does not require an image library is proposed. Third, again\nbecause of the usage of color combinations, artifacts that are normally seen in\ntraditional frameworks using single color are avoided. We present encouraging\nresults generated from this new framework and its potential in several possible\napplications including color transfer of photos and paintings.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 21:20:49 GMT"}, {"version": "v2", "created": "Sun, 2 Nov 2014 05:26:54 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["He", "Li", ""], ["Qi", "Hairong", ""], ["Zaretzki", "Russell", ""]]}, {"id": "1307.3755", "submitter": "Nikesh Dattani", "authors": "Lila Kari (1), Kathleen A. Hill (2), Abu Sadat Sayem (1), Nathaniel\n  Bryans (3), Katelyn Davis (2), Nikesh S. Dattani (4), ((1) Department of\n  Computer Science, University of Western Ontario, Canada, (2) Department of\n  Biology, University of Western Ontario, Canada, (3) Microsoft Corporation,\n  (4) Department of Chemistry, Oxford University, UK)", "title": "Map of Life: Measuring and Visualizing Species' Relatedness with\n  \"Molecular Distance Maps\"", "comments": "13 pages, 8 figures. Funded by: NSERC/CRSNG (Natural Science &\n  Engineering Research Council of Canada / Conseil de recherches en sciences\n  naturelles et en g\\'enie du Canada), and the Oxford University Press.\n  Acknowledgements: Ronghai Tu, Tao Tao, Steffen Kopecki, Andre Lachance,\n  Jeremy McNeil, Greg Thorn, Oxford University Mathematical Institute", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CV q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel combination of methods that (i) portrays quantitative\ncharacteristics of a DNA sequence as an image, (ii) computes distances between\nthese images, and (iii) uses these distances to output a map wherein each\nsequence is a point in a common Euclidean space. In the resulting \"Molecular\nDistance Map\" each point signifies a DNA sequence, and the geometric distance\nbetween any two points reflects the degree of relatedness between the\ncorresponding sequences and species.\n  Molecular Distance Maps present compelling visual representations of\nrelationships between species and could be used for taxonomic clarifications,\nfor species identification, and for studies of evolutionary history. One of the\nadvantages of this method is its general applicability since, as sequence\nalignment is not required, the DNA sequences chosen for comparison can be\ncompletely different regions in different genomes. In fact, this method can be\nused to compare any two DNA sequences. For example, in our dataset of 3,176\nmitochondrial DNA sequences, it correctly finds the mtDNA sequences most\nclosely related to that of the anatomically modern human (the Neanderthal, the\nDenisovan, and the chimp), and it finds that the sequence most different from\nit belongs to a cucumber. Furthermore, our method can be used to compare real\nsequences to artificial, computer-generated, DNA sequences. For example, it is\nused to determine that the distances between a Homo sapiens sapiens mtDNA and\nartificial sequences of the same length and same trinucleotide frequencies can\nbe larger than the distance between the same human mtDNA and the mtDNA of a\nfruit-fly.\n  We demonstrate this method's promising potential for taxonomical\nclarifications by applying it to a diverse variety of cases that have been\nhistorically controversial, such as the genus Polypterus, the family Tarsiidae,\nand the vast (super)kingdom Protista.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 17:16:57 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Kari", "Lila", ""], ["Hill", "Kathleen A.", ""], ["Sayem", "Abu Sadat", ""], ["Bryans", "Nathaniel", ""], ["Davis", "Katelyn", ""], ["Dattani", "Nikesh S.", ""]]}, {"id": "1307.3759", "submitter": "Evgeniy Martyushev", "authors": "Evgeniy Martyushev", "title": "A Minimal Six-Point Auto-Calibration Algorithm", "comments": "7 pages, 4 figures", "journal-ref": "Proceedings of the 23rd International Conference on Computer\n  Graphics and Vision, September 16-20, 2013 Vladivostok, Russia", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-iterative auto-calibration algorithm is presented. It deals with a\nminimal set of six scene points in three views taken by a camera with fixed but\nunknown intrinsic parameters. Calibration is based on the image correspondences\nonly. The algorithm is implemented and validated on synthetic image data.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 17:37:36 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Martyushev", "Evgeniy", ""]]}, {"id": "1307.3782", "submitter": "Karim Ahmed", "authors": "Karim M. Mahmoud", "title": "Handwritten Digits Recognition using Deep Convolutional Neural Network:\n  An Experimental Study using EBlearn", "comments": "This paper has been withdrawn by the author due to some errors and\n  incomplete study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, results of an experimental study of a deep convolution neural\nnetwork architecture which can classify different handwritten digits using\nEBLearn library are reported. The purpose of this neural network is to classify\ninput images into 10 different classes or digits (0-9) and to explore new\nfindings. The input dataset used consists of digits images of size 32X32 in\ngrayscale (MNIST dataset).\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 21:03:39 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 16:05:33 GMT"}, {"version": "v3", "created": "Fri, 22 Apr 2016 18:45:01 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Mahmoud", "Karim M.", ""]]}, {"id": "1307.3811", "submitter": "Weifeng Liu", "authors": "Weifeng Liu, Dacheng Tao, Jun Cheng, and Yuanyan Tang", "title": "Multiview Hessian Discriminative Sparse Coding for Image Annotation", "comments": "35 pages", "journal-ref": "Computer vision and image understanding,118(2014) 50-60", "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding represents a signal sparsely by using an overcomplete\ndictionary, and obtains promising performance in practical computer vision\napplications, especially for signal restoration tasks such as image denoising\nand image inpainting. In recent years, many discriminative sparse coding\nalgorithms have been developed for classification problems, but they cannot\nnaturally handle visual data represented by multiview features. In addition,\nexisting sparse coding algorithms use graph Laplacian to model the local\ngeometry of the data distribution. It has been identified that Laplacian\nregularization biases the solution towards a constant function which possibly\nleads to poor extrapolating power. In this paper, we present multiview Hessian\ndiscriminative sparse coding (mHDSC) which seamlessly integrates Hessian\nregularization with discriminative sparse coding for multiview learning\nproblems. In particular, mHDSC exploits Hessian regularization to steer the\nsolution which varies smoothly along geodesics in the manifold, and treats the\nlabel information as an additional view of feature for incorporating the\ndiscriminative power for image annotation. We conduct extensive experiments on\nPASCAL VOC'07 dataset and demonstrate the effectiveness of mHDSC for image\nannotation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 03:14:05 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Liu", "Weifeng", ""], ["Tao", "Dacheng", ""], ["Cheng", "Jun", ""], ["Tang", "Yuanyan", ""]]}, {"id": "1307.4048", "submitter": "Pavan Kumar D S", "authors": "D. S. Pavan Kumar, N. Vishnu Prasad, Vikas Joshi, S. Umesh", "title": "Modified SPLICE and its Extension to Non-Stereo Data for Noise Robust\n  Speech Recognition", "comments": "Submitted to Automatic Speech Recognition and Understanding (ASRU)\n  2013 Workshop", "journal-ref": null, "doi": "10.1109/ASRU.2013.6707725", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a modification to the training process of the popular SPLICE\nalgorithm has been proposed for noise robust speech recognition. The\nmodification is based on feature correlations, and enables this stereo-based\nalgorithm to improve the performance in all noise conditions, especially in\nunseen cases. Further, the modified framework is extended to work for\nnon-stereo datasets where clean and noisy training utterances, but not stereo\ncounterparts, are required. Finally, an MLLR-based computationally efficient\nrun-time noise adaptation method in SPLICE framework has been proposed. The\nmodified SPLICE shows 8.6% absolute improvement over SPLICE in Test C of\nAurora-2 database, and 2.93% overall. Non-stereo method shows 10.37% and 6.93%\nabsolute improvements over Aurora-2 and Aurora-4 baseline models respectively.\nRun-time adaptation shows 9.89% absolute improvement in modified framework as\ncompared to SPLICE for Test C, and 4.96% overall w.r.t. standard MLLR\nadaptation on HMMs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 18:39:10 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Kumar", "D. S. Pavan", ""], ["Prasad", "N. Vishnu", ""], ["Joshi", "Vikas", ""], ["Umesh", "S.", ""]]}, {"id": "1307.4516", "submitter": "Laurence Aroquiaraj", "authors": "I. Laurence Aroquiaraj, K. Thangavel", "title": "Mammogram Edge Detection Using Hybrid Soft Computing Methods", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is a crucial step in a wide range of method image\nprocessing systems. It is useful in visualization of the different objects\npresent in the image. In spite of the several methods available in the\nliterature, image segmentation still a challenging problem in most of image\nprocessing applications. The challenge comes from the fuzziness of image\nobjects and the overlapping of the different regions. Detection of edges in an\nimage is a very important step towards understanding image features. There are\nlarge numbers of edge detection operators available, each designed to be\nsensitive to certain types of edges. The Quality of edge detection can be\nmeasured from several criteria objectively. Some criteria are proposed in terms\nof mathematical measurement, some of them are based on application and\nimplementation requirements. Since edges often occur at image locations\nrepresenting object boundaries, edge detection is extensively used in image\nsegmentation when images are divided into areas corresponding to different\nobjects. This can be used specifically for enhancing the tumor area in\nmammographic images. Different methods are available for edge detection like\nRoberts, Sobel, Prewitt, Canny, Log edge operators. In this paper a novel\nalgorithms for edge detection has been proposed for mammographic images. Breast\nboundary, pectoral region and tumor location can be seen clearly by using this\nmethod. For comparison purpose Roberts, Sobel, Prewitt, Canny, Log edge\noperators are used and their results are displayed. Experimental results\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 06:45:23 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Aroquiaraj", "I. Laurence", ""], ["Thangavel", "K.", ""]]}, {"id": "1307.4592", "submitter": "Pierre Weiss", "authors": "J\\'er\\^ome Fehrenbach, Pierre Weiss", "title": "Processing stationary noise: model and parameter selection in\n  variational methods", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive or multiplicative stationary noise recently became an important\nissue in applied fields such as microscopy or satellite imaging. Relatively few\nworks address the design of dedicated denoising methods compared to the usual\nwhite noise setting. We recently proposed a variational algorithm to tackle\nthis issue. In this paper, we analyze this problem from a statistical point of\nview and provide deterministic properties of the solutions of the associated\nvariational problems. In the first part of this work, we demonstrate that in\nmany practical problems, the noise can be assimilated to a colored Gaussian\nnoise. We provide a quantitative measure of the distance between a stationary\nprocess and the corresponding Gaussian process. In the second part, we focus on\nthe Gaussian setting and analyze denoising methods which consist of minimizing\nthe sum of a total variation term and an $l^2$ data fidelity term. While the\nconstrained formulation of this problem allows to easily tune the parameters,\nthe Lagrangian formulation can be solved more efficiently since the problem is\nstrongly convex. Our second contribution consists in providing analytical\nvalues of the regularization parameter in order to approximately satisfy\nMorozov's discrepancy principle.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 12:14:52 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Fehrenbach", "J\u00e9r\u00f4me", ""], ["Weiss", "Pierre", ""]]}, {"id": "1307.4717", "submitter": "Laurence Aroquiaraj", "authors": "T. Dharani, I. Laurence Aroquiaraj", "title": "Content Based Image Retrieval System using Feature Classification with\n  Modified KNN Algorithm", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature means countenance, remote sensing scene objects with similar\ncharacteristics, associated to interesting scene elements in the image\nformation process. They are classified into three types in image processing,\nthat is low, middle and high. Low level features are color, texture and middle\nlevel feature is shape and high level feature is semantic gap of objects. An\nimage retrieval system is a computer system for browsing, searching and\nretrieving images from a large image database. Content Based Image Retrieval is\na technique which uses visual features of image such as color, shape, texture\nto search user required image from large image database according to user\nrequests in the form of a query. MKNN is an enhancing method of KNN. The\nproposed KNN classification is called MKNN. MKNN contains two parts for\nprocessing, they are validity of the train samples and applying weighted KNN.\nThe validity of each point is computed according to its neighbors. In our\nproposal, Modified K-Nearest Neighbor can be considered a kind of weighted KNN\nso that the query label is approximated by weighting the neighbors of the\nquery.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 18:22:24 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Dharani", "T.", ""], ["Aroquiaraj", "I. Laurence", ""]]}, {"id": "1307.4990", "submitter": "Purnendu  Banerjee", "authors": "Purnendu Banerjee and B. B. Chaudhuri", "title": "Video Text Localization using Wavelet and Shearlet Transforms", "comments": "arXiv admin note: text overlap with arXiv:1101.0553 by other authors", "journal-ref": null, "doi": "10.1117/12.2036077", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text in video is useful and important in indexing and retrieving the video\ndocuments efficiently and accurately. In this paper, we present a new method of\ntext detection using a combined dictionary consisting of wavelets and a\nrecently introduced transform called shearlets. Wavelets provide optimally\nsparse expansion for point-like structures and shearlets provide optimally\nsparse expansions for curve-like structures. By combining these two features we\nhave computed a high frequency sub-band to brighten the text part. Then K-means\nclustering is used for obtaining text pixels from the Standard Deviation (SD)\nof combined coefficient of wavelets and shearlets as well as the union of\nwavelets and shearlets features. Text parts are obtained by grouping\nneighboring regions based on geometric properties of the classified output\nframe of unsupervised K-means classification. The proposed method tested on a\nstandard as well as newly collected database shows to be superior to some\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 15:58:19 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2013 11:57:23 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Banerjee", "Purnendu", ""], ["Chaudhuri", "B. B.", ""]]}, {"id": "1307.5102", "submitter": "Stefano Gonella", "authors": "Stefano Gonella, Jarvis D. Haupt", "title": "Automated Defect Localization via Low Rank Plus Outlier Modeling of\n  Propagating Wavefield Data", "comments": "16 pages, 9 figures, Submitted to the IEEE Transactions on\n  Ultrasonics, Ferroelectrics and Frequency Control on August 30th 2012", "journal-ref": "IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency\n  Control, v. 60, n.12, pp. 2553 - 2565", "doi": "10.1109/TUFFC.2013.2854", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes an agnostic inference strategy for material diagnostics,\nconceived within the context of laser-based non-destructive evaluation methods,\nwhich extract information about structural anomalies from the analysis of\nacoustic wavefields measured on the structure's surface by means of a scanning\nlaser interferometer. The proposed approach couples spatiotemporal windowing\nwith low rank plus outlier modeling, to identify a priori unknown deviations in\nthe propagating wavefields caused by material inhomogeneities or defects, using\nvirtually no knowledge of the structural and material properties of the medium.\nThis characteristic makes the approach particularly suitable for diagnostics\nscenarios where the mechanical and material models are complex, unknown, or\nunreliable. We demonstrate our approach in a simulated environment using\nbenchmark point and line defect localization problems based on propagating\nflexural waves in a thin plate.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 00:06:59 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Gonella", "Stefano", ""], ["Haupt", "Jarvis D.", ""]]}, {"id": "1307.5161", "submitter": "Xavier Boix", "authors": "Gemma Roig, Xavier Boix, Luc Van Gool", "title": "Random Binary Mappings for Kernel Learning and Efficient SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines (SVMs) are powerful learners that have led to\nstate-of-the-art results in various computer vision problems. SVMs suffer from\nvarious drawbacks in terms of selecting the right kernel, which depends on the\nimage descriptors, as well as computational and memory efficiency. This paper\nintroduces a novel kernel, which serves such issues well. The kernel is learned\nby exploiting a large amount of low-complex, randomized binary mappings of the\ninput feature. This leads to an efficient SVM, while also alleviating the task\nof kernel selection. We demonstrate the capabilities of our kernel on 6\nstandard vision benchmarks, in which we combine several common image\ndescriptors, namely histograms (Flowers17 and Daimler), attribute-like\ndescriptors (UCI, OSR, and a-VOC08), and Sparse Quantization (ImageNet).\nResults show that our kernel learning adapts well to the different descriptors\ntypes, achieving the performance of the kernels specifically tuned for each\nimage descriptor, and with similar evaluation cost as efficient SVM methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 08:47:32 GMT"}, {"version": "v2", "created": "Fri, 28 Mar 2014 08:49:17 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Roig", "Gemma", ""], ["Boix", "Xavier", ""], ["Van Gool", "Luc", ""]]}, {"id": "1307.5348", "submitter": "Oguz Semerci", "authors": "Oguz Semerci, Ning Hao, Misha E. Kilmer and Eric L. Miller", "title": "Tensor-based formulation and nuclear norm regularization for\n  multi-energy computed tomography", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2305840", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of energy selective, photon counting X-ray detectors allows\nfor a wide range of new possibilities in the area of computed tomographic image\nformation. Under the assumption of perfect energy resolution, here we propose a\ntensor-based iterative algorithm that simultaneously reconstructs the X-ray\nattenuation distribution for each energy. We use a multi-linear image model\nrather than a more standard \"stacked vector\" representation in order to develop\nnovel tensor-based regularizers. Specifically, we model the multi-spectral\nunknown as a 3-way tensor where the first two dimensions are space and the\nthird dimension is energy. This approach allows for the design of tensor\nnuclear norm regularizers, which like its two dimensional counterpart, is a\nconvex function of the multi-spectral unknown. The solution to the resulting\nconvex optimization problem is obtained using an alternating direction method\nof multipliers (ADMM) approach. Simulation results shows that the generalized\ntensor nuclear norm can be used as a stand alone regularization technique for\nthe energy selective (spectral) computed tomography (CT) problem and when\ncombined with total variation regularization it enhances the regularization\ncapabilities especially at low energy images where the effects of noise are\nmost prominent.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 23:22:12 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Semerci", "Oguz", ""], ["Hao", "Ning", ""], ["Kilmer", "Misha E.", ""], ["Miller", "Eric L.", ""]]}, {"id": "1307.5551", "submitter": "Sira Ferradans", "authors": "Sira Ferradans, Nicolas Papadakis, Gabriel Peyr\\'e and\n  Jean-Fran\\c{c}ois Aujol", "title": "Regularized Discrete Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a generalization of the discrete optimal transport,\nwith applications to color image manipulations. This new formulation includes a\nrelaxation of the mass conservation constraint and a regularization term. These\ntwo features are crucial for image processing tasks, which necessitate to take\ninto account families of multimodal histograms, with large mass variation\nacross modes.\n  The corresponding relaxed and regularized transportation problem is the\nsolution of a convex optimization problem. Depending on the regularization\nused, this minimization can be solved using standard linear programming methods\nor first order proximal splitting schemes.\n  The resulting transportation plan can be used as a color transfer map, which\nis robust to mass variation across images color palettes. Furthermore, the\nregularization of the transport plan helps to remove colorization artifacts due\nto noise amplification.\n  We also extend this framework to the computation of barycenters of\ndistributions. The barycenter is the solution of an optimization problem, which\nis separately convex with respect to the barycenter and the transportation\nplans, but not jointly convex. A block coordinate descent scheme converges to a\nstationary point of the energy. We show that the resulting algorithm can be\nused for color normalization across several images. The relaxed and regularized\nbarycenter defines a common color palette for those images. Applying color\ntransfer toward this average palette performs a color normalization of the\ninput images.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2013 17:55:10 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Ferradans", "Sira", ""], ["Papadakis", "Nicolas", ""], ["Peyr\u00e9", "Gabriel", ""], ["Aujol", "Jean-Fran\u00e7ois", ""]]}, {"id": "1307.5591", "submitter": "Subra Mukherjee", "authors": "Subra Mukherjee, Karen Das", "title": "A Novel Equation based Classifier for Detecting Human in Images", "comments": "published with international journal of Computer Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 72(6):9-16, June\n  2013", "doi": "10.5120/12496-7272", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape based classification is one of the most challenging tasks in the field\nof computer vision. Shapes play a vital role in object recognition. The basic\nshapes in an image can occur in varying scale, position and orientation. And\nspecially when detecting human, the task becomes more challenging owing to the\nlargely varying size, shape, posture and clothing of human. So, in our work we\ndetect human, based on the head-shoulder shape as it is the most unvarying part\nof human body. Here, firstly a new and a novel equation named as the Omega\nEquation that describes the shape of human head-shoulder is developed and based\non this equation, a classifier is designed particularly for detecting human\npresence in a scene. The classifier detects human by analyzing some of the\ndiscriminative features of the values of the parameters obtained from the Omega\nequation. The proposed method has been tested on a variety of shape dataset\ntaking into consideration the complexities of human head-shoulder shape. In all\nthe experiments the proposed method demonstrated satisfactory results.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 05:13:03 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Mukherjee", "Subra", ""], ["Das", "Karen", ""]]}, {"id": "1307.5653", "submitter": "Duc Phu Chau", "authors": "Duc Phu Chau (INRIA Sophia Antipolis), Julien Badie (INRIA Sophia\n  Antipolis), Fran\\c{c}ois Bremond (INRIA Sophia Antipolis), Monique Thonnat\n  (INRIA Sophia Antipolis)", "title": "Online Tracking Parameter Adaptation based on Evaluation", "comments": "IEEE International Conference on Advanced Video and Signal-based\n  Surveillance (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter tuning is a common issue for many tracking algorithms. In order to\nsolve this problem, this paper proposes an online parameter tuning to adapt a\ntracking algorithm to various scene contexts. In an offline training phase,\nthis approach learns how to tune the tracker parameters to cope with different\ncontexts. In the online control phase, once the tracking quality is evaluated\nas not good enough, the proposed approach computes the current context and\ntunes the tracking parameters using the learned values. The experimental\nresults show that the proposed approach improves the performance of the\ntracking algorithm and outperforms recent state of the art trackers. This paper\nbrings two contributions: (1) an online tracking evaluation, and (2) a method\nto adapt online tracking parameters to scene contexts.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 11:09:33 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Chau", "Duc Phu", "", "INRIA Sophia Antipolis"], ["Badie", "Julien", "", "INRIA Sophia\n  Antipolis"], ["Bremond", "Fran\u00e7ois", "", "INRIA Sophia Antipolis"], ["Thonnat", "Monique", "", "INRIA Sophia Antipolis"]]}, {"id": "1307.5684", "submitter": "Lucas Paletta", "authors": "Jason Satel, Ross Story, Matthew D. Hilchey, Zhiguo Wang and Raymond\n  M. Klein", "title": "Using a Dynamic Neural Field Model to Explore a Direct Collicular\n  Inhibition Account of Inhibition of Return", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/01", "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the interval between a transient ash of light (a \"cue\") and a second\nvisual response signal (a \"target\") exceeds at least 200ms, responding is\nslowest in the direction indicated by the first signal. This phenomenon is\ncommonly referred to as inhibition of return (IOR). The dynamic neural field\nmodel (DNF) has proven to have broad explanatory power for IOR, effectively\ncapturing many empirical results. Previous work has used a short-term\ndepression (STD) implementation of IOR, but this approach fails to explain many\nbehavioral phenomena observed in the literature. Here, we explore a variant\nmodel of IOR involving a combination of STD and delayed direct collicular\ninhibition. We demonstrate that this hybrid model can better reproduce\nestablished behavioural results. We use the results of this model to propose\nseveral experiments that would yield particularly valuable insight into the\nnature of the neurophysiological mechanisms underlying IOR.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 12:47:18 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Satel", "Jason", ""], ["Story", "Ross", ""], ["Hilchey", "Matthew D.", ""], ["Wang", "Zhiguo", ""], ["Klein", "Raymond M.", ""]]}, {"id": "1307.5691", "submitter": "Lucas Paletta", "authors": "Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin and\n  Thierry Dutoit", "title": "A study of parameters affecting visual saliency assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/02", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the early 2000s, computational visual saliency has been a very active\nresearch area. Each year, more and more new models are published in the main\ncomputer vision conferences. Nowadays, one of the big challenges is to find a\nway to fairly evaluate all of these models. In this paper, a new framework is\nproposed to assess models of visual saliency. This evaluation is divided into\nthree experiments leading to the proposition of a new evaluation framework.\nEach experiment is based on a basic question: 1) there are two ground truths\nfor saliency evaluation: what are the differences between eye fixations and\nmanually segmented salient regions?, 2) the properties of the salient regions:\nfor example, do large, medium and small salient regions present different\ndifficulties for saliency models? and 3) the metrics used to assess saliency\nmodels: what advantages would there be to mix them with PCA? Statistical\nanalysis is used here to answer each of these three questions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 13:01:36 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Riche", "Nicolas", ""], ["Duvinage", "Matthieu", ""], ["Mancas", "Matei", ""], ["Gosselin", "Bernard", ""], ["Dutoit", "Thierry", ""]]}, {"id": "1307.5693", "submitter": "Lucas Paletta", "authors": "Yasin Kavak, Erkut Erdem and Aykut Erdem", "title": "Visual saliency estimation by integrating features using multiple kernel\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/03", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few decades, significant achievements have been attained in\npredicting where humans look at images through different computational models.\nHowever, how to determine contributions of different visual features to overall\nsaliency still remains an open problem. To overcome this issue, a recent class\nof models formulates saliency estimation as a supervised learning problem and\naccordingly apply machine learning techniques. In this paper, we also address\nthis challenging problem and propose to use multiple kernel learning (MKL) to\ncombine information coming from different feature dimensions and to perform\nintegration at an intermediate level. Besides, we suggest to use responses of a\nrecently proposed filterbank of object detectors, known as Object-Bank, as\nadditional semantic high-level features. Here we show that our MKL-based\nframework together with the proposed object-specific features provide\nstate-of-the-art performance as compared to SVM or AdaBoost-based saliency\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 13:09:12 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Kavak", "Yasin", ""], ["Erdem", "Erkut", ""], ["Erdem", "Aykut", ""]]}, {"id": "1307.5702", "submitter": "Lucas Paletta", "authors": "Samuel F. Dodge and Lina J. Karam", "title": "Is Bottom-Up Attention Useful for Scene Recognition?", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/04", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system employs a selective attention mechanism to understand\nthe visual world in an eficient manner. In this paper, we show how\ncomputational models of this mechanism can be exploited for the computer vision\napplication of scene recognition. First, we consider saliency weighting and\nsaliency pruning, and provide a comparison of the performance of different\nattention models in these approaches in terms of classification accuracy.\nPruning can achieve a high degree of computational savings without\nsignificantly sacrificing classification accuracy. In saliency weighting,\nhowever, we found that classification performance does not improve. In\naddition, we present a new method to incorporate salient and non-salient\nregions for improved classification accuracy. We treat the salient and\nnon-salient regions separately and combine them using Multiple Kernel Learning.\nWe evaluate our approach using the UIUC sports dataset and find that with a\nsmall training size, our method improves upon the classification accuracy of\nthe baseline bag of features approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 13:38:16 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Dodge", "Samuel F.", ""], ["Karam", "Lina J.", ""]]}, {"id": "1307.5710", "submitter": "Lucas Paletta", "authors": "Jan T\u007f\\\"unnermann, Dieter Enns, and B\u007f\\\"arbel Mertsching", "title": "Saliency-Guided Perceptual Grouping Using Motion Cues in Region-Based\n  Artificial Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/05", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region-based artificial attention constitutes a framework for bio-inspired\nattentional processes on an intermediate abstraction level for the use in\ncomputer vision and mobile robotics. Segmentation algorithms produce regions of\ncoherently colored pixels. These serve as proto-objects on which the\nattentional processes determine image portions of relevance. A single\nregion---which not necessarily represents a full object---constitutes the focus\nof attention. For many post-attentional tasks, however, such as identifying or\ntracking objects, single segments are not sufficient. Here, we present a\nsaliency-guided approach that groups regions that potentially belong to the\nsame object based on proximity and similarity of motion. We compare our results\nto object selection by thresholding saliency maps and a further\nattention-guided strategy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 13:48:13 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["T\u007f\u00fcnnermann", "Jan", ""], ["Enns", "Dieter", ""], ["Mertsching", "B\u007f\u00e4rbel", ""]]}, {"id": "1307.5713", "submitter": "Lucas Paletta", "authors": "Min Zhao, Andre G. Marquez", "title": "Understanding Humans' Strategies in Maze Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/06", "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigating through a visual maze relies on the strategic use of eye movements\nto select and identify the route. When navigating the maze, there are\ntrade-offs between exploring to the environment and relying on memory. This\nstudy examined strategies used to navigating through novel and familiar mazes\nthat were viewed from above and traversed by a mouse cursor. Eye and mouse\nmovements revealed two modes that almost never occurred concurrently:\nexploration and guidance. Analyses showed that people learned mazes and were\nable to devise and carry out complex, multi-faceted strategies that traded-off\nvisual exploration against active motor performance. These strategies took into\naccount available visual information, memory, confidence, the estimated cost in\ntime for exploration, and idiosyncratic tolerance for error. Understanding the\nstrategies humans used for maze solving is valuable for applications in\ncognitive neuroscience as well as in AI, robotics and human-robot interactions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 13:57:10 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Zhao", "Min", ""], ["Marquez", "Andre G.", ""]]}, {"id": "1307.5720", "submitter": "Lucas Paletta", "authors": "Esther L. Colombini, Alexandre S. Sim\\~oes and Carlos H. C. Ribeiro", "title": "Top-down and Bottom-up Feature Combination for Multi-sensor Attentive\n  Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/07", "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information available to robots in real tasks is widely distributed both\nin time and space, requiring the agent to search for relevant data. In humans,\nthat face the same problem when sounds, images and smells are presented to\ntheir sensors in a daily scene, a natural system is applied: Attention. As\nvision plays an important role in our routine, most research regarding\nattention has involved this sensorial system and the same has been replicated\nto the robotics field. However,most of the robotics tasks nowadays do not rely\nonly in visual data, that are still costly. To allow the use of attentive\nconcepts with other robotics sensors that are usually used in tasks such as\nnavigation, self-localization, searching and mapping, a generic attentional\nmodel has been previously proposed. In this work, feature mapping functions\nwere designed to build feature maps to this attentive model from data from\nrange scanner and sonar sensors. Experiments were performed in a high fidelity\nsimulated robotics environment and results have demonstrated the capability of\nthe model on dealing with both salient stimuli and goal-driven attention over\nmultiple features extracted from multiple sensors.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 14:09:30 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Colombini", "Esther L.", ""], ["Sim\u00f5es", "Alexandre S.", ""], ["Ribeiro", "Carlos H. C.", ""]]}, {"id": "1307.5748", "submitter": "Riccardo Satta", "authors": "Riccardo Satta", "title": "Appearance Descriptors for Person Re-identification: a Comprehensive\n  Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video-surveillance, person re-identification is the task of recognising\nwhether an individual has already been observed over a network of cameras.\nTypically, this is achieved by exploiting the clothing appearance, as classical\nbiometric traits like the face are impractical in real-world video surveillance\nscenarios. Clothing appearance is represented by means of low-level\n\\textit{local} and/or \\textit{global} features of the image, usually extracted\naccording to some part-based body model to treat different body parts (e.g.\ntorso and legs) independently. This paper provides a comprehensive review of\ncurrent approaches to build appearance descriptors for person\nre-identification. The most relevant techniques are described in detail, and\ncategorised according to the body models and features used. The aim of this\nwork is to provide a structured body of knowledge and a starting point for\nresearchers willing to conduct novel investigations on this challenging topic.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 15:41:57 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Satta", "Riccardo", ""]]}, {"id": "1307.5800", "submitter": "Subra Mukherjee", "authors": "Subra Mukherjee and Karen Das", "title": "An Adaptive GMM Approach to Background Subtraction for Application in\n  Real Time Surveillance", "comments": "5 Pages", "journal-ref": "International Journal of Research in Engineering and\n  Technology,Volume 2, Issue 1, Jan-2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient security management has become an important parameter in todays\nworld. As the problem is growing, there is an urgent need for the introduction\nof advanced technology and equipment to improve the state-of art of\nsurveillance. In this paper we propose a model for real time background\nsubtraction using AGMM. The proposed model is robust and adaptable to dynamic\nbackground, fast illumination changes, repetitive motion. Also we have\nincorporated a method for detecting shadows using the Horpresert color model.\nThe proposed model can be employed for monitoring areas where movement or entry\nis highly restricted. So on detection of any unexpected events in the scene an\nalarm can be triggered and hence we can achieve real time surveillance even in\nthe absence of constant human monitoring.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 17:59:28 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Mukherjee", "Subra", ""], ["Das", "Karen", ""]]}, {"id": "1307.5996", "submitter": "Nicolas Dobigeon", "authors": "Qi Wei, Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Bayesian Fusion of Multi-Band Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Bayesian fusion technique for remotely sensed multi-band\nimages is presented. The observed images are related to the high spectral and\nhigh spatial resolution image to be recovered through physical degradations,\ne.g., spatial and spectral blurring and/or subsampling defined by the sensor\ncharacteristics. The fusion problem is formulated within a Bayesian estimation\nframework. An appropriate prior distribution exploiting geometrical\nconsideration is introduced. To compute the Bayesian estimator of the scene of\ninterest from its posterior distribution, a Markov chain Monte Carlo algorithm\nis designed to generate samples asymptotically distributed according to the\ntarget distribution. To efficiently sample from this high-dimension\ndistribution, a Hamiltonian Monte Carlo step is introduced in the Gibbs\nsampling strategy. The efficiency of the proposed fusion method is evaluated\nwith respect to several state-of-the-art fusion techniques. In particular, low\nspatial resolution hyperspectral and multispectral images are fused to produce\na high spatial resolution hyperspectral image.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 09:44:36 GMT"}, {"version": "v2", "created": "Tue, 26 Aug 2014 09:34:49 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Wei", "Qi", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1307.6008", "submitter": "Guang Yang A", "authors": "Guang Yang, John H. Hipwell, David J. Hawkes and Simon R. Arridge", "title": "Numerical Methods for Coupled Reconstruction and Registration in Digital\n  Breast Tomosynthesis", "comments": "29 pages, 22 figures; The Annals of the British Machine Vision\n  Association and Society for Pattern Recognition (BMVA) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Digital Breast Tomosynthesis (DBT) provides an insight into the fine details\nof normal fibroglandular tissues and abnormal lesions by reconstructing a\npseudo-3D image of the breast. In this respect, DBT overcomes a major\nlimitation of conventional X-ray mammography by reducing the confounding\neffects caused by the superposition of breast tissue. In a breast cancer\nscreening or diagnostic context, a radiologist is interested in detecting\nchange, which might be indicative of malignant disease. To help automate this\ntask image registration is required to establish spatial correspondence between\ntime points. Typically, images, such as MRI or CT, are first reconstructed and\nthen registered. This approach can be effective if reconstructing using a\ncomplete set of data. However, for ill-posed, limited-angle problems such as\nDBT, estimating the deformation is complicated by the significant artefacts\nassociated with the reconstruction, leading to severe inaccuracies in the\nregistration. This paper presents a mathematical framework, which couples the\ntwo tasks and jointly estimates both image intensities and the parameters of a\ntransformation.\n  We evaluate our methods using various computational digital phantoms,\nuncompressed breast MR images, and in-vivo DBT simulations. Firstly, we compare\nboth iterative and simultaneous methods to the conventional, sequential method\nusing an affine transformation model. We show that jointly estimating image\nintensities and parametric transformations gives superior results with respect\nto reconstruction fidelity and registration accuracy. Also, we incorporate a\nnon-rigid B-spline transformation model into our simultaneous method. The\nresults demonstrate a visually plausible recovery of the deformation with\npreservation of the reconstruction fidelity.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 10:04:26 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Yang", "Guang", ""], ["Hipwell", "John H.", ""], ["Hawkes", "David J.", ""], ["Arridge", "Simon R.", ""]]}, {"id": "1307.6170", "submitter": "Lucas Paletta", "authors": "Lucas Paletta (JOANNEUM RESEARCH Forschungsgesellschaft mbH), Laurent\n  Itti (University of Southern California, USA), Bj\\\"orn Schuller (Technische\n  Universit\\\"at M\\\"unchen, Germany), Fang Fang (Peking University, China)", "title": "6th International Symposium on Attention in Cognitive Systems 2013", "comments": "conference", "journal-ref": null, "doi": null, "report-no": "ISACS/2013", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers accepted at the 6th International Symposium\non Attention in Cognitive Systems (ISACS 2013), held in Beijing, August 5,\n2013. The aim of this symposium is to highlight the central role of attention\non various kinds of performance in cognitive systems processing. It brings\ntogether researchers and developers from both academia and industry, from\ncomputer vision, robotics, perception psychology, psychophysics and\nneuroscience, in order to provide an interdisciplinary forum to present and\ncommunicate on computational models of attention, with the focus on\ninterdependencies with visual cognition. Furthermore, it intends to investigate\nrelevant objectives for performance comparison, to document and to investigate\npromising application domains, and to discuss visual attention with reference\nto other aspects of AI enabled systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 12:27:26 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2013 07:43:09 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Paletta", "Lucas", "", "JOANNEUM RESEARCH Forschungsgesellschaft mbH"], ["Itti", "Laurent", "", "University of Southern California, USA"], ["Schuller", "Bj\u00f6rn", "", "Technische\n  Universit\u00e4t M\u00fcnchen, Germany"], ["Fang", "Fang", "", "Peking University, China"]]}, {"id": "1307.6303", "submitter": "Junyan Wang", "authors": "Junyan Wang and Kap Luk Chan", "title": "Matching-Constrained Active Contours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In object segmentation by active contours, the initial contour is often\nrequired. Conventionally, the initial contour is provided by the user. This\npaper extends the conventional active contour model by incorporating feature\nmatching in the formulation, which gives rise to a novel matching-constrained\nactive contour. The numerical solution to the new optimization model provides\nan automated framework of object segmentation without user intervention. The\nmain idea is to incorporate feature point matching as a constraint in active\ncontour models. To this effect, we obtain a mathematical model of interior\npoints to boundary contour such that matching of interior feature points gives\ncontour alignment, and we formulate the matching score as a constraint to\nactive contour model such that the feature matching of maximum score that gives\nthe contour alignment provides the initial feasible solution to the constrained\noptimization model of segmentation. The constraint also ensures that the\noptimal contour does not deviate too much from the initial contour.\nProjected-gradient descent equations are derived to solve the constrained\noptimization. In the experiments, we show that our method is capable of\nachieving the automatic object segmentation, and it outperforms the related\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 06:18:44 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Wang", "Junyan", ""], ["Chan", "Kap Luk", ""]]}, {"id": "1307.6542", "submitter": "Shofwatul Uyun Mrs", "authors": "Shofwatul 'Uyun, Sri Hartati, Agus Harjoko, Subanar", "title": "Selection Mammogram Texture Descriptors Based on Statistics Properties\n  Backpropagation Structure", "comments": "5 pages, International Journal of Computer Science and Information\n  Security (IJCSIS) Vol. 11, No. 5, May 2013 Article ID 26041306, arXiv admin\n  note: substantial text overlap with arXiv:1306.5960; arXiv:1306.6489", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Computer Aided Diagnosis (CAD) system has been developed for the early\ndetection of breast cancer, one of the most deadly cancer for women. The benign\nof mammogram has different texture from malignant. There are fifty mammogram\nimages used in this work which are divided for training and testing. Therefore,\nthe selection of the right texture to determine the level of accuracy of CAD\nsystem is important. The first and second order statistics are the texture\nfeature extraction methods which can be used on a mammogram. This work\nclassifies texture descriptor into nine groups where the extraction of features\nis classified using backpropagation learning with two types of multi-layer\nperceptron (MLP). The best texture descriptor as selected when the value of\nregression 1 appears in both the MLP-1 and the MLP-2 with the number of epoches\nless than 1000. The results of testing show that the best selected texture\ndescriptor is the second order (combination) using all direction (0, 45, 90 and\n135) that have twenty four descriptors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 02:42:29 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["'Uyun", "Shofwatul", ""], ["Hartati", "Sri", ""], ["Harjoko", "Agus", ""], ["Subanar", "", ""]]}, {"id": "1307.6544", "submitter": "Mohammed El-Dosuky", "authors": "M. A. El-Dosuky", "title": "Veni Vidi Vici, A Three-Phase Scenario For Parameter Space Analysis in\n  Image Analysis and Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic analysis of the enormous sets of images is a critical task in life\nsciences. This faces many challenges such as: algorithms are highly\nparameterized, significant human input is intertwined, and lacking a standard\nmeta-visualization approach. This paper proposes an alternative iterative\napproach for optimizing input parameters, saving time by minimizing the user\ninvolvement, and allowing for understanding the workflow of algorithms and\ndiscovering new ones. The main focus is on developing an interactive\nvisualization technique that enables users to analyze the relationships between\nsampled input parameters and corresponding output. This technique is\nimplemented as a prototype called Veni Vidi Vici, or \"I came, I saw, I\nconquered.\" This strategy is inspired by the mathematical formulas of numbering\ncomputable functions and is developed atop ImageJ, a scientific image\nprocessing program. A case study is presented to investigate the proposed\nframework. Finally, the paper explores some potential future issues in the\napplication of the proposed approach in parameter space analysis in\nvisualization.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 04:49:24 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["El-Dosuky", "M. A.", ""]]}, {"id": "1307.6549", "submitter": "Michael Bronstein", "authors": "Michael M. Bronstein, Klaus Glashoff, Terry A. Loring", "title": "Making Laplacians commute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we construct multimodal spectral geometry by finding a pair of\nclosest commuting operators (CCO) to a given pair of Laplacians. The CCOs are\njointly diagonalizable and hence have the same eigenbasis. Our construction\nnaturally extends classical data analysis tools based on spectral geometry,\nsuch as diffusion maps and spectral clustering. We provide several synthetic\nand real examples of applications in dimensionality reduction, shape analysis,\nand clustering, demonstrating that our method better captures the inherent\nstructure of multi-modal data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 10:57:31 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Bronstein", "Michael M.", ""], ["Glashoff", "Klaus", ""], ["Loring", "Terry A.", ""]]}, {"id": "1307.6962", "submitter": "Yalin Bastanlar", "authors": "Yalin Bastanlar", "title": "Reduced egomotion estimation drift using omnidirectional views", "comments": "Another publisher does not want this article to be shared at\n  arxiv.org in order to publish it", "journal-ref": "Electronic Letters on Computer Vision and Image Analysis,\n  vol.13(3), p.1-12, 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of camera motion from a given image sequence becomes degraded as\nthe length of the sequence increases. In this letter, this phenomenon is\ndemonstrated and an approach to increase the estimation accuracy is proposed.\nThe proposed method uses an omnidirectional camera in addition to the\nperspective one and takes advantage of its enlarged view by exploiting the\ncorrespondences between the omnidirectional and perspective images. Simulated\nand real image experiments show that the proposed approach improves the\nestimation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 09:06:44 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2013 20:46:41 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Bastanlar", "Yalin", ""]]}, {"id": "1307.7198", "submitter": "Kenji Okuma", "authors": "Kenji Okuma and David G. Lowe and James J. Little", "title": "Self-Learning for Player Localization in Sports Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel self-learning framework that automates the\nlabel acquisition process for improving models for detecting players in\nbroadcast footage of sports games. Unlike most previous self-learning\napproaches for improving appearance-based object detectors from videos, we\nallow an unknown, unconstrained number of target objects in a more generalized\nvideo sequence with non-static camera views. Our self-learning approach uses a\nlatent SVM learning algorithm and deformable part models to represent the shape\nand colour information of players, constraining their motions, and learns the\ncolour of the playing field by a gentle Adaboost algorithm. We combine those\nimage cues and discover additional labels automatically from unlabelled data.\nIn our experiments, our approach exploits both labelled and unlabelled data in\nsparsely labelled videos of sports games, providing a mean performance\nimprovement of over 20% in the average precision for detecting sports players\nand improved tracking, when videos contain very few labelled images.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 00:34:41 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Okuma", "Kenji", ""], ["Lowe", "David G.", ""], ["Little", "James J.", ""]]}, {"id": "1307.7466", "submitter": "Damien Duff", "authors": "Damien Jade Duff and Esra Erdem and Volkan Patoglu", "title": "Integration of 3D Object Recognition and Planning for Robotic\n  Manipulation: A Preliminary Report", "comments": "Knowledge Representation and Reasoning in Robotics Workshop at ICLP\n  2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate different approaches to integrating object recognition and\nplanning in a tabletop manipulation domain with the set of objects used in the\n2012 RoboCup@Work competition. Results of our preliminary experiments show\nthat, with some approaches, close integration of perception and planning\nimproves the quality of plans, as well as the computation times of feasible\nplans.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 05:40:49 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Duff", "Damien Jade", ""], ["Erdem", "Esra", ""], ["Patoglu", "Volkan", ""]]}, {"id": "1307.7474", "submitter": "R Subash Chandra Boss", "authors": "R. Subash Chandra Boss, K. Thangavel, D. Arul Pon Daniel", "title": "Automatic Mammogram image Breast Region Extraction and Removal of\n  Pectoral Muscle", "comments": "8 Pages, 5 Figures", "journal-ref": "International Journal of Scientific & Engineering Research, Volume\n  4, Issue 5, May-2013 1727", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently Mammography is a most effective imaging modality used by\nradiologists for the screening of breast cancer. Finding an accurate, robust\nand efficient breast region segmentation technique still remains a challenging\nproblem in digital mammography. Extraction of the breast profile region and the\nremoval of pectoral muscle are essential pre-processing steps in Computer Aided\nDiagnosis (CAD) system for the diagnosis of breast cancer. Primarily it allows\nthe search for abnormalities to be limited to the region of the breast tissue\nwithout undue influence from the background of the mammogram. The presence of\npectoral muscle in mammograms biases detection procedures, which recommends\nremoving the pectoral muscle during mammogram image pre-processing. The\npresence of pectoral muscle in mammograms may disturb or influence the\ndetection of breast cancer as the pectoral muscle and mammographic parenchymas\nappear similar. The goal of breast region extraction is reducing the image size\nwithout losing anatomic information, it improve the accuracy of the overall CAD\nsystem. The main objective of this study is to propose an automated method to\nidentify the pectoral muscle in Medio-Lateral Oblique (MLO) view mammograms. In\nthis paper, we proposed histogram based 8-neighborhood connected component\nlabelling method for breast region extraction and removal of pectoral muscle.\nThe proposed method is evaluated by using the mean values of accuracy and\nerror. The comparative analysis shows that the proposed method identifies the\nbreast region more accurately.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 06:42:25 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Boss", "R. Subash Chandra", ""], ["Thangavel", "K.", ""], ["Daniel", "D. Arul Pon", ""]]}, {"id": "1307.7521", "submitter": "Mohsen Joneidi", "authors": "Mohsen Joneidi, Parvin Ahmadi, Mostafa Sadeghi, Nazanin Rahnavard", "title": "Union of Low-Rank Subspaces Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of signal detection using a flexible and general model is\nconsidered. Due to applicability and flexibility of sparse signal\nrepresentation and approximation, it has attracted a lot of attention in many\nsignal processing areas. In this paper, we propose a new detection method based\non sparse decomposition in a union of subspaces (UoS) model. Our proposed\ndetector uses a dictionary that can be interpreted as a bank of matched\nsubspaces. This improves the performance of signal detection, as it is a\ngeneralization for detectors. Low-rank assumption for the desired signals\nimplies that the representations of these signals in terms of some proper bases\nwould be sparse. Our proposed detector exploits sparsity in its decision rule.\nWe demonstrate the high efficiency of our method in the cases of voice activity\ndetection in speech processing.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 09:54:51 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2013 14:32:54 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2013 20:47:40 GMT"}, {"version": "v4", "created": "Mon, 28 Oct 2013 18:58:22 GMT"}, {"version": "v5", "created": "Sun, 23 Feb 2014 14:06:25 GMT"}, {"version": "v6", "created": "Tue, 16 Feb 2016 16:33:14 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Joneidi", "Mohsen", ""], ["Ahmadi", "Parvin", ""], ["Sadeghi", "Mostafa", ""], ["Rahnavard", "Nazanin", ""]]}, {"id": "1307.7800", "submitter": "Yongsub  Lim", "authors": "Yongsub Lim, Kyomin Jung, Pushmeet Kohli", "title": "Efficient Energy Minimization for Enforcing Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy minimization algorithms, such as graph cuts, enable the computation of\nthe MAP solution under certain probabilistic models such as Markov random\nfields. However, for many computer vision problems, the MAP solution under the\nmodel is not the ground truth solution. In many problem scenarios, the system\nhas access to certain statistics of the ground truth. For instance, in image\nsegmentation, the area and boundary length of the object may be known. In these\ncases, we want to estimate the most probable solution that is consistent with\nsuch statistics, i.e., satisfies certain equality or inequality constraints.\n  The above constrained energy minimization problem is NP-hard in general, and\nis usually solved using Linear Programming formulations, which relax the\nintegrality constraints. This paper proposes a novel method that finds the\ndiscrete optimal solution of such problems by maximizing the corresponding\nLagrangian dual. This method can be applied to any constrained energy\nminimization problem whose unconstrained version is polynomial time solvable,\nand can handle multiple, equality or inequality, and linear or non-linear\nconstraints. We demonstrate the efficacy of our method on the\nforeground/background image segmentation problem, and show that it produces\nimpressive segmentation results with less error, and runs more than 20 times\nfaster than the state-of-the-art LP relaxation based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 03:46:38 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Lim", "Yongsub", ""], ["Jung", "Kyomin", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1307.7848", "submitter": "Lucas Paletta", "authors": "Lucas Paletta, Katrin Santner and Gerald Fritz", "title": "An Integrated System for 3D Gaze Recovery and Semantic Analysis of Human\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/08", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes a computer vision system that enables pervasive mapping\nand monitoring of human attention. The key contribution is that our methodology\nenables full 3D recovery of the gaze pointer, human view frustum and associated\nhuman centered measurements directly into an automatically computed 3D model in\nreal-time. We apply RGB-D SLAM and descriptor matching methodologies for the 3D\nmodeling, localization and fully automated annotation of ROIs (regions of\ninterest) within the acquired 3D model. This innovative methodology will open\nnew avenues for attention studies in real world environments, bringing new\npotential into automated processing for human factors technologies.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 07:18:20 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Paletta", "Lucas", ""], ["Santner", "Katrin", ""], ["Fritz", "Gerald", ""]]}, {"id": "1307.7851", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Hao Xu, Xian-Sheng Hua, and Shipeng Li", "title": "Hybrid Affinity Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we address a problem of managing tagged images with hybrid\nsummarization. We formulate this problem as finding a few image exemplars to\nrepresent the image set semantically and visually, and solve it in a hybrid way\nby exploiting both visual and textual information associated with images. We\npropose a novel approach, called homogeneous and heterogeneous message\npropagation ($\\text{H}^\\text{2}\\text{MP}$). Similar to the affinity propagation\n(AP) approach, $\\text{H}^\\text{2}\\text{MP}$ reduce the conventional\n\\emph{vector} message propagation to \\emph{scalar} message propagation to make\nthe algorithm more efficient. Beyond AP that can only handle homogeneous data,\n$\\text{H}^\\text{2}\\text{MP}$ generalizes it to exploit extra heterogeneous\nrelations and the generalization is non-trivial as the reduction to scalar\nmessages from vector messages is more challenging. The main advantages of our\napproach lie in 1) that $\\text{H}^\\text{2}\\text{MP}$ exploits visual similarity\nand in addition the useful information from the associated tags, including the\nassociations relation between images and tags and the relations within tags,\nand 2) that the summary is both visually and semantically satisfactory. In\naddition, our approach can also present a textual summary to a tagged image\ncollection, which can be used to automatically generate a textual description.\nThe experimental results demonstrate the effectiveness and efficiency of the\nroposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 07:30:46 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Wang", "Jingdong", ""], ["Xu", "Hao", ""], ["Hua", "Xian-Sheng", ""], ["Li", "Shipeng", ""]]}, {"id": "1307.7852", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Jing Wang, Gang Zeng, Zhuowen Tu, Rui Gan, and Shipeng\n  Li", "title": "Scalable $k$-NN graph construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The $k$-NN graph has played a central role in increasingly popular\ndata-driven techniques for various learning and vision tasks; yet, finding an\nefficient and effective way to construct $k$-NN graphs remains a challenge,\nespecially for large-scale high-dimensional data. In this paper, we propose a\nnew approach to construct approximate $k$-NN graphs with emphasis in:\nefficiency and accuracy. We hierarchically and randomly divide the data points\ninto subsets and build an exact neighborhood graph over each subset, achieving\na base approximate neighborhood graph; we then repeat this process for several\ntimes to generate multiple neighborhood graphs, which are combined to yield a\nmore accurate approximate neighborhood graph. Furthermore, we propose a\nneighborhood propagation scheme to further enhance the accuracy. We show both\ntheoretical and empirical accuracy and efficiency of our approach to $k$-NN\ngraph construction and demonstrate significant speed-up in dealing with large\nscale visual data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 07:33:31 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Wang", "Jingdong", ""], ["Wang", "Jing", ""], ["Zeng", "Gang", ""], ["Tu", "Zhuowen", ""], ["Gan", "Rui", ""], ["Li", "Shipeng", ""]]}, {"id": "1307.8233", "submitter": "Lucas Paletta", "authors": "Jan T\u007f\\\"unnermann, Markus Hennig, Michael Silbernagel and B\u007f\\\"arbel\n  Mertsching", "title": "A Prototyping Environment for Integrated Artificial Attention Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/09", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial visual attention systems aim to support technical systems in\nvisual tasks by applying the concepts of selective attention observed in humans\nand other animals. Such systems are typically evaluated against ground truth\nobtained from human gaze-data or manually annotated test images. When applied\nto robotics, the systems are required to be adaptable to the target system.\nHere, we describe a flexible environment based on a robotic middleware layer\nallowing the development and testing of attention-guided vision systems. In\nsuch a framework, the systems can be tested with input from various sources,\ndifferent attention algorithms at the core, and diverse subsequent tasks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 06:24:28 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["T\u007f\u00fcnnermann", "Jan", ""], ["Hennig", "Markus", ""], ["Silbernagel", "Michael", ""], ["Mertsching", "B\u007f\u00e4rbel", ""]]}, {"id": "1307.8405", "submitter": "Jinyun Yan", "authors": "Zixuan Wang, Jinyun Yan", "title": "Who and Where: People and Location Co-Clustering", "comments": "2013 IEEE International Conference on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the clustering problem on images where each image\ncontains patches in people and location domains. We exploit the correlation\nbetween people and location domains, and proposed a semi-supervised\nco-clustering algorithm to cluster images. Our algorithm updates the\ncorrelation links at the runtime, and produces clustering in both domains\nsimultaneously. We conduct experiments in a manually collected dataset and a\nFlickr dataset. The result shows that the such correlation improves the\nclustering performance.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 17:53:10 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Wang", "Zixuan", ""], ["Yan", "Jinyun", ""]]}]