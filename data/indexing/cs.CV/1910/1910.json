[{"id": "1910.00032", "submitter": "Ye Lyu", "authors": "Ye Lyu, George Vosselman, Gui-Song Xia, Michael Ying Yang", "title": "LIP: Learning Instance Propagation for Video Object Segmentation", "comments": "ICCVW19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the task of segmenting foreground objects from background in\na video, i.e. video object segmentation (VOS), has received considerable\nattention. In this paper, we propose a single end-to-end trainable deep neural\nnetwork, convolutional gated recurrent Mask-RCNN, for tackling the\nsemi-supervised VOS task. We take advantage of both the instance segmentation\nnetwork (Mask-RCNN) and the visual memory module (Conv-GRU) to tackle the VOS\ntask. The instance segmentation network predicts masks for instances, while the\nvisual memory module learns to selectively propagate information for multiple\ninstances simultaneously, which handles the appearance change, the variation of\nscale and pose and the occlusions between objects. After offline and online\ntraining under purely instance segmentation losses, our approach is able to\nachieve satisfactory results without any post-processing or synthetic video\ndata augmentation. Experimental results on DAVIS 2016 dataset and DAVIS 2017\ndataset have demonstrated the effectiveness of our method for video object\nsegmentation task.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 18:03:09 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Lyu", "Ye", ""], ["Vosselman", "George", ""], ["Xia", "Gui-Song", ""], ["Yang", "Michael Ying", ""]]}, {"id": "1910.00033", "submitter": "Aniruddha Saha", "authors": "Aniruddha Saha, Akshayvarun Subramanya, Hamed Pirsiavash", "title": "Hidden Trigger Backdoor Attacks", "comments": "AAAI 2020 - Main Technical Track (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of deep learning algorithms in various domains, studying\nadversarial attacks to secure deep models in real world applications has become\nan important research topic. Backdoor attacks are a form of adversarial attacks\non deep networks where the attacker provides poisoned data to the victim to\ntrain the model with, and then activates the attack by showing a specific small\ntrigger pattern at the test time. Most state-of-the-art backdoor attacks either\nprovide mislabeled poisoning data that is possible to identify by visual\ninspection, reveal the trigger in the poisoned data, or use noise to hide the\ntrigger. We propose a novel form of backdoor attack where poisoned data look\nnatural with correct labels and also more importantly, the attacker hides the\ntrigger in the poisoned data and keeps the trigger secret until the test time.\nWe perform an extensive study on various image classification settings and show\nthat our attack can fool the model by pasting the trigger at random locations\non unseen images although the model performs well on clean data. We also show\nthat our proposed attack cannot be easily defended using a state-of-the-art\ndefense algorithm for backdoor attacks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 18:03:28 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 02:13:34 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Saha", "Aniruddha", ""], ["Subramanya", "Akshayvarun", ""], ["Pirsiavash", "Hamed", ""]]}, {"id": "1910.00058", "submitter": "Po-Yao Huang", "authors": "Po-Yao Huang, Xiaojun Chang, Alexander Hauptmann", "title": "Multi-Head Attention with Diversity for Learning Grounded Multilingual\n  Multimodal Representations", "comments": "Accepted at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim of promoting and understanding the multilingual version of image\nsearch, we leverage visual object detection and propose a model with diverse\nmulti-head attention to learn grounded multilingual multimodal representations.\nSpecifically, our model attends to different types of textual semantics in two\nlanguages and visual objects for fine-grained alignments between sentences and\nimages. We introduce a new objective function which explicitly encourages\nattention diversity to learn an improved visual-semantic embedding space. We\nevaluate our model in the German-Image and English-Image matching tasks on the\nMulti30K dataset, and in the Semantic Textual Similarity task with the English\ndescriptions of visual content. Results show that our model yields a\nsignificant performance gain over other methods in all of the three tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 18:58:03 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Huang", "Po-Yao", ""], ["Chang", "Xiaojun", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1910.00068", "submitter": "Aniruddha Saha", "authors": "Aniruddha Saha, Akshayvarun Subramanya, Koninika Patil, Hamed\n  Pirsiavash", "title": "Role of Spatial Context in Adversarial Robustness for Object Detection", "comments": "CVPR 2020 Workshop on Adversarial Machine Learning in Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The benefits of utilizing spatial context in fast object detection algorithms\nhave been studied extensively. Detectors increase inference speed by doing a\nsingle forward pass per image which means they implicitly use contextual\nreasoning for their predictions. However, one can show that an adversary can\ndesign adversarial patches which do not overlap with any objects of interest in\nthe scene and exploit contextual reasoning to fool standard detectors. In this\npaper, we examine this problem and design category specific adversarial patches\nwhich make a widely used object detector like YOLO blind to an attacker chosen\nobject category. We also show that limiting the use of spatial context during\nobject detector training improves robustness to such adversaries. We believe\nthe existence of context based adversarial attacks is concerning since the\nadversarial patch can affect predictions without being in vicinity of any\nobjects of interest. Hence, defending against such attacks becomes challenging\nand we urge the research community to give attention to this vulnerability.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 19:41:05 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 01:54:50 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 02:53:22 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Saha", "Aniruddha", ""], ["Subramanya", "Akshayvarun", ""], ["Patil", "Koninika", ""], ["Pirsiavash", "Hamed", ""]]}, {"id": "1910.00095", "submitter": "Shreyas Fadnavis", "authors": "Shreyas Fadnavis, Hamza Farooq, Maryam Afzali, Christoph Lenglet,\n  Tryphon Georgiou, Hu Cheng, Sharlene Newman, Shahnawaz Ahmed, Rafael Neto\n  Henriques, Eric Peterson, Serge Koudoro, Ariel Rokem, Eleftherios\n  Garyfallidis", "title": "Fitting IVIM with Variable Projection and Simplicial Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting multi-exponential models to Diffusion MRI (dMRI) data has always been\nchallenging due to various underlying complexities. In this work, we introduce\na novel and robust fitting framework for the standard two-compartment IVIM\nmicrostructural model. This framework provides a significant improvement over\nthe existing methods and helps estimate the associated diffusion and perfusion\nparameters of IVIM in an automatic manner. As a part of this work we provide\ncapabilities to switch between more advanced global optimization methods such\nas simplicial homology (SH) and differential evolution (DE). Our experiments\nshow that the results obtained from this simultaneous fitting procedure\ndisentangle the model parameters in a reduced subspace. The proposed framework\nextends the seminal work originated in the MIX framework, with improved\nprocedures for multi-stage fitting. This framework has been made available as\nan open-source Python implementation and disseminated to the community through\nthe DIPY project.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 07:40:16 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 20:42:40 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 19:49:11 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Fadnavis", "Shreyas", ""], ["Farooq", "Hamza", ""], ["Afzali", "Maryam", ""], ["Lenglet", "Christoph", ""], ["Georgiou", "Tryphon", ""], ["Cheng", "Hu", ""], ["Newman", "Sharlene", ""], ["Ahmed", "Shahnawaz", ""], ["Henriques", "Rafael Neto", ""], ["Peterson", "Eric", ""], ["Koudoro", "Serge", ""], ["Rokem", "Ariel", ""], ["Garyfallidis", "Eleftherios", ""]]}, {"id": "1910.00099", "submitter": "Wenhao Ding", "authors": "Wenhao Ding, Mengdi Xu, Ding Zhao", "title": "CMTS: Conditional Multiple Trajectory Synthesizer for Generating\n  Safety-critical Driving Scenarios", "comments": "Submitted to ICRA 2020, 8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naturalistic driving trajectories are crucial for the performance of\nautonomous driving algorithms. However, most of the data is collected in safe\nscenarios leading to the duplication of trajectories which are easy to be\nhandled by currently developed algorithms. When considering safety, testing\nalgorithms in near-miss scenarios that rarely show up in off-the-shelf datasets\nis a vital part of the evaluation. As a remedy, we propose a near-miss data\nsynthesizing framework based on Variational Bayesian methods and term it as\nConditional Multiple Trajectory Synthesizer (CMTS). We leverage a generative\nmodel conditioned on road maps to bridge safe and collision driving data by\nrepresenting their distribution in the latent space. By sampling from the\nnear-miss distribution, we can synthesize safety-critical data crucial for\nunderstanding traffic scenarios but not shown in neither the original dataset\nnor the collision dataset. Our experimental results demonstrate that the\naugmented dataset covers more kinds of driving scenarios, especially the\nnear-miss ones, which help improve the trajectory prediction accuracy and the\ncapability of dealing with risky driving scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 13:43:14 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 18:03:36 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ding", "Wenhao", ""], ["Xu", "Mengdi", ""], ["Zhao", "Ding", ""]]}, {"id": "1910.00101", "submitter": "Maymoonah Toubeh", "authors": "Maymoonah Toubeh and Pratap Tokekar", "title": "Risk-Aware Planning by Confidence Estimation using Deep Learning-Based\n  Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes the use of Bayesian approximations of uncertainty from\ndeep learning in a robot planner, showing that this produces more cautious\nactions in safety-critical scenarios. The case study investigated is motivated\nby a setup where an aerial robot acts as a \"scout\" for a ground robot. This is\nuseful when the below area is unknown or dangerous, with applications in space\nexploration, military, or search-and-rescue. Images taken from the aerial view\nare used to provide a less obstructed map to guide the navigation of the robot\non the ground. Experiments are conducted using a deep learning semantic image\nsegmentation, followed by a path planner based on the resulting cost map, to\nprovide an empirical analysis of the proposed method. A comparison with similar\napproaches is presented to portray the usefulness of certain techniques, or\nvariations within a technique, in similar experimental settings. The method is\nanalyzed to assess the impact of variations in the uncertainty extraction, as\nwell as the absence of an uncertainty metric, on the overall system with the\nuse of a defined metric which measures surprise to the planner. The analysis is\nperformed on multiple datasets, showing a similar trend of lower surprise when\nuncertainty information is incorporated in the planning, given threshold values\nof the hyperparameters in the uncertainty extraction have been met. We find\nthat taking uncertainty into account leads to paths that could be 18% less\nrisky on an average.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 15:20:41 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Toubeh", "Maymoonah", ""], ["Tokekar", "Pratap", ""]]}, {"id": "1910.00116", "submitter": "Tony Tung", "authors": "Yuanlu Xu, Song-Chun Zhu, Tony Tung", "title": "DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare", "comments": "11 pages, 8 figures, International Conference on Computer Vision\n  (ICCV) 2019, Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DenseRaC, a novel end-to-end framework for jointly estimating 3D\nhuman pose and body shape from a monocular RGB image. Our two-step framework\ntakes the body pixel-to-surface correspondence map (i.e., IUV map) as proxy\nrepresentation and then performs estimation of parameterized human pose and\nshape. Specifically, given an estimated IUV map, we develop a deep neural\nnetwork optimizing 3D body reconstruction losses and further integrating a\nrender-and-compare scheme to minimize differences between the input and the\nrendered output, i.e., dense body landmarks, body part masks, and adversarial\npriors. To boost learning, we further construct a large-scale synthetic dataset\n(MOCA) utilizing web-crawled Mocap sequences, 3D scans and animations. The\ngenerated data covers diversified camera views, human actions and body shapes,\nand is paired with full ground truth. Our model jointly learns to represent the\n3D human body from hybrid datasets, mitigating the problem of unpaired training\ndata. Our experiments show that DenseRaC obtains superior performance against\nstate of the art on public benchmarks of various humanrelated tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 21:34:31 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 17:52:02 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Xu", "Yuanlu", ""], ["Zhu", "Song-Chun", ""], ["Tung", "Tony", ""]]}, {"id": "1910.00127", "submitter": "Dan Helmick", "authors": "Max Bajracharya, James Borders, Dan Helmick, Thomas Kollar, Michael\n  Laskey, John Leichty, Jeremy Ma, Umashankar Nagarajan, Akiyoshi Ochiai, Josh\n  Petersen, Krishna Shankar, Kevin Stone, Yutaka Takaoka", "title": "A Mobile Manipulation System for One-Shot Teaching of Complex Tasks in\n  Homes", "comments": "The video is available at: https://youtu.be/HSyAGMGikLk. 7 pages, 5\n  figures, accepted by IEEE 2020 Robotics International Conference on Robotics\n  and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a mobile manipulation hardware and software system capable of\nautonomously performing complex human-level tasks in real homes, after being\ntaught the task with a single demonstration from a person in virtual reality.\nThis is enabled by a highly capable mobile manipulation robot, whole-body task\nspace hybrid position/force control, teaching of parameterized primitives\nlinked to a robust learned dense visual embeddings representation of the scene,\nand a task graph of the taught behaviors. We demonstrate the robustness of the\napproach by presenting results for performing a variety of tasks, under\ndifferent environmental conditions, in multiple real homes. Our approach\nachieves 85% overall success rate on three tasks that consist of an average of\n45 behaviors each.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 22:03:07 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 17:44:06 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 21:24:02 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Bajracharya", "Max", ""], ["Borders", "James", ""], ["Helmick", "Dan", ""], ["Kollar", "Thomas", ""], ["Laskey", "Michael", ""], ["Leichty", "John", ""], ["Ma", "Jeremy", ""], ["Nagarajan", "Umashankar", ""], ["Ochiai", "Akiyoshi", ""], ["Petersen", "Josh", ""], ["Shankar", "Krishna", ""], ["Stone", "Kevin", ""], ["Takaoka", "Yutaka", ""]]}, {"id": "1910.00130", "submitter": "Jonathon Luiten", "authors": "Jonathon Luiten and Tobias Fischer and Bastian Leibe", "title": "Track to Reconstruct and Reconstruct to Track", "comments": "RA-L 2020 and ICRA 2020", "journal-ref": "IEEE Robotics and Automation Letters 5.2 (2020): 1803-1810", "doi": "10.1109/LRA.2020.2969183", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking and 3D reconstruction are often performed together, with\ntracking used as input for reconstruction. However, the obtained\nreconstructions also provide useful information for improving tracking. We\npropose a novel method that closes this loop, first tracking to reconstruct,\nand then reconstructing to track. Our approach, MOTSFusion (Multi-Object\nTracking, Segmentation and dynamic object Fusion), exploits the 3D motion\nextracted from dynamic object reconstructions to track objects through long\nperiods of complete occlusion and to recover missing detections. Our approach\nfirst builds up short tracklets using 2D optical flow, and then fuses these\ninto dynamic 3D object reconstructions. The precise 3D object motion of these\nreconstructions is used to merge tracklets through occlusion into long-term\ntracks, and to locate objects when detections are missing. On KITTI, our\nreconstruction-based tracking reduces the number of ID switches of the initial\ntracklets by more than 50%, and outperforms all previous approaches for both\nbounding box and segmentation tracking.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 22:05:59 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 14:37:03 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 04:28:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Luiten", "Jonathon", ""], ["Fischer", "Tobias", ""], ["Leibe", "Bastian", ""]]}, {"id": "1910.00132", "submitter": "Kevin Duarte", "authors": "Kevin Duarte, Yogesh S Rawat, Mubarak Shah", "title": "CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule\n  Routing", "comments": "8 pages, 6 figures, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a capsule-based approach for semi-supervised video\nobject segmentation. Current video object segmentation methods are frame-based\nand often require optical flow to capture temporal consistency across frames\nwhich can be difficult to compute. To this end, we propose a video based\ncapsule network, CapsuleVOS, which can segment several frames at once\nconditioned on a reference frame and segmentation mask. This conditioning is\nperformed through a novel routing algorithm for attention-based efficient\ncapsule selection. We address two challenging issues in video object\nsegmentation: 1) segmentation of small objects and 2) occlusion of objects\nacross time. The issue of segmenting small objects is addressed with a zooming\nmodule which allows the network to process small spatial regions of the video.\nApart from this, the framework utilizes a novel memory module based on\nrecurrent networks which helps in tracking objects when they move out of frame\nor are occluded. The network is trained end-to-end and we demonstrate its\neffectiveness on two benchmark video object segmentation datasets; it\noutperforms current offline approaches on the Youtube-VOS dataset while having\na run-time that is almost twice as fast as competing methods. The code is\npublicly available at https://github.com/KevinDuarte/CapsuleVOS.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 22:12:43 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Duarte", "Kevin", ""], ["Rawat", "Yogesh S", ""], ["Shah", "Mubarak", ""]]}, {"id": "1910.00138", "submitter": "Cosmin Bonchis", "authors": "Victor Bogdan, Cosmin Bonchi\\c{s}, Ciprian Orhei", "title": "Custom Extended Sobel Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge detection is widely and fundamental feature used in various algorithms\nin computer vision to determine the edges in an image. The edge detection\nalgorithm is used to determine the edges in an image which are further used by\nvarious algorithms from line detection to machine learning that can determine\nobjects based on their contour. Inspired by new convolution techniques in\nmachine learning we discuss here the idea of extending the standard Sobel\nkernels, which are used to compute the gradient of an image in order to find\nits edges. We compare the result of our custom extended filters with the\nresults of the standard Sobel filter and other edge detection filters using\ndifferent image sets and algorithms. We present statistical results regarding\nthe custom extended Sobel filters improvements.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 22:30:09 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Bogdan", "Victor", ""], ["Bonchi\u015f", "Cosmin", ""], ["Orhei", "Ciprian", ""]]}, {"id": "1910.00199", "submitter": "Joseph Viviano", "authors": "Joseph D. Viviano, Becks Simpson, Francis Dutil, Yoshua Bengio, and\n  Joseph Paul Cohen", "title": "Saliency is a Possible Red Herring When Diagnosing Poor Generalization", "comments": "25 pages, 27 figures, 5 tables, code in paper\n  (https://github.com/josephdviviano/saliency-red-herring). Published at\n  International Conference on Learning Representations (ICLR) 2021. Previously\n  titled \"Underwhelming Generalization Improvements from Controlling Feature\n  Attribution\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poor generalization is one symptom of models that learn to predict target\nvariables using spuriously-correlated image features present only in the\ntraining distribution instead of the true image features that denote a class.\nIt is often thought that this can be diagnosed visually using attribution (aka\nsaliency) maps. We study if this assumption is correct. In some prediction\ntasks, such as for medical images, one may have some images with masks drawn by\na human expert, indicating a region of the image containing relevant\ninformation to make the prediction. We study multiple methods that take\nadvantage of such auxiliary labels, by training networks to ignore distracting\nfeatures which may be found outside of the region of interest. This mask\ninformation is only used during training and has an impact on generalization\naccuracy depending on the severity of the shift between the training and test\ndistributions. Surprisingly, while these methods improve generalization\nperformance in the presence of a covariate shift, there is no strong\ncorrespondence between the correction of attribution towards the features a\nhuman expert has labelled as important and generalization performance. These\nresults suggest that the root cause of poor generalization may not always be\nspatially defined, and raise questions about the utility of masks as\n\"attribution priors\" as well as saliency maps for explainable predictions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 04:29:18 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 05:25:18 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 16:40:27 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Viviano", "Joseph D.", ""], ["Simpson", "Becks", ""], ["Dutil", "Francis", ""], ["Bengio", "Yoshua", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "1910.00216", "submitter": "Akihiro Nakamura", "authors": "Akihiro Nakamura, Tatsuya Harada", "title": "Revisiting Fine-tuning for Few-shot Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is the process of learning novel classes using only a few\nexamples and it remains a challenging task in machine learning. Many\nsophisticated few-shot learning algorithms have been proposed based on the\nnotion that networks can easily overfit to novel examples if they are simply\nfine-tuned using only a few examples. In this study, we show that in the\ncommonly used low-resolution mini-ImageNet dataset, the fine-tuning method\nachieves higher accuracy than common few-shot learning algorithms in the 1-shot\ntask and nearly the same accuracy as that of the state-of-the-art algorithm in\nthe 5-shot task. We then evaluate our method with more practical tasks, namely\nthe high-resolution single-domain and cross-domain tasks. With both tasks, we\nshow that our method achieves higher accuracy than common few-shot learning\nalgorithms. We further analyze the experimental results and show that: 1) the\nretraining process can be stabilized by employing a low learning rate, 2) using\nadaptive gradient optimizers during fine-tuning can increase test accuracy, and\n3) test accuracy can be improved by updating the entire network when a large\ndomain-shift exists between base and novel classes.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 06:21:50 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 04:53:10 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Nakamura", "Akihiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1910.00272", "submitter": "Samuel St-Jean", "authors": "Samuel St-Jean and Max A. Viergever and Alexander Leemans", "title": "Harmonization of diffusion MRI datasets with adaptive dictionary\n  learning", "comments": "v5 Peer review for Human Brain Mapping v4: Peer review round 2 v3:\n  Peer reviewed version v2: Fix minor text issue + add supp materials v1: To be\n  submitted to Neuroimage", "journal-ref": "Human Brain Mapping, hbm. 25117 (2020)", "doi": "10.1002/hbm.25117", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging is a noninvasive imaging technique that\ncan indirectly infer the microstructure of tissues and provide metrics which\nare subject to normal variability across subjects. Potentially abnormal values\nor features may yield essential information to support analysis of controls and\npatients cohorts, but subtle confounds affecting diffusion MRI, such as those\ndue to difference in scanning protocols or hardware, can lead to systematic\nerrors which could be mistaken for purely biologically driven variations\namongst subjects. In this work, we propose a new harmonization algorithm based\non adaptive dictionary learning to mitigate the unwanted variability caused by\ndifferent scanner hardware while preserving the natural biological variability\npresent in the data. Overcomplete dictionaries, which are learned automatically\nfrom the data and do not require paired samples, are then used to reconstruct\nthe data from a different scanner, removing variability present in the source\nscanner in the process. We use the publicly available database from an\ninternational challenge to evaluate the method, which was acquired on three\ndifferent scanners and with two different protocols, and propose a new mapping\ntowards a scanner-agnostic space. Results show that the effect size of the four\nstudied diffusion metrics is preserved while removing variability attributable\nto the scanner. Experiments with alterations using a free water compartment,\nwhich is not simulated in the training data, shows that the effect size induced\nby the alterations is also preserved after harmonization. The algorithm is\nfreely available and could help multicenter studies in pooling their data,\nwhile removing scanner specific confounds, and increase statistical power in\nthe process.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 09:28:53 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 10:56:13 GMT"}, {"version": "v3", "created": "Sun, 5 Jan 2020 15:02:33 GMT"}, {"version": "v4", "created": "Wed, 19 Feb 2020 20:56:30 GMT"}, {"version": "v5", "created": "Thu, 11 Jun 2020 21:40:28 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["St-Jean", "Samuel", ""], ["Viergever", "Max A.", ""], ["Leemans", "Alexander", ""]]}, {"id": "1910.00287", "submitter": "Attila Szab\\'o", "authors": "Attila Szab\\'o, Givi Meishvili, Paolo Favaro", "title": "Unsupervised Generative 3D Shape Learning from Natural Images", "comments": "The paper is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present, to the best of our knowledge, the first method to\nlearn a generative model of 3D shapes from natural images in a fully\nunsupervised way. For example, we do not use any ground truth 3D or 2D\nannotations, stereo video, and ego-motion during the training. Our approach\nfollows the general strategy of Generative Adversarial Networks, where an image\ngenerator network learns to create image samples that are realistic enough to\nfool a discriminator network into believing that they are natural images. In\ncontrast, in our approach the image generation is split into 2 stages. In the\nfirst stage a generator network outputs 3D objects. In the second, a\ndifferentiable renderer produces an image of the 3D objects from random\nviewpoints. The key observation is that a realistic 3D object should yield a\nrealistic rendering from any plausible viewpoint. Thus, by randomizing the\nchoice of the viewpoint our proposed training forces the generator network to\nlearn an interpretable 3D representation disentangled from the viewpoint. In\nthis work, a 3D representation consists of a triangle mesh and a texture map\nthat is used to color the triangle surface by using the UV-mapping technique.\nWe provide analysis of our learning approach, expose its ambiguities and show\nhow to overcome them. Experimentally, we demonstrate that our method can learn\nrealistic 3D shapes of faces by using only the natural images of the FFHQ\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 10:20:59 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Szab\u00f3", "Attila", ""], ["Meishvili", "Givi", ""], ["Favaro", "Paolo", ""]]}, {"id": "1910.00296", "submitter": "Gianluca Maguolo", "authors": "Loris Nanni, Gianluca Maguolo, Fabio Pancino", "title": "Insect pest image detection and recognition based on bio-inspired\n  methods", "comments": null, "journal-ref": null, "doi": "10.1016/j.ecoinf.2020.101089", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insect pests recognition is necessary for crop protection in many areas of\nthe world. In this paper we propose an automatic classifier based on the fusion\nbetween saliency methods and convolutional neural networks. Saliency methods\nare famous image processing algorithms that highlight the most relevant pixels\nof an image. In this paper, we use three different saliency methods as image\npreprocessing and create three different images for every saliency method.\nHence, we create 3x3=9 new images for every original image to train different\nconvolutional neural networks. We evaluate the performance of every\npreprocessing/network couple and we also evaluate the performance of their\nensemble. We test our approach on both a small dataset and the large IP102\ndataset. Our best ensembles reaches the state of the art accuracy on both the\nsmaller dataset (92.43%) and the IP102 dataset (61.93%), approaching the\nperformance of human experts on the smaller one. Besides, we share our MATLAB\ncode at: https://github.com/LorisNanni/.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 10:40:50 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 00:59:15 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 20:21:37 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Nanni", "Loris", ""], ["Maguolo", "Gianluca", ""], ["Pancino", "Fabio", ""]]}, {"id": "1910.00324", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Ondrej Chum, Cordelia\n  Schmid", "title": "Graph convolutional networks for learning with few clean and many noisy\n  labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of learning a classifier from noisy\nlabels when a few clean labeled examples are given. The structure of clean and\nnoisy data is modeled by a graph per class and Graph Convolutional Networks\n(GCN) are used to predict class relevance of noisy examples. For each class,\nthe GCN is treated as a binary classifier, which learns to discriminate clean\nfrom noisy examples using a weighted binary cross-entropy loss function. The\nGCN-inferred \"clean\" probability is then exploited as a relevance measure. Each\nnoisy example is weighted by its relevance when learning a classifier for the\nend task. We evaluate our method on an extended version of a few-shot learning\nproblem, where the few clean examples of novel classes are supplemented with\nadditional noisy data. Experimental results show that our GCN-based cleaning\nprocess significantly improves the classification accuracy over not cleaning\nthe noisy data, as well as standard few-shot classification where only few\nclean examples are used.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 11:56:09 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 22:01:35 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 21:33:51 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Iscen", "Ahmet", ""], ["Tolias", "Giorgos", ""], ["Avrithis", "Yannis", ""], ["Chum", "Ondrej", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1910.00370", "submitter": "Yijun Bian", "authors": "Yijun Bian, Qingquan Song, Mengnan Du, Jun Yao, Huanhuan Chen, Xia Hu", "title": "Sub-Architecture Ensemble Pruning in Neural Architecture Search", "comments": "Accepted by TNNLS. This work was done when the first author was a\n  visiting research scholar at Texas A&M University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) is gaining more and more attention in recent\nyears due to its flexibility and remarkable capability to reduce the burden of\nneural network design. To achieve better performance, however, the searching\nprocess usually costs massive computations that might not be affordable for\nresearchers and practitioners. While recent attempts have employed ensemble\nlearning methods to mitigate the enormous computational cost, however, they\nneglect a key property of ensemble methods, namely diversity, which leads to\ncollecting more similar sub-architectures with potential redundancy in the\nfinal design. To tackle this problem, we propose a pruning method for NAS\nensembles called \"Sub-Architecture Ensemble Pruning in Neural Architecture\nSearch (SAEP).\" It targets to leverage diversity and to achieve sub-ensemble\narchitectures at a smaller size with comparable performance to ensemble\narchitectures that are not pruned. Three possible solutions are proposed to\ndecide which sub-architectures to prune during the searching process.\nExperimental results exhibit the effectiveness of the proposed method by\nlargely reducing the number of sub-architectures without degrading the\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 13:26:54 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:37:48 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Bian", "Yijun", ""], ["Song", "Qingquan", ""], ["Du", "Mengnan", ""], ["Yao", "Jun", ""], ["Chen", "Huanhuan", ""], ["Hu", "Xia", ""]]}, {"id": "1910.00387", "submitter": "Fei Wu", "authors": "Fei Wu, Thomas Michel and Alexandre Briot", "title": "Leveraging Model Interpretability and Stability to increase Model\n  Robustness", "comments": "2019 ICCV workshop on Interpreting and Explaining Visual AI models; 8\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art Deep Neural Networks (DNN) can now achieve above human level\naccuracy on image classification tasks. However their outstanding performances\ncome along with a complex inference mechanism making them arduously\ninterpretable models. In order to understand the underlying prediction rules of\nDNNs, Dhamdhere et al. propose an interpretability method to break down a DNN\nprediction score as sum of its hidden unit contributions, in the form of a\nmetric called conductance. Analyzing conductances of DNN hidden units, we find\nout there is a difference in how wrong and correct predictions are inferred. We\nidentify distinguishable patterns of hidden unit activations for wrong and\ncorrect predictions. We then use an error detector in the form of a binary\nclassifier on top of the DNN to automatically discriminate wrong and correct\npredictions of the DNN based on their hidden unit activations. Detected wrong\npredictions are discarded, increasing the model robustness. A different\napproach to distinguish wrong and correct predictions of DNNs is proposed by\nWang et al. whose method is based on the premise that input samples leading a\nDNN into making wrong predictions are less stable to the DNN weight changes\nthan correctly classified input samples. In our study, we compare both methods\nand find out by combining them that better detection of wrong predictions can\nbe achieved.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 13:51:56 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 23:23:04 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Wu", "Fei", ""], ["Michel", "Thomas", ""], ["Briot", "Alexandre", ""]]}, {"id": "1910.00443", "submitter": "Johannes Stegmaier", "authors": "Manuel Traub and Johannes Stegmaier", "title": "Towards Automatic Embryo Staging in 3D+T Microscopy Images using\n  Convolutional Neural Networks and PointNets", "comments": "10 pages, 3 figures, 1 table, accepted paper at the Simulation and\n  Synthesis in Medical Imaging (SASHIMI) Workshop held at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic analyses and comparisons of different stages of embryonic\ndevelopment largely depend on a highly accurate spatiotemporal alignment of the\ninvestigated data sets. In this contribution, we assess multiple approaches for\nautomatic staging of developing embryos that were imaged with time-resolved 3D\nlight-sheet microscopy. The methods comprise image-based convolutional neural\nnetworks as well as an approach based on the PointNet architecture that\ndirectly operates on 3D point clouds of detected cell nuclei centroids. The\nexperiments with four wild-type zebrafish embryos render both approaches\nsuitable for automatic staging with average deviations of 21 - 34 minutes.\nMoreover, a proof-of-concept evaluation based on simulated 3D+t point cloud\ndata sets shows that average deviations of less than 7 minutes are possible.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:29:23 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 19:40:13 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 09:01:32 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Traub", "Manuel", ""], ["Stegmaier", "Johannes", ""]]}, {"id": "1910.00462", "submitter": "Ivan Donadello", "authors": "Ivan Donadello and Luciano Serafini", "title": "Compensating Supervision Incompleteness with Prior Knowledge in Semantic\n  Image Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Image Interpretation is the task of extracting a structured semantic\ndescription from images. This requires the detection of visual relationships:\ntriples (subject,relation,object) describing a semantic relation between a\nsubject and an object. A pure supervised approach to visual relationship\ndetection requires a complete and balanced training set for all the possible\ncombinations of (subject, relation, object). However, such training sets are\nnot available and would require a prohibitive human effort. This implies the\nability of predicting triples which do not appear in the training set. This\nproblem is called zero-shot learning. State-of-the-art approaches to zero-shot\nlearning exploit similarities among relationships in the training set or\nexternal linguistic knowledge. In this paper, we perform zero-shot learning by\nusing Logic Tensor Networks, a novel Statistical Relational Learning framework\nthat exploits both the similarities with other seen relationships and\nbackground knowledge, expressed with logical constraints between subjects,\nrelations and objects. The experiments on the Visual Relationship Dataset show\nthat the use of logical constraints outperforms the current methods. This\nimplies that background knowledge can be used to alleviate the incompleteness\nof training sets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:56:08 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Donadello", "Ivan", ""], ["Serafini", "Luciano", ""]]}, {"id": "1910.00470", "submitter": "Ambra Demontis Ph.D.", "authors": "Angelo Sotgiu, Ambra Demontis, Marco Melis, Battista Biggio, Giorgio\n  Fumera, Xiaoyi Feng and Fabio Roli", "title": "Deep Neural Rejection against Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the impressive performances reported by deep neural networks in\ndifferent application domains, they remain largely vulnerable to adversarial\nexamples, i.e., input samples that are carefully perturbed to cause\nmisclassification at test time. In this work, we propose a deep neural\nrejection mechanism to detect adversarial examples, based on the idea of\nrejecting samples that exhibit anomalous feature representations at different\nnetwork layers. With respect to competing approaches, our method does not\nrequire generating adversarial examples at training time, and it is less\ncomputationally demanding. To properly evaluate our method, we define an\nadaptive white-box attack that is aware of the defense mechanism and aims to\nbypass it. Under this worst-case setting, we empirically show that our approach\noutperforms previously-proposed methods that detect adversarial examples by\nonly analyzing the feature representation provided by the output network layer.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:08:34 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 17:51:25 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 13:42:24 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Sotgiu", "Angelo", ""], ["Demontis", "Ambra", ""], ["Melis", "Marco", ""], ["Biggio", "Battista", ""], ["Fumera", "Giorgio", ""], ["Feng", "Xiaoyi", ""], ["Roli", "Fabio", ""]]}, {"id": "1910.00527", "submitter": "Wei Zhang", "authors": "Wei Zhang, Wei Li, Lei Han", "title": "A Three-dimensional Convolutional-Recurrent Network for Convective Storm\n  Nowcasting", "comments": "13 pages, 11 figures, accepted by 2019 IEEE International Conference\n  on Big Knowledge The copyright of this paper has been transferred to the\n  IEEE, please comply with the copyright of the IEEE", "journal-ref": "In 2019 IEEE International Conference on Big Knowledge (ICBK) (pp.\n  333-340) 2019", "doi": "10.1109/ICBK.2019.00052", "report-no": null, "categories": "cs.CV physics.ao-ph physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very short-term convective storm forecasting, termed nowcasting, has long\nbeen an important issue and has attracted substantial interest. Existing\nnowcasting methods rely principally on radar images and are limited in terms of\nnowcasting storm initiation and growth. Real-time re-analysis of meteorological\ndata supplied by numerical models provides valuable information about\nthree-dimensional (3D), atmospheric, boundary layer thermal dynamics, such as\ntemperature and wind. To mine such data, we here develop a\nconvolution-recurrent, hybrid deep-learning method with the following\ncharacteristics: (1) the use of cell-based oversampling to increase the number\nof training samples; this mitigates the class imbalance issue; (2) the use of\nboth raw 3D radar data and 3D meteorological data re-analyzed via multi-source\n3D convolution without any need for handcraft feature engineering; and (3) the\nstacking of convolutional neural networks on a long short-term memory\nencoder/decoder that learns the spatiotemporal patterns of convective\nprocesses. Experimental results demonstrated that our method performs better\nthan other extrapolation methods. Qualitative analysis yielded encouraging\nnowcasting results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 16:28:32 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 14:19:41 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Zhang", "Wei", ""], ["Li", "Wei", ""], ["Han", "Lei", ""]]}, {"id": "1910.00532", "submitter": "David Paulius", "authors": "David Paulius, Yongqiang Huang, Jason Meloncon and Yu Sun", "title": "Manipulation Motion Taxonomy and Coding for Robots", "comments": "IROS 2019 Submission -- 6 pages", "journal-ref": null, "doi": "10.1109/IROS40897.2019.8967754", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a taxonomy of manipulations as seen especially in\ncooking for 1) grouping manipulations from the robotics point of view, 2)\nconsolidating aliases and removing ambiguity for motion types, and 3) provide a\npath to transferring learned manipulations to new unlearned manipulations.\nUsing instructional videos as a reference, we selected a list of common\nmanipulation motions seen in cooking activities grouped into similar motions\nbased on several trajectory and contact attributes. Manipulation codes are then\ndeveloped based on the taxonomy attributes to represent the manipulation\nmotions. The manipulation taxonomy is then used for comparing motion data in\nthe Daily Interactive Manipulation (DIM) data set to reveal their motion\nsimilarities.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 16:34:07 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 04:22:02 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Paulius", "David", ""], ["Huang", "Yongqiang", ""], ["Meloncon", "Jason", ""], ["Sun", "Yu", ""]]}, {"id": "1910.00541", "submitter": "Matteo Poggi", "authors": "Pier Luigi Dovesi, Matteo Poggi, Lorenzo Andraghetti, Miquel Mart\\'i,\n  Hedvig Kjellstr\\\"om, Alessandro Pieropan, Stefano Mattoccia", "title": "Real-Time Semantic Stereo Matching", "comments": "8 pages, 3 figures. Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding is paramount in robotics, self-navigation, augmented\nreality, and many other fields. To fully accomplish this task, an autonomous\nagent has to infer the 3D structure of the sensed scene (to know where it looks\nat) and its content (to know what it sees). To tackle the two tasks, deep\nneural networks trained to infer semantic segmentation and depth from stereo\nimages are often the preferred choices. Specifically, Semantic Stereo Matching\ncan be tackled by either standalone models trained for the two tasks\nindependently or joint end-to-end architectures. Nonetheless, as proposed so\nfar, both solutions are inefficient because requiring two forward passes in the\nformer case or due to the complexity of a single network in the latter,\nalthough jointly tackling both tasks is usually beneficial in terms of\naccuracy. In this paper, we propose a single compact and lightweight\narchitecture for real-time semantic stereo matching. Our framework relies on\ncoarse-to-fine estimations in a multi-stage fashion, allowing: i) very fast\ninference even on embedded devices, with marginal drops in accuracy, compared\nto state-of-the-art networks, ii) trade accuracy for speed, according to the\nspecific application requirements. Experimental results on high-end GPUs as\nwell as on an embedded Jetson TX2 confirm the superiority of semantic stereo\nmatching compared to standalone tasks and highlight the versatility of our\nframework on any hardware and for any application.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 16:52:29 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 20:23:56 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Dovesi", "Pier Luigi", ""], ["Poggi", "Matteo", ""], ["Andraghetti", "Lorenzo", ""], ["Mart\u00ed", "Miquel", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Pieropan", "Alessandro", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "1910.00556", "submitter": "Ronan Fablet", "authors": "Ronan Fablet, Lucas Drumetz, Fran\\c{c}ois Rousseau", "title": "End-to-end learning of energy-based representations for\n  irregularly-sampled signals and images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For numerous domains, including for instance earth observation, medical\nimaging, astrophysics,..., available image and signal datasets often involve\nirregular space-time sampling patterns and large missing data rates. These\nsampling properties may be critical to apply state-of-the-art learning-based\n(e.g., auto-encoders, CNNs,...), fully benefit from the available large-scale\nobservations and reach breakthroughs in the reconstruction and identification\nof processes of interest. In this paper, we address the end-to-end learning of\nrepresentations of signals, images and image sequences from irregularly-sampled\ndata, i.e. when the training data involved missing data. From an analogy to\nBayesian formulation, we consider energy-based representations. Two energy\nforms are investigated: one derived from auto-encoders and one relating to\nGibbs priors. The learning stage of these energy-based representations (or\npriors) involve a joint interpolation issue, which amounts to solving an energy\nminimization problem under observation constraints. Using a\nneural-network-based implementation of the considered energy forms, we can\nstate an end-to-end learning scheme from irregularly-sampled data. We\ndemonstrate the relevance of the proposed representations for different\ncase-studies: namely, multivariate time series, 2D images and image sequences.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:31:31 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Fablet", "Ronan", ""], ["Drumetz", "Lucas", ""], ["Rousseau", "Fran\u00e7ois", ""]]}, {"id": "1910.00572", "submitter": "Cheng Peng", "authors": "Cheng Peng and David Weikersdorfer", "title": "Map as The Hidden Sensor: Fast Odometry-Based Global Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust global localization is essential to robotics\napplications. We propose a novel global localization method that employs the\nmap traversability as a hidden observation. The resulting map-corrected\nodometry localization is able to provide an accurate belief tensor of the robot\nstate. Our method can be used for blind robots in dark or highly reflective\nareas. In contrast to odometry drift in long-term, our method using only\nodometry and the map converges in longterm. Our method can also be integrated\nwith other sensors to boost the localization performance. The algorithm does\nnot have any initial state assumption and tracks all possible robot states at\nall times. Therefore, our method is global and is robust in the event of\nambiguous observations. We parallel each step of our algorithm such that it can\nbe performed in real-time (up to ~ 300 Hz) using GPU. We validate our algorithm\nin different publicly available floor-plans and show that it is able to\nconverge to the ground truth fast while being robust to ambiguities.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 18:28:03 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Peng", "Cheng", ""], ["Weikersdorfer", "David", ""]]}, {"id": "1910.00579", "submitter": "Jesse Zhang", "authors": "Daiyaan Arfeen, Jesse Zhang", "title": "Unsupervised Projection Networks for Generative Adversarial Networks", "comments": "6 Pages, 8 Figures, ICCV 2019 Workshop: Sensing, Understanding and\n  Synthesizing Humans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of unsupervised learning to train projection networks that\nproject onto the latent space of an already trained generator. We apply our\nmethod to a trained StyleGAN, and use our projection network to perform image\nsuper-resolution and clustering of images into semantically identifiable\ngroups.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 23:31:46 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 17:08:33 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Arfeen", "Daiyaan", ""], ["Zhang", "Jesse", ""]]}, {"id": "1910.00618", "submitter": "Maria Bauza", "authors": "Maria Bauza, Ferran Alet, Yen-Chen Lin, Tomas Lozano-Perez, Leslie P.\n  Kaelbling, Phillip Isola, and Alberto Rodriguez", "title": "Omnipush: accurate, diverse, real-world dataset of pushing dynamics with\n  RGB-D video", "comments": "IROS 2019, 8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pushing is a fundamental robotic skill. Existing work has shown how to\nexploit models of pushing to achieve a variety of tasks, including grasping\nunder uncertainty, in-hand manipulation and clearing clutter. Such models,\nhowever, are approximate, which limits their applicability.\n  Learning-based methods can reason directly from raw sensory data with\naccuracy, and have the potential to generalize to a wider diversity of\nscenarios. However, developing and testing such methods requires rich-enough\ndatasets. In this paper we introduce Omnipush, a dataset with high variety of\nplanar pushing behavior.\n  In particular, we provide 250 pushes for each of 250 objects, all recorded\nwith RGB-D and a high precision tracking system. The objects are constructed so\nas to systematically explore key factors that affect pushing --the shape of the\nobject and its mass distribution-- which have not been broadly explored in\nprevious datasets, and allow to study generalization in model learning.\n  Omnipush includes a benchmark for meta-learning dynamic models, which\nrequires algorithms that make good predictions and estimate their own\nuncertainty. We also provide an RGB video prediction benchmark and propose\nother relevant tasks that can be suited with this dataset.\n  Data and code are available at\n\\url{https://web.mit.edu/mcube/omnipush-dataset/}.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 18:58:19 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Bauza", "Maria", ""], ["Alet", "Ferran", ""], ["Lin", "Yen-Chen", ""], ["Lozano-Perez", "Tomas", ""], ["Kaelbling", "Leslie P.", ""], ["Isola", "Phillip", ""], ["Rodriguez", "Alberto", ""]]}, {"id": "1910.00628", "submitter": "Athma Lakshmi Narayanan", "authors": "Athma Narayanan, Avinash Siravuru, Behzad Dariush", "title": "Sensor Fusion: Gated Recurrent Fusion to Learn Driving Behavior from\n  Temporal Multimodal Data", "comments": "Accepted to Robotics and Automation Letters 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Tactical Driver Behavior modeling problem requires understanding of\ndriver actions in complicated urban scenarios from a rich multi modal signals\nincluding video, LiDAR and CAN bus data streams. However, the majority of deep\nlearning research is focused either on learning the vehicle/environment state\n(sensor fusion) or the driver policy (from temporal data), but not both.\nLearning both tasks end-to-end offers the richest distillation of knowledge,\nbut presents challenges in formulation and successful training. In this work,\nwe propose promising first steps in this direction. Inspired by the gating\nmechanisms in LSTM, we propose gated recurrent fusion units (GRFU) that learn\nfusion weighting and temporal weighting simultaneously. We demonstrate it's\nsuperior performance over multimodal and temporal baselines in supervised\nregression and classification tasks, all in the realm of autonomous navigation.\nWe note a 10% improvement in the mAP score over state-of-the-art for tactical\ndriver behavior classification in HDD dataset and a 20% drop in overall Mean\nsquared error for steering action regression on TORCS dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 19:34:09 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 17:31:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Narayanan", "Athma", ""], ["Siravuru", "Avinash", ""], ["Dariush", "Behzad", ""]]}, {"id": "1910.00650", "submitter": "Xiaobo Qu", "authors": "Tieyuan Lu, Xinlin Zhang, Yihui Huang, Yonggui Yang, Gang Guo, Lijun\n  Bao, Feng Huang, Di Guo, Xiaobo Qu", "title": "pISTA-SENSE-ResNet for Parallel MRI Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmr.2020.106790", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging has been widely applied in clinical diagnosis,\nhowever, is limited by its long data acquisition time. Although imaging can be\naccelerated by sparse sampling and parallel imaging, achieving promising\nreconstruction images with a fast reconstruction speed remains a challenge.\nRecently, deep learning approaches have attracted a lot of attention for its\nencouraging reconstruction results but without a proper interpretability. In\nthis letter, to enable high-quality image reconstruction for the parallel\nmagnetic resonance imaging, we design the network structure from the\nperspective of sparse iterative reconstruction and enhance it with the residual\nstructure. The experimental results of a public knee dataset show that compared\nwith the optimization-based method and the latest deep learning parallel\nimaging methods, the proposed network has less error in reconstruction and is\nmore stable under different acceleration factors.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 15:18:05 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Lu", "Tieyuan", ""], ["Zhang", "Xinlin", ""], ["Huang", "Yihui", ""], ["Yang", "Yonggui", ""], ["Guo", "Gang", ""], ["Bao", "Lijun", ""], ["Huang", "Feng", ""], ["Guo", "Di", ""], ["Qu", "Xiaobo", ""]]}, {"id": "1910.00652", "submitter": "Chad DeChant", "authors": "Delia Bullock, Andrew Mangeni, Tyr Wiesner-Hanks, Chad DeChant, Ethan\n  L. Stewart, Nicholas Kaczmar, Judith M. Kolkman, Rebecca J. Nelson, Michael\n  A. Gore, and Hod Lipson", "title": "Automated Weed Detection in Aerial Imagery with Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate the ability to discriminate between cultivated\nmaize plant and grass or grass-like weed image segments using the context\nsurrounding the image segments. While convolutional neural networks have\nbrought state of the art accuracies within object detection, errors arise when\nobjects in different classes share similar features. This scenario often occurs\nwhen objects in images are viewed at too small of a scale to discern distinct\ndifferences in features, causing images to be incorrectly classified or\nlocalized. To solve this problem, we will explore using context when\nclassifying image segments. This technique involves feeding a convolutional\nneural network a central square image along with a border of its direct\nsurroundings at train and test times. This means that although images are\nlabelled at a smaller scale to preserve accurate localization, the network\nclassifies the images and learns features that include the wider context. We\ndemonstrate the benefits of this context technique in the object detection task\nthrough a case study of grass (foxtail) and grass-like (yellow nutsedge) weed\ndetection in maize fields. In this standard situation, adding context alone\nnearly halved the error of the neural network from 7.1% to 4.3%. After only one\nepoch with context, the network also achieved a higher accuracy than the\nnetwork without context did after 50 epochs. The benefits of using the context\ntechnique are likely to particularly evident in agricultural contexts in which\nparts (such as leaves) of several plants may appear similar when not taking\ninto account the context in which those parts appear.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 20:25:55 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 12:22:20 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 19:34:08 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Bullock", "Delia", ""], ["Mangeni", "Andrew", ""], ["Wiesner-Hanks", "Tyr", ""], ["DeChant", "Chad", ""], ["Stewart", "Ethan L.", ""], ["Kaczmar", "Nicholas", ""], ["Kolkman", "Judith M.", ""], ["Nelson", "Rebecca J.", ""], ["Gore", "Michael A.", ""], ["Lipson", "Hod", ""]]}, {"id": "1910.00663", "submitter": "Jonathan Chung", "authors": "Jonathan Chung, Thomas Delteil", "title": "A Computationally Efficient Pipeline Approach to Full Page Offline\n  Handwritten Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Offline handwriting recognition with deep neural networks is usually limited\nto words or lines due to large computational costs. In this paper, a less\ncomputationally expensive full page offline handwritten text recognition\nframework is introduced. This framework includes a pipeline that locates\nhandwritten text with an object detection neural network and recognises the\ntext within the detected regions using features extracted with a multi-scale\nconvolutional neural network (CNN) fed into a bidirectional long short term\nmemory (LSTM) network. This framework achieves comparable error rates to state\nof the art frameworks while using less memory and time. The results in this\npaper demonstrate the potential of this framework and future work can\ninvestigate production ready and deployable handwritten text recognisers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 20:46:03 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 19:58:40 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Chung", "Jonathan", ""], ["Delteil", "Thomas", ""]]}, {"id": "1910.00673", "submitter": "Li Yao", "authors": "Tobi Olatunji, Li Yao", "title": "Learning to estimate label uncertainty for automatic radiology report\n  parsing", "comments": "Med-NeurIPS-2019 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrapping labels from radiology reports has become the scalable\nalternative to provide inexpensive ground truth for medical imaging. Because of\nthe domain specific nature, state-of-the-art report labeling tools are\npredominantly rule-based. These tools, however, typically yield a binary 0 or 1\nprediction that indicates the presence or absence of abnormalities. These hard\ntargets are then used as ground truth to train image models in the downstream,\nforcing models to express high degree of certainty even on cases where\nspecificity is low. This could negatively impact the statistical efficiency of\nimage models. We address such an issue by training a Bidirectional Long-Short\nTerm Memory Network to augment heuristic-based discrete labels of X-ray reports\nfrom all body regions and achieve performance comparable or better than\ndomain-specific NLP, but with additional uncertainty estimates which enable\nfiner downstream image model training.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 21:20:33 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Olatunji", "Tobi", ""], ["Yao", "Li", ""]]}, {"id": "1910.00679", "submitter": "Bernd Pfrommer", "authors": "Bernd Pfrommer and Kostas Daniilidis", "title": "TagSLAM: Robust SLAM with Fiducial Markers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TagSLAM provides a convenient, flexible, and robust way of performing\nSimultaneous Localization and Mapping (SLAM) with AprilTag fiducial markers. By\nleveraging a few simple abstractions (bodies, tags, cameras), TagSLAM provides\na front end to the GTSAM factor graph optimizer that makes it possible to\nrapidly design a range of experiments that are based on tags: full SLAM,\nextrinsic camera calibration with non-overlapping views, visual localization\nfor ground truth, loop closure for odometry, pose estimation etc. We discuss in\ndetail how TagSLAM initializes the factor graph in a robust way, and present\nloop closure as an application example. TagSLAM is a ROS based open source\npackage and can be found at https://berndpfrommer.github.io/tagslam_web.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 21:28:03 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Pfrommer", "Bernd", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1910.00694", "submitter": "Aayush Chaudhary", "authors": "Aayush K.Chaudhary, Rakshit Kothari, Manoj Acharya, Shusil Dangi,\n  Nitinraj Nair, Reynold Bailey, Christopher Kanan, Gabriel Diaz, Jeff B. Pelz", "title": "RITnet: Real-time Semantic Segmentation of the Eye for Gaze Tracking", "comments": "This model is the winning submission for OpenEDS Semantic\n  Segmentation Challenge for Eye images\n  https://research.fb.com/programs/openeds-challenge/. To appear in ICCVW 2019.\n  (\"Pre-trained models and source code are available\n  https://bitbucket.org/eye-ush/ritnet/.\")", "journal-ref": null, "doi": "10.1109/ICCVW.2019.00568", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate eye segmentation can improve eye-gaze estimation and support\ninteractive computing based on visual attention; however, existing eye\nsegmentation methods suffer from issues such as person-dependent accuracy, lack\nof robustness, and an inability to be run in real-time. Here, we present the\nRITnet model, which is a deep neural network that combines U-Net and DenseNet.\nRITnet is under 1 MB and achieves 95.3\\% accuracy on the 2019 OpenEDS Semantic\nSegmentation challenge. Using a GeForce GTX 1080 Ti, RITnet tracks at $>$\n300Hz, enabling real-time gaze tracking applications. Pre-trained models and\nsource code are available https://bitbucket.org/eye-ush/ritnet/.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:06:49 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Chaudhary", "Aayush K.", ""], ["Kothari", "Rakshit", ""], ["Acharya", "Manoj", ""], ["Dangi", "Shusil", ""], ["Nair", "Nitinraj", ""], ["Bailey", "Reynold", ""], ["Kanan", "Christopher", ""], ["Diaz", "Gabriel", ""], ["Pelz", "Jeff B.", ""]]}, {"id": "1910.00701", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang, Han Zhang, Sercan O. Arik, Honglak Lee, Tomas Pfister", "title": "Distilling Effective Supervision from Severe Label Noise", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting large-scale data with clean labels for supervised training of\nneural networks is practically challenging. Although noisy labels are usually\ncheap to acquire, existing methods suffer a lot from label noise. This paper\ntargets at the challenge of robust training at high label noise regimes. The\nkey insight to achieve this goal is to wisely leverage a small trusted set to\nestimate exemplar weights and pseudo labels for noisy data in order to reuse\nthem for supervised training. We present a holistic framework to train deep\nneural networks in a way that is highly invulnerable to label noise. Our method\nsets the new state of the art on various types of label noise and achieves\nexcellent performance on large-scale datasets with real-world label noise. For\ninstance, on CIFAR100 with a $40\\%$ uniform noise ratio and only 10 trusted\nlabeled data per class, our method achieves $80.2{\\pm}0.3\\%$ classification\naccuracy, where the error rate is only $1.4\\%$ higher than a neural network\ntrained without label noise. Moreover, increasing the noise ratio to $80\\%$,\nour method still maintains a high accuracy of $75.5{\\pm}0.2\\%$, compared to the\nprevious best accuracy $48.2\\%$.\n  Source code available:\nhttps://github.com/google-research/google-research/tree/master/ieg\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:34:29 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 22:06:28 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2019 23:50:48 GMT"}, {"version": "v4", "created": "Mon, 30 Mar 2020 16:59:37 GMT"}, {"version": "v5", "created": "Fri, 12 Jun 2020 23:58:13 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Zizhao", ""], ["Zhang", "Han", ""], ["Arik", "Sercan O.", ""], ["Lee", "Honglak", ""], ["Pfister", "Tomas", ""]]}, {"id": "1910.00713", "submitter": "Maani Ghaffari Jadidi", "authors": "Tzu-Yuan Lin, William Clark, Ryan M. Eustice, Jessy W. Grizzle,\n  Anthony Bloch, Maani Ghaffari", "title": "Adaptive Continuous Visual Odometry from RGB-D Images", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the recently developed continuous visual odometry\nframework for RGB-D cameras to an adaptive framework via online hyperparameter\nlearning. We focus on the case of isotropic kernels with a scalar as the\nlength-scale. In practice and as expected, the length-scale has remarkable\nimpacts on the performance of the original framework. Previously it was handled\nusing a fixed set of conditions within the solver to reduce the length-scale as\nthe algorithm reaches a local minimum. We automate this process by a greedy\ngradient descent step at each iteration to find the next-best length-scale.\nFurthermore, to handle failure cases in the gradient descent step where the\ngradient is not well-behaved, such as the absence of structure or texture in\nthe scene, we use a search interval for the length-scale and guide it gradually\ntoward the smaller values. This latter strategy reverts the adaptive framework\nto the original setup. The experimental evaluations using publicly available\nRGB-D benchmarks show the proposed adaptive continuous visual odometry\noutperforms the original framework and the current state-of-the-art. We also\nmake the software for the developed algorithm publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 23:29:16 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Lin", "Tzu-Yuan", ""], ["Clark", "William", ""], ["Eustice", "Ryan M.", ""], ["Grizzle", "Jessy W.", ""], ["Bloch", "Anthony", ""], ["Ghaffari", "Maani", ""]]}, {"id": "1910.00714", "submitter": "Clebeson Santos Msc", "authors": "Clebeson Canuto, Plinio Moreno, Jorge Samatelo, Raquel Vassallo,\n  Jos\\'e Santos-Victor", "title": "Action Anticipation for Collaborative Environments: The Impact of\n  Contextual Information and Uncertainty-Based Prediction", "comments": "27 pages, 16 figures, Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To interact with humans in collaborative environments, machines need to be\nable to predict (i.e., anticipate) future events, and execute actions in a\ntimely manner. However, the observation of the human limb movements may not be\nsufficient to anticipate their actions unambiguously. In this work, we consider\ntwo additional sources of information (i.e., context) over time, gaze, movement\nand object information, and study how these additional contextual cues improve\nthe action anticipation performance. We address action anticipation as a\nclassification task, where the model takes the available information as the\ninput and predicts the most likely action. We propose to use the uncertainty\nabout each prediction as an online decision-making criterion for action\nanticipation. Uncertainty is modeled as a stochastic process applied to a\ntime-based neural network architecture, which improves the conventional\nclass-likelihood (i.e., deterministic) criterion. The main contributions of\nthis paper are four-fold: (i) We propose a novel and effective decision-making\ncriterion that can be used to anticipate actions even in situations of high\nambiguity; (ii) we propose a deep architecture that outperforms previous\nresults in the action anticipation task when using the Acticipate collaborative\ndataset; (iii) we show that contextual information is important to disambiguate\nthe interpretation of similar actions; and (iv) we also provide a formal\ndescription of three existing performance metrics that can be easily used to\nevaluate action anticipation models.Our results on the Acticipate dataset\nshowed the importance of contextual information and the uncertainty criterion\nfor action anticipation. We achieve an average accuracy of 98.75% in the\nanticipation task using only an average of 25% of observations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 23:30:08 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 06:17:03 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Canuto", "Clebeson", ""], ["Moreno", "Plinio", ""], ["Samatelo", "Jorge", ""], ["Vassallo", "Raquel", ""], ["Santos-Victor", "Jos\u00e9", ""]]}, {"id": "1910.00721", "submitter": "Zheming Zhou", "authors": "Zheming Zhou, Xiaotong Chen, Odest Chadwicke Jenkins", "title": "LIT: Light-field Inference of Transparency for Refractive Object\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translucency is prevalent in everyday scenes. As such, perception of\ntransparent objects is essential for robots to perform manipulation. Compared\nwith texture-rich or texture-less Lambertian objects, transparency induces\nsignificant uncertainty on object appearances. Ambiguity can be due to changes\nin lighting, viewpoint, and backgrounds, each of which brings challenges to\nexisting object pose estimation algorithms. In this work, we propose LIT, a\ntwo-stage method for transparent object pose estimation using light-field\nsensing and photorealistic rendering. LIT employs multiple filters specific to\nlight-field imagery in deep networks to capture transparent material\nproperties, with robust depth and pose estimators based on generative sampling.\nAlong with the LIT algorithm, we introduce the light-field transparent object\ndataset ProLIT for the tasks of recognition, localization and pose estimation.\nWith respect to this ProLIT dataset, we demonstrate that LIT can outperform\nboth state-of-the-art end-to-end pose estimation methods and a generative pose\nestimator on transparent objects.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 00:15:00 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 02:27:16 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 01:42:34 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2020 17:22:39 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zhou", "Zheming", ""], ["Chen", "Xiaotong", ""], ["Jenkins", "Odest Chadwicke", ""]]}, {"id": "1910.00722", "submitter": "Sudhir Sornapudi", "authors": "Sudhir Sornapudi, G. T. Brown, Zhiyun Xue, Rodney Long, Lisa Allen,\n  Sameer Antani", "title": "Comparing Deep Learning Models for Multi-cell Classification in\n  Liquid-based Cervical Cytology Images", "comments": "AMIA 2019 Annual Symposium, Washington DC", "journal-ref": "AMIA Annu Symp Proc. 2019 (2019) 820-827", "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid-based cytology (LBC) is a reliable automated technique for the\nscreening of Papanicolaou (Pap) smear data. It is an effective technique for\ncollecting a majority of the cervical cells and aiding cytopathologists in\nlocating abnormal cells. Most methods published in the research literature rely\non accurate cell segmentation as a prior, which remains challenging due to a\nvariety of factors, e.g., stain consistency, presence of clustered cells, etc.\nWe propose a method for automatic classification of cervical slide images\nthrough generation of labeled cervical patch data and extracting deep\nhierarchical features by fine-tuning convolution neural networks, as well as a\nnovel graph-based cell detection approach for cellular level evaluation. The\nresults show that the proposed pipeline can classify images of both single cell\nand overlapping cells. The VGG-19 model is found to be the best at classifying\nthe cervical cytology patch data with 95 % accuracy under precision-recall\ncurve.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 00:20:23 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Sornapudi", "Sudhir", ""], ["Brown", "G. T.", ""], ["Xue", "Zhiyun", ""], ["Long", "Rodney", ""], ["Allen", "Lisa", ""], ["Antani", "Sameer", ""]]}, {"id": "1910.00724", "submitter": "Souvik Kundu", "authors": "Souvik Kundu, Saurav Prakash, Haleh Akrami, Peter A. Beerel, Keith M.\n  Chugg", "title": "A Pre-defined Sparse Kernel Based Convolution for Deep CNNs", "comments": "8 pages, 12 figures, Computer vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high demand for computational and storage resources severely impede the\ndeployment of deep convolutional neural networks (CNNs) in limited-resource\ndevices. Recent CNN architectures have proposed reduced complexity versions\n(e.g. SuffleNet and MobileNet) but at the cost of modest decreases inaccuracy.\nThis paper proposes pSConv, a pre-defined sparse 2D kernel-based convolution,\nwhich promises significant improvements in the trade-off between complexity and\naccuracy for both CNN training and inference. To explore the potential of this\napproach, we have experimented with two widely accepted datasets, CIFAR-10 and\nTiny ImageNet, in sparse variants of both the ResNet18 and VGG16 architectures.\nOur approach shows a parameter count reduction of up to 4.24x with modest\ndegradation in classification accuracy relative to that of standard CNNs. Our\napproach outperforms a popular variant of ShuffleNet using a variant of\nResNet18 with pSConv having 3x3 kernels with only four of nine elements not\nfixed at zero. In particular, the parameter count is reduced by 1.7x for\nCIFAR-10 and 2.29x for Tiny ImageNet with an increased accuracy of ~4%.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 00:38:38 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 16:31:05 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Kundu", "Souvik", ""], ["Prakash", "Saurav", ""], ["Akrami", "Haleh", ""], ["Beerel", "Peter A.", ""], ["Chugg", "Keith M.", ""]]}, {"id": "1910.00726", "submitter": "Gaurav Mittal", "authors": "Gaurav Mittal and Baoyuan Wang", "title": "Animating Face using Disentangled Audio Representations", "comments": "Accepted at WACV 2020 (Winter conference on Applications of Computer\n  Vision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All previous methods for audio-driven talking head generation assume the\ninput audio to be clean with a neutral tone. As we show empirically, one can\neasily break these systems by simply adding certain background noise to the\nutterance or changing its emotional tone (to such as sad). To make talking head\ngeneration robust to such variations, we propose an explicit audio\nrepresentation learning framework that disentangles audio sequences into\nvarious factors such as phonetic content, emotional tone, background noise and\nothers. We conduct experiments to validate that conditioned on disentangled\ncontent representation, the generated mouth movement by our model is\nsignificantly more accurate than previous approaches (without disentangled\nlearning) in the presence of noise and emotional variations. We further\ndemonstrate that our framework is compatible with current state-of-the-art\napproaches by replacing their original audio learning component with ours. To\nour best knowledge, this is the first work which improves the performance of\ntalking head generation from disentangled audio representation perspective,\nwhich is important for many real-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 00:47:19 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Mittal", "Gaurav", ""], ["Wang", "Baoyuan", ""]]}, {"id": "1910.00727", "submitter": "Varun Chandrasekaran", "authors": "Lakshya Jain, Varun Chandrasekaran, Uyeong Jang, Wilson Wu, Andrew\n  Lee, Andy Yan, Steven Chen, Somesh Jha, Sanjit A. Seshia", "title": "Analyzing and Improving Neural Networks by Generating Semantic\n  Counterexamples through Differentiable Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even as deep neural networks (DNNs) have achieved remarkable success on\nvision-related tasks, their performance is brittle to transformations in the\ninput. Of particular interest are semantic transformations that model changes\nthat have a basis in the physical world, such as rotations, translations,\nchanges in lighting or camera pose. In this paper, we show how differentiable\nrendering can be utilized to generate images that are informative, yet\nrealistic, and which can be used to analyze DNN performance and improve its\nrobustness through data augmentation. Given a differentiable renderer and a\nDNN, we show how to use off-the-shelf attacks from adversarial machine learning\nto generate semantic counterexamples -- images where semantic features are\nchanged as to produce misclassifications or misdetections. We validate our\napproach on DNNs for image classification and object detection. For\nclassification, we show that semantic counterexamples, when used to augment the\ndataset, (i) improve generalization performance (ii) enhance robustness to\nsemantic transformations, and (iii) transfer between models. Additionally, in\ncomparison to sampling-based semantic augmentation, our technique generates\nmore informative data in a sample efficient manner.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 00:47:57 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 22:08:36 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Jain", "Lakshya", ""], ["Chandrasekaran", "Varun", ""], ["Jang", "Uyeong", ""], ["Wu", "Wilson", ""], ["Lee", "Andrew", ""], ["Yan", "Andy", ""], ["Chen", "Steven", ""], ["Jha", "Somesh", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1910.00736", "submitter": "Jizong Peng", "authors": "Xuan Li, Yuchen Lu, Peng Xu, Jizong Peng, Christian Desrosiers, Xue\n  Liu", "title": "Boosting Image Recognition with Non-differentiable Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of image recognition with\nnon-differentiable constraints. A lot of real-life recognition applications\nrequire a rich output structure with deterministic constraints that are\ndiscrete or modeled by a non-differentiable function. A prime example is\nrecognizing digit sequences, which are restricted by such rules (e.g.,\n\\textit{container code detection}, \\textit{social insurance number\nrecognition}, etc.). We investigate the usefulness of adding non-differentiable\nconstraints in learning for the task of digit sequence recognition. Toward this\ngoal, we synthesize six different datasets from MNIST and Cropped SVHN, with\nthree discrete rules inspired by real-life protocols. To deal with the\nnon-differentiability of these rules, we propose a reinforcement learning\napproach based on the policy gradient method. We find that incorporating this\nrule-based reinforcement can effectively increase the accuracy for all datasets\nand provide a good inductive bias which improves the model even with limited\ndata. On one of the datasets, MNIST\\_Rule2, models trained with rule-based\nreinforcement increase the accuracy by 4.7\\% for 2000 samples and 23.6\\% for\n500 samples. We further test our model against synthesized adversarial\nexamples, e.g., blocking out digits, and observe that adding our rule-based\nreinforcement increases the model robustness with a relatively smaller\nperformance drop.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 01:21:21 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Li", "Xuan", ""], ["Lu", "Yuchen", ""], ["Xu", "Peng", ""], ["Peng", "Jizong", ""], ["Desrosiers", "Christian", ""], ["Liu", "Xue", ""]]}, {"id": "1910.00741", "submitter": "Shresth Verma", "authors": "Shresth Verma, Joydip Dhar", "title": "Emergence of Writing Systems Through Multi-Agent Cooperation", "comments": "Under Review as Student Abstract at AAAI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CL cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to communicate is considered an essential task to develop a general\nAI. While recent literature in language evolution has studied emergent language\nthrough discrete or continuous message symbols, there has been little work in\nthe emergence of writing systems in artificial agents. In this paper, we\npresent a referential game setup with two agents, where the mode of\ncommunication is a written language system that emerges during the play. We\nshow that the agents can learn to coordinate successfully using this mode of\ncommunication. Further, we study how the game rules affect the writing system\ntaxonomy by proposing a consistency metric.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 01:35:14 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Verma", "Shresth", ""], ["Dhar", "Joydip", ""]]}, {"id": "1910.00748", "submitter": "Nikita Srivatsan", "authors": "Nikita Srivatsan, Jonathan T. Barron, Dan Klein, Taylor\n  Berg-Kirkpatrick", "title": "A Deep Factorization of Style and Structure in Fonts", "comments": "EMNLP 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep factorization model for typographic analysis that\ndisentangles content from style. Specifically, a variational inference\nprocedure factors each training glyph into the combination of a\ncharacter-specific content embedding and a latent font-specific style variable.\nThe underlying generative model combines these factors through an asymmetric\ntranspose convolutional process to generate the image of the glyph itself. When\ntrained on corpora of fonts, our model learns a manifold over font styles that\ncan be used to analyze or reconstruct new, unseen fonts. On the task of\nreconstructing missing glyphs from an unknown font given only a small number of\nobservations, our model outperforms both a strong nearest neighbors baseline\nand a state-of-the-art discriminative model from prior work.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 02:24:12 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 01:43:18 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Srivatsan", "Nikita", ""], ["Barron", "Jonathan T.", ""], ["Klein", "Dan", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1910.00754", "submitter": "Sangryul Jeon", "authors": "Sangryul Jeon, Dongbo Min, Seungryong Kim, Kwanghoon Sohn", "title": "Joint Learning of Semantic Alignment and Object Landmark Detection", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) based approaches for semantic alignment\nand object landmark detection have improved their performance significantly.\nCurrent efforts for the two tasks focus on addressing the lack of massive\ntraining data through weakly- or unsupervised learning frameworks. In this\npaper, we present a joint learning approach for obtaining dense correspondences\nand discovering object landmarks from semantically similar images. Based on the\nkey insight that the two tasks can mutually provide supervisions to each other,\nour networks accomplish this through a joint loss function that alternatively\nimposes a consistency constraint between the two tasks, thereby boosting the\nperformance and addressing the lack of training data in a principled manner. To\nthe best of our knowledge, this is the first attempt to address the lack of\ntraining data for the two tasks through the joint learning. To further improve\nthe robustness of our framework, we introduce a probabilistic learning\nformulation that allows only reliable matches to be used in the joint learning\nprocess. With the proposed method, state-of-the-art performance is attained on\nseveral standard benchmarks for semantic matching and landmark detection,\nincluding a newly introduced dataset, JLAD, which contains larger number of\nchallenging image pairs than existing datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 02:43:47 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Jeon", "Sangryul", ""], ["Min", "Dongbo", ""], ["Kim", "Seungryong", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1910.00780", "submitter": "Kartikeya Bhardwaj", "authors": "Kartikeya Bhardwaj, Guihong Li, Radu Marculescu", "title": "How does topology influence gradient propagation and model performance\n  of deep networks with DenseNet-type skip connections?", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DenseNets introduce concatenation-type skip connections that achieve\nstate-of-the-art accuracy in several computer vision tasks. In this paper, we\nreveal that the topology of the concatenation-type skip connections is closely\nrelated to the gradient propagation which, in turn, enables a predictable\nbehavior of DNNs' test performance. To this end, we introduce a new metric\ncalled NN-Mass to quantify how effectively information flows through DNNs.\nMoreover, we empirically show that NN-Mass also works for other types of skip\nconnections, e.g., for ResNets, Wide-ResNets (WRNs), and MobileNets, which\ncontain addition-type skip connections (i.e., residuals or inverted residuals).\nAs such, for both DenseNet-like CNNs and ResNets/WRNs/MobileNets, our\ntheoretically grounded NN-Mass can identify models with similar accuracy,\ndespite having significantly different size/compute requirements. Detailed\nexperiments on both synthetic and real datasets (e.g., MNIST, CIFAR-10,\nCIFAR-100, ImageNet) provide extensive evidence for our insights. Finally, the\nclosed-form equation of our NN-Mass enables us to design significantly\ncompressed DenseNets (for CIFAR-10) and MobileNets (for ImageNet) directly at\ninitialization without time-consuming training and/or searching.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 05:25:47 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 02:51:57 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 00:04:30 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bhardwaj", "Kartikeya", ""], ["Li", "Guihong", ""], ["Marculescu", "Radu", ""]]}, {"id": "1910.00895", "submitter": "Ayush Gaud", "authors": "Ayush Gaud, Y V S Harish and K Madhava Krishna", "title": "Object Parsing in Sequences Using CoordConv Gated Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a monocular object parsing framework for consistent keypoint\nlocalization by capturing temporal correlation on sequential data. In this\npaper, we propose a novel recurrent network based architecture to model\nlong-range dependencies between intermediate features which are highly useful\nin tasks like keypoint localization and tracking. We leverage the\nexpressiveness of the popular stacked hourglass architecture and augment it by\nadopting memory units between intermediate layers of the network with weights\nshared across stages for video frames. We observe that this weight sharing\nscheme not only enables us to frame hourglass architecture as a recurrent\nnetwork but also prove to be highly effective in producing increasingly refined\nestimates for sequential tasks. Furthermore, we propose a new memory cell, we\ncall CoordConvGRU which learns to selectively preserve spatio-temporal\ncorrelation and showcase our results on the keypoint localization task. The\nexperiments show that our approach is able to model the motion dynamics between\nthe frames and significantly outperforms the baseline hourglass network. Even\nthough our network is trained on a synthetically rendered dataset, we observe\nthat with minimal fine tuning on 300 real images we are able to achieve\nperformance at par with various state-of-the-art methods trained with the same\nlevel of supervisory inputs. By using a simpler architecture than other methods\nenables us to run it in real time on a standard GPU which is desirable for such\napplications. Finally, we make our architectures and 524 annotated sequences of\ncars from KITTI dataset publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 11:58:16 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Gaud", "Ayush", ""], ["Harish", "Y V S", ""], ["Krishna", "K Madhava", ""]]}, {"id": "1910.00932", "submitter": "Ji Lin", "authors": "Ji Lin, Chuang Gan, Song Han", "title": "Training Kinetics in 15 Minutes: Large-scale Distributed Training on\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep video recognition is more computationally expensive than image\nrecognition, especially on large-scale datasets like Kinetics [1]. Therefore,\ntraining scalability is essential to handle a large amount of videos. In this\npaper, we study the factors that impact the training scalability of video\nnetworks. We recognize three bottlenecks, including data loading (data movement\nfrom disk to GPU), communication (data movement over networking), and\ncomputation FLOPs. We propose three design guidelines to improve the\nscalability: (1) fewer FLOPs and hardware-friendly operator to increase the\ncomputation efficiency; (2) fewer input frames to reduce the data movement and\nincrease the data loading efficiency; (3) smaller model size to reduce the\nnetworking traffic and increase the networking efficiency. With these\nguidelines, we designed a new operator Temporal Shift Module (TSM) that is\nefficient and scalable for distributed training. TSM model can achieve 1.8x\nhigher throughput compared to previous I3D models. We scale up the training of\nthe TSM model to 1,536 GPUs, with a mini-batch of 12,288 video clips/98,304\nimages, without losing the accuracy. With such hardware-aware model design, we\nare able to scale up the training on Summit supercomputer and reduce the\ntraining time on Kinetics dataset from 49 hours 55 minutes to 14 minutes 13\nseconds, achieving a top-1 accuracy of 74.0%, which is 1.6x and 2.9x faster\nthan previous 3D video models with higher accuracy. The code and more details\ncan be found here: http://tsm-hanlab.mit.edu.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:58:07 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 23:04:39 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Lin", "Ji", ""], ["Gan", "Chuang", ""], ["Han", "Song", ""]]}, {"id": "1910.00944", "submitter": "Alberto De Souza", "authors": "Pedro Azevedo, Sabrina S. Panceri, R\\^anik Guidolini, Vinicius B.\n  Cardoso, Claudine Badue, Thiago Oliveira-Santos and Alberto F. De Souza", "title": "Bio-Inspired Foveated Technique for Augmented-Range Vehicle Detection\n  Using Deep Neural Networks", "comments": "Paper accepted at IJCNN 2019", "journal-ref": null, "doi": "10.1109/IJCNN.2019.8851947", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a bio-inspired foveated technique to detect cars in a long range\ncamera view using a deep convolutional neural network (DCNN) for the IARA\nself-driving car. The DCNN receives as input (i) an image, which is captured by\na camera installed on IARA's roof; and (ii) crops of the image, which are\ncentered in the waypoints computed by IARA's path planner and whose sizes\nincrease with the distance from IARA. We employ an overlap filter to discard\ndetections of the same car in different crops of the same image based on the\npercentage of overlap of detections' bounding boxes. We evaluated the\nperformance of the proposed augmented-range vehicle detection system (ARVDS)\nusing the hardware and software infrastructure available in the IARA\nself-driving car. Using IARA, we captured thousands of images of real traffic\nsituations containing cars in a long range. Experimental results show that\nARVDS increases the Average Precision (AP) of long range car detection from\n29.51% (using a single whole image) to 63.15%.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 13:37:58 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Azevedo", "Pedro", ""], ["Panceri", "Sabrina S.", ""], ["Guidolini", "R\u00e2nik", ""], ["Cardoso", "Vinicius B.", ""], ["Badue", "Claudine", ""], ["Oliveira-Santos", "Thiago", ""], ["De Souza", "Alberto F.", ""]]}, {"id": "1910.00950", "submitter": "Youngeun Kim", "authors": "Youngeun Kim, Seunghyeon Kim, Taekyung Kim and Changick Kim", "title": "CNN-based Semantic Segmentation using Level Set Loss", "comments": "2019 IEEE Winter Conference on Applications of Computer Vision\n  (WACV). IEEE, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thesedays, Convolutional Neural Networks are widely used in semantic\nsegmentation. However, since CNN-based segmentation networks produce\nlow-resolution outputs with rich semantic information, it is inevitable that\nspatial details (e.g., small bjects and fine boundary information) of\nsegmentation results will be lost. To address this problem, motivated by a\nvariational approach to image segmentation (i.e., level set theory), we propose\na novel loss function called the level set loss which is designed to refine\nspatial details of segmentation results. To deal with multiple classes in an\nimage, we first decompose the ground truth into binary images. Note that each\nbinary image consists of background and regions belonging to a class. Then we\nconvert level set functions into class probability maps and calculate the\nenergy for each class. The network is trained to minimize the weighted sum of\nthe level set loss and the cross-entropy loss. The proposed level set loss\nimproves the spatial details of segmentation results in a time and memory\nefficient way. Furthermore, our experimental results show that the proposed\nloss function achieves better performance than previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 13:45:33 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Kim", "Youngeun", ""], ["Kim", "Seunghyeon", ""], ["Kim", "Taekyung", ""], ["Kim", "Changick", ""]]}, {"id": "1910.00956", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Jaime L. Shaw, Yibin Xie, Debiao Li, Anthony G.\n  Christodoulou", "title": "Deep learning within a priori temporal feature spaces for large-scale\n  dynamic MR image reconstruction: Application to 5-D cardiac MR Multitasking", "comments": "Early accepted by MICCAI 2019", "journal-ref": "Medical Image Computing and Computer Assisted Intervention 2019 pp\n  495-504", "doi": "10.1007/978-3-030-32245-8_55", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High spatiotemporal resolution dynamic magnetic resonance imaging (MRI) is a\npowerful clinical tool for imaging moving structures as well as to reveal and\nquantify other physical and physiological dynamics. The low speed of MRI\nnecessitates acceleration methods such as deep learning reconstruction from\nunder-sampled data. However, the massive size of many dynamic MRI problems\nprevents deep learning networks from directly exploiting global temporal\nrelationships. In this work, we show that by applying deep neural networks\ninside a priori calculated temporal feature spaces, we enable deep learning\nreconstruction with global temporal modeling even for image sequences with\n>40,000 frames. One proposed variation of our approach using dilated\nmulti-level Densely Connected Network (mDCN) speeds up feature space coordinate\ncalculation by 3000x compared to conventional iterative methods, from 20\nminutes to 0.39 seconds. Thus, the combination of low-rank tensor and deep\nlearning models not only makes large-scale dynamic MRI feasible but also\npractical for routine clinical application.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 13:52:38 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Chen", "Yuhua", ""], ["Shaw", "Jaime L.", ""], ["Xie", "Yibin", ""], ["Li", "Debiao", ""], ["Christodoulou", "Anthony G.", ""]]}, {"id": "1910.00962", "submitter": "Wenqi Li", "authors": "Wenqi Li, Fausto Milletar\\`i, Daguang Xu, Nicola Rieke, Jonny Hancox,\n  Wentao Zhu, Maximilian Baust, Yan Cheng, S\\'ebastien Ourselin, M. Jorge\n  Cardoso, Andrew Feng", "title": "Privacy-preserving Federated Brain Tumour Segmentation", "comments": "MICCAI MLMI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to medical data privacy regulations, it is often infeasible to collect\nand share patient data in a centralised data lake. This poses challenges for\ntraining machine learning algorithms, such as deep convolutional networks,\nwhich often require large numbers of diverse training examples. Federated\nlearning sidesteps this difficulty by bringing code to the patient data owners\nand only sharing intermediate model training updates among them. Although a\nhigh-accuracy model could be achieved by appropriately aggregating these model\nupdates, the model shared could indirectly leak the local training examples. In\nthis paper, we investigate the feasibility of applying differential-privacy\ntechniques to protect the patient data in a federated learning setup. We\nimplement and evaluate practical federated learning systems for brain tumour\nsegmentation on the BraTS dataset. The experimental results show that there is\na trade-off between model performance and privacy protection costs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:02:06 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Li", "Wenqi", ""], ["Milletar\u00ec", "Fausto", ""], ["Xu", "Daguang", ""], ["Rieke", "Nicola", ""], ["Hancox", "Jonny", ""], ["Zhu", "Wentao", ""], ["Baust", "Maximilian", ""], ["Cheng", "Yan", ""], ["Ourselin", "S\u00e9bastien", ""], ["Cardoso", "M. Jorge", ""], ["Feng", "Andrew", ""]]}, {"id": "1910.00969", "submitter": "Andreas Hinterreiter", "authors": "Andreas Hinterreiter, Peter Ruch, Holger Stitz, Martin Ennemoser,\n  J\\\"urgen Bernard, Hendrik Strobelt, Marc Streit", "title": "ConfusionFlow: A model-agnostic visualization for temporal analysis of\n  classifier confusion", "comments": "Changes compared to previous version: Reintroduced NN pruning use\n  case; restructured Evaluation section; several additional minor revisions.\n  Submitted as Minor Revision to IEEE TVCG on 2020-07-02", "journal-ref": null, "doi": "10.1109/TVCG.2020.3012063", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers are among the most widely used supervised machine learning\nalgorithms. Many classification models exist, and choosing the right one for a\ngiven task is difficult. During model selection and debugging, data scientists\nneed to assess classifiers' performances, evaluate their learning behavior over\ntime, and compare different models. Typically, this analysis is based on\nsingle-number performance measures such as accuracy. A more detailed evaluation\nof classifiers is possible by inspecting class errors. The confusion matrix is\nan established way for visualizing these class errors, but it was not designed\nwith temporal or comparative analysis in mind. More generally, established\nperformance analysis systems do not allow a combined temporal and comparative\nanalysis of class-level information. To address this issue, we propose\nConfusionFlow, an interactive, comparative visualization tool that combines the\nbenefits of class confusion matrices with the visualization of performance\ncharacteristics over time. ConfusionFlow is model-agnostic and can be used to\ncompare performances for different model types, model architectures, and/or\ntraining and test datasets. We demonstrate the usefulness of ConfusionFlow in a\ncase study on instance selection strategies in active learning. We further\nassess the scalability of ConfusionFlow and present a use case in the context\nof neural network pruning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:18:09 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 11:35:57 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 17:01:55 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hinterreiter", "Andreas", ""], ["Ruch", "Peter", ""], ["Stitz", "Holger", ""], ["Ennemoser", "Martin", ""], ["Bernard", "J\u00fcrgen", ""], ["Strobelt", "Hendrik", ""], ["Streit", "Marc", ""]]}, {"id": "1910.00983", "submitter": "Mark Van der Merwe", "authors": "Mark Van der Merwe, Qingkai Lu, Balakumar Sundaralingam, Martin Matak,\n  Tucker Hermans", "title": "Learning Continuous 3D Reconstructions for Geometrically Aware Grasping", "comments": "IEEE Conference on Robotics and Automation 2020 (ICRA 2020)\n  Camera-Ready. Includes updated experiments from initial submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enabled remarkable improvements in grasp synthesis for\npreviously unseen objects from partial object views. However, existing\napproaches lack the ability to explicitly reason about the full 3D geometry of\nthe object when selecting a grasp, relying on indirect geometric reasoning\nderived when learning grasp success networks. This abandons explicit geometric\nreasoning, such as avoiding undesired robot object collisions. We propose to\nutilize a novel, learned 3D reconstruction to enable geometric awareness in a\ngrasping system. We leverage the structure of the reconstruction network to\nlearn a grasp success classifier which serves as the objective function for a\ncontinuous grasp optimization. We additionally explicitly constrain the\noptimization to avoid undesired contact, directly using the reconstruction. We\nexamine the role of geometry in grasping both in the training of grasp metrics\nand through 96 robot grasping trials. Our results can be found on\nhttps://sites.google.com/view/reconstruction-grasp/.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:40:16 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 04:25:35 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Van der Merwe", "Mark", ""], ["Lu", "Qingkai", ""], ["Sundaralingam", "Balakumar", ""], ["Matak", "Martin", ""], ["Hermans", "Tucker", ""]]}, {"id": "1910.00993", "submitter": "Misha Kilmer", "authors": "Elizabeth Newman and Misha E. Kilmer", "title": "Non-negative Tensor Patch Dictionary Approaches for Image Compression\n  and Deblurring Applications", "comments": "25 pages, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work (Soltani, Kilmer, Hansen, BIT 2016), an algorithm for\nnon-negative tensor patch dictionary learning in the context of X-ray CT\nimaging and based on a tensor-tensor product called the $t$-product (Kilmer and\nMartin, 2011) was presented. Building on that work, in this paper, we use of\nnon-negative tensor patch-based dictionaries trained on other data, such as\nfacial image data, for the purposes of either compression or image deblurring.\nWe begin with an analysis in which we address issues such as suitability of the\ntensor-based approach relative to a matrix-based approach, dictionary size and\npatch size to balance computational efficiency and qualitative representations.\nNext, we develop an algorithm that is capable of recovering non-negative tensor\ncoefficients given a non-negative tensor dictionary. The algorithm is based on\na variant of the Modified Residual Norm Steepest Descent method. We show how to\naugment the algorithm to enforce sparsity in the tensor coefficients, and note\nthat the approach has broader applicability since it can be applied to the\nmatrix case as well. We illustrate the surprising result that dictionaries\ntrained on image data from one class can be successfully used to represent and\ncompress image data from different classes and across different resolutions.\nFinally, we address the use of non-negative tensor dictionaries in image\ndeblurring. We show that tensor treatment of the deblurring problem coupled\nwith non-negative tensor patch dictionaries can give superior restorations as\ncompared to standard treatment of the non-negativity constrained deblurring\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:24:22 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Newman", "Elizabeth", ""], ["Kilmer", "Misha E.", ""]]}, {"id": "1910.01007", "submitter": "John Mellor", "authors": "John F. J. Mellor, Eunbyung Park, Yaroslav Ganin, Igor Babuschkin,\n  Tejas Kulkarni, Dan Rosenbaum, Andy Ballard, Theophane Weber, Oriol Vinyals,\n  S. M. Ali Eslami", "title": "Unsupervised Doodling and Painting with Improved SPIRAL", "comments": "See https://learning-to-paint.github.io for an interactive version of\n  this paper, with videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate using reinforcement learning agents as generative models of\nimages (extending arXiv:1804.01118). A generative agent controls a simulated\npainting environment, and is trained with rewards provided by a discriminator\nnetwork simultaneously trained to assess the realism of the agent's samples,\neither unconditional or reconstructions. Compared to prior work, we make a\nnumber of improvements to the architectures of the agents and discriminators\nthat lead to intriguing and at times surprising results. We find that when\nsufficiently constrained, generative agents can learn to produce images with a\ndegree of visual abstraction, despite having only ever seen real photographs\n(no human brush strokes). And given enough time with the painting environment,\nthey can produce images with considerable realism. These results show that,\nunder the right circumstances, some aspects of human drawing can emerge from\nsimulated embodiment, without the need for external supervision, imitation or\nsocial cues. Finally, we note the framework's potential for use in creative\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 15:12:06 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Mellor", "John F. J.", ""], ["Park", "Eunbyung", ""], ["Ganin", "Yaroslav", ""], ["Babuschkin", "Igor", ""], ["Kulkarni", "Tejas", ""], ["Rosenbaum", "Dan", ""], ["Ballard", "Andy", ""], ["Weber", "Theophane", ""], ["Vinyals", "Oriol", ""], ["Eslami", "S. M. Ali", ""]]}, {"id": "1910.01050", "submitter": "Domonkos Varga", "authors": "Domonkos Varga", "title": "Empirical evaluation of full-reference image quality metrics on MDID\n  database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, our goal is to give a comprehensive evaluation of 32\nstate-of-the-art FR-IQA metrics using the recently published MDID. This\ndatabase contains distorted images derived from a set of reference, pristine\nimages using random types and levels of distortions. Specifically, Gaussian\nnoise, Gaussian blur, contrast change, JPEG noise, and JPEG2000 noise were\nconsidered.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:13:31 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 18:08:01 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Varga", "Domonkos", ""]]}, {"id": "1910.01089", "submitter": "Juan Luis Gonzalez Bello", "authors": "Juan Luis Gonzalez Bello and Munchurl Kim", "title": "Deep 3D Pan via adaptive \"t-shaped\" convolutions with global and local\n  adaptive dilations", "comments": "Check our video at https://www.youtube.com/watch?v=o0b-e282Rt4", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have shown promising results in many\nlow-level vision tasks. However, solving the single-image-based view synthesis\nis still an open problem. In particular, the generation of new images at\nparallel camera views given a single input image is of great interest, as it\nenables 3D visualization of the 2D input scenery. We propose a novel network\narchitecture to perform stereoscopic view synthesis at arbitrary camera\npositions along the X-axis, or Deep 3D Pan, with \"t-shaped\" adaptive kernels\nequipped with globally and locally adaptive dilations. Our proposed network\narchitecture, the monster-net, is devised with a novel \"t-shaped\" adaptive\nkernel with globally and locally adaptive dilation, which can efficiently\nincorporate global camera shift into and handle local 3D geometries of the\ntarget image's pixels for the synthesis of naturally looking 3D panned views\nwhen a 2-D input image is given. Extensive experiments were performed on the\nKITTI, CityScapes and our VICLAB_STEREO indoors dataset to prove the efficacy\nof our method. Our monster-net significantly outperforms the state-of-the-art\nmethod, SOTA, by a large margin in all metrics of RMSE, PSNR, and SSIM. Our\nproposed monster-net is capable of reconstructing more reliable image\nstructures in synthesized images with coherent geometry. Moreover, the\ndisparity information that can be extracted from the \"t-shaped\" kernel is much\nmore reliable than that of the SOTA for the unsupervised monocular depth\nestimation task, confirming the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 17:09:58 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 15:29:41 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 01:52:19 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Bello", "Juan Luis Gonzalez", ""], ["Kim", "Munchurl", ""]]}, {"id": "1910.01091", "submitter": "Mohammed Abuhamad", "authors": "Changhun Jung, Mohammed Abuhamad, Jumabek Alikhanov, Aziz Mohaisen,\n  Kyungja Han, and DaeHun Nyang", "title": "W-Net: A CNN-based Architecture for White Blood Cells Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided methods for analyzing white blood cells (WBC) have become\nwidely popular due to the complexity of the manual process. Recent works have\nshown highly accurate segmentation and detection of white blood cells from\nmicroscopic blood images. However, the classification of the observed cells is\nstill a challenge and highly demanded as the distribution of the five types\nreflects on the condition of the immune system. This work proposes W-Net, a\nCNN-based method for WBC classification. We evaluate W-Net on a real-world\nlarge-scale dataset, obtained from The Catholic University of Korea, that\nincludes 6,562 real images of the five WBC types. W-Net achieves an average\naccuracy of 97%.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 17:13:42 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Jung", "Changhun", ""], ["Abuhamad", "Mohammed", ""], ["Alikhanov", "Jumabek", ""], ["Mohaisen", "Aziz", ""], ["Han", "Kyungja", ""], ["Nyang", "DaeHun", ""]]}, {"id": "1910.01112", "submitter": "Utkarsh Ojha", "authors": "Utkarsh Ojha, Krishna Kumar Singh, Cho-Jui Hsieh, Yong Jae Lee", "title": "Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in\n  Class-Imbalanced Data", "comments": "Camera ready version for NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel unsupervised generative model that learns to disentangle\nobject identity from other low-level aspects in class-imbalanced data. We first\ninvestigate the issues surrounding the assumptions about uniformity made by\nInfoGAN, and demonstrate its ineffectiveness to properly disentangle object\nidentity in imbalanced data. Our key idea is to make the discovery of the\ndiscrete latent factor of variation invariant to identity-preserving\ntransformations in real images, and use that as a signal to learn the\nappropriate latent distribution representing object identity. Experiments on\nboth artificial (MNIST, 3D cars, 3D chairs, ShapeNet) and real-world\n(YouTube-Faces) imbalanced datasets demonstrate the effectiveness of our method\nin disentangling object identity as a latent factor of variation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:50:44 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 10:48:53 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ojha", "Utkarsh", ""], ["Singh", "Krishna Kumar", ""], ["Hsieh", "Cho-Jui", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1910.01122", "submitter": "Shinya Sumikura", "authors": "Shinya Sumikura, Mikiya Shibuya, Ken Sakurada", "title": "OpenVSLAM: A Versatile Visual SLAM Framework", "comments": "Accepted to ACM Multimedia 2019 Open Source Software Competition.\n  Video: https://www.youtube.com/watch?v=Ro_s3Lbx5ms", "journal-ref": null, "doi": "10.1145/3343031.3350539", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce OpenVSLAM, a visual SLAM framework with high\nusability and extensibility. Visual SLAM systems are essential for AR devices,\nautonomous control of robots and drones, etc. However, conventional open-source\nvisual SLAM frameworks are not appropriately designed as libraries called from\nthird-party programs. To overcome this situation, we have developed a novel\nvisual SLAM framework. This software is designed to be easily used and\nextended. It incorporates several useful features and functions for research\nand development. OpenVSLAM is released at\nhttps://github.com/xdspacelab/openvslam under the 2-clause BSD license.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 18:00:01 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 06:43:19 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Sumikura", "Shinya", ""], ["Shibuya", "Mikiya", ""], ["Sakurada", "Ken", ""]]}, {"id": "1910.01182", "submitter": "Salimeh Yasaei Sekeh", "authors": "Salimeh Yasaei Sekeh, Madan Ravi Ganesh, Shurjo Banerjee, Jason J.\n  Corso, and Alfred O. Hero", "title": "A Geometric Approach to Online Streaming Feature Selection", "comments": "10 page, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Streaming Feature Selection (OSFS) is a sequential learning problem\nwhere individual features across all samples are made available to algorithms\nin a streaming fashion. In this work, firstly, we assert that OSFS's main\nassumption of having data from all the samples available at runtime is\nunrealistic and introduce a new setting where features and samples are streamed\nconcurrently called OSFS with Streaming Samples (OSFS-SS). Secondly, the\nprimary OSFS method, SAOLA utilizes an unbounded mutual information measure and\nrequires multiple comparison steps between the stored and incoming feature sets\nto evaluate a feature's importance. We introduce Geometric Online Adaption, an\nalgorithm that requires relatively less feature comparison steps and uses a\nbounded conditional geometric dependency measure. Our algorithm outperforms\nseveral OSFS baselines including SAOLA on a variety of datasets. We also extend\nSAOLA to work in the OSFS-SS setting and show that GOA continues to achieve the\nbest results. Thirdly, the current paradigm of the OSFS algorithm comparison is\nflawed. Algorithms are measured by comparing the number of features used and\nthe accuracy obtained by the learner, two properties that are fundamentally at\nodds with one another. Without fixing a limit on either of these properties,\nthe qualities of features obtained by different algorithms are incomparable. We\ntry to rectify this inconsistency by fixing the maximum number of features\navailable to the learner and comparing algorithms in terms of their accuracy.\nAdditionally, we characterize the behaviour of SAOLA and GOA on feature sets\nderived from popular deep convolutional featurizers.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 19:36:46 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 04:49:21 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Sekeh", "Salimeh Yasaei", ""], ["Ganesh", "Madan Ravi", ""], ["Banerjee", "Shurjo", ""], ["Corso", "Jason J.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1910.01197", "submitter": "Bin Zhu", "authors": "Bin Zhu, Xin Guo, Kenneth Barner, Charles Boncelet", "title": "Automatic Group Cohesiveness Detection With Multi-modal Features", "comments": null, "journal-ref": null, "doi": "10.1145/3340555.3355716", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group cohesiveness is a compelling and often studied composition in group\ndynamics and group performance. The enormous number of web images of groups of\npeople can be used to develop an effective method to detect group cohesiveness.\nThis paper introduces an automatic group cohesiveness prediction method for the\n7th Emotion Recognition in the Wild (EmotiW 2019) Grand Challenge in the\ncategory of Group-based Cohesion Prediction. The task is to predict the\ncohesive level for a group of people in images. To tackle this problem, a\nhybrid network including regression models which are separately trained on face\nfeatures, skeleton features, and scene features is proposed. Predicted\nregression values, corresponding to each feature, are fused for the final\ncohesive intensity. Experimental results demonstrate that the proposed hybrid\nnetwork is effective and makes promising improvements. A mean squared error\n(MSE) of 0.444 is achieved on the testing sets which outperforms the baseline\nMSE of 0.5.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:04:42 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Zhu", "Bin", ""], ["Guo", "Xin", ""], ["Barner", "Kenneth", ""], ["Boncelet", "Charles", ""]]}, {"id": "1910.01198", "submitter": "Brigit Schroeder", "authors": "Brigit Schroeder, Hanlin Tang, Alexandre Alahi", "title": "Using Image Priors to Improve Scene Understanding", "comments": "Accepted to Women in Computer Vision (WiCV) Workshop at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation algorithms that can robustly segment objects across\nmultiple camera viewpoints are crucial for assuring navigation and safety in\nemerging applications such as autonomous driving. Existing algorithms treat\neach image in isolation, but autonomous vehicles often revisit the same\nlocations or maintain information from the immediate past. We propose a simple\nyet effective method for leveraging these image priors to improve semantic\nsegmentation of images from sequential driving datasets. We examine several\nmethods to fuse these temporal scene priors, and introduce a prior fusion\nnetwork that is able to learn how to transfer this information. The prior\nfusion model improves the accuracy over the non-prior baseline from 69.1% to\n73.3% for dynamic classes, and from 88.2% to 89.1% for static classes. Compared\nto models such as FCN-8, our prior method achieves the same accuracy with 5\ntimes fewer parameters. We used a simple encoder decoder backbone, but this\ngeneral prior fusion method could be applied to more complex semantic\nsegmentation backbones. We also discuss how structured representations of\nscenes in the form of a scene graph could be leveraged as priors to further\nimprove scene understanding.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:08:26 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Schroeder", "Brigit", ""], ["Tang", "Hanlin", ""], ["Alahi", "Alexandre", ""]]}, {"id": "1910.01210", "submitter": "Mihir Prabhudesai", "authors": "Mihir Prabhudesai, Hsiao-Yu Fish Tung, Syed Ashar Javed, Maximilian\n  Sieb, Adam W. Harley, Katerina Fragkiadaki", "title": "Embodied Language Grounding with 3D Visual Feature Representations", "comments": null, "journal-ref": "Conference on Computer Vision and Pattern Recognition. 2020, pp.\n  2220-2229", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose associating language utterances to 3D visual abstractions of the\nscene they describe. The 3D visual abstractions are encoded as 3-dimensional\nvisual feature maps. We infer these 3D visual scene feature maps from RGB\nimages of the scene via view prediction: when the generated 3D scene feature\nmap is neurally projected from a camera viewpoint, it should match the\ncorresponding RGB image. We present generative models that condition on the\ndependency tree of an utterance and generate a corresponding visual 3D feature\nmap as well as reason about its plausibility, and detector models that\ncondition on both the dependency tree of an utterance and a related image and\nlocalize the object referents in the 3D feature map inferred from the image.\nOur model outperforms models of language and vision that associate language\nwith 2D CNN activations or 2D images by a large margin in a variety of tasks,\nsuch as, classifying plausibility of utterances, detecting referential\nexpressions, and supplying rewards for trajectory optimization of object\nplacement policies from language instructions. We perform numerous ablations\nand show the improved performance of our detectors is due to its better\ngeneralization across camera viewpoints and lack of object interferences in the\ninferred 3D feature space, and the improved performance of our generators is\ndue to their ability to spatially reason about objects and their configurations\nin 3D when mapping from language to scenes.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:37:27 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 16:04:27 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 21:31:18 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Prabhudesai", "Mihir", ""], ["Tung", "Hsiao-Yu Fish", ""], ["Javed", "Syed Ashar", ""], ["Sieb", "Maximilian", ""], ["Harley", "Adam W.", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1910.01219", "submitter": "Shresth Verma", "authors": "Raj Kuwar Gupta, Shresth Verma, KV Arya, Soumya Agarwal, Prince Gupta", "title": "IIITM Face: A Database for Facial Attribute Detection in Constrained and\n  Simulated Unconstrained Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenges of face attribute detection specifically\nin the Indian context. While there are numerous face datasets in unconstrained\nenvironments, none of them captures emotions in different face orientations.\nMoreover, there is an under-representation of people of Indian ethnicity in\nthese datasets since they have been scraped from popular search engines. As a\nresult, the performance of state-of-the-art techniques can't be evaluated on\nIndian faces. In this work, we introduce a new dataset, IIITM Face, for the\nscientific community to address these challenges. Our dataset includes 107\nparticipants who exhibit 6 emotions in 3 different face orientations. Each of\nthese images is further labelled on attributes like gender, presence of\nmoustache, beard or eyeglasses, clothes worn by the subjects and the density of\ntheir hair. Moreover, the images are captured in high resolution with specific\nbackground colors which can be easily replaced by cluttered backgrounds to\nsimulate `in the Wild' behaviour. We demonstrate the same by constructing IIITM\nFace-SUE. Both IIITM Face and IIITM Face-SUE have been benchmarked across key\nmulti-label metrics for the research community to compare their results.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 21:03:44 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Gupta", "Raj Kuwar", ""], ["Verma", "Shresth", ""], ["Arya", "KV", ""], ["Agarwal", "Soumya", ""], ["Gupta", "Prince", ""]]}, {"id": "1910.01221", "submitter": "Bingyang Wen", "authors": "Bingyang Wen and Sergul Aydore", "title": "ROMark: A Robust Watermarking System Using Adversarial Training", "comments": "5 pages, 1 figure, Machine Learning with Guarantees workshop at\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability and easy access to digital communication increase the risk\nof copyrighted material piracy. In order to detect illegal use or distribution\nof data, digital watermarking has been proposed as a suitable tool. It protects\nthe copyright of digital content by embedding imperceptible information into\nthe data in the presence of an adversary. The goal of the adversary is to\nremove the copyrighted content of the data. Therefore, an efficient\nwatermarking framework must be robust to multiple image-processing operations\nknown as attacks that can alter embedded copyright information. Another line of\nresearch \\textit{adversarial machine learning} also tackles with similar\nproblems to guarantee robustness to imperceptible perturbations of the input.\nIn this work, we propose to apply robust optimization from adversarial machine\nlearning to improve the robustness of a CNN-based watermarking framework. Our\nexperimental results on the COCO dataset show that the robustness of a\nwatermarking framework can be improved by utilizing robust optimization in\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 21:05:05 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Wen", "Bingyang", ""], ["Aydore", "Sergul", ""]]}, {"id": "1910.01225", "submitter": "Alexey Sidnev", "authors": "Alexey Sidnev, Alexey Trushkov, Maxim Kazakov, Ivan Korolev, Vladislav\n  Sorokin", "title": "DeepMark: One-Shot Clothing Detection", "comments": "Published in ICCV 2019 Workshop", "journal-ref": null, "doi": "10.1109/ICCVW.2019.00399", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-shot approach, DeepMark, for fast clothing detection as a\nmodification of a multi-target network, CenterNet, is proposed in the paper.\nThe state-of-the-art accuracy of 0.723 mAP for bounding box detection task and\n0.532 mAP for landmark detection task on the DeepFashion2 Challenge dataset\nwere achieved. The proposed architecture can be used effectively on the\nlow-power devices.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 21:18:17 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Sidnev", "Alexey", ""], ["Trushkov", "Alexey", ""], ["Kazakov", "Maxim", ""], ["Korolev", "Ivan", ""], ["Sorokin", "Vladislav", ""]]}, {"id": "1910.01236", "submitter": "Holger Roth", "authors": "Holger Roth, Ling Zhang, Dong Yang, Fausto Milletari, Ziyue Xu,\n  Xiaosong Wang, Daguang Xu", "title": "Weakly supervised segmentation from extreme points", "comments": "Accepted at the MICCAI Workshop for Large-scale Annotation of\n  Biomedical data and Expert Label Synthesis, Shenzen, China, 2019", "journal-ref": "LABELS 2019, HAL-MICCAI 2019, CuRIOUS 2019. Lecture Notes in\n  Computer Science, vol 11851. Springer, Cham", "doi": "10.1007/978-3-030-33642-4_5", "report-no": "LNCS, volume 11851", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotation of medical images has been a major bottleneck for the development\nof accurate and robust machine learning models. Annotation is costly and\ntime-consuming and typically requires expert knowledge, especially in the\nmedical domain. Here, we propose to use minimal user interaction in the form of\nextreme point clicks in order to train a segmentation model that can, in turn,\nbe used to speed up the annotation of medical images. We use extreme points in\neach dimension of a 3D medical image to constrain an initial segmentation based\non the random walker algorithm. This segmentation is then used as a weak\nsupervisory signal to train a fully convolutional network that can segment the\norgan of interest based on the provided user clicks. We show that the network's\npredictions can be refined through several iterations of training and\nprediction using the same weakly annotated data. Ultimately, our method has the\npotential to speed up the generation process of new training datasets for the\ndevelopment of new machine learning and deep learning-based models for, but not\nexclusively, medical image analysis.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 21:54:00 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Roth", "Holger", ""], ["Zhang", "Ling", ""], ["Yang", "Dong", ""], ["Milletari", "Fausto", ""], ["Xu", "Ziyue", ""], ["Wang", "Xiaosong", ""], ["Xu", "Daguang", ""]]}, {"id": "1910.01241", "submitter": "Akin Caliskan", "authors": "Akin Caliskan, Armin Mustafa, Evren Imre, Adrian Hilton", "title": "Learning Dense Wide Baseline Stereo Matching for People", "comments": "To appear in 3D Reconstruction in the Wild Workshop, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for stereo work on narrow baseline image pairs giving\nlimited performance between wide baseline views. This paper proposes a\nframework to learn and estimate dense stereo for people from wide baseline\nimage pairs. A synthetic people stereo patch dataset (S2P2) is introduced to\nlearn wide baseline dense stereo matching for people. The proposed framework\nnot only learns human specific features from synthetic data but also exploits\npooling layer and data augmentation to adapt to real data. The network learns\nfrom the human specific stereo patches from the proposed dataset for\nwide-baseline stereo estimation. In addition to patch match learning, a stereo\nconstraint is introduced in the framework to solve wide baseline stereo\nreconstruction of humans. Quantitative and qualitative performance evaluation\nagainst state-of-the-art methods of proposed method demonstrates improved wide\nbaseline stereo reconstruction on challenging datasets. We show that it is\npossible to learn stereo matching from synthetic people dataset and improve\nperformance on real datasets for stereo reconstruction of people from narrow\nand wide baseline stereo data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 22:24:11 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Caliskan", "Akin", ""], ["Mustafa", "Armin", ""], ["Imre", "Evren", ""], ["Hilton", "Adrian", ""]]}, {"id": "1910.01242", "submitter": "Holger Roth", "authors": "Holger Roth and Wentao Zhu and Dong Yang and Ziyue Xu and Daguang Xu", "title": "Cardiac Segmentation of LGE MRI with Noisy Labels", "comments": "Accepted at the MICCAI Workshop Statistical Atlases and Computational\n  Modeling of the Heart (STACOM), MS-CMRSeg 2019: Multi-sequence Cardiac MR\n  Segmentation Challenge, Shenzen, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we attempt the segmentation of cardiac structures in late\ngadolinium-enhanced (LGE) magnetic resonance images (MRI) using only minimal\nsupervision in a two-step approach. In the first step, we register a small set\nof five LGE cardiac magnetic resonance (CMR) images with ground truth labels to\na set of 40 target LGE CMR images without annotation. Each manually annotated\nground truth provides labels of the myocardium and the left ventricle (LV) and\nright ventricle (RV) cavities, which are used as atlases. After multi-atlas\nlabel fusion by majority voting, we possess noisy labels for each of the\ntargeted LGE images. A second set of manual labels exists for 30 patients of\nthe target LGE CMR images, but are annotated on different MRI sequences (bSSFP\nand T2-weighted). Again, we use multi-atlas label fusion with a consistency\nconstraint to further refine our noisy labels if additional annotations in\nother modalities are available for a given patient. In the second step, we\ntrain a deep convolutional network for semantic segmentation on the target data\nwhile using data augmentation techniques to avoid over-fitting to the noisy\nlabels. After inference and simple post-processing, we achieve our final\nsegmentation for the targeted LGE CMR images, resulting in an average Dice of\n0.890, 0.780, and 0.844 for LV cavity, LV myocardium, and RV cavity,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 22:26:20 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Roth", "Holger", ""], ["Zhu", "Wentao", ""], ["Yang", "Dong", ""], ["Xu", "Ziyue", ""], ["Xu", "Daguang", ""]]}, {"id": "1910.01254", "submitter": "Masih Aminbeidokhti", "authors": "Masih Aminbeidokhti, Marco Pedersoli, Patrick Cardinal, Eric Granger", "title": "Emotion Recognition with Spatial Attention and Temporal Softmax Pooling", "comments": "9 pages; 2 figures; 2 tables; Best paper award at ICIAR 2019", "journal-ref": null, "doi": "10.1007/978-3-030-27202-9_29", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based emotion recognition is a challenging task because it requires to\ndistinguish the small deformations of the human face that represent emotions,\nwhile being invariant to stronger visual differences due to different\nidentities. State-of-the-art methods normally use complex deep learning models\nsuch as recurrent neural networks (RNNs, LSTMs, GRUs), convolutional neural\nnetworks (CNNs, C3D, residual networks) and their combination. In this paper,\nwe propose a simpler approach that combines a CNN pre-trained on a public\ndataset of facial images with (1) a spatial attention mechanism, to localize\nthe most important regions of the face for a given emotion, and (2) temporal\nsoftmax pooling, to select the most important frames of the given video.\nResults on the challenging EmotiW dataset show that this approach can achieve\nhigher accuracy than more complex approaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 23:53:10 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 02:52:19 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Aminbeidokhti", "Masih", ""], ["Pedersoli", "Marco", ""], ["Cardinal", "Patrick", ""], ["Granger", "Eric", ""]]}, {"id": "1910.01255", "submitter": "Yiping Lu", "authors": "Bin Dong, Jikai Hou, Yiping Lu, Zhihua Zhang", "title": "Distillation $\\approx$ Early Stopping? Harvesting Dark Knowledge\n  Utilizing Anisotropic Information Retrieval For Overparameterized Neural\n  Network", "comments": "Accepted by NeurIPS 2019 Workshop on Machine Learning with\n  Guarantees. Submitted to other places", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distillation is a method to transfer knowledge from one model to another and\noften achieves higher accuracy with the same capacity. In this paper, we aim to\nprovide a theoretical understanding on what mainly helps with the distillation.\nOur answer is \"early stopping\". Assuming that the teacher network is\noverparameterized, we argue that the teacher network is essentially harvesting\ndark knowledge from the data via early stopping. This can be justified by a new\nconcept, {Anisotropic Information Retrieval (AIR)}, which means that the neural\nnetwork tends to fit the informative information first and the non-informative\ninformation (including noise) later. Motivated by the recent development on\ntheoretically analyzing overparameterized neural networks, we can characterize\nAIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new\nunderstanding of distillation. With that, we further utilize distillation to\nrefine noisy labels. We propose a self-distillation algorithm to sequentially\ndistill knowledge from the network in the previous training epoch to avoid\nmemorizing the wrong labels. We also demonstrate, both theoretically and\nempirically, that self-distillation can benefit from more than just early\nstopping. Theoretically, we prove convergence of the proposed algorithm to the\nground truth labels for randomly initialized overparameterized neural networks\nin terms of $\\ell_2$ distance, while the previous result was on convergence in\n$0$-$1$ loss. The theoretical result ensures the learned neural network enjoy a\nmargin on the training data which leads to better generalization. Empirically,\nwe achieve better testing accuracy and entirely avoid early stopping which\nmakes the algorithm more user-friendly.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 23:53:39 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Dong", "Bin", ""], ["Hou", "Jikai", ""], ["Lu", "Yiping", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1910.01256", "submitter": "Daniel Ruiz", "authors": "Daniel V. Ruiz, Bruno A. Krinski, Eduardo Todt", "title": "ANDA: A Novel Data Augmentation Technique Applied to Salient Object\n  Detection", "comments": "Accepted for presentation at the International Conference on Advanced\n  Robotics (ICAR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel data augmentation technique (ANDA) applied\nto the Salient Object Detection (SOD) context. Standard data augmentation\ntechniques proposed in the literature, such as image cropping, rotation,\nflipping, and resizing, only generate variations of the existing examples,\nproviding a limited generalization. Our method has the novelty of creating new\nimages, by combining an object with a new background while retaining part of\nits salience in this new context; To do so, the ANDA technique relies on the\nlinear combination between labeled salient objects and new backgrounds,\ngenerated by removing the original salient object in a process known as image\ninpainting. Our proposed technique allows for more precise control of the\nobject's position and size while preserving background information. Aiming to\nevaluate our proposed method, we trained multiple deep neural networks and\ncompared the effect that our technique has in each one. We also compared our\nmethod with other data augmentation techniques. Our findings show that\ndepending on the network improvement can be up to 14.1% in the F-measure and\ndecay of up to 2.6% in the Mean Absolute Error.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 00:00:38 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ruiz", "Daniel V.", ""], ["Krinski", "Bruno A.", ""], ["Todt", "Eduardo", ""]]}, {"id": "1910.01268", "submitter": "Andr\\'eanne Lemay", "authors": "Andr\\'eanne Lemay", "title": "Kidney Recognition in CT Using YOLOv3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organ localization can be challenging considering the heterogeneity of\nmedical images and the biological diversity from one individual to another. The\ncontribution of this paper is to overview the performance of the object\ndetection model, YOLOv3, on kidney localization in 2D and in 3D from CT scans.\nThe model obtained a 0.851 Dice score in 2D and 0.742 in 3D. The SSD, a similar\nstate-of-the-art object detection model, showed similar scores on the test set.\nYOLOv3 and SSD demonstrated the ability to detect kidneys on a wide variety of\nCT scans including patients suffering from different renal conditions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 01:26:18 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Lemay", "Andr\u00e9anne", ""]]}, {"id": "1910.01269", "submitter": "Gopal Sharma", "authors": "Gopal Sharma, Evangelos Kalogerakis and Subhransu Maji", "title": "Learning Point Embeddings from Shape Repositories for Few-Shot\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User generated 3D shapes in online repositories contain rich information\nabout surfaces, primitives, and their geometric relations, often arranged in a\nhierarchy. We present a framework for learning representations of 3D shapes\nthat reflect the information present in this meta data and show that it leads\nto improved generalization for semantic segmentation tasks. Our approach is a\npoint embedding network that generates a vectorial representation of the 3D\npoints such that it reflects the grouping hierarchy and tag data. The main\nchallenge is that the data is noisy and highly variable. To this end, we\npresent a tree-aware metric-learning approach and demonstrate that such learned\nembeddings offer excellent transfer to semantic segmentation tasks, especially\nwhen training data is limited. Our approach reduces the relative error by\n$10.2\\%$ with $8$ training examples, by $11.72\\%$ with $120$ training examples\non the ShapeNet semantic segmentation benchmark, in comparison to the network\ntrained from scratch. By utilizing tag data the relative error is reduced by\n$12.8\\%$ with $8$ training examples, in comparison to the network trained from\nscratch. These improvements come at no additional labeling cost as the meta\ndata is freely available.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 01:26:31 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Sharma", "Gopal", ""], ["Kalogerakis", "Evangelos", ""], ["Maji", "Subhransu", ""]]}, {"id": "1910.01271", "submitter": "Alexander Wong", "authors": "Alexander Wong, Mahmoud Famuori, Mohammad Javad Shafiee, Francis Li,\n  Brendan Chwyl, and Jonathan Chung", "title": "YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural\n  Network for Object Detection", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection remains an active area of research in the field of computer\nvision, and considerable advances and successes has been achieved in this area\nthrough the design of deep convolutional neural networks for tackling object\ndetection. Despite these successes, one of the biggest challenges to widespread\ndeployment of such object detection networks on edge and mobile scenarios is\nthe high computational and memory requirements. As such, there has been growing\nresearch interest in the design of efficient deep neural network architectures\ncatered for edge and mobile usage. In this study, we introduce YOLO Nano, a\nhighly compact deep convolutional neural network for the task of object\ndetection. A human-machine collaborative design strategy is leveraged to create\nYOLO Nano, where principled network design prototyping, based on design\nprinciples from the YOLO family of single-shot object detection network\narchitectures, is coupled with machine-driven design exploration to create a\ncompact network with highly customized module-level macroarchitecture and\nmicroarchitecture designs tailored for the task of embedded object detection.\nThe proposed YOLO Nano possesses a model size of ~4.0MB (>15.1x and >8.3x\nsmaller than Tiny YOLOv2 and Tiny YOLOv3, respectively) and requires 4.57B\noperations for inference (>34% and ~17% lower than Tiny YOLOv2 and Tiny YOLOv3,\nrespectively) while still achieving an mAP of ~69.1% on the VOC 2007 dataset\n(~12% and ~10.7% higher than Tiny YOLOv2 and Tiny YOLOv3, respectively).\nExperiments on inference speed and power efficiency on a Jetson AGX Xavier\nembedded module at different power budgets further demonstrate the efficacy of\nYOLO Nano for embedded scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 01:29:26 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Wong", "Alexander", ""], ["Famuori", "Mahmoud", ""], ["Shafiee", "Mohammad Javad", ""], ["Li", "Francis", ""], ["Chwyl", "Brendan", ""], ["Chung", "Jonathan", ""]]}, {"id": "1910.01275", "submitter": "Sicong Tang", "authors": "Sicong Tang, Feitong Tan, Kelvin Cheng, Zhaoyang Li, Siyu Zhu, Ping\n  Tan", "title": "A Neural Network for Detailed Human Depth Estimation from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a neural network to estimate a detailed depth map of the\nforeground human in a single RGB image. The result captures geometry details\nsuch as cloth wrinkles, which are important in visualization applications. To\nachieve this goal, we separate the depth map into a smooth base shape and a\nresidual detail shape and design a network with two branches to regress them\nrespectively. We design a training strategy to ensure both base and detail\nshapes can be faithfully learned by the corresponding network branches.\nFurthermore, we introduce a novel network layer to fuse a rough depth map and\nsurface normals to further improve the final result. Quantitative comparison\nwith fused `ground truth' captured by real depth cameras and qualitative\nexamples on unconstrained Internet images demonstrate the strength of the\nproposed method. The code is available at\nhttps://github.com/sfu-gruvi-3dv/deep_human.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 01:54:22 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 08:35:33 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Tang", "Sicong", ""], ["Tan", "Feitong", ""], ["Cheng", "Kelvin", ""], ["Li", "Zhaoyang", ""], ["Zhu", "Siyu", ""], ["Tan", "Ping", ""]]}, {"id": "1910.01279", "submitter": "Haofan Wang", "authors": "Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui\n  Ding, Piotr Mardziel, Xia Hu", "title": "Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural\n  Networks", "comments": "Accepted to CVPR 2020: Workshop on Fair, Data Efficient and Trusted\n  Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, increasing attention has been drawn to the internal mechanisms of\nconvolutional neural networks, and the reason why the network makes specific\ndecisions. In this paper, we develop a novel post-hoc visual explanation method\ncalled Score-CAM based on class activation mapping. Unlike previous class\nactivation mapping based approaches, Score-CAM gets rid of the dependence on\ngradients by obtaining the weight of each activation map through its forward\npassing score on target class, the final result is obtained by a linear\ncombination of weights and activation maps. We demonstrate that Score-CAM\nachieves better visual performance and fairness for interpreting the decision\nmaking process. Our approach outperforms previous methods on both recognition\nand localization tasks, it also passes the sanity check. We also indicate its\napplication as debugging tools. Official code has been released.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 02:05:15 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 10:41:36 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Wang", "Haofan", ""], ["Wang", "Zifan", ""], ["Du", "Mengnan", ""], ["Yang", "Fan", ""], ["Zhang", "Zijian", ""], ["Ding", "Sirui", ""], ["Mardziel", "Piotr", ""], ["Hu", "Xia", ""]]}, {"id": "1910.01286", "submitter": "Jingwei Ji", "authors": "Jingwei Ji, Kaidi Cao, Juan Carlos Niebles", "title": "Learning Temporal Action Proposals With Fewer Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action proposals are a common module in action detection pipelines\ntoday. Most current methods for training action proposal modules rely on fully\nsupervised approaches that require large amounts of annotated temporal action\nintervals in long video sequences. The large cost and effort in annotation that\nthis entails motivate us to study the problem of training proposal modules with\nless supervision. In this work, we propose a semi-supervised learning algorithm\nspecifically designed for training temporal action proposal networks. When only\na small number of labels are available, our semi-supervised method generates\nsignificantly better proposals than the fully-supervised counterpart and other\nstrong semi-supervised baselines. We validate our method on two challenging\naction detection video datasets, ActivityNet v1.3 and THUMOS14. We show that\nour semi-supervised approach consistently matches or outperforms the fully\nsupervised state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 02:54:04 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ji", "Jingwei", ""], ["Cao", "Kaidi", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1910.01348", "submitter": "Jang Hyun Cho", "authors": "Jang Hyun Cho and Bharath Hariharan", "title": "On the Efficacy of Knowledge Distillation", "comments": "13 pages, including Appendix", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a thorough evaluation of the efficacy of knowledge\ndistillation and its dependence on student and teacher architectures. Starting\nwith the observation that more accurate teachers often don't make good\nteachers, we attempt to tease apart the factors that affect knowledge\ndistillation performance. We find crucially that larger models do not often\nmake better teachers. We show that this is a consequence of mismatched\ncapacity, and that small students are unable to mimic large teachers. We find\ntypical ways of circumventing this (such as performing a sequence of knowledge\ndistillation steps) to be ineffective. Finally, we show that this effect can be\nmitigated by stopping the teacher's training early. Our results generalize\nacross datasets and models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 08:14:13 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Cho", "Jang Hyun", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1910.01370", "submitter": "Alessandro Masullo", "authors": "Alessandro Masullo, Tilo Burghardt, Toby Perrett, Dima Damen, Majid\n  Mirmehdi", "title": "Sit-to-Stand Analysis in the Wild using Silhouettes for Longitudinal\n  Health Monitoring", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-27272-2_15", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first fully automated Sit-to-Stand or Stand-to-Sit (StS)\nanalysis framework for long-term monitoring of patients in free-living\nenvironments using video silhouettes. Our method adopts a coarse-to-fine time\nlocalisation approach, where a deep learning classifier identifies possible StS\nsequences from silhouettes, and a smart peak detection stage provides fine\nlocalisation based on 3D bounding boxes. We tested our method on data from real\nhomes of participants and monitored patients undergoing total hip or knee\nreplacement. Our results show 94.4% overall accuracy in the coarse localisation\nand an error of 0.026 m/s in the speed of ascent measurement, highlighting\nimportant trends in the recuperation of patients who underwent surgery.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 09:19:12 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Masullo", "Alessandro", ""], ["Burghardt", "Tilo", ""], ["Perrett", "Toby", ""], ["Damen", "Dima", ""], ["Mirmehdi", "Majid", ""]]}, {"id": "1910.01389", "submitter": "K\\'evin Atighehchi", "authors": "Kevin Atighehchi, Loubna Ghammam, Koray Karabina, and Patrick Lacharme", "title": "A Cryptanalysis of Two Cancelable Biometric Schemes based on\n  Index-of-Max Hashing", "comments": "Some revisions and addition of acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancelable biometric schemes generate secure biometric templates by combining\nuser specific tokens and biometric data. The main objective is to create\nirreversible, unlinkable, and revocable templates, with high accuracy in\nmatching. In this paper, we cryptanalyze two recent cancelable biometric\nschemes based on a particular locality sensitive hashing function, index-of-max\n(IoM): Gaussian Random Projection-IoM (GRP-IoM) and Uniformly Random\nPermutation-IoM (URP-IoM). As originally proposed, these schemes were claimed\nto be resistant against reversibility, authentication, and linkability attacks\nunder the stolen token scenario. We propose several attacks against GRP-IoM and\nURP-IoM, and argue that both schemes are severely vulnerable against\nauthentication and linkability attacks. We also propose better, but not yet\npractical, reversibility attacks against GRP-IoM. The correctness and practical\nimpact of our attacks are verified over the same dataset provided by the\nauthors of these two schemes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:28:02 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 21:27:38 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 23:08:19 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Atighehchi", "Kevin", ""], ["Ghammam", "Loubna", ""], ["Karabina", "Koray", ""], ["Lacharme", "Patrick", ""]]}, {"id": "1910.01403", "submitter": "Kimia Dinashi", "authors": "Kimia Dinashi, Ramin Toosi, Mohammad Ali Akhaee", "title": "Face Manifold: Manifold Learning for Synthetic Face Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face is one of the most important things for communication with the world\naround us. It also forms our identity and expressions. Estimating the face\nstructure is a fundamental task in computer vision with applications in\ndifferent areas such as face recognition and medical surgeries. Recently, deep\nlearning techniques achieved significant results for 3D face reconstruction\nfrom flat images. The main challenge of such techniques is a vital need for\nlarge 3D face datasets. Usually, this challenge is handled by synthetic face\ngeneration. However, synthetic datasets suffer from the existence of\nnon-possible faces. Here, we propose a face manifold learning method for\nsynthetic diverse face dataset generation. First, the face structure is divided\ninto the shape and expression groups. Then, a fully convolutional autoencoder\nnetwork is exploited to deal with the non-possible faces, and, simultaneously,\npreserving the dataset diversity. Simulation results show that the proposed\nmethod is capable of denoising highly corrupted faces. The diversity of the\ngenerated dataset is evaluated qualitatively and quantitatively and compared to\nthe existing methods. Experiments show that our manifold learning method\noutperforms the state of the art methods significantly.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 11:00:17 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 12:27:12 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Dinashi", "Kimia", ""], ["Toosi", "Ramin", ""], ["Akhaee", "Mohammad Ali", ""]]}, {"id": "1910.01409", "submitter": "Dexuan Zhang", "authors": "Dexuan Zhang, Tatsuya Harada", "title": "A General Upper Bound for Unsupervised Domain Adaptation", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel upper bound of target error to address the\nproblem for unsupervised domain adaptation. Recent studies reveal that a deep\nneural network can learn transferable features which generalize well to novel\ntasks. Furthermore, a theory proposed by Ben-David et al. (2010) provides a\nupper bound for target error when transferring the knowledge, which can be\nsummarized as minimizing the source error and distance between marginal\ndistributions simultaneously. However, common methods based on the theory\nusually ignore the joint error such that samples from different classes might\nbe mixed together when matching marginal distribution. And in such case, no\nmatter how we minimize the marginal discrepancy, the target error is not\nbounded due to an increasing joint error. To address this problem, we propose a\ngeneral upper bound taking joint error into account, such that the undesirable\ncase can be properly penalized. In addition, we utilize constrained hypothesis\nspace to further formalize a tighter bound as well as a novel cross margin\ndiscrepancy to measure the dissimilarity between hypotheses which alleviates\ninstability during adversarial learning. Extensive empirical evidence shows\nthat our proposal outperforms related approaches in image classification error\nrates on standard domain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 11:31:14 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 07:40:18 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zhang", "Dexuan", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1910.01417", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Stefanos Zafeiriou", "title": "Exploiting multi-CNN features in CNN-RNN based Dimensional Emotion\n  Recognition on the OMG in-the-wild Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel CNN-RNN based approach, which exploits multiple\nCNN features for dimensional emotion recognition in-the-wild, utilizing the\nOne-Minute Gradual-Emotion (OMG-Emotion) dataset. Our approach includes first\npre-training with the relevant and large in size, Aff-Wild and Aff-Wild2\nemotion databases. Low-, mid- and high-level features are extracted from the\ntrained CNN component and are exploited by RNN subnets in a multi-task\nframework. Their outputs constitute an intermediate level prediction; final\nestimates are obtained as the mean or median values of these predictions.\nFusion of the networks is also examined for boosting the obtained performance,\nat Decision-, or at Model-level; in the latter case a RNN was used for the\nfusion. Our approach, although using only the visual modality, outperformed\nstate-of-the-art methods that utilized audio and visual modalities. Some of our\ndevelopments have been submitted to the OMG-Emotion Challenge, ranking second\namong the technologies which used only visual information for valence\nestimation; ranking third overall. Through extensive experimentation, we\nfurther show that arousal estimation is greatly improved when low-level\nfeatures are combined with high-level ones.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 11:56:41 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 10:28:25 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1910.01426", "submitter": "Nan Meng", "authors": "Nan Meng, Hayden K.-H. So, Xing Sun and Edmund Y. Lam", "title": "High-dimensional Dense Residual Convolutional Neural Network for Light\n  Field Reconstruction", "comments": "14 pages. IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (2019)", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2945027", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of high-dimensional light field reconstruction and\ndevelop a learning-based framework for spatial and angular super-resolution.\nMany current approaches either require disparity clues or restore the spatial\nand angular details separately. Such methods have difficulties with\nnon-Lambertian surfaces or occlusions. In contrast, we formulate light field\nsuper-resolution (LFSR) as tensor restoration and develop a learning framework\nbased on a two-stage restoration with 4-dimensional (4D) convolution. This\nallows our model to learn the features capturing the geometry information\nencoded in multiple adjacent views. Such geometric features vary near the\nocclusion regions and indicate the foreground object border. To train a\nfeasible network, we propose a novel normalization operation based on a group\nof views in the feature maps, design a stage-wise loss function, and develop\nthe multi-range training strategy to further improve the performance.\nEvaluations are conducted on a number of light field datasets including\nreal-world scenes, synthetic data, and microscope light fields. The proposed\nmethod achieves superior performance and less execution time comparing with\nother state-of-the-art schemes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 12:15:08 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 07:31:33 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 16:03:46 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2020 05:04:29 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Meng", "Nan", ""], ["So", "Hayden K. -H.", ""], ["Sun", "Xing", ""], ["Lam", "Edmund Y.", ""]]}, {"id": "1910.01442", "submitter": "Chuang Gan", "authors": "Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio\n  Torralba, Joshua B. Tenenbaum", "title": "CLEVRER: CoLlision Events for Video REpresentation and Reasoning", "comments": "The first two authors contributed equally to this work. Accepted as\n  Oral Spotlight as ICLR 2020. Project page: http://clevrer.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to reason about temporal and causal events from videos lies at\nthe core of human intelligence. Most video reasoning benchmarks, however, focus\non pattern recognition from complex visual and language input, instead of on\ncausal structure. We study the complementary problem, exploring the temporal\nand causal structures behind videos of objects with simple visual appearance.\nTo this end, we introduce the CoLlision Events for Video REpresentation and\nReasoning (CLEVRER), a diagnostic video dataset for systematic evaluation of\ncomputational models on a wide range of reasoning tasks. Motivated by the\ntheory of human casual judgment, CLEVRER includes four types of questions:\ndescriptive (e.g., \"what color\"), explanatory (\"what is responsible for\"),\npredictive (\"what will happen next\"), and counterfactual (\"what if\"). We\nevaluate various state-of-the-art models for visual reasoning on our benchmark.\nWhile these models thrive on the perception-based task (descriptive), they\nperform poorly on the causal tasks (explanatory, predictive and\ncounterfactual), suggesting that a principled approach for causal reasoning\nshould incorporate the capability of both perceiving complex visual and\nlanguage inputs, and understanding the underlying dynamics and causal\nrelations. We also study an oracle model that explicitly combines these\ncomponents via symbolic representations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:16:36 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 00:09:07 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Yi", "Kexin", ""], ["Gan", "Chuang", ""], ["Li", "Yunzhu", ""], ["Kohli", "Pushmeet", ""], ["Wu", "Jiajun", ""], ["Torralba", "Antonio", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1910.01460", "submitter": "Yunlu Chen", "authors": "Yunlu Chen, Thomas Mensink, Efstratios Gavves", "title": "3D Neighborhood Convolution: Learning Depth-Aware Features for RGB-D and\n  RGB Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge for RGB-D segmentation is how to effectively incorporate 3D\ngeometric information from the depth channel into 2D appearance features. We\npropose to model the effective receptive field of 2D convolution based on the\nscale and locality from the 3D neighborhood. Standard convolutions are local in\nthe image space ($u, v$), often with a fixed receptive field of 3x3 pixels. We\npropose to define convolutions local with respect to the corresponding point in\nthe 3D real-world space ($x, y, z$), where the depth channel is used to adapt\nthe receptive field of the convolution, which yields the resulting filters\ninvariant to scale and focusing on the certain range of depth. We introduce 3D\nNeighborhood Convolution (3DN-Conv), a convolutional operator around 3D\nneighborhoods. Further, we can use estimated depth to use our RGB-D based\nsemantic segmentation model from RGB input. Experimental results validate that\nour proposed 3DN-Conv operator improves semantic segmentation, using either\nground-truth depth (RGB-D) or estimated depth (RGB).\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:34:04 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Chen", "Yunlu", ""], ["Mensink", "Thomas", ""], ["Gavves", "Efstratios", ""]]}, {"id": "1910.01466", "submitter": "Daniel Hernandez-Juarez", "authors": "Daniel Hernandez-Juarez, Lukas Schneider, Pau Cebrian, Antonio\n  Espinosa, David Vazquez, Antonio M. Lopez, Uwe Franke, Marc Pollefeys, Juan\n  C. Moure", "title": "Slanted Stixels: A way to represent steep streets", "comments": "Journal preprint (published in IJCV 2019:\n  https://link.springer.com/article/10.1007/s11263-019-01226-9). arXiv admin\n  note: text overlap with arXiv:1707.05397", "journal-ref": "IJCV 2019", "doi": "10.1007/s11263-019-01226-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents and evaluates a novel compact scene representation based\non Stixels that infers geometric and semantic information. Our approach\novercomes the previous rather restrictive geometric assumptions for Stixels by\nintroducing a novel depth model to account for non-flat roads and slanted\nobjects. Both semantic and depth cues are used jointly to infer the scene\nrepresentation in a sound global energy minimization formulation.\n  Furthermore, a novel approximation scheme is introduced in order to\nsignificantly reduce the computational complexity of the Stixel algorithm, and\nthen achieve real-time computation capabilities. The idea is to first perform\nan over-segmentation of the image, discarding the unlikely Stixel cuts, and\napply the algorithm only on the remaining Stixel cuts. This work presents a\nnovel over-segmentation strategy based on a Fully Convolutional Network (FCN),\nwhich outperforms an approach based on using local extrema of the disparity\nmap.\n  We evaluate the proposed methods in terms of semantic and geometric accuracy\nas well as run-time on four publicly available benchmark datasets. Our approach\nmaintains accuracy on flat road scene datasets while improving substantially on\na novel non-flat road dataset.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 11:52:01 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Hernandez-Juarez", "Daniel", ""], ["Schneider", "Lukas", ""], ["Cebrian", "Pau", ""], ["Espinosa", "Antonio", ""], ["Vazquez", "David", ""], ["Lopez", "Antonio M.", ""], ["Franke", "Uwe", ""], ["Pollefeys", "Marc", ""], ["Moure", "Juan C.", ""]]}, {"id": "1910.01467", "submitter": "Wonpyo Park", "authors": "Wonpyo Park, Paul Hongsuck Seo, Bohyung Han, Minsu Cho", "title": "Regularizing Neural Networks via Stochastic Branch Layers", "comments": "ACML 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel stochastic regularization technique for deep neural\nnetworks, which decomposes a layer into multiple branches with different\nparameters and merges stochastically sampled combinations of the outputs from\nthe branches during training. Since the factorized branches can collapse into a\nsingle branch through a linear operation, inference requires no additional\ncomplexity compared to the ordinary layers. The proposed regularization method,\nreferred to as StochasticBranch, is applicable to any linear layers such as\nfully-connected or convolution layers. The proposed regularizer allows the\nmodel to explore diverse regions of the model parameter space via multiple\ncombinations of branches to find better local minima. An extensive set of\nexperiments shows that our method effectively regularizes networks and further\nimproves the generalization performance when used together with other existing\nregularization techniques.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:44:55 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Park", "Wonpyo", ""], ["Seo", "Paul Hongsuck", ""], ["Han", "Bohyung", ""], ["Cho", "Minsu", ""]]}, {"id": "1910.01568", "submitter": "Francesco Marra", "authors": "Francesco Marra and Cristiano Saltori and Giulia Boato and Luisa\n  Verdoliva", "title": "Incremental learning for the detection and classification of\n  GAN-generated images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current developments in computer vision and deep learning allow to\nautomatically generate hyper-realistic images, hardly distinguishable from real\nones. In particular, human face generation achieved a stunning level of\nrealism, opening new opportunities for the creative industry but, at the same\ntime, new scary scenarios where such content can be maliciously misused.\nTherefore, it is essential to develop innovative methodologies to automatically\ntell apart real from computer generated multimedia, possibly able to follow the\nevolution and continuous improvement of data in terms of quality and realism.\nIn the last few years, several deep learning-based solutions have been proposed\nfor this problem, mostly based on Convolutional Neural Networks (CNNs).\nAlthough results are good in controlled conditions, it is not clear how such\nproposals can adapt to real-world scenarios, where learning needs to\ncontinuously evolve as new types of generated data appear. In this work, we\ntackle this problem by proposing an approach based on incremental learning for\nthe detection and classification of GAN-generated images. Experiments on a\ndataset comprising images generated by several GAN-based architectures show\nthat the proposed method is able to correctly perform discrimination when new\nGANs are presented to the network\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 16:14:57 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 18:47:26 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Marra", "Francesco", ""], ["Saltori", "Cristiano", ""], ["Boato", "Giulia", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1910.01634", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Hyojin Kim, Jayaraman J. Thiagarajan, K. Aditya Mohan,\n  Kyle M. Champley", "title": "Improving Limited Angle CT Reconstruction with a Robust GAN Prior", "comments": "NeurIPS 2019 Workshop on Deep Inverse Problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited angle CT reconstruction is an under-determined linear inverse problem\nthat requires appropriate regularization techniques to be solved. In this work\nwe study how pre-trained generative adversarial networks (GANs) can be used to\nclean noisy, highly artifact laden reconstructions from conventional\ntechniques, by effectively projecting onto the inferred image manifold. In\nparticular, we use a robust version of the popularly used GAN prior for inverse\nproblems, based on a recent technique called corruption mimicking, that\nsignificantly improves the reconstruction quality. The proposed approach\noperates in the image space directly, as a result of which it does not need to\nbe trained or require access to the measurement model, is scanner agnostic, and\ncan work over a wide range of sensing scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 17:52:14 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 23:19:21 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 23:22:24 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2020 17:40:27 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Anirudh", "Rushil", ""], ["Kim", "Hyojin", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Mohan", "K. Aditya", ""], ["Champley", "Kyle M.", ""]]}, {"id": "1910.01636", "submitter": "Florent Chiaroni", "authors": "Florent Chiaroni, Mohamed-Cherif Rahal, Nicolas Hueber, Frederic\n  Dufaux", "title": "Self-supervised learning for autonomous vehicles perception: A\n  conciliation between analytical and learning methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, supervised deep learning techniques yield the best state-of-the-art\nprediction performances for a wide variety of computer vision tasks. However,\nsuch supervised techniques generally require a large amount of manually labeled\ntraining data. In the context of autonomous vehicles perception, this\nrequirement is critical, as the distribution of sensor data can continuously\nchange and include several unexpected variations. It turns out that a category\nof learning techniques, referred to as self-supervised learning (SSL), consists\nof replacing the manual labeling effort by an automatic labeling process.\nThanks to their ability to learn on the application time and in varying\nenvironments, state-of-the-art SSL techniques provide a valid alternative to\nsupervised learning for a variety of different tasks, including long-range\ntraversable area segmentation, moving obstacle instance segmentation, long-term\nmoving obstacle tracking, or depth map prediction. In this tutorial-style\narticle, we present an overview and a general formalization of the concept of\nself-supervised learning (SSL) for autonomous vehicles perception. This\nformalization provides helpful guidelines for developing novel frameworks based\non generic SSL principles. Moreover, it enables to point out significant\nchallenges in the design of future SSL systems.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 17:56:18 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 21:03:21 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chiaroni", "Florent", ""], ["Rahal", "Mohamed-Cherif", ""], ["Hueber", "Nicolas", ""], ["Dufaux", "Frederic", ""]]}, {"id": "1910.01665", "submitter": "Jizong Peng", "authors": "Jizong Peng, Christian Desrosiers, Marco Pedersoli", "title": "Information based Deep Clustering: An experimental study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, two methods have shown outstanding performance for clustering\nimages and jointly learning the feature representation. The first, called\nInformation Maximiz-ing Self-Augmented Training (IMSAT), maximizes the mutual\ninformation between input and clusters while using a regularization term based\non virtual adversarial examples. The second, named Invariant Information\nClustering (IIC), maximizes the mutual information between the clustering of a\nsample and its geometrically transformed version. These methods use mutual\ninformation in distinct ways and leverage different kinds of transformations.\nThis work proposes a comprehensive analysis of transformation and losses for\ndeep clustering, where we compare numerous combinations of these two components\nand evaluate how they interact with one another. Results suggest that mutual\ninformation between a sample and its transformed representation leads to\nstate-of-the-art performance for deep clustering, especially when used jointly\nwith geometrical and adversarial transformations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 18:07:57 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 01:14:25 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Peng", "Jizong", ""], ["Desrosiers", "Christian", ""], ["Pedersoli", "Marco", ""]]}, {"id": "1910.01666", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Shusen Liu, Peer-Timo\n  Bremer, Brian K. Spears", "title": "Exploring Generative Physics Models with Scientific Priors in Inertial\n  Confinement Fusion", "comments": "Machine Learning for Physical Sciences Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is significant interest in using modern neural networks for scientific\napplications due to their effectiveness in modeling highly complex, non-linear\nproblems in a data-driven fashion. However, a common challenge is to verify the\nscientific plausibility or validity of outputs predicted by a neural network.\nThis work advocates the use of known scientific constraints as a lens into\nevaluating, exploring, and understanding such predictions for the problem of\ninertial confinement fusion.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 18:08:31 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Liu", "Shusen", ""], ["Bremer", "Peer-Timo", ""], ["Spears", "Brian K.", ""]]}, {"id": "1910.01681", "submitter": "Oleg Shipitko", "authors": "Mikhail O. Chekanov, Oleg S. Shipitko, Egor I. Ershov", "title": "1-point RANSAC for Circular Motion Estimation in Computed Tomography\n  (CT)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a RANSAC-based algorithm for determining the axial\nrotation angle of an object from a pair of its tomographic projections. An\nequation is derived for calculating the rotation angle using one correct\nkeypoints correspondence of two tomographic projections. The proposed algorithm\nconsists of the following steps: keypoints detection and matching, rotation\nangle estimation for each correspondence, outliers filtering with the RANSAC\nalgorithm, finally, calculation of the desired angle by minimizing the\nre-projection error from the remaining correspondences. To validate the\nproposed method an experimental comparison against methods based on analysis of\nthe distribution of the angles computed from all correspondences is conducted.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 18:48:32 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Chekanov", "Mikhail O.", ""], ["Shipitko", "Oleg S.", ""], ["Ershov", "Egor I.", ""]]}, {"id": "1910.01684", "submitter": "Harshit Gupta", "authors": "Jaejun Yoo, Kyong Hwan Jin, Harshit Gupta, Jerome Yerly, Matthias\n  Stuber, and Michael Unser", "title": "Time-Dependent Deep Image Prior for Dynamic MRI", "comments": "11 pages, 6 figures. First Author has been changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel unsupervised deep-learning-based algorithm for dynamic\nmagnetic resonance imaging (MRI) reconstruction. Dynamic MRI requires rapid\ndata acquisition for the study of moving organs such as the heart. Existing\nreconstruction methods suffer from restrictions either in the model design or\nin the absence of ground-truth data, resulting in low image quality. We\nintroduce a generalized version of the deep-image-prior approach, which\noptimizes the network weights to fit a sequence of sparsely acquired dynamic\nMRI measurements. Our method needs neither prior training nor additional data.\nIn particular, for cardiac images, it does not require the marking of\nheartbeats or the reordering of spokes. The key ingredients of our method are\nthreefold: 1) a fixed low-dimensional manifold that encodes the temporal\nvariations of images; 2) a network that maps the manifold into a more\nexpressive latent space; and 3) a convolutional neural network that generates a\ndynamic series of MRI images from the latent variables and that favors their\nconsistency with the measurements in k-space. Our method outperforms the\nstate-of-the-art methods quantitatively and qualitatively in both retrospective\nand real fetal cardiac datasets. To the best of our knowledge, this is the\nfirst unsupervised deep-learning-based method that can reconstruct the\ncontinuous variation of dynamic MRI sequences with high spatial resolution.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 18:55:47 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 15:47:01 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yoo", "Jaejun", ""], ["Jin", "Kyong Hwan", ""], ["Gupta", "Harshit", ""], ["Yerly", "Jerome", ""], ["Stuber", "Matthias", ""], ["Unser", "Michael", ""]]}, {"id": "1910.01694", "submitter": "Jingrong Lin", "authors": "Jingrong Lin, Keegan Lensink, Eldad Haber", "title": "Fluid Flow Mass Transport for Generative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks have been shown to be powerful in generating\ncontent. To this end, they have been studied intensively in the last few years.\nNonetheless, training these networks requires solving a saddle point problem\nthat is difficult to solve and slowly converging. Motivated from techniques in\nthe registration of point clouds and by the fluid flow formulation of mass\ntransport, we investigate a new formulation that is based on strict\nminimization, without the need for the maximization. The formulation views the\nproblem as a matching problem rather than an adversarial one and thus allows us\nto quickly converge and obtain meaningful metrics in the optimization path.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 19:14:52 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 20:04:45 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Lin", "Jingrong", ""], ["Lensink", "Keegan", ""], ["Haber", "Eldad", ""]]}, {"id": "1910.01701", "submitter": "Chen Fu", "authors": "Chen Fu, Chiyu Dong, Xiao Zhang and John M. Dolan", "title": "Low-cost LIDAR based Vehicle Pose Estimation and Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting surrounding vehicles by low-cost LIDAR has been drawing enormous\nattention. In low-cost LIDAR, vehicles present a multi-layer L-Shape. Based on\nour previous optimization/criteria-based L-Shape fitting algorithm, we here\npropose a data-driven and model-based method for robust vehicle segmentation\nand tracking. The new method uses T-linkage RANSAC to take a limited amount of\nnoisy data and performs a robust segmentation for a moving car against noise.\nCompared with our previous method, T-Linkage RANSAC is more tolerant of\nobservation uncertainties, i.e., the number of sides of the target being\nobserved, and gets rid of the L-Shape assumption. In addition, a vehicle\ntracking system with Multi-Model Association (MMA) is built upon the\nsegmentation result, which provides smooth trajectories of tracked objects. A\nmanually labeled dataset from low-cost multi-layer LIDARs for validation will\nalso be released with the paper. Experiments on the dataset show that the new\napproach outperforms previous ones based on multiple criteria. The new\nalgorithm can also run in real-time.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 19:38:04 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Fu", "Chen", ""], ["Dong", "Chiyu", ""], ["Zhang", "Xiao", ""], ["Dolan", "John M.", ""]]}, {"id": "1910.01712", "submitter": "Shih-Han Chou", "authors": "Shih-Han Chou, Cheng Sun, Wen-Yen Chang, Wan-Ting Hsu, Min Sun,\n  Jianlong Fu", "title": "360-Indoor: Towards Learning Real-World Objects in 360{\\deg} Indoor\n  Equirectangular Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there are several widely used object detection datasets, current\ncomputer vision algorithms are still limited in conventional images. Such\nimages narrow our vision in a restricted region. On the other hand, 360{\\deg}\nimages provide a thorough sight. In this paper, our goal is to provide a\nstandard dataset to facilitate the vision and machine learning communities in\n360{\\deg} domain. To facilitate the research, we present a real-world 360{\\deg}\npanoramic object detection dataset, 360-Indoor, which is a new benchmark for\nvisual object detection and class recognition in 360{\\deg} indoor images. It is\nachieved by gathering images of complex indoor scenes containing common objects\nand the intensive annotated bounding field-of-view. In addition, 360-Indoor has\nseveral distinct properties: (1) the largest category number (37 labels in\ntotal). (2) the most complete annotations on average (27 bounding boxes per\nimage). The selected 37 objects are all common in indoor scene. With around 3k\nimages and 90k labels in total, 360-Indoor achieves the largest dataset for\ndetection in 360{\\deg} images. In the end, extensive experiments on the\nstate-of-the-art methods for both classification and detection are provided. We\nwill release this dataset in the near future.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 20:36:49 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Chou", "Shih-Han", ""], ["Sun", "Cheng", ""], ["Chang", "Wen-Yen", ""], ["Hsu", "Wan-Ting", ""], ["Sun", "Min", ""], ["Fu", "Jianlong", ""]]}, {"id": "1910.01717", "submitter": "Feng Liu", "authors": "Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, Anil Jain", "title": "On the Detection of Digital Face Manipulation", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting manipulated facial images and videos is an increasingly important\ntopic in digital media forensics. As advanced face synthesis and manipulation\nmethods are made available, new types of fake face representations are being\ncreated which have raised significant concerns for their use in social media.\nHence, it is crucial to detect manipulated face images and localize manipulated\nregions. Instead of simply using multi-task learning to simultaneously detect\nmanipulated images and predict the manipulated mask (regions), we propose to\nutilize an attention mechanism to process and improve the feature maps for the\nclassification task. The learned attention maps highlight the informative\nregions to further improve the binary classification (genuine face v. fake\nface), and also visualize the manipulated regions. To enable our study of\nmanipulated face detection and localization, we collect a large-scale database\nthat contains numerous types of facial forgeries. With this dataset, we perform\na thorough analysis of data-driven fake face detection. We show that the use of\nan attention mechanism improves facial forgery detection and manipulated region\nlocalization.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 20:51:47 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 18:06:15 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 00:35:38 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 16:36:27 GMT"}, {"version": "v5", "created": "Sat, 24 Oct 2020 01:41:08 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Dang", "Hao", ""], ["Liu", "Feng", ""], ["Stehouwer", "Joel", ""], ["Liu", "Xiaoming", ""], ["Jain", "Anil", ""]]}, {"id": "1910.01735", "submitter": "Bo Jiang", "authors": "Bo Jiang, Beibei Wang, Jin Tang and Bin Luo", "title": "GmCN: Graph Mask Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have shown very powerful for graph data\nrepresentation and learning tasks. Existing GCNs usually conduct feature\naggregation on a fixed neighborhood graph in which each node computes its\nrepresentation by aggregating the feature representations of all its neighbors\nwhich is biased by its own representation. However, this fixed aggregation\nstrategy is not guaranteed to be optimal for GCN based graph learning and also\ncan be affected by some graph structure noises, such as incorrect or undesired\nedge connections. To address these issues, we propose a novel Graph mask\nConvolutional Network (GmCN) in which nodes can adaptively select the optimal\nneighbors in their feature aggregation to better serve GCN learning. GmCN can\nbe theoretically interpreted by a regularization framework, based on which we\nderive a simple update algorithm to determine the optimal mask adaptively in\nGmCN training process. Experiments on several datasets validate the\neffectiveness of GmCN.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 02:59:13 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 08:50:58 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Jiang", "Bo", ""], ["Wang", "Beibei", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "1910.01751", "submitter": "Suraj Nair", "authors": "Suraj Nair, Yuke Zhu, Silvio Savarese, Li Fei-Fei", "title": "Causal Induction from Visual Observations for Goal Directed Tasks", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal reasoning has been an indispensable capability for humans and other\nintelligent animals to interact with the physical world. In this work, we\npropose to endow an artificial agent with the capability of causal reasoning\nfor completing goal-directed tasks. We develop learning-based approaches to\ninducing causal knowledge in the form of directed acyclic graphs, which can be\nused to contextualize a learned goal-conditional policy to perform tasks in\nnovel environments with latent causal structures. We leverage attention\nmechanisms in our causal induction model and goal-conditional policy, enabling\nus to incrementally generate the causal graph from the agent's visual\nobservations and to selectively use the induced graph for determining actions.\nOur experiments show that our method effectively generalizes towards completing\nnew tasks in novel environments with previously unseen causal structures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 22:32:40 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Nair", "Suraj", ""], ["Zhu", "Yuke", ""], ["Savarese", "Silvio", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1910.01763", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Andriy Myronenko, Ziyue Xu, Wenqi Li, Holger Roth, Yufang\n  Huang, Fausto Milletari, Daguang Xu", "title": "NeurReg: Neural Registration and Its Application to Image Segmentation", "comments": "WACV 2020 first round early accept; supplementary\n  https://drive.google.com/file/d/1kzTLQn8cpoQNAYWUDJMtN5HcqhbWbl7G/view?usp=sharing;\n  code will be released soon under NVIDIA open source; demos\n  https://www.youtube.com/watch?v=GYLD7t7dSAg&t=3s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration is a fundamental task in medical image analysis which can be\napplied to several tasks including image segmentation, intra-operative\ntracking, multi-modal image alignment, and motion analysis. Popular\nregistration tools such as ANTs and NiftyReg optimize an objective function for\neach pair of images from scratch which is time-consuming for large images with\ncomplicated deformation. Facilitated by the rapid progress of deep learning,\nlearning-based approaches such as VoxelMorph have been emerging for image\nregistration. These approaches can achieve competitive performance in a\nfraction of a second on advanced GPUs. In this work, we construct a neural\nregistration framework, called NeurReg, with a hybrid loss of displacement\nfields and data similarity, which substantially improves the current\nstate-of-the-art of registrations. Within the framework, we simulate various\ntransformations by a registration simulator which generates fixed image and\ndisplacement field ground truth for training. Furthermore, we design three\nsegmentation frameworks based on the proposed registration framework: 1)\natlas-based segmentation, 2) joint learning of both segmentation and\nregistration tasks, and 3) multi-task learning with atlas-based segmentation as\nan intermediate feature. Extensive experimental results validate the\neffectiveness of the proposed NeurReg framework based on various metrics: the\nendpoint error (EPE) of the predicted displacement field, mean square error\n(MSE), normalized local cross-correlation (NLCC), mutual information (MI), Dice\ncoefficient, uncertainty estimation, and the interpretability of the\nsegmentation. The proposed NeurReg improves registration accuracy with fast\ninference speed, which can greatly accelerate related medical image analysis\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 00:07:22 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zhu", "Wentao", ""], ["Myronenko", "Andriy", ""], ["Xu", "Ziyue", ""], ["Li", "Wenqi", ""], ["Roth", "Holger", ""], ["Huang", "Yufang", ""], ["Milletari", "Fausto", ""], ["Xu", "Daguang", ""]]}, {"id": "1910.01764", "submitter": "Vitor Guizilini", "authors": "Rares Ambrus, Vitor Guizilini, Jie Li, Sudeep Pillai and Adrien Gaidon", "title": "Two Stream Networks for Self-Supervised Ego-Motion Estimation", "comments": "Conference on Robot Learning (CoRL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning depth and camera ego-motion from raw unlabeled RGB video streams is\nseeing exciting progress through self-supervision from strong geometric cues.\nTo leverage not only appearance but also scene geometry, we propose a novel\nself-supervised two-stream network using RGB and inferred depth information for\naccurate visual odometry. In addition, we introduce a sparsity-inducing data\naugmentation policy for ego-motion learning that effectively regularizes the\npose network to enable stronger generalization performance. As a result, we\nshow that our proposed two-stream pose network achieves state-of-the-art\nresults among learning-based methods on the KITTI odometry benchmark, and is\nespecially suited for self-supervision at scale. Our experiments on a\nlarge-scale urban driving dataset of 1 million frames indicate that the\nperformance of our proposed architecture does indeed scale progressively with\nmore data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 00:31:49 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 19:26:35 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 18:10:55 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Ambrus", "Rares", ""], ["Guizilini", "Vitor", ""], ["Li", "Jie", ""], ["Pillai", "Sudeep", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1910.01765", "submitter": "Vitor Guizilini", "authors": "Vitor Guizilini, Jie Li, Rares Ambrus, Sudeep Pillai and Adrien Gaidon", "title": "Robust Semi-Supervised Monocular Depth Estimation with Reprojected\n  Distances", "comments": "Conference on Robot Learning (CoRL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense depth estimation from a single image is a key problem in computer\nvision, with exciting applications in a multitude of robotic tasks. Initially\nviewed as a direct regression problem, requiring annotated labels as\nsupervision at training time, in the past few years a substantial amount of\nwork has been done in self-supervised depth training based on strong geometric\ncues, both from stereo cameras and more recently from monocular video\nsequences. In this paper we investigate how these two approaches (supervised &\nself-supervised) can be effectively combined, so that a depth model can learn\nto encode true scale from sparse supervision while achieving high fidelity\nlocal accuracy by leveraging geometric cues. To this end, we propose a novel\nsupervised loss term that complements the widely used photometric loss, and\nshow how it can be used to train robust semi-supervised monocular depth\nestimation models. Furthermore, we evaluate how much supervision is actually\nnecessary to train accurate scale-aware monocular depth models, showing that\nwith our proposed framework, very sparse LiDAR information, with as few as 4\nbeams (less than 100 valid depth values per image), is enough to achieve\nresults competitive with the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 00:32:20 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 19:26:48 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 17:59:41 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Guizilini", "Vitor", ""], ["Li", "Jie", ""], ["Ambrus", "Rares", ""], ["Pillai", "Sudeep", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1910.01789", "submitter": "Akshay L Chandra", "authors": "Akshay L Chandra, Sai Vikas Desai, Vineeth N Balasubramanian, Seishi\n  Ninomiya, Wei Guo", "title": "Active Learning with Point Supervision for Cost-Effective Panicle\n  Detection in Cereal Crops", "comments": "Accepted as a journal paper at BMC Plant Methods (February 2020)", "journal-ref": null, "doi": "10.1186/S13007-020-00575-8", "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Panicle density of cereal crops such as wheat and sorghum is one of the main\ncomponents for plant breeders and agronomists in understanding the yield of\ntheir crops. To phenotype the panicle density effectively, researchers agree\nthere is a significant need for computer vision-based object detection\ntechniques. Especially in recent times, research in deep learning-based object\ndetection shows promising results in various agricultural studies. However,\ntraining such systems usually requires a lot of bounding-box labeled data.\nSince crops vary by both environmental and genetic conditions, acquisition of\nhuge amount of labeled image datasets for each crop is expensive and\ntime-consuming. Thus, to catalyze the widespread usage of automatic object\ndetection for crop phenotyping, a cost-effective method to develop such\nautomated systems is essential. We propose a point supervision based active\nlearning approach for panicle detection in cereal crops. In our approach, the\nmodel constantly interacts with a human annotator by iteratively querying the\nlabels for only the most informative images, as opposed to all images in a\ndataset. Our query method is specifically designed for cereal crops which\nusually tend to have panicles with low variance in appearance. Our method\nreduces labeling costs by intelligently leveraging low-cost weak labels (object\ncenters) for picking the most informative images for which strong labels\n(bounding boxes) are required. We show promising results on two publicly\navailable cereal crop datasets - Sorghum and Wheat. On Sorghum, 6 variants of\nour proposed method outperform the best baseline method with more than 55%\nsavings in labeling time. Similarly, on Wheat, 3 variants of our proposed\nmethods outperform the best baseline method with more than 50% of savings in\nlabeling time.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 02:56:43 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 09:06:44 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 07:26:02 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Chandra", "Akshay L", ""], ["Desai", "Sai Vikas", ""], ["Balasubramanian", "Vineeth N", ""], ["Ninomiya", "Seishi", ""], ["Guo", "Wei", ""]]}, {"id": "1910.01821", "submitter": "Yan Wang", "authors": "Yan Wang, Kensuke Harada, Weiwei Wan", "title": "Motion Planning through Demonstration to Deal with Complex Motions in\n  Assembly Process", "comments": "7 pages, 12 figures, 1 table, Humanoids 2019 conference accepted\n  paper. Video can be found here: https://www.youtube.com/watch?v=hQxP-YVKMwc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex and skillful motions in actual assembly process are challenging for\nthe robot to generate with existing motion planning approaches, because some\nkey poses during the human assembly can be too skillful for the robot to\nrealize automatically. In order to deal with this problem, this paper develops\na motion planning method using skillful motions from demonstration, which can\nbe applied to complete robotic assembly process including complex and skillful\nmotions. In order to demonstrate conveniently without redundant third-party\ndevices, we attach augmented reality (AR) markers to the manipulated object to\ntrack and capture poses of the object during the human assembly process, which\nare employed as key poses to execute motion planning by the planner. Derivative\nof every key pose serves as criterion to determine the priority of use of key\nposes in order to accelerate the motion planning. The effectiveness of the\npresented method is verified through some numerical examples and actual robot\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 07:25:46 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Wang", "Yan", ""], ["Harada", "Kensuke", ""], ["Wan", "Weiwei", ""]]}, {"id": "1910.01833", "submitter": "Tanner Bohn", "authors": "Tanner Bohn, Yining Hu, Charles X. Ling", "title": "Few-Shot Abstract Visual Reasoning With Spectral Features", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an image preprocessing technique capable of improving the\nperformance of few-shot classifiers on abstract visual reasoning tasks. Many\nvisual reasoning tasks with abstract features are easy for humans to learn with\nfew examples but very difficult for computer vision approaches with the same\nnumber of samples, despite the ability for deep learning models to learn\nabstract features. Same-different (SD) problems represent a type of visual\nreasoning task requiring knowledge of pattern repetition within individual\nimages, and modern computer vision approaches have largely faltered on these\nclassification problems, even when provided with vast amounts of training data.\nWe propose a simple method for solving these problems based on the insight that\nremoving peaks from the amplitude spectrum of an image is capable of\nemphasizing the unique parts of the image. When combined with several\nclassifiers, our method performs well on the SD SVRT tasks with few-shot\nlearning, improving upon the best comparable results on all tasks, with average\nabsolute accuracy increases nearly 40% for some classifiers. In particular, we\nfind that combining Relational Networks with this image preprocessing approach\nimproves their performance from chance-level to over 90% accuracy on several SD\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 08:15:15 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Bohn", "Tanner", ""], ["Hu", "Yining", ""], ["Ling", "Charles X.", ""]]}, {"id": "1910.01837", "submitter": "Johannes Rabold", "authors": "Johannes Rabold, Hannah Deininger, Michael Siebers, Ute Schmid", "title": "Enriching Visual with Verbal Explanations for Relational Concepts --\n  Combining LIME with Aleph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing number of deep learning applications, there is a growing\ndemand for explanations. Visual explanations provide information about which\nparts of an image are relevant for a classifier's decision. However,\nhighlighting of image parts (e.g., an eye) cannot capture the relevance of a\nspecific feature value for a class (e.g., that the eye is wide open).\nFurthermore, highlighting cannot convey whether the classification depends on\nthe mere presence of parts or on a specific spatial relation between them.\nConsequently, we present an approach that is capable of explaining a\nclassifier's decision in terms of logic rules obtained by the Inductive Logic\nProgramming system Aleph. The examples and the background knowledge needed for\nAleph are based on the explanation generation method LIME. We demonstrate our\napproach with images of a blocksworld domain. First, we show that our approach\nis capable of identifying a single relation as important explanatory construct.\nAfterwards, we present the more complex relational concept of towers. Finally,\nwe show how the generated relational rules can be explicitly related with the\ninput image, resulting in richer explanations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 08:51:41 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Rabold", "Johannes", ""], ["Deininger", "Hannah", ""], ["Siebers", "Michael", ""], ["Schmid", "Ute", ""]]}, {"id": "1910.01842", "submitter": "Duc Tam Nguyen", "authors": "Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi\n  Hoai Phuong Nguyen, Laura Beggel, Thomas Brox", "title": "SELF: Learning to Filter Noisy Labels with Self-Ensembling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been shown to over-fit a dataset when being\ntrained with noisy labels for a long enough time. To overcome this problem, we\npresent a simple and effective method self-ensemble label filtering (SELF) to\nprogressively filter out the wrong labels during training. Our method improves\nthe task performance by gradually allowing supervision only from the\npotentially non-noisy (clean) labels and stops learning on the filtered noisy\nlabels. For the filtering, we form running averages of predictions over the\nentire training dataset using the network output at different training epochs.\nWe show that these ensemble estimates yield more accurate identification of\ninconsistent predictions throughout training than the single estimates of the\nnetwork at the most recent training epoch. While filtered samples are removed\nentirely from the supervised training loss, we dynamically leverage them via\nsemi-supervised learning in the unsupervised loss. We demonstrate the positive\neffect of such an approach on various image classification tasks under both\nsymmetric and asymmetric label noise and at different noise ratios. It\nsubstantially outperforms all previous works on noise-aware learning across\ndifferent datasets and can be applied to a broad set of network architectures.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 08:59:54 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Nguyen", "Duc Tam", ""], ["Mummadi", "Chaithanya Kumar", ""], ["Ngo", "Thi Phuong Nhung", ""], ["Nguyen", "Thi Hoai Phuong", ""], ["Beggel", "Laura", ""], ["Brox", "Thomas", ""]]}, {"id": "1910.01843", "submitter": "Philipp Kratzer", "authors": "Philipp Kratzer, Marc Toussaint, Jim Mainprice", "title": "Prediction of Human Full-Body Movements with Motion Optimization and\n  Recurrent Neural Networks", "comments": "International Conference on Robotics and Automation (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human movement prediction is difficult as humans naturally exhibit complex\nbehaviors that can change drastically from one environment to the next. In\norder to alleviate this issue, we propose a prediction framework that decouples\nshort-term prediction, linked to internal body dynamics, and long-term\nprediction, linked to the environment and task constraints. In this work we\ninvestigate encoding short-term dynamics in a recurrent neural network, while\nwe account for environmental constraints, such as obstacle avoidance, using\ngradient-based trajectory optimization. Experiments on real motion data\ndemonstrate that our framework improves the prediction with respect to\nstate-of-the-art motion prediction methods, as it accounts to beforehand unseen\nenvironmental structures. Moreover we demonstrate on an example, how this\nframework can be used to plan robot trajectories that are optimized to\ncoordinate with a human partner.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 09:04:56 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 10:57:07 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Kratzer", "Philipp", ""], ["Toussaint", "Marc", ""], ["Mainprice", "Jim", ""]]}, {"id": "1910.01851", "submitter": "Andres Ussa Caycedo", "authors": "Jyotibdha Acharya, Andres Ussa Caycedo, Vandana Reddy Padala, Rishi\n  Raj Sidhu Singh, Garrick Orchard, Bharath Ramesh and Arindam Basu", "title": "EBBIOT: A Low-complexity Tracking Algorithm for Surveillance in IoVT\n  Using Stationary Neuromorphic Vision Sensors", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present EBBIOT-a novel paradigm for object tracking using\nstationary neuromorphic vision sensors in low-power sensor nodes for the\nInternet of Video Things (IoVT). Different from fully event based tracking or\nfully frame based approaches, we propose a mixed approach where we create\nevent-based binary images (EBBI) that can use memory efficient noise filtering\nalgorithms. We exploit the motion triggering aspect of neuromorphic sensors to\ngenerate region proposals based on event density counts with >1000X less memory\nand computes compared to frame based approaches. We also propose a simple\noverlap based tracker (OT) with prediction based handling of occlusion. Our\noverall approach requires 7X less memory and 3X less computations than\nconventional noise filtering and event based mean shift (EBMS) tracking.\nFinally, we show that our approach results in significantly higher precision\nand recall compared to EBMS approach as well as Kalman Filter tracker when\nevaluated over 1.1 hours of traffic recordings at two different locations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 09:50:55 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Acharya", "Jyotibdha", ""], ["Caycedo", "Andres Ussa", ""], ["Padala", "Vandana Reddy", ""], ["Singh", "Rishi Raj Sidhu", ""], ["Orchard", "Garrick", ""], ["Ramesh", "Bharath", ""], ["Basu", "Arindam", ""]]}, {"id": "1910.01853", "submitter": "Zied Selmi", "authors": "Zied Selmi, Mohamed Ben Halima, Umapada Pal and M.Adel Alimi", "title": "DELP-DAR System for License Plate Detection and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic License Plate detection and Recognition (ALPR) is a quite popular\nand active research topic in the field of computer vision, image processing and\nintelligent transport systems. ALPR is used to make detection and recognition\nprocesses more robust and efficient in highly complicated environments and\nbackgrounds. Several research investigations are still necessary due to some\nconstraints such as: completeness of numbering systems of countries, different\ncolors, various languages, multiple sizes and varied fonts. For this, we\npresent in this paper an automatic framework for License Plate (LP) detection\nand recognition from complex scenes. Our framework is based on mask region\nconvolutional neural networks used for LP detection, segmentation and\nrecognition. Although some studies have focused on LP detection, LP\nrecognition, LP segmentation or just two of them, our study uses the maskr-cnn\nin the three stages. The evaluation of our framework is enhanced by four\ndatasets for different countries and consequently with various languages. In\nfact, it tested on four datasets including images captured from multiple scenes\nunder numerous conditions such as varied orientation, poor quality images,\nblurred images and complex environmental backgrounds. Extensive experiments\nshow the robustness and efficiency of our suggested framework in all datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 10:02:57 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Selmi", "Zied", ""], ["Halima", "Mohamed Ben", ""], ["Pal", "Umapada", ""], ["Alimi", "M. Adel", ""]]}, {"id": "1910.01858", "submitter": "Rakesh Katuwal Mr.", "authors": "Rakesh Katuwal, P.N. Suganthan", "title": "Stacked Autoencoder Based Deep Random Vector Functional Link Neural\n  Network for Classification", "comments": "29 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme learning machine (ELM), which can be viewed as a variant of Random\nVector Functional Link (RVFL) network without the input-output direct\nconnections, has been extensively used to create multi-layer (deep) neural\nnetworks. Such networks employ randomization based autoencoders (AE) for\nunsupervised feature extraction followed by an ELM classifier for final\ndecision making. Each randomization based AE acts as an independent feature\nextractor and a deep network is obtained by stacking several such AEs. Inspired\nby the better performance of RVFL over ELM, in this paper, we propose several\ndeep RVFL variants by utilizing the framework of stacked autoencoders.\nSpecifically, we introduce direct connections (feature reuse) from preceding\nlayers to the fore layers of the network as in the original RVFL network. Such\nconnections help to regularize the randomization and also reduce the model\ncomplexity. Furthermore, we also introduce denoising criterion, recovering\nclean inputs from their corrupted versions, in the autoencoders to achieve\nbetter higher level representations than the ordinary autoencoders. Extensive\nexperiments on several classification datasets show that our proposed deep\nnetworks achieve overall better and faster generalization than the other\nrelevant state-of-the-art deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 10:25:24 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 13:15:43 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 09:33:35 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 05:25:33 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Katuwal", "Rakesh", ""], ["Suganthan", "P. N.", ""]]}, {"id": "1910.01877", "submitter": "James Clough", "authors": "James R. Clough, Nicholas Byrne, Ilkay Oksuz, Veronika A. Zimmer,\n  Julia A. Schnabel, Andrew P. King", "title": "A Topological Loss Function for Deep-Learning based Image Segmentation\n  using Persistent Homology", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3013679", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for training neural networks to perform image or volume\nsegmentation in which prior knowledge about the topology of the segmented\nobject can be explicitly provided and then incorporated into the training\nprocess. By using the differentiable properties of persistent homology, a\nconcept used in topological data analysis, we can specify the desired topology\nof segmented objects in terms of their Betti numbers and then drive the\nproposed segmentations to contain the specified topological features.\nImportantly this process does not require any ground-truth labels, just prior\nknowledge of the topology of the structure being segmented. We demonstrate our\napproach in three experiments. Firstly we create a synthetic task in which\nhandwritten MNIST digits are de-noised, and show that using this kind of\ntopological prior knowledge in the training of the network significantly\nimproves the quality of the de-noised digits. Secondly we perform an experiment\nin which the task is segmenting the myocardium of the left ventricle from\ncardiac magnetic resonance images. We show that the incorporation of the prior\nknowledge of the topology of this anatomy improves the resulting segmentations\nin terms of both the topological accuracy and the Dice coefficient. Thirdly, we\nextend the method to 3D volumes and demonstrate its performance on the task of\nsegmenting the placenta from ultrasound data, again showing that incorporating\ntopological priors improves performance on this challenging task. We find that\nembedding explicit prior knowledge in neural network segmentation tasks is most\nbeneficial when the segmentation task is especially challenging and that it can\nbe used in either a semi-supervised or post-processing context to extract a\nuseful training gradient from images without pixelwise labels.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 11:18:49 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 11:23:51 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Clough", "James R.", ""], ["Byrne", "Nicholas", ""], ["Oksuz", "Ilkay", ""], ["Zimmer", "Veronika A.", ""], ["Schnabel", "Julia A.", ""], ["King", "Andrew P.", ""]]}, {"id": "1910.01902", "submitter": "Gino Gulamhussene", "authors": "Gino Gulamhussene, Fabian Joeres, Marko Rak, Maciej Pech, Christian\n  Hansen", "title": "4D MRI: Robust sorting of free breathing MRI slices for use in\n  interventional settings", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0235175", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: We aim to develop a robust 4D MRI method for large FOVs enabling the\nextraction of irregular respiratory motion that is readily usable with all MRI\nmachines and thus applicable to support a wide range of interventional\nsettings.\n  Method: We propose a 4D MRI reconstruction method to capture an arbitrary\nnumber of breathing states. It uses template updates in navigator slices and\nsearch regions for fast and robust vessel cross-section tracking. It captures\nFOVs of 255 mm x 320 mm x 228 mm at a spatial resolution of 1.82 mm x 1.82 mm x\n4mm and temporal resolution of 200ms. A total of 37 4D MRIs of 13 healthy\nsubjects were reconstructed to validate the method. A quantitative evaluation\nof the reconstruction rate and speed of both the new and baseline method was\nperformed. Additionally, a study with ten radiologists was conducted to assess\nthe subjective reconstruction quality of both methods.\n  Results: Our results indicate improved mean reconstruction rates compared to\nthe baseline method (79.4\\% vs. 45.5\\%) and improved mean reconstruction times\n(24s vs. 73s) per subject. Interventional radiologists perceive the\nreconstruction quality of our method as higher compared to the baseline (262.5\npoints vs. 217.5 points, p=0.02).\n  Conclusions: Template updates are an effective and efficient way to increase\n4D MRI reconstruction rates and to achieve better reconstruction quality.\nSearch regions reduce reconstruction time. These improvements increase the\napplicability of 4D MRI as a base for seamless support of interventional image\nguidance in percutaneous interventions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 12:37:31 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 14:11:50 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Gulamhussene", "Gino", ""], ["Joeres", "Fabian", ""], ["Rak", "Marko", ""], ["Pech", "Maciej", ""], ["Hansen", "Christian", ""]]}, {"id": "1910.01907", "submitter": "Adith Boloor", "authors": "Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yevgeniy\n  Vorobeychik, Xuan Zhang", "title": "Attacking Vision-based Perception in End-to-End Autonomous Driving\n  Models", "comments": "under review in the Journal of Systems Architecture 2019. arXiv admin\n  note: substantial text overlap with arXiv:1903.05157", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in machine learning, especially techniques such as deep\nneural networks, are enabling a range of emerging applications. One such\nexample is autonomous driving, which often relies on deep learning for\nperception. However, deep learning-based perception has been shown to be\nvulnerable to a host of subtle adversarial manipulations of images.\nNevertheless, the vast majority of such demonstrations focus on perception that\nis disembodied from end-to-end control. We present novel end-to-end attacks on\nautonomous driving in simulation, using simple physically realizable attacks:\nthe painting of black lines on the road. These attacks target deep neural\nnetwork models for end-to-end autonomous driving control. A systematic\ninvestigation shows that such attacks are easy to engineer, and we describe\nscenarios (e.g., right turns) in which they are highly effective. We define\nseveral objective functions that quantify the success of an attack and develop\ntechniques based on Bayesian Optimization to efficiently traverse the search\nspace of higher dimensional attacks. Additionally, we define a novel class of\nhijacking attacks, where painted lines on the road cause the driver-less car to\nfollow a target path. Through the use of network deconvolution, we provide\ninsights into the successful attacks, which appear to work by mimicking\nactivations of entirely different scenarios. Our code is available at\nhttps://github.com/xz-group/AdverseDrive\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:56:55 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Boloor", "Adith", ""], ["Garimella", "Karthik", ""], ["He", "Xin", ""], ["Gill", "Christopher", ""], ["Vorobeychik", "Yevgeniy", ""], ["Zhang", "Xuan", ""]]}, {"id": "1910.01911", "submitter": "Junghoon Seo", "authors": "Junghoon Seo, Seungwon Lee, Beomsu Kim, Taegyun Jeon", "title": "Revisiting Classical Bagging with Modern Transfer Learning for\n  On-the-fly Disaster Damage Detector", "comments": "Accepted at the 2019 NeurIPS Workshop on Artificial Intelligence for\n  Humanitarian Assistance and Disaster Response(AI+HADR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic post-disaster damage detection using aerial imagery is crucial for\nquick assessment of damage caused by disaster and development of a recovery\nplan. The main problem preventing us from creating an applicable model in\npractice is that damaged (positive) examples we are trying to detect are much\nharder to obtain than undamaged (negative) examples, especially in short time.\nIn this paper, we revisit the classical bootstrap aggregating approach in the\ncontext of modern transfer learning for data-efficient disaster damage\ndetection. Unlike previous classical ensemble learning articles, our work\npoints out the effectiveness of simple bagging in deep transfer learning that\nhas been underestimated in the context of imbalanced classification. Benchmark\nresults on the AIST Building Change Detection dataset show that our approach\nsignificantly outperforms existing methodologies, including the recently\nproposed disentanglement learning.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 12:47:58 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Seo", "Junghoon", ""], ["Lee", "Seungwon", ""], ["Kim", "Beomsu", ""], ["Jeon", "Taegyun", ""]]}, {"id": "1910.01923", "submitter": "Weijiang Yu", "authors": "Weijiang Yu, Xiaodan Liang, Ke Gong, Chenhan Jiang, Nong Xiao, Liang\n  Lin", "title": "Layout-Graph Reasoning for Fashion Landmark Detection", "comments": "9 pages, 5 figures, CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting dense landmarks for diverse clothes, as a fundamental technique for\nclothes analysis, has attracted increasing research attention due to its huge\napplication potential. However, due to the lack of modeling underlying semantic\nlayout constraints among landmarks, prior works often detect ambiguous and\nstructure-inconsistent landmarks of multiple overlapped clothes in one person.\nIn this paper, we propose to seamlessly enforce structural layout relationships\namong landmarks on the intermediate representations via multiple stacked\nlayout-graph reasoning layers. We define the layout-graph as a hierarchical\nstructure including a root node, body-part nodes (e.g. upper body, lower body),\ncoarse clothes-part nodes (e.g. collar, sleeve) and leaf landmark nodes (e.g.\nleft-collar, right-collar). Each Layout-Graph Reasoning(LGR) layer aims to map\nfeature representations into structural graph nodes via a Map-to-Node module,\nperforms reasoning over structural graph nodes to achieve global layout\ncoherency via a layout-graph reasoning module, and then maps graph nodes back\nto enhance feature representations via a Node-to-Map module. The layout-graph\nreasoning module integrates a graph clustering operation to generate\nrepresentations of intermediate nodes (bottom-up inference) and then a graph\ndeconvolution operation (top-down inference) over the whole graph. Extensive\nexperiments on two public fashion landmark datasets demonstrate the superiority\nof our model. Furthermore, to advance the fine-grained fashion landmark\nresearch for supporting more comprehensive clothes generation and attribute\nrecognition, we contribute the first Fine-grained Fashion Landmark Dataset\n(FFLD) containing 200k images annotated with at most 32 key-points for 13\nclothes types.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 12:59:16 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Yu", "Weijiang", ""], ["Liang", "Xiaodan", ""], ["Gong", "Ke", ""], ["Jiang", "Chenhan", ""], ["Xiao", "Nong", ""], ["Lin", "Liang", ""]]}, {"id": "1910.01933", "submitter": "Pavel Korshunov", "authors": "Pavel Korshunov and S\\'ebastien Marcel", "title": "Vulnerability of Face Recognition to Deep Morphing", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.08685", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly easy to automatically swap faces in images and video or\nmorph two faces into one using generative adversarial networks (GANs). The high\nquality of the resulted deep-morph raises the question of how vulnerable the\ncurrent face recognition systems are to such fake images and videos. It also\ncalls for automated ways to detect these GAN-generated faces. In this paper, we\npresent the publicly available dataset of the Deepfake videos with faces\nmorphed with a GAN-based algorithm. To generate these videos, we used open\nsource software based on GANs, and we emphasize that training and blending\nparameters can significantly impact the quality of the resulted videos. We show\nthat the state of the art face recognition systems based on VGG and Facenet\nneural networks are vulnerable to the deep morph videos, with 85.62 and 95.00\nfalse acceptance rates, respectively, which means methods for detecting these\nvideos are necessary. We consider several baseline approaches for detecting\ndeep morphs and find that the method based on visual quality metrics (often\nused in presentation attack detection domain) leads to the best performance\nwith 8.97 equal error rate. Our experiments demonstrate that GAN-generated deep\nmorph videos are challenging for both face recognition systems and existing\ndetection methods, and the further development of deep morphing technologies\nwill make it even more so.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 12:34:08 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Korshunov", "Pavel", ""], ["Marcel", "S\u00e9bastien", ""]]}, {"id": "1910.01968", "submitter": "Florent Chiaroni", "authors": "Florent Chiaroni, Ghazaleh Khodabandelou, Mohamed-Cherif Rahal,\n  Nicolas Hueber, Frederic Dufaux", "title": "Generating Relevant Counter-Examples from a Positive Unlabeled Dataset\n  for Image Classification", "comments": "Submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With surge of available but unlabeled data, Positive Unlabeled (PU) learning\nis becoming a thriving challenge. This work deals with this demanding task for\nwhich recent GAN-based PU approaches have demonstrated promising results.\nGenerative adversarial Networks (GANs) are not hampered by deterministic bias\nor need for specific dimensionality. However, existing GAN-based PU approaches\nalso present some drawbacks such as sensitive dependence to prior knowledge, a\ncumbersome architecture or first-stage overfitting. To settle these issues, we\npropose to incorporate a biased PU risk within the standard GAN discriminator\nloss function. In this manner, the discriminator is constrained to request the\ngenerator to converge towards the unlabeled samples distribution while\ndiverging from the positive samples distribution. This enables the proposed\nmodel, referred to as D-GAN, to exclusively learn the counter-examples\ndistribution without prior knowledge. Experiments demonstrate that our approach\noutperforms state-of-the-art PU methods without prior by overcoming their\nissues.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 14:33:24 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Chiaroni", "Florent", ""], ["Khodabandelou", "Ghazaleh", ""], ["Rahal", "Mohamed-Cherif", ""], ["Hueber", "Nicolas", ""], ["Dufaux", "Frederic", ""]]}, {"id": "1910.02012", "submitter": "Simone Parisotto Dr", "authors": "Simone Parisotto, Luca Calatroni, Aur\\'elie Bugeau, Nicolas Papadakis\n  and Carola-Bibiane Sch\\\"onlieb", "title": "Variational Osmosis for Non-linear Image Fusion", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new variational model for non-linear image fusion. Our approach\nis based on the use of an osmosis energy term related to the one studied in\nVogel et al. (2013) and Weickert et al. (2013) The minimization of the proposed\nnon-convex energy realizes visually plausible image data fusion, invariant to\nmultiplicative brightness changes. On the practical side, it requires minimal\nsupervision and parameter tuning and can encode prior information on the\nstructure of the images to be fused. For the numerical solution of the proposed\nmodel, we develop a primal-dual algorithm and we apply the resulting\nminimization scheme to solve multi-modal face fusion, color transfer and\ncultural heritage conservation problems. Visual and quantitative comparisons to\nstate-of-the-art approaches prove the out-performance and the flexibility of\nour method.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:08:35 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 18:42:14 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Parisotto", "Simone", ""], ["Calatroni", "Luca", ""], ["Bugeau", "Aur\u00e9lie", ""], ["Papadakis", "Nicolas", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1910.02027", "submitter": "Yunji Kim", "authors": "Yunji Kim, Seonghyeon Nam, In Cho, Seon Joo Kim", "title": "Unsupervised Keypoint Learning for Guiding Class-Conditional Video\n  Prediction", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep video prediction model conditioned on a single image and an\naction class. To generate future frames, we first detect keypoints of a moving\nobject and predict future motion as a sequence of keypoints. The input image is\nthen translated following the predicted keypoints sequence to compose future\nframes. Detecting the keypoints is central to our algorithm, and our method is\ntrained to detect the keypoints of arbitrary objects in an unsupervised manner.\nMoreover, the detected keypoints of the original videos are used as\npseudo-labels to learn the motion of objects. Experimental results show that\nour method is successfully applied to various datasets without the cost of\nlabeling keypoints in videos. The detected keypoints are similar to\nhuman-annotated labels, and prediction results are more realistic compared to\nthe previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:39:25 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Kim", "Yunji", ""], ["Nam", "Seonghyeon", ""], ["Cho", "In", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1910.02029", "submitter": "Arun Balajee Vasudevan", "authors": "Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool", "title": "Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention\n  and Spatial Memory", "comments": "Accepted to IJCV 2020, 20 pages, 10 Figures, Demo Video:\n  https://people.ee.ethz.ch/~arunv/resources/talk2nav.mp4", "journal-ref": null, "doi": "10.1007/s11263-020-01374-3", "report-no": null, "categories": "cs.CV cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of robots in society keeps expanding, bringing with it the necessity\nof interacting and communicating with humans. In order to keep such interaction\nintuitive, we provide automatic wayfinding based on verbal navigational\ninstructions. Our first contribution is the creation of a large-scale dataset\nwith verbal navigation instructions. To this end, we have developed an\ninteractive visual navigation environment based on Google Street View; we\nfurther design an annotation method to highlight mined anchor landmarks and\nlocal directions between them in order to help annotators formulate typical,\nhuman references to those. The annotation task was crowdsourced on the AMT\nplatform, to construct a new Talk2Nav dataset with $10,714$ routes. Our second\ncontribution is a new learning method. Inspired by spatial cognition research\non the mental conceptualization of navigational instructions, we introduce a\nsoft dual attention mechanism defined over the segmented language instructions\nto jointly extract two partial instructions -- one for matching the next\nupcoming visual landmark and the other for matching the local directions to the\nnext landmark. On the similar lines, we also introduce spatial memory scheme to\nencode the local directional transitions. Our work takes advantage of the\nadvance in two lines of research: mental formalization of verbal navigational\ninstructions and training neural network agents for automatic way finding.\nExtensive experiments show that our method significantly outperforms previous\nnavigation methods. For demo video, dataset and code, please refer to our\nproject page: https://www.trace.ethz.ch/publications/2019/talk2nav/index.html\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:44:59 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 11:25:09 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 12:03:18 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Vasudevan", "Arun Balajee", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1910.02034", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Ahmed Farahat, Chetan Gupta", "title": "Generative Adversarial Networks for Failure Prediction", "comments": "ECML PKDD 2019 (The European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prognostics and Health Management (PHM) is an emerging engineering discipline\nwhich is concerned with the analysis and prediction of equipment health and\nperformance. One of the key challenges in PHM is to accurately predict\nimpending failures in the equipment. In recent years, solutions for failure\nprediction have evolved from building complex physical models to the use of\nmachine learning algorithms that leverage the data generated by the equipment.\nHowever, failure prediction problems pose a set of unique challenges that make\ndirect application of traditional classification and prediction algorithms\nimpractical. These challenges include the highly imbalanced training data, the\nextremely high cost of collecting more failure samples, and the complexity of\nthe failure patterns. Traditional oversampling techniques will not be able to\ncapture such complexity and accordingly result in overfitting the training\ndata. This paper addresses these challenges by proposing a novel algorithm for\nfailure prediction using Generative Adversarial Networks (GAN-FP). GAN-FP first\nutilizes two GAN networks to simultaneously generate training samples and build\nan inference network that can be used to predict failures for new samples.\nGAN-FP first adopts an infoGAN to generate realistic failure and non-failure\nsamples, and initialize the weights of the first few layers of the inference\nnetwork. The inference network is then tuned by optimizing a weighted loss\nobjective using only real failure and non-failure samples. The inference\nnetwork is further tuned using a second GAN whose purpose is to guarantee the\nconsistency between the generated samples and corresponding labels. GAN-FP can\nbe used for other imbalanced classification problems as well.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:51:55 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zheng", "Shuai", ""], ["Farahat", "Ahmed", ""], ["Gupta", "Chetan", ""]]}, {"id": "1910.02055", "submitter": "Hang Chu", "authors": "Hang Chu, Daiqing Li, David Acuna, Amlan Kar, Maria Shugrina, Xinkai\n  Wei, Ming-Yu Liu, Antonio Torralba, Sanja Fidler", "title": "Neural Turtle Graphics for Modeling City Road Layouts", "comments": "ICCV-2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Turtle Graphics (NTG), a novel generative model for spatial\ngraphs, and demonstrate its applications in modeling city road layouts.\nSpecifically, we represent the road layout using a graph where nodes in the\ngraph represent control points and edges in the graph represent road segments.\nNTG is a sequential generative model parameterized by a neural network. It\niteratively generates a new node and an edge connecting to an existing node\nconditioned on the current graph. We train NTG on Open Street Map data and show\nthat it outperforms existing approaches using a set of diverse performance\nmetrics. Moreover, our method allows users to control styles of generated road\nlayouts mimicking existing cities as well as to sketch parts of the city road\nlayout to be synthesized. In addition to synthesis, the proposed NTG finds uses\nin an analytical task of aerial road parsing. Experimental results show that it\nachieves state-of-the-art performance on the SpaceNet dataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:30:00 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Chu", "Hang", ""], ["Li", "Daiqing", ""], ["Acuna", "David", ""], ["Kar", "Amlan", ""], ["Shugrina", "Maria", ""], ["Wei", "Xinkai", ""], ["Liu", "Ming-Yu", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "1910.02058", "submitter": "Markus Frey", "authors": "Markus Frey, Matthias Nau", "title": "Memory efficient brain tumor segmentation using an\n  autoencoder-regularized U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Early diagnosis and accurate segmentation of brain tumors are imperative for\nsuccessful treatment. Unfortunately, manual segmentation is time consuming,\ncostly and despite extensive human expertise often inaccurate. Here, we present\nan MRI-based tumor segmentation framework using an autoencoder-regularized\n3D-convolutional neural network. We trained the model on manually segmented\nstructural T1, T1ce, T2, and Flair MRI images of 335 patients with tumors of\nvariable severity, size and location. We then tested the model using\nindependent data of 125 patients and successfully segmented brain tumors into\nthree subregions: the tumor core (TC), the enhancing tumor (ET) and the whole\ntumor (WT). We also explored several data augmentations and preprocessing steps\nto improve segmentation performance. Importantly, our model was implemented on\na single NVIDIA GTX1060 graphics unit and hence optimizes tumor segmentation\nfor widely affordable hardware. In sum, we present a memory-efficient and\naffordable solution to tumor segmentation to support the accurate diagnostics\nof oncological brain pathologies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:33:07 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Frey", "Markus", ""], ["Nau", "Matthias", ""]]}, {"id": "1910.02060", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Vladimir G. Kim, Eli Shechtman, Jun Saito, Serge\n  Belongie", "title": "Neural Puppet: Generative Layered Cartoon Characters", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning based method for generating new animations of a cartoon\ncharacter given a few example images. Our method is designed to learn from a\ntraditionally animated sequence, where each frame is drawn by an artist, and\nthus the input images lack any common structure, correspondences, or labels. We\nexpress pose changes as a deformation of a layered 2.5D template mesh, and\ndevise a novel architecture that learns to predict mesh deformations matching\nthe template to a target image. This enables us to extract a common\nlow-dimensional structure from a diverse set of character poses. We combine\nrecent advances in differentiable rendering as well as mesh-aware models to\nsuccessfully align common template even if only a few character images are\navailable during training. In addition to coarse poses, character appearance\nalso varies due to shading, out-of-plane motions, and artistic effects. We\ncapture these subtle changes by applying an image translation network to refine\nthe mesh rendering, providing an end-to-end model to generate new animations of\na character with high visual quality. We demonstrate that our generative model\ncan be used to synthesize in-between frames and to create data-driven\ndeformation. Our template fitting procedure outperforms state-of-the-art\ngeneric techniques for detecting image correspondences.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:40:51 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 22:07:00 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 23:54:09 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Poursaeed", "Omid", ""], ["Kim", "Vladimir G.", ""], ["Shechtman", "Eli", ""], ["Saito", "Jun", ""], ["Belongie", "Serge", ""]]}, {"id": "1910.02066", "submitter": "Selim Engin", "authors": "Selim Engin, Eric Mitchell, Daewon Lee, Volkan Isler, Daniel D. Lee", "title": "Higher Order Function Networks for View Planning and Multi-View\n  Reconstruction", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of planning views for a robot to acquire images of an\nobject for visual inspection and reconstruction. In contrast to offline methods\nwhich require a 3D model of the object as input or online methods which rely on\nonly local measurements, our method uses a neural network which encodes shape\ninformation for a large number of objects. We build on recent deep learning\nmethods capable of generating a complete 3D reconstruction of an object from a\nsingle image. Specifically, in this work, we extend a recent method which uses\nHigher Order Functions (HOF) to represent the shape of the object. We present a\nnew generalization of this method to incorporate multiple images as input and\nestablish a connection between visibility and reconstruction quality. This\nrelationship forms the foundation of our view planning method where we compute\nviewpoints to visually cover the output of the multi-view HOF network with as\nfew images as possible. Experiments indicate that our method provides a good\ncompromise between online and offline methods: Similar to online methods, our\nmethod does not require the true object model as input. In terms of number of\nviews, it is much more efficient. In most cases, its performance is comparable\nto the optimal offline case even on object classes the network has not been\ntrained on.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:46:13 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Engin", "Selim", ""], ["Mitchell", "Eric", ""], ["Lee", "Daewon", ""], ["Isler", "Volkan", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1910.02101", "submitter": "Wenju Xu", "authors": "Wenju Xu and Yuanwei Wu and Wenchi Ma and Guanghui Wang", "title": "Adaptively Denoising Proposal Collection for Weakly Supervised Object\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of weakly supervised object\nlocalization (WSL), which trains a detection network on the dataset with only\nimage-level annotations. The proposed approach is built on the observation that\nthe proposal set from the training dataset is a collection of background,\nobject parts, and objects. Several strategies are taken to adaptively eliminate\nthe noisy proposals and generate pseudo object-level annotations for the weakly\nlabeled dataset. A multiple instance learning (MIL) algorithm enhanced by\nmask-out strategy is adopted to collect the class-specific object proposals,\nwhich are then utilized to adapt a pre-trained classification network to a\ndetection network. In addition, the detection results from the detection\nnetwork are re-weighted by jointly considering the detection scores and the\noverlap ratio of proposals in a proposal subset optimization framework. The\noptimal proposals work as object-level labels that enable a pseudo-strongly\nsupervised dataset for training the detection network. Consequently, we\nestablish a fully adaptive detection network. Extensive evaluations on the\nPASCAL VOC 2007 and 2012 datasets demonstrate a significant improvement\ncompared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 18:42:32 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 21:12:53 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xu", "Wenju", ""], ["Wu", "Yuanwei", ""], ["Ma", "Wenchi", ""], ["Wang", "Guanghui", ""]]}, {"id": "1910.02106", "submitter": "Wenju Xu", "authors": "Wenju Xu and Dongkyu Choi and Guanghui Wang", "title": "Direct Visual-Inertial Odometry with Semi-Dense Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a direct visual-inertial odometry system. In particular, a\ntightly coupled nonlinear optimization based method is proposed by integrating\nthe recent advances in direct dense tracking and Inertial Measurement Unit\n(IMU) pre-integration, and a factor graph optimization is adapted to estimate\nthe pose of the camera and rebuild a semi-dense map. Two sliding windows are\nmaintained in the proposed approach. The first one, based on Direct Sparse\nOdometry (DSO), is to estimate the depths of candidate points for mapping and\ndense visual tracking. In the second one, measurements from the IMU\npre-integration and dense visual tracking are fused probabilistically using a\ntightly-coupled, optimization-based sensor fusion framework. As a result, the\nIMU pre-integration provides additional constraints to suppress the scale drift\ninduced by the visual odometry. Evaluations on real-world benchmark datasets\nshow that the proposed method achieves competitive results in indoor scenes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 18:49:22 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Xu", "Wenju", ""], ["Choi", "Dongkyu", ""], ["Wang", "Guanghui", ""]]}, {"id": "1910.02150", "submitter": "Stefan Klus", "authors": "Stefan Klus, Patrick Gel{\\ss}", "title": "Tensor-based algorithms for image classification", "comments": null, "journal-ref": "Algorithms, 12(11), 240, 2019", "doi": "10.3390/a12110240", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest in machine learning with tensor networks has been growing\nrapidly in recent years. We show that tensor-based methods developed for\nlearning the governing equations of dynamical systems from data can, in the\nsame way, be used for supervised learning problems and propose two novel\napproaches for image classification. One is a kernel-based reformulation of the\npreviously introduced MANDy (multidimensional approximation of nonlinear\ndynamics), the other an alternating ridge regression in the tensor-train\nformat. We apply both methods to the MNIST and fashion MNIST data set and show\nthat the approaches are competitive with state-of-the-art neural network-based\nclassifiers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 21:16:33 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 22:31:02 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Klus", "Stefan", ""], ["Gel\u00df", "Patrick", ""]]}, {"id": "1910.02165", "submitter": "Sriramya Bhamidipati", "authors": "Sriramya Bhamidipati, Grace Xingxin Gao", "title": "SLAM-based Integrity Monitoring Using GPS and Fish-eye Camera", "comments": "32nd International Technical Meeting of the Satellite Division of the\n  Institute of Navigation, ION GNSS+ 2019, Miami, FL, Sept 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban navigation using GPS and fish-eye camera suffers from multipath effects\nin GPS measurements and data association errors in pixel intensities across\nimage frames. We propose a Simultaneous Localization and Mapping (SLAM)-based\nIntegrity Monitoring (IM) algorithm to compute the position protection levels\nwhile accounting for multiple faults in both GPS and vision. We perform graph\noptimization using the sequential data of GPS pseudoranges, pixel intensities,\nvehicle dynamics, and satellite ephemeris to simultaneously localize the\nvehicle as well as the landmarks, namely GPS satellites and key image pixels in\nthe world frame. We estimate the fault mode vector by analyzing the temporal\ncorrelation across the GPS measurement residuals and spatial correlation across\nthe vision intensity residuals. In particular, to detect and isolate the vision\nfaults, we developed a superpixel-based piecewise Random Sample Consensus\n(RANSAC) technique to perform spatial voting across image pixels. For an\nestimated fault mode, we compute the protection levels by applying worst-case\nfailure slope analysis to the linearized Graph-SLAM framework. We perform\nground vehicle experiments in the semi-urban area of Champaign, IL and have\ndemonstrated the successful detection and isolation of multiple faults. We also\nvalidate tighter protection levels and lower localization errors achieved via\nthe proposed algorithm as compared to SLAM-based IM that utilizes only GPS\nmeasurements.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 23:05:52 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Bhamidipati", "Sriramya", ""], ["Gao", "Grace Xingxin", ""]]}, {"id": "1910.02175", "submitter": "Deepta Rajan", "authors": "Deepta Rajan, David Beymer, Shafiqul Abedin and Ehsan Dehghan", "title": "Pi-PE: A Pipeline for Pulmonary Embolism Detection using Sparsely\n  Annotated 3D CT Images", "comments": "2019 NeurIPS ML4H (Proceedings of Machine Learning Research)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary embolisms (PE) are known to be one of the leading causes for\ncardiac-related mortality. Due to inherent variabilities in how PE manifests\nand the cumbersome nature of manual diagnosis, there is growing interest in\nleveraging AI tools for detecting PE. In this paper, we build a two-stage\ndetection pipeline that is accurate, computationally efficient, robust to\nvariations in PE types and kernels used for CT reconstruction, and most\nimportantly, does not require dense annotations. Given the challenges in\nacquiring expert annotations in large-scale datasets, our approach produces\nstate-of-the-art results with very sparse emboli contours (at 10mm slice\nspacing), while using models with significantly lower number of parameters. We\nachieve AUC scores of 0.94 on the validation set and 0.85 on the test set of\nhighly severe PEs. Using a large, real-world dataset characterized by complex\nPE types and patients from multiple hospitals, we present an elaborate\nempirical study and provide guidelines for designing highly generalizable\npipelines.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 00:01:28 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 16:37:22 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 20:56:02 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Rajan", "Deepta", ""], ["Beymer", "David", ""], ["Abedin", "Shafiqul", ""], ["Dehghan", "Ehsan", ""]]}, {"id": "1910.02181", "submitter": "Chaitanya Ahuja", "authors": "Chaitanya Ahuja, Shugao Ma, Louis-Philippe Morency, Yaser Sheikh", "title": "To React or not to React: End-to-End Visual Pose Forecasting for\n  Personalized Avatar during Dyadic Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non verbal behaviours such as gestures, facial expressions, body posture, and\npara-linguistic cues have been shown to complement or clarify verbal messages.\nHence to improve telepresence, in form of an avatar, it is important to model\nthese behaviours, especially in dyadic interactions. Creating such personalized\navatars not only requires to model intrapersonal dynamics between a avatar's\nspeech and their body pose, but it also needs to model interpersonal dynamics\nwith the interlocutor present in the conversation. In this paper, we introduce\na neural architecture named Dyadic Residual-Attention Model (DRAM), which\nintegrates intrapersonal (monadic) and interpersonal (dyadic) dynamics using\nselective attention to generate sequences of body pose conditioned on audio and\nbody pose of the interlocutor and audio of the human operating the avatar. We\nevaluate our proposed model on dyadic conversational data consisting of pose\nand audio of both participants, confirming the importance of adaptive attention\nbetween monadic and dyadic dynamics when predicting avatar pose. We also\nconduct a user study to analyze judgments of human observers. Our results\nconfirm that the generated body pose is more natural, models intrapersonal\ndynamics and interpersonal dynamics better than non-adaptive monadic/dyadic\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 00:19:36 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Ahuja", "Chaitanya", ""], ["Ma", "Shugao", ""], ["Morency", "Louis-Philippe", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1910.02190", "submitter": "Dmytro Mishkin", "authors": "Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee, Gary Bradski", "title": "Kornia: an Open Source Differentiable Computer Vision Library for\n  PyTorch", "comments": "Updated adversarial attack example", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents Kornia -- an open source computer vision library which\nconsists of a set of differentiable routines and modules to solve generic\ncomputer vision problems. The package uses PyTorch as its main backend both for\nefficiency and to take advantage of the reverse-mode auto-differentiation to\ndefine and compute the gradient of complex functions. Inspired by OpenCV,\nKornia is composed of a set of modules containing operators that can be\ninserted inside neural networks to train models to perform image\ntransformations, camera calibration, epipolar geometry, and low level image\nprocessing techniques, such as filtering and edge detection that operate\ndirectly on high dimensional tensor representations. Examples of classical\nvision problems implemented using our framework are provided including a\nbenchmark comparing to existing vision libraries.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 01:29:54 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 11:55:48 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Riba", "Edgar", ""], ["Mishkin", "Dmytro", ""], ["Ponsa", "Daniel", ""], ["Rublee", "Ethan", ""], ["Bradski", "Gary", ""]]}, {"id": "1910.02192", "submitter": "Fania Mokhayeri", "authors": "Fania Mokhayeri, Eric Granger", "title": "A Paired Sparse Representation Model for Robust Face Recognition from a\n  Single Sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sparse representation-based classification (SRC) has been shown to achieve a\nhigh level of accuracy in face recognition (FR). However, matching faces\ncaptured in unconstrained video against a gallery with a single reference\nfacial still per individual typically yields low accuracy. For improved\nrobustness to intra-class variations, SRC techniques for FR have recently been\nextended to incorporate variational information from an external generic set\ninto an auxiliary dictionary. Despite their success in handling linear\nvariations, non-linear variations (e.g., pose and expressions) between probe\nand reference facial images cannot be accurately reconstructed with a linear\ncombination of images in the gallery and auxiliary dictionaries because they do\nnot share the same type of variations. In order to account for non-linear\nvariations due to pose, a paired sparse representation model is introduced\nallowing for joint use of variational information and synthetic face images.\nThe proposed model, called synthetic plus variational model, reconstructs a\nprobe image by jointly using (1) a variational dictionary and (2) a gallery\ndictionary augmented with a set of synthetic images generated over a wide\ndiversity of pose angles. The augmented gallery dictionary is then encouraged\nto pair the same sparsity pattern with the variational dictionary for similar\npose angles by solving a newly formulated simultaneous sparsity-based\noptimization problem. Experimental results obtained on Chokepoint and COX-S2V\ndatasets, using different face representations, indicate that the proposed\napproach can outperform state-of-the-art SRC-based methods for still-to-video\nFR with a single sample per person.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 01:58:45 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Mokhayeri", "Fania", ""], ["Granger", "Eric", ""]]}, {"id": "1910.02201", "submitter": "Jun Miura", "authors": "Motoki Kojima and Jun Miura", "title": "Early Estimation of User's Intention of Tele-Operation Using Object\n  Affordance and Hand Motion in a Dual First-Person Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method of estimating the intention of a user's motion\nin a robot tele-operation scenario. One of the issues in tele-operation is\nlatency, which occurs due to various reasons such as a slow robot motion and a\nnarrow communication channel. An effective way of reducing the latency is to\nestimate the human intention of motions and to move the robot proactively. To\nenable a reliable early intention estimation, we use both hand motion and\nobject affordances in a dual first-person vision (robot and user) with an HMD.\nExperimental results in an object pickup scenario show the effectiveness of the\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 03:21:54 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Kojima", "Motoki", ""], ["Miura", "Jun", ""]]}, {"id": "1910.02206", "submitter": "Xingjian Zhen", "authors": "Xingjian Zhen, Rudrasis Chakraborty, Nicholas Vogt, Barbara B.\n  Bendlin, Vikas Singh", "title": "Dilated Convolutional Neural Networks for Sequential Manifold-valued\n  Data", "comments": null, "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efforts are underway to study ways via which the power of deep neural\nnetworks can be extended to non-standard data types such as structured data\n(e.g., graphs) or manifold-valued data (e.g., unit vectors or special\nmatrices). Often, sizable empirical improvements are possible when the geometry\nof such data spaces are incorporated into the design of the model,\narchitecture, and the algorithms. Motivated by neuroimaging applications, we\nstudy formulations where the data are {\\em sequential manifold-valued\nmeasurements}. This case is common in brain imaging, where the samples\ncorrespond to symmetric positive definite matrices or orientation distribution\nfunctions. Instead of a recurrent model which poses computational/technical\nissues, and inspired by recent results showing the viability of dilated\nconvolutional models for sequence prediction, we develop a dilated\nconvolutional neural network architecture for this task. On the technical side,\nwe show how the modules needed in our network can be derived while explicitly\ntaking the Riemannian manifold structure into account. We show how the\noperations needed can leverage known results for calculating the weighted\nFr\\'{e}chet Mean (wFM). Finally, we present scientific results for group\ndifference analysis in Alzheimer's disease (AD) where the groups are derived\nusing AD pathology load: here the model finds several brain fiber bundles that\nare related to AD even when the subjects are all still cognitively healthy.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 04:09:37 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Zhen", "Xingjian", ""], ["Chakraborty", "Rudrasis", ""], ["Vogt", "Nicholas", ""], ["Bendlin", "Barbara B.", ""], ["Singh", "Vikas", ""]]}, {"id": "1910.02212", "submitter": "Maosen Li", "authors": "Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng Wang, Qi Tian", "title": "Symbiotic Graph Neural Networks for 3D Skeleton-based Human Action\n  Recognition and Motion Prediction", "comments": "submitted to IEEE-TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D skeleton-based action recognition and motion prediction are two essential\nproblems of human activity understanding. In many previous works: 1) they\nstudied two tasks separately, neglecting internal correlations; 2) they did not\ncapture sufficient relations inside the body. To address these issues, we\npropose a symbiotic model to handle two tasks jointly; and we propose two\nscales of graphs to explicitly capture relations among body-joints and\nbody-parts. Together, we propose symbiotic graph neural networks, which contain\na backbone, an action-recognition head, and a motion-prediction head. Two heads\nare trained jointly and enhance each other. For the backbone, we propose\nmulti-branch multi-scale graph convolution networks to extract spatial and\ntemporal features. The multi-scale graph convolution networks are based on\njoint-scale and part-scale graphs. The joint-scale graphs contain actional\ngraphs, capturing action-based relations, and structural graphs, capturing\nphysical constraints. The part-scale graphs integrate body-joints to form\nspecific parts, representing high-level relations. Moreover, dual bone-based\ngraphs and networks are proposed to learn complementary features. We conduct\nextensive experiments for skeleton-based action recognition and motion\nprediction with four datasets, NTU-RGB+D, Kinetics, Human3.6M, and CMU Mocap.\nExperiments show that our symbiotic graph neural networks achieve better\nperformances on both tasks compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 05:29:03 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Li", "Maosen", ""], ["Chen", "Siheng", ""], ["Chen", "Xu", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "1910.02222", "submitter": "Jamal Ahmed Rahim", "authors": "Jamal Ahmed Rahim, Kwan-Yee Kenneth Wong", "title": "Colored Transparent Object Matting from a Single Image Using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper proposes a deep learning based method for colored transparent\nobject matting from a single image. Existing approaches for transparent object\nmatting often require multiple images and long processing times, which greatly\nhinder their applications on real-world transparent objects. The recently\nproposed TOM-Net can produce a matte for a colorless transparent object from a\nsingle image in a single fast feed-forward pass. In this paper, we extend\nTOM-Net to handle colored transparent object by modeling the intrinsic color of\na transparent object with a color filter. We formulate the problem of colored\ntransparent object matting as simultaneously estimating an object mask, a color\nfilter, and a refractive flow field from a single image, and present a deep\nlearning framework for learning this task. We create a large-scale synthetic\ndataset for training our network. We also capture a real dataset for\nevaluation. Experiments on both synthetic and real datasets show promising\nresults, which demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 06:45:33 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Rahim", "Jamal Ahmed", ""], ["Wong", "Kwan-Yee Kenneth", ""]]}, {"id": "1910.02224", "submitter": "Limeng Qiao", "authors": "Limeng Qiao, Yemin Shi, Jia Li, Yaowei Wang, Tiejun Huang, Yonghong\n  Tian", "title": "Transductive Episodic-Wise Adaptive Metric for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning, which aims at extracting new concepts rapidly from\nextremely few examples of novel classes, has been featured into the\nmeta-learning paradigm recently. Yet, the key challenge of how to learn a\ngeneralizable classifier with the capability of adapting to specific tasks with\nseverely limited data still remains in this domain. To this end, we propose a\nTransductive Episodic-wise Adaptive Metric (TEAM) framework for few-shot\nlearning, by integrating the meta-learning paradigm with both deep metric\nlearning and transductive inference. With exploring the pairwise constraints\nand regularization prior within each task, we explicitly formulate the\nadaptation procedure into a standard semi-definite programming problem. By\nsolving the problem with its closed-form solution on the fly with the setup of\ntransduction, our approach efficiently tailors an episodic-wise metric for each\ntask to adapt all features from a shared task-agnostic embedding space into a\nmore discriminative task-specific metric space. Moreover, we further leverage\nan attention-based bi-directional similarity strategy for extracting the more\nrobust relationship between queries and prototypes. Extensive experiments on\nthree benchmark datasets show that our framework is superior to other existing\napproaches and achieves the state-of-the-art performance in the few-shot\nliterature.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 07:07:53 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Qiao", "Limeng", ""], ["Shi", "Yemin", ""], ["Li", "Jia", ""], ["Wang", "Yaowei", ""], ["Huang", "Tiejun", ""], ["Tian", "Yonghong", ""]]}, {"id": "1910.02235", "submitter": "Yao Zhang", "authors": "Yao Zhang, Yixin Wang, Feng Hou, Jiawei Yang, Guangwei Xiong, Jiang\n  Tian, Cheng Zhong", "title": "Cascaded Volumetric Convolutional Network for Kidney Tumor Segmentation\n  from CT volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation of kidney and tumor from 3D CT scans is necessary for\nthe diagnosis, monitoring, and treatment planning of the disease. In this\npaper, we describe a two-stage framework for kidney and tumor segmentation\nbased on 3D fully convolutional network (FCN). The first stage preliminarily\nlocate the kidney and cut off the irrelevant background to reduce class\nimbalance and computation cost. Then the second stage precisely segment the\nkidney and tumor on the cropped patch. The proposed method ranks the 4th place\nout of 105 competitive teams in MICCAI 2019 KiTS Challenge with a Composite\nDice of 90.24%.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 08:44:25 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 09:22:58 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhang", "Yao", ""], ["Wang", "Yixin", ""], ["Hou", "Feng", ""], ["Yang", "Jiawei", ""], ["Xiong", "Guangwei", ""], ["Tian", "Jiang", ""], ["Zhong", "Cheng", ""]]}, {"id": "1910.02241", "submitter": "Xinrui Zhuang", "authors": "Xinrui Zhuang, Yuexiang Li, Yifan Hu, Kai Ma, Yujiu Yang, Yefeng Zheng", "title": "Self-supervised Feature Learning for 3D Medical Images by Playing a\n  Rubik's Cube", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Witnessed the development of deep learning, increasing number of studies try\nto build computer aided diagnosis systems for 3D volumetric medical data.\nHowever, as the annotations of 3D medical data are difficult to acquire, the\nnumber of annotated 3D medical images is often not enough to well train the\ndeep learning networks. The self-supervised learning deeply exploiting the\ninformation of raw data is one of the potential solutions to loose the\nrequirement of training data. In this paper, we propose a self-supervised\nlearning framework for the volumetric medical images. A novel proxy task, i.e.,\nRubik's cube recovery, is formulated to pre-train 3D neural networks. The proxy\ntask involves two operations, i.e., cube rearrangement and cube rotation, which\nenforce networks to learn translational and rotational invariant features from\nraw 3D data. Compared to the train-from-scratch strategy, fine-tuning from the\npre-trained network leads to a better accuracy on various tasks, e.g., brain\nhemorrhage classification and brain tumor segmentation. We show that our\nself-supervised learning approach can substantially boost the accuracies of 3D\ndeep learning networks on the volumetric medical datasets without using extra\ndata. To our best knowledge, this is the first work focusing on the\nself-supervised learning of 3D neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 10:02:13 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Zhuang", "Xinrui", ""], ["Li", "Yuexiang", ""], ["Hu", "Yifan", ""], ["Ma", "Kai", ""], ["Yang", "Yujiu", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1910.02244", "submitter": "Laurent Meunier", "authors": "Laurent Meunier, Jamal Atif, Olivier Teytaud", "title": "Yet another but more efficient black-box adversarial attack: tiling and\n  evolution strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new black-box attack achieving state of the art performances.\nOur approach is based on a new objective function, borrowing ideas from\n$\\ell_\\infty$-white box attacks, and particularly designed to fit\nderivative-free optimization requirements. It only requires to have access to\nthe logits of the classifier without any other information which is a more\nrealistic scenario. Not only we introduce a new objective function, we extend\nprevious works on black box adversarial attacks to a larger spectrum of\nevolution strategies and other derivative-free optimization methods. We also\nhighlight a new intriguing property that deep neural networks are not robust to\nsingle shot tiled attacks. Our models achieve, with a budget limited to\n$10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3\nclassifier with $630$ queries to the network on average in the untargeted\nattacks setting, which is an improvement by $90$ queries of the current state\nof the art. In the targeted setting, we are able to reach, with a limited\nbudget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries\non average, i.e. we need $800$ queries less than the current state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 10:36:47 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 10:48:51 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Meunier", "Laurent", ""], ["Atif", "Jamal", ""], ["Teytaud", "Olivier", ""]]}, {"id": "1910.02258", "submitter": "Amirhossein Kardoost", "authors": "Amirhossein Kardoost, Sabine M\\\"uller, Joachim Weickert, Margret\n  Keuper", "title": "Object Segmentation Tracking from Generic Video Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a light-weight variational framework for online tracking of object\nsegmentations in videos based on optical flow and image boundaries. While\nhigh-end computer vision methods on this task rely on sequence specific\ntraining of dedicated CNN architectures, we show the potential of a variational\nmodel, based on generic video information from motion and color. Such cues are\nusually required for tasks such as robot navigation or grasp estimation. We\nleverage them directly for video object segmentation and thus provide accurate\nsegmentations at potentially very low extra cost. Our simple method can provide\ncompetitive results compared to the costly CNN-based methods with parameter\ntuning. Furthermore, we show that our approach can be combined with\nstate-of-the-art CNN-based segmentations in order to improve over their\nrespective results. We evaluate our method on the datasets DAVIS 16,17 and\nSegTrack v2.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 12:47:23 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 12:49:08 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 17:22:19 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Kardoost", "Amirhossein", ""], ["M\u00fcller", "Sabine", ""], ["Weickert", "Joachim", ""], ["Keuper", "Margret", ""]]}, {"id": "1910.02285", "submitter": "Xukun Li", "authors": "Wei Wu, Xukun Li, Peng Du, Guanjing Lang, Min Xu, Kaijin Xu, Lanjuan\n  Li", "title": "A Deep Learning System That Generates Quantitative CT Reports for\n  Diagnosing Pulmonary Tuberculosis", "comments": null, "journal-ref": null, "doi": "10.1007/s10489-020-02051-1", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a deep learning model-based system to automatically generate a\nquantitative Computed Tomography (CT) diagnostic report for Pulmonary\nTuberculosis (PTB) cases.501 CT imaging datasets from 223 patients with active\nPTB were collected, and another 501 cases from a healthy population served as\nnegative samples.2884 lesions of PTB were carefully labeled and classified\nmanually by professional radiologists.Three state-of-the-art 3D convolution\nneural network (CNN) models were trained and evaluated in the inspection of PTB\nCT images. Transfer learning method was also utilized during this process. The\nbest model was selected to annotate the spatial location of lesions and\nclassify them into miliary, infiltrative, caseous, tuberculoma and cavitary\ntypes simultaneously.Then the Noisy-Or Bayesian function was used to generate\nan overall infection probability.Finally, a quantitative diagnostic report was\nexported.The results showed that the recall and precision rates, from the\nperspective of a single lesion region of PTB, were 85.9% and 89.2%\nrespectively. The overall recall and precision rates,from the perspective of\none PTB case, were 98.7% and 93.7%, respectively. Moreover, the precision rate\nof the PTB lesion type classification was 90.9%.The new method might serve as\nan effective reference for decision making by clinical doctors.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 15:55:25 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Wu", "Wei", ""], ["Li", "Xukun", ""], ["Du", "Peng", ""], ["Lang", "Guanjing", ""], ["Xu", "Min", ""], ["Xu", "Kaijin", ""], ["Li", "Lanjuan", ""]]}, {"id": "1910.02312", "submitter": "Vivek Sharma", "authors": "Vivek Sharma, Praneeth Vepakomma, Tristan Swedish, Ken Chang,\n  Jayashree Kalpathy-Cramer, Ramesh Raskar", "title": "ExpertMatcher: Automating ML Model Selection for Users in Resource\n  Constrained Countries", "comments": "In NeurIPS Workshop on Machine learning for the Developing World\n  (ML4D)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we introduce ExpertMatcher, a method for automating deep\nlearning model selection using autoencoders. Specifically, we are interested in\nperforming inference on data sources that are distributed across many clients\nusing pretrained expert ML networks on a centralized server. The ExpertMatcher\nassigns the most relevant model(s) in the central server given the client's\ndata representation. This allows resource-constrained clients in developing\ncountries to utilize the most relevant ML models for their given task without\nhaving to evaluate the performance of each ML model. The method is generic and\ncan be beneficial in any setup where there are local clients and numerous\ncentralized expert ML models.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 18:58:59 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Sharma", "Vivek", ""], ["Vepakomma", "Praneeth", ""], ["Swedish", "Tristan", ""], ["Chang", "Ken", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1910.02319", "submitter": "Artur Jordao", "authors": "Artur Jordao, Maiko Lie, Victor Hugo Cunha de Melo and William Robson\n  Schwartz", "title": "Covariance-free Partial Least Squares: An Incremental Dimensionality\n  Reduction Method", "comments": "Accepted for publication at Winter Conference on Applications of\n  Computer Vision (WACV) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction plays an important role in computer vision problems\nsince it reduces computational cost and is often capable of yielding more\ndiscriminative data representation. In this context, Partial Least Squares\n(PLS) has presented notable results in tasks such as image classification and\nneural network optimization. However, PLS is infeasible on large datasets, such\nas ImageNet, because it requires all the data to be in memory in advance, which\nis often impractical due to hardware limitations. Additionally, this\nrequirement prevents us from employing PLS on streaming applications where the\ndata are being continuously generated. Motivated by this, we propose a novel\nincremental PLS, named Covariance-free Incremental Partial Least Squares\n(CIPLS), which learns a low-dimensional representation of the data using a\nsingle sample at a time. In contrast to other state-of-the-art approaches,\ninstead of adopting a partially-discriminative or SGD-based model, we extend\nNonlinear Iterative Partial Least Squares (NIPALS) -- the standard algorithm\nused to compute PLS -- for incremental processing. Among the advantages of this\napproach are the preservation of discriminative information across all\ncomponents, the possibility of employing its score matrices for feature\nselection, and its computational efficiency. We validate CIPLS on face\nverification and image classification tasks, where it outperforms several other\nincremental dimensionality reduction techniques. In the context of feature\nselection, CIPLS achieves comparable results when compared to state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 19:45:50 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 14:23:13 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Jordao", "Artur", ""], ["Lie", "Maiko", ""], ["de Melo", "Victor Hugo Cunha", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1910.02334", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Benet Oriol Sabat, Cristian Canton Ferrer, Xavier Giro-i-Nieto", "title": "Hate Speech in Pixels: Detection of Offensive Memes towards Automatic\n  Moderation", "comments": "AI for Social Good Workshop at NeurIPS 2019 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the challenge of hate speech detection in Internet memes,\nand attempts using visual information to automatically detect hate speech,\nunlike any previous work of our knowledge. Memes are pixel-based multimedia\ndocuments that contain photos or illustrations together with phrases which,\nwhen combined, usually adopt a funny meaning. However, hate memes are also used\nto spread hate through social networks, so their automatic detection would help\nreduce their harmful societal impact. Our results indicate that the model can\nlearn to detect some of the memes, but that the task is far from being solved\nwith this simple architecture. While previous work focuses on linguistic hate\nspeech, our experiments indicate how the visual modality can be much more\ninformative for hate speech detection than the linguistic one in memes. In our\nexperiments, we built a dataset of 5,020 memes to train and evaluate a\nmulti-layer perceptron over the visual and language representations, whether\nindependently or fused. The source code and mode and models are available\nhttps://github.com/imatge-upc/hate-speech-detection .\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 22:05:43 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Sabat", "Benet Oriol", ""], ["Ferrer", "Cristian Canton", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1910.02354", "submitter": "Guangyu Shen", "authors": "Guangyu Shen, Chengzhi Mao, Junfeng Yang, Baishakhi Ray", "title": "AdvSPADE: Realistic Unrestricted Attacks for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the inherent robustness of segmentation models, traditional\nnorm-bounded attack methods show limited effect on such type of models. In this\npaper, we focus on generating unrestricted adversarial examples for semantic\nsegmentation models. We demonstrate a simple and effective method to generate\nunrestricted adversarial examples using conditional generative adversarial\nnetworks (CGAN) without any hand-crafted metric. The na\\\"ive implementation of\nCGAN, however, yields inferior image quality and low attack success rate.\nInstead, we leverage the SPADE (Spatially-adaptive denormalization) structure\nwith an additional loss item to generate effective adversarial attacks in a\nsingle step. We validate our approach on the popular Cityscapes and ADE20K\ndatasets, and demonstrate that our synthetic adversarial examples are not only\nrealistic, but also improve the attack success rate by up to 41.0\\% compared\nwith the state of the art adversarial attack methods including PGD.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 02:16:18 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 19:53:09 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 02:38:18 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Shen", "Guangyu", ""], ["Mao", "Chengzhi", ""], ["Yang", "Junfeng", ""], ["Ray", "Baishakhi", ""]]}, {"id": "1910.02411", "submitter": "Terence Broad", "authors": "Terence Broad, Mick Grierson", "title": "Transforming the output of GANs by fine-tuning them with features from\n  different datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we present a method for fine-tuning pre-trained GANs with\nfeatures from different datasets, resulting in the transformation of the output\ndistribution into a new distribution with novel characteristics. The weights of\nthe generator are updated using the weighted sum of the losses from a\ncross-dataset classifier and the frozen weights of the pre-trained\ndiscriminator. We discuss details of the technical implementation and share\nsome of the visual results from this training process.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 10:29:11 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Broad", "Terence", ""], ["Grierson", "Mick", ""]]}, {"id": "1910.02420", "submitter": "Essam Rashed", "authors": "Essam A. Rashed, Jose Gomez-Tames, Akimasa Hirata", "title": "Deep learning-based development of personalized human head model with\n  non-uniform conductivity for brain stimulation", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, 2020", "doi": "10.1109/TMI.2020.2969682", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electromagnetic stimulation of the human brain is a key tool for the\nneurophysiological characterization and diagnosis of several neurological\ndisorders. Transcranial magnetic stimulation (TMS) is one procedure that is\ncommonly used clinically. However, personalized TMS requires a pipeline for\naccurate head model generation to provide target-specific stimulation. This\nprocess includes intensive segmentation of several head tissues based on\nmagnetic resonance imaging (MRI), which has significant potential for\nsegmentation error, especially for low-contrast tissues. Additionally, a\nuniform electrical conductivity is assigned to each tissue in the model, which\nis an unrealistic assumption based on conventional volume conductor modeling.\nThis paper proposes a novel approach to the automatic estimation of electric\nconductivity in the human head for volume conductor models without anatomical\nsegmentation. A convolutional neural network is designed to estimate\npersonalized electrical conductivity values based on anatomical information\nobtained from T1- and T2-weighted MRI scans. This approach can avoid the\ntime-consuming process of tissue segmentation and maximize the advantages of\nposition-dependent conductivity assignment based on water content values\nestimated from MRI intensity values. The computational results of the proposed\napproach provide similar but smoother electric field results for the brain when\ncompared to conventional approaches.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 11:33:13 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 06:00:57 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Rashed", "Essam A.", ""], ["Gomez-Tames", "Jose", ""], ["Hirata", "Akimasa", ""]]}, {"id": "1910.02425", "submitter": "Jannik Kossen", "authors": "Jannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, Kristian\n  Kersting", "title": "Structured Object-Aware Physics Prediction for Video Modeling and\n  Planning", "comments": "Published as a conference paper at 2020 International Conference for\n  Learning Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans observe a physical system, they can easily locate objects,\nunderstand their interactions, and anticipate future behavior, even in settings\nwith complicated and previously unseen interactions. For computers, however,\nlearning such models from videos in an unsupervised fashion is an unsolved\nresearch problem. In this paper, we present STOVE, a novel state-space model\nfor videos, which explicitly reasons about objects and their positions,\nvelocities, and interactions. It is constructed by combining an image model and\na dynamics model in compositional manner and improves on previous work by\nreusing the dynamics model for inference, accelerating and regularizing\ntraining. STOVE predicts videos with convincing physical behavior over hundreds\nof timesteps, outperforms previous unsupervised models, and even approaches the\nperformance of supervised baselines. We further demonstrate the strength of our\nmodel as a simulator for sample efficient model-based control in a task with\nheavily interacting objects.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 11:48:26 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 09:38:20 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Kossen", "Jannik", ""], ["Stelzner", "Karl", ""], ["Hussing", "Marcel", ""], ["Voelcker", "Claas", ""], ["Kersting", "Kristian", ""]]}, {"id": "1910.02433", "submitter": "Mimi Zhang Dr", "authors": "Mimi Zhang", "title": "Weighted Clustering Ensemble: A Review", "comments": null, "journal-ref": "Pattern Recognition, 2019", "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering ensemble has emerged as a powerful tool for improving both the\nrobustness and the stability of results from individual clustering methods.\nWeighted clustering ensemble arises naturally from clustering ensemble. One of\nthe arguments for weighted clustering ensemble is that elements (clusterings or\nclusters) in a clustering ensemble are of different quality, or that objects or\nfeatures are of varying significance. However, it is not possible to directly\napply the weighting mechanisms from classification (supervised) domain to\nclustering (unsupervised) domain, also because clustering is inherently an\nill-posed problem. This paper provides an overview of weighted clustering\nensemble by discussing different types of weights, major approaches to\ndetermining weight values, and applications of weighted clustering ensemble to\ncomplex data. The unifying framework presented in this paper will help\nclustering practitioners select the most appropriate weighting mechanisms for\ntheir own problems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 12:16:29 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Zhang", "Mimi", ""]]}, {"id": "1910.02442", "submitter": "Yuchao Dai Dr.", "authors": "Liyuan Pan, Yuchao Dai, Miaomiao Liu, Fatih Porikli, Quan Pan", "title": "Joint Stereo Video Deblurring, Scene Flow Estimation and Moving Object\n  Segmentation", "comments": "Accepted by IEEE Transactions on Image Processing 2019. arXiv admin\n  note: text overlap with arXiv:1704.03273", "journal-ref": null, "doi": "10.1109/TIP.2019.2945867", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo videos for the dynamic scenes often show unpleasant blurred effects\ndue to the camera motion and the multiple moving objects with large depth\nvariations. Given consecutive blurred stereo video frames, we aim to recover\nthe latent clean images, estimate the 3D scene flow and segment the multiple\nmoving objects. These three tasks have been previously addressed separately,\nwhich fail to exploit the internal connections among these tasks and cannot\nachieve optimality. In this paper, we propose to jointly solve these three\ntasks in a unified framework by exploiting their intrinsic connections. To this\nend, we represent the dynamic scenes with the piece-wise planar model, which\nexploits the local structure of the scene and expresses various dynamic scenes.\nUnder our model, these three tasks are naturally connected and expressed as the\nparameter estimation of 3D scene structure and camera motion (structure and\nmotion for the dynamic scenes). By exploiting the blur model constraint, the\nmoving objects and the 3D scene structure, we reach an energy minimization\nformulation for joint deblurring, scene flow and segmentation. We evaluate our\napproach extensively on both synthetic datasets and publicly available real\ndatasets with fast-moving objects, camera motion, uncontrolled lighting\nconditions and shadows. Experimental results demonstrate that our method can\nachieve significant improvement in stereo video deblurring, scene flow\nestimation and moving object segmentation, over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 12:55:47 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Pan", "Liyuan", ""], ["Dai", "Yuchao", ""], ["Liu", "Miaomiao", ""], ["Porikli", "Fatih", ""], ["Pan", "Quan", ""]]}, {"id": "1910.02445", "submitter": "Christoph Heindl", "authors": "Christoph Heindl, Markus Ikeda, Gernot St\\\"ubl, Andreas Pichler, Josef\n  Scharinger", "title": "Enhanced Human-Machine Interaction by Combining Proximity Sensing with\n  Global Perception", "comments": "IROS 2019 / 2nd Workshop on Proximity Perception", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The raise of collaborative robotics has led to wide range of sensor\ntechnologies to detect human-machine interactions: at short distances,\nproximity sensors detect nontactile gestures virtually occlusion-free, while at\nmedium distances, active depth sensors are frequently used to infer human\nintentions. We describe an optical system for large workspaces to capture human\npose based on a single panoramic color camera. Despite the two-dimensional\ninput, our system is able to predict metric 3D pose information over larger\nfield of views than would be possible with active depth measurement cameras. We\nmerge posture context with proximity perception to reduce occlusions and\nimprove accuracy at long distances. We demonstrate the capabilities of our\nsystem in two use cases involving multiple humans and robots.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 13:17:57 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 10:48:31 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 05:49:43 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Heindl", "Christoph", ""], ["Ikeda", "Markus", ""], ["St\u00fcbl", "Gernot", ""], ["Pichler", "Andreas", ""], ["Scharinger", "Josef", ""]]}, {"id": "1910.02451", "submitter": "Maike Stern", "authors": "Maike Lorena Stern and Martin Schellenberger", "title": "Fully Convolutional Networks for Chip-wise Defect Detection Employing\n  Photoluminescence Images", "comments": "14 pages, 12 figures", "journal-ref": "J Intell Manuf (2020) 1-14", "doi": "10.1007/s10845-020-01563-4", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient quality control is inevitable in the manufacturing of\nlight-emitting diodes (LEDs). Because defective LED chips may be traced back to\ndifferent causes, a time and cost-intensive electrical and optical contact\nmeasurement is employed. Fast photoluminescence measurements, on the other\nhand, are commonly used to detect wafer separation damages but also hold the\npotential to enable an efficient detection of all kinds of defective LED chips.\nOn a photoluminescence image, every pixel corresponds to an LED chip's\nbrightness after photoexcitation, revealing performance information. But due to\nunevenly distributed brightness values and varying defect patterns,\nphotoluminescence images are not yet employed for a comprehensive defect\ndetection. In this work, we show that fully convolutional networks can be used\nfor chip-wise defect detection, trained on a small data-set of\nphotoluminescence images. Pixel-wise labels allow us to classify each and every\nchip as defective or not. Being measurement-based, labels are easy to procure\nand our experiments show that existing discrepancies between training images\nand labels do not hinder network training. Using weighted loss calculation, we\nwere able to equalize our highly unbalanced class categories. Due to the\nconsistent use of skip connections and residual shortcuts, our network is able\nto predict a variety of structures, from extensive defect clusters up to single\ndefective LED chips.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 13:49:03 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Stern", "Maike Lorena", ""], ["Schellenberger", "Martin", ""]]}, {"id": "1910.02490", "submitter": "Antoni Rosinol", "authors": "Antoni Rosinol, Marcus Abate, Yun Chang, Luca Carlone", "title": "Kimera: an Open-Source Library for Real-Time Metric-Semantic\n  Localization and Mapping", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an open-source C++ library for real-time metric-semantic\nvisual-inertial Simultaneous Localization And Mapping (SLAM). The library goes\nbeyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM,\nVINS- Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling\nin 3D. Kimera is designed with modularity in mind and has four key components:\na visual-inertial odometry (VIO) module for fast and accurate state estimation,\na robust pose graph optimizer for global trajectory estimation, a lightweight\n3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic\nreconstruction module. The modules can be run in isolation or in combination,\nhence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM\nsystem. Kimera runs in real-time on a CPU and produces a 3D metric-semantic\nmesh from semantically labeled images, which can be obtained by modern deep\nlearning methods. We hope that the flexibility, computational efficiency,\nrobustness, and accuracy afforded by Kimera will build a solid basis for future\nmetric-semantic SLAM and perception research, and will allow researchers across\nmultiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark\nand prototype their own efforts without having to start from scratch.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 18:26:25 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 02:43:56 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 04:53:48 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Rosinol", "Antoni", ""], ["Abate", "Marcus", ""], ["Chang", "Yun", ""], ["Carlone", "Luca", ""]]}, {"id": "1910.02509", "submitter": "Tyler Hayes", "authors": "Tyler L. Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya,\n  Christopher Kanan", "title": "REMIND Your Neural Network to Prevent Catastrophic Forgetting", "comments": "To appear in the European Conference on Computer Vision (ECCV-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People learn throughout life. However, incrementally updating conventional\nneural networks leads to catastrophic forgetting. A common remedy is replay,\nwhich is inspired by how the brain consolidates memory. Replay involves\nfine-tuning a network on a mixture of new and old instances. While there is\nneuroscientific evidence that the brain replays compressed memories, existing\nmethods for convolutional networks replay raw images. Here, we propose REMIND,\na brain-inspired approach that enables efficient replay with compressed\nrepresentations. REMIND is trained in an online manner, meaning it learns one\nexample at a time, which is closer to how humans learn. Under the same\nconstraints, REMIND outperforms other methods for incremental class learning on\nthe ImageNet ILSVRC-2012 dataset. We probe REMIND's robustness to data ordering\nschemes known to induce catastrophic forgetting. We demonstrate REMIND's\ngenerality by pioneering online learning for Visual Question Answering (VQA).\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 19:48:23 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 16:58:09 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 17:10:44 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Hayes", "Tyler L.", ""], ["Kafle", "Kushal", ""], ["Shrestha", "Robik", ""], ["Acharya", "Manoj", ""], ["Kanan", "Christopher", ""]]}, {"id": "1910.02526", "submitter": "M. Salman Asif", "authors": "Yucheng Zheng and M. Salman Asif", "title": "Joint Image and Depth Estimation with Mask-Based Lensless Cameras", "comments": "Added new experiments with camera prototype", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mask-based lensless cameras replace the lens of a conventional camera with a\ncustom mask. These cameras can potentially be very thin and even flexible.\nRecently, it has been demonstrated that such mask-based cameras can recover\nlight intensity and depth information of a scene. Existing depth recovery\nalgorithms either assume that the scene consists of a small number of depth\nplanes or solve a sparse recovery problem over a large 3D volume. Both these\napproaches fail to recover the scenes with large depth variations. In this\npaper, we propose a new approach for depth estimation based on an alternating\ngradient descent algorithm that jointly estimates a continuous depth map and\nlight distribution of the unknown scene from its lensless measurements. We\npresent simulation results on image and depth reconstruction for a variety of\n3D test scenes. A comparison between the proposed algorithm and other method\nshows that our algorithm is more robust for natural scenes with a large range\nof depths. We built a prototype lensless camera and present experimental\nresults for reconstruction of intensity and depth maps of different real\nobjects.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 21:12:07 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 04:15:51 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Zheng", "Yucheng", ""], ["Asif", "M. Salman", ""]]}, {"id": "1910.02527", "submitter": "Iro Armeni", "authors": "Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R. Zamir, Martin Fischer,\n  Jitendra Malik, Silvio Savarese", "title": "3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive semantic understanding of a scene is important for many\napplications - but in what space should diverse semantic information (e.g.,\nobjects, scene categories, material types, texture, etc.) be grounded and what\nshould be its structure? Aspiring to have one unified structure that hosts\ndiverse types of semantics, we follow the Scene Graph paradigm in 3D,\ngenerating a 3D Scene Graph. Given a 3D mesh and registered panoramic images,\nwe construct a graph that spans the entire building and includes semantics on\nobjects (e.g., class, material, and other attributes), rooms (e.g., scene\ncategory, volume, etc.) and cameras (e.g., location, etc.), as well as the\nrelationships among these entities.\n  However, this process is prohibitively labor heavy if done manually. To\nalleviate this we devise a semi-automatic framework that employs existing\ndetection methods and enhances them using two main constraints: I. framing of\nquery images sampled on panoramas to maximize the performance of 2D detectors,\nand II. multi-view consistency enforcement across 2D detections that originate\nin different camera locations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 21:13:02 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Armeni", "Iro", ""], ["He", "Zhi-Yang", ""], ["Gwak", "JunYoung", ""], ["Zamir", "Amir R.", ""], ["Fischer", "Martin", ""], ["Malik", "Jitendra", ""], ["Savarese", "Silvio", ""]]}, {"id": "1910.02543", "submitter": "Xiang Li", "authors": "Xiang Li, Chen Lin, Chuming Li, Ming Sun, Wei Wu, Junjie Yan, Wanli\n  Ouyang", "title": "Improving One-shot NAS by Suppressing the Posterior Fading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in automated neural architecture search (NAS). To\nimprove the efficiency of NAS, previous approaches adopt weight sharing method\nto force all models share the same set of weights. However, it has been\nobserved that a model performing better with shared weights does not\nnecessarily perform better when trained alone. In this paper, we analyse\nexisting weight sharing one-shot NAS approaches from a Bayesian point of view\nand identify the posterior fading problem, which compromises the effectiveness\nof shared weights. To alleviate this problem, we present a practical approach\nto guide the parameter posterior towards its true distribution. Moreover, a\nhard latency constraint is introduced during the search so that the desired\nlatency can be achieved. The resulted method, namely Posterior Convergent NAS\n(PC-NAS), achieves state-of-the-art performance under standard GPU latency\nconstraint on ImageNet. In our small search space, our model PC-NAS-S attains\n76.8 % top-1 accuracy, 2.1% higher than MobileNetV2 (1.4x) with the same\nlatency. When adopted to the large search space, PC-NAS-L achieves 78.1 % top-1\naccuracy within 11ms. The discovered architecture also transfers well to other\ncomputer vision applications such as object detection and person\nre-identification.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 22:31:37 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Li", "Xiang", ""], ["Lin", "Chen", ""], ["Li", "Chuming", ""], ["Sun", "Ming", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "1910.02550", "submitter": "Shreeyak Sajjan", "authors": "Shreeyak S. Sajjan, Matthew Moore, Mike Pan, Ganesh Nagaraja, Johnny\n  Lee, Andy Zeng, Shuran Song", "title": "ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation", "comments": "Project Website: https://sites.google.com/view/cleargrasp, 13 pages,\n  13 figures, submitted to ICRA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparent objects are a common part of everyday life, yet they possess\nunique visual properties that make them incredibly difficult for standard 3D\nsensors to produce accurate depth estimates for. In many cases, they often\nappear as noisy or distorted approximations of the surfaces that lie behind\nthem. To address these challenges, we present ClearGrasp -- a deep learning\napproach for estimating accurate 3D geometry of transparent objects from a\nsingle RGB-D image for robotic manipulation. Given a single RGB-D image of\ntransparent objects, ClearGrasp uses deep convolutional networks to infer\nsurface normals, masks of transparent surfaces, and occlusion boundaries. It\nthen uses these outputs to refine the initial depth estimates for all\ntransparent surfaces in the scene. To train and test ClearGrasp, we construct a\nlarge-scale synthetic dataset of over 50,000 RGB-D images, as well as a\nreal-world test benchmark with 286 RGB-D images of transparent objects and\ntheir ground truth geometries. The experiments demonstrate that ClearGrasp is\nsubstantially better than monocular depth estimation baselines and is capable\nof generalizing to real-world images and novel objects. We also demonstrate\nthat ClearGrasp can be applied out-of-the-box to improve grasping algorithms'\nperformance on transparent objects. Code, data, and benchmarks will be\nreleased. Supplementary materials available on the project website:\nhttps://sites.google.com/view/cleargrasp\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 23:22:56 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 17:29:36 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Sajjan", "Shreeyak S.", ""], ["Moore", "Matthew", ""], ["Pan", "Mike", ""], ["Nagaraja", "Ganesh", ""], ["Lee", "Johnny", ""], ["Zeng", "Andy", ""], ["Song", "Shuran", ""]]}, {"id": "1910.02560", "submitter": "Wenju Xu", "authors": "Wenju Xu and Shawn Keshmiri and Guanghui Wang", "title": "Stacked Wasserstein Autoencoder", "comments": "arXiv admin note: text overlap with arXiv:1902.05581", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating distributions over complicated manifolds, such as natural\nimages, are conceptually attractive. The deep latent variable model, trained\nusing variational autoencoders and generative adversarial networks, is now a\nkey technique for representation learning. However, it is difficult to unify\nthese two models for exact latent-variable inference and parallelize both\nreconstruction and sampling, partly due to the regularization under the latent\nvariables, to match a simple explicit prior distribution. These approaches are\nprone to be oversimplified, and can only characterize a few modes of the true\ndistribution. Based on the recently proposed Wasserstein autoencoder (WAE) with\na new regularization as an optimal transport. The paper proposes a stacked\nWasserstein autoencoder (SWAE) to learn a deep latent variable model. SWAE is a\nhierarchical model, which relaxes the optimal transport constraints at two\nstages. At the first stage, the SWAE flexibly learns a representation\ndistribution, i.e., the encoded prior; and at the second stage, the encoded\nrepresentation distribution is approximated with a latent variable model under\nthe regularization encouraging the latent distribution to match the explicit\nprior. This model allows us to generate natural textual outputs as well as\nperform manipulations in the latent space to induce changes in the output\nspace. Both quantitative and qualitative results demonstrate the superior\nperformance of SWAE compared with the state-of-the-art approaches in terms of\nfaithful reconstruction and generation quality.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:07:42 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Xu", "Wenju", ""], ["Keshmiri", "Shawn", ""], ["Wang", "Guanghui", ""]]}, {"id": "1910.02562", "submitter": "Wenwen Yu", "authors": "Ning Lu, Wenwen Yu, Xianbiao Qi, Yihao Chen, Ping Gong, Rong Xiao,\n  Xiang Bai", "title": "MASTER: Multi-Aspect Non-local Network for Scene Text Recognition", "comments": "Accepted by Pattern Recognition. Ning Lu and Wenwen Yu are co-first\n  authors", "journal-ref": null, "doi": "10.1016/j.patcog.2021.107980", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Attention-based scene text recognizers have gained huge success, which\nleverages a more compact intermediate representation to learn 1d- or 2d-\nattention by a RNN-based encoder-decoder architecture. However, such methods\nsuffer from attention-drift problem because high similarity among encoded\nfeatures leads to attention confusion under the RNN-based local attention\nmechanism. Moreover, RNN-based methods have low efficiency due to poor\nparallelization. To overcome these problems, we propose the MASTER, a\nself-attention based scene text recognizer that (1) not only encodes the\ninput-output attention but also learns self-attention which encodes\nfeature-feature and target-target relationships inside the encoder and decoder\nand (2) learns a more powerful and robust intermediate representation to\nspatial distortion, and (3) owns a great training efficiency because of high\ntraining parallelization and a high-speed inference because of an efficient\nmemory-cache mechanism. Extensive experiments on various benchmarks demonstrate\nthe superior performance of our MASTER on both regular and irregular scene\ntext. Pytorch code can be found at https://github.com/wenwenyu/MASTER-pytorch,\nand Tensorflow code can be found at https://github.com/jiangxiluning/MASTER-TF.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 00:31:01 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 11:55:15 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 06:21:38 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lu", "Ning", ""], ["Yu", "Wenwen", ""], ["Qi", "Xianbiao", ""], ["Chen", "Yihao", ""], ["Gong", "Ping", ""], ["Xiao", "Rong", ""], ["Bai", "Xiang", ""]]}, {"id": "1910.02564", "submitter": "Manuel Serra Nunes", "authors": "Manuel Serra Nunes, Atabak Dehban, Plinio Moreno, Jos\\'e Santos-Victor", "title": "Action-conditioned Benchmarking of Robotic Video Prediction Models: a\n  Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A defining characteristic of intelligent systems is the ability to make\naction decisions based on the anticipated outcomes. Video prediction systems\nhave been demonstrated as a solution for predicting how the future will unfold\nvisually, and thus, many models have been proposed that are capable of\npredicting future frames based on a history of observed frames~(and sometimes\nrobot actions). However, a comprehensive method for determining the fitness of\ndifferent video prediction models at guiding the selection of actions is yet to\nbe developed. Current metrics assess video prediction models based on human\nperception of frame quality. In contrast, we argue that if these systems are to\nbe used to guide action, necessarily, the actions the robot performs should be\nencoded in the predicted frames. In this paper, we are proposing a new metric\nto compare different video prediction models based on this argument. More\nspecifically, we propose an action inference system and quantitatively rank\ndifferent models based on how well we can infer the robot actions from the\npredicted frames. Our extensive experiments show that models with high\nperceptual scores can perform poorly in the proposed action inference tests and\nthus, may not be suitable options to be used in robot planning systems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 00:39:17 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Nunes", "Manuel Serra", ""], ["Dehban", "Atabak", ""], ["Moreno", "Plinio", ""], ["Santos-Victor", "Jos\u00e9", ""]]}, {"id": "1910.02579", "submitter": "Md Kamrul Hasan", "authors": "Md Kamrul Hasan, Nazmus Sakib, Joshua Field, Richard R. Love and\n  Sheikh I. Ahamed", "title": "A Novel Technique of Noninvasive Hemoglobin Level Measurement Using HSV\n  Value of Fingertip Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Over the last decade, smartphones have changed radically to support us with\nmHealth technology, cloud computing, and machine learning algorithm. Having its\nmultifaceted facilities, we present a novel smartphone-based noninvasive\nhemoglobin (Hb) level prediction model by analyzing hue, saturation and value\n(HSV) of a fingertip video. Here, we collect 60 videos of 60 subjects from two\ndifferent locations: Blood Center of Wisconsin, USA and AmaderGram, Bangladesh.\nWe extract red, green, and blue (RGB) pixel intensities of selected images of\nthose videos captured by the smartphone camera with flash on. Then we convert\nRGB values of selected video frames of a fingertip video into HSV color space\nand we generate histogram values of these HSV pixel intensities. We average\nthese histogram values of a fingertip video and consider as an observation\nagainst the gold standard Hb concentration. We generate two input feature\nmatrices based on observation of two different data sets. Partial Least Squares\n(PLS) algorithm is applied on the input feature matrix. We observe R2=0.95 in\nboth data sets through our research. We analyze our data using Python OpenCV,\nMatlab, and R statistics tool.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 02:17:03 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Hasan", "Md Kamrul", ""], ["Sakib", "Nazmus", ""], ["Field", "Joshua", ""], ["Love", "Richard R.", ""], ["Ahamed", "Sheikh I.", ""]]}, {"id": "1910.02593", "submitter": "Zhen Han", "authors": "Zhen Han, Enyan Dai, Xu Jia, Xiaoying Ren, Shuaijun Chen, Chunjing Xu,\n  Jianzhuang Liu, Qi Tian", "title": "Unsupervised Image Super-Resolution with an Indirect Supervised Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of single image super-resolution (SISR) aims at reconstructing a\nhigh-resolution (HR) image from a low-resolution (LR) image. Although\nsignificant progress has been made by deep learning models, they are trained on\nsynthetic paired data in a supervised way and do not perform well on real data.\nThere are several attempts that directly apply unsupervised image translation\nmodels to address such a problem. However, unsupervised low-level vision\nproblem poses more challenge on the accuracy of translation. In this work,we\npropose a novel framework which is composed of two stages: 1) unsupervised\nimage translation between real LR images and synthetic LR images; 2) supervised\nsuper-resolution from approximated real LR images to HR images. It takes the\nsynthetic LR images as a bridge and creates an indirect supervised path from\nreal LR images to HR images. Any existed deep learning based image\nsuper-resolution model can be integrated into the second stage of the proposed\nframework for further improvement. In addition it shows great flexibility in\nbalancing between distortion and perceptual quality under unsupervised setting.\nThe proposed method is evaluated on both NTIRE 2017 and 2018 challenge datasets\nand achieves favorable performance against supervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 03:34:49 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 13:18:52 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Han", "Zhen", ""], ["Dai", "Enyan", ""], ["Jia", "Xu", ""], ["Ren", "Xiaoying", ""], ["Chen", "Shuaijun", ""], ["Xu", "Chunjing", ""], ["Liu", "Jianzhuang", ""], ["Tian", "Qi", ""]]}, {"id": "1910.02602", "submitter": "Basura Fernando", "authors": "Yan Bin Ng, Basura Fernando", "title": "Human Action Sequence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper classifies human action sequences from videos using a machine\ntranslation model. In contrast to classical human action classification which\noutputs a set of actions, our method output a sequence of action in the\nchronological order of the actions performed by the human. Therefore our method\nis evaluated using sequential performance measures such as Bilingual Evaluation\nUnderstudy (BLEU) scores. Action sequence classification has many applications\nsuch as learning from demonstration, action segmentation, detection,\nlocalization and video captioning. Furthermore, we use our model that is\ntrained to output action sequences to solve downstream tasks; such as video\ncaptioning and action localization. We obtain state of the art results for\nvideo captioning in challenging Charades dataset obtaining BLEU-4 score of 34.8\nand METEOR score of 33.6 outperforming previous state-of-the-art of 18.8 and\n19.5 respectively. Similarly, on ActivityNet captioning, we obtain excellent\nresults in-terms of ROUGE (20.24) and CIDER (37.58) scores. For action\nlocalization, without using any explicit start/end action annotations, our\nmethod obtains localization performance of 22.2 mAP outperforming prior fully\nsupervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 04:27:01 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Ng", "Yan Bin", ""], ["Fernando", "Basura", ""]]}, {"id": "1910.02618", "submitter": "Sobas Mehboob", "authors": "Memoona Tahira, Sobas Mehboob, Anis U. Rahman and Omar Arif", "title": "CrowdFix: An Eyetracking Dataset of Real Life Crowd Videos", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human visual attention and saliency is an integral part of\nvision research. In this context, there is an ever-present need for fresh and\ndiverse benchmark datasets, particularly for insight into special use cases\nlike crowded scenes. We contribute to this end by: (1) reviewing the dynamics\nbehind saliency and crowds. (2) using eye tracking to create a dynamic human\neye fixation dataset over a new set of crowd videos gathered from the Internet.\nThe videos are annotated into three distinct density levels. (3) Finally, we\nevaluate state-of-the-art saliency models on our dataset to identify possible\nimprovements for the design and creation of a more robust saliency model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 05:43:49 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 07:40:19 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Tahira", "Memoona", ""], ["Mehboob", "Sobas", ""], ["Rahman", "Anis U.", ""], ["Arif", "Omar", ""]]}, {"id": "1910.02624", "submitter": "Weifeng Ge", "authors": "Weifeng Ge, Sheng Guo, Weilin Huang, and Matthew R. Scott", "title": "Label-PEnet: Sequential Label Propagation and Enhancement Networks for\n  Weakly Supervised Instance Segmentation", "comments": "Rectifiy some typos in Arxiv title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised instance segmentation aims to detect and segment object\ninstances precisely, given imagelevel labels only. Unlike previous methods\nwhich are composed of multiple offline stages, we propose Sequential Label\nPropagation and Enhancement Networks (referred as Label-PEnet) that\nprogressively transform image-level labels to pixel-wise labels in a\ncoarse-to-fine manner. We design four cascaded modules including multi-label\nclassification, object detection, instance refinement and instance\nsegmentation, which are implemented sequentially by sharing the same backbone.\nThe cascaded pipeline is trained alternatively with a curriculum learning\nstrategy that generalizes labels from high-level images to low-level pixels\ngradually with increasing accuracy. In addition, we design a proposal\ncalibration module to explore the ability of classification networks to find\nkey pixels that identify object parts, which serves as a post validation\nstrategy running in the inverse order. We evaluate the efficiency of our\nLabel-PEnet in mining instance masks on standard benchmarks: PASCAL VOC 2007\nand 2012. Experimental results show that Label-PEnet outperforms the\nstate-of-the-art algorithms by a clear margin, and obtains comparable\nperformance even with the fully-supervised approaches.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 06:28:16 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 18:23:37 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 08:22:51 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Ge", "Weifeng", ""], ["Guo", "Sheng", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""]]}, {"id": "1910.02629", "submitter": "Zhenyue Qin", "authors": "Zhenyue Qin and Dongwoo Kim", "title": "Softmax Is Not an Artificial Trick: An Information-Theoretic View of\n  Softmax in Neural Networks", "comments": "Withdrawn due to Zhenyue Qin uploading the manuscript without consent\n  of the other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite great popularity of applying softmax to map the non-normalised\noutputs of a neural network to a probability distribution over predicting\nclasses, this normalised exponential transformation still seems to be\nartificial. A theoretic framework that incorporates softmax as an intrinsic\ncomponent is still lacking. In this paper, we view neural networks embedding\nsoftmax from an information-theoretic perspective. Under this view, we can\nnaturally and mathematically derive log-softmax as an inherent component in a\nneural network for evaluating the conditional mutual information between\nnetwork output vectors and labels given an input datum. We show that training\ndeterministic neural networks through maximising log-softmax is equivalent to\nenlarging the conditional mutual information, i.e., feeding label information\ninto network outputs. We also generalise our informative-theoretic perspective\nto neural networks with stochasticity and derive information upper and lower\nbounds of log-softmax. In theory, such an information-theoretic view offers\nrationality support for embedding softmax in neural networks; in practice, we\neventually demonstrate a computer vision application example of how to employ\nour information-theoretic view to filter out targeted objects on images.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 06:46:06 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 00:34:56 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 05:59:37 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Qin", "Zhenyue", ""], ["Kim", "Dongwoo", ""]]}, {"id": "1910.02653", "submitter": "Ajay Jain", "authors": "Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter\n  Abbeel, Kurt Keutzer, Ion Stoica, Joseph E. Gonzalez", "title": "Checkmate: Breaking the Memory Wall with Optimal Tensor\n  Rematerialization", "comments": "In Proceedings of 3rd Conference Machine Learning and Systems 2020\n  (MLSys 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We formalize the problem of trading-off DNN training time and memory\nrequirements as the tensor rematerialization optimization problem, a\ngeneralization of prior checkpointing strategies. We introduce Checkmate, a\nsystem that solves for optimal rematerialization schedules in reasonable times\n(under an hour) using off-the-shelf MILP solvers or near-optimal schedules with\nan approximation algorithm, then uses these schedules to accelerate millions of\ntraining iterations. Our method scales to complex, realistic architectures and\nis hardware-aware through the use of accelerator-specific, profile-based cost\nmodels. In addition to reducing training cost, Checkmate enables real-world\nnetworks to be trained with up to 5.1x larger input sizes. Checkmate is an\nopen-source project, available at https://github.com/parasj/checkmate.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 07:54:06 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 17:57:45 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 17:46:43 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Jain", "Paras", ""], ["Jain", "Ajay", ""], ["Nrusimha", "Aniruddha", ""], ["Gholami", "Amir", ""], ["Abbeel", "Pieter", ""], ["Keutzer", "Kurt", ""], ["Stoica", "Ion", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1910.02672", "submitter": "Wei Qiu", "authors": "Wei Qiu, Jiaming Guo, Xiang Li, Mengjia Xu, Mo Zhang, Ning Guo and\n  Quanzheng Li", "title": "Multi-label Detection and Classification of Red Blood Cells in\n  Microscopic Images", "comments": "Wei Qiu, Jiaming Guo and Xiang Li contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell detection and cell type classification from biomedical images play an\nimportant role for high-throughput imaging and various clinical application.\nWhile classification of single cell sample can be performed with standard\ncomputer vision and machine learning methods, analysis of multi-label samples\n(region containing congregating cells) is more challenging, as separation of\nindividual cells can be difficult (e.g. touching cells) or even impossible\n(e.g. overlapping cells). As multi-instance images are common in analyzing Red\nBlood Cell (RBC) for Sickle Cell Disease (SCD) diagnosis, we develop and\nimplement a multi-instance cell detection and classification framework to\naddress this challenge. The framework firstly trains a region proposal model\nbased on Region-based Convolutional Network (RCNN) to obtain bounding-boxes of\nregions potentially containing single or multiple cells from input microscopic\nimages, which are extracted as image patches. High-level image features are\nthen calculated from image patches through a pre-trained Convolutional Neural\nNetwork (CNN) with ResNet-50 structure. Using these image features inputs, six\nnetworks are then trained to make multi-label prediction of whether a given\npatch contains cells belonging to a specific cell type. As the six networks are\ntrained with image patches consisting of both individual cells and\ntouching/overlapping cells, they can effectively recognize cell types that are\npresented in multi-instance image samples. Finally, for the purpose of SCD\ntesting, we train another machine learning classifier to predict whether the\ngiven image patch contains abnormal cell type based on outputs from the six\nnetworks. Testing result of the proposed framework shows that it can achieve\ngood performance in automatic cell detection and classification.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 08:40:49 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 12:44:04 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Qiu", "Wei", ""], ["Guo", "Jiaming", ""], ["Li", "Xiang", ""], ["Xu", "Mengjia", ""], ["Zhang", "Mo", ""], ["Guo", "Ning", ""], ["Li", "Quanzheng", ""]]}, {"id": "1910.02675", "submitter": "Nico Lang", "authors": "Steve Branson, Jan Dirk Wegner, David Hall, Nico Lang, Konrad\n  Schindler, Pietro Perona", "title": "From Google Maps to a Fine-Grained Catalog of Street trees", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, Volume 135,\n  January 2018, Pages 13-30", "doi": "10.1016/j.isprsjprs.2017.11.008", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Up-to-date catalogs of the urban tree population are important for\nmunicipalities to monitor and improve quality of life in cities. Despite much\nresearch on automation of tree mapping, mainly relying on dedicated airborne\nLiDAR or hyperspectral campaigns, trees are still mostly mapped manually in\npractice. We present a fully automated tree detection and species recognition\npipeline to process thousands of trees within a few hours using publicly\navailable aerial and street view images of Google MapsTM. These data provide\nrich information (viewpoints, scales) from global tree shapes to bark textures.\nOur work-flow is built around a supervised classification that automatically\nlearns the most discriminative features from thousands of trees and\ncorresponding, public tree inventory data. In addition, we introduce a change\ntracker to keep urban tree inventories up-to-date. Changes of individual trees\nare recognized at city-scale by comparing street-level images of the same tree\nlocation at two different times. Drawing on recent advances in computer vision\nand machine learning, we apply convolutional neural networks (CNN) for all\nclassification tasks. We propose the following pipeline: download all available\npanoramas and overhead images of an area of interest, detect trees per image\nand combine multi-view detections in a probabilistic framework, adding prior\nknowledge; recognize fine-grained species of detected trees. In a later,\nseparate module, track trees over time and identify the type of change. We\nbelieve this is the first work to exploit publicly available image data for\nfine-grained tree mapping at city-scale, respectively over many thousands of\ntrees. Experiments in the city of Pasadena, California, USA show that we can\ndetect > 70% of the street trees, assign correct species to > 80% for 40\ndifferent species, and correctly detect and classify changes in > 90% of the\ncases.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 08:52:50 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Branson", "Steve", ""], ["Wegner", "Jan Dirk", ""], ["Hall", "David", ""], ["Lang", "Nico", ""], ["Schindler", "Konrad", ""], ["Perona", "Pietro", ""]]}, {"id": "1910.02696", "submitter": "Kirsten Koolstra", "authors": "Kirsten Koolstra, Peter B\\\"ornert, Boudewijn Lelieveldt, Andrew Webb,\n  Oleh Dzyubachyk", "title": "Hierarchical stochastic neighbor embedding as a tool for visualizing the\n  encoding capability of magnetic resonance fingerprinting dictionaries", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Magnetic Resonance Fingerprinting (MRF) the quality of the estimated\nparameter maps depends on the encoding capability of the variable flip angle\ntrain. In this work we show how the dimensionality reduction technique\nHierarchical Stochastic Neighbor Embedding (HSNE) can be used to obtain insight\ninto the encoding capability of different MRF sequences. Embedding\nhigh-dimensional MRF dictionaries into a lower-dimensional space and\nvisualizing them with colors, being a surrogate for location in low-dimensional\nspace, provides a comprehensive overview of particular dictionaries and, in\naddition, enables comparison of different sequences. Dictionaries for various\nsequences and sequence lengths were compared to each other, and the effect of\ntransmit field variations on the encoding capability was assessed. Clear\ndifferences in encoding capability were observed between different sequences,\nand HSNE results accurately reflect those obtained from an MRF matching\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 09:48:38 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Koolstra", "Kirsten", ""], ["B\u00f6rnert", "Peter", ""], ["Lelieveldt", "Boudewijn", ""], ["Webb", "Andrew", ""], ["Dzyubachyk", "Oleh", ""]]}, {"id": "1910.02702", "submitter": "Ilja Manakov", "authors": "Ilja Manakov, Markus Rohm, Christoph Kern, Benedikt Schworm, Karsten\n  Kortuem, Volker Tresp", "title": "Noise as Domain Shift: Denoising Medical Images by Unpaired Image\n  Translation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-33391-1_1", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We cast the problem of image denoising as a domain translation problem\nbetween high and low noise domains. By modifying the cycleGAN model, we are\nable to learn a mapping between these domains on unpaired retinal optical\ncoherence tomography images. In quantitative measurements and a qualitative\nevaluation by ophthalmologists, we show how this approach outperforms other\nestablished methods. The results indicate that the network differentiates\nsubtle changes in the level of noise in the image. Further investigation of the\nmodel's feature maps reveals that it has learned to distinguish retinal layers\nand other distinct regions of the images.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 10:16:31 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Manakov", "Ilja", ""], ["Rohm", "Markus", ""], ["Kern", "Christoph", ""], ["Schworm", "Benedikt", ""], ["Kortuem", "Karsten", ""], ["Tresp", "Volker", ""]]}, {"id": "1910.02713", "submitter": "Ilja Manakov", "authors": "Ilja Manakov, Volker Tresp", "title": "Push it to the Limit: Discover Edge-Cases in Image Data with\n  Autoencoders", "comments": "Accepted as a workshop paper at MEDNeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we focus on the problem of identifying semantic factors of\nvariation in large image datasets. By training a convolutional Autoencoder on\nthe image data, we create encodings, which describe each datapoint at a higher\nlevel of abstraction than pixel-space. We then apply Principal Component\nAnalysis to the encodings to disentangle the factors of variation in the data.\nSorting the dataset according to the values of individual principal components,\nwe find that samples at the high and low ends of the distribution often share\nspecific semantic characteristics. We refer to these groups of samples as\nsemantic groups. When applied to real-world data, this method can help discover\nunwanted edge-cases.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 10:36:40 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Manakov", "Ilja", ""], ["Tresp", "Volker", ""]]}, {"id": "1910.02718", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi", "title": "Continual Learning in Neural Networks", "comments": "PhD Thesis, Supervisor: Tinne Tuytelaars", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks have exceeded human-level performance in\naccomplishing several individual tasks (e.g. voice recognition, object\nrecognition, and video games). However, such success remains modest compared to\nhuman intelligence that can learn and perform an unlimited number of tasks.\nHumans' ability of learning and accumulating knowledge over their lifetime is\nan essential aspect of their intelligence. Continual machine learning aims at a\nhigher level of machine intelligence through providing the artificial agents\nwith the ability to learn online from a non-stationary and never-ending stream\nof data. A key component of such a never-ending learning process is to overcome\nthe catastrophic forgetting of previously seen data, a problem that neural\nnetworks are well known to suffer from. The work described in this thesis has\nbeen dedicated to the investigation of continual learning and solutions to\nmitigate the forgetting phenomena in neural networks. To approach the continual\nlearning problem, we first assume a task incremental setting where tasks are\nreceived one at a time and data from previous tasks are not stored. Since the\ntask incremental setting can't be assumed in all continual learning scenarios,\nwe also study the more general online continual setting. We consider an\ninfinite stream of data drawn from a non-stationary distribution with a\nsupervisory or self-supervisory training signal. The proposed methods in this\nthesis have tackled important aspects of continual learning. They were\nevaluated on different benchmarks and over various learning sequences. Advances\nin the state of the art of continual learning have been shown and challenges\nfor bringing continual learning into application were critically identified.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 10:52:14 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 09:48:14 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Aljundi", "Rahaf", ""]]}, {"id": "1910.02738", "submitter": "Romeil Sandhu", "authors": "Bipul Islam, Ji Liu, Anthony Yezzi, Romeil Sandhu", "title": "An Interactive Control Approach to 3D Shape Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately reconstruct the 3D facets of a scene is one of the\nkey problems in robotic vision. However, even with recent advances with machine\nlearning, there is no high-fidelity universal 3D reconstruction method for this\noptimization problem as schemes often cater to specific image modalities and\nare often biased by scene abnormalities. Simply put, there always remains an\ninformation gap due to the dynamic nature of real-world scenarios. To this end,\nwe demonstrate a feedback control framework which invokes operator inputs (also\nprone to errors) in order to augment existing reconstruction schemes. For\nproof-of-concept, we choose a classical region-based stereoscopic\nreconstruction approach and show how an ill-posed model can be augmented with\noperator input to be much more robust to scene artifacts. We provide necessary\nconditions for stability via Lyapunov analysis and perhaps more importantly, we\nshow that the stability depends on a notion of absolute curvature.\nMathematically, this aligns with previous work that has shown Ricci curvature\nas proxy for functional robustness of dynamical networked systems. We conclude\nwith results that show how our method can improve standalone reconstruction\nschemes.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 11:45:55 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Islam", "Bipul", ""], ["Liu", "Ji", ""], ["Yezzi", "Anthony", ""], ["Sandhu", "Romeil", ""]]}, {"id": "1910.02747", "submitter": "Eren Aksoy", "authors": "Georgios Tzelepis, Ahraz Asif, Saimir Baci, Selcuk Cavdar, and Eren\n  Erdal Aksoy", "title": "Deep Neural Network Compression for Image Classification and Object\n  Detection", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been notorious for being computationally expensive. This\nis mainly because neural networks are often over-parametrized and most likely\nhave redundant nodes or layers as they are getting deeper and wider. Their\ndemand for hardware resources prohibits their extensive use in embedded devices\nand puts restrictions on tasks like real-time image classification or object\ndetection. In this work, we propose a network-agnostic model compression method\ninfused with a novel dynamical clustering approach to reduce the computational\ncost and memory footprint of deep neural networks. We evaluated our new\ncompression method on five different state-of-the-art image classification and\nobject detection networks. In classification networks, we pruned about 95% of\nnetwork parameters. In advanced detection networks such as YOLOv3, our proposed\ncompression method managed to reduce the model parameters up to 59.70% which\nyielded 110X less memory without sacrificing much in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 12:19:37 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Tzelepis", "Georgios", ""], ["Asif", "Ahraz", ""], ["Baci", "Saimir", ""], ["Cavdar", "Selcuk", ""], ["Aksoy", "Eren Erdal", ""]]}, {"id": "1910.02793", "submitter": "Eric Hofesmann", "authors": "Madan Ravi Ganesh, Eric Hofesmann, Nathan Louis, Jason Corso", "title": "ViP: Video Platform for PyTorch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the Video Platform for PyTorch (ViP), a deep\nlearning-based framework designed to handle and extend to any problem domain\nbased on videos. ViP supports (1) a single unified interface applicable to all\nvideo problem domains, (2) quick prototyping of video models, (3) executing\nlarge-batch operations with reduced memory consumption, and (4) easy and\nreproducible experimental setups. ViP's core functionality is built with\nflexibility and modularity in mind to allow for smooth data flow between\ndifferent parts of the platform and benchmarking against existing methods. In\nproviding a software platform that supports multiple video-based problem\ndomains, we allow for more cross-pollination of models, ideas and stronger\ngeneralization in the video understanding research community.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 13:49:50 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Ganesh", "Madan Ravi", ""], ["Hofesmann", "Eric", ""], ["Louis", "Nathan", ""], ["Corso", "Jason", ""]]}, {"id": "1910.02806", "submitter": "Hyojin Bahng", "authors": "Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, Seong Joon Oh", "title": "Learning De-biased Representations with Biased Representations", "comments": "Accepted to ICML 2020. Code available at\n  https://github.com/clovaai/rebias", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms are trained and evaluated by splitting data\nfrom a single source into training and test sets. While such focus on\nin-distribution learning scenarios has led to interesting advancement, it has\nnot been able to tell if models are relying on dataset biases as shortcuts for\nsuccessful prediction (e.g., using snow cues for recognising snowmobiles),\nresulting in biased models that fail to generalise when the bias shifts to a\ndifferent class. The cross-bias generalisation problem has been addressed by\nde-biasing training data through augmentation or re-sampling, which are often\nprohibitive due to the data collection cost (e.g., collecting images of a\nsnowmobile on a desert) and the difficulty of quantifying or expressing biases\nin the first place. In this work, we propose a novel framework to train a\nde-biased representation by encouraging it to be different from a set of\nrepresentations that are biased by design. This tactic is feasible in many\nscenarios where it is much easier to define a set of biased representations\nthan to define and quantify bias. We demonstrate the efficacy of our method\nacross a variety of synthetic and real-world biases; our experiments show that\nthe method discourages models from taking bias shortcuts, resulting in improved\ngeneralisation. Source code is available at https://github.com/clovaai/rebias.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:11:13 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 16:46:24 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 11:51:02 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Bahng", "Hyojin", ""], ["Chun", "Sanghyuk", ""], ["Yun", "Sangdoo", ""], ["Choo", "Jaegul", ""], ["Oh", "Seong Joon", ""]]}, {"id": "1910.02818", "submitter": "Iulia Paraicu", "authors": "Iulia Paraicu and Marius Leordeanu", "title": "Learning Navigation by Visual Localization and Trajectory Prediction", "comments": "Submitted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When driving, people make decisions based on current traffic as well as their\ndesired route. They have a mental map of known routes and are often able to\nnavigate without needing directions. Current self-driving models improve their\nperformances when using additional GPS information. Here we aim to push forward\nself-driving research and perform route planning even in the absence of GPS.\nOur system learns to predict in real-time vehicle's current location and future\ntrajectory, as a function of time, on a known map, given only the raw video\nstream and the intended destination. The GPS signal is available only at\ntraining time, with training data annotation being fully automatic. Different\nfrom other published models, we predict the vehicle's trajectory for up to\nseven seconds ahead, from which complete steering, speed and acceleration\ninformation can be derived for the entire time span. Trajectories capture\nnavigational information on multiple levels, from instant steering commands\nthat depend on present traffic and obstacles ahead, to longer-term navigation\ndecisions, towards a specific destination. We collect our dataset with a\nregular car and a smartphone that records video and GPS streams. The GPS data\nis used to derive ground-truth supervision labels and create an analytical\nrepresentation of the traversed map. In tests, our system outperforms published\nmethods on visual localization and steering and gives accurate navigation\nassistance between any two known locations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:31:38 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Paraicu", "Iulia", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1910.02840", "submitter": "Chris Finlay", "authors": "Aram-Alexandre Pooladian and Chris Finlay and Adam M Oberman", "title": "Farkas layers: don't shift the data, fix the geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successfully training deep neural networks often requires either batch\nnormalization, appropriate weight initialization, both of which come with their\nown challenges. We propose an alternative, geometrically motivated method for\ntraining. Using elementary results from linear programming, we introduce Farkas\nlayers: a method that ensures at least one neuron is active at a given layer.\nFocusing on residual networks with ReLU activation, we empirically demonstrate\na significant improvement in training capacity in the absence of batch\nnormalization or methods of initialization across a broad range of network\nsizes on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 15:24:37 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Pooladian", "Aram-Alexandre", ""], ["Finlay", "Chris", ""], ["Oberman", "Adam M", ""]]}, {"id": "1910.02844", "submitter": "Sripad Krishna Devalla", "authors": "Haris Cheong, Sripad Krishna Devalla, Tan Hung Pham, Zhang Liang, Tin\n  Aung Tun, Xiaofei Wang, Shamira Perera, Leopold Schmetterer, Aung Tin, Craig\n  Boote, Alexandre H.Thiery, Michael J. A. Girard", "title": "DeshadowGAN: A Deep Learning Approach to Remove Shadows from Optical\n  Coherence Tomography Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Purpose: To remove retinal shadows from optical coherence tomography (OCT)\nimages of the optic nerve head(ONH).\n  Methods:2328 OCT images acquired through the center of the ONH using a\nSpectralis OCT machine for both eyes of 13 subjects were used to train a\ngenerative adversarial network (GAN) using a custom loss function. Image\nquality was assessed qualitatively (for artifacts) and quantitatively using the\nintralayer contrast: a measure of shadow visibility ranging from 0\n(shadow-free) to 1 (strong shadow) and compared to compensated images. This was\ncomputed in the Retinal Nerve Fiber Layer (RNFL), the Inner Plexiform Layer\n(IPL), the Photoreceptor layer (PR) and the Retinal Pigment Epithelium (RPE)\nlayers.\n  Results: Output images had improved intralayer contrast in all ONH tissue\nlayers. On average the intralayer contrast decreased by 33.7$\\pm$6.81%,\n28.8$\\pm$10.4%, 35.9$\\pm$13.0%, and43.0$\\pm$19.5%for the RNFL, IPL, PR, and RPE\nlayers respectively, indicating successful shadow removal across all depths.\nThis compared to 70.3$\\pm$22.7%, 33.9$\\pm$11.5%, 47.0$\\pm$11.2%,\n26.7$\\pm$19.0%for compensation. Output images were also free from artifacts\ncommonly observed with compensation.\n  Conclusions: DeshadowGAN significantly corrected blood vessel shadows in OCT\nimages of the ONH. Our algorithm may be considered as a pre-processing step to\nimprove the performance of a wide range of algorithms including those currently\nbeing used for OCT image segmentation, denoising, and classification.\n  Translational Relevance: DeshadowGAN could be integrated to existing OCT\ndevices to improve the diagnosis and prognosis of ocular pathologies.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:11:45 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Cheong", "Haris", ""], ["Devalla", "Sripad Krishna", ""], ["Pham", "Tan Hung", ""], ["Liang", "Zhang", ""], ["Tun", "Tin Aung", ""], ["Wang", "Xiaofei", ""], ["Perera", "Shamira", ""], ["Schmetterer", "Leopold", ""], ["Tin", "Aung", ""], ["Boote", "Craig", ""], ["Thiery", "Alexandre H.", ""], ["Girard", "Michael J. A.", ""]]}, {"id": "1910.02923", "submitter": "Samuel Budd", "authors": "Samuel Budd, Emma C Robinson, Bernhard Kainz", "title": "A Survey on Active Learning and Human-in-the-Loop Deep Learning for\n  Medical Image Analysis", "comments": "Medical Image Analysis Volume 71 2021\n  https://doi.org/10.1016/j.media.2021.102062", "journal-ref": "Medical Image Analysis, Volume 71, 2021, 102062, ISSN 1361-8415", "doi": "10.1016/j.media.2021.102062", "report-no": null, "categories": "cs.LG cs.CV cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fully automatic deep learning has become the state-of-the-art technique for\nmany tasks including image acquisition, analysis and interpretation, and for\nthe extraction of clinically useful information for computer-aided detection,\ndiagnosis, treatment planning, intervention and therapy. However, the unique\nchallenges posed by medical image analysis suggest that retaining a human end\nuser in any deep learning enabled system will be beneficial. In this review we\ninvestigate the role that humans might play in the development and deployment\nof deep learning enabled diagnostic applications and focus on techniques that\nwill retain a significant input from a human end user. Human-in-the-Loop\ncomputing is an area that we see as increasingly important in future research\ndue to the safety-critical nature of working in the medical domain. We evaluate\nfour key areas that we consider vital for deep learning in the clinical\npractice: (1) Active Learning to choose the best data to annotate for optimal\nmodel performance; (2) Interaction with model outputs - using iterative\nfeedback to steer models to optima for a given prediction and offering\nmeaningful ways to interpret and respond to predictions; (3) Practical\nconsiderations - developing full scale applications and the key considerations\nthat need to be made before deployment; (4) Future Prospective and Unanswered\nQuestions - knowledge gaps and related research fields that will benefit\nhuman-in-the-loop computing as they evolve. We offer our opinions on the most\npromising directions of research and how various aspects of each area might be\nunified towards common goals.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:24:33 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 12:07:48 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Budd", "Samuel", ""], ["Robinson", "Emma C", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1910.02932", "submitter": "Kashif Ahmad Dr", "authors": "Kashif Ahmad, Konstantin Pogorelov, Mohib Ullah, Michael Riegler,\n  Nicola Conci, Johannes Langguth, Ala Al-Fuqaha", "title": "Multi-Modal Machine Learning for Flood Detection in News, Social Media\n  and Satellite Sequences", "comments": null, "journal-ref": "MediaEval 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our methods for the MediaEval 2019 Mul-timedia\nSatellite Task, which is aiming to extract complementaryinformation associated\nwith adverse events from Social Media andsatellites. For the first challenge,\nwe propose a framework jointly uti-lizing colour, object and scene-level\ninformation to predict whetherthe topic of an article containing an image is a\nflood event or not.Visual features are combined using early and late fusion\ntechniquesachieving an average F1-score of82.63,82.40,81.40and76.77. Forthe\nmulti-modal flood level estimation, we rely on both visualand textual\ninformation achieving an average F1-score of58.48and46.03, respectively.\nFinally, for the flooding detection in time-based satellite image sequences we\nused a combination of classicalcomputer-vision and machine learning approaches\nachieving anaverage F1-score of58.82%\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:49:59 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Ahmad", "Kashif", ""], ["Pogorelov", "Konstantin", ""], ["Ullah", "Mohib", ""], ["Riegler", "Michael", ""], ["Conci", "Nicola", ""], ["Langguth", "Johannes", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "1910.02935", "submitter": "Aydan Gasimova", "authors": "Aydan Gasimova", "title": "Automated Enriched Medical Concept Generation for Chest X-ray Images", "comments": "MICCAI ML-CDS Workshop 2019", "journal-ref": null, "doi": "10.1007/978-3-030-33850-3_10", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decision support tools that rely on supervised learning require large amounts\nof expert annotations. Using past radiological reports obtained from hospital\narchiving systems has many advantages as training data above manual\nsingle-class labels: they are expert annotations available in large quantities,\ncovering a population-representative variety of pathologies, and they provide\nadditional context to pathology diagnoses, such as anatomical location and\nseverity. Learning to auto-generate such reports from images present many\nchallenges such as the difficulty in representing and generating long,\nunstructured textual information, accounting for spelling errors and\nrepetition/redundancy, and the inconsistency across different annotators. We\ntherefore propose to first learn visually-informative medical concepts from raw\nreports, and, using the concept predictions as image annotations, learn to\nauto-generate structured reports directly from images. We validate our approach\non the OpenI [2] chest x-ray dataset, which consists of frontal and lateral\nviews of chest x-ray images, their corresponding raw textual reports and manual\nmedical subject heading (MeSH ) annotations made by radiologists.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:52:37 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gasimova", "Aydan", ""]]}, {"id": "1910.02940", "submitter": "Hang Gao", "authors": "Hang Gao, Xizhou Zhu, Steve Lin, Jifeng Dai", "title": "Deformable Kernels: Adapting Effective Receptive Fields for Object\n  Deformation", "comments": "Project page:\n  http://people.eecs.berkeley.edu/~hangg/deformable-kernels/. Accepted as\n  conference paper in ICLR 2020. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are not aware of an object's geometric variations,\nwhich leads to inefficient utilization of model and data capacity. To overcome\nthis issue, recent works on deformation modeling seek to spatially reconfigure\nthe data towards a common arrangement such that semantic recognition suffers\nless from deformation. This is typically done by augmenting static operators\nwith learned free-form sampling grids in the image space, dynamically tuned to\nthe data and task for adapting the receptive field. Yet adapting the receptive\nfield does not quite reach the actual goal -- what really matters to the\nnetwork is the \"effective\" receptive field (ERF), which reflects how much each\npixel contributes. It is thus natural to design other approaches to adapt the\nERF directly during runtime.\n  In this work, we instantiate one possible solution as Deformable Kernels\n(DKs), a family of novel and generic convolutional operators for handling\nobject deformations by directly adapting the ERF while leaving the receptive\nfield untouched. At the heart of our method is the ability to resample the\noriginal kernel space towards recovering the deformation of objects. This\napproach is justified with theoretical insights that the ERF is strictly\ndetermined by data sampling locations and kernel values. We implement DKs as\ngeneric drop-in replacements of rigid kernels and conduct a series of empirical\nstudies whose results conform with our theories. Over several tasks and\nstandard base models, our approach compares favorably against prior works that\nadapt during runtime. In addition, further experiments suggest a working\nmechanism orthogonal and complementary to previous works.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:58:10 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 07:10:24 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Gao", "Hang", ""], ["Zhu", "Xizhou", ""], ["Lin", "Steve", ""], ["Dai", "Jifeng", ""]]}, {"id": "1910.02974", "submitter": "Marcella Cornia", "authors": "Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara", "title": "SMArT: Training Shallow Memory-aware Transformers for Robotic\n  Explainability", "comments": "ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate natural language explanations conditioned on the\nvisual perception is a crucial step towards autonomous agents which can explain\nthemselves and communicate with humans. While the research efforts in image and\nvideo captioning are giving promising results, this is often done at the\nexpense of the computational requirements of the approaches, limiting their\napplicability to real contexts. In this paper, we propose a fully-attentive\ncaptioning algorithm which can provide state-of-the-art performances on\nlanguage generation while restricting its computational demands. Our model is\ninspired by the Transformer model and employs only two Transformer layers in\nthe encoding and decoding stages. Further, it incorporates a novel memory-aware\nencoding of image regions. Experiments demonstrate that our approach achieves\ncompetitive results in terms of caption quality while featuring reduced\ncomputational demands. Further, to evaluate its applicability on autonomous\nagents, we conduct experiments on simulated scenes taken from the perspective\nof domestic robots.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:03:14 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 18:15:50 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 14:35:47 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1910.02989", "submitter": "Kai Zhang", "authors": "Kai Zhang, Jin Sun, Noah Snavely", "title": "Leveraging Vision Reconstruction Pipelines for Satellite Imagery", "comments": "Project Page: https://kai-46.github.io/VisSat/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing 3D geometry from satellite imagery is an important topic of\nresearch. However, disparities exist between how this 3D reconstruction problem\nis handled in the remote sensing context and how multi-view reconstruction\npipelines have been developed in the computer vision community. In this paper,\nwe explore whether state-of-the-art reconstruction pipelines from the vision\ncommunity can be applied to the satellite imagery. Along the way, we address\nseveral challenges adapting vision-based structure from motion and multi-view\nstereo methods. We show that vision pipelines can offer competitive speed and\naccuracy in the satellite context.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:14:28 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 15:23:32 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Zhang", "Kai", ""], ["Sun", "Jin", ""], ["Snavely", "Noah", ""]]}, {"id": "1910.02993", "submitter": "Daniel Y. Fu", "authors": "Daniel Y. Fu, Will Crichton, James Hong, Xinwei Yao, Haotian Zhang,\n  Anh Truong, Avanika Narayan, Maneesh Agrawala, Christopher R\\'e, Kayvon\n  Fatahalian", "title": "Rekall: Specifying Video Events using Compositions of Spatiotemporal\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world video analysis applications require the ability to identify\ndomain-specific events in video, such as interviews and commercials in TV news\nbroadcasts, or action sequences in film. Unfortunately, pre-trained models to\ndetect all the events of interest in video may not exist, and training new\nmodels from scratch can be costly and labor-intensive. In this paper, we\nexplore the utility of specifying new events in video in a more traditional\nmanner: by writing queries that compose outputs of existing, pre-trained\nmodels. To write these queries, we have developed Rekall, a library that\nexposes a data model and programming model for compositional video event\nspecification. Rekall represents video annotations from different sources\n(object detectors, transcripts, etc.) as spatiotemporal labels associated with\ncontinuous volumes of spacetime in a video, and provides operators for\ncomposing labels into queries that model new video events. We demonstrate the\nuse of Rekall in analyzing video from cable TV news broadcasts, films,\nstatic-camera vehicular video streams, and commercial autonomous vehicle logs.\nIn these efforts, domain experts were able to quickly (in a few hours to a day)\nauthor queries that enabled the accurate detection of new events (on par with,\nand in some cases much more accurate than, learned approaches) and to rapidly\nretrieve video clips for human-in-the-loop tasks such as video content curation\nand training data curation. Finally, in a user study, novice users of Rekall\nwere able to author queries to retrieve new events in video given just one hour\nof query development time.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:18:37 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Fu", "Daniel Y.", ""], ["Crichton", "Will", ""], ["Hong", "James", ""], ["Yao", "Xinwei", ""], ["Zhang", "Haotian", ""], ["Truong", "Anh", ""], ["Narayan", "Avanika", ""], ["Agrawala", "Maneesh", ""], ["R\u00e9", "Christopher", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "1910.03013", "submitter": "Vladimir Katkovnik", "authors": "Vladimir Katkovnik, Igor Shevkunov, Karen Egiazarian", "title": "Hyperspectral holography and spectroscopy: computational features of\n  inverse discrete cosine transform", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadband hyperspectral digital holography and Fourier transform spectroscopy\nare important instruments in various science and application fields. In the\ndigital hyperspectral holography and spectroscopy the variable of interest are\nobtained as inverse discrete cosine transforms of observed diffractive\nintensity patterns. In these notes, we provide a variety of algorithms for the\ninverse cosine transform with the proofs of perfect spectrum reconstruction, as\nwell as we discuss and illustrate some nontrivial features of these algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 12:15:16 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Katkovnik", "Vladimir", ""], ["Shevkunov", "Igor", ""], ["Egiazarian", "Karen", ""]]}, {"id": "1910.03025", "submitter": "Hyenkyun Woo", "authors": "Hyenkyun Woo", "title": "Bregman-divergence-guided Legendre exponential dispersion model with\n  finite cumulants (K-LED)", "comments": "21pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.IT cs.LG math.IT math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential dispersion model is a useful framework in machine learning and\nstatistics. Primarily, thanks to the additive structure of the model, it can be\nachieved without difficulty to estimate parameters including mean. However,\ntight conditions on cumulant function, such as analyticity, strict convexity,\nand steepness, reduce the class of exponential dispersion model. In this work,\nwe present relaxed exponential dispersion model K-LED (Legendre exponential\ndispersion model with K cumulants). The cumulant function of the proposed model\nis a convex function of Legendre type having continuous partial derivatives of\nK-th order on the interior of a convex domain. Most of the K-LED models are\ndeveloped via Bregman-divergence-guided log-concave density function with\ncoercivity shape constraints. The main advantage of the proposed model is that\nthe first cumulant (or the mean parameter space) of the 1-LED model is easily\ncomputed through the extended global optimum property of Bregman divergence. An\nextended normal distribution is introduced as an example of 1-LED based on\nTweedie distribution. On top of that, we present 2-LED satisfying mean-variance\nrelation of quasi-likelihood function. There is an equivalence between a\nsubclass of quasi-likelihood function and a regular 2-LED model, of which the\ncanonical parameter space is open. A typical example is a regular 2-LED model\nwith power variance function, i.e., a variance is in proportion to the power of\nthe mean of observations. This model is equivalent to a subclass of\nbeta-divergence (or a subclass of quasi-likelihood function with power variance\nfunction). Furthermore, a new parameterized K-LED model, the cumulant function\nof which is the convex extended logistic loss function, is proposed. This model\nincludes Bernoulli distribution and Poisson distribution.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 11:24:31 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Woo", "Hyenkyun", ""]]}, {"id": "1910.03084", "submitter": "Kamran Kowsari", "authors": "Rasoul Sali, Lubaina Ehsan, Kamran Kowsari, Marium Khan, Christopher\n  A. Moskaluk, Sana Syed, Donald E. Brown", "title": "CeliacNet: Celiac Disease Severity Diagnosis on Duodenal\n  Histopathological Images Using Deep Residual Networks", "comments": "accepted at IEEE International Conference on Bioinformatics and\n  Biomedicine (IEEE BIBM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Celiac Disease (CD) is a chronic autoimmune disease that affects the small\nintestine in genetically predisposed children and adults. Gluten exposure\ntriggers an inflammatory cascade which leads to compromised intestinal barrier\nfunction. If this enteropathy is unrecognized, this can lead to anemia,\ndecreased bone density, and, in longstanding cases, intestinal cancer. The\nprevalence of the disorder is 1% in the United States. An intestinal (duodenal)\nbiopsy is considered the \"gold standard\" for diagnosis. The mild CD might go\nunnoticed due to non-specific clinical symptoms or mild histologic features. In\nour current work, we trained a model based on deep residual networks to\ndiagnose CD severity using a histological scoring system called the modified\nMarsh score. The proposed model was evaluated using an independent set of 120\nwhole slide images from 15 CD patients and achieved an AUC greater than 0.96 in\nall classes. These results demonstrate the diagnostic power of the proposed\nmodel for CD severity classification using histological images.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 21:06:41 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Sali", "Rasoul", ""], ["Ehsan", "Lubaina", ""], ["Kowsari", "Kamran", ""], ["Khan", "Marium", ""], ["Moskaluk", "Christopher A.", ""], ["Syed", "Sana", ""], ["Brown", "Donald E.", ""]]}, {"id": "1910.03088", "submitter": "Wei Zhan", "authors": "Wei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey Clausse, Maximilian\n  Naumann, Julius Kummerle, Hendrik Konigshof, Christoph Stiller, Arnaud de La\n  Fortelle, and Masayoshi Tomizuka", "title": "INTERACTION Dataset: An INTERnational, Adversarial and Cooperative\n  moTION Dataset in Interactive Driving Scenarios with Semantic Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior-related research areas such as motion prediction/planning,\nrepresentation/imitation learning, behavior modeling/generation, and algorithm\ntesting, require support from high-quality motion datasets containing\ninteractive driving scenarios with different driving cultures. In this paper,\nwe present an INTERnational, Adversarial and Cooperative moTION dataset\n(INTERACTION dataset) in interactive driving scenarios with semantic maps. Five\nfeatures of the dataset are highlighted. 1) The interactive driving scenarios\nare diverse, including urban/highway/ramp merging and lane changes, roundabouts\nwith yield/stop signs, signalized intersections, intersections with\none/two/all-way stops, etc. 2) Motion data from different countries and\ndifferent continents are collected so that driving preferences and styles in\ndifferent cultures are naturally included. 3) The driving behavior is highly\ninteractive and complex with adversarial and cooperative motions of various\ntraffic participants. Highly complex behavior such as negotiations,\naggressive/irrational decisions and traffic rule violations are densely\ncontained in the dataset, while regular behavior can also be found from\ncautious car-following, stop, left/right/U-turn to rational lane-change and\ncycling and pedestrian crossing, etc. 4) The levels of criticality span wide,\nfrom regular safe operations to dangerous, near-collision maneuvers. Real\ncollision, although relatively slight, is also included. 5) Maps with complete\nsemantic information are provided with physical layers, reference lines,\nlanelet connections and traffic rules. The data is recorded from drones and\ntraffic cameras. Statistics of the dataset in terms of number of entities and\ninteraction density are also provided, along with some utilization examples in\na variety of behavior-related research areas. The dataset can be downloaded via\nhttps://interaction-dataset.com.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 17:26:51 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Zhan", "Wei", ""], ["Sun", "Liting", ""], ["Wang", "Di", ""], ["Shi", "Haojie", ""], ["Clausse", "Aubrey", ""], ["Naumann", "Maximilian", ""], ["Kummerle", "Julius", ""], ["Konigshof", "Hendrik", ""], ["Stiller", "Christoph", ""], ["de La Fortelle", "Arnaud", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1910.03119", "submitter": "Chun Pong Lau", "authors": "Chun Pong Lau, Hossein Souri, Rama Chellappa", "title": "ATFaceGAN: Single Face Image Restoration and Recognition from\n  Atmospheric Turbulence", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": "10.1109/FG47880.2020.00012", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image degradation due to atmospheric turbulence is common while capturing\nimages at long ranges. To mitigate the degradation due to turbulence which\nincludes deformation and blur, we propose a generative single frame restoration\nalgorithm which disentangles the blur and deformation due to turbulence and\nreconstructs a restored image. The disentanglement is achieved by decomposing\nthe distortion due to turbulence into blur and deformation components using\ndeblur generator and deformation correction generator respectively. Two paths\nof restoration are implemented to regularize the disentanglement and generate\ntwo restored images from one degraded image. A fusion function combines the\nfeatures of the restored images to reconstruct a sharp image with rich details.\nAdversarial and perceptual losses are added to reconstruct a sharp image and\nsuppress the artifacts respectively. Extensive experiments demonstrate the\neffectiveness of the proposed restoration algorithm, which achieves\nsatisfactory performance in face restoration and face recognition.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 22:44:32 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 04:56:39 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Lau", "Chun Pong", ""], ["Souri", "Hossein", ""], ["Chellappa", "Rama", ""]]}, {"id": "1910.03126", "submitter": "Jiunn-Kai Huang", "authors": "Jiunn-Kai Huang and Jessy W. Grizzle", "title": "Improvements to Target-Based 3D LiDAR to Camera Calibration", "comments": null, "journal-ref": "IEEE Access, vol. 8, 2020, pp. 134101-134110", "doi": "10.1109/ACCESS.2020.3010734", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The homogeneous transformation between a LiDAR and monocular camera is\nrequired for sensor fusion tasks, such as SLAM. While determining such a\ntransformation is not considered glamorous in any sense of the word, it is\nnonetheless crucial for many modern autonomous systems. Indeed, an error of a\nfew degrees in rotation or a few percent in translation can lead to 20 cm\ntranslation errors at a distance of 5 m when overlaying a LiDAR image on a\ncamera image. The biggest impediments to determining the transformation\naccurately are the relative sparsity of LiDAR point clouds and systematic\nerrors in their distance measurements. This paper proposes (1) the use of\ntargets of known dimension and geometry to ameliorate target pose estimation in\nface of the quantization and systematic errors inherent in a LiDAR image of a\ntarget, and (2) a fitting method for the LiDAR to monocular camera\ntransformation that fundamentally assumes the camera image data is the most\naccurate information in one's possession.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 23:03:16 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 20:05:04 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 15:07:13 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Huang", "Jiunn-Kai", ""], ["Grizzle", "Jessy W.", ""]]}, {"id": "1910.03135", "submitter": "Ankur Handa", "authors": "Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang, Yu-Wei Chao, Qian\n  Wan, Stan Birchfield, Nathan Ratliff, Dieter Fox", "title": "DexPilot: Vision Based Teleoperation of Dexterous Robotic Hand-Arm\n  System", "comments": "17 pages, first version of DexPilot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Teleoperation offers the possibility of imparting robotic systems with\nsophisticated reasoning skills, intuition, and creativity to perform tasks.\nHowever, current teleoperation solutions for high degree-of-actuation (DoA),\nmulti-fingered robots are generally cost-prohibitive, while low-cost offerings\nusually provide reduced degrees of control. Herein, a low-cost, vision based\nteleoperation system, DexPilot, was developed that allows for complete control\nover the full 23 DoA robotic system by merely observing the bare human hand.\nDexPilot enables operators to carry out a variety of complex manipulation tasks\nthat go beyond simple pick-and-place operations. This allows for collection of\nhigh dimensional, multi-modality, state-action data that can be leveraged in\nthe future to learn sensorimotor policies for challenging manipulation tasks.\nThe system performance was measured through speed and reliability metrics\nacross two human demonstrators on a variety of tasks. The videos of the\nexperiments can be found at https://sites.google.com/view/dex-pilot.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 23:43:32 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 20:58:22 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Handa", "Ankur", ""], ["Van Wyk", "Karl", ""], ["Yang", "Wei", ""], ["Liang", "Jacky", ""], ["Chao", "Yu-Wei", ""], ["Wan", "Qian", ""], ["Birchfield", "Stan", ""], ["Ratliff", "Nathan", ""], ["Fox", "Dieter", ""]]}, {"id": "1910.03151", "submitter": "Qilong Wang", "authors": "Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, Qinghua\n  Hu", "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural\n  Networks", "comments": "Accepted to CVPR 2020; Project Page:\n  https://github.com/BangguWu/ECANet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, channel attention mechanism has demonstrated to offer great\npotential in improving the performance of deep convolutional neural networks\n(CNNs). However, most existing methods dedicate to developing more\nsophisticated attention modules for achieving better performance, which\ninevitably increase model complexity. To overcome the paradox of performance\nand complexity trade-off, this paper proposes an Efficient Channel Attention\n(ECA) module, which only involves a handful of parameters while bringing clear\nperformance gain. By dissecting the channel attention module in SENet, we\nempirically show avoiding dimensionality reduction is important for learning\nchannel attention, and appropriate cross-channel interaction can preserve\nperformance while significantly decreasing model complexity. Therefore, we\npropose a local cross-channel interaction strategy without dimensionality\nreduction, which can be efficiently implemented via $1D$ convolution.\nFurthermore, we develop a method to adaptively select kernel size of $1D$\nconvolution, determining coverage of local cross-channel interaction. The\nproposed ECA module is efficient yet effective, e.g., the parameters and\ncomputations of our modules against backbone of ResNet50 are 80 vs. 24.37M and\n4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more\nthan 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on\nimage classification, object detection and instance segmentation with backbones\nof ResNets and MobileNetV2. The experimental results show our module is more\nefficient while performing favorably against its counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 01:14:26 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 08:26:48 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 00:58:55 GMT"}, {"version": "v4", "created": "Tue, 7 Apr 2020 13:53:51 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Wang", "Qilong", ""], ["Wu", "Banggu", ""], ["Zhu", "Pengfei", ""], ["Li", "Peihua", ""], ["Zuo", "Wangmeng", ""], ["Hu", "Qinghua", ""]]}, {"id": "1910.03152", "submitter": "Ziwen Wang", "authors": "Henry H. Yu, Jiang Liu, Hao Sun, Ziwen Wang, Haotian Zhang", "title": "GetNet: Get Target Area for Image Pairing", "comments": "Accepted by Image and Vision Computing New Zealand (IVCNZ 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image pairing is an important research task in the field of computer vision.\nAnd finding image pairs containing objects of the same category is the basis of\nmany tasks such as tracking and person re-identification, etc., and it is also\nthe focus of our research. Existing traditional methods and deep learning-based\nmethods have some degree of defects in speed or accuracy. In this paper, we\nmade improvements on the Siamese network and proposed GetNet. The proposed\nmethod GetNet combines STN and Siamese network to get the target area first and\nthen perform subsequent processing. Experiments show that our method achieves\ncompetitive results in speed and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 01:18:33 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Yu", "Henry H.", ""], ["Liu", "Jiang", ""], ["Sun", "Hao", ""], ["Wang", "Ziwen", ""], ["Zhang", "Haotian", ""]]}, {"id": "1910.03157", "submitter": "Alan Wu", "authors": "Alan Wu, AJ Piergiovanni, Michael S. Ryoo", "title": "Model-based Behavioral Cloning with Future Image Similarity Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a visual imitation learning framework that enables learning of\nrobot action policies solely based on expert samples without any robot trials.\nRobot exploration and on-policy trials in a real-world environment could often\nbe expensive/dangerous. We present a new approach to address this problem by\nlearning a future scene prediction model solely on a collection of expert\ntrajectories consisting of unlabeled example videos and actions, and by\nenabling generalized action cloning using future image similarity. The robot\nlearns to visually predict the consequences of taking an action, and obtains\nthe policy by evaluating how similar the predicted future image is to an expert\nimage. We develop a stochastic action-conditioned convolutional autoencoder,\nand present how we take advantage of future images for robot learning. We\nconduct experiments in simulated and real-life environments using a ground\nmobility robot with and without obstacles, and compare our models to multiple\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 01:32:54 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Wu", "Alan", ""], ["Piergiovanni", "AJ", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1910.03159", "submitter": "Daniel Barry", "authors": "Daniel Barry and Munir Shah and Merel Keijsers and Humayun Khan and\n  Banon Hopman", "title": "xYOLO: A Model For Real-Time Object Detection In Humanoid Soccer On\n  Low-End Hardware", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of onboard vision processing for areas such as the\ninternet of things (IoT), edge computing and autonomous robots, there is\nincreasing demand for computationally efficient convolutional neural network\n(CNN) models to perform real-time object detection on resource constraints\nhardware devices. Tiny-YOLO is generally considered as one of the faster object\ndetectors for low-end devices and is the basis for our work. Our experiments on\nthis network have shown that Tiny-YOLO can achieve 0.14 frames per second(FPS)\non the Raspberry Pi 3 B, which is too slow for soccer playing autonomous\nhumanoid robots detecting goal and ball objects. In this paper we propose an\nadaptation to the YOLO CNN model named xYOLO, that can achieve object detection\nat a speed of 9.66 FPS on the Raspberry Pi 3 B. This is achieved by trading an\nacceptable amount of accuracy, making the network approximately 70 times faster\nthan Tiny-YOLO. Greater inference speed-ups were also achieved on a desktop CPU\nand GPU. Additionally we contribute an annotated Darknet dataset for goal and\nball detection.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 01:33:52 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Barry", "Daniel", ""], ["Shah", "Munir", ""], ["Keijsers", "Merel", ""], ["Khan", "Humayun", ""], ["Hopman", "Banon", ""]]}, {"id": "1910.03166", "submitter": "Pingping Zhang Dr", "authors": "Pingping Zhang and Wei Liu and Yinjie Lei and Hongyu Wang and Huchuan\n  Lu", "title": "Deep Multiphase Level Set for Scene Parsing", "comments": "12 pages, 8 tables, 8 figures", "journal-ref": null, "doi": "10.1109/TIP.2019.2957915", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Fully Convolutional Network (FCN) seems to be the go-to\narchitecture for image segmentation, including semantic scene parsing. However,\nit is difficult for a generic FCN to discriminate pixels around the object\nboundaries, thus FCN based methods may output parsing results with inaccurate\nboundaries. Meanwhile, level set based active contours are superior to the\nboundary estimation due to the sub-pixel accuracy that they achieve. However,\nthey are quite sensitive to initial settings. To address these limitations, in\nthis paper we propose a novel Deep Multiphase Level Set (DMLS) method for\nsemantic scene parsing, which efficiently incorporates multiphase level sets\ninto deep neural networks. The proposed method consists of three modules, i.e.,\nrecurrent FCNs, adaptive multiphase level set, and deeply supervised learning.\nMore specifically, recurrent FCNs learn multi-level representations of input\nimages with different contexts. Adaptive multiphase level set drives the\ndiscriminative contour for each semantic class, which makes use of the\nadvantages of both global and local information. In each time-step of the\nrecurrent FCNs, deeply supervised learning is incorporated for model training.\nExtensive experiments on three public benchmarks have shown that our proposed\nmethod achieves new state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 01:58:24 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 02:53:07 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Zhang", "Pingping", ""], ["Liu", "Wei", ""], ["Lei", "Yinjie", ""], ["Wang", "Hongyu", ""], ["Lu", "Huchuan", ""]]}, {"id": "1910.03182", "submitter": "Kerry A Nice", "authors": "Kerry A. Nice, Jasper S. Wijnands, Ariane Middel, Jingcheng Wang,\n  Yiming Qiu, Nan Zhao, Jason Thompson, Gideon D.P.A. Aschwanden, Haifeng Zhao,\n  Mark Stevenson", "title": "Sky pixel detection in outdoor imagery using an adaptive algorithm and\n  machine learning", "comments": "This revision accepted in Urban Climate. 17 pages, 12 figures", "journal-ref": null, "doi": "10.1016/j.uclim.2019.100572", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision techniques enable automated detection of sky pixels in\noutdoor imagery. In urban climate, sky detection is an important first step in\ngathering information about urban morphology and sky view factors. However,\nobtaining accurate results remains challenging and becomes even more complex\nusing imagery captured under a variety of lighting and weather conditions.\n  To address this problem, we present a new sky pixel detection system\ndemonstrated to produce accurate results using a wide range of outdoor imagery\ntypes. Images are processed using a selection of mean-shift segmentation,\nK-means clustering, and Sobel filters to mark sky pixels in the scene. The\nalgorithm for a specific image is chosen by a convolutional neural network,\ntrained with 25,000 images from the Skyfinder data set, reaching 82% accuracy\nfor the top three classes. This selection step allows the sky marking to follow\nan adaptive process and to use different techniques and parameters to best suit\na particular image. An evaluation of fourteen different techniques and\nparameter sets shows that no single technique can perform with high accuracy\nacross varied Skyfinder and Google Street View data sets. However, by using our\nadaptive process, large increases in accuracy are observed. The resulting\nsystem is shown to perform better than other published techniques.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 02:45:38 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 23:01:34 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Nice", "Kerry A.", ""], ["Wijnands", "Jasper S.", ""], ["Middel", "Ariane", ""], ["Wang", "Jingcheng", ""], ["Qiu", "Yiming", ""], ["Zhao", "Nan", ""], ["Thompson", "Jason", ""], ["Aschwanden", "Gideon D. P. A.", ""], ["Zhao", "Haifeng", ""], ["Stevenson", "Mark", ""]]}, {"id": "1910.03188", "submitter": "Rahul-Vigneswaran K", "authors": "Rahul-Vigneswaran K, Sachin-Kumar S, Neethu Mohan, Soman KP", "title": "Dynamic Mode Decomposition based feature for Image Classification", "comments": "Selected for Spotlight presentation at TENCON 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Irrespective of the fact that Machine learning has produced groundbreaking\nresults, it demands an enormous amount of data in order to perform so. Even\nthough data production has been in its all-time high, almost all the data is\nunlabelled, hence making them unsuitable for training the algorithms. This\npaper proposes a novel method of extracting the features using Dynamic Mode\nDecomposition (DMD). The experiment is performed using data samples from\nImagenet. The learning is done using SVM-linear, SVM-RBF, Random Kitchen Sink\napproach (RKS). The results have shown that DMD features with RKS give\ncompeting results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 03:09:42 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["K", "Rahul-Vigneswaran", ""], ["S", "Sachin-Kumar", ""], ["Mohan", "Neethu", ""], ["KP", "Soman", ""]]}, {"id": "1910.03191", "submitter": "Matthew Hancock PhD", "authors": "Matthew C Hancock and Jerry F Magnan", "title": "Level set image segmentation with velocity term learned from data with\n  applications to lung nodule segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Lung nodule segmentation, i.e., the algorithmic delineation of the\nlung nodule surface, is a fundamental component of computational nodule\nanalysis pipelines. We propose a new method for segmentation that is a machine\nlearning based extension of current approaches, using labeled image examples to\nimprove its accuracy.\n  Approach: We introduce an extension of the standard level set image\nsegmentation method where the velocity function is learned from data via\nmachine learning regression methods, rather than a priori designed. Instead,\nthe method employs a set of features to learn a velocity function that guides\nthe level set evolution from initialization.\n  Results: We apply the method to image volumes of lung nodules from CT scans\nin the publicly available LIDC dataset, obtaining an average intersection over\nunion score of 0.7185($\\pm$0.1114), which is competitive with other methods. We\nanalyze segmentation performance by anatomical and appearance-based categories\nof the nodules, finding that the method performs better for isolated nodules\nwith well-defined margins. We find that the segmentation performance for\nnodules in more complex surroundings and having more complex CT appearance is\nimproved with the addition of combined global-local features.\n  Conclusions: The level set machine learning segmentation approach proposed\nherein is competitive with current methods. It provides accurate lung nodule\nsegmentation results in a variety of anatomical contexts.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 03:13:17 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 02:26:22 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 01:42:22 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Hancock", "Matthew C", ""], ["Magnan", "Jerry F", ""]]}, {"id": "1910.03213", "submitter": "Wojciech Michal Matkowski", "authors": "Wojciech Michal Matkowski, Frodo Kin Sun Chan, Adams Wai Kin Kong", "title": "A Study on Wrist Identification for Forensic Investigation", "comments": null, "journal-ref": "Image and Vision Computing, vol. 88, August 2019, pp 96-112", "doi": "10.1016/j.imavis.2019.05.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Criminal and victim identification based on crime scene images is an\nimportant part of forensic investigation. Criminals usually avoid\nidentification by covering their faces and tattoos in the evidence images,\nwhich are taken in uncontrolled environments. Existing identification methods,\nwhich make use of biometric traits, such as vein, skin mark, height, skin\ncolor, weight, race, etc., are considered for solving this problem. The soft\nbiometric traits, including skin color, gender, height, weight and race,\nprovide useful information but not distinctive enough. Veins and skin marks are\nlimited to high resolution images and some body sites may neither have enough\nskin marks nor clear veins. Terrorists and rioters tend to expose their wrists\nin a gesture of triumph, greeting or salute, while paedophiles usually show\nthem when touching victims. However, wrists were neglected by the biometric\ncommunity for forensic applications. In this paper, a wrist identification\nalgorithm, which includes skin segmentation, key point localization, image to\ntemplate alignment, large feature set extraction, and classification, is\nproposed. The proposed algorithm is evaluated on NTU-Wrist-Image-Database-v1,\nwhich consists of 3945 images from 731 different wrists, including 205 pairs of\nwrist images collected from the Internet, taken under uneven illuminations with\ndifferent poses and resolutions. The experimental results show that wrist is a\nuseful clue for criminal and victim identification. Keywords: biometrics,\ncriminal and victim identification, forensics, wrist.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 05:04:11 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Matkowski", "Wojciech Michal", ""], ["Chan", "Frodo Kin Sun", ""], ["Kong", "Adams Wai Kin", ""]]}, {"id": "1910.03220", "submitter": "Kerry A Nice", "authors": "Kerry A. Nice, Jason Thompson, Jasper S. Wijnands, Gideon D.P.A.\n  Aschwanden, Mark Stevenson", "title": "The 'Paris-end' of town? Urban typology through machine learning", "comments": null, "journal-ref": null, "doi": "10.3390/urbansci4020027", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The confluence of recent advances in availability of geospatial information,\ncomputing power, and artificial intelligence offers new opportunities to\nunderstand how and where our cities differ or are alike. Departing from a\ntraditional `top-down' analysis of urban design features, this project analyses\nmillions of images of urban form (consisting of street view, satellite imagery,\nand street maps) to find shared characteristics. A (novel) neural network-based\nframework is trained with imagery from the largest 1692 cities in the world and\nthe resulting models are used to compare within-city locations from Melbourne\nand Sydney to determine the closest connections between these areas and their\ninternational comparators. This work demonstrates a new, consistent, and\nobjective method to begin to understand the relationship between cities and\ntheir health, transport, and environmental consequences of their design. The\nresults show specific advantages and disadvantages using each type of imagery.\nNeural networks trained with map imagery will be highly influenced by the mix\nof roads, public transport, and green and blue space as well as the structure\nof these elements. The colours of natural and built features stand out as\ndominant characteristics in satellite imagery. The use of street view imagery\nwill emphasise the features of a human scaled visual geography of streetscapes.\nFinally, and perhaps most importantly, this research also answers the age-old\nquestion, ``Is there really a `Paris-end' to your city?''.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 05:26:33 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Nice", "Kerry A.", ""], ["Thompson", "Jason", ""], ["Wijnands", "Jasper S.", ""], ["Aschwanden", "Gideon D. P. A.", ""], ["Stevenson", "Mark", ""]]}, {"id": "1910.03227", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Hossein Javidnia, Murhaf Hossari, Matthew Nicholson,\n  Killian McCabe, Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu, and\n  Fran\\c{c}ois Piti\\'e", "title": "Identifying Candidate Spaces for Advert Implantation", "comments": "Published in Proc. IEEE 7th International Conference on Computer\n  Science and Network Technology, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual advertising is an important and promising feature in the area of\nonline advertising. It involves integrating adverts onto live or recorded\nvideos for product placements and targeted advertisements. Such integration of\nadverts is primarily done by video editors in the post-production stage, which\nis cumbersome and time-consuming. Therefore, it is important to automatically\nidentify candidate spaces in a video frame, wherein new adverts can be\nimplanted. The candidate space should match the scene perspective, and also\nhave a high quality of experience according to human subjective judgment. In\nthis paper, we propose the use of a bespoke neural net that can assist the\nvideo editors in identifying candidate spaces. We benchmark our approach\nagainst several deep-learning architectures on a large-scale image dataset of\ncandidate spaces of outdoor scenes. Our work is the first of its kind in this\narea of multimedia and augmented reality applications, and achieves the best\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 06:12:53 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Javidnia", "Hossein", ""], ["Hossari", "Murhaf", ""], ["Nicholson", "Matthew", ""], ["McCabe", "Killian", ""], ["Nautiyal", "Atul", ""], ["Conran", "Clare", ""], ["Tang", "Jian", ""], ["Xu", "Wei", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "1910.03230", "submitter": "Wenhu Chen", "authors": "Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, Jingjing Liu", "title": "Meta Module Network for Compositional Visual Reasoning", "comments": "Accepted to WACV 21 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Module Network (NMN) exhibits strong interpretability and\ncompositionality thanks to its handcrafted neural modules with explicit\nmulti-hop reasoning capability. However, most NMNs suffer from two critical\ndrawbacks: 1) scalability: customized module for specific function renders it\nimpractical when scaling up to a larger set of functions in complex tasks; 2)\ngeneralizability: rigid pre-defined module inventory makes it difficult to\ngeneralize to unseen functions in new tasks/domains. To design a more powerful\nNMN architecture for practical use, we propose Meta Module Network (MMN)\ncentered on a novel meta module, which can take in function recipes and morph\ninto diverse instance modules dynamically. The instance modules are then woven\ninto an execution graph for complex visual reasoning, inheriting the strong\nexplainability and compositionality of NMN. With such a flexible instantiation\nmechanism, the parameters of instance modules are inherited from the central\nmeta module, retaining the same model complexity as the function set grows,\nwhich promises better scalability. Meanwhile, as functions are encoded into the\nembedding space, unseen functions can be readily represented based on its\nstructural similarity with previously observed ones, which ensures better\ngeneralizability. Experiments on GQA and CLEVR datasets validate the\nsuperiority of MMN over state-of-the-art NMN designs. Synthetic experiments on\nheld-out unseen functions from GQA dataset also demonstrate the strong\ngeneralizability of MMN. Our code and model are released in Github\nhttps://github.com/wenhuchen/Meta-Module-Network.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 06:28:24 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 19:39:46 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 20:28:29 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2020 17:55:05 GMT"}, {"version": "v5", "created": "Sun, 8 Nov 2020 02:52:51 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chen", "Wenhu", ""], ["Gan", "Zhe", ""], ["Li", "Linjie", ""], ["Cheng", "Yu", ""], ["Wang", "William", ""], ["Liu", "Jingjing", ""]]}, {"id": "1910.03239", "submitter": "Christoph Heindl", "authors": "Christoph Heindl, Markus Ikeda, Gernot St\\\"ubl, Andreas Pichler, Josef\n  Scharinger", "title": "Metric Pose Estimation for Human-Machine Interaction Using Monocular\n  Vision", "comments": "IROS 2019, Factory of the Future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of collaborative robotics in production requires new\nautomation technologies that take human and machine equally into account. In\nthis work, we describe a monocular camera based system to detect human-machine\ninteractions from a bird's-eye perspective. Our system predicts poses of humans\nand robots from a single wide-angle color image. Even though our approach works\non 2D color input, we lift the majority of detections to a metric 3D space. Our\nsystem merges pose information with predefined virtual sensors to coordinate\nhuman-machine interactions. We demonstrate the advantages of our system in\nthree use cases.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 07:00:05 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Heindl", "Christoph", ""], ["Ikeda", "Markus", ""], ["St\u00fcbl", "Gernot", ""], ["Pichler", "Andreas", ""], ["Scharinger", "Josef", ""]]}, {"id": "1910.03240", "submitter": "Ricard Durall Lopez", "authors": "Ricard Durall, Franz-Josef Pfreundt, Janis Keuper", "title": "Semi Few-Shot Attribute Translation", "comments": "arXiv admin note: text overlap with arXiv:1904.04232,\n  arXiv:1901.02199 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown remarkable success in image-to-image translation\nfor attribute transfer applications. However, most of existing approaches are\nbased on deep learning and require an abundant amount of labeled data to\nproduce good results, therefore limiting their applicability. In the same vein,\nrecent advances in meta-learning have led to successful implementations with\nlimited available data, allowing so-called few-shot learning.\n  In this paper, we address this limitation of supervised methods, by proposing\na novel approach based on GANs. These are trained in a meta-training manner,\nwhich allows them to perform image-to-image translations using just a few\nlabeled samples from a new target class. This work empirically demonstrates the\npotential of training a GAN for few shot image-to-image translation on hair\ncolor attribute synthesis tasks, opening the door to further research on\ngenerative transfer learning.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 07:02:19 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 15:59:54 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Durall", "Ricard", ""], ["Pfreundt", "Franz-Josef", ""], ["Keuper", "Janis", ""]]}, {"id": "1910.03244", "submitter": "Shijie Ai", "authors": "Shijie Ai, Lili Pan and Yazhou Ren", "title": "Self-Paced Deep Regression Forests for Facial Age Estimation", "comments": "7 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial age estimation is an important and challenging problem in computer\nvision. Existing approaches usually employ deep neural networks (DNNs) to fit\nthe mapping from facial features to age, even though there exist some noisy and\nconfusing samples. We argue that it is more desirable to distinguish noisy and\nconfusing facial images from regular ones, and alleviate the interference\narising from them. To this end, we propose self-paced deep regression forests\n(SP-DRFs) -- a gradual learning DNNs framework for age estimation. As the model\nis learned gradually, from simplicity to complexity, it tends to emphasize more\non reliable samples and avoid bad local minima. Moreover, the proposed\ncapped-likelihood function helps to exclude noisy samples in training,\nrendering our SP-DRFs significantly more robust. We demonstrate the efficacy of\nSP-DRFs on Morph II and FG-NET datasets, where our model achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 07:08:45 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 03:29:57 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 10:08:45 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 12:09:31 GMT"}, {"version": "v5", "created": "Fri, 17 Apr 2020 11:36:39 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Ai", "Shijie", ""], ["Pan", "Lili", ""], ["Ren", "Yazhou", ""]]}, {"id": "1910.03274", "submitter": "Priya Kansal Dr.", "authors": "Priya Kansal, Sabari Nathan", "title": "Eyenet: Attention based Convolutional Encoder-Decoder Network for Eye\n  Region Segmentation", "comments": "To be appear in ICCVW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the immersive development in the field of augmented and virtual reality,\naccurate and speedy eye-tracking is required. Facebook Research has organized a\nchallenge, named OpenEDS Semantic Segmentation challenge for per-pixel\nsegmentation of the key eye regions: the sclera, the iris, the pupil, and\neverything else (background). There are two constraints set for the\nparticipants viz MIOU and the computational complexity of the model. More\nrecently, researchers have achieved quite a good result using the convolutional\nneural networks (CNN) in segmenting eyeregions. However, the environmental\nchallenges involved in this task such as low resolution, blur, unusual glint\nand, illumination, off-angles, off-axis, use of glasses and different color of\niris region hinder the accuracy of segmentation. To address the challenges in\neye segmentation, the present work proposes a robust and computationally\nefficient attention-based convolutional encoder-decoder network for segmenting\nall the eye regions. Our model, named EyeNet, includes modified residual units\nas the backbone, two types of attention blocks and multi-scale supervision for\nsegmenting the aforesaid four eye regions. Our proposed model achieved a total\nscore of 0.974(EDS Evaluation metric) on test data, which demonstrates superior\nresults compared to the baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 08:43:27 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Kansal", "Priya", ""], ["Nathan", "Sabari", ""]]}, {"id": "1910.03334", "submitter": "Taoran Wei", "authors": "Taoran Wei, Danhua Cao, Xingru Jiang, Caiyun Zheng, Lizhe Liu", "title": "Defective samples simulation through Neural Style Transfer for automatic\n  surface defect segment", "comments": "To be published in 2019 International Conference on Optical\n  Instrument and Technology (OIT 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the lack of defect samples in industrial product quality inspection,\ntrained segmentation model tends to overfit when applied online. To address\nthis problem, we propose a defect sample simulation algorithm based on neural\nstyle transfer. The simulation algorithm requires only a small number of defect\nsamples for training, and can efficiently generate simulation samples for\nnext-step segmentation task. In our work, we introduce a masked histogram\nmatching module to maintain color consistency of the generated area and the\ntrue defect. To preserve the texture consistency with the surrounding pixels,\nwe take the fast style transfer algorithm to blend the generated area into the\nbackground. At the same time, we also use the histogram loss to further improve\nthe quality of the generated image. Besides, we propose a novel structure of\nsegment net to make it more suitable for defect segmentation task. We train the\nsegment net with the real defect samples and the generated simulation samples\nseparately on the button datasets. The results show that the F1 score of the\nmodel trained with only the generated simulation samples reaches 0.80, which is\nbetter than the real sample result.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 10:55:19 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Wei", "Taoran", ""], ["Cao", "Danhua", ""], ["Jiang", "Xingru", ""], ["Zheng", "Caiyun", ""], ["Liu", "Lizhe", ""]]}, {"id": "1910.03336", "submitter": "Kai Fischer", "authors": "Victor Vaquero, Kai Fischer, Francesc Moreno-Noguer, Alberto Sanfeliu,\n  Stefan Milz", "title": "Improving Map Re-localization with Deep 'Movable' Objects Segmentation\n  on 3D LiDAR Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization and Mapping is an essential component to enable Autonomous\nVehicles navigation, and requires an accuracy exceeding that of commercial\nGPS-based systems. Current odometry and mapping algorithms are able to provide\nthis accurate information. However, the lack of robustness of these algorithms\nagainst dynamic obstacles and environmental changes, even for short time\nperiods, forces the generation of new maps on every session without taking\nadvantage of previously obtained ones. In this paper we propose the use of a\ndeep learning architecture to segment movable objects from 3D LiDAR point\nclouds in order to obtain longer-lasting 3D maps. This will in turn allow for\nbetter, faster and more accurate re-localization and trajectoy estimation on\nsubsequent days. We show the effectiveness of our approach in a very dynamic\nand cluttered scenario, a supermarket parking lot. For that, we record several\nsequences on different days and compare localization errors with and without\nour movable objects segmentation method. Results show that we are able to\naccurately re-locate over a filtered map, consistently reducing trajectory\nerrors between an average of 35.1% with respect to a non-filtered map version\nand of 47.9% with respect to a standalone map created on the current session.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 11:11:27 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Vaquero", "Victor", ""], ["Fischer", "Kai", ""], ["Moreno-Noguer", "Francesc", ""], ["Sanfeliu", "Alberto", ""], ["Milz", "Stefan", ""]]}, {"id": "1910.03343", "submitter": "Jean-Benoit Delbrouck", "authors": "Jean-Benoit Delbrouck and Antoine Maiorca and Nathan Hubens and\n  St\\'ephane Dupont", "title": "Modulated Self-attention Convolutional Network for VQA", "comments": "Accepted at NeurIPS 2019 workshop: ViGIL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As new data-sets for real-world visual reasoning and compositional question\nanswering are emerging, it might be needed to use the visual feature extraction\nas a end-to-end process during training. This small contribution aims to\nsuggest new ideas to improve the visual processing of traditional convolutional\nnetwork for visual question answering (VQA). In this paper, we propose to\nmodulate by a linguistic input a CNN augmented with self-attention. We show\nencouraging relative improvements for future research in this direction.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 11:28:38 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 16:59:23 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Delbrouck", "Jean-Benoit", ""], ["Maiorca", "Antoine", ""], ["Hubens", "Nathan", ""], ["Dupont", "St\u00e9phane", ""]]}, {"id": "1910.03412", "submitter": "Arul Selvam Periyasamy", "authors": "Arul Selvam Periyasamy, Max Schwarz, and Sven Behnke", "title": "Refining 6D Object Pose Predictions using Abstract Render-and-Compare", "comments": "Accepted for IEEE-RAS International Conference on Humanoid Robots,\n  Toronto, Canada, to appear October 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robotic systems often require precise scene analysis capabilities, especially\nin unstructured, cluttered situations, as occurring in human-made environments.\nWhile current deep-learning based methods yield good estimates of object poses,\nthey often struggle with large amounts of occlusion and do not take\ninter-object effects into account. Vision as inverse graphics is a promising\nconcept for detailed scene analysis. A key element for this idea is a method\nfor inferring scene parameter updates from the rasterized 2D scene. However,\nthe rasterization process is notoriously difficult to invert, both due to the\nprojection and occlusion process, but also due to secondary effects such as\nlighting or reflections. We propose to remove the latter from the process by\nmapping the rasterized image into an abstract feature space learned in a\nself-supervised way from pixel correspondences. Using only a light-weight\ninverse rendering module, this allows us to refine 6D object pose estimations\nin highly cluttered scenes by optimizing a simple pixel-wise difference in the\nabstract image representation. We evaluate our approach on the challenging\nYCB-Video dataset, where it yields large improvements and demonstrates a large\nbasin of attraction towards the correct object poses.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 14:17:39 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Periyasamy", "Arul Selvam", ""], ["Schwarz", "Max", ""], ["Behnke", "Sven", ""]]}, {"id": "1910.03455", "submitter": "Abby Stylianou", "authors": "Abby Stylianou, Richard Souvenir and Robert Pless", "title": "TraffickCam: Explainable Image Matching For Sex Trafficking\n  Investigations", "comments": "Presented at AAAI FSS-19: Artificial Intelligence in Government and\n  Public Sector, Arlington, Virginia, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigations of sex trafficking sometimes have access to photographs of\nvictims in hotel rooms. These images directly link victims to places, which can\nhelp verify where victims have been trafficked or where traffickers might\noperate in the future. Current machine learning approaches give promising\nresults in image search to find the matching hotel. This paper explores\napproaches to make this end-to-end system better support government and law\nenforcement requirements, including improved performance, visualization\napproaches that explain what parts of the image led to a match, and\ninfrastructure to support exporting the results of a query.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:24:30 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Stylianou", "Abby", ""], ["Souvenir", "Richard", ""], ["Pless", "Robert", ""]]}, {"id": "1910.03468", "submitter": "Matteo Terzi", "authors": "Matteo Terzi, Gian Antonio Susto and Pratik Chaudhari", "title": "Directional Adversarial Training for Cost Sensitive Deep Learning\n  Classification Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications of Machine Learning it is of paramount\nimportance not only to provide accurate predictions, but also to ensure certain\nlevels of robustness. Adversarial Training is a training procedure aiming at\nproviding models that are robust to worst-case perturbations around predefined\npoints. Unfortunately, one of the main issues in adversarial training is that\nrobustness w.r.t. gradient-based attackers is always achieved at the cost of\nprediction accuracy. In this paper, a new algorithm, called Wasserstein\nProjected Gradient Descent (WPGD), for adversarial training is proposed. WPGD\nprovides a simple way to obtain cost-sensitive robustness, resulting in a finer\ncontrol of the robustness-accuracy trade-off. Moreover, WPGD solves an optimal\ntransport problem on the output space of the network and it can efficiently\ndiscover directions where robustness is required, allowing to control the\ndirectional trade-off between accuracy and robustness. The proposed WPGD is\nvalidated in this work on image recognition tasks with different benchmark\ndatasets and architectures. Moreover, real world-like datasets are often\nunbalanced: this paper shows that when dealing with such type of datasets, the\nperformance of adversarial training are mainly affected in term of standard\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:40:09 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Terzi", "Matteo", ""], ["Susto", "Gian Antonio", ""], ["Chaudhari", "Pratik", ""]]}, {"id": "1910.03472", "submitter": "Maurice Weber", "authors": "Maurice Weber, Cedric Renggli, Helmut Grabner, Ce Zhang", "title": "Observer Dependent Lossy Image Compression", "comments": "@German Conference on Pattern Recognition (DAGM GCPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently advanced the state-of-the-art in image\ncompression and surpassed many traditional compression algorithms. The training\nof such networks involves carefully trading off entropy of the latent\nrepresentation against reconstruction quality. The term quality crucially\ndepends on the observer of the images which, in the vast majority of\nliterature, is assumed to be human. In this paper, we aim to go beyond this\nnotion of compression quality and look at human visual perception and image\nclassification simultaneously. To that end, we use a family of loss functions\nthat allows to optimize deep image compression depending on the observer and to\ninterpolate between human perceived visual quality and classification accuracy,\nenabling a more unified view on image compression. Our extensive experiments\nshow that using perceptual loss functions to train a compression system\npreserves classification accuracy much better than traditional codecs such as\nBPG without requiring retraining of classifiers on compressed images. For\nexample, compressing ImageNet to 0.25 bpp reduces Inception-ResNet\nclassification accuracy by only 2%. At the same time, when using a human\nfriendly loss function, the same compression system achieves competitive\nperformance in terms of MS-SSIM. By combining these two objective functions, we\nshow that there is a pronounced trade-off in compression quality between the\nhuman visual system and classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:43:29 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 10:11:58 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Weber", "Maurice", ""], ["Renggli", "Cedric", ""], ["Grabner", "Helmut", ""], ["Zhang", "Ce", ""]]}, {"id": "1910.03483", "submitter": "Mariella Dimiccoli", "authors": "Mariella Dimiccoli, Herwig Wendt", "title": "Learning event representations for temporal segmentation of image\n  sequences by dynamic graph embedding", "comments": "Accepted in IEEE Transactions on Image Processing, 2020. To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, self-supervised learning has proved to be effective to learn\nrepresentations of events suitable for temporal segmentation in image\nsequences, where events are understood as sets of temporally adjacent images\nthat are semantically perceived as a whole. However, although this approach\ndoes not require expensive manual annotations, it is data hungry and suffers\nfrom domain adaptation problems. As an alternative, in this work, we propose a\nnovel approach for learning event representations named Dynamic Graph Embedding\n(DGE). The assumption underlying our model is that a sequence of images can be\nrepresented by a graph that encodes both semantic and temporal similarity. The\nkey novelty of DGE is to learn jointly the graph and its graph embedding. At\nits core, DGE works by iterating over two steps: 1) updating the graph\nrepresenting the semantic and temporal similarity of the data based on the\ncurrent data representation, and 2) updating the data representation to take\ninto account the current data graph structure. The main advantage of DGE over\nstate-of-the-art self-supervised approaches is that it does not require any\ntraining set, but instead learns iteratively from the data itself a\nlow-dimensional embedding that reflects their temporal and semantic similarity.\nExperimental results on two benchmark datasets of real image sequences captured\nat regular time intervals demonstrate that the proposed DGE leads to event\nrepresentations effective for temporal segmentation. In particular, it achieves\nrobust temporal segmentation on the EDUBSeg and EDUBSeg-Desc benchmark\ndatasets, outperforming the state of the art. Additional experiments on two\nHuman Motion Segmentation benchmark datasets demonstrate the generalization\ncapabilities of the proposed DGE.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:48:50 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 10:42:25 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 11:03:37 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Dimiccoli", "Mariella", ""], ["Wendt", "Herwig", ""]]}, {"id": "1910.03517", "submitter": "Georg Muntingh PhD", "authors": "Oliver J.D. Barrowclough, Sverre Briseid, Georg Muntingh, Torbj{\\o}rn\n  Viksand", "title": "Real-time processing of high-resolution video and 3D model-based\n  tracking for remote towers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality video data is a core component in emerging remote tower\noperations as it inherently contains a huge amount of information on which an\nair traffic controller can base decisions. Various digital technologies also\nhave the potential to exploit this data to bring enhancements, including\ntracking ground movements by relating events in the video view to their\npositions in 3D space. The total resolution of remote tower setups with\nmultiple cameras often exceeds 25 million RGB pixels and is captured at 30\nframes per second or more. It is thus a challenge to efficiently process all\nthe data in such a way as to provide relevant real-time enhancements to the\ncontroller. In this paper we discuss how a number of improvements can be\nimplemented efficiently on a single workstation by decoupling processes and\nutilizing hardware for parallel computing. We also highlight how decoupling the\nprocesses in this way increases resilience of the software solution in the\nsense that failure of a single component does not impair the function of the\nother components.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:22:29 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 12:41:52 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Barrowclough", "Oliver J. D.", ""], ["Briseid", "Sverre", ""], ["Muntingh", "Georg", ""], ["Viksand", "Torbj\u00f8rn", ""]]}, {"id": "1910.03548", "submitter": "Ting Yao", "authors": "Yingwei Pan and Yehao Li and Qi Cai and Yang Chen and Ting Yao", "title": "Multi-Source Domain Adaptation and Semi-Supervised Domain Adaptation\n  with Focus on Visual Domain Adaptation Challenge 2019", "comments": "Rank 1 in Multi-Source Domain Adaptation of Visual Domain Adaptation\n  Challenge (VisDA-2019). Source code of each task:\n  https://github.com/Panda-Peter/visda2019-multisource and\n  https://github.com/Panda-Peter/visda2019-semisupervised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This notebook paper presents an overview and comparative analysis of our\nsystems designed for the following two tasks in Visual Domain Adaptation\nChallenge (VisDA-2019): multi-source domain adaptation and semi-supervised\ndomain adaptation.\n  Multi-Source Domain Adaptation: We investigate both pixel-level and\nfeature-level adaptation for multi-source domain adaptation task, i.e.,\ndirectly hallucinating labeled target sample via CycleGAN and learning\ndomain-invariant feature representations through self-learning. Moreover, the\nmechanism of fusing features from different backbones is further studied to\nfacilitate the learning of domain-invariant classifiers. Source code and\npre-trained models are available at\n\\url{https://github.com/Panda-Peter/visda2019-multisource}.\n  Semi-Supervised Domain Adaptation: For this task, we adopt a standard\nself-learning framework to construct a classifier based on the labeled source\nand target data, and generate the pseudo labels for unlabeled target data.\nThese target data with pseudo labels are then exploited to re-training the\nclassifier in a following iteration. Furthermore, a prototype-based\nclassification module is additionally utilized to strengthen the predictions.\nSource code and pre-trained models are available at\n\\url{https://github.com/Panda-Peter/visda2019-semisupervised}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 17:17:35 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 05:28:07 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Pan", "Yingwei", ""], ["Li", "Yehao", ""], ["Cai", "Qi", ""], ["Chen", "Yang", ""], ["Yao", "Ting", ""]]}, {"id": "1910.03560", "submitter": "Jong-Chyi Su", "authors": "Jong-Chyi Su, Subhransu Maji, Bharath Hariharan", "title": "When Does Self-supervision Improve Few-shot Learning?", "comments": "ECCV 2020 camera ready. This is an updated version of \"Boosting\n  Supervision with Self-Supervision for Few-shot Learning\" arXiv:1906.07079", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the role of self-supervised learning (SSL) in the context of\nfew-shot learning. Although recent research has shown the benefits of SSL on\nlarge unlabeled datasets, its utility on small datasets is relatively\nunexplored. We find that SSL reduces the relative error rate of few-shot\nmeta-learners by 4%-27%, even when the datasets are small and only utilizing\nimages within the datasets. The improvements are greater when the training set\nis smaller or the task is more challenging. Although the benefits of SSL may\nincrease with larger training sets, we observe that SSL can hurt the\nperformance when the distributions of images used for meta-learning and SSL are\ndifferent. We conduct a systematic study by varying the degree of domain shift\nand analyzing the performance of several meta-learners on a multitude of\ndomains. Based on this analysis we present a technique that automatically\nselects images for SSL from a large, generic pool of unlabeled images for a\ngiven dataset that provides further improvements.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 17:47:14 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 06:08:35 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Su", "Jong-Chyi", ""], ["Maji", "Subhransu", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1910.03561", "submitter": "John Zarka", "authors": "John Zarka, Louis Thiry, Tom\\'as Angles, St\\'ephane Mallat", "title": "Deep Network Classification by Scattering and Homotopy Dictionary\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a sparse scattering deep convolutional neural network, which\nprovides a simple model to analyze properties of deep representation learning\nfor classification. Learning a single dictionary matrix with a classifier\nyields a higher classification accuracy than AlexNet over the ImageNet 2012\ndataset. The network first applies a scattering transform that linearizes\nvariabilities due to geometric transformations such as translations and small\ndeformations. A sparse $\\ell^1$ dictionary coding reduces intra-class\nvariability while preserving class separation through projections over unions\nof linear spaces. It is implemented in a deep convolutional network with a\nhomotopy algorithm having an exponential convergence. A convergence proof is\ngiven in a general framework that includes ALISTA. Classification results are\nanalyzed on ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 17:47:44 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 23:16:15 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 17:32:42 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Zarka", "John", ""], ["Thiry", "Louis", ""], ["Angles", "Tom\u00e1s", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1910.03568", "submitter": "Yufei Ye", "authors": "Yufei Ye, Dhiraj Gandhi, Abhinav Gupta, Shubham Tulsiani", "title": "Object-centric Forward Modeling for Model Predictive Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to learn an object-centric forward model, and show\nthat this allows us to plan for sequences of actions to achieve distant desired\ngoals. We propose to model a scene as a collection of objects, each with an\nexplicit spatial location and implicit visual feature, and learn to model the\neffects of actions using random interaction data. Our model allows capturing\nthe robot-object and object-object interactions, and leads to more\nsample-efficient and accurate predictions. We show that this learned model can\nbe leveraged to search for action sequences that lead to desired goal\nconfigurations, and that in conjunction with a learned correction module, this\nallows for robust closed loop execution. We present experiments both in\nsimulation and the real world, and show that our approach improves over\nalternate implicit or pixel-space forward models. Please see our project page\n(https://judyye.github.io/ocmpc/) for result videos.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 17:58:10 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Ye", "Yufei", ""], ["Gandhi", "Dhiraj", ""], ["Gupta", "Abhinav", ""], ["Tulsiani", "Shubham", ""]]}, {"id": "1910.03579", "submitter": "Yin Bi", "authors": "Yin Bi, Aaron Chadha, Alhabib Abbas, Eirina Bourtsoulatze and Yiannis\n  Andreopoulos", "title": "Graph-based Spatial-temporal Feature Learning for Neuromorphic Vision\n  Sensing", "comments": "16 pages, 5 figures. This work is a journal extension of our ICCV'19\n  paper arXiv:1908.06648", "journal-ref": null, "doi": "10.1109/TIP.2020.3023597", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic vision sensing (NVS)\\ devices represent visual information as\nsequences of asynchronous discrete events (a.k.a., \"spikes\") in response to\nchanges in scene reflectance. Unlike conventional active pixel sensing (APS),\nNVS allows for significantly higher event sampling rates at substantially\nincreased energy efficiency and robustness to illumination changes. However,\nfeature representation for NVS is far behind its APS-based counterparts,\nresulting in lower performance in high-level computer vision tasks. To fully\nutilize its sparse and asynchronous nature, we propose a compact graph\nrepresentation for NVS, which allows for end-to-end learning with graph\nconvolution neural networks. We couple this with a novel end-to-end feature\nlearning framework that accommodates both appearance-based and motion-based\ntasks. The core of our framework comprises a spatial feature learning module,\nwhich utilizes residual-graph convolutional neural networks (RG-CNN), for\nend-to-end learning of appearance-based features directly from graphs. We\nextend this with our proposed Graph2Grid block and temporal feature learning\nmodule for efficiently modelling temporal dependencies over multiple graphs and\na long temporal extent. We show how our framework can be configured for object\nclassification, action recognition and action similarity labeling. Importantly,\nour approach preserves the spatial and temporal coherence of spike events,\nwhile requiring less computation and memory. The experimental validation shows\nthat our proposed framework outperforms all recent methods on standard\ndatasets. Finally, to address the absence of large real-world NVS datasets for\ncomplex recognition tasks, we introduce, evaluate and make available the\nAmerican Sign Language letters (ASL-DVS), as well as human action dataset\n(UCF101-DVS, HMDB51-DVS and ASLAN-DVS).\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 10:55:57 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 16:19:35 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bi", "Yin", ""], ["Chadha", "Aaron", ""], ["Abbas", "Alhabib", ""], ["Bourtsoulatze", "Eirina", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1910.03617", "submitter": "Manel Mart\\'inez-Ram\\'on", "authors": "Manish Bhattarai and Manel Mart\\'inez-Ram\\'on", "title": "A Deep Learning Framework for Detection of Targets in Thermal Images to\n  Improve Firefighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent detection and processing capabilities can be instrumental to\nimproving the safety, efficiency, and successful completion of rescue missions\nconducted by firefighters in emergency first response settings. The objective\nof this research is to create an automated system that is capable of real-time,\nintelligent object detection and recognition and facilitates the improved\nsituational awareness of firefighters during an emergency response. We have\nexplored state of the art machine/deep learning techniques to achieve this\nobjective. The goal for this work is to enhance the situational awareness of\nfirefighters by effectively exploiting the information gathered from infrared\ncameras carried by firefighters. To accomplish this, we use a trained deep\nConvolutional Neural Network (CNN) system to classify and identify objects of\ninterest from thermal imagery in real time. In the midst of those critical\ncircumstances created by structure fire, this system is able to accurately\ninform the decision making process of firefighters with real-time up-to-date\nscene information by extracting, processing, and analyzing crucial information.\nWith the new information produced by the framework, firefighters are able to\nmake more informed inferences about the circumstances for their safe navigation\nthrough such hazardous and potentially catastrophic environments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:08:42 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 17:30:46 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 21:54:44 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bhattarai", "Manish", ""], ["Mart\u00ednez-Ram\u00f3n", "Manel", ""]]}, {"id": "1910.03624", "submitter": "Ali Dabouei", "authors": "Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, Jeremy Dawson,\n  Nasser M. Nasrabadi", "title": "SmoothFool: An Efficient Framework for Computing Smooth Adversarial\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are susceptible to adversarial manipulations in the\ninput domain. The extent of vulnerability has been explored intensively in\ncases of $\\ell_p$-bounded and $\\ell_p$-minimal adversarial perturbations.\nHowever, the vulnerability of DNNs to adversarial perturbations with specific\nstatistical properties or frequency-domain characteristics has not been\nsufficiently explored. In this paper, we study the smoothness of perturbations\nand propose SmoothFool, a general and computationally efficient framework for\ncomputing smooth adversarial perturbations. Through extensive experiments, we\nvalidate the efficacy of the proposed method for both the white-box and\nblack-box attack scenarios. In particular, we demonstrate that: (i) there exist\nextremely smooth adversarial perturbations for well-established and widely used\nnetwork architectures, (ii) smoothness significantly enhances the robustness of\nperturbations against state-of-the-art defense mechanisms, (iii) smoothness\nimproves the transferability of adversarial perturbations across both data\npoints and network architectures, and (iv) class categories exhibit a variable\nrange of susceptibility to smooth perturbations. Our results suggest that\nsmooth APs can play a significant role in exploring the vulnerability extent of\nDNNs to adversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:22:21 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Dabouei", "Ali", ""], ["Soleymani", "Sobhan", ""], ["Taherkhani", "Fariborz", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1910.03634", "submitter": "Prerna Kashyap", "authors": "Prerna Kashyap, Samrat Phatale, Iddo Drori", "title": "Prose for a Painting", "comments": null, "journal-ref": "ICCV Workshop on Closing the Loop Between Vision and Language,\n  2019", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Painting captions are often dry and simplistic which motivates us to describe\na painting creatively in the style of Shakespearean prose. This is a difficult\nproblem, since there does not exist a large supervised dataset from paintings\nto Shakespearean prose. Our solution is to use an intermediate English poem\ndescription of the painting and then apply language style transfer which\nresults in Shakespearean prose describing the painting. We rate our results by\nhuman evaluation on a Likert scale, and evaluate the quality of language style\ntransfer using BLEU score as a function of prose length. We demonstrate the\napplicability and limitations of our approach by generating Shakespearean prose\nfor famous paintings. We make our models and code publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:39:49 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Kashyap", "Prerna", ""], ["Phatale", "Samrat", ""], ["Drori", "Iddo", ""]]}, {"id": "1910.03638", "submitter": "Mahesh Chandra Mukkamala", "authors": "Mahesh Chandra Mukkamala, Felix Westerkamp, Emanuel Laude, Daniel\n  Cremers, Peter Ochs", "title": "Bregman Proximal Framework for Deep Linear Neural Networks", "comments": "34 pages, 54 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical assumption for the analysis of first order optimization methods is\nthe Lipschitz continuity of the gradient of the objective function. However,\nfor many practical applications this assumption is violated, including loss\nfunctions in deep learning. To overcome this issue, certain extensions based on\ngeneralized proximity measures known as Bregman distances were introduced. This\ninitiated the development of the Bregman proximal gradient (BPG) algorithm and\nan inertial variant (momentum based) CoCaIn BPG, which however rely on problem\ndependent Bregman distances. In this paper, we develop Bregman distances for\nusing BPG methods to train Deep Linear Neural Networks. The main implications\nof our results are strong convergence guarantees for these algorithms. We also\npropose several strategies for their efficient implementation, for example,\nclosed form updates and a closed form expression for the inertial parameter of\nCoCaIn BPG. Moreover, the BPG method requires neither diminishing step sizes\nnor line search, unlike its corresponding Euclidean version. We numerically\nillustrate the competitiveness of the proposed methods compared to existing\nstate of the art schemes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:45:34 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Mukkamala", "Mahesh Chandra", ""], ["Westerkamp", "Felix", ""], ["Laude", "Emanuel", ""], ["Cremers", "Daniel", ""], ["Ochs", "Peter", ""]]}, {"id": "1910.03648", "submitter": "Qianru Sun", "authors": "Qianru Sun, Yaoyao Liu, Zhaozheng Chen, Tat-Seng Chua, and Bernt\n  Schiele", "title": "Meta-Transfer Learning through Hard Tasks", "comments": "An extended version of a paper published in CVPR2019. Under review.\n  arXiv admin note: substantial text overlap with arXiv:1812.02391", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Meta-learning has been proposed as a framework to address the challenging\nfew-shot learning setting. The key idea is to leverage a large number of\nsimilar few-shot tasks in order to learn how to adapt a base-learner to a new\ntask for which only a few labeled samples are available. As deep neural\nnetworks (DNNs) tend to overfit using a few samples only, typical meta-learning\nmodels use shallow neural networks, thus limiting its effectiveness. In order\nto achieve top performance, some recent works tried to use the DNNs pre-trained\non large-scale datasets but mostly in straight-forward manners, e.g., (1)\ntaking their weights as a warm start of meta-training, and (2) freezing their\nconvolutional layers as the feature extractor of base-learners. In this paper,\nwe propose a novel approach called meta-transfer learning (MTL) which learns to\ntransfer the weights of a deep NN for few-shot learning tasks. Specifically,\nmeta refers to training multiple tasks, and transfer is achieved by learning\nscaling and shifting functions of DNN weights for each task. In addition, we\nintroduce the hard task (HT) meta-batch scheme as an effective learning\ncurriculum that further boosts the learning efficiency of MTL. We conduct\nfew-shot learning experiments and report top performance for five-class\nfew-shot recognition tasks on three challenging benchmarks: miniImageNet,\ntieredImageNet and Fewshot-CIFAR100 (FC100). Extensive comparisons to related\nworks validate that our MTL approach trained with the proposed HT meta-batch\nscheme achieves top performance. An ablation study also shows that both\ncomponents contribute to fast convergence and high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 06:05:18 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Sun", "Qianru", ""], ["Liu", "Yaoyao", ""], ["Chen", "Zhaozheng", ""], ["Chua", "Tat-Seng", ""], ["Schiele", "Bernt", ""]]}, {"id": "1910.03667", "submitter": "Jos\\'e Ignacio Orlando PhD", "authors": "Jos\\'e Ignacio Orlando, Huazhu Fu, Jo\\~ao Barbossa Breda, Karel van\n  Keer, Deepti R. Bathula, Andr\\'es Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng,\n  Jeyoung Kim, JoonHo Lee, Joonseok Lee, Xiaoxiao Li, Peng Liu, Shuai Lu,\n  Balamurali Murugesan, Valery Naranjo, Sai Samarth R. Phaye, Sharath M.\n  Shankaranarayana, Apoorva Sikka, Jaemin Son, Anton van den Hengel, Shujun\n  Wang, Junyan Wu, Zifeng Wu, Guanghui Xu, Yongli Xu, Pengshuai Yin, Fei Li,\n  Xiulan Zhang, Yanwu Xu, Xiulan Zhang, Hrvoje Bogunovi\\'c", "title": "REFUGE Challenge: A Unified Framework for Evaluating Automated Methods\n  for Glaucoma Assessment from Fundus Photographs", "comments": "Accepted for publication in Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2019.101570", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is one of the leading causes of irreversible but preventable\nblindness in working age populations. Color fundus photography (CFP) is the\nmost cost-effective imaging modality to screen for retinal disorders. However,\nits application to glaucoma has been limited to the computation of a few\nrelated biomarkers such as the vertical cup-to-disc ratio. Deep learning\napproaches, although widely applied for medical image analysis, have not been\nextensively used for glaucoma assessment due to the limited size of the\navailable data sets. Furthermore, the lack of a standardize benchmark strategy\nmakes difficult to compare existing methods in a uniform way. In order to\novercome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE\n(\\url{https://refuge.grand-challenge.org}), held in conjunction with MICCAI\n2018. The challenge consisted of two primary tasks, namely optic disc/cup\nsegmentation and glaucoma classification. As part of REFUGE, we have publicly\nreleased a data set of 1200 fundus images with ground truth segmentations and\nclinical glaucoma labels, currently the largest existing one. We have also\nbuilt an evaluation framework to ease and ensure fairness in the comparison of\ndifferent models, encouraging the development of novel techniques in the field.\n12 teams qualified and participated in the online challenge. This paper\nsummarizes their methods and analyzes their corresponding results. In\nparticular, we observed that two of the top-ranked teams outperformed two human\nexperts in the glaucoma classification task. Furthermore, the segmentation\nresults were in general consistent with the ground truth annotations, with\ncomplementary outcomes that can be further exploited by ensembling the results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 20:20:43 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Orlando", "Jos\u00e9 Ignacio", ""], ["Fu", "Huazhu", ""], ["Breda", "Jo\u00e3o Barbossa", ""], ["van Keer", "Karel", ""], ["Bathula", "Deepti R.", ""], ["Diaz-Pinto", "Andr\u00e9s", ""], ["Fang", "Ruogu", ""], ["Heng", "Pheng-Ann", ""], ["Kim", "Jeyoung", ""], ["Lee", "JoonHo", ""], ["Lee", "Joonseok", ""], ["Li", "Xiaoxiao", ""], ["Liu", "Peng", ""], ["Lu", "Shuai", ""], ["Murugesan", "Balamurali", ""], ["Naranjo", "Valery", ""], ["Phaye", "Sai Samarth R.", ""], ["Shankaranarayana", "Sharath M.", ""], ["Sikka", "Apoorva", ""], ["Son", "Jaemin", ""], ["Hengel", "Anton van den", ""], ["Wang", "Shujun", ""], ["Wu", "Junyan", ""], ["Wu", "Zifeng", ""], ["Xu", "Guanghui", ""], ["Xu", "Yongli", ""], ["Yin", "Pengshuai", ""], ["Li", "Fei", ""], ["Zhang", "Xiulan", ""], ["Xu", "Yanwu", ""], ["Zhang", "Xiulan", ""], ["Bogunovi\u0107", "Hrvoje", ""]]}, {"id": "1910.03676", "submitter": "Ehsan Adeli", "authors": "Ehsan Adeli, Qingyu Zhao, Adolf Pfefferbaum, Edith V. Sullivan, Li\n  Fei-Fei, Juan Carlos Niebles, Kilian M. Pohl", "title": "Representation Learning with Statistical Independence to Mitigate Bias", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presence of bias (in datasets or tasks) is inarguably one of the most\ncritical challenges in machine learning applications that has alluded to\npivotal debates in recent years. Such challenges range from spurious\nassociations between variables in medical studies to the bias of race in gender\nor face recognition systems. Controlling for all types of biases in the dataset\ncuration stage is cumbersome and sometimes impossible. The alternative is to\nuse the available data and build models incorporating fair representation\nlearning. In this paper, we propose such a model based on adversarial training\nwith two competing objectives to learn features that have (1) maximum\ndiscriminative power with respect to the task and (2) minimal statistical mean\ndependence with the protected (bias) variable(s). Our approach does so by\nincorporating a new adversarial loss function that encourages a vanished\ncorrelation between the bias and the learned features. We apply our method to\nsynthetic data, medical images (containing task bias), and a dataset for gender\nclassification (containing dataset bias). Our results show that the learned\nfeatures by our method not only result in superior prediction performance but\nalso are unbiased. The code is available at\nhttps://github.com/QingyuZhao/BR-Net/.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 20:33:58 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 06:27:59 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 22:30:34 GMT"}, {"version": "v4", "created": "Fri, 20 Nov 2020 17:57:38 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Adeli", "Ehsan", ""], ["Zhao", "Qingyu", ""], ["Pfefferbaum", "Adolf", ""], ["Sullivan", "Edith V.", ""], ["Fei-Fei", "Li", ""], ["Niebles", "Juan Carlos", ""], ["Pohl", "Kilian M.", ""]]}, {"id": "1910.03695", "submitter": "Sehyun Chun", "authors": "Sehyun Chun, Nima Hamidi Ghalehjegh, Joseph B. Choi, Chris W. Schwarz,\n  John G. Gaspar, Daniel V. McGehee, Stephen S. Baek", "title": "NADS-Net: A Nimble Architecture for Driver and Seat Belt Detection via\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new convolutional neural network (CNN) architecture for 2D driver/passenger\npose estimation and seat belt detection is proposed in this paper. The new\narchitecture is more nimble and thus more suitable for in-vehicle monitoring\ntasks compared to other generic pose estimation algorithms. The new\narchitecture, named NADS-Net, utilizes the feature pyramid network (FPN)\nbackbone with multiple detection heads to achieve the optimal performance for\ndriver/passenger state detection tasks. The new architecture is validated on a\nnew data set containing video clips of 100 drivers in 50 driving sessions that\nare collected for this study. The detection performance is analyzed under\ndifferent demographic, appearance, and illumination conditions. The results\npresented in this paper may provide meaningful insights for the autonomous\ndriving research community and automotive industry for future algorithm\ndevelopment and data collection.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 21:16:28 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Chun", "Sehyun", ""], ["Ghalehjegh", "Nima Hamidi", ""], ["Choi", "Joseph B.", ""], ["Schwarz", "Chris W.", ""], ["Gaspar", "John G.", ""], ["McGehee", "Daniel V.", ""], ["Baek", "Stephen S.", ""]]}, {"id": "1910.03729", "submitter": "Xiaofan Zhang", "authors": "Hong Yu, Xiaofan Zhang, Lingjun Song, Liren Jiang, Xiaodi Huang, Wen\n  Chen, Chenbin Zhang, Jiahui Li, Jiji Yang, Zhiqiang Hu, Qi Duan, Wanyuan\n  Chen, Xianglei He, Jinshuang Fan, Weihai Jiang, Li Zhang, Chengmin Qiu,\n  Minmin Gu, Weiwei Sun, Yangqiong Zhang, Guangyin Peng, Weiwei Shen, Guohui Fu", "title": "Large-scale Gastric Cancer Screening and Localization Using Multi-task\n  Deep Neural Network", "comments": "under minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gastric cancer is one of the most common cancers, which ranks third among the\nleading causes of cancer death. Biopsy of gastric mucosa is a standard\nprocedure in gastric cancer screening test. However, manual pathological\ninspection is labor-intensive and time-consuming. Besides, it is challenging\nfor an automated algorithm to locate the small lesion regions in the gigapixel\nwhole-slide image and make the decision correctly.To tackle these issues, we\ncollected large-scale whole-slide image dataset with detailed lesion region\nannotation and designed a whole-slide image analyzing framework consisting of 3\nnetworks which could not only determine the screening result but also present\nthe suspicious areas to the pathologist for reference. Experiments demonstrated\nthat our proposed framework achieves sensitivity of 97.05% and specificity of\n92.72% in screening task and Dice coefficient of 0.8331 in segmentation task.\nFurthermore, we tested our best model in real-world scenario on 10,315\nwhole-slide images collected from 4 medical centers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 00:27:02 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 15:12:46 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2020 23:45:16 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Yu", "Hong", ""], ["Zhang", "Xiaofan", ""], ["Song", "Lingjun", ""], ["Jiang", "Liren", ""], ["Huang", "Xiaodi", ""], ["Chen", "Wen", ""], ["Zhang", "Chenbin", ""], ["Li", "Jiahui", ""], ["Yang", "Jiji", ""], ["Hu", "Zhiqiang", ""], ["Duan", "Qi", ""], ["Chen", "Wanyuan", ""], ["He", "Xianglei", ""], ["Fan", "Jinshuang", ""], ["Jiang", "Weihai", ""], ["Zhang", "Li", ""], ["Qiu", "Chengmin", ""], ["Gu", "Minmin", ""], ["Sun", "Weiwei", ""], ["Zhang", "Yangqiong", ""], ["Peng", "Guangyin", ""], ["Shen", "Weiwei", ""], ["Fu", "Guohui", ""]]}, {"id": "1910.03731", "submitter": "Vivek Sharma", "authors": "Vivek Sharma, Praneeth Vepakomma, Tristan Swedish, Ken Chang,\n  Jayashree Kalpathy-Cramer, Ramesh Raskar", "title": "ExpertMatcher: Automating ML Model Selection for Clients using Hidden\n  Representations", "comments": "In NeurIPS Workshop on Robust AI in Financial Services: Data,\n  Fairness, Explainability, Trustworthiness, and Privacy, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been the development of Split Learning, a framework for\ndistributed computation where model components are split between the client and\nserver (Vepakomma et al., 2018b). As Split Learning scales to include many\ndifferent model components, there needs to be a method of matching client-side\nmodel components with the best server-side model components. A solution to this\nproblem was introduced in the ExpertMatcher (Sharma et al., 2019) framework,\nwhich uses autoencoders to match raw data to models. In this work, we propose\nan extension of ExpertMatcher, where matching can be performed without the need\nto share the client's raw data representation. The technique is applicable to\nsituations where there are local clients and centralized expert ML models, but\nthe sharing of raw data is constrained.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 00:42:16 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Sharma", "Vivek", ""], ["Vepakomma", "Praneeth", ""], ["Swedish", "Tristan", ""], ["Chang", "Ken", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1910.03781", "submitter": "Hengyue Liang", "authors": "Hengyue Liang, Xibai Lou, Yang Yang and Changhyun Choi", "title": "Learning Visual Affordances with Target-Orientated Deep Q-Network to\n  Grasp Objects by Harnessing Environmental Fixtures", "comments": "To appear on ICRA21 Xi'an", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a challenging object grasping task and proposes a\nself-supervised learning approach. The goal of the task is to grasp an object\nwhich is not feasible with a single parallel gripper, but only with harnessing\nenvironment fixtures (e.g., walls, furniture, heavy objects). This\nSlide-to-Wall grasping task assumes no prior knowledge except the partial\nobservation of a target object. Hence the robot should learn an effective\npolicy given a scene observation that may include the target object,\nenvironmental fixtures, and any other disturbing objects. We formulate the\nproblem as visual affordances learning for which Target-Oriented Deep Q-Network\n(TO-DQN) is proposed to efficiently learn visual affordance maps (i.e., Q-maps)\nto guide robot actions. Since the training necessitates robot's exploration and\ncollision with the fixtures, TO-DQN is first trained safely with a simulated\nrobot manipulator and then applied to a real robot. We empirically show that\nTO-DQN can learn to solve the task in different environment settings in\nsimulation and outperforms a standard and a variant of Deep Q-Network (DQN) in\nterms of training efficiency and robustness. The testing performance in both\nsimulation and real-robot experiments shows that the policy trained by TO-DQN\nachieves comparable performance to humans.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 04:08:03 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 01:32:21 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liang", "Hengyue", ""], ["Lou", "Xibai", ""], ["Yang", "Yang", ""], ["Choi", "Changhyun", ""]]}, {"id": "1910.03814", "submitter": "Raul Gomez", "authors": "Raul Gomez, Jaume Gibert, Lluis Gomez, Dimosthenis Karatzas", "title": "Exploring Hate Speech Detection in Multimodal Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we target the problem of hate speech detection in multimodal\npublications formed by a text and an image. We gather and annotate a large\nscale dataset from Twitter, MMHS150K, and propose different models that jointly\nanalyze textual and visual information for hate speech detection, comparing\nthem with unimodal detection. We provide quantitative and qualitative results\nand analyze the challenges of the proposed task. We find that, even though\nimages are useful for the hate speech detection task, current multimodal models\ncannot outperform models analyzing only text. We discuss why and open the field\nand the dataset for further research.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 06:53:39 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Gomez", "Raul", ""], ["Gibert", "Jaume", ""], ["Gomez", "Lluis", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1910.03839", "submitter": "Yinglong Wang", "authors": "Yinglong Wang, Haokui Zhang, Yu Liu, Qinfeng Shi, Bing Zeng", "title": "Gradient Information Guided Deraining with A Novel Network and\n  Adversarial Training", "comments": "12 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning based methods have made significant progress\nin rain-removing. However, the existing methods usually do not have good\ngeneralization ability, which leads to the fact that almost all of existing\nmethods have a satisfied performance on removing a specific type of rain\nstreaks, but may have a relatively poor performance on other types of rain\nstreaks. In this paper, aiming at removing multiple types of rain streaks from\nsingle images, we propose a novel deraining framework (GRASPP-GAN), which has\nbetter generalization capacity. Specifically, a modified ResNet-18 which\nextracts the deep features of rainy images and a revised ASPP structure which\nadapts to the various shapes and sizes of rain streaks are composed together to\nform the backbone of our deraining network. Taking the more prominent\ncharacteristics of rain streaks in the gradient domain into consideration, a\ngradient loss is introduced to help to supervise our deraining training\nprocess, for which, a Sobel convolution layer is built to extract the gradient\ninformation flexibly. To further boost the performance, an adversarial learning\nscheme is employed for the first time to train the proposed network. Extensive\nexperiments on both real-world and synthetic datasets demonstrate that our\nmethod outperforms the state-of-the-art deraining methods quantitatively and\nqualitatively. In addition, without any modifications, our proposed framework\nalso achieves good visual performance on dehazing.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 08:27:09 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Wang", "Yinglong", ""], ["Zhang", "Haokui", ""], ["Liu", "Yu", ""], ["Shi", "Qinfeng", ""], ["Zeng", "Bing", ""]]}, {"id": "1910.03849", "submitter": "Fangyi Liu", "authors": "Fangyi Liu and Lei Zhang", "title": "View Confusion Feature Learning for Person Re-identification", "comments": "accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is an important task in video surveillance that aims\nto associate people across camera views at different locations and time. View\nvariability is always a challenging problem seriously degrading person\nre-identification performance. Most of the existing methods either focus on how\nto learn view invariant feature or how to combine view-wise features. In this\npaper, we mainly focus on how to learn view-invariant features by getting rid\nof view specific information through a view confusion learning mechanism.\nSpecifically, we propose an end-toend trainable framework, called View\nConfusion Feature Learning (VCFL), for person Re-ID across cameras. To the best\nof our knowledge, VCFL is originally proposed to learn view-invariant\nidentity-wise features, and it is a kind of combination of view-generic and\nview-specific methods. Classifiers and feature centers are utilized to achieve\nview confusion. Furthermore, we extract sift-guided features by using\nbag-of-words model to help supervise the training of deep networks and enhance\nthe view invariance of features. In experiments, our approach is validated on\nthree benchmark datasets including CUHK01, CUHK03, and MARKET1501, which show\nthe superiority of the proposed method over several state-of-the-art approaches\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 08:55:03 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Liu", "Fangyi", ""], ["Zhang", "Lei", ""]]}, {"id": "1910.03850", "submitter": "Rizhao Cai", "authors": "Rizhao Cai and Changsheng Chen", "title": "Learning deep forest with multi-scale Local Binary Pattern features for\n  face anti-spoofing", "comments": "8 pages, 6 figures, submitted to Pattern Recognition Letter in\n  December 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Anti-Spoofing (FAS) is significant for the security of face recognition\nsystems. Convolutional Neural Networks (CNNs) have been introduced to the field\nof the FAS and have achieved competitive performance. However, CNN-based\nmethods are vulnerable to the adversarial attack. Attackers could generate\nadversarial-spoofing examples to circumvent a CNN-based face liveness detector.\nStudies about the transferability of the adversarial attack reveal that\nutilizing handcrafted feature-based methods could improve security in a\nsystem-level. Therefore, handcrafted feature-based methods are worth our\nexploration. In this paper, we introduce the deep forest, which is proposed as\nan alternative towards CNNs by Zhou et al., in the problem of the FAS. To the\nbest of our knowledge, this is the first attempt at exploiting the deep forest\nin the problem of FAS. Moreover, we propose to re-devise the representation\nconstructing by using LBP descriptors rather than the Grained-Scanning\nMechanism in the original scheme. Our method achieves competitive results. On\nthe benchmark database IDIAP REPLAY-ATTACK, 0\\% Equal Error Rate (EER) is\nachieved. This work provides a competitive option in a fusing scheme for\nimproving system-level security and offers important ideas to those who want to\nexplore methods besides CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 08:57:10 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Cai", "Rizhao", ""], ["Chen", "Changsheng", ""]]}, {"id": "1910.03853", "submitter": "Fuhai Chen", "authors": "Fuhai Chen, Rongrong Ji, Chengpeng Dai, Xiaoshuai Sun, Chia-Wen Lin,\n  Jiayi Ji, Baochang Zhang, Feiyue Huang, Liujuan Cao", "title": "Semantic-aware Image Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deblurring has achieved exciting progress in recent years. However,\ntraditional methods fail to deblur severely blurred images, where semantic\ncontents appears ambiguously. In this paper, we conduct image deblurring guided\nby the semantic contents inferred from image captioning. Specially, we propose\na novel Structured-Spatial Semantic Embedding model for image deblurring\n(termed S3E-Deblur), which introduces a novel Structured-Spatial Semantic tree\nmodel (S3-tree) to bridge two basic tasks in computer vision: image deblurring\n(ImD) and image captioning (ImC). In particular, S3-tree captures and\nrepresents the semantic contents in structured spatial features in ImC, and\nthen embeds the spatial features of the tree nodes into GAN based ImD.\nCo-training on S3-tree, ImC, and ImD is conducted to optimize the overall model\nin a multi-task end-to-end manner. Extensive experiments on severely blurred\nMSCOCO and GoPro datasets demonstrate the significant superiority of S3E-Deblur\ncompared to the state-of-the-arts on both ImD and ImC tasks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 08:59:45 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Chen", "Fuhai", ""], ["Ji", "Rongrong", ""], ["Dai", "Chengpeng", ""], ["Sun", "Xiaoshuai", ""], ["Lin", "Chia-Wen", ""], ["Ji", "Jiayi", ""], ["Zhang", "Baochang", ""], ["Huang", "Feiyue", ""], ["Cao", "Liujuan", ""]]}, {"id": "1910.03858", "submitter": "Zhijie Fang", "authors": "Zhijie Fang, Antonio M. L\\'opez", "title": "Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation", "comments": "Paper accepted by IEEE Trans. on Intelligent Transportation Systems.\n  arXiv admin note: substantial text overlap with arXiv:1807.10580", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating the intentions of vulnerable road users (VRUs) such as\npedestrians and cyclists is critical for performing safe and comfortable\ndriving maneuvers. This is the case for human driving and, thus, should be\ntaken into account by systems providing any level of driving assistance, from\nadvanced driver assistant systems (ADAS) to fully autonomous vehicles (AVs). In\nthis paper, we show how the latest advances on monocular vision-based human\npose estimation, i.e. those relying on deep Convolutional Neural Networks\n(CNNs), enable to recognize the intentions of such VRUs. In the case of\ncyclists, we assume that they follow traffic rules to indicate future maneuvers\nwith arm signals. In the case of pedestrians, no indications can be assumed.\nInstead, we hypothesize that the walking pattern of a pedestrian allows to\ndetermine if he/she has the intention of crossing the road in the path of the\nego-vehicle, so that the ego-vehicle must maneuver accordingly (e.g. slowing\ndown or stopping). In this paper, we show how the same methodology can be used\nfor recognizing pedestrians and cyclists' intentions. For pedestrians, we\nperform experiments on the JAAD dataset. For cyclists, we did not found an\nanalogous dataset, thus, we created our own one by acquiring and annotating\nvideos which we share with the research community. Overall, the proposed\npipeline provides new state-of-the-art results on the intention recognition of\nVRUs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 09:06:49 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Fang", "Zhijie", ""], ["L\u00f3pez", "Antonio M.", ""]]}, {"id": "1910.03866", "submitter": "Leonie Henschel", "authors": "Leonie Henschel, Sailesh Conjeti, Santiago Estrada, Kersten Diers,\n  Bruce Fischl, Martin Reuter", "title": "FastSurfer -- A fast and accurate deep learning based neuroimaging\n  pipeline", "comments": "Submitted to NeuroImage", "journal-ref": null, "doi": "10.1016/j.neuroimage.2020.117012", "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional neuroimage analysis pipelines involve computationally intensive,\ntime-consuming optimization steps, and thus, do not scale well to large cohort\nstudies with thousands or tens of thousands of individuals. In this work we\npropose a fast and accurate deep learning based neuroimaging pipeline for the\nautomated processing of structural human brain MRI scans, replicating\nFreeSurfer's anatomical segmentation including surface reconstruction and\ncortical parcellation. To this end, we introduce an advanced deep learning\narchitecture capable of whole brain segmentation into 95 classes. The network\narchitecture incorporates local and global competition via competitive dense\nblocks and competitive skip pathways, as well as multi-slice information\naggregation that specifically tailor network performance towards accurate\nsegmentation of both cortical and sub-cortical structures. Further, we perform\nfast cortical surface reconstruction and thickness analysis by introducing a\nspectral spherical embedding and by directly mapping the cortical labels from\nthe image to the surface. This approach provides a full FreeSurfer alternative\nfor volumetric analysis (in under 1 minute) and surface-based thickness\nanalysis (within only around 1h runtime). For sustainability of this approach\nwe perform extensive validation: we assert high segmentation accuracy on\nseveral unseen datasets, measure generalizability and demonstrate increased\ntest-retest reliability, and high sensitivity to group differences in dementia.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 09:41:14 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 15:57:59 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 10:32:33 GMT"}, {"version": "v4", "created": "Fri, 29 May 2020 13:45:00 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Henschel", "Leonie", ""], ["Conjeti", "Sailesh", ""], ["Estrada", "Santiago", ""], ["Diers", "Kersten", ""], ["Fischl", "Bruce", ""], ["Reuter", "Martin", ""]]}, {"id": "1910.03876", "submitter": "Younkwan Lee", "authors": "Younkwan Lee, Juhyun Lee, Hoyeon Ahn, Moongu Jeon", "title": "SNIDER: Single Noisy Image Denoising and Rectification for Improving\n  License Plate Recognition", "comments": "accepted to ICCV 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an algorithm for real-world license plate\nrecognition (LPR) from a low-quality image. Our method is built upon a\nframework that includes denoising and rectification, and each task is conducted\nby Convolutional Neural Networks. Existing denoising and rectification have\nbeen treated separately as a single network in previous research. In contrast\nto the previous work, we here propose an end-to-end trainable network for image\nrecovery, Single Noisy Image DEnoising and Rectification (SNIDER), which\nfocuses on solving both the problems jointly. It overcomes those obstacles by\ndesigning a novel network to address the denoising and rectification jointly.\nMoreover, we propose a way to leverage optimization with the auxiliary tasks\nfor multi-task fitting and novel training losses. Extensive experiments on two\nchallenging LPR datasets demonstrate the effectiveness of our proposed method\nin recovering the high-quality license plate image from the low-quality one and\nshow that the the proposed method outperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 10:01:07 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Lee", "Younkwan", ""], ["Lee", "Juhyun", ""], ["Ahn", "Hoyeon", ""], ["Jeon", "Moongu", ""]]}, {"id": "1910.03892", "submitter": "Daan de Geus", "authors": "Daan de Geus, Panagiotis Meletis, Gijs Dubbelman", "title": "Fast Panoptic Segmentation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an end-to-end network for fast panoptic\nsegmentation. This network, called Fast Panoptic Segmentation Network (FPSNet),\ndoes not require computationally costly instance mask predictions or merging\nheuristics. This is achieved by casting the panoptic task into a custom dense\npixel-wise classification task, which assigns a class label or an instance id\nto each pixel. We evaluate FPSNet on the Cityscapes and Pascal VOC datasets,\nand find that FPSNet is faster than existing panoptic segmentation methods,\nwhile achieving better or similar panoptic segmentation performance. On the\nCityscapes validation set, we achieve a Panoptic Quality score of 55.1%, at\nprediction times of 114 milliseconds for images with a resolution of 1024x2048\npixels. For lower resolutions of the Cityscapes dataset and for the Pascal VOC\ndataset, FPSNet runs at 22 and 35 frames per second, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 10:41:28 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["de Geus", "Daan", ""], ["Meletis", "Panagiotis", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "1910.03903", "submitter": "Danil Galeev", "authors": "Danila Rukhovich, Danil Galeev", "title": "MixMatch Domain Adaptaion: Prize-winning solution for both tracks of\n  VisDA 2019 challenge", "comments": "accepted at TASK-CV 2019 at ICCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a domain adaptation (DA) system that can be used in multi-source\nand semi-supervised settings. Using the proposed method we achieved 2nd place\non multi-source track and 3rd place on semi-supervised track of the VisDA 2019\nchallenge (http://ai.bu.edu/visda-2019/). The source code of the method is\navailable at https://github.com/filaPro/visda2019.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 11:22:59 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Rukhovich", "Danila", ""], ["Galeev", "Danil", ""]]}, {"id": "1910.03905", "submitter": "Feroz Ali T M", "authors": "T M Feroz Ali, Subhasis Chaudhuri", "title": "A Semi-Supervised Maximum Margin Metric Learning Approach for Small\n  Scale Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video surveillance, person re-identification is the task of searching\nperson images in non-overlapping cameras. Though supervised methods for person\nre-identification have attained impressive performance, obtaining large scale\ncross-view labeled training data is very expensive. However, unlabelled data is\navailable in abundance. In this paper, we propose a semi-supervised metric\nlearning approach that can utilize information in unlabelled data with the help\nof a few labelled training samples. We also address the small sample size\nproblem that inherently occurs due to the few labeled training data. Our method\nlearns a discriminative space where within class samples collapse to singular\npoints, achieving the least within class variance, and then use a maximum\nmargin criterion over a high dimensional kernel space to maximally separate the\ndistinct class samples. A maximum margin criterion with two levels of high\ndimensional mappings to kernel space is used to obtain better cross-view\ndiscrimination of the identities. Cross-view affinity learning with reciprocal\nnearest neighbor constraints is used to mine new pseudo-classes from the\nunlabelled data and update the distance metric iteratively. We attain\nstate-of-the-art performance on four challenging datasets with a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 11:26:30 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Ali", "T M Feroz", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "1910.03910", "submitter": "Nils Gessert", "authors": "Nils Gessert and Maximilian Nielsen and Mohsin Shaikh and Ren\\'e\n  Werner and Alexander Schlaefer", "title": "Skin Lesion Classification Using Ensembles of Multi-Resolution\n  EfficientNets with Meta Data", "comments": "First place at the ISIC 2019 Skin Lesion Classification Challenge\n  https://challenge2019.isic-archive.com/leaderboard.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our method for the ISIC 2019 Skin Lesion\nClassification Challenge. The challenge comes with two tasks. For task 1, skin\nlesions have to be classified based on dermoscopic images. For task 2,\ndermoscopic images and additional patient meta data have to be used. A diverse\ndataset of 25000 images was provided for training, containing images from eight\nclasses. The final test set contains an additional, unknown class. We address\nthis challenging problem with a simple, data driven approach by including\nexternal data with skin lesions types that are not present in the training set.\nFurthermore, multi-class skin lesion classification comes with the problem of\nsevere class imbalance. We try to overcome this problem by using loss\nbalancing. Also, the dataset contains images with very different resolutions.\nWe take care of this property by considering different model input resolutions\nand different cropping strategies. To incorporate meta data such as age,\nanatomical site, and sex, we use an additional dense neural network and fuse\nits features with the CNN. We aggregate all our models with an ensembling\nstrategy where we search for the optimal subset of models. Our best ensemble\nachieves a balanced accuracy of 74.2% using five-fold cross-validation. On the\nofficial test set our method is ranked first for both tasks with a balanced\naccuracy of 63.6% for task 1 and 63.4% for task 2.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 11:36:45 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Gessert", "Nils", ""], ["Nielsen", "Maximilian", ""], ["Shaikh", "Mohsin", ""], ["Werner", "Ren\u00e9", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1910.03915", "submitter": "Antonio D'Innocente", "authors": "Antonio D'Innocente, Silvia Bucci, Barbara Caputo, Tatiana Tommasi", "title": "Learning to Generalize One Sample at a Time with Self-Supervision", "comments": "Submitted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep networks have significantly increased the performance of visual\nrecognition methods, it is still challenging to achieve the robustness across\nvisual domains that is necessary for real-world applications. To tackle this\nissue, research on domain adaptation and generalization has flourished over the\nlast decade. An important aspect to consider when assessing the work done in\nthe literature so far is the amount of data annotation necessary for training\neach approach, both at the source and target level. In this paper we argue that\nthe data annotation overload should be minimal, as it is costly. Hence, we\npropose to use self-supervised learning to achieve domain generalization and\nadaptation. We consider learning regularities from non annotated data as an\nauxiliary task, and cast the problem within an Auxiliary Learning principled\nframework. Moreover, we suggest to further exploit the ability to learn about\nvisual domains from non annotated images by learning from target data while\ntesting, as data are presented to the algorithm one sample at a time. Results\non three different scenarios confirm the value of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 11:58:29 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 12:59:46 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 14:11:10 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["D'Innocente", "Antonio", ""], ["Bucci", "Silvia", ""], ["Caputo", "Barbara", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "1910.03923", "submitter": "Feroz Ali T M", "authors": "T M Feroz Ali, Kalpesh K Patel, Rajbabu Velmurugan, Subhasis Chaudhuri", "title": "Multiple Kernel Fisher Discriminant Metric Learning for Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification addresses the problem of matching pedestrian images\nacross disjoint camera views. Design of feature descriptor and distance metric\nlearning are the two fundamental tasks in person re-identification. In this\npaper, we propose a metric learning framework for person re-identification,\nwhere the discriminative metric space is learned using Kernel Fisher\nDiscriminant Analysis (KFDA), to simultaneously maximize the inter-class\nvariance as well as minimize the intra-class variance. We derive a Mahalanobis\nmetric induced by KFDA and argue that KFDA is efficient to be applied for\nmetric learning in person re-identification. We also show how the efficiency of\nKFDA in metric learning can be further enhanced for person re-identification by\nusing two simple yet efficient multiple kernel learning methods. We conduct\nextensive experiments on three benchmark datasets for person re-identification\nand demonstrate that the proposed approaches have competitive performance with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 12:15:22 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Ali", "T M Feroz", ""], ["Patel", "Kalpesh K", ""], ["Velmurugan", "Rajbabu", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "1910.03973", "submitter": "Yazhan Zhang", "authors": "Yazhan Zhang, Weihao Yuan, Zicheng Kan, Michael Yu Wang", "title": "Towards Learning to Detect and Predict Contact Events on Vision-based\n  Tactile Sensors", "comments": "10 pages, 7 figures, Accepted to conference on Robot Learning (CoRL\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In essence, successful grasp boils down to correct responses to multiple\ncontact events between fingertips and objects. In most scenarios, tactile\nsensing is adequate to distinguish contact events. Due to the nature of high\ndimensionality of tactile information, classifying spatiotemporal tactile\nsignals using conventional model-based methods is difficult. In this work, we\npropose to predict and classify tactile signal using deep learning methods,\nseeking to enhance the adaptability of the robotic grasp system to external\nevent changes that may lead to grasping failure. We develop a deep learning\nframework and collect 6650 tactile image sequences with a vision-based tactile\nsensor, and the neural network is integrated into a contact-event-based robotic\ngrasping system. In grasping experiments, we achieved 52% increase in terms of\nobject lifting success rate with contact detection, significantly higher\nrobustness under unexpected loads with slip prediction compared with open-loop\ngrasps, demonstrating that integration of the proposed framework into robotic\ngrasping system substantially improves picking success rate and capability to\nwithstand external disturbances.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 13:25:12 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Zhang", "Yazhan", ""], ["Yuan", "Weihao", ""], ["Kan", "Zicheng", ""], ["Wang", "Michael Yu", ""]]}, {"id": "1910.03986", "submitter": "Guilherme Aresta", "authors": "Guilherme Aresta, Carlos Ferreira, Jo\\~ao Pedrosa, Teresa Ara\\'ujo,\n  Jo\\~ao Rebelo, Eduardo Negr\\~ao, Margarida Morgado, Filipe Alves, Ant\\'onio\n  Cunha, Isabel Ramos and Aur\\'elio Campilho", "title": "Did you miss it? Automatic lung nodule detection combined with gaze\n  information improves radiologists' screening performance", "comments": "Submitted to IEEE Transactions on Biomedical Engineering (TBME)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Early diagnosis of lung cancer via computed tomography can significantly\nreduce the morbidity and mortality rates associated with the pathology.\nHowever, search lung nodules is a high complexity task, which affects the\nsuccess of screening programs. Whilst computer-aided detection systems can be\nused as second observers, they may bias radiologists and introduce significant\ntime overheads. With this in mind, this study assesses the potential of using\ngaze information for integrating automatic detection systems in the clinical\npractice. For that purpose, 4 radiologists were asked to annotate 20 scans from\na public dataset while being monitored by an eye tracker device and an\nautomatic lung nodule detection system was developed. Our results show that\nradiologists follow a similar search routine and tend to have lower fixation\nperiods in regions where finding errors occur. The overall detection\nsensitivity of the specialists was 0.67$\\pm$0.07, whereas the system achieved\n0.69. Combining the annotations of one radiologist with the automatic system\nsignificantly improves the detection performance to similar levels of two\nannotators. Likewise, combining the findings of radiologist with the detection\nalgorithm only for low fixation regions still significantly improves the\ndetection sensitivity without increasing the number of false-positives. The\ncombination of the automatic system with the gaze information allows to\nmitigate possible errors of the radiologist without some of the issues usually\nassociated with automatic detection system.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 13:43:42 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Aresta", "Guilherme", ""], ["Ferreira", "Carlos", ""], ["Pedrosa", "Jo\u00e3o", ""], ["Ara\u00fajo", "Teresa", ""], ["Rebelo", "Jo\u00e3o", ""], ["Negr\u00e3o", "Eduardo", ""], ["Morgado", "Margarida", ""], ["Alves", "Filipe", ""], ["Cunha", "Ant\u00f3nio", ""], ["Ramos", "Isabel", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "1910.03997", "submitter": "Martin Hahner", "authors": "Martin Hahner, Dengxin Dai, Christos Sakaridis, Jan-Nico Zaech, Luc\n  Van Gool", "title": "Semantic Understanding of Foggy Scenes with Purely Synthetic Data", "comments": "independent class IoU scores corrected for BiSiNet architecture", "journal-ref": null, "doi": "10.1109/ITSC.2019.8917518", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of semantic scene understanding under foggy\nroad conditions. Although marked progress has been made in semantic scene\nunderstanding over the recent years, it is mainly concentrated on clear weather\noutdoor scenes. Extending semantic segmentation methods to adverse weather\nconditions like fog is crucially important for outdoor applications such as\nself-driving cars. In this paper, we propose a novel method, which uses purely\nsynthetic data to improve the performance on unseen real-world foggy scenes\ncaptured in the streets of Zurich and its surroundings. Our results highlight\nthe potential and power of photo-realistic synthetic images for training and\nespecially fine-tuning deep neural nets. Our contributions are threefold, 1) we\ncreated a purely synthetic, high-quality foggy dataset of 25,000 unique outdoor\nscenes, that we call Foggy Synscapes and plan to release publicly 2) we show\nthat with this data we outperform previous approaches on real-world foggy test\ndata 3) we show that a combination of our data and previously used data can\neven further improve the performance on real-world foggy data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 14:04:59 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 07:42:58 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Hahner", "Martin", ""], ["Dai", "Dengxin", ""], ["Sakaridis", "Christos", ""], ["Zaech", "Jan-Nico", ""], ["Van Gool", "Luc", ""]]}, {"id": "1910.04009", "submitter": "Konstantin Bulatov", "authors": "Konstantin Bulatov, Daniil Matalov, Vladimir V. Arlazarov", "title": "MIDV-2019: Challenges of the modern mobile-based document OCR", "comments": "6 pages, 3 figures, 3 tables, 18 references, submitted and accepted\n  to the 12th International Conference on Machine Vision (ICMV 2019)", "journal-ref": "Proc. SPIE 11433 ICMV-2019 (2020), 114332N", "doi": "10.1117/12.2558438", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of identity documents using mobile devices has become a topic of\na wide range of computer vision research. The portfolio of methods and\nalgorithms for solving such tasks as face detection, document detection and\nrectification, text field recognition, and other, is growing, and the scarcity\nof datasets has become an important issue. One of the openly accessible\ndatasets for evaluating such methods is MIDV-500, containing video clips of 50\nidentity document types in various conditions. However, the variability of\ncapturing conditions in MIDV-500 did not address some of the key issues, mainly\nsignificant projective distortions and different lighting conditions. In this\npaper we present a MIDV-2019 dataset, containing video clips shot with modern\nhigh-resolution mobile cameras, with strong projective distortions and with low\nlighting conditions. The description of the added data is presented, and\nexperimental baselines for text field recognition in different conditions. The\ndataset is available for download at\nftp://smartengines.com/midv-500/extra/midv-2019/.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 14:12:27 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Bulatov", "Konstantin", ""], ["Matalov", "Daniil", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "1910.04030", "submitter": "Malay Singh", "authors": "Malay Singh, Emarene Mationg Kalaw, Wang Jie, Mundher Al-Shabi, Chin\n  Fong Wong, Danilo Medina Giron, Kian-Tai Chong, Maxine Tan, Zeng Zeng, Hwee\n  Kuan Lee", "title": "Cribriform pattern detection in prostate histopathological images using\n  deep learning models", "comments": "21 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architecture, size, and shape of glands are most important patterns used by\npathologists for assessment of cancer malignancy in prostate histopathological\ntissue slides. Varying structures of glands along with cumbersome manual\nobservations may result in subjective and inconsistent assessment. Cribriform\ngland with irregular border is an important feature in Gleason pattern 4. We\npropose using deep neural networks for cribriform pattern classification in\nprostate histopathological images. $163708$ Hematoxylin and Eosin (H\\&E)\nstained images were extracted from histopathologic tissue slides of $19$\npatients with prostate cancer and annotated for cribriform patterns. Our\nautomated image classification system analyses the H\\&E images to classify them\nas either `Cribriform' or `Non-cribriform'. Our system uses various deep\nlearning approaches and hand-crafted image pixel intensity-based features. We\npresent our results for cribriform pattern detection across various parameters\nand configuration allowed by our system. The combination of fine-tuned deep\nlearning models outperformed the state-of-art nuclei feature based methods. Our\nimage classification system achieved the testing accuracy of $85.93~\\pm~7.54$\n(cross-validated) and $88.04~\\pm~5.63$ ( additional unseen test set) across\nthree folds. In this paper, we present an annotated cribriform dataset along\nwith analysis of deep learning models and hand-crafted features for cribriform\npattern detection in prostate histopathological images.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 14:44:49 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Singh", "Malay", ""], ["Kalaw", "Emarene Mationg", ""], ["Jie", "Wang", ""], ["Al-Shabi", "Mundher", ""], ["Wong", "Chin Fong", ""], ["Giron", "Danilo Medina", ""], ["Chong", "Kian-Tai", ""], ["Tan", "Maxine", ""], ["Zeng", "Zeng", ""], ["Lee", "Hwee Kuan", ""]]}, {"id": "1910.04061", "submitter": "Zongjing Cao", "authors": "Zongjing Cao, Hyo Jong Lee", "title": "Improved Res2Net model for Person re-identification", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification has become a very popular research topic in the\ncomputer vision community owing to its numerous applications and growing\nimportance in visual surveillance. Person re-identification remains challenging\ndue to occlusion, illumination and significant intra-class variations across\ndifferent cameras. In this paper, we propose a multi-task network base on an\nimproved Res2Net model that simultaneously computes the identification loss and\nverification loss of two pedestrian images. Given a pair of pedestrian images,\nthe system predicts the identities of the two input images and whether they\nbelong to the same identity. In order to obtain deeper feature information of\npedestrians, we propose to use the latest Res2Net model for feature extraction\nof each input image. Experiments on several large-scale person\nre-identification benchmark datasets demonstrate the accuracy of our approach.\nFor example, rank-1 accuracies are 83.18% (+1.38) and 93.14% (+0.84) for the\nDukeMTMC and Market-1501 datasets, respectively. The proposed method shows\nencouraging improvements compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 12:12:11 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 10:24:25 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Cao", "Zongjing", ""], ["Lee", "Hyo Jong", ""]]}, {"id": "1910.04066", "submitter": "Xin Deng", "authors": "Xin Deng, Pier Luigi Dragotti", "title": "Deep Convolutional Neural Network for Multi-modal Image Restoration and\n  Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep convolutional neural network to solve\nthe general multi-modal image restoration (MIR) and multi-modal image fusion\n(MIF) problems. Different from other methods based on deep learning, our\nnetwork architecture is designed by drawing inspirations from a new proposed\nmulti-modal convolutional sparse coding (MCSC) model. The key feature of the\nproposed network is that it can automatically split the common information\nshared among different modalities, from the unique information that belongs to\neach single modality, and is therefore denoted with CU-Net, i.e., Common and\nUnique information splitting network. Specifically, the CU-Net is composed of\nthree modules, i.e., the unique feature extraction module (UFEM), common\nfeature preservation module (CFPM), and image reconstruction module (IRM). The\narchitecture of each module is derived from the corresponding part in the MCSC\nmodel, which consists of several learned convolutional sparse coding (LCSC)\nblocks. Extensive numerical results verify the effectiveness of our method on a\nvariety of MIR and MIF tasks, including RGB guided depth image\nsuper-resolution, flash guided non-flash image denoising, multi-focus and\nmulti-exposure image fusion.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:20:15 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Deng", "Xin", ""], ["Dragotti", "Pier Luigi", ""]]}, {"id": "1910.04071", "submitter": "Annika Reinke", "authors": "Lena Maier-Hein, Annika Reinke, Michal Kozubek, Anne L. Martel, Tal\n  Arbel, Matthias Eisenmann, Allan Hanbuary, Pierre Jannin, Henning M\\\"uller,\n  Sinan Onogur, Julio Saez-Rodriguez, Bram van Ginneken, Annette\n  Kopp-Schneider, Bennett Landman", "title": "BIAS: Transparent reporting of biomedical image analysis challenges", "comments": "2 Appendices - Appendix A: BIAS reporting guideline for biomedical\n  image analysis challenges, Appendix B: Glossary; 2 Supplements - Suppl 1:\n  Form for summarizing information on challenge organization, Suppl 2:\n  Structured description of a challenge design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of biomedical image analysis challenges organized per year is\nsteadily increasing. These international competitions have the purpose of\nbenchmarking algorithms on common data sets, typically to identify the best\nmethod for a given problem. Recent research, however, revealed that common\npractice related to challenge reporting does not allow for adequate\ninterpretation and reproducibility of results. To address the discrepancy\nbetween the impact of challenges and the quality (control), the Biomedical I\nmage Analysis ChallengeS (BIAS) initiative developed a set of recommendations\nfor the reporting of challenges. The BIAS statement aims to improve the\ntransparency of the reporting of a biomedical image analysis challenge\nregardless of field of application, image modality or task category assessed.\nThis article describes how the BIAS statement was developed and presents a\nchecklist which authors of biomedical image analysis challenges are encouraged\nto include in their submission when giving a paper on a challenge into review.\nThe purpose of the checklist is to standardize and facilitate the review\nprocess and raise interpretability and reproducibility of challenge results by\nmaking relevant information explicit.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:30:33 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 06:27:03 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 12:26:11 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2020 06:58:41 GMT"}, {"version": "v5", "created": "Mon, 31 Aug 2020 13:04:02 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Maier-Hein", "Lena", ""], ["Reinke", "Annika", ""], ["Kozubek", "Michal", ""], ["Martel", "Anne L.", ""], ["Arbel", "Tal", ""], ["Eisenmann", "Matthias", ""], ["Hanbuary", "Allan", ""], ["Jannin", "Pierre", ""], ["M\u00fcller", "Henning", ""], ["Onogur", "Sinan", ""], ["Saez-Rodriguez", "Julio", ""], ["van Ginneken", "Bram", ""], ["Kopp-Schneider", "Annette", ""], ["Landman", "Bennett", ""]]}, {"id": "1910.04074", "submitter": "Xin Deng", "authors": "Xin Deng, Ren Yang, Mai Xu, Pier Luigi Dragotti", "title": "Wavelet Domain Style Transfer for an Effective Perception-distortion\n  Tradeoff in Single Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In single image super-resolution (SISR), given a low-resolution (LR) image,\none wishes to find a high-resolution (HR) version of it which is both accurate\nand photo-realistic. Recently, it has been shown that there exists a\nfundamental tradeoff between low distortion and high perceptual quality, and\nthe generative adversarial network (GAN) is demonstrated to approach the\nperception-distortion (PD) bound effectively. In this paper, we propose a novel\nmethod based on wavelet domain style transfer (WDST), which achieves a better\nPD tradeoff than the GAN based methods. Specifically, we propose to use 2D\nstationary wavelet transform (SWT) to decompose one image into low-frequency\nand high-frequency sub-bands. For the low-frequency sub-band, we improve its\nobjective quality through an enhancement network. For the high-frequency\nsub-band, we propose to use WDST to effectively improve its perceptual quality.\nBy feat of the perfect reconstruction property of wavelets, these sub-bands can\nbe re-combined to obtain an image which has simultaneously high objective and\nperceptual quality. The numerical results on various datasets show that our\nmethod achieves the best trade-off between the distortion and perceptual\nquality among the existing state-of-the-art SISR methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:44:59 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Deng", "Xin", ""], ["Yang", "Ren", ""], ["Xu", "Mai", ""], ["Dragotti", "Pier Luigi", ""]]}, {"id": "1910.04076", "submitter": "Senthil Yogamani", "authors": "Varun Ravi Kumar, Sandesh Athni Hiremath, Stefan Milz, Christian Witt,\n  Clement Pinnard, Senthil Yogamani and Patrick Mader", "title": "FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation\n  using Monocular Fisheye Camera for Autonomous Driving", "comments": "Minor fixes added after ICRA 2020 camera ready submission. ICRA 2020\n  presentation video - https://www.youtube.com/watch?v=qAsdpHP5e8c&t", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisheye cameras are commonly used in applications like autonomous driving and\nsurveillance to provide a large field of view ($>180^{\\circ}$). However, they\ncome at the cost of strong non-linear distortions which require more complex\nalgorithms. In this paper, we explore Euclidean distance estimation on fisheye\ncameras for automotive scenes. Obtaining accurate and dense depth supervision\nis difficult in practice, but self-supervised learning approaches show\npromising results and could potentially overcome the problem. We present a\nnovel self-supervised scale-aware framework for learning Euclidean distance and\nego-motion from raw monocular fisheye videos without applying rectification.\nWhile it is possible to perform piece-wise linear approximation of fisheye\nprojection surface and apply standard rectilinear models, it has its own set of\nissues like re-sampling distortion and discontinuities in transition regions.\nTo encourage further research in this area, we will release our dataset as part\nof the WoodScape project \\cite{yogamani2019woodscape}. We further evaluated the\nproposed algorithm on the KITTI dataset and obtained state-of-the-art results\ncomparable to other self-supervised monocular methods. Qualitative results on\nan unseen fisheye video demonstrate impressive performance\nhttps://youtu.be/Sgq1WzoOmXg.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:51:38 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 12:06:13 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 18:11:34 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2020 19:29:03 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Kumar", "Varun Ravi", ""], ["Hiremath", "Sandesh Athni", ""], ["Milz", "Stefan", ""], ["Witt", "Christian", ""], ["Pinnard", "Clement", ""], ["Yogamani", "Senthil", ""], ["Mader", "Patrick", ""]]}, {"id": "1910.04093", "submitter": "Johannes Lehner", "authors": "Johannes Lehner, Andreas Mitterecker, Thomas Adler, Markus Hofmarcher,\n  Bernhard Nessler, Sepp Hochreiter", "title": "Patch Refinement -- Localized 3D Object Detection", "comments": "Machine Learning for Autonomous Driving Workshop at the 33rd\n  Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Patch Refinement a two-stage model for accurate 3D object\ndetection and localization from point cloud data. Patch Refinement is composed\nof two independently trained Voxelnet-based networks, a Region Proposal Network\n(RPN) and a Local Refinement Network (LRN). We decompose the detection task\ninto a preliminary Bird's Eye View (BEV) detection step and a local 3D\ndetection step. Based on the proposed BEV locations by the RPN, we extract\nsmall point cloud subsets (\"patches\"), which are then processed by the LRN,\nwhich is less limited by memory constraints due to the small area of each\npatch. Therefore, we can apply encoding with a higher voxel resolution locally.\nThe independence of the LRN enables the use of additional augmentation\ntechniques and allows for an efficient, regression focused training as it uses\nonly a small fraction of each scene. Evaluated on the KITTI 3D object detection\nbenchmark, our submission from January 28, 2019, outperformed all previous\nentries on all three difficulties of the class car, using only 50 % of the\navailable training data and only LiDAR information.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:11:17 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Lehner", "Johannes", ""], ["Mitterecker", "Andreas", ""], ["Adler", "Thomas", ""], ["Hofmarcher", "Markus", ""], ["Nessler", "Bernhard", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1910.04099", "submitter": "Chuhang Zou", "authors": "Chuhang Zou, Jheng-Wei Su, Chi-Han Peng, Alex Colburn, Qi Shan, Peter\n  Wonka, Hung-Kuo Chu, Derek Hoiem", "title": "Manhattan Room Layout Reconstruction from a Single 360 image: A\n  Comparative Study of State-of-the-art Methods", "comments": "Accepted by International Journal of Computer Vision (IJCV), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches for predicting layouts from 360 panoramas produce excellent\nresults. These approaches build on a common framework consisting of three\nsteps: a pre-processing step based on edge-based alignment, prediction of\nlayout elements, and a post-processing step by fitting a 3D layout to the\nlayout elements. Until now, it has been difficult to compare the methods due to\nmultiple different design decisions, such as the encoding network (e.g. SegNet\nor ResNet), type of elements predicted (e.g. corners, wall/floor boundaries, or\nsemantic segmentation), or method of fitting the 3D layout. To address this\nchallenge, we summarize and describe the common framework, the variants, and\nthe impact of the design decisions. For a complete evaluation, we also propose\nextended annotations for the Matterport3D dataset [3], and introduce two\ndepth-based evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:22:04 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 04:59:25 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2020 05:15:51 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zou", "Chuhang", ""], ["Su", "Jheng-Wei", ""], ["Peng", "Chi-Han", ""], ["Colburn", "Alex", ""], ["Shan", "Qi", ""], ["Wonka", "Peter", ""], ["Chu", "Hung-Kuo", ""], ["Hoiem", "Derek", ""]]}, {"id": "1910.04104", "submitter": "Ruihang Chu", "authors": "Ruihang Chu, Yifan Sun, Yadong Li, Zheng Liu, Chi Zhang, Yichen Wei", "title": "Vehicle Re-identification with Viewpoint-aware Metric Learning", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers vehicle re-identification (re-ID) problem. The extreme\nviewpoint variation (up to 180 degrees) poses great challenges for existing\napproaches. Inspired by the behavior in human's recognition process, we propose\na novel viewpoint-aware metric learning approach. It learns two metrics for\nsimilar viewpoints and different viewpoints in two feature spaces,\nrespectively, giving rise to viewpoint-aware network (VANet). During training,\ntwo types of constraints are applied jointly. During inference, viewpoint is\nfirstly estimated and the corresponding metric is used. Experimental results\nconfirm that VANet significantly improves re-ID accuracy, especially when the\npair is observed from different viewpoints. Our method establishes the new\nstate-of-the-art on two benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:32:01 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Chu", "Ruihang", ""], ["Sun", "Yifan", ""], ["Li", "Yadong", ""], ["Liu", "Zheng", ""], ["Zhang", "Chi", ""], ["Wei", "Yichen", ""]]}, {"id": "1910.04107", "submitter": "Konstantin Bulatov", "authors": "Konstantin Bulatov, Boris Savelyev, Vladimir V. Arlazarov", "title": "Next integrated result modelling for stopping the text field recognition\n  process in a video using a result model with per-character alternatives", "comments": "6 pages, 3 figures, 1 table, submitted and accepted for the 12th\n  International Conference on Machine Vision (ICMV 2019)", "journal-ref": "Proc. SPIE 11433 ICMV-2019 (2020), 114332M", "doi": "10.1117/12.2559447", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of document analysis and recognition using mobile devices for\ncapturing, and the field of object recognition in a video stream, an important\nproblem is determining the time when the capturing process should be stopped.\nEfficient stopping influences not only the total time spent for performing\nrecognition and data entry, but the expected accuracy of the result as well.\nThis paper is directed on extending the stopping method based on next\nintegrated recognition result modelling, in order for it to be used within a\nstring result recognition model with per-character alternatives. The stopping\nmethod and notes on its extension are described, and experimental evaluation is\nperformed on an open dataset MIDV-500. The method was compares with previously\npublished methods based on input observations clustering. The obtained results\nindicate that the stopping method based on the next integrated result modelling\nallows to achieve higher accuracy, even when compared with the best achievable\nconfiguration of the competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:43:42 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Bulatov", "Konstantin", ""], ["Savelyev", "Boris", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "1910.04142", "submitter": "Arunkumar Byravan", "authors": "Arunkumar Byravan, Jost Tobias Springenberg, Abbas Abdolmaleki, Roland\n  Hafner, Michael Neunert, Thomas Lampe, Noah Siegel, Nicolas Heess, Martin\n  Riedmiller", "title": "Imagined Value Gradients: Model-Based Policy Optimization with\n  Transferable Latent Dynamics Models", "comments": "To appear at the 3rd annual Conference on Robot Learning, Osaka,\n  Japan (CoRL 2019). 24 pages including appendix (main paper - 8 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are masters at quickly learning many complex tasks, relying on an\napproximate understanding of the dynamics of their environments. In much the\nsame way, we would like our learning agents to quickly adapt to new tasks. In\nthis paper, we explore how model-based Reinforcement Learning (RL) can\nfacilitate transfer to new tasks. We develop an algorithm that learns an\naction-conditional, predictive model of expected future observations, rewards\nand values from which a policy can be derived by following the gradient of the\nestimated value along imagined trajectories. We show how robust policy\noptimization can be achieved in robot manipulation tasks even with approximate\nmodels that are learned directly from vision and proprioception. We evaluate\nthe efficacy of our approach in a transfer learning scenario, re-using\npreviously learned models on tasks with different reward structures and visual\ndistractors, and show a significant improvement in learning speed compared to\nstrong off-policy baselines. Videos with results can be found at\nhttps://sites.google.com/view/ivg-corl19\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 17:37:52 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Byravan", "Arunkumar", ""], ["Springenberg", "Jost Tobias", ""], ["Abdolmaleki", "Abbas", ""], ["Hafner", "Roland", ""], ["Neunert", "Michael", ""], ["Lampe", "Thomas", ""], ["Siegel", "Noah", ""], ["Heess", "Nicolas", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1910.04149", "submitter": "Kwonjoon Lee", "authors": "Siyang Wang, Justin Lazarow, Kwonjoon Lee, Zhuowen Tu", "title": "Unaligned Image-to-Sequence Transformation with Loop Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of modeling sequential visual phenomena. Given examples\nof a phenomena that can be divided into discrete time steps, we aim to take an\ninput from any such time and realize this input at all other time steps in the\nsequence. Furthermore, we aim to do this without ground-truth aligned sequences\n-- avoiding the difficulties needed for gathering aligned data. This\ngeneralizes the unpaired image-to-image problem from generating pairs to\ngenerating sequences. We extend cycle consistency to loop consistency and\nalleviate difficulties associated with learning in the resulting long chains of\ncomputation. We show competitive results compared to existing image-to-image\ntechniques when modeling several different data sets including the Earth's\nseasons and aging of human faces.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 17:50:45 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Wang", "Siyang", ""], ["Lazarow", "Justin", ""], ["Lee", "Kwonjoon", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1910.04254", "submitter": "Alexander Preuhs", "authors": "Alexander Preuhs, Michael Manhart, Philipp Roser, Bernhard Stimpel,\n  Christopher Syben, Marios Psychogios, Markus Kowarschik, Andreas Maier", "title": "Image Quality Assessment for Rigid Motion Compensation", "comments": "Accepted at MedNeurips 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic stroke imaging with C-arm cone-beam computed tomography (CBCT)\nenables reduction of time-to-therapy for endovascular procedures. However, the\nprolonged acquisition time compared to helical CT increases the likelihood of\nrigid patient motion. Rigid motion corrupts the geometry alignment assumed\nduring reconstruction, resulting in image blurring or streaking artifacts. To\nreestablish the geometry, we estimate the motion trajectory by an autofocus\nmethod guided by a neural network, which was trained to regress the\nreprojection error, based on the image information of a reconstructed slice.\nThe network was trained with CBCT scans from 19 patients and evaluated using an\nadditional test patient. It adapts well to unseen motion amplitudes and\nachieves superior results in a motion estimation benchmark compared to the\ncommonly used entropy-based method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 21:03:45 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 16:01:13 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Preuhs", "Alexander", ""], ["Manhart", "Michael", ""], ["Roser", "Philipp", ""], ["Stimpel", "Bernhard", ""], ["Syben", "Christopher", ""], ["Psychogios", "Marios", ""], ["Kowarschik", "Markus", ""], ["Maier", "Andreas", ""]]}, {"id": "1910.04256", "submitter": "Chirag Agarwal", "authors": "Chirag Agarwal, Anh Nguyen", "title": "Explaining image classifiers by removing input features using generative\n  models", "comments": "Accepted to Asian Conference on Computer Vision (ACCV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perturbation-based explanation methods often measure the contribution of an\ninput feature to an image classifier's outputs by heuristically removing it via\ne.g. blurring, adding noise, or graying out, which often produce unrealistic,\nout-of-samples. Instead, we propose to integrate a generative inpainter into\nthree representative attribution methods to remove an input feature. Our\nproposed change improved all three methods in (1) generating more plausible\ncounterfactual samples under the true data distribution; (2) being more\naccurate according to three metrics: object localization, deletion, and\nsaliency metrics; and (3) being more robust to hyperparameter changes. Our\nfindings were consistent across both ImageNet and Places365 datasets and two\ndifferent pairs of classifiers and inpainters.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 21:08:25 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 03:36:07 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 17:52:27 GMT"}, {"version": "v4", "created": "Sat, 25 Jul 2020 09:14:35 GMT"}, {"version": "v5", "created": "Thu, 30 Jul 2020 01:27:39 GMT"}, {"version": "v6", "created": "Sun, 4 Oct 2020 19:27:15 GMT"}, {"version": "v7", "created": "Tue, 6 Oct 2020 16:08:42 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Agarwal", "Chirag", ""], ["Nguyen", "Anh", ""]]}, {"id": "1910.04287", "submitter": "Saeed Anwar", "authors": "Muhammad Tahir, Saeed Anwar, and Ajmal Mian", "title": "Deep localization of protein structures in fluorescence microscopy\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate localization of proteins from fluorescence microscopy images is a\nchallenging task due to the inter-class similarities and intra-class\ndisparities introducing grave concerns in addressing multi-class classification\nproblems. Conventional machine learning-based image prediction relies heavily\non pre-processing such as normalization and segmentation followed by\nhand-crafted feature extraction before classification to identify useful and\ninformative as well as application specific features.We propose an end-to-end\nProtein Localization Convolutional Neural Network (PLCNN) that classifies\nprotein localization images more accurately and reliably. PLCNN directly\nprocesses raw imagery without involving any pre-processing steps and produces\noutputs without any customization or parameter adjustment for a particular\ndataset. The output of our approach is computed from probabilities produced by\nthe network. Experimental analysis is performed on five publicly available\nbenchmark datasets. PLCNN consistently outperformed the existing\nstate-of-the-art approaches from machine learning and deep architectures.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 22:53:19 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 01:16:04 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Tahir", "Muhammad", ""], ["Anwar", "Saeed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1910.04324", "submitter": "Younkwan Lee", "authors": "Younkwan Lee, Jiwon Jun, Yoojin Hong, Moongu Jeon", "title": "Practical License Plate Recognition in Unconstrained Surveillance\n  Systems with Adversarial Super-Resolution", "comments": "Accepted at VISAPP, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although most current license plate (LP) recognition applications have been\nsignificantly advanced, they are still limited to ideal environments where\ntraining data are carefully annotated with constrained scenes. In this paper,\nwe propose a novel license plate recognition method to handle unconstrained\nreal world traffic scenes. To overcome these difficulties, we use adversarial\nsuper-resolution (SR), and one-stage character segmentation and recognition.\nCombined with a deep convolutional network based on VGG-net, our method\nprovides simple but reasonable training procedure. Moreover, we introduce\nGIST-LP, a challenging LP dataset where image samples are effectively collected\nfrom unconstrained surveillance scenes. Experimental results on AOLP and\nGIST-LP dataset illustrate that our method, without any scene-specific\nadaptation, outperforms current LP recognition approaches in accuracy and\nprovides visual enhancement in our SR results that are easier to understand\nthan original data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 01:37:21 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Lee", "Younkwan", ""], ["Jun", "Jiwon", ""], ["Hong", "Yoojin", ""], ["Jeon", "Moongu", ""]]}, {"id": "1910.04326", "submitter": "Younkwan Lee", "authors": "Younkwan Lee, Juhyun Lee, Yoojin Hong, YeongMin Ko, Moongu Jeon", "title": "Unconstrained Road Marking Recognition with Generative Adversarial\n  Networks", "comments": "Accepted at IEEE Intelligent Vehicles Symposium (IV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent road marking recognition has achieved great success in the past few\nyears along with the rapid development of deep learning. Although considerable\nadvances have been made, they are often over-dependent on unrepresentative\ndatasets and constrained conditions. In this paper, to overcome these\ndrawbacks, we propose an alternative method that achieves higher accuracy and\ngenerates high-quality samples as data augmentation. With the following two\nmajor contributions: 1) The proposed deblurring network can successfully\nrecover a clean road marking from a blurred one by adopting generative\nadversarial networks (GAN). 2) The proposed data augmentation method, based on\nmutual information, can preserve and learn semantic context from the given\ndataset. We construct and train a class-conditional GAN to increase the size of\ntraining set, which makes it suitable to recognize target. The experimental\nresults have shown that our proposed framework generates deblurred clean\nsamples from blurry ones, and outperforms other methods even with unconstrained\nroad marking datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 01:53:50 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Lee", "Younkwan", ""], ["Lee", "Juhyun", ""], ["Hong", "Yoojin", ""], ["Ko", "YeongMin", ""], ["Jeon", "Moongu", ""]]}, {"id": "1910.04331", "submitter": "Haoran Dou", "authors": "Haoran Dou, Xin Yang, Jikuan Qian, Wufeng Xue, Hao Qin, Xu Wang,\n  Lequan Yu, Shujun Wang, Yi Xiong, Pheng-Ann Heng, Dong Ni", "title": "Agent with Warm Start and Active Termination for Plane Localization in\n  3D Ultrasound", "comments": "9 pages, 5 figures, 1 table. Accepted by MICCAI 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard plane localization is crucial for ultrasound (US) diagnosis. In\nprenatal US, dozens of standard planes are manually acquired with a 2D probe.\nIt is time-consuming and operator-dependent. In comparison, 3D US containing\nmultiple standard planes in one shot has the inherent advantages of less\nuser-dependency and more efficiency. However, manual plane localization in US\nvolume is challenging due to the huge search space and large fetal posture\nvariation. In this study, we propose a novel reinforcement learning (RL)\nframework to automatically localize fetal brain standard planes in 3D US. Our\ncontribution is two-fold. First, we equip the RL framework with a\nlandmark-aware alignment module to provide warm start and strong spatial bounds\nfor the agent actions, thus ensuring its effectiveness. Second, instead of\npassively and empirically terminating the agent inference, we propose a\nrecurrent neural network based strategy for active termination of the agent's\ninteraction procedure. This improves both the accuracy and efficiency of the\nlocalization system. Extensively validated on our in-house large dataset, our\napproach achieves the accuracy of 3.4mm/9.6{\\deg} and 2.7mm/9.1{\\deg} for the\ntranscerebellar and transthalamic plane localization, respectively. Ourproposed\nRL framework is general and has the potential to improve the efficiency and\nstandardization of US scanning.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 02:21:52 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Dou", "Haoran", ""], ["Yang", "Xin", ""], ["Qian", "Jikuan", ""], ["Xue", "Wufeng", ""], ["Qin", "Hao", ""], ["Wang", "Xu", ""], ["Yu", "Lequan", ""], ["Wang", "Shujun", ""], ["Xiong", "Yi", ""], ["Heng", "Pheng-Ann", ""], ["Ni", "Dong", ""]]}, {"id": "1910.04335", "submitter": "Marvin Chanc\\'an", "authors": "Marvin Chanc\\'an and Michael Milford", "title": "CityLearn: Diverse Real-World Environments for Sample-Efficient\n  Navigation Policy Learning", "comments": "Preprint version of article accepted to ICRA 2020", "journal-ref": "2020 IEEE International Conference on Robotics and Automation\n  (ICRA)", "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual navigation tasks in real-world environments often require both\nself-motion and place recognition feedback. While deep reinforcement learning\nhas shown success in solving these perception and decision-making problems in\nan end-to-end manner, these algorithms require large amounts of experience to\nlearn navigation policies from high-dimensional data, which is generally\nimpractical for real robots due to sample complexity. In this paper, we address\nthese problems with two main contributions. We first leverage place recognition\nand deep learning techniques combined with goal destination feedback to\ngenerate compact, bimodal image representations that can then be used to\neffectively learn control policies from a small amount of experience. Second,\nwe present an interactive framework, CityLearn, that enables for the first time\ntraining and deployment of navigation algorithms across city-sized, realistic\nenvironments with extreme visual appearance changes. CityLearn features more\nthan 10 benchmark datasets, often used in visual place recognition and\nautonomous driving research, including over 100 recorded traversals across 60\ncities around the world. We evaluate our approach on two CityLearn\nenvironments, training our navigation policy on a single traversal. Results\nshow our method can be over 2 orders of magnitude faster than when using raw\nimages, and can also generalize across extreme visual changes including day to\nnight and summer to winter transitions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 02:34:34 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 10:24:13 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chanc\u00e1n", "Marvin", ""], ["Milford", "Michael", ""]]}, {"id": "1910.04357", "submitter": "Wei Xu", "authors": "Xinyi Huang, Suphanut Jamonnak, Ye Zhao, Boyu Wang, Minh Hoai, Kevin\n  Yager, Wei Xu", "title": "Visual Understanding of Multiple Attributes Learning Model of X-Ray\n  Scattering Images", "comments": "5 pages, 2 figures, ICCV conference co-held XAIC workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This extended abstract presents a visualization system, which is designed for\ndomain scientists to visually understand their deep learning model of\nextracting multiple attributes in x-ray scattering images. The system focuses\non studying the model behaviors related to multiple structural attributes. It\nallows users to explore the images in the feature space, the classification\noutput of different attributes, with respect to the actual attributes labelled\nby domain scientists. Abundant interactions allow users to flexibly select\ninstance images, their clusters, and compare them visually in details. Two\npreliminary case studies demonstrate its functionalities and usefulness.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 03:51:58 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Huang", "Xinyi", ""], ["Jamonnak", "Suphanut", ""], ["Zhao", "Ye", ""], ["Wang", "Boyu", ""], ["Hoai", "Minh", ""], ["Yager", "Kevin", ""], ["Xu", "Wei", ""]]}, {"id": "1910.04392", "submitter": "Yonglin Tian", "authors": "Yonglin Tian, Kunfeng Wang, Yuang Wang, Yulin Tian, Zilei Wang,\n  Fei-Yue Wang", "title": "Adaptive and Azimuth-Aware Fusion Network of Multimodal Local Features\n  for 3D Object Detection", "comments": "Accepted by Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the construction of stronger local features and the\neffective fusion of image and LiDAR data. We adopt different modalities of\nLiDAR data to generate richer features and present an adaptive and\nazimuth-aware network to aggregate local features from image, bird's eye view\nmaps and point cloud. Our network mainly consists of three subnetworks: ground\nplane estimation network, region proposal network and adaptive fusion network.\nThe ground plane estimation network extracts features of point cloud and\npredicts the parameters of a plane which are used for generating abundant 3D\nanchors. The region proposal network generates features of image and bird's eye\nview maps to output region proposals. To integrate heterogeneous image and\npoint cloud features, the adaptive fusion network explicitly adjusts the\nintensity of multiple local features and achieves the orientation consistency\nbetween image and LiDAR data by introduce an azimuth-aware fusion module.\nExperiments are conducted on KITTI dataset and the results validate the\nadvantages of our aggregation of multimodal local features and the adaptive\nfusion network.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 07:07:01 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 10:13:08 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Tian", "Yonglin", ""], ["Wang", "Kunfeng", ""], ["Wang", "Yuang", ""], ["Tian", "Yulin", ""], ["Wang", "Zilei", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "1910.04396", "submitter": "Junyeop Lee", "authors": "Junyeop Lee, Sungrae Park, Jeonghun Baek, Seong Joon Oh, Seonghyeon\n  Kim, Hwalsuk Lee", "title": "On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition (STR) is the task of recognizing character sequences\nin natural scenes. While there have been great advances in STR methods, current\nmethods still fail to recognize texts in arbitrary shapes, such as heavily\ncurved or rotated texts, which are abundant in daily life (e.g. restaurant\nsigns, product labels, company logos, etc). This paper introduces a novel\narchitecture to recognizing texts of arbitrary shapes, named Self-Attention\nText Recognition Network (SATRN), which is inspired by the Transformer. SATRN\nutilizes the self-attention mechanism to describe two-dimensional (2D) spatial\ndependencies of characters in a scene text image. Exploiting the full-graph\npropagation of self-attention, SATRN can recognize texts with arbitrary\narrangements and large inter-character spacing. As a result, SATRN outperforms\nexisting STR models by a large margin of 5.7 pp on average in \"irregular text\"\nbenchmarks. We provide empirical analyses that illustrate the inner mechanisms\nand the extent to which the model is applicable (e.g. rotated and multi-line\ntext). We will open-source the code.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 07:20:54 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Lee", "Junyeop", ""], ["Park", "Sungrae", ""], ["Baek", "Jeonghun", ""], ["Oh", "Seong Joon", ""], ["Kim", "Seonghyeon", ""], ["Lee", "Hwalsuk", ""]]}, {"id": "1910.04397", "submitter": "Kyujin Shim", "authors": "Junyoung Byun, Kyujin Shim, Changick Kim", "title": "BitNet: Learning-Based Bit-Depth Expansion", "comments": "Accepted by ACCV 2018, Authors Byun and Shim contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bit-depth is the number of bits for each color channel of a pixel in an\nimage. Although many modern displays support unprecedented higher bit-depth to\nshow more realistic and natural colors with a high dynamic range, most media\nsources are still in bit-depth of 8 or lower. Since insufficient bit-depth may\ngenerate annoying false contours or lose detailed visual appearance, bit-depth\nexpansion (BDE) from low bit-depth (LBD) images to high bit-depth (HBD) images\nbecomes more and more important. In this paper, we adopt a learning-based\napproach for BDE and propose a novel CNN-based bit-depth expansion network\n(BitNet) that can effectively remove false contours and restore visual details\nat the same time. We have carefully designed our BitNet based on an\nencoder-decoder architecture with dilated convolutions and a novel multi-scale\nfeature integration. We have performed various experiments with four different\ndatasets including MIT-Adobe FiveK, Kodak, ESPL v2, and TESTIMAGES, and our\nproposed BitNet has achieved state-of-the-art performance in terms of PSNR and\nSSIM among other existing BDE methods and famous CNN-based image processing\nnetworks. Unlike previous methods that separately process each color channel,\nwe treat all RGB channels at once and have greatly improved color restoration.\nIn addition, our network has shown the fastest computational speed in near\nreal-time.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 07:21:19 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Byun", "Junyoung", ""], ["Shim", "Kyujin", ""], ["Kim", "Changick", ""]]}, {"id": "1910.04416", "submitter": "Kashif Ahmad", "authors": "Syed Zohaib, Kashif Ahmad, Nicola Conci, Ala Al-Fuqaha", "title": "Sentiment Analysis from Images of Natural Disasters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media have been widely exploited to detect and gather relevant\ninformation about opinions and events. However, the relevance of the\ninformation is very subjective and rather depends on the application and the\nend-users. In this article, we tackle a specific facet of social media data\nprocessing, namely the sentiment analysis of disaster-related images by\nconsidering people's opinions, attitudes, feelings and emotions. We analyze how\nvisual sentiment analysis can improve the results for the\nend-users/beneficiaries in terms of mining information from social media. We\nalso identify the challenges and related applications, which could help\ndefining a benchmark for future research efforts in visual sentiment analysis.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 07:59:11 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Zohaib", "Syed", ""], ["Ahmad", "Kashif", ""], ["Conci", "Nicola", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "1910.04456", "submitter": "Soumick Chatterjee", "authors": "Chompunuch Sarasaen, Soumick Chatterjee, Mario Breitkopf, Domenico\n  Iuso, Georg Rose and Oliver Speck", "title": "Breathing deformation model -- application to multi-resolution abdominal\n  MRI", "comments": "2019 41st Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC)", "journal-ref": null, "doi": "10.1109/EMBC.2019.885770", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic MRI is a technique of acquiring a series of images continuously to\nfollow the physiological changes over time. However, such fast imaging results\nin low resolution images. In this work, abdominal deformation model computed\nfrom dynamic low resolution images have been applied to high resolution image,\nacquired previously, to generate dynamic high resolution MRI. Dynamic low\nresolution images were simulated into different breathing phases (inhale and\nexhale). Then, the image registration between breathing time points was\nperformed using the B-spline SyN deformable model and using cross-correlation\nas a similarity metric. The deformation model between different breathing\nphases were estimated from highly undersampled data. This deformation model was\nthen applied to the high resolution images to obtain high resolution images of\ndifferent breathing phases. The results indicated that the deformation model\ncould be computed from relatively very low resolution images.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 09:53:43 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Sarasaen", "Chompunuch", ""], ["Chatterjee", "Soumick", ""], ["Breitkopf", "Mario", ""], ["Iuso", "Domenico", ""], ["Rose", "Georg", ""], ["Speck", "Oliver", ""]]}, {"id": "1910.04465", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Yi Yang", "title": "Searching for A Robust Neural Architecture in Four GPU Hours", "comments": "Minor modifications to the CVPR 2019 camera-ready version (add code\n  link)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional neural architecture search (NAS) approaches are based on\nreinforcement learning or evolutionary strategy, which take more than 3000 GPU\nhours to find a good model on CIFAR-10. We propose an efficient NAS approach\nlearning to search by gradient descent. Our approach represents the search\nspace as a directed acyclic graph (DAG). This DAG contains billions of\nsub-graphs, each of which indicates a kind of neural architecture. To avoid\ntraversing all the possibilities of the sub-graphs, we develop a differentiable\nsampler over the DAG. This sampler is learnable and optimized by the validation\nloss after training the sampled architecture. In this way, our approach can be\ntrained in an end-to-end fashion by gradient descent, named Gradient-based\nsearch using Differentiable Architecture Sampler (GDAS). In experiments, we can\nfinish one searching procedure in four GPU hours on CIFAR-10, and the\ndiscovered model obtains a test error of 2.82\\% with only 2.5M parameters,\nwhich is on par with the state-of-the-art. Code is publicly available on\nGitHub: https://github.com/D-X-Y/NAS-Projects.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 10:14:48 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 05:43:51 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Dong", "Xuanyi", ""], ["Yang", "Yi", ""]]}, {"id": "1910.04473", "submitter": "Shusuke Takahama", "authors": "Shusuke Takahama, Yusuke Kurose, Yusuke Mukuta, Hiroyuki Abe, Masashi\n  Fukayama, Akihiko Yoshizawa, Masanobu Kitagawa, Tatsuya Harada", "title": "Multi-Stage Pathological Image Classification using Semantic\n  Segmentation", "comments": "Accepted to ICCV2019. ICCV paper version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological image analysis is an essential process for the discovery of\ndiseases such as cancer. However, it is challenging to train CNN on whole slide\nimages (WSIs) of gigapixel resolution considering the available memory\ncapacity. Most of the previous works divide high resolution WSIs into small\nimage patches and separately input them into the model to classify it as a\ntumor or a normal tissue. However, patch-based classification uses only\npatch-scale local information but ignores the relationship between neighboring\npatches. If we consider the relationship of neighboring patches and global\nfeatures, we can improve the classification performance. In this paper, we\npropose a new model structure combining the patch-based classification model\nand whole slide-scale segmentation model in order to improve the prediction\nperformance of automatic pathological diagnosis. We extract patch features from\nthe classification model and input them into the segmentation model to obtain a\nwhole slide tumor probability heatmap. The classification model considers\npatch-scale local features, and the segmentation model can take global\ninformation into account. We also propose a new optimization method that\nretains gradient information and trains the model partially for end-to-end\nlearning with limited GPU memory capacity. We apply our method to the\ntumor/normal prediction on WSIs and the classification performance is improved\ncompared with the conventional patch-based method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 10:45:22 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Takahama", "Shusuke", ""], ["Kurose", "Yusuke", ""], ["Mukuta", "Yusuke", ""], ["Abe", "Hiroyuki", ""], ["Fukayama", "Masashi", ""], ["Yoshizawa", "Akihiko", ""], ["Kitagawa", "Masanobu", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1910.04476", "submitter": "Zhi-Song Liu", "authors": "Zhi-Song Liu, Li-Wen Wang, Chu-Tak Li, Wan-Chi Siu, Yui-Lam Chan", "title": "Image Super-Resolution via Attention based Back Projection Networks", "comments": "9 pages, 7 figures, ABPN", "journal-ref": "IEEE International Conference on Computer Vision 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based image Super-Resolution (SR) has shown rapid development\ndue to its ability of big data digestion. Generally, deeper and wider networks\ncan extract richer feature maps and generate SR images with remarkable quality.\nHowever, the more complex network we have, the more time consumption is\nrequired for practical applications. It is important to have a simplified\nnetwork for efficient image SR. In this paper, we propose an Attention based\nBack Projection Network (ABPN) for image super-resolution. Similar to some\nrecent works, we believe that the back projection mechanism can be further\ndeveloped for SR. Enhanced back projection blocks are suggested to iteratively\nupdate low- and high-resolution feature residues. Inspired by recent studies on\nattention models, we propose a Spatial Attention Block (SAB) to learn the\ncross-correlation across features at different layers. Based on the assumption\nthat a good SR image should be close to the original LR image after\ndown-sampling. We propose a Refined Back Projection Block (RBPB) for final\nreconstruction. Extensive experiments on some public and AIM2019 Image\nSuper-Resolution Challenge datasets show that the proposed ABPN can provide\nstate-of-the-art or even better performance in both quantitative and\nqualitative measurements.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 10:49:20 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Liu", "Zhi-Song", ""], ["Wang", "Li-Wen", ""], ["Li", "Chu-Tak", ""], ["Siu", "Wan-Chi", ""], ["Chan", "Yui-Lam", ""]]}, {"id": "1910.04488", "submitter": "Stefano Cerri", "authors": "Sveinn P\\'alsson, Stefano Cerri, Andrea Dittadi, Koen Van Leemput", "title": "Semi-Supervised Variational Autoencoder for Survival Prediction", "comments": "Published in the pre-conference proceeding of \"2019 International\n  MICCAI BraTS Challenge\"", "journal-ref": null, "doi": "10.1007/978-3-030-46643-5_12", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a semi-supervised variational autoencoder for\nclassification of overall survival groups from tumor segmentation masks. The\nmodel can use the output of any tumor segmentation algorithm, removing all\nassumptions on the scanning platform and the specific type of pulse sequences\nused, thereby increasing its generalization properties. Due to its\nsemi-supervised nature, the method can learn to classify survival time by using\na relatively small number of labeled subjects. We validate our model on the\npublicly available dataset from the Multimodal Brain Tumor Segmentation\nChallenge (BraTS) 2019.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 11:31:10 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["P\u00e1lsson", "Sveinn", ""], ["Cerri", "Stefano", ""], ["Dittadi", "Andrea", ""], ["Van Leemput", "Koen", ""]]}, {"id": "1910.04562", "submitter": "Chufeng Tang", "authors": "Chufeng Tang, Lu Sheng, Zhaoxiang Zhang, Xiaolin Hu", "title": "Improving Pedestrian Attribute Recognition With Weakly-Supervised\n  Multi-Scale Attribute-Specific Localization", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian attribute recognition has been an emerging research topic in the\narea of video surveillance. To predict the existence of a particular attribute,\nit is demanded to localize the regions related to the attribute. However, in\nthis task, the region annotations are not available. How to carve out these\nattribute-related regions remains challenging. Existing methods applied\nattribute-agnostic visual attention or heuristic body-part localization\nmechanisms to enhance the local feature representations, while neglecting to\nemploy attributes to define local feature areas. We propose a flexible\nAttribute Localization Module (ALM) to adaptively discover the most\ndiscriminative regions and learns the regional features for each attribute at\nmultiple levels. Moreover, a feature pyramid architecture is also introduced to\nenhance the attribute-specific localization at low-levels with high-level\nsemantic guidance. The proposed framework does not require additional region\nannotations and can be trained end-to-end with multi-level deep supervision.\nExtensive experiments show that the proposed method achieves state-of-the-art\nresults on three pedestrian attribute datasets, including PETA, RAP, and\nPA-100K.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 13:44:18 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Tang", "Chufeng", ""], ["Sheng", "Lu", ""], ["Zhang", "Zhaoxiang", ""], ["Hu", "Xiaolin", ""]]}, {"id": "1910.04576", "submitter": "Yuhui Xu", "authors": "Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Wenrui Dai,\n  Yingyong Qi, Yiran Chen, Weiyao Lin, Hongkai Xiong", "title": "Trained Rank Pruning for Efficient Deep Neural Networks", "comments": "overlap with arXiv:1812.02402, in order to merge the two submissions\n  such that withdraw this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accelerate DNNs inference, low-rank approximation has been widely adopted\nbecause of its solid theoretical rationale and efficient implementations.\nSeveral previous works attempted to directly approximate a pre-trained model by\nlow-rank decomposition; however, small approximation errors in parameters can\nripple over a large prediction loss. Apparently, it is not optimal to separate\nlow-rank approximation from training. Unlike previous works, this paper\nintegrates low rank approximation and regularization into the training process.\nWe propose Trained Rank Pruning (TRP), which alternates between low rank\napproximation and training. TRP maintains the capacity of the original network\nwhile imposing low-rank constraints during training. A nuclear regularization\noptimized by stochastic sub-gradient descent is utilized to further promote low\nrank in TRP. Networks trained with TRP has a low-rank structure in nature, and\nis approximated with negligible performance loss, thus eliminating fine-tuning\nafter low rank approximation. The proposed method is comprehensively evaluated\non CIFAR-10 and ImageNet, outperforming previous compression counterparts using\nlow rank approximation. Our code is available at:\nhttps://github.com/yuhuixu1993/Trained-Rank-Pruning.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 07:27:33 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 03:25:39 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 02:13:33 GMT"}, {"version": "v4", "created": "Thu, 23 Jan 2020 21:02:36 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Xu", "Yuhui", ""], ["Li", "Yuxi", ""], ["Zhang", "Shuai", ""], ["Wen", "Wei", ""], ["Wang", "Botao", ""], ["Dai", "Wenrui", ""], ["Qi", "Yingyong", ""], ["Chen", "Yiran", ""], ["Lin", "Weiyao", ""], ["Xiong", "Hongkai", ""]]}, {"id": "1910.04597", "submitter": "Ben Glocker", "authors": "Ben Glocker, Robert Robinson, Daniel C. Castro, Qi Dou, Ender\n  Konukoglu", "title": "Machine Learning with Multi-Site Imaging Data: An Empirical Study on the\n  Impact of Scanner Effects", "comments": "Presented at the Medical Imaging meets NeurIPS Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is an empirical study to investigate the impact of scanner effects when\nusing machine learning on multi-site neuroimaging data. We utilize structural\nT1-weighted brain MRI obtained from two different studies, Cam-CAN and UK\nBiobank. For the purpose of our investigation, we construct a dataset\nconsisting of brain scans from 592 age- and sex-matched individuals, 296\nsubjects from each original study. Our results demonstrate that even after\ncareful pre-processing with state-of-the-art neuroimaging pipelines a\nclassifier can easily distinguish between the origin of the data with very high\naccuracy. Our analysis on the example application of sex classification\nsuggests that current approaches to harmonize data are unable to remove\nscanner-specific bias leading to overly optimistic performance estimates and\npoor generalization. We conclude that multi-site data harmonization remains an\nopen challenge and particular care needs to be taken when using such data with\nadvanced machine learning methods for predictive modelling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 14:24:42 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Glocker", "Ben", ""], ["Robinson", "Robert", ""], ["Castro", "Daniel C.", ""], ["Dou", "Qi", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1910.04639", "submitter": "Aaron Hertzmann", "authors": "Aaron Hertzmann", "title": "Visual Indeterminacy in GAN Art", "comments": "Leonardo / SIGGRAPH 2020 Art Papers", "journal-ref": "Leonardo, Volume 53, Issue 4, August 2020", "doi": "10.1162/leon_a_01930", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores visual indeterminacy as a description for artwork created\nwith Generative Adversarial Networks (GANs). Visual indeterminacy describes\nimages which appear to depict real scenes, but, on closer examination, defy\ncoherent spatial interpretation. GAN models seem to be predisposed to producing\nindeterminate images, and indeterminacy is a key feature of much modern\nrepresentational art, as well as most GAN art. It is hypothesized that\nindeterminacy is a consequence of a powerful-but-imperfect image synthesis\nmodel that must combine general classes of objects, scenes, and textures.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:19:11 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 18:17:26 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 03:23:11 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Hertzmann", "Aaron", ""]]}, {"id": "1910.04641", "submitter": "Fida Mohammad Thoker", "authors": "Fida Mohammad Thoker and Juergen Gall", "title": "Cross-modal knowledge distillation for action recognition", "comments": "Published in: 2019 IEEE International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem how a network for action recognition\nthat has been trained on a modality like RGB videos can be adapted to recognize\nactions for another modality like sequences of 3D human poses. To this end, we\nextract the knowledge of the trained teacher network for the source modality\nand transfer it to a small ensemble of student networks for the target\nmodality. For the cross-modal knowledge distillation, we do not require any\nannotated data. Instead we use pairs of sequences of both modalities as\nsupervision, which are straightforward to acquire. In contrast to previous\nworks for knowledge distillation that use a KL-loss, we show that the\ncross-entropy loss together with mutual learning of a small ensemble of student\nnetworks performs better. In fact, the proposed approach for cross-modal\nknowledge distillation nearly achieves the accuracy of a student network\ntrained with full supervision.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:22:53 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Thoker", "Fida Mohammad", ""], ["Gall", "Juergen", ""]]}, {"id": "1910.04668", "submitter": "Aljo\\v{s}a O\\v{s}ep", "authors": "Johannes Gro{\\ss}, Aljosa Osep, Bastian Leibe", "title": "AlignNet-3D: Fast Point Cloud Registration of Partially Observed Objects", "comments": "Presented at 3DV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods tackling multi-object tracking need to estimate the number of targets\nin the sensing area as well as to estimate their continuous state. While the\nmajority of existing methods focus on data association, precise state (3D pose)\nestimation is often only coarsely estimated by approximating targets with\ncentroids or (3D) bounding boxes. However, in automotive scenarios, motion\nperception of surrounding agents is critical and inaccuracies in the vehicle\nclose-range can have catastrophic consequences. In this work, we focus on\nprecise 3D track state estimation and propose a learning-based approach for\nobject-centric relative motion estimation of partially observed objects.\nInstead of approximating targets with their centroids, our approach is capable\nof utilizing noisy 3D point segments of objects to estimate their motion. To\nthat end, we propose a simple, yet effective and efficient network, \\method,\nthat learns to align point clouds. Our evaluation on two different datasets\ndemonstrates that our method outperforms computationally expensive, global 3D\nregistration methods while being significantly more efficient. We make our\ndata, code, and models available at\nhttps://www.vision.rwth-aachen.de/page/alignnet.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:15:45 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gro\u00df", "Johannes", ""], ["Osep", "Aljosa", ""], ["Leibe", "Bastian", ""]]}, {"id": "1910.04695", "submitter": "Ethan Shaotran", "authors": "Ethan Shaotran, Jonathan J. Cruz, Vijay Janapa Reddi", "title": "GLADAS: Gesture Learning for Advanced Driver Assistance Systems", "comments": "9 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-computer interaction (HCI) is crucial for the safety of lives as\nautonomous vehicles (AVs) become commonplace. Yet, little effort has been put\ntoward ensuring that AVs understand humans on the road. In this paper, we\npresent GLADAS, a simulator-based research platform designed to teach AVs to\nunderstand pedestrian hand gestures. GLADAS supports the training, testing, and\nvalidation of deep learning-based self-driving car gesture recognition systems.\nWe focus on gestures as they are a primordial (i.e, natural and common) way to\ninteract with cars. To the best of our knowledge, GLADAS is the first system of\nits kind designed to provide an infrastructure for further research into\nhuman-AV interaction. We also develop a hand gesture recognition algorithm for\nself-driving cars, using GLADAS to evaluate its performance. Our results show\nthat an AV understands human gestures 85.91% of the time, reinforcing the need\nfor further research into human-AV interaction.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:55:45 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Shaotran", "Ethan", ""], ["Cruz", "Jonathan J.", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "1910.04725", "submitter": "Gihan Jayatilaka", "authors": "Gihan Jayatilaka, Harshana Weligampola, Suren Sritharan, Pankayraj\n  Pathmanathan, Roshan Ragel and Isuru Nawinne", "title": "Non-contact Infant Sleep Apnea Detection", "comments": "Gihan Jayatilaka, Harshana Weligampola and Suren Sritharan are\n  equally contributing authors", "journal-ref": null, "doi": "10.1109/ICIIS47346.2019.9063269", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep apnea is a breathing disorder where a person repeatedly stops breathing\nin sleep. Early detection is crucial for infants because it might bring long\nterm adversities. The existing accurate detection mechanism (pulse oximetry) is\na skin contact measurement. The existing non-contact mechanisms (acoustics,\nvideo processing) are not accurate enough. This paper presents a novel\nalgorithm for the detection of sleep apnea with video processing. The solution\nis non-contact, accurate and lightweight enough to run on a single board\ncomputer. The paper discusses the accuracy of the algorithm on real data,\nadvantages of the new algorithm, its limitations and suggests future\nimprovements.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:34:41 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Jayatilaka", "Gihan", ""], ["Weligampola", "Harshana", ""], ["Sritharan", "Suren", ""], ["Pathmanathan", "Pankayraj", ""], ["Ragel", "Roshan", ""], ["Nawinne", "Isuru", ""]]}, {"id": "1910.04742", "submitter": "Jessica Lee", "authors": "Jessica Lee, Deva Ramanan and Rohit Girdhar", "title": "MetaPix: Few-Shot Video Retargeting", "comments": "Short version accepted to NeurIPS'19 MetaLearn Workshop. Full version\n  accepted to ICLR 2020. Webpage: https://imjal.github.io/MetaPix/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of unsupervised retargeting of human actions from one\nvideo to another. We consider the challenging setting where only a few frames\nof the target is available. The core of our approach is a conditional\ngenerative model that can transcode input skeletal poses (automatically\nextracted with an off-the-shelf pose estimator) to output target frames.\nHowever, it is challenging to build a universal transcoder because humans can\nappear wildly different due to clothing and background scene geometry. Instead,\nwe learn to adapt - or personalize - a universal generator to the particular\nhuman and background in the target. To do so, we make use of meta-learning to\ndiscover effective strategies for on-the-fly personalization. One significant\nbenefit of meta-learning is that the personalized transcoder naturally enforces\ntemporal coherence across its generated frames; all frames contain consistent\nclothing and background geometry of the target. We experiment on in-the-wild\ninternet videos and images and show our approach improves over widely-used\nbaselines for the task.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:51:44 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 21:09:45 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Lee", "Jessica", ""], ["Ramanan", "Deva", ""], ["Girdhar", "Rohit", ""]]}, {"id": "1910.04744", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar and Deva Ramanan", "title": "CATER: A diagnostic dataset for Compositional Actions and TEmporal\n  Reasoning", "comments": "ICLR 2020 (oral). Webpage/code/data:\n  https://rohitgirdhar.github.io/CATER", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has undergone a dramatic revolution in performance, driven in\nlarge part through deep features trained on large-scale supervised datasets.\nHowever, much of these improvements have focused on static image analysis;\nvideo understanding has seen rather modest improvements. Even though new\ndatasets and spatiotemporal models have been proposed, simple frame-by-frame\nclassification methods often still remain competitive. We posit that current\nvideo datasets are plagued with implicit biases over scene and object structure\nthat can dwarf variations in temporal structure. In this work, we build a video\ndataset with fully observable and controllable object and scene bias, and which\ntruly requires spatiotemporal understanding in order to be solved. Our dataset,\nnamed CATER, is rendered synthetically using a library of standard 3D objects,\nand tests the ability to recognize compositions of object movements that\nrequire long-term reasoning. In addition to being a challenging dataset, CATER\nalso provides a plethora of diagnostic tools to analyze modern spatiotemporal\nvideo architectures by being completely observable and controllable. Using\nCATER, we provide insights into some of the most recent state of the art deep\nvideo architectures.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:52:19 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 03:39:21 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Girdhar", "Rohit", ""], ["Ramanan", "Deva", ""]]}, {"id": "1910.04748", "submitter": "Yi-Wen Chen", "authors": "Yi-Wen Chen, Yi-Hsuan Tsai, Tiantian Wang, Yen-Yu Lin, Ming-Hsuan Yang", "title": "Referring Expression Object Segmentation with Caption-Aware Consistency", "comments": "Accepted in BMVC'19, project page at\n  https://github.com/wenz116/lang2seg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expressions are natural language descriptions that identify a\nparticular object within a scene and are widely used in our daily\nconversations. In this work, we focus on segmenting the object in an image\nspecified by a referring expression. To this end, we propose an end-to-end\ntrainable comprehension network that consists of the language and visual\nencoders to extract feature representations from both domains. We introduce the\nspatial-aware dynamic filters to transfer knowledge from text to image, and\neffectively capture the spatial information of the specified object. To better\ncommunicate between the language and visual modules, we employ a caption\ngeneration network that takes features shared across both domains as input, and\nimproves both representations via a consistency that enforces the generated\nsentence to be similar to the given referring expression. We evaluate the\nproposed framework on two referring expression datasets and show that our\nmethod performs favorably against the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:55:00 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Chen", "Yi-Wen", ""], ["Tsai", "Yi-Hsuan", ""], ["Wang", "Tiantian", ""], ["Lin", "Yen-Yu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1910.04751", "submitter": "Bowen Cheng", "authors": "Bowen Cheng and Maxwell D. Collins and Yukun Zhu and Ting Liu and\n  Thomas S. Huang and Hartwig Adam and Liang-Chieh Chen", "title": "Panoptic-DeepLab", "comments": "This work is presented at ICCV 2019 Joint COCO and Mapillary\n  Recognition Challenge Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Panoptic-DeepLab, a bottom-up and single-shot approach for\npanoptic segmentation. Our Panoptic-DeepLab is conceptually simple and delivers\nstate-of-the-art results. In particular, we adopt the dual-ASPP and\ndual-decoder structures specific to semantic, and instance segmentation,\nrespectively. The semantic segmentation branch is the same as the typical\ndesign of any semantic segmentation model (e.g., DeepLab), while the instance\nsegmentation branch is class-agnostic, involving a simple instance center\nregression. Our single Panoptic-DeepLab sets the new state-of-art at all three\nCityscapes benchmarks, reaching 84.2% mIoU, 39.0% AP, and 65.5% PQ on test set,\nand advances results on the other challenging Mapillary Vistas.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:57:19 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 15:27:18 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 01:13:12 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Cheng", "Bowen", ""], ["Collins", "Maxwell D.", ""], ["Zhu", "Yukun", ""], ["Liu", "Ting", ""], ["Huang", "Thomas S.", ""], ["Adam", "Hartwig", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "1910.04754", "submitter": "Jungseok Hong", "authors": "Jungseok Hong, Michael Fulton, and Junaed Sattar", "title": "A Generative Approach Towards Improved Robotic Detection of Marine\n  Litter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to address data scarcity problems in\nunderwater image datasets for visual detection of marine debris. The proposed\napproach relies on a two-stage variational autoencoder (VAE) and a binary\nclassifier to evaluate the generated imagery for quality and realism. From the\nimages generated by the two-stage VAE, the binary classifier selects \"good\nquality\" images and augments the given dataset with them. Lastly, a multi-class\nclassifier is used to evaluate the impact of the augmentation process by\nmeasuring the accuracy of an object detector trained on combinations of real\nand generated trash images. Our results show that the classifier trained with\nthe augmented data outperforms the one trained only with the real data. This\napproach will not only be valid for the underwater trash classification problem\npresented in this paper, but it will also be useful for any data-dependent task\nfor which collecting more images is challenging or infeasible.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 05:28:47 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Hong", "Jungseok", ""], ["Fulton", "Michael", ""], ["Sattar", "Junaed", ""]]}, {"id": "1910.04755", "submitter": "Dmitry Zhukov", "authors": "David Prokhorov, Dmitry Zhukov, Olga Barinova, Anna Vorontsova, Anton\n  Konushin", "title": "Measuring robustness of Visual SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous localization and mapping (SLAM) is an essential component of\nrobotic systems. In this work we perform a feasibility study of RGB-D SLAM for\nthe task of indoor robot navigation. Recent visual SLAM methods, e.g. ORBSLAM2\n\\cite{mur2017orb}, demonstrate really impressive accuracy, but the experiments\nin the papers are usually conducted on just a few sequences, that makes it\ndifficult to reason about the robustness of the methods. Another problem is\nthat all available RGB-D datasets contain the trajectories with very complex\ncamera motions. In this work we extensively evaluate ORBSLAM2 to better\nunderstand the state-of-the-art. First, we conduct experiments on the popular\npublicly available datasets for RGB-D SLAM across the conventional metrics. We\nperform statistical analysis of the results and find correlations between the\nmetrics and the attributes of the trajectories. Then, we introduce a new large\nand diverse HomeRobot dataset where we model the motions of a simple home\nrobot. Our dataset is created using physically-based rendering with realistic\nlighting and contains the scenes composed by human designers. It includes\nthousands of sequences, that is two orders of magnitude greater than in\nprevious works. We find that while in many cases the accuracy of SLAM is very\ngood, the robustness is still an issue.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 06:12:44 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Prokhorov", "David", ""], ["Zhukov", "Dmitry", ""], ["Barinova", "Olga", ""], ["Vorontsova", "Anna", ""], ["Konushin", "Anton", ""]]}, {"id": "1910.04760", "submitter": "Qi Li", "authors": "Qi Li, Long Mai, Michael A. Alcorn, Anh Nguyen", "title": "A cost-effective method for improving and re-purposing large,\n  pre-trained GANs by fine-tuning their class-embeddings", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, pre-trained generative models have been increasingly popular and\nuseful to both the research and wider communities. Specifically, BigGANs a\nclass-conditional Generative Adversarial Networks trained on\nImageNet---achieved excellent, state-of-the-art capability in generating\nrealistic photos. However, fine-tuning or training BigGANs from scratch is\npractically impossible for most researchers and engineers because (1) GAN\ntraining is often unstable and suffering from mode-collapse; and (2) the\ntraining requires a significant amount of computation, 256 Google TPUs for 2\ndays or 8xV100 GPUs for 15 days. Importantly, many pre-trained generative\nmodels both in NLP and image domains were found to contain biases that are\nharmful to society. Thus, we need computationally-feasible methods for\nmodifying and re-purposing these huge, pre-trained models for downstream tasks.\nIn this paper, we propose a cost-effective optimization method for improving\nand re-purposing BigGANs by fine-tuning only the class-embedding layer. We show\nthe effectiveness of our model-editing approach in three tasks: (1)\nsignificantly improving the realism and diversity of samples of complete\nmode-collapse classes; (2) re-purposing ImageNet BigGANs for generating images\nfor Places365; and (3) de-biasing or improving the sample diversity for\nselected ImageNet classes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:18:28 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 18:34:35 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 06:35:26 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2020 21:46:34 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Li", "Qi", ""], ["Mai", "Long", ""], ["Alcorn", "Michael A.", ""], ["Nguyen", "Anh", ""]]}, {"id": "1910.04778", "submitter": "Hengrui Luo", "authors": "Hengrui Luo, Justin Strait", "title": "Combining Geometric and Topological Information for Boundary Estimation", "comments": "38 pages with appendices, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in computer vision is boundary estimation, where the\ngoal is to delineate the boundary of objects in an image. In this paper, we\npropose a method which jointly incorporates geometric and topological\ninformation within an image to simultaneously estimate boundaries for objects\nwithin images with more complex topologies. We use a topological\nclustering-based method to assist initialization of the Bayesian active contour\nmodel. This combines pixel clustering, boundary smoothness, and potential prior\nshape information to produce an estimated object boundary. Active contour\nmethods are knownto be extremely sensitive to algorithm initialization, relying\non the user to provide a reasonable starting curve to the algorithm. In the\npresence of images featuring objects with complex topological structures, such\nas objects with holes or multiple objects, the user must initialize separate\ncurves for each boundary of interest. Our proposed topologically-guided method\ncan provide an interpretable, smart initialization in these settings, freeing\nup the user from potential pitfalls associated with objects of complex\ntopological structure. We provide a detailed simulation study comparing our\ninitialization to boundary estimates obtained from standard segmentation\nalgorithms. The method is demonstrated on artificial image datasets from\ncomputer vision, as well as real-world applications to skin lesion and neural\ncellular images, for which multiple topological features can be identified.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 18:00:10 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 15:36:03 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 23:27:16 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Luo", "Hengrui", ""], ["Strait", "Justin", ""]]}, {"id": "1910.04792", "submitter": "Shruti Jadon", "authors": "Shruti Jadon, Mahmood Jasim", "title": "Unsupervised video summarization framework using keyframe extraction and\n  video skimming", "comments": "5 pages, 3 figures. Technical Report", "journal-ref": "2020 IEEE 5th International Conference on Computing Communication\n  and Automation (ICCCA)", "doi": "10.1109/ICCCA49541.2020.9250764", "report-no": "140 - 145", "categories": "cs.IR cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video is one of the robust sources of information and the consumption of\nonline and offline videos has reached an unprecedented level in the last few\nyears. A fundamental challenge of extracting information from videos is a\nviewer has to go through the complete video to understand the context, as\nopposed to an image where the viewer can extract information from a single\nframe. Apart from context understanding, it almost impossible to create a\nuniversal summarized video for everyone, as everyone has their own bias of\nkeyframe, e.g; In a soccer game, a coach person might consider those frames\nwhich consist of information on player placement, techniques, etc; however, a\nperson with less knowledge about a soccer game, will focus more on frames which\nconsist of goals and score-board. Therefore, if we were to tackle problem video\nsummarization through a supervised learning path, it will require extensive\npersonalized labeling of data. In this paper, we attempt to solve video\nsummarization through unsupervised learning by employing traditional\nvision-based algorithmic methodologies for accurate feature extraction from\nvideo frames. We have also proposed a deep learning-based feature extraction\nfollowed by multiple clustering methods to find an effective way of summarizing\na video by interesting key-frame extraction. We have compared the performance\nof these approaches on the SumMe dataset and showcased that using deep\nlearning-based feature extraction has been proven to perform better in case of\ndynamic viewpoint videos.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 18:14:48 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 18:27:25 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Jadon", "Shruti", ""], ["Jasim", "Mahmood", ""]]}, {"id": "1910.04794", "submitter": "Angelica I. Aviles-Rivero", "authors": "Jianchao Zhang, Angelica I. Aviles-Rivero, Daniel Heydecker, Xiaosheng\n  Zhuang, Raymond Chan, Carola-Bibiane Sch\\\"onlieb", "title": "Dynamic Spectral Residual Superpixels", "comments": null, "journal-ref": "Zhang, J., Aviles-Rivero, A. I., Heydecker, D., Zhuang, X., Chan,\n  R., & Schonlieb, C. B. (2021). Dynamic spectral residual superpixels. Pattern\n  Recognition, 112, 107705", "doi": "10.1016/j.patcog.2020.107705", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of segmenting an image into superpixels in the\ncontext of $k$-means clustering, in which we wish to decompose an image into\nlocal, homogeneous regions corresponding to the underlying objects. Our novel\napproach builds upon the widely used Simple Linear Iterative Clustering (SLIC),\nand incorporate a measure of objects' structure based on the spectral residual\nof an image. Based on this combination, we propose a modified initialisation\nscheme and search metric, which helps keeps fine-details. This combination\nleads to better adherence to object boundaries, while preventing unnecessary\nsegmentation of large, uniform areas, while remaining computationally tractable\nin comparison to other methods. We demonstrate through numerical and visual\nexperiments that our approach outperforms the state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 18:15:30 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 17:01:11 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhang", "Jianchao", ""], ["Aviles-Rivero", "Angelica I.", ""], ["Heydecker", "Daniel", ""], ["Zhuang", "Xiaosheng", ""], ["Chan", "Raymond", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1910.04797", "submitter": "Weinan Song", "authors": "Yuan Liang, Weinan Song, J.P. Dym, Kun Wang, Lei He", "title": "CompareNet: Anatomical Segmentation Network with Deep Non-local Label\n  Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label propagation is a popular technique for anatomical segmentation. In this\nwork, we propose a novel deep framework for label propagation based on\nnon-local label fusion. Our framework, named CompareNet, incorporates subnets\nfor both extracting discriminating features, and learning the similarity\nmeasure, which lead to accurate segmentation. We also introduce the voxel-wise\nclassification as an unary potential to the label fusion function, for\nalleviating the search failure issue of the existing non-local fusion\nstrategies. Moreover, CompareNet is end-to-end trainable, and all the\nparameters are learnt together for the optimal performance. By evaluating\nCompareNet on two public datasets IBSRv2 and MICCAI 2012 for brain\nsegmentation, we show it outperforms state-of-the-art methods in accuracy,\nwhile being robust to pathologies.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 18:23:38 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Liang", "Yuan", ""], ["Song", "Weinan", ""], ["Dym", "J. P.", ""], ["Wang", "Kun", ""], ["He", "Lei", ""]]}, {"id": "1910.04814", "submitter": "Nima Tajbakhsh", "authors": "Nima Tajbakhsh, Brian Lai, Shilpa Ananth, Xiaowei Ding", "title": "ErrorNet: Learning error representations from limited data to improve\n  vascular segmentation", "comments": "Accepted in ISBI 2019. The supplementary material is only available\n  in the arxiv version of our paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have proved effective in segmenting\nlesions and anatomies in various medical imaging modalities. However, in the\npresence of small sample size and domain shift problems, these models often\nproduce masks with non-intuitive segmentation mistakes. In this paper, we\npropose a segmentation framework called ErrorNet, which learns to correct these\nsegmentation mistakes through the repeated process of injecting systematic\nsegmentation errors to the segmentation result based on a learned shape prior,\nfollowed by attempting to predict the injected error. During inference,\nErrorNet corrects the segmentation mistakes by adding the predicted error map\nto the initial segmentation result. ErrorNet has advantages over alternatives\nbased on domain adaptation or CRF-based post processing, because it requires\nneither domain-specific parameter tuning nor any data from the target domains.\nWe have evaluated ErrorNet using five public datasets for the task of retinal\nvessel segmentation. The selected datasets differ in size and patient\npopulation, allowing us to evaluate the effectiveness of ErrorNet in handling\nsmall sample size and domain shift problems. Our experiments demonstrate that\nErrorNet outperforms a base segmentation model, a CRF-based post processing\nscheme, and a domain adaptation method, with a greater performance gain in the\npresence of the aforementioned dataset limitations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 18:54:15 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 21:24:28 GMT"}, {"version": "v3", "created": "Sat, 18 Jan 2020 19:40:57 GMT"}, {"version": "v4", "created": "Sun, 2 Feb 2020 00:40:05 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Tajbakhsh", "Nima", ""], ["Lai", "Brian", ""], ["Ananth", "Shilpa", ""], ["Ding", "Xiaowei", ""]]}, {"id": "1910.04851", "submitter": "Charles Corbi\\`ere", "authors": "Charles Corbi\\`ere, Nicolas Thome, Avner Bar-Hen, Matthieu Cord,\n  Patrick P\\'erez", "title": "Addressing Failure Prediction by Learning Model Confidence", "comments": "NeurIPS 2019 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing reliably the confidence of a deep neural network and predicting its\nfailures is of primary importance for the practical deployment of these models.\nIn this paper, we propose a new target criterion for model confidence,\ncorresponding to the True Class Probability (TCP). We show how using the TCP is\nmore suited than relying on the classic Maximum Class Probability (MCP). We\nprovide in addition theoretical guarantees for TCP in the context of failure\nprediction. Since the true class is by essence unknown at test time, we propose\nto learn TCP criterion on the training set, introducing a specific learning\nscheme adapted to this context. Extensive experiments are conducted for\nvalidating the relevance of the proposed approach. We study various network\narchitectures, small and large scale datasets for image classification and\nsemantic segmentation. We show that our approach consistently outperforms\nseveral strong methods, from MCP to Bayesian uncertainty, as well as recent\napproaches specifically designed for failure prediction.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 08:23:45 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 15:09:46 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Corbi\u00e8re", "Charles", ""], ["Thome", "Nicolas", ""], ["Bar-Hen", "Avner", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1910.04852", "submitter": "Kushal Datta", "authors": "Kushal Datta, Imtiaz Hossain, Sun Choi, Vikram Saletore, Kyle Ambert,\n  William J. Godinez and Xian Zhang", "title": "Training Multiscale-CNN for Large Microscopy Image Classification in One\n  Hour", "comments": "15 pages, 10 figures", "journal-ref": "Workshop on Scalable Data Analytics in Scientific Computing,\n  International SuperComputing 2019, Frankfurt, Germany", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to train neural networks that use large images require to\neither crop or down-sample data during pre-processing, use small batch sizes,\nor split the model across devices mainly due to the prohibitively limited\nmemory capacity available on GPUs and emerging accelerators. These techniques\noften lead to longer time to convergence or time to train (TTT), and in some\ncases, lower model accuracy. CPUs, on the other hand, can leverage significant\namounts of memory. While much work has been done on parallelizing neural\nnetwork training on multiple CPUs, little attention has been given to tune\nneural network training with large images on CPUs. In this work, we train a\nmulti-scale convolutional neural network (M-CNN) to classify large biomedical\nimages for high content screening in one hour. The ability to leverage large\nmemory capacity on CPUs enables us to scale to larger batch sizes without\nhaving to crop or down-sample the input images. In conjunction with large batch\nsizes, we find a generalized methodology of linearly scaling of learning rate\nand train M-CNN to state-of-the-art (SOTA) accuracy of 99% within one hour. We\nachieve fast time to convergence using 128 two socket Intel Xeon 6148 processor\nnodes with 192GB DDR4 memory connected with 100Gbps Intel Omnipath\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 22:33:48 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 19:35:44 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Datta", "Kushal", ""], ["Hossain", "Imtiaz", ""], ["Choi", "Sun", ""], ["Saletore", "Vikram", ""], ["Ambert", "Kyle", ""], ["Godinez", "William J.", ""], ["Zhang", "Xian", ""]]}, {"id": "1910.04853", "submitter": "Kiwoo Shin", "authors": "Kiwoo Shin and Masayoshi Tomizuka", "title": "epBRM: Improving a Quality of 3D Object Detection using End Point Box\n  Regression Module", "comments": null, "journal-ref": "Intelligent Vehicles Symposium 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an endpoint box regression module(epBRM), which is designed for\npredicting precise 3D bounding boxes using raw LiDAR 3D point clouds. The\nproposed epBRM is built with sequence of small networks and is computationally\nlightweight. Our approach can improve a 3D object detection performance by\npredicting more precise 3D bounding box coordinates. The proposed approach\nrequires 40 minutes of training to improve the detection performance. Moreover,\nepBRM imposes less than 12ms to network inference time for up-to 20 objects.\n  The proposed approach utilizes a spatial transformation mechanism to simplify\nthe box regression task. Adopting spatial transformation mechanism into epBRM\nmakes it possible to improve the quality of detection with a small sized\nnetwork.\n  We conduct in-depth analysis of the effect of various spatial transformation\nmechanisms applied on raw LiDAR 3D point clouds. We also evaluate the proposed\nepBRM by applying it to several state-of-the-art 3D object detection systems.\n  We evaluate our approach on KITTI dataset, a standard 3D object detection\nbenchmark for autonomous vehicles. The proposed epBRM enhances the overlaps\nbetween ground truth bounding boxes and detected bounding boxes, and improves\n3D object detection. Our proposed method evaluated in KITTI test server\noutperforms current state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 22:42:17 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 23:06:28 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Shin", "Kiwoo", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1910.04854", "submitter": "Daniel Seita", "authors": "Daniel Seita, Aditya Ganapathi, Ryan Hoque, Minho Hwang, Edward Cen,\n  Ajay Kumar Tanwani, Ashwin Balakrishna, Brijen Thananjeyan, Jeffrey\n  Ichnowski, Nawid Jamali, Katsu Yamane, Soshi Iba, John Canny, Ken Goldberg", "title": "Deep Imitation Learning of Sequential Fabric Smoothing From an\n  Algorithmic Supervisor", "comments": "Supplementary material is available at\n  https://sites.google.com/view/fabric-smoothing ; Version 2 has significant\n  improvements with new results and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential pulling policies to flatten and smooth fabrics have applications\nfrom surgery to manufacturing to home tasks such as bed making and folding\nclothes. Due to the complexity of fabric states and dynamics, we apply deep\nimitation learning to learn policies that, given color (RGB), depth (D), or\ncombined color-depth (RGBD) images of a rectangular fabric sample, estimate\npick points and pull vectors to spread the fabric to maximize coverage. To\ngenerate data, we develop a fabric simulator and an algorithmic supervisor that\nhas access to complete state information. We train policies in simulation using\ndomain randomization and dataset aggregation (DAgger) on three tiers of\ndifficulty in the initial randomized configuration. We present results\ncomparing five baseline policies to learned policies and report systematic\ncomparisons of RGB vs D vs RGBD images as inputs. In simulation, learned\npolicies achieve comparable or superior performance to analytic baselines. In\n180 physical experiments with the da Vinci Research Kit (dVRK) surgical robot,\nRGBD policies trained in simulation attain coverage of 83% to 95% depending on\ndifficulty tier, suggesting that effective fabric smoothing policies can be\nlearned from an algorithmic supervisor and that depth sensing is a valuable\naddition to color alone. Supplementary material is available at\nhttps://sites.google.com/view/fabric-smoothing.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 22:06:14 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 22:26:31 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Seita", "Daniel", ""], ["Ganapathi", "Aditya", ""], ["Hoque", "Ryan", ""], ["Hwang", "Minho", ""], ["Cen", "Edward", ""], ["Tanwani", "Ajay Kumar", ""], ["Balakrishna", "Ashwin", ""], ["Thananjeyan", "Brijen", ""], ["Ichnowski", "Jeffrey", ""], ["Jamali", "Nawid", ""], ["Yamane", "Katsu", ""], ["Iba", "Soshi", ""], ["Canny", "John", ""], ["Goldberg", "Ken", ""]]}, {"id": "1910.04855", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Stefanos Zafeiriou", "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task\n  Learning and ArcFace", "comments": "oral presentation in BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective computing has been largely limited in terms of available data\nresources. The need to collect and annotate diverse in-the-wild datasets has\nbecome apparent with the rise of deep learning models, as the default approach\nto address any computer vision task. Some in-the-wild databases have been\nrecently proposed. However: i) their size is small, ii) they are not\naudiovisual, iii) only a small part is manually annotated, iv) they contain a\nsmall number of subjects, or v) they are not annotated for all main behavior\ntasks (valence-arousal estimation, action unit detection and basic expression\nclassification). To address these, we substantially extend the largest\navailable in-the-wild database (Aff-Wild) to study continuous emotions such as\nvalence and arousal. Furthermore, we annotate parts of the database with basic\nexpressions and action units. As a consequence, for the first time, this allows\nthe joint study of all three types of behavior states. We call this database\nAff-Wild2. We conduct extensive experiments with CNN and CNN-RNN architectures\nthat use visual and audio modalities; these networks are trained on Aff-Wild2\nand their performance is then evaluated on 10 publicly available emotion\ndatabases. We show that the networks achieve state-of-the-art performance for\nthe emotion recognition tasks. Additionally, we adapt the ArcFace loss function\nin the emotion recognition context and use it for training two new networks on\nAff-Wild2 and then re-train them in a variety of diverse expression recognition\ndatabases. The networks are shown to improve the existing state-of-the-art. The\ndatabase, emotion recognition models and source code are available at\nhttp://ibug.doc.ic.ac.uk/resources/aff-wild2.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 22:45:18 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1910.04856", "submitter": "Giuseppe Lancioni", "authors": "Marco Zamprogno, Marco Passon, Niki Martinel, Giuseppe Serra, Giuseppe\n  Lancioni, Christian Micheloni, Carlo Tasso, Gian Luca Foresti", "title": "Video-Based Convolutional Attention for Person Re-Identification", "comments": "11 pages, 2 figures. Accepted by ICIAP2019, 20th International\n  Conference on IMAGE ANALYSIS AND PROCESSING, Trento, Italy, 9-13 September,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of video-based person\nre-identification, which is the task of associating videos of the same person\ncaptured by different and non-overlapping cameras. We propose a Siamese\nframework in which video frames of the person to re-identify and of the\ncandidate one are processed by two identical networks which produce a\nsimilarity score. We introduce an attention mechanisms to capture the relevant\ninformation both at frame level (spatial information) and at video level\n(temporal information given by the importance of a specific frame within the\nsequence). One of the novelties of our approach is given by a joint concurrent\nprocessing of both frame and video levels, providing in such a way a very\nsimple architecture. Despite this fact, our approach achieves better\nperformance than the state-of-the-art on the challenging iLIDS-VID dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:23:43 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Zamprogno", "Marco", ""], ["Passon", "Marco", ""], ["Martinel", "Niki", ""], ["Serra", "Giuseppe", ""], ["Lancioni", "Giuseppe", ""], ["Micheloni", "Christian", ""], ["Tasso", "Carlo", ""], ["Foresti", "Gian Luca", ""]]}, {"id": "1910.04857", "submitter": "Suryabhan Singh Hada", "authors": "Suryabhan Singh Hada and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Sampling the \"Inverse Set\" of a Neuron: An Approach to Understanding\n  Neural Nets", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent success of deep neural networks in computer vision, it is\nimportant to understand the internal working of these networks. What does a\ngiven neuron represent? The concepts captured by a neuron may be hard to\nunderstand or express in simple terms. The approach we propose in this paper is\nto characterize the region of input space that excites a given neuron to a\ncertain level; we call this the inverse set. This inverse set is a complicated\nhigh dimensional object that we explore by an optimization-based sampling\napproach. Inspection of samples of this set by a human can reveal regularities\nthat help to understand the neuron. This goes beyond approaches which were\nlimited to finding an image which maximally activates the neuron or using\nMarkov chain Monte Carlo to sample images, but this is very slow, generates\nsamples with little diversity and lacks control over the activation value of\nthe generated samples. Our approach also allows us to explore the intersection\nof inverse sets of several neurons and other variations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 02:22:43 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 00:49:03 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hada", "Suryabhan Singh", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1910.04858", "submitter": "Lu Mi", "authors": "Lu Mi, Hao Wang, Yonglong Tian, Nir Shavit", "title": "Training-Free Uncertainty Estimation for Dense Regression: Sensitivity\n  as a Surrogate", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty estimation is an essential step in the evaluation of the\nrobustness for deep learning models in computer vision, especially when applied\nin risk-sensitive areas. However, most state-of-the-art deep learning models\neither fail to obtain uncertainty estimation or need significant modification\n(e.g., formulating a proper Bayesian treatment) to obtain it. Most previous\nmethods are not able to take an arbitrary model off the shelf and generate\nuncertainty estimation without retraining or redesigning it. To address this\ngap, we perform a systematic exploration into training-free uncertainty\nestimation for dense regression, an unrecognized yet important problem, and\nprovide a theoretical construction justifying such estimations. We propose\nthree simple and scalable methods to analyze the variance of outputs from a\ntrained network under tolerable perturbations: infer-transformation,\ninfer-noise, and infer-dropout. They operate solely during inference, without\nthe need to re-train, re-design, or fine-tune the model, as typically required\nby state-of-the-art uncertainty estimation methods. Surprisingly, even without\ninvolving such perturbations in training, our methods produce comparable or\neven better uncertainty estimation when compared to training-required\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 02:30:02 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 03:11:41 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Mi", "Lu", ""], ["Wang", "Hao", ""], ["Tian", "Yonglong", ""], ["Shavit", "Nir", ""]]}, {"id": "1910.04859", "submitter": "Peng Jin", "authors": "Xingyuan Chen, Ping Cai, Peng Jin, Haokun Du, Hongjun Wang, Xingyu\n  Dai, Jiajun Chen", "title": "The Detection of Distributional Discrepancy for Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The text generated by neural language models is not as good as the real text.\nThis means that their distributions are different. Generative Adversarial Nets\n(GAN) are used to alleviate it. However, some researchers argue that GAN\nvariants do not work at all. When both sample quality (such as Bleu) and sample\ndiversity (such as self-Bleu) are taken into account, the GAN variants even are\nworse than a well-adjusted language model. But, Bleu and self-Bleu can not\nprecisely measure this distributional discrepancy. In fact, how to measure the\ndistributional discrepancy between real text and generated text is still an\nopen problem. In this paper, we theoretically propose two metric functions to\nmeasure the distributional difference between real text and generated text.\nBesides that, a method is put forward to estimate them. First, we evaluate\nlanguage model with these two functions and find the difference is huge. Then,\nwe try several methods to use the detected discrepancy signal to improve the\ngenerator. However the difference becomes even bigger than before.\nExperimenting on two existing language GANs, the distributional discrepancy\nbetween real text and generated text increases with more adversarial learning\nrounds. It demonstrates both of these language GANs fail.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 07:12:34 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 06:24:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Chen", "Xingyuan", ""], ["Cai", "Ping", ""], ["Jin", "Peng", ""], ["Du", "Haokun", ""], ["Wang", "Hongjun", ""], ["Dai", "Xingyu", ""], ["Chen", "Jiajun", ""]]}, {"id": "1910.04860", "submitter": "Zhengming Ding", "authors": "Zhengming Ding, Yandong Guo, Lei Zhang, Yun Fu", "title": "Generative One-Shot Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  One-shot face recognition measures the ability to identify persons with only\nseeing them at one glance, and is a hallmark of human visual intelligence. It\nis challenging for conventional machine learning approaches to mimic this way,\nsince limited data are hard to effectively represent the data variance. The\ngoal of one-shot face recognition is to learn a large-scale face recognizer,\nwhich is capable to fight off the data imbalance challenge. In this paper, we\npropose a novel generative adversarial one-shot face recognizer, attempting to\nsynthesize meaningful data for one-shot classes by adapting the data variances\nfrom other normal classes. Specifically, we target at building a more effective\ngeneral face classifier for both normal persons and one-shot persons.\nTechnically, we design a new loss function by formulating knowledge transfer\ngenerator and a general classifier into a unified framework. Such a two-player\nminimax optimization can guide the generation of more effective data, which\neffectively promote the underrepresented classes in the learned model and lead\nto a remarkable improvement in face recognition performance. We evaluate our\nproposed model on the MS-Celeb-1M one-shot learning benchmark task, where we\ncould recognize 94.98% of the test images at the precision of 99% for the\none-shot classes, keeping an overall Top1 accuracy at $99.80\\%$ for the normal\nclasses. To the best of our knowledge, this is the best performance among all\nthe published methods using this benchmark task with the same setup, including\nall the participants in the recent MS-Celeb-1M challenge at ICCV\n2017\\footnote{http://www.msceleb.org/challenge2/2017}.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 17:13:06 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Ding", "Zhengming", ""], ["Guo", "Yandong", ""], ["Zhang", "Lei", ""], ["Fu", "Yun", ""]]}, {"id": "1910.04861", "submitter": "Carl Yang", "authors": "Carl Yang, Do Huy Hoang, Tomas Mikolov, Jiawei Han", "title": "Place Deduplication with Embeddings", "comments": "Published at WWW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the advancing mobile location services, people nowadays can post\nabout places to share visiting experience on-the-go. A large place graph not\nonly helps users explore interesting destinations, but also provides\nopportunities for understanding and modeling the real world. To improve\ncoverage and flexibility of the place graph, many platforms import places data\nfrom multiple sources, which unfortunately leads to the emergence of numerous\nduplicated places that severely hinder subsequent location-related services. In\nthis work, we take the anonymous place graph from Facebook as an example to\nsystematically study the problem of place deduplication: We carefully formulate\nthe problem, study its connections to various related tasks that lead to\nseveral promising basic models, and arrive at a systematic two-step data-driven\npipeline based on place embedding with multiple novel techniques that works\nsignificantly better than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 02:05:30 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Yang", "Carl", ""], ["Hoang", "Do Huy", ""], ["Mikolov", "Tomas", ""], ["Han", "Jiawei", ""]]}, {"id": "1910.04862", "submitter": "Kazuhide Okamoto", "authors": "Justin Zheng, Kazuhide Okamoto, Panagiotis Tsiotras", "title": "Vision-Based Autonomous Vehicle Control using the Two-Point Visual\n  Driver Control Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new self-driving framework that uses a human driver\ncontrol model, whose feature-input values are extracted from images using deep\nconvolutional neural networks (CNNs). The development of image processing\ntechniques using CNNs along with accelerated computing hardware has recently\nenabled real-time detection of these feature-input values. The use of human\ndriver models can lead to more \"natural\" driving behavior of self-driving\nvehicles. Specifically, we use the well-known two-point visual driver control\nmodel as the controller, and we use a top-down lane cost map CNN and the YOLOv2\nCNN to extract feature-input values. This framework relies exclusively on\ninputs from low-cost sensors like a monocular camera and wheel speed sensors.\nWe experimentally validate the proposed framework on an outdoor track using a\n1/5th-scale autonomous vehicle platform.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 17:02:49 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Zheng", "Justin", ""], ["Okamoto", "Kazuhide", ""], ["Tsiotras", "Panagiotis", ""]]}, {"id": "1910.04864", "submitter": "Lichao Chen", "authors": "Lichao Chen, Sudhir Singh, Thomas Kailath, and Vwani Roychowdhury", "title": "Brain-inspired automated visual object discovery and detection", "comments": null, "journal-ref": "PNAS January 2, 2019 116 (1) 96-105", "doi": "10.1073/pnas.1802103115", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant recent progress, machine vision systems lag considerably\nbehind their biological counterparts in performance, scalability, and\nrobustness. A distinctive hallmark of the brain is its ability to automatically\ndiscover and model objects, at multiscale resolutions, from repeated exposures\nto unlabeled contextual data and then to be able to robustly detect the learned\nobjects under various nonideal circumstances, such as partial occlusion and\ndifferent view angles. Replication of such capabilities in a machine would\nrequire three key ingredients: (i) access to large-scale perceptual data of the\nkind that humans experience, (ii) flexible representations of objects, and\n(iii) an efficient unsupervised learning algorithm. The Internet fortunately\nprovides unprecedented access to vast amounts of visual data. This paper\nleverages the availability of such data to develop a scalable framework for\nunsupervised learning of object prototypes--brain-inspired flexible, scale, and\nshift invariant representations of deformable objects (e.g., humans,\nmotorcycles, cars, airplanes) comprised of parts, their different\nconfigurations and views, and their spatial relationships. Computationally, the\nobject prototypes are represented as geometric associative networks using\nprobabilistic constructs such as Markov random fields. We apply our framework\nto various datasets and show that our approach is computationally scalable and\ncan construct accurate and operational part-aware object models much more\nefficiently than in much of the recent computer vision literature. We also\npresent efficient algorithms for detection and localization in new scenes of\nobjects and their partial views.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 01:55:46 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Chen", "Lichao", ""], ["Singh", "Sudhir", ""], ["Kailath", "Thomas", ""], ["Roychowdhury", "Vwani", ""]]}, {"id": "1910.04865", "submitter": "Adewale Akinfaderin", "authors": "Adewale Akinfaderin and Olamilekan Wahab", "title": "NASS-AI: Towards Digitization of Parliamentary Bills using Document\n  Level Embedding and Bidirectional Long Short-Term Memory", "comments": "Presented at NeurIPS 2019 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been several reports in the Nigerian and International media about\nthe Senators and House of Representative Members of the Nigerian National\nAssembly (NASS) being the highest paid in the world. Despite this high-level of\nparliamentary compensation and a lack of oversight, most of the legislative\nduties like bills introduced and vote proceedings are shrouded in mystery\nwithout an open and annotated corpus. In this paper, we present results from\nongoing research on the categorization of bills introduced in the Nigerian\nparliament since the fourth republic (1999 - 2018). For this task, we employed\na multi-step approach which involves extracting text from scanned and embedded\npdfs with low to medium quality using Optical Character Recognition (OCR) tools\nand labeling them into eight categories. We investigate the performance of\ndocument level embedding for feature representation of the extracted texts\nbefore using a Bidirectional Long Short-Term Memory (Bi-LSTM) for our\nclassifier. The performance was further compared with other feature\nrepresentation and machine learning techniques. We believe that these results\nare well-positioned to have a substantial impact on the quest to meet the basic\nopen data charter principles.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 00:39:02 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Akinfaderin", "Adewale", ""], ["Wahab", "Olamilekan", ""]]}, {"id": "1910.04866", "submitter": "Rula Amer M.Sc", "authors": "Rula Amer, Jannette Nassar, David Bendahan, Hayit Greenspan and Noam\n  Ben-Eliezer", "title": "Automatic Segmentation of Muscle Tissue and Inter-muscular Fat in Thigh\n  and Calf MRI Images", "comments": "9 pages, 4 figures, 2 tables, MICCAI 2019, the 22nd International\n  Conference on Medical Image Computing and Computer Assisted Intervention", "journal-ref": null, "doi": "10.1007/978-3-030-32245-8_25", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) of thigh and calf muscles is one of the most\neffective techniques for estimating fat infiltration into muscular dystrophies.\nThe infiltration of adipose tissue into the diseased muscle region varies in\nits severity across, and within, patients. In order to efficiently quantify the\ninfiltration of fat, accurate segmentation of muscle and fat is needed. An\nestimation of the amount of infiltrated fat is typically done visually by\nexperts. Several algorithmic solutions have been proposed for automatic\nsegmentation. While these methods may work well in mild cases, they struggle in\nmoderate and severe cases due to the high variability in the intensity of\ninfiltration, and the tissue's heterogeneous nature. To address these\nchallenges, we propose a deep-learning approach, producing robust results with\nhigh Dice Similarity Coefficient (DSC) of 0.964, 0.917 and 0.933 for\nmuscle-region, healthy muscle and inter-muscular adipose tissue (IMAT)\nsegmentation, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:48:31 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Amer", "Rula", ""], ["Nassar", "Jannette", ""], ["Bendahan", "David", ""], ["Greenspan", "Hayit", ""], ["Ben-Eliezer", "Noam", ""]]}, {"id": "1910.04867", "submitter": "Neil Houlsby", "authors": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen,\n  Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim\n  Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen,\n  Marcin Michalski, Olivier Bousquet, Sylvain Gelly, Neil Houlsby", "title": "A Large-scale Study of Representation Learning with the Visual Task\n  Adaptation Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning promises to unlock deep learning for the long tail of\nvision tasks without expensive labelled datasets. Yet, the absence of a unified\nevaluation for general visual representations hinders progress. Popular\nprotocols are often too constrained (linear classification), limited in\ndiversity (ImageNet, CIFAR, Pascal-VOC), or only weakly related to\nrepresentation quality (ELBO, reconstruction error). We present the Visual Task\nAdaptation Benchmark (VTAB), which defines good representations as those that\nadapt to diverse, unseen tasks with few examples. With VTAB, we conduct a\nlarge-scale study of many popular publicly-available representation learning\nalgorithms. We carefully control confounders such as architecture and tuning\nbudget. We address questions like: How effective are ImageNet representations\nbeyond standard natural datasets? How do representations trained via generative\nand discriminative models compare? To what extent can self-supervision replace\nlabels? And, how close are we to general visual representations?\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:06:29 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 13:36:15 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Zhai", "Xiaohua", ""], ["Puigcerver", "Joan", ""], ["Kolesnikov", "Alexander", ""], ["Ruyssen", "Pierre", ""], ["Riquelme", "Carlos", ""], ["Lucic", "Mario", ""], ["Djolonga", "Josip", ""], ["Pinto", "Andre Susano", ""], ["Neumann", "Maxim", ""], ["Dosovitskiy", "Alexey", ""], ["Beyer", "Lucas", ""], ["Bachem", "Olivier", ""], ["Tschannen", "Michael", ""], ["Michalski", "Marcin", ""], ["Bousquet", "Olivier", ""], ["Gelly", "Sylvain", ""], ["Houlsby", "Neil", ""]]}, {"id": "1910.04868", "submitter": "Haraldur Hallgr\\'imsson", "authors": "Haraldur T. Hallgrimsson, Richika Sharan, Scott T. Grafton, Ambuj K.\n  Singh", "title": "Estimating localized complexity of white-matter wiring with GANs", "comments": "Three page extended abstract, accepted to Medical Imaging meets\n  NeurIPS 2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-vivo examination of the physical connectivity of axonal projections\nthrough the white matter of the human brain is made possible by diffusion\nweighted magnetic resonance imaging (dMRI) Analysis of dMRI commonly considers\nderived scalar metrics such as fractional anisotrophy as proxies for \"white\nmatter integrity,\" and differences of such measures have been observed as\nsignificantly correlating with various neurological diagnosis and clinical\nmeasures such as executive function, presence of multiple sclerosis, and\ngenetic similarity. The analysis of such voxel measures is confounded in areas\nof more complicated fiber wiring due to crossing, kissing, and dispersing\nfibers. Recently, Volz et al. introduced a simple probabilistic measure of the\ncount of distinct fiber populations within a voxel, which was shown to reduce\nvariance in group comparisons. We propose a complementary measure that\nconsiders the complexity of a voxel in context of its local region, with an aim\nto quantify the localized wiring complexity of every part of white matter. This\nallows, for example, identification of particularly ambiguous regions of the\nbrain for tractographic approaches of modeling global wiring connectivity. Our\nmethod builds on recent advances in image inpainting, in which the task is to\nplausibly fill in a missing region of an image. Our proposed method builds on a\nBayesian estimate of heteroscedastic aleatoric uncertainty of a region of white\nmatter by inpainting it from its context. We define the localized wiring\ncomplexity of white matter as how accurately and confidently a well-trained\nmodel can predict the missing patch. In our results, we observe low aleatoric\nuncertainty along major neuronal pathways which increases at junctions and\ntowards cortex boundaries. This directly quantifies the difficulty of lesion\ninpainting of dMRI images at all parts of white matter.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:45:32 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 02:48:04 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Hallgrimsson", "Haraldur T.", ""], ["Sharan", "Richika", ""], ["Grafton", "Scott T.", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1910.04869", "submitter": "Favyen Bastani", "authors": "Favyen Bastani, Songtao He, Satvat Jagwani, Edward Park, Sofiane\n  Abbar, Mohammad Alizadeh, Hari Balakrishnan, Sanjay Chawla, Sam Madden,\n  Mohammad Amin Sadeghi", "title": "Inferring and Improving Street Maps with Data-Driven Automation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Street maps are a crucial data source that help to inform a wide range of\ndecisions, from navigating a city to disaster relief and urban planning.\nHowever, in many parts of the world, street maps are incomplete or lag behind\nnew construction. Editing maps today involves a tedious process of manually\ntracing and annotating roads, buildings, and other map features.\n  Over the past decade, many automatic map inference systems have been proposed\nto automatically extract street map data from satellite imagery, aerial\nimagery, and GPS trajectory datasets. However, automatic map inference has\nfailed to gain traction in practice due to two key limitations: high error\nrates (low precision), which manifest in noisy inference outputs, and a lack of\nend-to-end system design to leverage inferred data to update existing street\nmaps.\n  At MIT and QCRI, we have developed a number of algorithms and approaches to\naddress these challenges, which we combined into a new system we call Mapster.\nMapster is a human-in-the-loop street map editing system that incorporates\nthree components to robustly accelerate the mapping process over traditional\ntools and workflows: high-precision automatic map inference, data refinement,\nand machine-assisted map editing.\n  Through an evaluation on a large-scale dataset including satellite imagery,\nGPS trajectories, and ground-truth map data in forty cities, we show that\nMapster makes automation practical for map editing, and enables the curation of\nmap datasets that are more complete and up-to-date at less cost.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 02:30:46 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 18:55:08 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Bastani", "Favyen", ""], ["He", "Songtao", ""], ["Jagwani", "Satvat", ""], ["Park", "Edward", ""], ["Abbar", "Sofiane", ""], ["Alizadeh", "Mohammad", ""], ["Balakrishnan", "Hari", ""], ["Chawla", "Sanjay", ""], ["Madden", "Sam", ""], ["Sadeghi", "Mohammad Amin", ""]]}, {"id": "1910.04870", "submitter": "Rachel Blin", "authors": "Rachel Blin, Samia Ainouz, St\\'ephane Canu and Fabrice Meriaudeau", "title": "Road scenes analysis in adverse weather conditions by\n  polarization-encoded images and adapted deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in road scenes is necessary to develop both autonomous\nvehicles and driving assistance systems. Even if deep neural networks for\nrecognition task have shown great performances using conventional images, they\nfail to detect objects in road scenes in complex acquisition situations. In\ncontrast, polarization images, characterizing the light wave, can robustly\ndescribe important physical properties of the object even under poor\nillumination or strong reflections. This paper shows how non-conventional\npolarimetric imaging modality overcomes the classical methods for object\ndetection especially in adverse weather conditions. The efficiency of the\nproposed method is mostly due to the high power of the polarimetry to\ndiscriminate any object by its reflective properties and on the use of deep\nneural networks for object detection. Our goal by this work, is to prove that\npolarimetry brings a real added value compared with RGB images for object\ndetection. Experimental results on our own dataset composed of road scene\nimages taken during adverse weather conditions show that polarimetry together\nwith deep learning can improve the state-of-the-art by about 20% to 50% on\ndifferent detection tasks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 13:47:46 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Blin", "Rachel", ""], ["Ainouz", "Samia", ""], ["Canu", "St\u00e9phane", ""], ["Meriaudeau", "Fabrice", ""]]}, {"id": "1910.04871", "submitter": "Daniele Cattaneo", "authors": "Daniele Cattaneo, Matteo Vaghi, Simone Fontana, Augusto Luis\n  Ballardini, Domenico Giorgio Sorrenti", "title": "Global visual localization in LiDAR-maps through shared 2D-3D embedding\n  space", "comments": "Accepted for presentation at IEEE ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Global localization is an important and widely studied problem for many\nrobotic applications. Place recognition approaches can be exploited to solve\nthis task, e.g., in the autonomous driving field. While most vision-based\napproaches match an image w.r.t. an image database, global visual localization\nwithin LiDAR-maps remains fairly unexplored, even though the path toward high\ndefinition 3D maps, produced mainly from LiDARs, is clear. In this work we\nleverage Deep Neural Network (DNN) approaches to create a shared embedding\nspace between images and LiDAR-maps, allowing for image to 3D-LiDAR place\nrecognition. We trained a 2D and a 3D DNN that create embeddings, respectively\nfrom images and from point clouds, that are close to each other whether they\nrefer to the same place. An extensive experimental activity is presented to\nassess the effectiveness of the approach w.r.t. different learning paradigms,\nnetwork architectures, and loss functions. All the evaluations have been\nperformed using the Oxford Robotcar Dataset, which encompasses a wide range of\nweather and light conditions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 09:59:00 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 12:14:33 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Cattaneo", "Daniele", ""], ["Vaghi", "Matteo", ""], ["Fontana", "Simone", ""], ["Ballardini", "Augusto Luis", ""], ["Sorrenti", "Domenico Giorgio", ""]]}, {"id": "1910.04874", "submitter": "John Keller", "authors": "John Keller, Sebastian Scherer", "title": "A Stereo Algorithm for Thin Obstacles and Reflective Objects", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo cameras are a popular choice for obstacle avoidance for outdoor\nlighweight, low-cost robotics applications. However, they are unable to sense\nthin and reflective objects well. Currently, many algorithms are tuned to\nperform well on indoor scenes like the Middlebury dataset. When navigating\noutdoors, reflective objects, like windows and glass, and thin obstacles, like\nwires, are not well handled by most stereo disparity algorithms. Reflections,\nrepeating patterns and objects parallel to the cameras' baseline causes\nmismatches between image pairs which leads to bad disparity estimates. Thin\nobstacles are difficult for many sliding window based disparity methods to\ndetect because they do not take up large portions of the pixels in the sliding\nwindow. We use a trinocular camera setup and micropolarizer camera capable of\ndetecting reflective objects to overcome these issues. We present a\nhierarchical disparity algorithm that reduces noise, separately identify wires\nusing semantic object triangulation in three images, and use information about\nthe polarization of light to estimate the disparity of reflective objects. We\nevaluate our approach on outdoor data that we collected. Our method contained\nan average of 9.27% of bad pixels compared to a typical stereo algorithm's\n18.4% of bad pixels in scenes containing reflective objects. Our trinocular and\nsemantic wire disparity methods detected 53% of wire pixels, whereas a typical\ntwo camera stereo algorithm detected 5%.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 14:37:56 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Keller", "John", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1910.04875", "submitter": "Xiaomeng Dong", "authors": "Xiaomeng Dong, Junpyo Hong, Hsi-Ming Chang, Michael Potter, Aritra\n  Chowdhury, Purujit Bahl, Vivek Soni, Yun-Chan Tsai, Rajesh Tamada, Gaurav\n  Kumar, Caroline Favart, V. Ratna Saripalli, Gopal Avinash", "title": "FastEstimator: A Deep Learning Library for Fast Prototyping and\n  Productization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As the complexity of state-of-the-art deep learning models increases by the\nmonth, implementation, interpretation, and traceability become\never-more-burdensome challenges for AI practitioners around the world. Several\nAI frameworks have risen in an effort to stem this tide, but the steady advance\nof the field has begun to test the bounds of their flexibility, expressiveness,\nand ease of use. To address these concerns, we introduce a radically flexible\nhigh-level open source deep learning framework for both research and industry.\nWe introduce FastEstimator.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 01:01:27 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 19:20:13 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Dong", "Xiaomeng", ""], ["Hong", "Junpyo", ""], ["Chang", "Hsi-Ming", ""], ["Potter", "Michael", ""], ["Chowdhury", "Aritra", ""], ["Bahl", "Purujit", ""], ["Soni", "Vivek", ""], ["Tsai", "Yun-Chan", ""], ["Tamada", "Rajesh", ""], ["Kumar", "Gaurav", ""], ["Favart", "Caroline", ""], ["Saripalli", "V. Ratna", ""], ["Avinash", "Gopal", ""]]}, {"id": "1910.04877", "submitter": "Sek Chai", "authors": "Prateeth Nayak, David Zhang, Sek Chai", "title": "Bit Efficient Quantization for Deep Neural Networks", "comments": "EMC2 - NeurIPS workshop 2019, #latentai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization for deep neural networks have afforded models for edge devices\nthat use less on-board memory and enable efficient low-power inference. In this\npaper, we present a comparison of model-parameter driven quantization\napproaches that can achieve as low as 3-bit precision without affecting\naccuracy. The post-training quantization approaches are data-free, and the\nresulting weight values are closely tied to the dataset distribution on which\nthe model has converged to optimality. We show quantization results for a\nnumber of state-of-art deep neural networks (DNN) using large dataset like\nImageNet. To better analyze quantization results, we describe the overall range\nand local sparsity of values afforded through various quantization schemes. We\nshow the methods to lower bit-precision beyond quantization limits with object\nclass clustering.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:43:12 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Nayak", "Prateeth", ""], ["Zhang", "David", ""], ["Chai", "Sek", ""]]}, {"id": "1910.04879", "submitter": "Yan Chi Vinci Chow", "authors": "Vinci Chow", "title": "Predicting Auction Price of Vehicle License Plate with Deep Residual\n  Learning", "comments": null, "journal-ref": "Trends and Applications in Knowledge Discovery and Data Mining.\n  PAKDD 2019. Lecture Notes in Computer Science, vol 11607. Springer, Cham", "doi": "10.1007/978-3-030-26142-9_16", "report-no": null, "categories": "cs.CV cs.LG econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to superstition, license plates with desirable combinations of characters\nare highly sought after in China, fetching prices that can reach into the\nmillions in government-held auctions. Despite the high stakes involved, there\nhas been essentially no attempt to provide price estimates for license plates.\nWe present an end-to-end neural network model that simultaneously predict the\nauction price, gives the distribution of prices and produces latent feature\nvectors. While both types of neural network architectures we consider\noutperform simpler machine learning methods, convolutional networks outperform\nrecurrent networks for comparable training time or model complexity. The\nresulting model powers our online price estimator and search engine.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:38:06 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Chow", "Vinci", ""]]}, {"id": "1910.04887", "submitter": "Samuel Sharpe", "authors": "Samuel Sharpe, Jin Yan, Fan Wu, Iddo Drori", "title": "Visual Natural Language Query Auto-Completion for Estimating Instance\n  Probabilities", "comments": null, "journal-ref": "CVPR Language and Vision Workshop, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new task of query auto-completion for estimating instance\nprobabilities. We complete a user query prefix conditioned upon an image. Given\nthe complete query, we fine tune a BERT embedding for estimating probabilities\nof a broad set of instances. The resulting instance probabilities are used for\nselection while being agnostic to the segmentation or attention mechanism. Our\nresults demonstrate that auto-completion using both language and vision\nperforms better than using only language, and that fine tuning a BERT embedding\nallows to efficiently rank instances in the image. In the spirit of\nreproducible research we make our data, models, and code available.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 21:46:26 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Sharpe", "Samuel", ""], ["Yan", "Jin", ""], ["Wu", "Fan", ""], ["Drori", "Iddo", ""]]}, {"id": "1910.04903", "submitter": "Arturo Pardo", "authors": "Arturo Pardo, Jos\\'e A. Guti\\'errez-Guti\\'errez, Jos\\'e Miguel\n  L\\'opez-Higuera, Brian W. Pogue, and Olga M. Conde", "title": "Coloring the Black Box: Visualizing neural network behavior with a\n  self-introspective model", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following work presents how autoencoding all the possible hidden\nactivations of a network for a given problem can provide insight about its\nstructure, behavior, and vulnerabilities. The method, termed\nself-introspection, can show that a trained model showcases similar activation\npatterns (albeit randomly distributed due to initialization) when shown data\nbelonging to the same category, and classification errors occur in fringe areas\nwhere the activations are not as clearly defined, suggesting some form of\nrandom, slowly varying, implicit encoding occurring within deep networks, that\ncan be observed with this representation. Additionally, obtaining a\nlow-dimensional representation of all the activations allows for (1) real-time\nmodel evaluation in the context of a multiclass classification problem, (2) the\nrearrangement of all hidden layers by their relevance in obtaining a specific\noutput, and (3) the obtainment of a framework where studying possible\ncounter-measures to noise and adversarial attacks is possible.\nSelf-introspection can show how damaged input data can modify the hidden\nactivations, producing an erroneous response. A few illustrative are\nimplemented for feedforward and convolutional models and the MNIST and CIFAR-10\ndatasets, showcasing its capabilities as a model evaluation framework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 23:02:12 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 10:41:04 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Pardo", "Arturo", ""], ["Guti\u00e9rrez-Guti\u00e9rrez", "Jos\u00e9 A.", ""], ["L\u00f3pez-Higuera", "Jos\u00e9 Miguel", ""], ["Pogue", "Brian W.", ""], ["Conde", "Olga M.", ""]]}, {"id": "1910.04918", "submitter": "Okyaz Eminaga", "authors": "Okyaz Eminaga, Yuri Tolkach, Christian Kunder, Mahmood Abbas, Ryan\n  Han, Rosalie Nolley, Axel Semjonow, Martin Boegemann, Sebastian Huss, Andreas\n  Loening, Robert West, Geoffrey Sonn, Richard Fan, Olaf Bettendorf, James\n  Brook and Daniel Rubin", "title": "Deep Learning for Prostate Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current study detects different morphologies related to prostate\npathology using deep learning models; these models were evaluated on 2,121\nhematoxylin and eosin (H&E) stain histology images captured using bright field\nmicroscopy, which spanned a variety of image qualities, origins (whole slide,\ntissue micro array, whole mount, Internet), scanning machines, timestamps, H&E\nstaining protocols, and institutions. For case usage, these models were applied\nfor the annotation tasks in clinician-oriented pathology reports for\nprostatectomy specimens. The true positive rate (TPR) for slides with prostate\ncancer was 99.7% by a false positive rate of 0.785%. The F1-scores of Gleason\npatterns reported in pathology reports ranged from 0.795 to 1.0 at the case\nlevel. TPR was 93.6% for the cribriform morphology and 72.6% for the ductal\nmorphology. The correlation between the ground truth and the prediction for the\nrelative tumor volume was 0.987 n. Our models cover the major components of\nprostate pathology and successfully accomplish the annotation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 00:10:59 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 07:34:27 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 00:14:28 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Eminaga", "Okyaz", ""], ["Tolkach", "Yuri", ""], ["Kunder", "Christian", ""], ["Abbas", "Mahmood", ""], ["Han", "Ryan", ""], ["Nolley", "Rosalie", ""], ["Semjonow", "Axel", ""], ["Boegemann", "Martin", ""], ["Huss", "Sebastian", ""], ["Loening", "Andreas", ""], ["West", "Robert", ""], ["Sonn", "Geoffrey", ""], ["Fan", "Richard", ""], ["Bettendorf", "Olaf", ""], ["Brook", "James", ""], ["Rubin", "Daniel", ""]]}, {"id": "1910.04919", "submitter": "Yongsheng Gao", "authors": "Bin Wang, Yongsheng Gao, Xiaohan Yu, Xiaohui Yuan, Shengwu Xiong,\n  Xianzhong Feng", "title": "From Species to Cultivar: Soybean Cultivar Recognition using Multiscale\n  Sliding Chord Matching of Leaf Images", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leaf image recognition techniques have been actively researched for plant\nspecies identification. However it remains unclear whether leaf patterns can\nprovide sufficient information for cultivar recognition. This paper reports the\nfirst attempt on soybean cultivar recognition from plant leaves which is not\nonly a challenging research problem but also important for soybean cultivar\nevaluation, selection and production in agriculture. In this paper, we propose\na novel multiscale sliding chord matching (MSCM) approach to extract leaf\npatterns that are distinctive for soybean cultivar identification. A chord is\ndefined to slide along the contour for measuring the synchronised patterns of\nexterior shape and interior appearance of soybean leaf images. A multiscale\nsliding chord strategy is developed to extract features in a coarse-to-fine\nhierarchical order. A joint description that integrates the leaf descriptors\nfrom different parts of a soybean plant is proposed for further enhancing the\ndiscriminative power of cultivar description. We built a cultivar leaf image\ndatabase, SoyCultivar, consisting of 1200 sample leaf images from 200 soybean\ncultivars for performance evaluation. Encouraging experimental results of the\nproposed method in comparison to the state-of-the-art leaf species recognition\nmethods demonstrate the availability of cultivar information in soybean leaves\nand effectiveness of the proposed MSCM for soybean cultivar identification,\nwhich may advance the research in leaf recognition from species to cultivar.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 00:21:43 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Wang", "Bin", ""], ["Gao", "Yongsheng", ""], ["Yu", "Xiaohan", ""], ["Yuan", "Xiaohui", ""], ["Xiong", "Shengwu", ""], ["Feng", "Xianzhong", ""]]}, {"id": "1910.04925", "submitter": "Hongxu Yin", "authors": "Hongxu Yin, Bilal Mukadam, Xiaoliang Dai, Niraj K. Jha", "title": "DiabDeep: Pervasive Diabetes Diagnosis based on Wearable Medical Sensors\n  and Efficient Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetes impacts the quality of life of millions of people. However, diabetes\ndiagnosis is still an arduous process, given that the disease develops and gets\ntreated outside the clinic. The emergence of wearable medical sensors (WMSs)\nand machine learning points to a way forward to address this challenge. WMSs\nenable a continuous mechanism to collect and analyze physiological signals.\nHowever, disease diagnosis based on WMS data and its effective deployment on\nresource-constrained edge devices remain challenging due to inefficient feature\nextraction and vast computation cost. In this work, we propose a framework\ncalled DiabDeep that combines efficient neural networks (called DiabNNs) with\nWMSs for pervasive diabetes diagnosis. DiabDeep bypasses the feature extraction\nstage and acts directly on WMS data. It enables both an (i) accurate inference\non the server, e.g., a desktop, and (ii) efficient inference on an edge device,\ne.g., a smartphone, based on varying design goals and resource budgets. On the\nserver, we stack sparsely connected layers to deliver high accuracy. On the\nedge, we use a hidden-layer long short-term memory based recurrent layer to cut\ndown on computation and storage. At the core of DiabDeep lies a grow-and-prune\ntraining flow: it leverages gradient-based growth and magnitude-based pruning\nalgorithms to learn both weights and connections for DiabNNs. We demonstrate\nthe effectiveness of DiabDeep through analyzing data from 52 participants. For\nserver (edge) side inference, we achieve a 96.3% (95.3%) accuracy in\nclassifying diabetics against healthy individuals, and a 95.7% (94.6%) accuracy\nin distinguishing among type-1/type-2 diabetic, and healthy individuals.\nAgainst conventional baselines, DiabNNs achieve higher accuracy, while reducing\nthe model size (FLOPs) by up to 454.5x (8.9x). Therefore, the system can be\nviewed as pervasive and efficient, yet very accurate.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 01:04:38 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Yin", "Hongxu", ""], ["Mukadam", "Bilal", ""], ["Dai", "Xiaoliang", ""], ["Jha", "Niraj K.", ""]]}, {"id": "1910.04935", "submitter": "Haoran Dou", "authors": "Xin Yang, Wenlong Shi, Haoran Dou, Jikuan Qian, Yi Wang, Wufeng Xue,\n  Shengli Li, Dong Ni, Pheng-Ann Heng", "title": "FetusMap: Fetal Pose Estimation in 3D Ultrasound", "comments": "9 pages, 6 figures, 2 tables. Accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D ultrasound (US) entrance inspires a multitude of automated prenatal\nexaminations. However, studies about the structuralized description of the\nwhole fetus in 3D US are still rare. In this paper, we propose to estimate the\n3D pose of fetus in US volumes to facilitate its quantitative analyses in\nglobal and local scales. Given the great challenges in 3D US, including the\nhigh volume dimension, poor image quality, symmetric ambiguity in anatomical\nstructures and large variations of fetal pose, our contribution is three-fold.\n(i) This is the first work about 3D pose estimation of fetus in the literature.\nWe aim to extract the skeleton of whole fetus and assign different\nsegments/joints with correct torso/limb labels. (ii) We propose a\nself-supervised learning (SSL) framework to finetune the deep network to form\nvisually plausible pose predictions. Specifically, we leverage the\nlandmark-based registration to effectively encode case-adaptive anatomical\npriors and generate evolving label proxy for supervision. (iii) To enable our\n3D network perceive better contextual cues with higher resolution input under\nlimited computing resource, we further adopt the gradient check-pointing (GCP)\nstrategy to save GPU memory and improve the prediction. Extensively validated\non a large 3D US dataset, our method tackles varying fetal poses and achieves\npromising results. 3D pose estimation of fetus has potentials in serving as a\nmap to provide navigation for many advanced studies.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 01:45:09 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Yang", "Xin", ""], ["Shi", "Wenlong", ""], ["Dou", "Haoran", ""], ["Qian", "Jikuan", ""], ["Wang", "Yi", ""], ["Xue", "Wufeng", ""], ["Li", "Shengli", ""], ["Ni", "Dong", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1910.04936", "submitter": "Ziwei Liao", "authors": "Ziwei Liao, Jieqi Shi, Xianyu Qi, Xiaoyu Zhang, Wei Wang, Yijia He,\n  Ran Wei, and Xiao Liu", "title": "Coarse-To-Fine Visual Localization Using Semantic Compact Map", "comments": "Video Demo: https://youtu.be/XbTc1YNPajc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust visual localization for urban vehicles remains challenging and\nunsolved. The limitation of computation efficiency and memory size has made it\nharder for large-scale applications. Since semantic information serves as a\nstable and compact representation of the environment, we propose a\ncoarse-to-fine localization system based on a semantic compact map. Pole-like\nobjects are stored in the compact map, then are extracted from semantically\nsegmented images as observations. Localization is performed by a particle\nfilter, followed by a pose alignment module decoupling translation and rotation\nto achieve better accuracy. We evaluate our system both on synthetic and\nrealistic datasets and compare it with two baselines, a state-of-art semantic\nfeature-based system, and a traditional SIFT feature-based system. Experiments\ndemonstrate that even with a significantly small map, such as a 10 KB map for a\n3.7 km long trajectory, our system provides a comparable accuracy with the\nbaselines.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 01:46:27 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 01:24:39 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 08:13:35 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Liao", "Ziwei", ""], ["Shi", "Jieqi", ""], ["Qi", "Xianyu", ""], ["Zhang", "Xiaoyu", ""], ["Wang", "Wei", ""], ["He", "Yijia", ""], ["Wei", "Ran", ""], ["Liu", "Xiao", ""]]}, {"id": "1910.04944", "submitter": "Alireza Tavakkoli", "authors": "Lee Easson and Alireza Tavakkoli and Jonathan Greenberg", "title": "An Automatic Digital Terrain Generation Technique for Terrestrial\n  Sensing and Virtual Reality Applications", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-33720-9_48", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification and modeling of the terrain from point cloud data is an\nimportant component of Terrestrial Remote Sensing (TRS) applications. The main\nfocus in terrain modeling is capturing details of complex geological features\nof landforms. Traditional terrain modeling approaches rely on the user to exert\ncontrol over terrain features. However, relying on the user input to manually\ndevelop the digital terrain becomes intractable when considering the amount of\ndata generated by new remote sensing systems capable of producing massive\naerial and ground-based point clouds from scanned environments. This article\nprovides a novel terrain modeling technique capable of automatically generating\naccurate and physically realistic Digital Terrain Models (DTM) from a variety\nof point cloud data. The proposed method runs efficiently on large-scale point\ncloud data with real-time performance over large segments of terrestrial\nlandforms. Moreover, generated digital models are designed to effectively\nrender within a Virtual Reality (VR) environment in real time. The paper\nconcludes with an in-depth discussion of possible research directions and\noutstanding technical and scientific challenges to improve the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 02:26:01 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Easson", "Lee", ""], ["Tavakkoli", "Alireza", ""], ["Greenberg", "Jonathan", ""]]}, {"id": "1910.04953", "submitter": "Chaitanya Mitash", "authors": "Chaitanya Mitash, Bowen Wen, Kostas Bekris, and Abdeslam Boularias", "title": "Scene-level Pose Estimation for Multiple Instances of Densely Packed\n  Objects", "comments": "To appear at the Conference on Robot Learning (CoRL) - 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces key machine learning operations that allow the\nrealization of robust, joint 6D pose estimation of multiple instances of\nobjects either densely packed or in unstructured piles from RGB-D data. The\nfirst objective is to learn semantic and instance-boundary detectors without\nmanual labeling. An adversarial training framework in conjunction with\nphysics-based simulation is used to achieve detectors that behave similarly in\nsynthetic and real data. Given the stochastic output of such detectors,\ncandidates for object poses are sampled. The second objective is to\nautomatically learn a single score for each pose candidate that represents its\nquality in terms of explaining the entire scene via a gradient boosted tree.\nThe proposed method uses features derived from surface and boundary alignment\nbetween the observed scene and the object model placed at hypothesized poses.\nScene-level, multi-instance pose estimation is then achieved by an integer\nlinear programming process that selects hypotheses that maximize the sum of the\nlearned individual scores, while respecting constraints, such as avoiding\ncollisions. To evaluate this method, a dataset of densely packed objects with\nchallenging setups for state-of-the-art approaches is collected. Experiments on\nthis dataset and a public one show that the method significantly outperforms\nalternatives in terms of 6D pose accuracy while trained only with synthetic\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 03:17:55 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Mitash", "Chaitanya", ""], ["Wen", "Bowen", ""], ["Bekris", "Kostas", ""], ["Boularias", "Abdeslam", ""]]}, {"id": "1910.04961", "submitter": "Yunyan Xing", "authors": "Yunyan Xing, Zongyuan Ge, Rui Zeng, Dwarikanath Mahapatra, Jarrel\n  Seah, Meng Law and Tom Drummond", "title": "Adversarial Pulmonary Pathology Translation for Pairwise Chest X-ray\n  Data Augmentation", "comments": "Code: https://github.com/yunyanxing/pairwise_xray_augmentation -\n  Accepted to the International Conference on Medical Image Computing and\n  Computer Assisted Intervention (MICCAI) 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32226-7_84", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works show that Generative Adversarial Networks (GANs) can be\nsuccessfully applied to chest X-ray data augmentation for lung disease\nrecognition. However, the implausible and distorted pathology features\ngenerated from the less than perfect generator may lead to wrong clinical\ndecisions. Why not keep the original pathology region? We proposed a novel\napproach that allows our generative model to generate high quality plausible\nimages that contain undistorted pathology areas. The main idea is to design a\ntraining scheme based on an image-to-image translation network to introduce\nvariations of new lung features around the pathology ground-truth area.\nMoreover, our model is able to leverage both annotated disease images and\nunannotated healthy lung images for the purpose of generation. We demonstrate\nthe effectiveness of our model on two tasks: (i) we invite certified\nradiologists to assess the quality of the generated synthetic images against\nreal and other state-of-the-art generative models, and (ii) data augmentation\nto improve the performance of disease localisation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 03:57:37 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 01:34:58 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Xing", "Yunyan", ""], ["Ge", "Zongyuan", ""], ["Zeng", "Rui", ""], ["Mahapatra", "Dwarikanath", ""], ["Seah", "Jarrel", ""], ["Law", "Meng", ""], ["Drummond", "Tom", ""]]}, {"id": "1910.04963", "submitter": "Mauricio Perez", "authors": "Mauricio Perez, Jun Liu and Alex C. Kot", "title": "Interaction Relational Network for Mutual Action Recognition", "comments": "12 pages, 6 figures, to be published in IEEE TMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person-person mutual action recognition (also referred to as interaction\nrecognition) is an important research branch of human activity analysis.\nCurrent solutions in the field -- mainly dominated by CNNs, GCNs and LSTMs --\noften consist of complicated architectures and mechanisms to embed the\nrelationships between the two persons on the architecture itself, to ensure the\ninteraction patterns can be properly learned. Our main contribution with this\nwork is by proposing a simpler yet very powerful architecture, named\nInteraction Relational Network, which utilizes minimal prior knowledge about\nthe structure of the human body. We drive the network to identify by itself how\nto relate the body parts from the individuals interacting. In order to better\nrepresent the interaction, we define two different relationships, leading to\nspecialized architectures and models for each. These multiple relationship\nmodels will then be fused into a single and special architecture, in order to\nleverage both streams of information for further enhancing the relational\nreasoning capability. Furthermore we define important structured pair-wise\noperations to extract meaningful extra information from each pair of joints --\ndistance and motion. Ultimately, with the coupling of an LSTM, our IRN is\ncapable of paramount sequential relational reasoning. These important\nextensions we made to our network can also be valuable to other problems that\nrequire sophisticated relational reasoning. Our solution is able to achieve\nstate-of-the-art performance on the traditional interaction recognition\ndatasets SBU and UT, and also on the mutual actions from the large-scale\ndataset NTU RGB+D. Furthermore, it obtains competitive performance in the NTU\nRGB+D 120 dataset interactions subset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 04:00:40 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 08:04:54 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Perez", "Mauricio", ""], ["Liu", "Jun", ""], ["Kot", "Alex C.", ""]]}, {"id": "1910.04964", "submitter": "Xin Wang", "authors": "Wenwu Zhu, Xin Wang, Hongzhi Li", "title": "Multi-modal Deep Analysis for Multimedia", "comments": "25 pages, 39 figures, IEEE Transactions on Circuits and Systems for\n  Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2940647", "report-no": null, "categories": "cs.MM cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of Internet and multimedia services in the past\ndecade, a huge amount of user-generated and service provider-generated\nmultimedia data become available. These data are heterogeneous and multi-modal\nin nature, imposing great challenges for processing and analyzing them.\nMulti-modal data consist of a mixture of various types of data from different\nmodalities such as texts, images, videos, audios etc. In this article, we\npresent a deep and comprehensive overview for multi-modal analysis in\nmultimedia. We introduce two scientific research problems, data-driven\ncorrelational representation and knowledge-guided fusion for multimedia\nanalysis. To address the two scientific problems, we investigate them from the\nfollowing aspects: 1) multi-modal correlational representation: multi-modal\nfusion of data across different modalities, and 2) multi-modal data and\nknowledge fusion: multi-modal fusion of data with domain knowledge. More\nspecifically, on data-driven correlational representation, we highlight three\nimportant categories of methods, such as multi-modal deep representation,\nmulti-modal transfer learning, and multi-modal hashing. On knowledge-guided\nfusion, we discuss the approaches for fusing knowledge with data and four\nexemplar applications that require various kinds of domain knowledge, including\nmulti-modal visual question answering, multi-modal video summarization,\nmulti-modal visual pattern mining and multi-modal recommendation. Finally, we\nbring forward our insights and future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 04:21:36 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 08:42:13 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zhu", "Wenwu", ""], ["Wang", "Xin", ""], ["Li", "Hongzhi", ""]]}, {"id": "1910.04981", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, and Stefan Winkler", "title": "Estimating Solar Irradiance Using Sky Imagers", "comments": "Published in Atmospheric Measurement Techniques (AMT), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground-based whole sky cameras are extensively used for localized monitoring\nof clouds nowadays. They capture hemispherical images of the sky at regular\nintervals using a fisheye lens. In this paper, we propose a framework for\nestimating solar irradiance from pictures taken by those imagers. Unlike\npyranometers, such sky images contain information about cloud coverage and can\nbe used to derive cloud movement. An accurate estimation of solar irradiance\nusing solely those images is thus a first step towards short-term forecasting\nof solar energy generation based on cloud movement. We derive and validate our\nmodel using pyranometers co-located with our whole sky imagers. We achieve a\nbetter performance in estimating solar irradiance and in particular its\nshort-term variations as compared to other related methods using ground-based\nobservations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 05:42:55 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Savoy", "Florian M.", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1910.04985", "submitter": "Guoli Wang", "authors": "Mengjia Yan, Mengao Zhao, Zining Xu, Qian Zhang, Guoli Wang, Zhizhong\n  Su", "title": "VarGFaceNet: An Efficient Variable Group Convolutional Neural Network\n  for Lightweight Face Recognition", "comments": "8 pages,2 figures. In Proceedings of the IEEE International\n  Conference on Computer Vision Workshop, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the discriminative and generalization ability of lightweight\nnetwork for face recognition, we propose an efficient variable group\nconvolutional network called VarGFaceNet. Variable group convolution is\nintroduced by VarGNet to solve the conflict between small computational cost\nand the unbalance of computational intensity inside a block. We employ variable\ngroup convolution to design our network which can support large scale face\nidentification while reduce computational cost and parameters. Specifically, we\nuse a head setting to reserve essential information at the start of the network\nand propose a particular embedding setting to reduce parameters of\nfully-connected layer for embedding. To enhance interpretation ability, we\nemploy an equivalence of angular distillation loss to guide our lightweight\nnetwork and we apply recursive knowledge distillation to relieve the\ndiscrepancy between the teacher model and the student model. The champion of\ndeepglint-light track of LFR (2019) challenge demonstrates the effectiveness of\nour model and approach. Implementation of VarGFaceNet will be released at\nhttps://github.com/zma-c-137/VarGFaceNet soon.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 06:16:09 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 07:06:54 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 12:30:31 GMT"}, {"version": "v4", "created": "Sun, 24 Nov 2019 09:24:06 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Yan", "Mengjia", ""], ["Zhao", "Mengao", ""], ["Xu", "Zining", ""], ["Zhang", "Qian", ""], ["Wang", "Guoli", ""], ["Su", "Zhizhong", ""]]}, {"id": "1910.04987", "submitter": "Yue Gao", "authors": "Yue Gao, Yuan Guo, Zhouhui Lian, Yingmin Tang, Jianguo Xiao", "title": "Artistic Glyph Image Synthesis via One-Stage Few-Shot Learning", "comments": "Accepted by SIGGRAPH Asia 2019, code and datasets:\n  https://hologerry.github.io/AGIS-Net/", "journal-ref": "ACM Trans. Graph. 38, 6, Article 185 (November 2019), 12 pages", "doi": "10.1145/3355089.3356574", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic generation of artistic glyph images is a challenging task that\nattracts many research interests. Previous methods either are specifically\ndesigned for shape synthesis or focus on texture transfer. In this paper, we\npropose a novel model, AGIS-Net, to transfer both shape and texture styles in\none-stage with only a few stylized samples. To achieve this goal, we first\ndisentangle the representations for content and style by using two encoders,\nensuring the multi-content and multi-style generation. Then we utilize two\ncollaboratively working decoders to generate the glyph shape image and its\ntexture image simultaneously. In addition, we introduce a local texture\nrefinement loss to further improve the quality of the synthesized textures. In\nthis manner, our one-stage model is much more efficient and effective than\nother multi-stage stacked methods. We also propose a large-scale dataset with\nChinese glyph images in various shape and texture styles, rendered from 35\nprofessional-designed artistic fonts with 7,326 characters and 2,460 synthetic\nartistic fonts with 639 characters, to validate the effectiveness and\nextendability of our method. Extensive experiments on both English and Chinese\nartistic glyph image datasets demonstrate the superiority of our model in\ngenerating high-quality stylized glyph images against other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 06:23:26 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 09:20:50 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Gao", "Yue", ""], ["Guo", "Yuan", ""], ["Lian", "Zhouhui", ""], ["Tang", "Yingmin", ""], ["Xiao", "Jianguo", ""]]}, {"id": "1910.04988", "submitter": "Rui Fan", "authors": "Rui Fan, Ming Liu", "title": "Road Damage Detection Based on Unsupervised Disparity Map Segmentation", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TITS.2019.2947206", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel road damage detection algorithm based on\nunsupervised disparity map segmentation. Firstly, a disparity map is\ntransformed by minimizing an energy function with respect to stereo rig roll\nangle and road disparity projection model. Instead of solving this energy\nminimization problem using non-linear optimization techniques, we directly find\nits numerical solution. The transformed disparity map is then segmented using\nOtus's thresholding method, and the damaged road areas can be extracted. The\nproposed algorithm requires no parameters when detecting road damage. The\nexperimental results illustrate that our proposed algorithm performs both\naccurately and efficiently. The pixel-level road damage detection accuracy is\napproximately 97.56%.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 06:31:28 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Fan", "Rui", ""], ["Liu", "Ming", ""]]}, {"id": "1910.04992", "submitter": "Emre Baspinar", "authors": "E. Baspinar, A. Sarti, G. Citti", "title": "A sub-Riemannian model of the visual cortex with frequency and phase", "comments": null, "journal-ref": null, "doi": "10.1186/s13408-020-00089-6", "report-no": null, "categories": "q-bio.NC cs.CV math.AP math.DG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel model of the primary visual cortex (V1)\nbased on orientation, frequency and phase selective behavior of the V1 simple\ncells. We start from the first level mechanisms of visual perception: receptive\nprofiles. The model interprets V1 as a fiber bundle over the 2-dimensional\nretinal plane by introducing orientation, frequency and phase as intrinsic\nvariables. Each receptive profile on the fiber is mathematically interpreted as\na rotated, frequency modulated and phase shifted Gabor function. We start from\nthe Gabor function and show that it induces in a natural way the model geometry\nand the associated horizontal connectivity modeling the neural connectivity\npatterns in V1. We provide an image enhancement algorithm employing the model\nframework. The algorithm is capable of exploiting not only orientation but also\nfrequency and phase information existing intrinsically in a 2-dimensional input\nimage. We provide the experimental results corresponding to the enhancement\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 06:41:11 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Baspinar", "E.", ""], ["Sarti", "A.", ""], ["Citti", "G.", ""]]}, {"id": "1910.04997", "submitter": "Sebastian Zambal", "authors": "Sebastian Zambal, Christoph Heindl, Christian Eitzinger, Josef\n  Scharinger", "title": "End-to-End Defect Detection in Automated Fiber Placement Based on\n  Artificially Generated Data", "comments": "Presented at Quality Control by Artificial Vision (QCAV), 2019", "journal-ref": null, "doi": "10.1117/12.2521739", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated fiber placement (AFP) is an advanced manufacturing technology that\nincreases the rate of production of composite materials. At the same time, the\nneed for adaptable and fast inline control methods of such parts raises.\nExisting inspection systems make use of handcrafted filter chains and feature\ndetectors, tuned for a specific measurement methods by domain experts. These\nmethods hardly scale to new defects or different measurement devices. In this\npaper, we propose to formulate AFP defect detection as an image segmentation\nproblem that can be solved in an end-to-end fashion using artificially\ngenerated training data. We employ a probabilistic graphical model to generate\ntraining images and annotations. We then train a deep neural network based on\nrecent architectures designed for image segmentation. This leads to an\nappealing method that scales well with new defect types and measurement devices\nand requires little real world data for training.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 07:10:24 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Zambal", "Sebastian", ""], ["Heindl", "Christoph", ""], ["Eitzinger", "Christian", ""], ["Scharinger", "Josef", ""]]}, {"id": "1910.05021", "submitter": "Pierluigi Zama Ramirez", "authors": "Pierluigi Zama Ramirez, Claudio Paternesi, Luca De Luigi, Luigi Lella,\n  Daniele De Gregorio, Luigi Di Stefano", "title": "Shooting Labels: 3D Semantic Labeling by Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Availability of a few, large-size, annotated datasets, like ImageNet, Pascal\nVOC and COCO, has lead deep learning to revolutionize computer vision research\nby achieving astonishing results in several vision tasks.We argue that new\ntools to facilitate generation of annotated datasets may help spreading\ndata-driven AI throughout applications and domains. In this work we propose\nShooting Labels, the first 3D labeling tool for dense 3D semantic segmentation\nwhich exploits Virtual Reality to render the labeling task as easy and fun as\nplaying a video-game. Our tool allows for semantically labeling large scale\nenvironments very expeditiously, whatever the nature of the 3D data at hand\n(e.g. point clouds, mesh). Furthermore, Shooting Labels efficiently integrates\nmultiusers annotations to improve the labeling accuracy automatically and\ncompute a label uncertainty map. Besides, within our framework the 3D\nannotations can be projected into 2D images, thereby speeding up also a\nnotoriously slow and expensive task such as pixel-wise semantic labeling. We\ndemonstrate the accuracy and efficiency of our tool in two different scenarios:\nan indoor workspace provided by Matterport3D and a large-scale outdoor\nenvironment reconstructed from 1000+ KITTI images.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 08:11:27 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 11:49:52 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ramirez", "Pierluigi Zama", ""], ["Paternesi", "Claudio", ""], ["De Luigi", "Luca", ""], ["Lella", "Luigi", ""], ["De Gregorio", "Daniele", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1910.05039", "submitter": "Zhou Yueyuan", "authors": "Chengtao Cai, Yueyuan Zhou, Yanming Wang", "title": "CHD:Consecutive Horizontal Dropout for Human Gait Feature Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite gait recognition and person re-identification researches have made a\nlot of progress, the accuracy of identification is not high enough in some\nspecific situations, for example, people carrying bags or changing coats. In\norder to alleviate above situations, we propose a simple but effective\nConsecutive Horizontal Dropout (CHD) method apply on human feature extraction\nin deep learning network to avoid overfitting. Within the CHD, we intensify the\nrobust of deep learning network for cross-view gait recognition and person\nre-identification. The experiments illustrate that the rank-1 accuracy on\ncross-view gait recognition task has been increased about 10% from 68.0% to\n78.201% and 8% from 83.545% to 91.364% in person re-identification task in\nwearing coat or jacket condition. In addition, 100% accuracy of NM condition\nwas first obtained with CHD. On the benchmarks of CASIA-B, above accuracies are\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 09:16:04 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 09:05:48 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Cai", "Chengtao", ""], ["Zhou", "Yueyuan", ""], ["Wang", "Yanming", ""]]}, {"id": "1910.05057", "submitter": "Elahe Arani", "authors": "Elahe Arani, Fahad Sarfraz, Bahram Zonooz", "title": "Noise as a Resource for Learning in Knowledge Distillation", "comments": "Accepted at IEEE Winter Conference on Applications of Computer Vision\n  (WACV, 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While noise is commonly considered a nuisance in computing systems, a number\nof studies in neuroscience have shown several benefits of noise in the nervous\nsystem from enabling the brain to carry out computations such as probabilistic\ninference as well as carrying additional information about the stimuli.\nSimilarly, noise has been shown to improve the performance of deep neural\nnetworks. In this study, we further investigate the effect of adding noise in\nthe knowledge distillation framework because of its resemblance to\ncollaborative subnetworks in the brain regions. We empirically show that\ninjecting constructive noise at different levels in the collaborative learning\nframework enables us to train the model effectively and distill desirable\ncharacteristics in the student model. In doing so, we propose three different\nmethods that target the common challenges in deep neural networks: minimizing\nthe performance gap between a compact model and large model (Fickle Teacher),\ntraining high performance compact adversarially robust models (Soft\nRandomization), and training models efficiently under label noise (Messy\nCollaboration). Our findings motivate further study in the role of noise as a\nresource for learning in a collaborative learning framework.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 09:58:50 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 11:52:30 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Arani", "Elahe", ""], ["Sarfraz", "Fahad", ""], ["Zonooz", "Bahram", ""]]}, {"id": "1910.05085", "submitter": "Viswanath Sivakumar", "authors": "Fedor Borisyuk, Albert Gordo, Viswanath Sivakumar", "title": "Rosetta: Large scale system for text detection and recognition in images", "comments": "Proceedings of the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining (KDD) 2018, London, United Kingdom", "journal-ref": null, "doi": "10.1145/3219819.3219861", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a deployed, scalable optical character recognition\n(OCR) system, which we call Rosetta, designed to process images uploaded daily\nat Facebook scale. Sharing of image content has become one of the primary ways\nto communicate information among internet users within social networks such as\nFacebook and Instagram, and the understanding of such media, including its\ntextual information, is of paramount importance to facilitate search and\nrecommendation applications. We present modeling techniques for efficient\ndetection and recognition of text in images and describe Rosetta's system\narchitecture. We perform extensive evaluation of presented technologies,\nexplain useful practical approaches to build an OCR system at scale, and\nprovide insightful intuitions as to why and how certain components work based\non the lessons learnt during the development and deployment of the system.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 11:24:45 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Borisyuk", "Fedor", ""], ["Gordo", "Albert", ""], ["Sivakumar", "Viswanath", ""]]}, {"id": "1910.05121", "submitter": "Annette Kopp-Schneider", "authors": "Manuel Wiesenfarth, Annika Reinke, Bennett A. Landman, Manuel Jorge\n  Cardoso, Lena Maier-Hein, Annette Kopp-Schneider", "title": "Methods and open-source toolkit for analyzing and visualizing challenge\n  results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical challenges have become the de facto standard for benchmarking\nbiomedical image analysis algorithms. While the number of challenges is\nsteadily increasing, surprisingly little effort has been invested in ensuring\nhigh quality design, execution and reporting for these international\ncompetitions. Specifically, results analysis and visualization in the event of\nuncertainties have been given almost no attention in the literature. Given\nthese shortcomings, the contribution of this paper is two-fold: (1) We present\na set of methods to comprehensively analyze and visualize the results of\nsingle-task and multi-task challenges and apply them to a number of simulated\nand real-life challenges to demonstrate their specific strengths and\nweaknesses; (2) We release the open-source framework challengeR as part of this\nwork to enable fast and wide adoption of the methodology proposed in this\npaper. Our approach offers an intuitive way to gain important insights into the\nrelative and absolute performance of algorithms, which cannot be revealed by\ncommonly applied visualization techniques. This is demonstrated by the\nexperiments performed within this work. Our framework could thus become an\nimportant tool for analyzing and visualizing challenge results in the field of\nbiomedical image analysis and beyond.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:33:53 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 13:51:36 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Wiesenfarth", "Manuel", ""], ["Reinke", "Annika", ""], ["Landman", "Bennett A.", ""], ["Cardoso", "Manuel Jorge", ""], ["Maier-Hein", "Lena", ""], ["Kopp-Schneider", "Annette", ""]]}, {"id": "1910.05134", "submitter": "Sijin Wang", "authors": "Sijin Wang, Ruiping Wang, Ziwei Yao, Shiguang Shan, Xilin Chen", "title": "Cross-modal Scene Graph Matching for Relationship-aware Image-Text\n  Retrieval", "comments": "Accepted by WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-text retrieval of natural scenes has been a popular research topic.\nSince image and text are heterogeneous cross-modal data, one of the key\nchallenges is how to learn comprehensive yet unified representations to express\nthe multi-modal data. A natural scene image mainly involves two kinds of visual\nconcepts, objects and their relationships, which are equally essential to\nimage-text retrieval. Therefore, a good representation should account for both\nof them. In the light of recent success of scene graph in many CV and NLP tasks\nfor describing complex natural scenes, we propose to represent image and text\nwith two kinds of scene graphs: visual scene graph (VSG) and textual scene\ngraph (TSG), each of which is exploited to jointly characterize objects and\nrelationships in the corresponding modality. The image-text retrieval task is\nthen naturally formulated as cross-modal scene graph matching. Specifically, we\ndesign two particular scene graph encoders in our model for VSG and TSG, which\ncan refine the representation of each node on the graph by aggregating\nneighborhood information. As a result, both object-level and relationship-level\ncross-modal features can be obtained, which favorably enables us to evaluate\nthe similarity of image and text in the two levels in a more plausible way. We\nachieve state-of-the-art results on Flickr30k and MSCOCO, which verifies the\nadvantages of our graph matching based approach for image-text retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:39:48 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Wang", "Sijin", ""], ["Wang", "Ruiping", ""], ["Yao", "Ziwei", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1910.05148", "submitter": "Mark Boss", "authors": "Mark Boss, Hendrik P.A. Lensch", "title": "Single Image BRDF Parameter Estimation with a Conditional Adversarial\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating plausible surfaces is an essential component in achieving a high\ndegree of realism in rendering. To relieve artists, who create these surfaces\nin a time-consuming, manual process, automated retrieval of the\nspatially-varying Bidirectional Reflectance Distribution Function (SVBRDF) from\na single mobile phone image is desirable. By leveraging a deep neural network,\nthis casual capturing method can be achieved. The trained network can estimate\nper pixel normal, base color, metallic and roughness parameters from the Disney\nBRDF. The input image is taken with a mobile phone lit by the camera flash. The\nnetwork is trained to compensate for environment lighting and thus learned to\nreduce artifacts introduced by other light sources. These losses contain a\nmulti-scale discriminator with an additional perceptual loss, a rendering loss\nusing a differentiable renderer, and a parameter loss. Besides the local\nprecision, this loss formulation generates material texture maps which are\nglobally more consistent. The network is set up as a generator network trained\nin an adversarial fashion to ensure that only plausible maps are produced. The\nestimated parameters not only reproduce the material faithfully in rendering\nbut capture the style of hand-authored materials due to the more global loss\nterms compared to previous works without requiring additional post-processing.\nBoth the resolution and the quality is improved.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:00:06 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Boss", "Mark", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1910.05149", "submitter": "Nicolas Farrugia", "authors": "Yusuf Pilavci (IMT Atlantique - ELEC, POLIMI), Nicolas Farrugia (IMT\n  Atlantique - ELEC)", "title": "Spectral Graph Wavelet Transform as Feature Extractor for Machine\n  Learning in Neuroimaging", "comments": null, "journal-ref": "International Conference on Acoustics, Speech, and Signal\n  Processing, May 2019, Brighton, United Kingdom", "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Signal Processing has become a very useful framework for signal\noperations and representations defined on irregular domains. Exploiting\ntransformations that are defined on graph models can be highly beneficial when\nthe graph encodes relationships between signals. In this work, we present the\nbenefits of using Spectral Graph Wavelet Transform (SGWT) as a feature\nextractor for machine learning on brain graphs. First, we consider a synthetic\nregression problem in which the smooth graph signals are generated as input\nwith additive noise, and the target is derived from the input without noise.\nThis enables us to optimize the spectrum coverage using different wavelet\nshapes. Finally, we present the benefits obtained by SGWT on a functional\nMagnetic Resonance Imaging (fMRI) open dataset on human subjects, with several\ngraphs and wavelet shapes, by demonstrating significant performance\nimprovements compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:00:44 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Pilavci", "Yusuf", "", "IMT Atlantique - ELEC, POLIMI"], ["Farrugia", "Nicolas", "", "IMT\n  Atlantique - ELEC"]]}, {"id": "1910.05200", "submitter": "Gaurav Bharaj", "authors": "Abdallah Dib, Gaurav Bharaj, Junghyun Ahn, Cedric Thebault,\n  Philippe-Henri Gosselin, Louis Chevallier", "title": "Face Reflectance and Geometry Modeling via Differentiable Ray Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel strategy to automatically reconstruct 3D faces from\nmonocular images with explicitly disentangled facial geometry (pose, identity\nand expression), reflectance (diffuse and specular albedo), and self-shadows.\nThe scene lights are modeled as a virtual light stage with pre-oriented area\nlights used in conjunction with differentiable Monte-Carlo ray tracing to\noptimize the scene and face parameters. With correctly disentangled\nself-shadows and specular reflection parameters, we can not only obtain robust\nfacial geometry reconstruction, but also gain explicit control over these\nparameters, with several practical applications. We can change facial\nexpressions with accurate resultant self-shadows or relight the scene and\nobtain accurate specular reflection and several other parameter combinations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:39:33 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Dib", "Abdallah", ""], ["Bharaj", "Gaurav", ""], ["Ahn", "Junghyun", ""], ["Thebault", "Cedric", ""], ["Gosselin", "Philippe-Henri", ""], ["Chevallier", "Louis", ""]]}, {"id": "1910.05280", "submitter": "Masato Tamura", "authors": "Masato Tamura, Tomokazu Murakami", "title": "Augmented Hard Example Mining for Generalizable Person Re-Identification", "comments": "Submit to WACV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the performance of person re-identification (Re-ID) has been much\nimproved by using sophisticated training methods and large-scale labelled\ndatasets, many existing methods make the impractical assumption that\ninformation of a target domain can be utilized during training. In practice, a\nRe-ID system often starts running as soon as it is deployed, hence training\nwith data from a target domain is unrealistic. To make Re-ID systems more\npractical, methods have been proposed that achieve high performance without\ninformation of a target domain. However, they need cumbersome tuning for\ntraining and unusual operations for testing. In this paper, we propose\naugmented hard example mining, which can be easily integrated to a common Re-ID\ntraining process and can utilize sophisticated models without any network\nmodification. The method discovers hard examples on the basis of classification\nprobabilities, and to make the examples harder, various types of augmentation\nare applied to the examples. Among those examples, excessively augmented ones\nare eliminated by a classification based selection process. Extensive analysis\nshows that our method successfully selects effective examples and achieves\nstate-of-the-art performance on publicly available benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 16:19:53 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Tamura", "Masato", ""], ["Murakami", "Tomokazu", ""]]}, {"id": "1910.05283", "submitter": "Bingnan Luo", "authors": "Bingnan Luo, Jie Shen, Shiyang Cheng, Yujiang Wang, Maja Pantic", "title": "Shape Constrained Network for Eye Segmentation in the Wild", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of eyes has long been a vital pre-processing step in\nmany biometric applications. Majority of the works focus only on high\nresolution eye images, while little has been done to segment the eyes from low\nquality images in the wild. However, this is a particularly interesting and\nmeaningful topic, as eyes play a crucial role in conveying the emotional state\nand mental well-being of a person. In this work, we take two steps toward\nsolving this problem: (1) We collect and annotate a challenging eye\nsegmentation dataset containing 8882 eye patches from 4461 facial images of\ndifferent resolutions, illumination conditions and head poses; (2) We develop a\nnovel eye segmentation method, Shape Constrained Network (SCN), that\nincorporates shape prior into the segmentation network training procedure.\nSpecifically, we learn the shape prior from our dataset using VAE-GAN, and\nleverage the pre-trained encoder and discriminator to regularise the training\nof SegNet. To improve the accuracy and quality of predicted masks, we replace\nthe loss of SegNet with three new losses: Intersection-over-Union (IoU) loss,\nshape discriminator loss and shape embedding loss. Extensive experiments shows\nthat our method outperforms state-of-the-art segmentation and landmark\ndetection methods in terms of mean IoU (mIoU) accuracy and the quality of\nsegmentation masks. The eye segmentation database is available at\nhttps://www.dropbox.com/s/yvveouvxsvti08x/Eye_Segmentation_Database.zip?dl=0.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 16:21:55 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Luo", "Bingnan", ""], ["Shen", "Jie", ""], ["Cheng", "Shiyang", ""], ["Wang", "Yujiang", ""], ["Pantic", "Maja", ""]]}, {"id": "1910.05309", "submitter": "Chuan-Chi Lai", "authors": "Li-Chun Wang, Chuan-Chi Lai, Hong-Han Shuai, Hsin-Piao Lin, Chi-Yu Li,\n  Teng-Hu Cheng, Chiun-Hsun Chen", "title": "Communications and Networking Technologies for Intelligent Drone\n  Cruisers", "comments": "6 pages, 11 figures, accepted by 2019 IEEE Globecom Workshops (GC\n  Wkshps): IEEE GLOBECOM 2019 Workshop on Space-Ground Integrated Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future mobile communication networks require an Aerial Base Station (ABS)\nwith fast mobility and long-term hovering capabilities. At present, unmanned\naerial vehicles (UAV) or drones do not have long flight times and are mainly\nused for monitoring, surveillance, and image post-processing. On the other\nhand, the traditional airship is too large and not easy to take off and land.\nTherefore, we propose to develop an \"Artificial Intelligence (AI)\nDrone-Cruiser\" base station that can help 5G mobile communication systems and\nbeyond quickly recover the network after a disaster and handle the instant\ncommunications by the flash crowd. The drone-cruiser base station can overcome\nthe communications problem for three types of flash crowds, such as in\nstadiums, parades, and large plaza so that an appropriate number of aerial base\nstations can be accurately deployed to meet large and dynamic traffic demands.\nArtificial intelligence can solve these problems by analyzing the collected\ndata, and then adjust the system parameters in the framework of Self-Organizing\nNetwork (SON) to achieve the goals of self-configuration, self-optimization,\nand self-healing. With the help of AI technologies, 5G networks can become more\nintelligent. This paper aims to provide a new type of service, On-Demand Aerial\nBase Station as a Service. This work needs to overcome the following five\ntechnical challenges: innovative design of drone-cruisers for the long-time\nhovering, crowd estimation and prediction, rapid 3D wireless channel learning\nand modeling, 3D placement of aerial base stations and the integration of WiFi\nfront-haul and millimeter wave/WiGig back-haul networks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:22:29 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Wang", "Li-Chun", ""], ["Lai", "Chuan-Chi", ""], ["Shuai", "Hong-Han", ""], ["Lin", "Hsin-Piao", ""], ["Li", "Chi-Yu", ""], ["Cheng", "Teng-Hu", ""], ["Chen", "Chiun-Hsun", ""]]}, {"id": "1910.05312", "submitter": "David Fiedler", "authors": "David Fiedler, Michal \\v{C}\\'ap, Jan Nykl, Pavol \\v{Z}ileck\\'y, and\n  Martin Schaefer", "title": "Map Matching Algorithm for Large-scale Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPS receivers embedded in cell phones and connected vehicles generate a\nseries of location measurements that can be used for various analytical\npurposes. A common pre-processing step of this data is the so-called map\nmatching. The goal of map matching is to infer the trajectory that the device\nfollowed in a road network from a potentially sparse series of noisy location\nmeasurements. Although accurate and robust map matching algorithms based on\nprobabilistic models exist, they are computationally heavy and thus impractical\nfor processing of large datasets. In this paper, we present a scalable\nmap-matching algorithm based on Dijkstra shortest path method, that is both\naccurate and applicable to large datasets. Our experiments on a\npublicly-available dataset showed that the proposed method achieves accuracy\nthat is comparable to that of the existing map matching methods using only a\nfraction of computational resources. In result, our algorithm can be used to\nefficiently process large datasets of noisy and potentially sparse location\ndata that would be unexploitable using existing techniques due to their high\ncomputational requirements.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 13:49:32 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Fiedler", "David", ""], ["\u010c\u00e1p", "Michal", ""], ["Nykl", "Jan", ""], ["\u017dileck\u00fd", "Pavol", ""], ["Schaefer", "Martin", ""]]}, {"id": "1910.05316", "submitter": "Xu Chen", "authors": "En Li and Liekang Zeng and Zhi Zhou and Xu Chen", "title": "Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge\n  Computing", "comments": "Accepted by IEEE Transactions on Wireless Communications, Sept 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a key technology of enabling Artificial Intelligence (AI) applications in\n5G era, Deep Neural Networks (DNNs) have quickly attracted widespread\nattention. However, it is challenging to run computation-intensive DNN-based\ntasks on mobile devices due to the limited computation resources. What's worse,\ntraditional cloud-assisted DNN inference is heavily hindered by the significant\nwide-area network latency, leading to poor real-time performance as well as low\nquality of user experience. To address these challenges, in this paper, we\npropose Edgent, a framework that leverages edge computing for DNN collaborative\ninference through device-edge synergy. Edgent exploits two design knobs: (1)\nDNN partitioning that adaptively partitions computation between device and edge\nfor purpose of coordinating the powerful cloud resource and the proximal edge\nresource for real-time DNN inference; (2) DNN right-sizing that further reduces\ncomputing latency via early exiting inference at an appropriate intermediate\nDNN layer. In addition, considering the potential network fluctuation in\nreal-world deployment, Edgentis properly design to specialize for both static\nand dynamic network environment. Specifically, in a static environment where\nthe bandwidth changes slowly, Edgent derives the best configurations with the\nassist of regression-based prediction models, while in a dynamic environment\nwhere the bandwidth varies dramatically, Edgent generates the best execution\nplan through the online change point detection algorithm that maps the current\nbandwidth state to the optimal configuration. We implement Edgent prototype\nbased on the Raspberry Pi and the desktop PC and the extensive experimental\nevaluations demonstrate Edgent's effectiveness in enabling on-demand\nlow-latency edge intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 00:53:44 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Li", "En", ""], ["Zeng", "Liekang", ""], ["Zhou", "Zhi", ""], ["Chen", "Xu", ""]]}, {"id": "1910.05318", "submitter": "Dimitrios Kollias", "authors": "Mengyao Liu and Dimitrios Kollias", "title": "Aff-Wild Database and AffWildNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of HCI, building an automatic system to recognize affect of\nhuman facial expression in real-world condition is very crucial to make machine\ninteract naturallisticaly with a man. However, existing facial emotion\ndatabases usually contain expression in the limited scenario under\nwell-controlled condition. Aff-Wild is currently the largest database\nconsisting of spontaneous facial expression in the wild annotated with valence\nand arousal. The first contribution of this project is the completion of\nextending Aff-Wild database which is fulfilled by collecting videos from\nYouTube on which the videos have spontaneous facial expressions in the wild,\nannotating videos with valence and arousal ranging in [-1,1], detecting faces\nin frames using FFLD2 detector and partitioning the whole data set into train,\nvalidate and test set, with 527056, 94223 and 135145 frames. The diversity is\nguaranteed regarding age, ethnicity and values of valence and arousal. The\nratio of male to female is close to 1. Regarding the techniques used to build\nthe automatic system, deep learning is outstanding since almost all winning\nmethods in emotion challenges adopt DNN techniques. The second contribution of\nthis project is that an end-to-end DNN is constructed to have joint CNN and RNN\nblock and gives the estimation on valence and arousal for each frame in\nsequential data. VGGFace, ResNet, DenseNet with the corresponding pre-trained\nmodel for CNN block and LSTM, GRU, IndRNN, Attention mechanism for RNN block\nare experimented aiming to find the best combination. Fine tuning and transfer\nlearning techniques are also tried out. By comparing the CCC evaluation value\non test data, the best model is found to be pre-trained VGGFace connected with\n2 layers GRU with attention mechanism. The models test performance is 0.555 CCC\nfor valence with sequence length 80 and 0.499 CCC for arousal with sequence\nlength 70.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 17:24:45 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 22:42:52 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Liu", "Mengyao", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05338", "submitter": "Minh Vu", "authors": "Minh H. Vu, Tufve Nyholm, Tommy L\\\"ofstedt", "title": "TuNet: End-to-end Hierarchical Brain Tumor Segmentation using Cascaded\n  Networks", "comments": "Accepted at MICCAI BrainLes 2019", "journal-ref": null, "doi": "10.1007/978-3-030-46640-4_17", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Glioma is one of the most common types of brain tumors; it arises in the\nglial cells in the human brain and in the spinal cord. In addition to having a\nhigh mortality rate, glioma treatment is also very expensive. Hence, automatic\nand accurate segmentation and measurement from the early stages are critical in\norder to prolong the survival rates of the patients and to reduce the costs of\nthe treatment. In the present work, we propose a novel end-to-end cascaded\nnetwork for semantic segmentation that utilizes the hierarchical structure of\nthe tumor sub-regions with ResNet-like blocks and Squeeze-and-Excitation\nmodules after each convolution and concatenation block. By utilizing\ncross-validation, an average ensemble technique, and a simple post-processing\ntechnique, we obtained dice scores of 88.06, 80.84, and 80.29, and Hausdorff\nDistances (95th percentile) of 6.10, 5.17, and 2.21 for the whole tumor, tumor\ncore, and enhancing tumor, respectively, on the online test set.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 19:02:58 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 10:52:11 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 06:43:53 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Vu", "Minh H.", ""], ["Nyholm", "Tufve", ""], ["L\u00f6fstedt", "Tommy", ""]]}, {"id": "1910.05370", "submitter": "Ilkay Oksuz", "authors": "Ilkay Oksuz, James R. Clough, Bram Ruijsink, Esther Puyol Anton,\n  Aurelien Bustin, Gastao Cruz, Claudia Prieto, Andrew P. King, Julia A.\n  Schnabel", "title": "Deep Learning Based Detection and Correction of Cardiac MR Motion\n  Artefacts During Reconstruction for High-Quality Segmentation", "comments": "Accepted for publication in IEEE TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting anatomical structures in medical images has been successfully\naddressed with deep learning methods for a range of applications. However, this\nsuccess is heavily dependent on the quality of the image that is being\nsegmented. A commonly neglected point in the medical image analysis community\nis the vast amount of clinical images that have severe image artefacts due to\norgan motion, movement of the patient and/or image acquisition related issues.\nIn this paper, we discuss the implications of image motion artefacts on cardiac\nMR segmentation and compare a variety of approaches for jointly correcting for\nartefacts and segmenting the cardiac cavity. The method is based on our\nrecently developed joint artefact detection and reconstruction method, which\nreconstructs high quality MR images from k-space using a joint loss function\nand essentially converts the artefact correction task to an under-sampled image\nreconstruction task by enforcing a data consistency term. In this paper, we\npropose to use a segmentation network coupled with this in an end-to-end\nframework. Our training optimises three different tasks: 1) image artefact\ndetection, 2) artefact correction and 3) image segmentation. We train the\nreconstruction network to automatically correct for motion-related artefacts\nusing synthetically corrupted cardiac MR k-space data and uncorrected\nreconstructed images. Using a test set of 500 2D+time cine MR acquisitions from\nthe UK Biobank data set, we achieve demonstrably good image quality and high\nsegmentation accuracy in the presence of synthetic motion artefacts. We\nshowcase better performance compared to various image correction architectures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:36:17 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 11:24:03 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 09:35:21 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2020 13:59:14 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Oksuz", "Ilkay", ""], ["Clough", "James R.", ""], ["Ruijsink", "Bram", ""], ["Anton", "Esther Puyol", ""], ["Bustin", "Aurelien", ""], ["Cruz", "Gastao", ""], ["Prieto", "Claudia", ""], ["King", "Andrew P.", ""], ["Schnabel", "Julia A.", ""]]}, {"id": "1910.05374", "submitter": "Luciano Oliveira", "authors": "Gustavo Neves, R\\^omulo Cerqueira, Jan Albiez, Luciano Oliveira", "title": "Rotation-invariant shipwreck recognition with forward-looking sonar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Under the sea, visible spectrum cameras have limited sensing capacity, being\nable to detect objects only in clear water, but in a constrained range.\nConsidering any sea water condition, sonars are more suitable to support\nautonomous underwater vehicles' navigation, even in turbid condition. Despite\nthat sonar suitability, this type of sensor does not provide high-density\ninformation, such as optical sensors, making the process of object recognition\nto be more complex. To deal with that problem, we propose a novel trainable\nmethod to detect and recognize (identify) specific target objects under the sea\nwith a forward-looking sonar. Our method has a preprocessing step in charge of\nstrongly reducing the sensor noise and seabed background. To represent the\nobject, our proposed method uses histogram of orientation gradient (HOG) as\nfeature extractor. HOG ultimately feed a multi-scale oriented detector combined\nwith a support vector machine to recognize specific trained objects in a\nrotation-invariant way. Performance assessment demonstrated promising results,\nfavoring the method to be applied in underwater remote sensing.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:50:18 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Neves", "Gustavo", ""], ["Cerqueira", "R\u00f4mulo", ""], ["Albiez", "Jan", ""], ["Oliveira", "Luciano", ""]]}, {"id": "1910.05375", "submitter": "Hyojin Kim", "authors": "Hyojin Kim, Rushil Anirudh, K. Aditya Mohan, Kyle Champley", "title": "Extreme Few-view CT Reconstruction using Deep Inference", "comments": "Deep Inverse NeurIPS 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of few-view x-ray Computed Tomography (CT) data is a highly\nill-posed problem. It is often used in applications that require low radiation\ndose in clinical CT, rapid industrial scanning, or fixed-gantry CT. Existing\nanalytic or iterative algorithms generally produce poorly reconstructed images,\nseverely deteriorated by artifacts and noise, especially when the number of\nx-ray projections is considerably low. This paper presents a deep\nnetwork-driven approach to address extreme few-view CT by incorporating\nconvolutional neural network-based inference into state-of-the-art iterative\nreconstruction. The proposed method interprets few-view sinogram data using\nattention-based deep networks to infer the reconstructed image. The predicted\nimage is then used as prior knowledge in the iterative algorithm for final\nreconstruction. We demonstrate effectiveness of the proposed approach by\nperforming reconstruction experiments on a chest CT dataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:55:44 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Kim", "Hyojin", ""], ["Anirudh", "Rushil", ""], ["Mohan", "K. Aditya", ""], ["Champley", "Kyle", ""]]}, {"id": "1910.05376", "submitter": "Dimitrios Kollias", "authors": "Alvertos Benroumpi and Dimitrios Kollias", "title": "AffWild Net and Aff-Wild Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions recognition is the task of recognizing people's emotions. Usually it\nis achieved by analyzing expression of peoples faces. There are two ways for\nrepresenting emotions: The categorical approach and the dimensional approach by\nusing valence and arousal values. Valence shows how negative or positive an\nemotion is and arousal shows how much it is activated. Recent deep learning\nmodels, that have to do with emotions recognition, are using the second\napproach, valence and arousal. Moreover, a more interesting concept, which is\nuseful in real life is the \"in the wild\" emotions recognition. \"In the wild\"\nmeans that the images analyzed for the recognition task, come from from real\nlife sources(online videos, online photos, etc.) and not from staged\nexperiments. So, they introduce unpredictable situations in the images, that\nhave to be modeled. The purpose of this project is to study the previous work\nthat was done for the \"in the wild\" emotions recognition concept, design a new\ndataset which has as a standard the \"Aff-wild\" database, implement new deep\nlearning models and evaluate the results. First, already existing databases and\ndeep learning models are presented. Then, inspired by them a new database is\ncreated which includes 507.208 frames in total from 106 videos, which were\ngathered from online sources. Then, the data are tested in a CNN model based on\nCNN-M architecture, in order to be sure about their usability. Next, the main\nmodel of this project is implemented. That is a Regression GAN which can\nexecute unsupervised and supervised learning at the same time. More\nspecifically, it keeps the main functionality of GANs, which is to produce fake\nimages that look as good as the real ones, while it can also predict valence\nand arousal values for both real and fake images. Finally, the database created\nearlier is applied to this model and the results are presented and evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:57:18 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 22:58:20 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Benroumpi", "Alvertos", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05395", "submitter": "Senthil Yogamani", "authors": "Hazem Rashed, Mohamed Ramzy, Victor Vaquero, Ahmad El Sallab, Ganesh\n  Sistu and Senthil Yogamani", "title": "FuseMODNet: Real-Time Camera and LiDAR based Moving Object Detection for\n  robust low-light Autonomous Driving", "comments": "Accepted for Oral presentation at ICCV 2019 Workshop on Autonomous\n  Driving. https://sites.google.com/view/fusemodnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving object detection is a critical task for autonomous vehicles. As\ndynamic objects represent higher collision risk than static ones, our own\nego-trajectories have to be planned attending to the future states of the\nmoving elements of the scene. Motion can be perceived using temporal\ninformation such as optical flow. Conventional optical flow computation is\nbased on camera sensors only, which makes it prone to failure in conditions\nwith low illumination. On the other hand, LiDAR sensors are independent of\nillumination, as they measure the time-of-flight of their own emitted lasers.\nIn this work, we propose a robust and real-time CNN architecture for Moving\nObject Detection (MOD) under low-light conditions by capturing motion\ninformation from both camera and LiDAR sensors. We demonstrate the impact of\nour algorithm on KITTI dataset where we simulate a low-light environment\ncreating a novel dataset \"Dark KITTI\". We obtain a 10.1% relative improvement\non Dark-KITTI, and a 4.25% improvement on standard KITTI relative to our\nbaselines. The proposed algorithm runs at 18 fps on a standard desktop GPU\nusing $256\\times1224$ resolution images.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 20:09:18 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 21:10:11 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 01:18:33 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Rashed", "Hazem", ""], ["Ramzy", "Mohamed", ""], ["Vaquero", "Victor", ""], ["Sallab", "Ahmad El", ""], ["Sistu", "Ganesh", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1910.05401", "submitter": "Leonardo De Laurentiis", "authors": "Leonardo De Laurentiis, Andrea Pomente, Fabio Del Frate, and Giovanni\n  Schiavon", "title": "Capsule and convolutional neural network-based SAR ship classification\n  in Sentinel-1 data", "comments": "Please check out the original SPIE paper for a complete list of\n  figures, tables, references and general content", "journal-ref": "SPIE Remote Sensing 2019: Proceedings Volume 11154, Active and\n  Passive Microwave Remote Sensing for Environmental Monitoring III; 1115405\n  (2019)", "doi": "10.1117/12.2532551", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic Aperture Radar (SAR) constitutes a fundamental asset for wide-areas\nmonitoring with high-resolution requirements. The first SAR sensors have given\nrise to coarse coastal and maritime monitoring applications, including oil\nspill, ship and ice floes detection. With the upgrade to very high-resolution\nsensors in the recent years, with relatively new SAR missions such as\nSentinel-1, a great deal of data providing a stronger information content has\nbeen released, enabling more refined studies on general targets features and\nthus permitting complex classifications, as for ship classification, which has\nbecome increasingly relevant given the growing need for coastal surveillance in\ncommercial and military segments. In the last decade, several works focused on\nthis topic have been presented, generally based on radiometric features\nprocessing; furthermore, in the very recent years a significant amount of\nresearch works have focused on emerging deep learning techniques, in particular\non Convolutional Neural Networks (CNN). Recently Capsule Neural Networks\n(CapsNets) have been presented, demonstrating a notable improvement in\ncapturing the properties of given entities, improving the use of spatial\ninformations, in particular of spatial dependence between features, a severely\nlacking feature in CNNs. In fact, CNNs pooling operations have been criticized\nfor losing spatial relations, thus special capsules, along with a new iterative\nrouting-by-agreement mechanism, have been proposed. In this work a comparison\nbetween Capsule and CNNs potential in the ship classification application\ndomain is shown, by leveraging the OpenSARShip, a SAR Sentinel-1 ship chips\ndataset; in particular, a performance comparison between capsule and various\nconvolutional architectures is built, demonstrating better performances of\nCapsNet in classifying ships within a small dataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 20:32:02 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["De Laurentiis", "Leonardo", ""], ["Pomente", "Andrea", ""], ["Del Frate", "Fabio", ""], ["Schiavon", "Giovanni", ""]]}, {"id": "1910.05411", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Jakob Grahn, Markus Eckerstorfer, Eirik Malnes,\n  Hannah Vickers", "title": "Snow avalanche segmentation in SAR images with Fully Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge about frequency and location of snow avalanche activity is\nessential for forecasting and mapping of snow avalanche hazard. Traditional\nfield monitoring of avalanche activity has limitations, especially when\nsurveying large and remote areas. In recent years, avalanche detection in\nSentinel-1 radar satellite imagery has been developed to improve monitoring.\nHowever, the current state-of-the-art detection algorithms, based on radar\nsignal processing techniques, are still much less accurate than human experts.\nTo reduce this gap, we propose a deep learning architecture for detecting\navalanches in Sentinel-1 radar images. We trained a neural network on 6,345\nmanually labelled avalanches from 117 Sentinel-1 images, each one consisting of\nsix channels that include backscatter and topographical information. Then, we\ntested our trained model on a new SAR image. Comparing to the manual labelling\n(the gold standard), we achieved an F1 score above 66\\%, while the\nstate-of-the-art detection algorithm sits at an F1 score of only 38\\%. A visual\ninspection of the results generated by our deep learning model shows that only\nsmall avalanches are undetected, while some avalanches that were originally not\nlabelled by the human expert are discovered.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 21:02:57 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 13:00:56 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Grahn", "Jakob", ""], ["Eckerstorfer", "Markus", ""], ["Malnes", "Eirik", ""], ["Vickers", "Hannah", ""]]}, {"id": "1910.05425", "submitter": "Mostafa Karimi", "authors": "Mostafa Karimi, Gopalkrishna Veni, Yen-Yun Yu", "title": "Illegible Text to Readable Text: An Image-to-Image Transformation using\n  Conditional Sliced Wasserstein Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text recognition from ancient handwritten record images is an\nimportant problem in the genealogy domain. However, critical challenges such as\nvarying noise conditions, vanishing texts, and variations in handwriting make\nthe recognition task difficult. We tackle this problem by developing a\nhandwritten-to-machine-print conditional Generative Adversarial network\n(HW2MP-GAN) model that formulates handwritten recognition as a\ntext-Image-to-text-Image translation problem where a given image, typically in\nan illegible form, is converted into another image, close to its machine-print\nform. The proposed model consists of three-components including a generator,\nand word-level and character-level discriminators. The model incorporates\nSliced Wasserstein distance (SWD) and U-Net architectures in HW2MP-GAN for\nbetter quality image-to-image transformation. Our experiments reveal that\nHW2MP-GAN outperforms state-of-the-art baseline cGAN models by almost 30 in\nFrechet Handwritten Distance (FHD), 0.6 on average Levenshtein distance and 39%\nin word accuracy for image-to-image translation on IAM database. Further,\nHW2MP-GAN improves handwritten recognition word accuracy by 1.3% compared to\nbaseline handwritten recognition models on the IAM database.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 22:01:24 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Karimi", "Mostafa", ""], ["Veni", "Gopalkrishna", ""], ["Yu", "Yen-Yun", ""]]}, {"id": "1910.05437", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Fakhri Karray, Mark Crowley", "title": "Roweis Discriminant Analysis: A Generalized Subspace Learning Method", "comments": "This is the paper for the methods Roweis Discriminant Analysis (RDA),\n  dual RDA, kernel RDA, and Roweisfaces. This is in memory of Sam Roweis (rest\n  in peace) to whom subspace and manifold learning owes significantly", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method which generalizes subspace learning based on\neigenvalue and generalized eigenvalue problems. This method, Roweis\nDiscriminant Analysis (RDA), is named after Sam Roweis to whom the field of\nsubspace learning owes significantly. RDA is a family of infinite number of\nalgorithms where Principal Component Analysis (PCA), Supervised PCA (SPCA), and\nFisher Discriminant Analysis (FDA) are special cases. One of the extreme\nspecial cases, which we name Double Supervised Discriminant Analysis (DSDA),\nuses the labels twice; it is novel and has not appeared elsewhere. We propose a\ndual for RDA for some special cases. We also propose kernel RDA, generalizing\nkernel PCA, kernel SPCA, and kernel FDA, using both dual RDA and representation\ntheory. Our theoretical analysis explains previously known facts such as why\nSPCA can use regression but FDA cannot, why PCA and SPCA have duals but FDA\ndoes not, why kernel PCA and kernel SPCA use kernel trick but kernel FDA does\nnot, and why PCA is the best linear method for reconstruction. Roweisfaces and\nkernel Roweisfaces are also proposed generalizing eigenfaces, Fisherfaces,\nsupervised eigenfaces, and their kernel variants. We also report experiments\nshowing the effectiveness of RDA and kernel RDA on some benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 23:14:09 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "1910.05445", "submitter": "Muzammil Behzad", "authors": "Muzammil Behzad, Nhat Vo, Xiaobai Li and Guoying Zhao", "title": "Landmarks-assisted Collaborative Deep Framework for Automatic 4D Facial\n  Expression Recognition", "comments": "Published in 15th IEEE International Conference on Automatic Face and\n  Gesture Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel landmarks-assisted collaborative end-to-end deep framework\nfor automatic 4D FER. Using 4D face scan data, we calculate its various\ngeometrical images, and afterwards use rank pooling to generate their dynamic\nimages encapsulating important facial muscle movements over time. As well, the\ngiven 3D landmarks are projected on a 2D plane as binary images and\nconvolutional layers are used to extract sequences of feature vectors for every\nlandmark video. During the training stage, the dynamic images are used to train\nan end-to-end deep network, while the feature vectors of landmark images are\nused train a long short-term memory (LSTM) network. The finally improved set of\nexpression predictions are obtained when the dynamic and landmark images\ncollaborate over multi-views using the proposed deep framework. Performance\nresults obtained from extensive experimentation on the widely-adopted BU-4DFE\ndatabase under globally used settings prove that our proposed collaborative\nframework outperforms the state-of-the-art 4D FER methods and reach a promising\nclassification accuracy of 96.7% demonstrating its effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 23:50:57 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 09:34:25 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Behzad", "Muzammil", ""], ["Vo", "Nhat", ""], ["Li", "Xiaobai", ""], ["Zhao", "Guoying", ""]]}, {"id": "1910.05448", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, David Ahmedt-Aristizabal, Sridha\n  Sridharan, Kristin Laurens, Patrick Johnston, and Clinton Fookes", "title": "Neural Memory Plasticity for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of machine learning, Neural Memory Networks (NMNs) have\nrecently achieved impressive results in a variety of application areas\nincluding visual question answering, trajectory prediction, object tracking,\nand language modelling. However, we observe that the attention based knowledge\nretrieval mechanisms used in current NMNs restricts them from achieving their\nfull potential as the attention process retrieves information based on a set of\nstatic connection weights. This is suboptimal in a setting where there are vast\ndifferences among samples in the data domain; such as anomaly detection where\nthere is no consistent criteria for what constitutes an anomaly. In this paper,\nwe propose a plastic neural memory access mechanism which exploits both static\nand dynamic connection weights in the memory read, write and output generation\nprocedures. We demonstrate the effectiveness and flexibility of the proposed\nmemory model in three challenging anomaly detection tasks in the medical\ndomain: abnormal EEG identification, MRI tumour type classification and\nschizophrenia risk detection in children. In all settings, the proposed\napproach outperforms the current state-of-the-art. Furthermore, we perform an\nin-depth analysis demonstrating the utility of neural plasticity for the\nknowledge retrieval process and provide evidence on how the proposed memory\nmodel generates sparse yet informative memory outputs.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 00:32:56 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Ahmedt-Aristizabal", "David", ""], ["Sridharan", "Sridha", ""], ["Laurens", "Kristin", ""], ["Johnston", "Patrick", ""], ["Fookes", "Clinton", ""]]}, {"id": "1910.05449", "submitter": "Yuning Chai", "authors": "Yuning Chai and Benjamin Sapp and Mayank Bansal and Dragomir Anguelov", "title": "MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for\n  Behavior Prediction", "comments": "Appears in CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting human behavior is a difficult and crucial task required for motion\nplanning. It is challenging in large part due to the highly uncertain and\nmulti-modal set of possible outcomes in real-world domains such as autonomous\ndriving. Beyond single MAP trajectory prediction, obtaining an accurate\nprobability distribution of the future is an area of active interest. We\npresent MultiPath, which leverages a fixed set of future state-sequence anchors\nthat correspond to modes of the trajectory distribution. At inference, our\nmodel predicts a discrete distribution over the anchors and, for each anchor,\nregresses offsets from anchor waypoints along with uncertainties, yielding a\nGaussian mixture at each time step. Our model is efficient, requiring only one\nforward inference pass to obtain multi-modal future distributions, and the\noutput is parametric, allowing compact communication and analytical\nprobabilistic queries. We show on several datasets that our model achieves more\naccurate predictions, and compared to sampling baselines, does so with an order\nof magnitude fewer trajectories.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 00:34:37 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Chai", "Yuning", ""], ["Sapp", "Benjamin", ""], ["Bansal", "Mayank", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "1910.05455", "submitter": "Kritaphat Songsri-in", "authors": "Kritaphat Songsri-in and Stefanos Zafeiriou", "title": "Complement Face Forensic Detection and Localization with FacialLandmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Generative Adversarial Networks (GANs) and image manipulating\nmethods are becoming more powerful and can produce highly realistic face images\nbeyond human recognition which have raised significant concerns regarding the\nauthenticity of digital media. Although there have been some prior works that\ntackle face forensic classification problem, it is not trivial to estimate\nedited locations from classification predictions. In this paper, we propose, to\nthe best of our knowledge, the first rigorous face forensic localization\ndataset, which consists of genuine, generated, and manipulated face images. In\nparticular, the pristine parts contain face images from CelebA and FFHQ\ndatasets. The fake images are generated from various GANs methods, namely\nDCGANs, LSGANs, BEGANs, WGAN-GP, ProGANs, and StyleGANs. Lastly, the edited\nsubset is generated from StarGAN and SEFCGAN based on free-form masks. In\ntotal, the dataset contains about 1.3 million facial images labelled with\ncorresponding binary masks.\n  Based on the proposed dataset, we demonstrated that explicit adding facial\nlandmarks information in addition to input images improves the performance. In\naddition, our proposed method consists of two branches and can coherently\npredict face forensic detection and localization to outperform the previous\nstate-of-the-art techniques on the newly proposed dataset as well as the\nfaceforecsic++ dataset especially on low-quality videos.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 01:09:15 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Songsri-in", "Kritaphat", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1910.05457", "submitter": "Xin Li", "authors": "Shan Jia and Xin Li and Chuanbo Hu and Zhengquan Xu", "title": "Spoofing and Anti-Spoofing with Wax Figure Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have witnessed rapid advances in both face presentation attack models and\npresentation attack detection (PAD) in recent years. Compared to widely studied\n2D face presentation attacks (e.g. printed photos and video replays), 3D face\npresentation attacks are more challenging because face recognition systems\n(FRS) is more easily confused by the 3D characteristics of materials similar to\nreal faces. Existing 3D face spoofing databases, mostly based on 3D facial\nmasks, are restricted to small data size and suffer from poor authenticity due\nto the difficulty and expense of mask production. In this work, we introduce a\nwax figure face database (WFFD) as a novel and super-realistic 3D face\npresentation attack. This database contains 2300 image pairs (totally 4600) and\n745 subjects including both real and wax figure faces with high diversity from\nonline collections. On one hand, our experiments have demonstrated the spoofing\npotential of WFFD on three popular FRSs. On the other hand, we have developed a\nmulti-feature voting scheme for wax figure face detection (anti-spoofing),\nwhich combines three discriminative features at the decision level. The\nproposed detection method was compared against several face PAD approaches and\nfound to outperform other competing methods. Surprisingly, our fusion-based\ndetection method achieves an Average Classification Error Rate (ACER) of\n11.73\\% on the WFFD database, which is even better than human-based detection.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 01:13:06 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Jia", "Shan", ""], ["Li", "Xin", ""], ["Hu", "Chuanbo", ""], ["Xu", "Zhengquan", ""]]}, {"id": "1910.05475", "submitter": "Qi Yao", "authors": "Qi Yao, Xiaojin Gong", "title": "Saliency Guided Self-attention Network for Weakly and Semi-supervised\n  Semantic Segmentation", "comments": "10 pages, 5 figures. Accepted by IEEE ACCESS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised semantic segmentation (WSSS) using only image-level labels\ncan greatly reduce the annotation cost and therefore has attracted considerable\nresearch interest. However, its performance is still inferior to the fully\nsupervised counterparts. To mitigate the performance gap, we propose a saliency\nguided self-attention network (SGAN) to address the WSSS problem. The\nintroduced self-attention mechanism is able to capture rich and extensive\ncontextual information but may mis-spread attentions to unexpected regions. In\norder to enable this mechanism to work effectively under weak supervision, we\nintegrate class-agnostic saliency priors into the self-attention mechanism and\nutilize class-specific attention cues as an additional supervision for SGAN.\nOur SGAN is able to produce dense and accurate localization cues so that the\nsegmentation performance is boosted. Moreover, by simply replacing the\nadditional supervisions with partially labeled ground-truth, SGAN works\neffectively for semi-supervised semantic segmentation as well. Experiments on\nthe PASCAL VOC 2012 and COCO datasets show that our approach outperforms all\nother state-of-the-art methods in both weakly and semi-supervised settings.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 03:17:44 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 08:24:38 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Yao", "Qi", ""], ["Gong", "Xiaojin", ""]]}, {"id": "1910.05483", "submitter": "Xiaoke Shen", "authors": "Xiaoke Shen and Ioannis Stamos", "title": "Frustum VoxNet for 3D object detection from RGB-D or Depth images", "comments": "page 8, add Acknowledgement. page 10, add Supplementary Material. The\n  paper got accepted by 2020 Winter Conference on Applications of Computer\n  Vision (WACV '20). The first arxiv version can be found here:\n  arXiv:1910.05483", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been a plethora of classification and detection systems\nfrom RGB as well as 3D images. In this work, we describe a new 3D object\ndetection system from an RGB-D or depth-only point cloud. Our system first\ndetects objects in 2D (either RGB or pseudo-RGB constructed from depth). The\nnext step is to detect 3D objects within the 3D frustums these 2D detections\ndefine. This is achieved by voxelizing parts of the frustums (since frustums\ncan be really large), instead of using the whole frustums as done in earlier\nwork. The main novelty of our system has to do with determining which parts (3D\nproposals) of the frustums to voxelize, thus allowing us to provide high\nresolution representations around the objects of interest. It also allows our\nsystem to have reduced memory requirements. These 3D proposals are fed to an\nefficient ResNet-based 3D Fully Convolutional Network (FCN). Our 3D detection\nsystem is fast and can be integrated into a robotics platform. With respect to\nsystems that do not perform voxelization (such as PointNet), our methods can\noperate without the requirement of subsampling of the datasets. We have also\nintroduced a pipelining approach that further improves the efficiency of our\nsystem. Results on SUN RGB-D dataset show that our system, which is based on a\nsmall network, can process 20 frames per second with comparable detection\nresults to the state-of-the-art, achieving a 2 times speedup.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 04:06:46 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 23:59:10 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Shen", "Xiaoke", ""], ["Stamos", "Ioannis", ""]]}, {"id": "1910.05518", "submitter": "Seunghan Yang", "authors": "Seunghan Yang, Yoonhyung Kim, Youngeun Kim, and Changick Kim", "title": "Combinational Class Activation Maps for Weakly Supervised Object\n  Localization", "comments": "The paper was accepted to the IEEE Winter Conference on Applications\n  of Computer Vision (WACV'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object localization has recently attracted attention since\nit aims to identify both class labels and locations of objects by using\nimage-level labels. Most previous methods utilize the activation map\ncorresponding to the highest activation source. Exploiting only one activation\nmap of the highest probability class is often biased into limited regions or\nsometimes even highlights background regions. To resolve these limitations, we\npropose to use activation maps, named combinational class activation maps\n(CCAM), which are linear combinations of activation maps from the highest to\nthe lowest probability class. By using CCAM for localization, we suppress\nbackground regions to help highlighting foreground objects more accurately. In\naddition, we design the network architecture to consider spatial relationships\nfor localizing relevant object regions. Specifically, we integrate non-local\nmodules into an existing base network at both low- and high-level layers. Our\nfinal model, named non-local combinational class activation maps (NL-CCAM),\nobtains superior performance compared to previous methods on representative\nobject localization benchmarks including ILSVRC 2016 and CUB-200-2011.\nFurthermore, we show that the proposed method has a great capability of\ngeneralization by visualizing other datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 07:29:59 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 07:53:46 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Yang", "Seunghan", ""], ["Kim", "Yoonhyung", ""], ["Kim", "Youngeun", ""], ["Kim", "Changick", ""]]}, {"id": "1910.05545", "submitter": "Yao Xiao", "authors": "Yao Xiao, Dan Meng, Cewu Lu, Chi-Keung Tang", "title": "Template-Instance Loss for Offline Handwritten Chinese Character\n  Recognition", "comments": "Accepted by ICDAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long-standing challenges for offline handwritten Chinese character\nrecognition (HCCR) are twofold: Chinese characters can be very diverse and\ncomplicated while similarly looking, and cursive handwriting (due to increased\nwriting speed and infrequent pen lifting) makes strokes and even characters\nconnected together in a flowing manner. In this paper, we propose the template\nand instance loss functions for the relevant machine learning tasks in offline\nhandwritten Chinese character recognition. First, the character template is\ndesigned to deal with the intrinsic similarities among Chinese characters.\nSecond, the instance loss can reduce category variance according to\nclassification difficulty, giving a large penalty to the outlier instance of\nhandwritten Chinese character. Trained with the new loss functions using our\ndeep network architecture HCCR14Layer model consisting of simple layers, our\nextensive experiments show that it yields state-of-the-art performance and\nbeyond for offline HCCR.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 09:59:17 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Xiao", "Yao", ""], ["Meng", "Dan", ""], ["Lu", "Cewu", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1910.05549", "submitter": "Jingjing Qian", "authors": "Jingjing Qian, Wei Jiang, Hao Luo, Hongyan Yu", "title": "Stripe-based and Attribute-aware Network: A Two-Branch Deep Model for\n  Vehicle Re-identification", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6501/ab8b81", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (Re-ID) has been attracting increasing interest in\nthe field of computer vision due to the growing utilization of surveillance\ncameras in public security. However, vehicle Re-ID still suffers a similarity\nchallenge despite the efforts made to solve this problem. This challenge\ninvolves distinguishing different instances with nearly identical appearances.\nIn this paper, we propose a novel two-branch stripe-based and attribute-aware\ndeep convolutional neural network (SAN) to learn the efficient feature\nembedding for vehicle Re-ID task. The two-branch neural network, consisting of\nstripe-based branch and attribute-aware branches, can adaptively extract the\ndiscriminative features from the visual appearance of vehicles. A horizontal\naverage pooling and dimension-reduced convolutional layers are inserted into\nthe stripe-based branch to achieve part-level features. Meanwhile, the\nattribute-aware branch extracts the global feature under the supervision of\nvehicle attribute labels to separate the similar vehicle identities with\ndifferent attribute annotations. Finally, the part-level and global features\nare concatenated together to form the final descriptor of the input image for\nvehicle Re-ID. The final descriptor not only can separate vehicles with\ndifferent attributes but also distinguish vehicle identities with the same\nattributes. The extensive experiments on both VehicleID and VeRi databases show\nthat the proposed SAN method outperforms other state-of-the-art vehicle Re-ID\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 10:51:35 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Qian", "Jingjing", ""], ["Jiang", "Wei", ""], ["Luo", "Hao", ""], ["Yu", "Hongyan", ""]]}, {"id": "1910.05562", "submitter": "Dongwan Kim", "authors": "Seungmin Lee, Dongwan Kim, Namil Kim, Seong-Gyun Jeong", "title": "Drop to Adapt: Learning Discriminative Features for Unsupervised Domain\n  Adaptation", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent works on domain adaptation exploit adversarial training to obtain\ndomain-invariant feature representations from the joint learning of feature\nextractor and domain discriminator networks. However, domain adversarial\nmethods render suboptimal performances since they attempt to match the\ndistributions among the domains without considering the task at hand. We\npropose Drop to Adapt (DTA), which leverages adversarial dropout to learn\nstrongly discriminative features by enforcing the cluster assumption.\nAccordingly, we design objective functions to support robust domain adaptation.\nWe demonstrate efficacy of the proposed method on various experiments and\nachieve consistent improvements in both image classification and semantic\nsegmentation tasks. Our source code is available at\nhttps://github.com/postBG/DTA.pytorch.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 13:21:25 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Lee", "Seungmin", ""], ["Kim", "Dongwan", ""], ["Kim", "Namil", ""], ["Jeong", "Seong-Gyun", ""]]}, {"id": "1910.05577", "submitter": "Xudong Lin", "authors": "Xudong Lin, Lin Ma, Wei Liu, Shih-Fu Chang", "title": "Context-Gated Convolution", "comments": "ECCV 2020 camera ready version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the basic building block of Convolutional Neural Networks (CNNs), the\nconvolutional layer is designed to extract local patterns and lacks the ability\nto model global context in its nature. Many efforts have been recently devoted\nto complementing CNNs with the global modeling ability, especially by a family\nof works on global feature interaction. In these works, the global context\ninformation is incorporated into local features before they are fed into\nconvolutional layers. However, research on neuroscience reveals that the\nneurons' ability of modifying their functions dynamically according to context\nis essential for the perceptual tasks, which has been overlooked in most of\nCNNs. Motivated by this, we propose one novel Context-Gated Convolution (CGC)\nto explicitly modify the weights of convolutional layers adaptively under the\nguidance of global context. As such, being aware of the global context, the\nmodulated convolution kernel of our proposed CGC can better extract\nrepresentative local patterns and compose discriminative features. Moreover,\nour proposed CGC is lightweight and applicable with modern CNN architectures,\nand consistently improves the performance of CNNs according to extensive\nexperiments on image classification, action recognition, and machine\ntranslation. Our code of this paper is available at\nhttps://github.com/XudongLinthu/context-gated-convolution.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 15:30:18 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 03:08:24 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 19:19:28 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 16:59:19 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Lin", "Xudong", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1910.05594", "submitter": "Ayman Wagdy", "authors": "Ayman Wagdy, Veronica Garcia-Hansen, Mohammed Elhenawy, Gillian\n  Isoardi, Robin Drogemuller, Fatma Fathy", "title": "Open-plan Glare Evaluator (OGE): A Demonstration of a New Glare\n  Prediction Approach Using Machine Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Predicting discomfort glare in open-plan offices is a challenging problem.\nAlthough glare research has existed for more than 50 years, all current glare\nmetrics have accuracy limitations, especially in open-plan offices with low\nlighting levels. Thus, it is crucial to develop a new method to predict glare\nmore accurately. This paper is the first to adopt Machine Learning (ML)\napproaches in the prediction of glare. This research aims to demonstrate the\nvalidity of this approach by comparing the accuracy of the new ML model for\nopen-plan offices (OGE) to the accuracy of the existing glare metrics using\nlocal dataset. To utilize and test this approach, Post-Occupancy Evaluation\n(POE) and High Dynamic Range (HDR) images were collected from 80 occupants\n(n=80) in four different open-plan offices in Brisbane, Australia.\nConsequently, various multi-region luminance values, luminance, and glare\nindices were calculated and examined as input features to train ML models. The\naccuracy of the ML model was compared to the accuracy of 24 indices which were\nalso evaluated using a Receiver Operating Characteristic (ROC) analysis to\nidentify the best cutoff values (thresholds) for each index in open-plan\nconfigurations. Results showed that the ML approach could predict glare with an\naccuracy of 83.8% (0.80 true positive rate and 0.86 true negative rate), which\noutperformed the accuracy of the previously developed glare metrics. OGE is\napplicable for open-plan office situations with low vertical illuminance (200\nto 600 lux). However, ML models can be trained with more substantial datasets\nto achieve global model.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 16:33:02 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 14:13:22 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Wagdy", "Ayman", ""], ["Garcia-Hansen", "Veronica", ""], ["Elhenawy", "Mohammed", ""], ["Isoardi", "Gillian", ""], ["Drogemuller", "Robin", ""], ["Fathy", "Fatma", ""]]}, {"id": "1910.05595", "submitter": "Kamran Ali", "authors": "Kamran Ali, Ilkin Isler, Charles Hughes", "title": "Facial Expression Recognition Using Human to Animated-Character\n  Expression Translation", "comments": "8 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition is a challenging task due to two major\nproblems: the presence of inter-subject variations in facial expression\nrecognition dataset and impure expressions posed by human subjects. In this\npaper we present a novel Human-to-Animation conditional Generative Adversarial\nNetwork (HA-GAN) to overcome these two problems by using many (human faces) to\none (animated face) mapping. Specifically, for any given input human expression\nimage, our HA-GAN transfers the expression information from the input image to\na fixed animated identity. Stylized animated characters from the Facial\nExpression Research Group-Database (FERGDB) are used for the generation of\nfixed identity. By learning this many-to-one identity mapping function using\nour proposed HA-GAN, the effect of inter-subject variations can be reduced in\nFacial Expression Recognition(FER). We also argue that the expressions in the\ngenerated animated images are pure expressions and since FER is performed on\nthese generated images, the performance of facial expression recognition is\nimproved. Our initial experimental results on the state-of-the-art datasets\nshow that facial expression recognition carried out on the generated animated\nimages using our HA-GAN framework outperforms the baseline deep neural network\nand produces comparable or even better results than the state-of-the-art\nmethods for facial expression recognition.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 16:36:54 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Ali", "Kamran", ""], ["Isler", "Ilkin", ""], ["Hughes", "Charles", ""]]}, {"id": "1910.05597", "submitter": "Karim Armanious", "authors": "Karim Armanious, Aastha Tanwar, Sherif Abdulatif, Thomas K\\\"ustner,\n  Sergios Gatidis, Bin Yang", "title": "Unsupervised Adversarial Correction of Rigid MR Motion Artifacts", "comments": "Submitted to IEEE ISBI 2020", "journal-ref": null, "doi": "10.1109/ISBI45749.2020.9098570", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion is one of the main sources for artifacts in magnetic resonance (MR)\nimages. It can have significant consequences on the diagnostic quality of the\nresultant scans. Previously, supervised adversarial approaches have been\nsuggested for the correction of MR motion artifacts. However, these approaches\nsuffer from the limitation of required paired co-registered datasets for\ntraining which are often hard or impossible to acquire. Building upon our\nprevious work, we introduce a new adversarial framework with a new generator\narchitecture and loss function for the unsupervised correction of severe rigid\nmotion artifacts in the brain region. Quantitative and qualitative comparisons\nwith other supervised and unsupervised translation approaches showcase the\nenhanced performance of the introduced framework.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 16:51:08 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Armanious", "Karim", ""], ["Tanwar", "Aastha", ""], ["Abdulatif", "Sherif", ""], ["K\u00fcstner", "Thomas", ""], ["Gatidis", "Sergios", ""], ["Yang", "Bin", ""]]}, {"id": "1910.05602", "submitter": "Gurudutt Perichetla", "authors": "Akash Saravanan, Gurudutt Perichetla, Dr. K. S. Gayathri", "title": "Facial Emotion Recognition using Convolutional Neural Networks", "comments": "AICV '18: International Symposium on Artificial Intelligence and\n  Computer Vision. College of Engineering, Guindy. Chennai, India (September\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition is a topic of great interest in most fields\nfrom artificial intelligence and gaming to marketing and healthcare. The goal\nof this paper is to classify images of human faces into one of seven basic\nemotions. A number of different models were experimented with, including\ndecision trees and neural networks before arriving at a final Convolutional\nNeural Network (CNN) model. CNNs work better for image recognition tasks since\nthey are able to capture spacial features of the inputs due to their large\nnumber of filters. The proposed model consists of six convolutional layers, two\nmax pooling layers and two fully connected layers. Upon tuning of the various\nhyperparameters, this model achieved a final accuracy of 0.60.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 17:31:19 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Saravanan", "Akash", ""], ["Perichetla", "Gurudutt", ""], ["Gayathri", "Dr. K. S.", ""]]}, {"id": "1910.05611", "submitter": "Yijie Xu", "authors": "Yijie Xu and Arushi Goel", "title": "Cross-Domain Image Classification through Neural-Style Transfer Data\n  Augmentation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In particular, the lack of sufficient amounts of domain-specific data can\nreduce the accuracy of a classifier. In this paper, we explore the effects of\nstyle transfer-based data transformation on the accuracy of a convolutional\nneural network classifiers in the context of automobile detection under adverse\nwinter weather conditions. The detection of automobiles under highly adverse\nweather conditions is a difficult task as such conditions present large amounts\nof noise in each image. The InceptionV2 architecture is trained on a composite\ndataset, consisting of either normal car image dataset , a mixture of normal\nand style transferred car images, or a mixture of normal car images and those\ntaken at blizzard conditions, at a ratio of 80:20. All three classifiers are\nthen tested on a dataset of car images taken at blizzard conditions and on\nvehicle-free snow landscape images. We evaluate and contrast the effectiveness\nof each classifier upon each dataset, and discuss the strengths and weaknesses\nof style-transfer based approaches to data augmentation.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 18:00:33 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Xu", "Yijie", ""], ["Goel", "Arushi", ""]]}, {"id": "1910.05613", "submitter": "Tomohiro Maeda", "authors": "Tomohiro Maeda, Guy Satat, Tristan Swedish, Lagnojita Sinha, Ramesh\n  Raskar", "title": "Recent Advances in Imaging Around Corners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seeing around corners, also known as non-line-of-sight (NLOS) imaging is a\ncomputational method to resolve or recover objects hidden around corners.\nRecent advances in imaging around corners have gained significant interest.\nThis paper reviews different types of existing NLOS imaging techniques and\ndiscusses the challenges that need to be addressed, especially for their\napplications outside of a constrained laboratory environment. Our goal is to\nintroduce this topic to broader research communities as well as provide\ninsights that would lead to further developments in this research area.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 18:01:52 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Maeda", "Tomohiro", ""], ["Satat", "Guy", ""], ["Swedish", "Tristan", ""], ["Sinha", "Lagnojita", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1910.05657", "submitter": "Prithviraj Dhar", "authors": "Prithviraj Dhar, Ankan Bansal, Carlos D. Castillo, Joshua Gleason, P.\n  Jonathon Phillips and Rama Chellappa", "title": "How are attributes expressed in face DCNNs?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As deep networks become increasingly accurate at recognizing faces, it is\nvital to understand how these networks process faces. While these networks are\nsolely trained to recognize identities, they also contain face related\ninformation such as sex, age, and pose of the face. The networks are not\ntrained to learn these attributes. We introduce expressivity as a measure of\nhow much a feature vector informs us about an attribute, where a feature vector\ncan be from internal or final layers of a network. Expressivity is computed by\na second neural network whose inputs are features and attributes. The output of\nthe second neural network approximates the mutual information between feature\nvectors and an attribute. We investigate the expressivity for two different\ndeep convolutional neural network (DCNN) architectures: a Resnet-101 and an\nInception Resnet v2. In the final fully connected layer of the networks, we\nfound the order of expressivity for facial attributes to be Age > Sex > Yaw.\nAdditionally, we studied the changes in the encoding of facial attributes over\ntraining iterations. We found that as training progresses, expressivities of\nyaw, sex, and age decrease. Our technique can be a tool for investigating the\nsources of bias in a network and a step towards explaining the network's\nidentity decisions.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 22:59:48 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Dhar", "Prithviraj", ""], ["Bansal", "Ankan", ""], ["Castillo", "Carlos D.", ""], ["Gleason", "Joshua", ""], ["Phillips", "P. Jonathon", ""], ["Chellappa", "Rama", ""]]}, {"id": "1910.05672", "submitter": "Sharif Amit Kamran", "authors": "Sharif Amit Kamran, Sourajit Saha, Ali Shihab Sabbir, Alireza\n  Tavakkoli", "title": "Optic-Net: A Novel Convolutional Neural Network for Diagnosis of Retinal\n  Diseases from Optical Tomography Images", "comments": "8 pages. Accepted to 18th IEEE International Conference on Machine\n  Learning and Applications (ICMLA 2019)", "journal-ref": null, "doi": "10.1109/ICMLA.2019.00165", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosing different retinal diseases from Spectral Domain Optical Coherence\nTomography (SD-OCT) images is a challenging task. Different automated\napproaches such as image processing, machine learning and deep learning\nalgorithms have been used for early detection and diagnosis of retinal\ndiseases. Unfortunately, these are prone to error and computational\ninefficiency, which requires further intervention from human experts. In this\npaper, we propose a novel convolution neural network architecture to\nsuccessfully distinguish between different degeneration of retinal layers and\ntheir underlying causes. The proposed novel architecture outperforms other\nclassification models while addressing the issue of gradient explosion. Our\napproach reaches near perfect accuracy of 99.8% and 100% for two separately\navailable Retinal SD-OCT data-set respectively. Additionally, our architecture\npredicts retinal diseases in real time while outperforming human\ndiagnosticians.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 03:02:35 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kamran", "Sharif Amit", ""], ["Saha", "Sourajit", ""], ["Sabbir", "Ali Shihab", ""], ["Tavakkoli", "Alireza", ""]]}, {"id": "1910.05678", "submitter": "Carlos Paniagua", "authors": "Carlos M. Paniagua Mejia", "title": "An Image Segmentation Model Based on a Variational Formulation", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from a variational formulation, we present a model for image\nsegmentation that employs both region statistics and edge information. This\ncombination allows for improved flexibility, making the proposed model suitable\nto process a wider class of images than purely region-based and edge-based\nmodels. We perform several simulations with real images that attest to the\nversatility of the model. We also show another set of experiments on images\nwith certain pathologies that suggest opportunities for improvement.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 03:45:33 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Mejia", "Carlos M. Paniagua", ""]]}, {"id": "1910.05680", "submitter": "Chao-Tsung Huang", "authors": "Chao-Tsung Huang, Yu-Chun Ding, Huan-Ching Wang, Chi-Wen Weng,\n  Kai-Ping Lin, Li-Wei Wang, Li-De Chen", "title": "eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge\n  Inference", "comments": "14 pages; appearing in IEEE/ACM International Symposium on\n  Microarchitecture (MICRO), 2019", "journal-ref": null, "doi": "10.1145/3352460.3358263", "report-no": null, "categories": "cs.DC cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have recently demonstrated superior\nquality for computational imaging applications. Therefore, they have great\npotential to revolutionize the image pipelines on cameras and displays.\nHowever, it is difficult for conventional CNN accelerators to support\nultra-high-resolution videos at the edge due to their considerable DRAM\nbandwidth and power consumption. Therefore, finding a further memory- and\ncomputation-efficient microarchitecture is crucial to speed up this coming\nrevolution.\n  In this paper, we approach this goal by considering the inference flow,\nnetwork model, instruction set, and processor design jointly to optimize\nhardware performance and image quality. We apply a block-based inference flow\nwhich can eliminate all the DRAM bandwidth for feature maps and accordingly\npropose a hardware-oriented network model, ERNet, to optimize image quality\nbased on hardware constraints. Then we devise a coarse-grained instruction set\narchitecture, FBISA, to support power-hungry convolution by massive\nparallelism. Finally,we implement an embedded processor---eCNN---which\naccommodates to ERNet and FBISA with a flexible processing architecture. Layout\nresults show that it can support high-quality ERNets for super-resolution and\ndenoising at up to 4K Ultra-HD 30 fps while using only DDR-400 and consuming\n6.94W on average. By comparison, the state-of-the-art Diffy uses dual-channel\nDDR3-2133 and consumes 54.3W to support lower-quality VDSR at Full HD 30 fps.\nLastly, we will also present application examples of high-performance style\ntransfer and object recognition to demonstrate the flexibility of eCNN.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 03:54:25 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Huang", "Chao-Tsung", ""], ["Ding", "Yu-Chun", ""], ["Wang", "Huan-Ching", ""], ["Weng", "Chi-Wen", ""], ["Lin", "Kai-Ping", ""], ["Wang", "Li-Wei", ""], ["Chen", "Li-De", ""]]}, {"id": "1910.05693", "submitter": "Christoph Haarburger", "authors": "Christoph Haarburger, Justus Schock, Daniel Truhn, Philippe Weitz,\n  Gustav Mueller-Franzes, Leon Weninger, Dorit Merhof", "title": "Radiomic Feature Stability Analysis based on Probabilistic Segmentations", "comments": "accepted at ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying image features that are robust with respect to segmentation\nvariability and domain shift is a tough challenge in radiomics. So far, this\nproblem has mainly been tackled in test-retest analyses. In this work we\nanalyze radiomics feature stability based on probabilistic segmentations. Based\non a public lung cancer dataset, we generate an arbitrary number of plausible\nsegmentations using a Probabilistic U-Net. From these segmentations, we extract\na high number of plausible feature vectors for each lung tumor and analyze\nfeature variance with respect to the segmentations. Our results suggest that\nthere are groups of radiomic features that are more (e.g. statistics features)\nand less (e.g. gray-level size zone matrix features) robust against\nsegmentation variability. Finally, we demonstrate that segmentation variance\nimpacts the performance of a prognostic lung cancer survival model and propose\na new and potentially more robust radiomics feature selection workflow.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 06:01:18 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 14:39:11 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Haarburger", "Christoph", ""], ["Schock", "Justus", ""], ["Truhn", "Daniel", ""], ["Weitz", "Philippe", ""], ["Mueller-Franzes", "Gustav", ""], ["Weninger", "Leon", ""], ["Merhof", "Dorit", ""]]}, {"id": "1910.05700", "submitter": "Devraj Mandal", "authors": "Devraj Mandal, Shrisha Bharadwaj, Soma Biswas", "title": "A Novel Self-Supervised Re-labeling Approach for Training with Noisy\n  Labels", "comments": "Expanded and Updated version of this paper has been accepted in WACV\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The major driving force behind the immense success of deep learning models is\nthe availability of large datasets along with their clean labels.\nUnfortunately, this is very difficult to obtain, which has motivated research\non the training of deep models in the presence of label noise and ways to avoid\nover-fitting on the noisy labels. In this work, we build upon the seminal work\nin this area, Co-teaching and propose a simple, yet efficient approach termed\nmCT-S2R (modified co-teaching with self-supervision and re-labeling) for this\ntask. First, to deal with significant amount of noise in the labels, we propose\nto use self-supervision to generate robust features without using any labels.\nNext, using a parallel network architecture, an estimate of the clean labeled\nportion of the data is obtained. Finally, using this data, a portion of the\nestimated noisy labeled portion is re-labeled, before resuming the network\ntraining with the augmented data. Extensive experiments on three standard\ndatasets show the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 06:50:36 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 15:29:39 GMT"}, {"version": "v3", "created": "Wed, 1 Jan 2020 07:36:42 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Mandal", "Devraj", ""], ["Bharadwaj", "Shrisha", ""], ["Biswas", "Soma", ""]]}, {"id": "1910.05704", "submitter": "Zhenzhou Wang", "authors": "Zhenzhou Wang", "title": "Contour Sparse Representation with SDD Features for Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slope difference distribution (SDD) is computed for the one-dimensional\ncurve. It is not only robust to calculate the partitioning point to separate\nthe curve logically, but also robust to calculate the clustering center of each\npart of the separated curve. SDD has been proposed for image segmentation and\nit outperforms all existing image segmentation methods. For verification\npurpose, we have made the Matlab codes of comparing SDD method with existing\nimage segmentation methods freely available at Matlab Central. The contour of\nthe object is similar to the histogram of the image. Thus, feature detection by\nSDD from the contour of the object is also feasible. In this letter, SDD\nfeatures are defined and they form the sparse representation of the object\ncontour. The reference model of each object is built based on the SDD features\nand then model matching is used for on line object recognition. The\nexperimental results are very encouraging. For the gesture recognition, SDD\nachieved 100% accuracy for two public datasets: the NUS dataset and the\nnear-infrared dataset. For the object recognition, SDD achieved 100% accuracy\nfor the Kimia 99 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 07:13:19 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 07:23:13 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Wang", "Zhenzhou", ""]]}, {"id": "1910.05728", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Shivansh Patel and Vinay P. Namboodiri", "title": "Granular Multimodal Attention Networks for Visual Dialog", "comments": "ICCV Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vision and language tasks have benefited from attention. There have been a\nnumber of different attention models proposed. However, the scale at which\nattention needs to be applied has not been well examined. Particularly, in this\nwork, we propose a new method Granular Multi-modal Attention, where we aim to\nparticularly address the question of the right granularity at which one needs\nto attend while solving the Visual Dialog task. The proposed method shows\nimprovement in both image and text attention networks. We then propose a\ngranular Multi-modal Attention network that jointly attends on the image and\ntext granules and shows the best performance. With this work, we observe that\nobtaining granular attention and doing exhaustive Multi-modal Attention appears\nto be the best way to attend while solving visual dialog.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 10:49:41 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Patro", "Badri N.", ""], ["Patel", "Shivansh", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1910.05733", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Yi Yang", "title": "One-Shot Neural Architecture Search via Self-Evaluated Template Network", "comments": "Minor modifications to the ICCV 2019 camera-ready version (add code\n  link)", "journal-ref": null, "doi": "10.1109/ICCV.2019.00378", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) aims to automate the search procedure of\narchitecture instead of manual design. Even if recent NAS approaches finish the\nsearch within days, lengthy training is still required for a specific\narchitecture candidate to get the parameters for its accurate evaluation.\nRecently one-shot NAS methods are proposed to largely squeeze the tedious\ntraining process by sharing parameters across candidates. In this way, the\nparameters for each candidate can be directly extracted from the shared\nparameters instead of training them from scratch. However, they have no sense\nof which candidate will perform better until evaluation so that the candidates\nto evaluate are randomly sampled and the top-1 candidate is considered the\nbest. In this paper, we propose a Self-Evaluated Template Network (SETN) to\nimprove the quality of the architecture candidates for evaluation so that it is\nmore likely to cover competitive candidates. SETN consists of two components:\n(1) an evaluator, which learns to indicate the probability of each individual\narchitecture being likely to have a lower validation loss. The candidates for\nevaluation can thus be selectively sampled according to this evaluator. (2) a\ntemplate network, which shares parameters among all candidates to amortize the\ntraining cost of generated candidates. In experiments, the architecture found\nby SETN achieves state-of-the-art performance on CIFAR and ImageNet benchmarks\nwithin comparable computation costs. Code is publicly available on GitHub:\nhttps://github.com/D-X-Y/AutoDL-Projects.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 11:25:40 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 09:04:32 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 08:09:55 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 13:32:40 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Dong", "Xuanyi", ""], ["Yang", "Yi", ""]]}, {"id": "1910.05751", "submitter": "Jia Liu", "authors": "Wenhua Zhang, Licheng Jiao, Jia Liu", "title": "Hierarchical Feature-Aware Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a hierarchical feature-aware tracking framework for\nefficient visual tracking. Recent years, ensembled trackers which combine\nmultiple component trackers have achieved impressive performance. In ensembled\ntrackers, the decision of results is usually a post-event process, i.e.,\ntracking result for each tracker is first obtained and then the suitable one is\nselected according to result ensemble. In this paper, we propose a pre-event\nmethod. We construct an expert pool with each expert being one set of features.\nFor each frame, several experts are first selected in the pool according to\ntheir past performance and then they are used to predict the object. The\nselection rate of each expert in the pool is then updated and tracking result\nis obtained according to result ensemble. We propose a novel pre-known\nexpert-adaptive selection strategy. Since the process is more efficient, more\nexperts can be constructed by fusing more types of features which leads to more\nrobustness. Moreover, with the novel expert selection strategy, overfitting\ncaused by fixed experts for each frame can be mitigated. Experiments on several\npublic available datasets demonstrate the superiority of the proposed method\nand its state-of-the-art performance among ensembled trackers.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 13:51:44 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 14:09:12 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Zhang", "Wenhua", ""], ["Jiao", "Licheng", ""], ["Liu", "Jia", ""]]}, {"id": "1910.05758", "submitter": "Gang Chen", "authors": "Gang Chen, Hongzhe Yu, Wei Dong, Xinjun Sheng, Xiangyang Zhu and Han\n  Ding", "title": "Learning to Navigate from Simulation via Spatial and Semantic\n  Information Synthesis with Noise Model Embedding", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While training an end-to-end navigation network in the real world is usually\nof high cost, simulation provides a safe and cheap environment in this training\nstage. However, training neural network models in simulation brings up the\nproblem of how to effectively transfer the model from simulation to the real\nworld (sim-to-real). In this work, we regard the environment representation as\na crucial element in this transfer process and propose a visual information\npyramid (VIP) model to systematically investigate a practical environment\nrepresentation. A novel representation composed of spatial and semantic\ninformation synthesis is then established accordingly, where noise model\nembedding is particularly considered. To explore the effectiveness of this\nrepresentation, we compared the performance with representations popularly used\nin the literature in both simulated and real-world scenarios. Results suggest\nthat our environment representation stands out. Furthermore, an analysis on the\nfeature map is implemented to investigate the effectiveness through inner\nreaction, which could be irradiative for future researches on end-to-end\nnavigation.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 14:13:39 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 07:22:25 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 02:30:28 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Chen", "Gang", ""], ["Yu", "Hongzhe", ""], ["Dong", "Wei", ""], ["Sheng", "Xinjun", ""], ["Zhu", "Xiangyang", ""], ["Ding", "Han", ""]]}, {"id": "1910.05770", "submitter": "Lamberto Ballan", "authors": "Tobia Tesan, Pasquale Coscia, Lamberto Ballan", "title": "A CNN-RNN Framework for Image Annotation from Visual Cues and Social\n  Network Metadata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images represent a commonly used form of visual communication among people.\nNevertheless, image classification may be a challenging task when dealing with\nunclear or non-common images needing more context to be correctly annotated.\nMetadata accompanying images on social-media represent an ideal source of\nadditional information for retrieving proper neighborhoods easing image\nannotation task. To this end, we blend visual features extracted from neighbors\nand their metadata to jointly leverage context and visual cues. Our models use\nmultiple semantic embeddings to achieve the dual objective of being robust to\nvocabulary changes between train and test sets and decoupling the architecture\nfrom the low-level metadata representation. Convolutional and recurrent neural\nnetworks (CNNs-RNNs) are jointly adopted to infer similarity among neighbors\nand query images. We perform comprehensive experiments on the NUS-WIDE dataset\nshowing that our models outperform state-of-the-art architectures based on\nimages and metadata, and decrease both sensory and semantic gaps to better\nannotate images.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 15:24:48 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 17:19:26 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Tesan", "Tobia", ""], ["Coscia", "Pasquale", ""], ["Ballan", "Lamberto", ""]]}, {"id": "1910.05774", "submitter": "Dimitrios Kollias", "authors": "Hanne Carlsson and Dimitrios Kollias", "title": "Image Generation and Recognition (Emotions)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) were proposed in 2014 by Goodfellow et\nal., and have since been extended into multiple computer vision applications.\nThis report provides a thorough survey of recent GAN research, outlining the\nvarious architectures and applications, as well as methods for training GANs\nand dealing with latent space. This is followed by a discussion of potential\nareas for future GAN research, including: evaluating GANs, better understanding\nGANs, and techniques for training GANs. The second part of this report outlines\nthe compilation of a dataset of images `in the wild' representing each of the 7\nbasic human emotions, and analyses experiments done when training a StarGAN on\nthis dataset combined with the FER2013 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 16:00:06 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 23:10:05 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Carlsson", "Hanne", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05784", "submitter": "Dimitrios Kollias", "authors": "Xia Yicheng and Dimitrios Kollias", "title": "Interpretable Deep Neural Networks for Dimensional and Categorical\n  Emotion Recognition in-the-wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions play an important role in people's life. Understanding and\nrecognising is not only important for interpersonal communication, but also has\npromising applications in Human-Computer Interaction, automobile safety and\nmedical research. This project focuses on extending the emotion recognition\ndatabase, and training the CNN + RNN emotion recognition neural networks with\nemotion category representation and valence \\& arousal representation. The\ncombined models are constructed by training the two representations\nsimultaneously. The comparison and analysis between the three types of model\nare discussed. The inner-relationship between two emotion representations and\nthe interpretability of the neural networks are investigated. The findings\nsuggest that categorical emotion recognition performance can benefit from\ntraining with a combined model. And the mapping of emotion category and valence\n\\& arousal values can explain this phenomenon.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 16:33:18 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 23:25:57 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Yicheng", "Xia", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05810", "submitter": "Samuel Sohn", "authors": "Samuel S. Sohn and Seonghyeon Moon and Honglu Zhou and Sejong Yoon and\n  Vladimir Pavlovic and Mubbasir Kapadia", "title": "Deep Crowd-Flow Prediction in Built Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the behavior of crowds in complex environments is a key\nrequirement in a multitude of application areas, including crowd and disaster\nmanagement, architectural design, and urban planning. Given a crowd's immediate\nstate, current approaches simulate crowd movement to arrive at a future state.\nHowever, most applications require the ability to predict hundreds of possible\nsimulation outcomes (e.g., under different environment and crowd situations) at\nreal-time rates, for which these approaches are prohibitively expensive.\n  In this paper, we propose an approach to instantly predict the long-term flow\nof crowds in arbitrarily large, realistic environments. Central to our approach\nis a novel CAGE representation consisting of Capacity, Agent, Goal, and\nEnvironment-oriented information, which efficiently encodes and decodes crowd\nscenarios into compact, fixed-size representations that are environmentally\nlossless. We present a framework to facilitate the accurate and efficient\nprediction of crowd flow in never-before-seen crowd scenarios. We conduct a\nseries of experiments to evaluate the efficacy of our approach and showcase\npositive results.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 18:43:55 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Sohn", "Samuel S.", ""], ["Moon", "Seonghyeon", ""], ["Zhou", "Honglu", ""], ["Yoon", "Sejong", ""], ["Pavlovic", "Vladimir", ""], ["Kapadia", "Mubbasir", ""]]}, {"id": "1910.05827", "submitter": "Jerry Wei", "authors": "Jerry Wei, Arief Suriawinata, Louis Vaickus, Bing Ren, Xiaoying Liu,\n  Jason Wei, Saeed Hassanpour", "title": "Generative Image Translation for Data Augmentation in Colorectal\n  Histopathology Images", "comments": "NeurIPS 2019 Machine Learning for Health Workshop Full Paper (19/111\n  accepted papers = 17% acceptance rate)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an image translation approach to generate augmented data for\nmitigating data imbalances in a dataset of histopathology images of colorectal\npolyps, adenomatous tumors that can lead to colorectal cancer if left\nuntreated. By applying cycle-consistent generative adversarial networks\n(CycleGANs) to a source domain of normal colonic mucosa images, we generate\nsynthetic colorectal polyp images that belong to diagnostically less common\npolyp classes. Generated images maintain the general structure of their source\nimage but exhibit adenomatous features that can be enhanced with our proposed\nfiltration module, called Path-Rank-Filter. We evaluate the quality of\ngenerated images through Turing tests with four gastrointestinal pathologists,\nfinding that at least two of the four pathologists could not identify generated\nimages at a statistically significant level. Finally, we demonstrate that using\nCycleGAN-generated images to augment training data improves the AUC of a\nconvolutional neural network for detecting sessile serrated adenomas by over\n10%, suggesting that our approach might warrant further research for other\nhistopathology image classification tasks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 20:43:53 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Wei", "Jerry", ""], ["Suriawinata", "Arief", ""], ["Vaickus", "Louis", ""], ["Ren", "Bing", ""], ["Liu", "Xiaoying", ""], ["Wei", "Jason", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1910.05839", "submitter": "Guan-An Wang", "authors": "Guan'an Wang, Tianzhu Zhang, Jian Cheng, Si Liu, Yang Yang, Zengguang\n  Hou", "title": "RGB-Infrared Cross-Modality Person Re-Identification via Joint Pixel and\n  Feature Alignment", "comments": "accepted by ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-Infrared (IR) person re-identification is an important and challenging\ntask due to large cross-modality variations between RGB and IR images. Most\nconventional approaches aim to bridge the cross-modality gap with feature\nalignment by feature representation learning. Different from existing methods,\nin this paper, we propose a novel and end-to-end Alignment Generative\nAdversarial Network (AlignGAN) for the RGB-IR RE-ID task. The proposed model\nenjoys several merits. First, it can exploit pixel alignment and feature\nalignment jointly. To the best of our knowledge, this is the first work to\nmodel the two alignment strategies jointly for the RGB-IR RE-ID problem.\nSecond, the proposed model consists of a pixel generator, a feature generator,\nand a joint discriminator. By playing a min-max game among the three\ncomponents, our model is able to not only alleviate the cross-modality and\nintra-modality variations but also learn identity-consistent features.\nExtensive experimental results on two standard benchmarks demonstrate that the\nproposed model performs favorably against state-of-the-art methods. Especially,\non SYSU-MM01 dataset, our model can achieve an absolute gain of 15.4% and 12.9%\nin terms of Rank-1 and mAP.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 21:38:43 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 20:29:17 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Wang", "Guan'an", ""], ["Zhang", "Tianzhu", ""], ["Cheng", "Jian", ""], ["Liu", "Si", ""], ["Yang", "Yang", ""], ["Hou", "Zengguang", ""]]}, {"id": "1910.05872", "submitter": "Hankook Lee", "authors": "Hankook Lee, Sung Ju Hwang, Jinwoo Shin", "title": "Self-supervised Label Augmentation via Input Transformations", "comments": "Accepted to ICML 2020. Code available at\n  https://github.com/hankook/SLA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning, which learns by constructing artificial labels\ngiven only the input signals, has recently gained considerable attention for\nlearning representations with unlabeled datasets, i.e., learning without any\nhuman-annotated supervision. In this paper, we show that such a technique can\nbe used to significantly improve the model accuracy even under fully-labeled\ndatasets. Our scheme trains the model to learn both original and\nself-supervised tasks, but is different from conventional multi-task learning\nframeworks that optimize the summation of their corresponding losses. Our main\nidea is to learn a single unified task with respect to the joint distribution\nof the original and self-supervised labels, i.e., we augment original labels\nvia self-supervision of input transformation. This simple, yet effective\napproach allows to train models easier by relaxing a certain invariant\nconstraint during learning the original and self-supervised tasks\nsimultaneously. It also enables an aggregated inference which combines the\npredictions from different augmentations to improve the prediction accuracy.\nFurthermore, we propose a novel knowledge transfer technique, which we refer to\nas self-distillation, that has the effect of the aggregated inference in a\nsingle (faster) inference. We demonstrate the large accuracy improvement and\nwide applicability of our framework on various fully-supervised settings, e.g.,\nthe few-shot and imbalanced classification scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 00:37:33 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 12:10:28 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Lee", "Hankook", ""], ["Hwang", "Sung Ju", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1910.05877", "submitter": "Dimitrios Kollias", "authors": "Valentin Richer and Dimitrios Kollias", "title": "Interpretable Deep Neural Networks for Facial Expression and Dimensional\n  Emotion Recognition in-the-wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we created a database with two types of annotations used in\nthe emotion recognition domain : Action Units and Valence Arousal to try to\nachieve better results than with only one model. The originality of the\napproach is also based on the type of architecture used to perform the\nprediction of the emotions : a categorical Generative Adversarial Network. This\nkind of dual network can generate images based on the pictures from the new\ndataset thanks to its generative network and decide if an image is fake or real\nthanks to its discriminative network as well as help to predict the annotations\nfor Action Units and Valence Arousal due to its categorical nature. GANs were\ntrained on the Action Units model only, then the Valence Arousal model only and\nthen on both the Action Units model and Valence Arousal model in order to test\ndifferent parameters and understand their influence. The generative and\ndiscriminative aspects of the GANs have performed interesting results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 01:33:06 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 23:17:07 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Richer", "Valentin", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05886", "submitter": "Yuwei Yang", "authors": "Yuwei Yang, Fanman Meng, Hongliang Li, Qingbo Wu, Xiaolong Xu and\n  Shuai Chen", "title": "A New Local Transformation Module for Few-shot Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot segmentation segments object regions of new classes with a few of\nmanual annotations. Its key step is to establish the transformation module\nbetween support images (annotated images) and query images (unlabeled images),\nso that the segmentation cues of support images can guide the segmentation of\nquery images. The existing methods form transformation model based on global\ncues, which however ignores the local cues that are verified in this paper to\nbe very important for the transformation. This paper proposes a new\ntransformation module based on local cues, where the relationship of the local\nfeatures is used for transformation. To enhance the generalization performance\nof the network, the relationship matrix is calculated in a high-dimensional\nmetric embedding space based on cosine distance. In addition, to handle the\nchallenging mapping problem from the low-level local relationships to\nhigh-level semantic cues, we propose to apply generalized inverse matrix of the\nannotation matrix of support images to transform the relationship matrix\nlinearly, which is non-parametric and class-agnostic. The result by the matrix\ntransformation can be regarded as an attention map with high-level semantic\ncues, based on which a transformation module can be built simply.The proposed\ntransformation module is a general module that can be used to replace the\ntransformation module in the existing few-shot segmentation frameworks. We\nverify the effectiveness of the proposed method on Pascal VOC 2012 dataset. The\nvalue of mIoU achieves at 57.0% in 1-shot and 60.6% in 5-shot, which\noutperforms the state-of-the-art method by 1.6% and 3.5%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 01:52:21 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 01:02:48 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Yang", "Yuwei", ""], ["Meng", "Fanman", ""], ["Li", "Hongliang", ""], ["Wu", "Qingbo", ""], ["Xu", "Xiaolong", ""], ["Chen", "Shuai", ""]]}, {"id": "1910.05899", "submitter": "Dongliang He", "authors": "Fan Yang, Xiao Liu, Dongliang He, Chuang Gan, Jian Wang, Chao Li, Fu\n  Li, Shilei Wen", "title": "TruNet: Short Videos Generation from Long Videos via Story-Preserving\n  Truncation", "comments": "ICCV intelligent short video workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a new problem, named as {\\em story-preserving long\nvideo truncation}, that requires an algorithm to automatically truncate a\nlong-duration video into multiple short and attractive sub-videos with each one\ncontaining an unbroken story. This differs from traditional video highlight\ndetection or video summarization problems in that each sub-video is required to\nmaintain a coherent and integral story, which is becoming particularly\nimportant for resource-production video sharing platforms such as Youtube,\nFacebook, TikTok, Kwai, etc. To address the problem, we collect and annotate a\nnew large video truncation dataset, named as TruNet, which contains 1470 videos\nwith on average 11 short stories per video. With the new dataset, we further\ndevelop and train a neural architecture for video truncation that consists of\ntwo components: a Boundary Aware Network (BAN) and a Fast-Forward Long\nShort-Term Memory (FF-LSTM). We first use the BAN to generate high quality\ntemporal proposals by jointly considering frame-level attractiveness and\nboundaryness. We then apply the FF-LSTM, which tends to capture high-order\ndependencies among a sequence of frames, to decide whether a temporal proposal\nis a coherent and integral story. We show that our proposed framework\noutperforms existing approaches for the story-preserving long video truncation\nproblem in both quantitative measures and user-study. The dataset is available\nfor public academic research usage at https://ai.baidu.com/broad/download.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 03:06:15 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Yang", "Fan", ""], ["Liu", "Xiao", ""], ["He", "Dongliang", ""], ["Gan", "Chuang", ""], ["Wang", "Jian", ""], ["Li", "Chao", ""], ["Li", "Fu", ""], ["Wen", "Shilei", ""]]}, {"id": "1910.05901", "submitter": "Zheng Zhu", "authors": "Junjie Huang, Zheng Zhu, Guan Huang", "title": "Multi-Stage HRNet: Multiple Stage High-Resolution Network for Human Pose\n  Estimation", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation are of importance for visual understanding tasks such\nas action recognition and human-computer interaction. In this work, we present\na Multiple Stage High-Resolution Network (Multi-Stage HRNet) to tackling the\nproblem of multi-person pose estimation in images. Specifically, we follow the\ntop-down pipelines and high-resolution representations are maintained during\nsingle-person pose estimation. In addition, multiple stage network and cross\nstage feature aggregation are adopted to further refine the keypoint position.\nThe resulting approach achieves promising results in COCO datasets. Our\nsingle-model-single-scale test configuration obtains 77.1 AP score in test-dev\nusing publicly available training data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 03:08:03 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Huang", "Junjie", ""], ["Zhu", "Zheng", ""], ["Huang", "Guan", ""]]}, {"id": "1910.05909", "submitter": "Yi Fang", "authors": "Xiang Li, Mingyang Wang, Congcong Wen, Lingjing Wang, Nan Zhou and Yi\n  Fang", "title": "Density-Aware Convolutional Networks with Context Encoding for Airborne\n  LiDAR Point Cloud Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better address challenging issues of the irregularity and inhomogeneity\ninherently present in 3D point clouds, researchers have been shifting their\nfocus from the design of hand-craft point feature towards the learning of 3D\npoint signatures using deep neural networks for 3D point cloud classification.\nRecent proposed deep learning based point cloud classification methods either\napply 2D CNN on projected feature images or apply 1D convolutional layers\ndirectly on raw point sets. These methods cannot adequately recognize\nfine-grained local structures caused by the uneven density distribution of the\npoint cloud data. In this paper, to address this challenging issue, we\nintroduced a density-aware convolution module which uses the point-wise density\nto re-weight the learnable weights of convolution kernels. The proposed\nconvolution module is able to fully approximate the 3D continuous convolution\non unevenly distributed 3D point sets. Based on this convolution module, we\nfurther developed a multi-scale fully convolutional neural network with\ndownsampling and upsampling blocks to enable hierarchical point feature\nlearning. In addition, to regularize the global semantic context, we\nimplemented a context encoding module to predict a global context encoding and\nformulated a context encoding regularizer to enforce the predicted context\nencoding to be aligned with the ground truth one. The overall network can be\ntrained in an end-to-end fashion with the raw 3D coordinates as well as the\nheight above ground as inputs. Experiments on the International Society for\nPhotogrammetry and Remote Sensing (ISPRS) 3D labeling benchmark demonstrated\nthe superiority of the proposed method for point cloud classification. Our\nmodel achieved a new state-of-the-art performance with an average F1 score of\n71.2% and improved the performance by a large margin on several categories.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 04:01:33 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Li", "Xiang", ""], ["Wang", "Mingyang", ""], ["Wen", "Congcong", ""], ["Wang", "Lingjing", ""], ["Zhou", "Nan", ""], ["Fang", "Yi", ""]]}, {"id": "1910.05911", "submitter": "James McCout Mr", "authors": "James McCouat, Ben Glocker", "title": "Vertebrae Detection and Localization in CT with Two-Stage CNNs and Dense\n  Annotations", "comments": "Accept into the MICCAI workshop MSKI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new, two-stage approach to the vertebrae centroid detection and\nlocalization problem. The first stage detects where the vertebrae appear in the\nscan using 3D samples, the second identifies the specific vertebrae within that\nregion-of-interest using 2D slices. Our solution utilizes new techniques to\nimprove the accuracy of the algorithm such as a revised approach to dense\nlabelling from sparse centroid annotations and usage of large anisotropic\nkernels in the base level of a U-net architecture to maximize the receptive\nfield. Our method improves the state-of-the-art's mean localization accuracy by\n0.87mm on a publicly available spine CT benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 04:12:59 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["McCouat", "James", ""], ["Glocker", "Ben", ""]]}, {"id": "1910.05972", "submitter": "Shakiba Kheradmand", "authors": "Shakiba Kheradmand, Parvaneh Saeedi, Jason Au, John Havelock", "title": "Preimplantation Blastomere Boundary Identification in HMC Microscopic\n  Images of Early Stage Human Embryos", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for identification of the boundary of embryonic\ncells (blastomeres) in Hoffman Modulation Contrast (HMC) microscopic images\nthat are taken between day one to day three. Identification of boundaries of\nblastomeres is a challenging task, especially in the cases containing four or\nmore cells. This is because these cells are bundled up tightly inside an\nembryo's membrane and any 2D image projection of such 3D embryo includes cell\noverlaps, occlusions, and projection ambiguities. Moreover, human embryos\ninclude fragmentation, which does not conform to any specific patterns or\nshape. Here we developed a model-based iterative approach, in which blastomeres\nare modeled as ellipses that conform to the local image features, such as edges\nand normals. In an iterative process, each image feature contributes only to\none candidate and is removed upon being associated to a model candidate. We\nhave tested the proposed algorithm on an image dataset comprising of 468 human\nembryos obtained from different sources. An overall Precision, Sensitivity and\nOverall Quality (OQ) of 92%, 88% and 83% are achieved.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 08:15:49 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Kheradmand", "Shakiba", ""], ["Saeedi", "Parvaneh", ""], ["Au", "Jason", ""], ["Havelock", "John", ""]]}, {"id": "1910.05986", "submitter": "Jinshi Yu", "authors": "Jinshi Yu, Weijun Sun, Yuning Qiu, Shengli Xie", "title": "An Efficient Tensor Completion Method via New Latent Nuclear Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tensor completion, the latent nuclear norm is commonly used to induce\nlow-rank structure, while substantially failing to capture the global\ninformation due to the utilization of unbalanced unfolding scheme. To overcome\nthis drawback, a new latent nuclear norm equipped with a more balanced\nunfolding scheme is defined for low-rank regularizer. Moreover, the new latent\nnuclear norm together with the Frank-Wolfe (FW) algorithm is developed as an\nefficient completion method by utilizing the sparsity structure of observed\ntensor. Specifically, both FW linear subproblem and line search only need to\naccess the observed entries, by which we can instead maintain the sparse\ntensors and a set of small basis matrices during iteration. Most operations are\nbased on sparse tensors, and the closed-form solution of FW linear subproblem\ncan be obtained from rank-one SVD. We theoretically analyze the\nspace-complexity and time-complexity of the proposed method, and show that it\nis much more efficient over other norm-based completion methods for\nhigher-order tensors. Extensive experimental results of visual-data inpainting\ndemonstrate that the proposed method is able to achieve state-of-the-art\nperformance at smaller costs of time and space, which is very meaningful for\nthe memory-limited equipment in practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 08:46:43 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Yu", "Jinshi", ""], ["Sun", "Weijun", ""], ["Qiu", "Yuning", ""], ["Xie", "Shengli", ""]]}, {"id": "1910.05996", "submitter": "Qiang Sun", "authors": "Qiang Sun, Liting Wang, Maohui Li, Longtao Zhang, Yuxiang Yang", "title": "A unified framework of predicting binary interestingness of images based\n  on discriminant correlation analysis and multiple kernel learning", "comments": "30 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the modern content-based image retrieval systems, there is an increasingly\ninterest in constructing a computationally effective model to predict the\ninterestingness of images since the measure of image interestingness could\nimprove the human-centered search satisfaction and the user experience in\ndifferent applications. In this paper, we propose a unified framework to\npredict the binary interestingness of images based on discriminant correlation\nanalysis (DCA) and multiple kernel learning (MKL) techniques. More specially,\non the one hand, to reduce feature redundancy in describing the interestingness\ncues of images, the DCA or multi-set discriminant correlation analysis (MDCA)\ntechnique is adopted to fuse multiple feature sets of the same type for\nindividual cues by taking into account the class structure among the samples\ninvolved to describe the three classical interestingness cues,\nunusualness,aesthetics as well as general preferences, with three sets of\ncompact and representative features; on the other hand, to make good use of the\nheterogeneity from the three sets of high-level features for describing the\ninterestingness cues, the SimpleMKL method is employed to enhance the\ngeneralization ability of the built model for the task of the binary\ninterestingness classification. Experimental results on the publicly-released\ninterestingness prediction data set have demonstrated the rationality and\neffectiveness of the proposed framework in the binary prediction of image\ninterestingness where we have conducted several groups of comparative studies\nacross different interestingness feature combinations, different\ninterestingness cues, as well as different feature types for the three\ninterestingness cues.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:03:48 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Sun", "Qiang", ""], ["Wang", "Liting", ""], ["Li", "Maohui", ""], ["Zhang", "Longtao", ""], ["Yang", "Yuxiang", ""]]}, {"id": "1910.05998", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Allen Y. Yang, Woojin Ko, Luisa Caldas", "title": "Optimization and Manipulation of Contextual Mutual Spaces for Multi-User\n  Virtual and Augmented Reality Interaction", "comments": "Accepted at 2020 IEEE Conference on Virtual Reality and 3D User\n  Interfaces (VR)", "journal-ref": null, "doi": "10.1109/VR46266.2020.00055", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial computing experiences are physically constrained by the geometry and\nsemantics of the local user environment. This limitation is elevated in remote\nmulti-user interaction scenarios, where finding a common virtual ground\nphysically accessible for all participants becomes challenging. Locating a\ncommon accessible virtual ground is difficult for the users themselves,\nparticularly if they are not aware of the spatial properties of other\nparticipants. In this paper, we introduce a framework to generate an optimal\nmutual virtual space for a multi-user interaction setting where remote users'\nroom spaces can have different layout and sizes. The framework further\nrecommends movement of surrounding furniture objects that expand the size of\nthe mutual space with minimal physical effort. Finally, we demonstrate the\nperformance of our solution on real-world datasets and also a real HoloLens\napplication. Results show the proposed algorithm can effectively discover\noptimal shareable space for multi-user virtual interaction and hence facilitate\nremote spatial computing communication in various collaborative workflows.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:10:54 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 05:36:47 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Yang", "Allen Y.", ""], ["Ko", "Woojin", ""], ["Caldas", "Luisa", ""]]}, {"id": "1910.06007", "submitter": "Rasmus R. Paulsen", "authors": "Rasmus R. Paulsen and Kristine Aavild Juhl and Thilde Marie Haspang\n  and Thomas Hansen and Melanie Ganz and Gudmundur Einarsson", "title": "Multi-view consensus CNN for 3D facial landmark placement", "comments": "This is a pre-print of an article published in proceedings of the\n  asian conference on computer vision 2018 (LNCS 11361). The final\n  authenticated version is available online at:\n  https://doi.org/10.1007/978-3-030-20887-5_44", "journal-ref": "Proceedings of the asian conference on computer vision 2018.\n  Lecture Notes in Computer Science, vol 11361. Springer", "doi": "10.1007/978-3-030-20887-5_44", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid increase in the availability of accurate 3D scanning devices has\nmoved facial recognition and analysis into the 3D domain. 3D facial landmarks\nare often used as a simple measure of anatomy and it is crucial to have\naccurate algorithms for automatic landmark placement. The current\nstate-of-the-art approaches have yet to gain from the dramatic increase in\nperformance reported in human pose tracking and 2D facial landmark placement\ndue to the use of deep convolutional neural networks (CNN). Development of deep\nlearning approaches for 3D meshes has given rise to the new subfield called\ngeometric deep learning, where one topic is the adaptation of meshes for the\nuse of deep CNNs. In this work, we demonstrate how methods derived from\ngeometric deep learning, namely multi-view CNNs, can be combined with recent\nadvances in human pose tracking. The method finds 2D landmark estimates and\npropagates this information to 3D space, where a consensus method determines\nthe accurate 3D face landmark position. We utilise the method on a standard 3D\nface dataset and show that it outperforms current methods by a large margin.\nFurther, we demonstrate how models trained on 3D range scans can be used to\naccurately place anatomical landmarks in magnetic resonance images.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:34:29 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Paulsen", "Rasmus R.", ""], ["Juhl", "Kristine Aavild", ""], ["Haspang", "Thilde Marie", ""], ["Hansen", "Thomas", ""], ["Ganz", "Melanie", ""], ["Einarsson", "Gudmundur", ""]]}, {"id": "1910.06014", "submitter": "Alexis Stoven-Dubois", "authors": "Alexis Stoven-Dubois, Kuntima Kiala Miguel, Aziz Dziri, Bertrand\n  Leroy, Roland Chapuis", "title": "A Collaborative Framework for High-Definition Mapping", "comments": "2019 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "2019 IEEE Intelligent Transportation Systems Conference (ITSC)", "doi": "10.1109/ITSC.2019.8917292", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For connected vehicles to have a substantial effect on road safety, it is\nrequired that accurate positions and trajectories can be shared. To this end,\nall vehicles must be accurately geolocalized in a common frame. This can be\nachieved by merging GNSS (Global Navigation Satellite System) information and\nvisual observations matched with a map of geo-positioned landmarks. Building\nsuch a map remains a challenge, and current solutions are facing strong\ncost-related limitations.\n  We present a collaborative framework for high-definition mapping, in which\nvehicles equipped with standard sensors, such as a GNSS receiver and a\nmono-visual camera, update a map of geolocalized landmarks. Our system is\ncomposed of two processing blocks: the first one is embedded in each vehicle,\nand aims at geolocalizing the vehicle and the detected feature marks. The\nsecond is operated on cloud servers, and uses observations from all the\nvehicles to compute updates for the map of geo-positioned landmarks. As the\nmap's landmarks are detected and positioned by more and more vehicles, the\naccuracy of the map increases, eventually converging in probability towards a\nnull error. The landmarks geo-positions are estimated in a stable and scalable\nway, enabling to provide dynamic map updates in an automatic manner.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:46:52 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 23:59:21 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Stoven-Dubois", "Alexis", ""], ["Miguel", "Kuntima Kiala", ""], ["Dziri", "Aziz", ""], ["Leroy", "Bertrand", ""], ["Chapuis", "Roland", ""]]}, {"id": "1910.06017", "submitter": "Hannes Fassold", "authors": "Hannes Fassold, Ridouane Ghermi", "title": "OmniTrack: Real-time detection and tracking of objects, text and logos\n  in video", "comments": "accepted for IEEE ISM Conference, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic detection and tracking of general objects (like persons,\nanimals or cars), text and logos in a video is crucial for many video\nunderstanding tasks, and usually real-time processing as required. We propose\nOmniTrack, an efficient and robust algorithm which is able to automatically\ndetect and track objects, text as well as brand logos in real-time. It combines\na powerful deep learning based object detector (YoloV3) with high-quality\noptical flow methods. Based on the reference YoloV3 C++ implementation, we did\nsome important performance optimizations which will be described. The major\nsteps in the training procedure for the combined detector for text and logo\nwill be presented. We will describe then the OmniTrack algorithm, consisting of\nthe phases preprocessing, feature calculation, prediction, matching and update.\nSeveral performance optimizations have been implemented there as well, like\ndoing the object detection and optical flow calculation asynchronously.\nExperiments show that the proposed algorithm runs in real-time for standard\ndefinition ($720x576$) video on a PC with a Quadro RTX 5000 GPU.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:52:40 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Fassold", "Hannes", ""], ["Ghermi", "Ridouane", ""]]}, {"id": "1910.06023", "submitter": "Ying Zheng", "authors": "Ying Zheng, Hongxun Yao, and Xiaoshuai Sun", "title": "Deep Semantic Parsing of Freehand Sketches with Homogeneous\n  Transformation, Soft-Weighted Loss, and Staged Learning", "comments": "13 pages, accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2020.3028466", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep framework for part-level semantic\nparsing of freehand sketches, which makes three main contributions that are\nexperimentally shown to have substantial practical merit. First, we propose a\nhomogeneous transformation method to address the problem of domain adaptation.\nFor the task of sketch parsing, there is no available data of labeled freehand\nsketches that can be directly used for model training. An alternative solution\nis to learn from datasets of real image parsing, while the domain adaptation is\nan inevitable problem. Unlike existing methods that utilize the edge maps of\nreal images to approximate freehand sketches, the proposed homogeneous\ntransformation method transforms the data from domains of real images and\nfreehand sketches into a homogeneous space to minimize the semantic gap.\nSecond, we design a soft-weighted loss function as guidance for the training\nprocess, which gives attention to both the ambiguous label boundary and class\nimbalance. Third, we present a staged learning strategy to improve the parsing\nperformance of the trained model, which takes advantage of the shared\ninformation and specific characteristic from different sketch categories.\nExtensive experimental results demonstrate the effectiveness of the above three\nmethods. Specifically, to evaluate the generalization ability of our\nhomogeneous transformation method, additional experiments for the task of\nsketch-based image retrieval are conducted on the QMUL FG-SBIR dataset.\nFinally, by integrating the proposed three methods into a unified framework of\ndeep semantic sketch parsing (DeepSSP), we achieve the state-of-the-art on the\npublic SketchParse dataset.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 10:21:44 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 01:56:43 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Zheng", "Ying", ""], ["Yao", "Hongxun", ""], ["Sun", "Xiaoshuai", ""]]}, {"id": "1910.06038", "submitter": "Ying Zheng", "authors": "Ying Zheng, Hongxun Yao, Xiaoshuai Sun, Shengping Zhang, Sicheng Zhao,\n  and Fatih Porikli", "title": "Sketch-Specific Data Augmentation for Freehand Sketch Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch recognition remains a significant challenge due to the limited\ntraining data and the substantial intra-class variance of freehand sketches for\nthe same object. Conventional methods for this task often rely on the\navailability of the temporal order of sketch strokes, additional cues acquired\nfrom different modalities and supervised augmentation of sketch datasets with\nreal images, which also limit the applicability and feasibility of these\nmethods in real scenarios.\n  In this paper, we propose a novel sketch-specific data augmentation (SSDA)\nmethod that leverages the quantity and quality of the sketches automatically.\nFrom the aspect of quantity, we introduce a Bezier pivot based deformation\n(BPD) strategy to enrich the training data. Towards quality improvement, we\npresent a mean stroke reconstruction (MSR) approach to generate a set of novel\ntypes of sketches with smaller intra-class variances. Both of these solutions\nare unrestricted from any multi-source data and temporal cues of sketches.\nFurthermore, we show that some recent deep convolutional neural network models\nthat are trained on generic classes of real images can be better choices than\nmost of the elaborate architectures that are designed explicitly for sketch\nrecognition. As SSDA can be integrated with any convolutional neural networks,\nit has a distinct advantage over the existing methods. Our extensive\nexperimental evaluations demonstrate that the proposed method achieves the\nstate-of-the-art results (84.27%) on the TU-Berlin dataset, outperforming the\nhuman performance by a remarkable 11.17% increase. Finally, more experiments\nshow the practical value of our approach for the task of sketch-based image\nretrieval.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 11:15:07 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 07:10:49 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Zheng", "Ying", ""], ["Yao", "Hongxun", ""], ["Sun", "Xiaoshuai", ""], ["Zhang", "Shengping", ""], ["Zhao", "Sicheng", ""], ["Porikli", "Fatih", ""]]}, {"id": "1910.06041", "submitter": "Vikas Agaradahalli Gurumurthy", "authors": "Vikas Agaradahalli Gurumurthy", "title": "Encoder-Decoder based CNN and Fully Connected CRFs for Remote Sensed\n  Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of remote-sensed imaging large volumes of very high\nresolution land cover images can now be obtained. Automation of object\nrecognition in these 2D images, however, is still a key issue. High intra-class\nvariance and low inter-class variance in Very High Resolution (VHR) images\nhamper the accuracy of prediction in object recognition tasks. Most successful\ntechniques in various computer vision tasks recently are based on deep\nsupervised learning. In this work, a deep Convolutional Neural Network (CNN)\nbased on symmetric encoder-decoder architecture with skip connections is\nemployed for the 2D semantic segmentation of most common land cover object\nclasses - impervious surface, buildings, low vegetation, trees and cars. Atrous\nconvolutions are employed to have large receptive field in the proposed CNN\nmodel. Further, the CNN outputs are post-processed using Fully Connected\nConditional Random Field (FCRF) model to refine the CNN pixel label\npredictions. The proposed CNN-FCRF model achieves an overall accuracy of 90.5%\non the ISPRS Vaihingen Dataset.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 11:22:18 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Gurumurthy", "Vikas Agaradahalli", ""]]}, {"id": "1910.06056", "submitter": "Martin Th\\\"ummel", "authors": "Martin Th\\\"ummel and Sven Sickert and Joachim Denzler", "title": "Facial Behavior Analysis using 4D Curvature Statistics for Presentation\n  Attack Detection", "comments": "Manuscript submitted for publication in IEEE International Conference\n  on Automatic Face & Gesture Recognition (FG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The uniqueness, complexity, and diversity of facial shapes and expressions\nled to success of facial biometric systems. Regardless of the accuracy of\ncurrent facial recognition methods, most of them are vulnerable against the\npresentation of sophisticated masks. In the highly monitored application\nscenario at airports and banks, fraudsters probably do not wear masks. However,\na deception will become more probable due to the increase of unsupervised\nauthentication using kiosks, eGates and mobile phones in self-service. To\nrobustly detect elastic 3D masks, one of the ultimate goals is to automatically\nanalyze the plausibility of the facial behavior based on a sequence of 3D face\nscans. Most importantly, such a method would also detect all less advanced\npresentation attacks using static 3D masks, bent photographs with eyeholes, and\nreplay attacks using monitors. Our proposed method achieves this goal by\ncomparing the temporal curvature change between presentation attacks and\ngenuine faces. For evaluation purposes, we recorded a challenging database\ncontaining replay attacks, static and elastic 3D masks using a high-quality 3D\nsensor. Based on the proposed representation, we found a clear separation\nbetween the low facial expressiveness of presentation attacks and the plausible\nbehavior of genuine faces.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 11:53:03 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 17:31:08 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 07:29:25 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Th\u00fcmmel", "Martin", ""], ["Sickert", "Sven", ""], ["Denzler", "Joachim", ""]]}, {"id": "1910.06067", "submitter": "Puneesh Deora", "authors": "Puneesh Deora, Bhavya Vasudeva, Saumik Bhattacharya, Pyari Mohan\n  Pradhan", "title": "Structure Preserving Compressive Sensing MRI Reconstruction using\n  Generative Adversarial Networks", "comments": "Accepted in IEEE CVPR Workshop on NTIRE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing magnetic resonance imaging (CS-MRI) accelerates the\nacquisition of MR images by breaking the Nyquist sampling limit. In this work,\na novel generative adversarial network (GAN) based framework for CS-MRI\nreconstruction is proposed. Leveraging a combination of patch-based\ndiscriminator and structural similarity index based loss, our model focuses on\npreserving high frequency content as well as fine textural details in the\nreconstructed image. Dense and residual connections have been incorporated in a\nU-net based generator architecture to allow easier transfer of information as\nwell as variable network length. We show that our algorithm outperforms\nstate-of-the-art methods in terms of quality of reconstruction and robustness\nto noise. Also, the reconstruction time, which is of the order of milliseconds,\nmakes it highly suitable for real-time clinical use.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 12:08:48 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 09:50:15 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Deora", "Puneesh", ""], ["Vasudeva", "Bhavya", ""], ["Bhattacharya", "Saumik", ""], ["Pradhan", "Pyari Mohan", ""]]}, {"id": "1910.06070", "submitter": "Hao Zhou", "authors": "Hao Zhou, Jorge Laval, Anye Zhou, Yu Wang, Wenchao Wu, Zhu Qing and\n  Srinivas Peeta", "title": "Review of Learning-based Longitudinal Motion Planning for Autonomous\n  Vehicles: Research Gaps between Self-driving and Traffic Congestion", "comments": "submitted to presentation at TRB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving technology companies and the research community are accelerating\ntheir pace to use machine learning longitudinal motion planning (mMP) for\nautonomous vehicles (AVs). This paper reviews the current state of the art in\nmMP, with an exclusive focus on its impact on traffic congestion. We identify\nthe availability of congestion scenarios in current datasets, and summarize the\nrequired features for training mMP. For learning methods, we survey the major\nmethods in both imitation learning and non-imitation learning. We also\nhighlight the emerging technologies adopted by some leading AV companies, e.g.\nTesla, Waymo, and Comma.ai. We find that: i) the AV industry has been mostly\nfocusing on the long tail problem related to safety and overlooked the impact\non traffic congestion, ii) the current public self-driving datasets have not\nincluded enough congestion scenarios, and mostly lack the necessary input\nfeatures/output labels to train mMP, and iii) albeit reinforcement learning\n(RL) approach can integrate congestion mitigation into the learning goal, the\nmajor mMP method adopted by industry is still behavior cloning (BC), whose\ncapability to learn a congestion-mitigating mMP remains to be seen. Based on\nthe review, the study identifies the research gaps in current mMP development.\nSome suggestions towards congestion mitigation for future mMP studies are\nproposed: i) enrich data collection to facilitate the congestion learning, ii)\nincorporate non-imitation learning methods to combine traffic efficiency into a\nsafety-oriented technical route, and iii) integrate domain knowledge from the\ntraditional car following (CF) theory to improve the string stability of mMP.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 19:19:48 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 15:37:49 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhou", "Hao", ""], ["Laval", "Jorge", ""], ["Zhou", "Anye", ""], ["Wang", "Yu", ""], ["Wu", "Wenchao", ""], ["Qing", "Zhu", ""], ["Peeta", "Srinivas", ""]]}, {"id": "1910.06072", "submitter": "Chang-Le Liu", "authors": "Chang-Le Liu, Kuang-Tsu Shih, Jiun-Woei Huang, Homer H. Chen", "title": "Light Field Synthesis by Training Deep Network in the Refocused Image\n  Domain", "comments": "Accepted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2020.2992354", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field imaging, which captures spatio-angular information of incident\nlight on image sensor, enables many interesting applications like image\nrefocusing and augmented reality. However, due to the limited sensor\nresolution, a trade-off exists between the spatial and angular resolution. To\nincrease the angular resolution, view synthesis techniques have been adopted to\ngenerate new views from existing views. However, traditional learning-based\nview synthesis mainly considers the image quality of each view of the light\nfield and neglects the quality of the refocused images. In this paper, we\npropose a new loss function called refocused image error (RIE) to address the\nissue. The main idea is that the image quality of the synthesized light field\nshould be optimized in the refocused image domain because it is where the light\nfield is perceived. We analyze the behavior of RIL in the spectral domain and\ntest the performance of our approach against previous approaches on both real\nand software-rendered light field datasets using objective assessment metrics\nsuch as MSE, MAE, PSNR, SSIM, and GMSD. Experimental results show that the\nlight field generated by our method results in better refocused images than\nprevious methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 12:13:37 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 20:51:53 GMT"}, {"version": "v3", "created": "Thu, 7 Nov 2019 02:09:39 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2020 13:49:52 GMT"}, {"version": "v5", "created": "Wed, 29 Apr 2020 03:07:29 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Liu", "Chang-Le", ""], ["Shih", "Kuang-Tsu", ""], ["Huang", "Jiun-Woei", ""], ["Chen", "Homer H.", ""]]}, {"id": "1910.06096", "submitter": "Giorgos Karvounas", "authors": "Giorgos Karvounas, Iason Oikonomidis, Antonis Argyros", "title": "ReActNet: Temporal Localization of Repetitive Activities in Real-World\n  Videos", "comments": "Accepted for presentation as a regular paper in the Intelligent\n  ShortVideo workshop, organized in conjunction with ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of temporal localization of repetitive activities in a\nvideo, i.e., the problem of identifying all segments of a video that contain\nsome sort of repetitive or periodic motion. To do so, the proposed method\nrepresents a video by the matrix of pairwise frame distances. These distances\nare computed on frame representations obtained with a convolutional neural\nnetwork. On top of this representation, we design, implement and evaluate\nReActNet, a lightweight convolutional neural network that classifies a given\nframe as belonging (or not) to a repetitive video segment. An important\nproperty of the employed representation is that it can handle repetitive\nsegments of arbitrary number and duration. Furthermore, the proposed training\nprocess requires a relatively small number of annotated videos. Our method\nraises several of the limiting assumptions of existing approaches regarding the\ncontents of the video and the types of the observed repetitive activities.\nExperimental results on recent, publicly available datasets validate our design\nchoices, verify the generalization potential of ReActNet and demonstrate its\nsuperior performance in comparison to the current state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 12:33:59 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Karvounas", "Giorgos", ""], ["Oikonomidis", "Iason", ""], ["Argyros", "Antonis", ""]]}, {"id": "1910.06138", "submitter": "Julia Guerrero-Viu", "authors": "Julia Guerrero-Viu, Clara Fernandez-Labrador, C\\'edric Demonceaux and\n  Jose J. Guerrero", "title": "What's in my Room? Object Recognition on Indoor Panoramic Images", "comments": "Project webpage: \"https://webdiis.unizar.es/~jguerrer/room_OR/\"", "journal-ref": "2020 IEEE International Conference on Robotics and Automation\n  (ICRA), pages 567-573", "doi": "10.1109/ICRA40945.2020.9197335", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, there has been a growing interest in taking advantage\nof the 360 panoramic images potential, while managing the new challenges they\nimply. While several tasks have been improved thanks to the contextual\ninformation these images offer, object recognition in indoor scenes still\nremains a challenging problem that has not been deeply investigated. This paper\nprovides an object recognition system that performs object detection and\nsemantic segmentation tasks by using a deep learning model adapted to match the\nnature of equirectangular images. From these results, instance segmentation\nmasks are recovered, refined and transformed into 3D bounding boxes that are\nplaced into the 3D model of the room. Quantitative and qualitative results\nsupport that our method outperforms the state of the art by a large margin and\nshow a complete understanding of the main objects in indoor scenes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 13:40:45 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 18:04:41 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Guerrero-Viu", "Julia", ""], ["Fernandez-Labrador", "Clara", ""], ["Demonceaux", "C\u00e9dric", ""], ["Guerrero", "Jose J.", ""]]}, {"id": "1910.06154", "submitter": "Dong Zeng", "authors": "Lisha Yao, Sui Li, Manman Zhu, Dong Zeng, Zhaoying Bian, Jianhua Ma", "title": "Direct Energy-resolving CT Imaging via Energy-integrating CT images\n  using a Unified Generative Adversarial Network", "comments": "5 pages, 3 figures, Accepted by MIC/NSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-resolving computed tomography (ErCT) has the ability to acquire\nenergy-dependent measurements simultaneously and quantitative material\ninformation with improved contrast-to-noise ratio. Meanwhile, ErCT imaging\nsystem is usually equipped with an advanced photon counting detector, which is\nexpensive and technically complex. Therefore, clinical ErCT scanners are not\nyet commercially available, and they are in various stage of completion. This\nmakes the researchers less accessible to the ErCT images. In this work, we\ninvestigate to produce ErCT images directly from existing energy-integrating CT\n(EiCT) images via deep neural network. Specifically, different from other\nnetworks that produce ErCT images at one specific energy, this model employs a\nunified generative adversarial network (uGAN) to concurrently train EiCT\ndatasets and ErCT datasets with different energies and then performs\nimage-to-image translation from existing EiCT images to multiple ErCT image\noutputs at various energy bins. In this study, the present uGAN generates ErCT\nimages at 70keV, 90keV, 110keV, and 130keV simultaneously from EiCT images\nat140kVp. We evaluate the present uGAN model on a set of over 1380 CT image\nslices and show that the present uGAN model can produce promising ErCT\nestimation results compared with the ground truth qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:05:17 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Yao", "Lisha", ""], ["Li", "Sui", ""], ["Zhu", "Manman", ""], ["Zeng", "Dong", ""], ["Bian", "Zhaoying", ""], ["Ma", "Jianhua", ""]]}, {"id": "1910.06160", "submitter": "Yanwei Pang", "authors": "Yanwei Pang and Jin Xie and Muhammad Haris Khan and Rao Muhammad Anwer\n  and Fahad Shahbaz Khan and Ling Shao", "title": "Mask-Guided Attention Network for Occluded Pedestrian Detection", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection relying on deep convolution neural networks has made\nsignificant progress. Though promising results have been achieved on standard\npedestrians, the performance on heavily occluded pedestrians remains far from\nsatisfactory. The main culprits are intra-class occlusions involving other\npedestrians and inter-class occlusions caused by other objects, such as cars\nand bicycles. These result in a multitude of occlusion patterns. We propose an\napproach for occluded pedestrian detection with the following contributions.\nFirst, we introduce a novel mask-guided attention network that fits naturally\ninto popular pedestrian detection pipelines. Our attention network emphasizes\non visible pedestrian regions while suppressing the occluded ones by modulating\nfull body features. Second, we empirically demonstrate that coarse-level\nsegmentation annotations provide reasonable approximation to their dense\npixel-wise counterparts. Experiments are performed on CityPersons and Caltech\ndatasets. Our approach sets a new state-of-the-art on both datasets. Our\napproach obtains an absolute gain of 9.5% in log-average miss rate, compared to\nthe best reported results on the heavily occluded (HO) pedestrian set of\nCityPersons test set. Further, on the HO pedestrian set of Caltech dataset, our\nmethod achieves an absolute gain of 5.0% in log-average miss rate, compared to\nthe best reported results. Code and models are available at:\nhttps://github.com/Leotju/MGAN.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:13:43 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 09:25:52 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Pang", "Yanwei", ""], ["Xie", "Jin", ""], ["Khan", "Muhammad Haris", ""], ["Anwer", "Rao Muhammad", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""]]}, {"id": "1910.06180", "submitter": "Vlad Hosu", "authors": "Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe", "title": "KonIQ-10k: An ecologically valid database for deep learning of blind\n  image quality assessment", "comments": "Published", "journal-ref": "Trans. Image Proc. 29 (2020) 4041-4056", "doi": "10.1109/TIP.2020.2967829", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods for image quality assessment (IQA) are limited due to\nthe small size of existing datasets. Extensive datasets require substantial\nresources both for generating publishable content and annotating it accurately.\nWe present a systematic and scalable approach to creating KonIQ-10k, the\nlargest IQA dataset to date, consisting of 10,073 quality scored images. It is\nthe first in-the-wild database aiming for ecological validity, concerning the\nauthenticity of distortions, the diversity of content, and quality-related\nindicators. Through the use of crowdsourcing, we obtained 1.2 million reliable\nquality ratings from 1,459 crowd workers, paving the way for more general IQA\nmodels. We propose a novel, deep learning model (KonCept512), to show an\nexcellent generalization beyond the test set (0.921 SROCC), to the current\nstate-of-the-art database LIVE-in-the-Wild (0.825 SROCC). The model derives its\ncore performance from the InceptionResNet architecture, being trained at a\nhigher resolution than previous models (512x384). Correlation analysis shows\nthat KonCept512 performs similar to having 9 subjective scores for each test\nimage.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:38:48 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 09:40:51 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Hosu", "Vlad", ""], ["Lin", "Hanhe", ""], ["Sziranyi", "Tamas", ""], ["Saupe", "Dietmar", ""]]}, {"id": "1910.06189", "submitter": "Li Wang", "authors": "Li Wang, Zixun Sun, Wentao Yao, Hui Zhan, Chengwei Zhu", "title": "Unsupervised Multi-stream Highlight detection for the Game \"Honor of\n  Kings\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the increasing popularity of E-sport live, Highlight Flashback has been\na critical functionality of live platforms, which aggregates the overall\nexciting fighting scenes in a few seconds. In this paper, we introduce a novel\ntraining strategy without any additional annotation to automatically generate\nhighlights for game video live. Considering that the existing manual edited\nclips contain more highlights than long game live videos, we perform pair-wise\nranking constraints across clips from edited and long live videos. A\nmulti-stream framework is also proposed to fuse spatial, temporal as well as\naudio features extracted from videos. To evaluate our method, we test on long\ngame live videos with an average length of about 15 minutes. Extensive\nexperimental results on videos demonstrate its satisfying performance on\nhighlights generation and effectiveness by the fusion of three streams.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:58:56 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 06:32:02 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Wang", "Li", ""], ["Sun", "Zixun", ""], ["Yao", "Wentao", ""], ["Zhan", "Hui", ""], ["Zhu", "Chengwei", ""]]}, {"id": "1910.06205", "submitter": "Maximilian Soelch", "authors": "Adnan Akhundov, Maximilian Soelch, Justin Bayer, Patrick van der Smagt", "title": "Variational Tracking and Prediction with Generative Disentangled\n  State-Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address tracking and prediction of multiple moving objects in visual data\nstreams as inference and sampling in a disentangled latent state-space model.\nBy encoding objects separately and including explicit position information in\nthe latent state space, we perform tracking via amortized variational Bayesian\ninference of the respective latent positions. Inference is implemented in a\nmodular neural framework tailored towards our disentangled latent space.\nGenerative and inference model are jointly learned from observations only.\nComparing to related prior work, we empirically show that our Markovian\nstate-space assumption enables faithful and much improved long-term prediction\nwell beyond the training horizon. Further, our inference model correctly\ndecomposes frames into objects, even in the presence of occlusions. Tracking\nperformance is increased significantly over prior art.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:20:30 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Akhundov", "Adnan", ""], ["Soelch", "Maximilian", ""], ["Bayer", "Justin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1910.06219", "submitter": "Ali Ghofrani", "authors": "Ali Ghofrani, Rahil Mahdian Toroghi, and Sayed Mojtaba Tabatabaie", "title": "ICPS-net: An End-to-End RGB-based Indoor Camera Positioning System using\n  deep convolutional neural networks", "comments": "7 pages, 8 figures, pre-print of ICMV2019 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Indoor positioning and navigation inside an area with no GPS-data\navailability is a challenging problem. There are applications such as augmented\nreality, autonomous driving, navigation of drones inside tunnels, in which\nindoor positioning gets crucial. In this paper, a tandem architecture of deep\nnetwork-based systems, for the first time to our knowledge, is developed to\naddress this problem. This structure is trained on the scene images being\nobtained through scanning of the desired area segments using photogrammetry. A\nCNN structure based on EfficientNet is trained as a classifier of the scenes,\nfollowed by a MobileNet CNN structure which is trained to perform as a\nregressor. The proposed system achieves amazingly fine precisions for both\nCartesian position and Quaternion information of the camera.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:43:56 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Ghofrani", "Ali", ""], ["Toroghi", "Rahil Mahdian", ""], ["Tabatabaie", "Sayed Mojtaba", ""]]}, {"id": "1910.06251", "submitter": "Shuai Li", "authors": "Shuai Li, Wanqing Li, Chris Cook, Yanbo Gao", "title": "Deep Independently Recurrent Neural Network (IndRNN)", "comments": "Extension of the CVPR2018 paper \"Independently Recurrent Neural\n  Network (IndRNN): Building A Longer and Deeper RNN\", with significant\n  improvements as described in the end of Section I. arXiv admin note: text\n  overlap with arXiv:1803.04831", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are known to be difficult to train due to\nthe gradient vanishing and exploding problems and thus difficult to learn\nlong-term patterns and construct deep networks. To address these problems, this\npaper proposes a new type of RNNs with the recurrent connection formulated as\nHadamard product, referred to as independently recurrent neural network\n(IndRNN), where neurons in the same layer are independent of each other and\nconnected across layers. Due to the better behaved gradient backpropagation,\nIndRNN with regulated recurrent weights effectively addresses the gradient\nvanishing and exploding problems and thus long-term dependencies can be\nlearned. Moreover, an IndRNN can work with non-saturated activation functions\nsuch as ReLU (rectified linear unit) and be still trained robustly. Different\ndeeper IndRNN architectures, including the basic stacked IndRNN, residual\nIndRNN and densely connected IndRNN, have been investigated, all of which can\nbe much deeper than the existing RNNs. Furthermore, IndRNN reduces the\ncomputation at each time step and can be over 10 times faster than the commonly\nused Long short-term memory (LSTM). Experimental results have shown that the\nproposed IndRNN is able to process very long sequences and construct very deep\nnetworks. Better performance has been achieved on various tasks with IndRNNs\ncompared with the traditional RNN, LSTM and the popular Transformer.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 09:43:49 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 12:51:05 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 09:19:00 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Li", "Shuai", ""], ["Li", "Wanqing", ""], ["Cook", "Chris", ""], ["Gao", "Yanbo", ""]]}, {"id": "1910.06259", "submitter": "David Stutz", "authors": "David Stutz, Matthias Hein, Bernt Schiele", "title": "Confidence-Calibrated Adversarial Training: Generalizing to Unseen\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training yields robust models against a specific threat model,\ne.g., $L_\\infty$ adversarial examples. Typically robustness does not generalize\nto previously unseen threat models, e.g., other $L_p$ norms, or larger\nperturbations. Our confidence-calibrated adversarial training (CCAT) tackles\nthis problem by biasing the model towards low confidence predictions on\nadversarial examples. By allowing to reject examples with low confidence,\nrobustness generalizes beyond the threat model employed during training. CCAT,\ntrained only on $L_\\infty$ adversarial examples, increases robustness against\nlarger $L_\\infty$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal\nadversarial examples and corrupted examples and yields better clean accuracy\ncompared to adversarial training. For thorough evaluation we developed novel\nwhite- and black-box attacks directly attacking CCAT by maximizing confidence.\nFor each threat model, we use $7$ attacks with up to $50$ restarts and $5000$\niterations and report worst-case robust test error, extended to our\nconfidence-thresholded setting, across all attacks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:38:03 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 16:34:42 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 16:15:44 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 12:03:44 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Stutz", "David", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1910.06261", "submitter": "Edgar Kaziakhmedov", "authors": "Edgar Kaziakhmedov, Klim Kireev, Grigorii Melnikov, Mikhail Pautov,\n  Aleksandr Petiushko", "title": "Real-world adversarial attack on MTCNN face detection system", "comments": null, "journal-ref": "2019 International Multi-Conference on Engineering, Computer and\n  Information Sciences (SIBIRCON)", "doi": "10.1109/SIBIRCON48586.2019.8958122", "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies proved that deep learning approaches achieve remarkable\nresults on face detection task. On the other hand, the advances gave rise to a\nnew problem associated with the security of the deep convolutional neural\nnetwork models unveiling potential risks of DCNNs based applications. Even\nminor input changes in the digital domain can result in the network being\nfooled. It was shown then that some deep learning-based face detectors are\nprone to adversarial attacks not only in a digital domain but also in the real\nworld. In the paper, we investigate the security of the well-known cascade CNN\nface detection system - MTCNN and introduce an easily reproducible and a robust\nway to attack it. We propose different face attributes printed on an ordinary\nwhite and black printer and attached either to the medical face mask or to the\nface directly. Our approach is capable of breaking the MTCNN detector in a\nreal-world scenario.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:42:09 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 22:29:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kaziakhmedov", "Edgar", ""], ["Kireev", "Klim", ""], ["Melnikov", "Grigorii", ""], ["Pautov", "Mikhail", ""], ["Petiushko", "Aleksandr", ""]]}, {"id": "1910.06271", "submitter": "Karim Armanious", "authors": "Karim Armanious, Sherif Abdulatif, Anish Rao Bhaktharaguttu, Thomas\n  K\\\"ustner, Tobias Hepp, Sergios Gatidis, Bin Yang", "title": "Organ-based Chronological Age Estimation based on 3D MRI Scans", "comments": "Submitted to IEEE EUSIPCO 2020", "journal-ref": null, "doi": "10.23919/Eusipco47968.2020.9287398", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals age differently depending on a multitude of different factors\nsuch as lifestyle, medical history and genetics. Often, the global\nchronological age is not indicative of the true ageing process. An organ-based\nage estimation would yield a more accurate health state assessment. In this\nwork, we propose a new deep learning architecture for organ-based age\nestimation based on magnetic resonance images (MRI). The proposed network is a\n3D convolutional neural network (CNN) with increased depth and width made\npossible by the hybrid utilization of inception and fire modules. We apply the\nproposed framework for the tasks of brain and knee age estimation. Quantitative\ncomparisons against concurrent MR-based regression networks and different 2D\nand 3D data feeding strategies illustrated the superior performance of the\nproposed work.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:55:10 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 11:39:16 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Armanious", "Karim", ""], ["Abdulatif", "Sherif", ""], ["Bhaktharaguttu", "Anish Rao", ""], ["K\u00fcstner", "Thomas", ""], ["Hepp", "Tobias", ""], ["Gatidis", "Sergios", ""], ["Yang", "Bin", ""]]}, {"id": "1910.06278", "submitter": "Feng Zhang", "authors": "Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, Ce Zhu", "title": "Distribution-Aware Coordinate Representation for Human Pose Estimation", "comments": "Results on the COCO keypoint detection challenge: 78.9% AP on the\n  test-dev set (Top-1 in the leaderbord by 12 Oct 2019) and 76.4% AP on the\n  test-challenge set. Project page: https://ilovepose.github.io/coco", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While being the de facto standard coordinate representation in human pose\nestimation, heatmap is never systematically investigated in the literature, to\nour best knowledge. This work fills this gap by studying the coordinate\nrepresentation with a particular focus on the heatmap. Interestingly, we found\nthat the process of decoding the predicted heatmaps into the final joint\ncoordinates in the original image space is surprisingly significant for human\npose estimation performance, which nevertheless was not recognised before. In\nlight of the discovered importance, we further probe the design limitations of\nthe standard coordinate decoding method widely used by existing methods, and\npropose a more principled distribution-aware decoding method. Meanwhile, we\nimprove the standard coordinate encoding process (i.e. transforming\nground-truth coordinates to heatmaps) by generating accurate heatmap\ndistributions for unbiased model training. Taking the two together, we\nformulate a novel Distribution-Aware coordinate Representation of Keypoint\n(DARK) method. Serving as a model-agnostic plug-in, DARK significantly improves\nthe performance of a variety of state-of-the-art human pose estimation models.\nExtensive experiments show that DARK yields the best results on two common\nbenchmarks, MPII and COCO, consistently validating the usefulness and\neffectiveness of our novel coordinate representation idea.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 17:03:56 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Zhang", "Feng", ""], ["Zhu", "Xiatian", ""], ["Dai", "Hanbin", ""], ["Ye", "Mao", ""], ["Zhu", "Ce", ""]]}, {"id": "1910.06302", "submitter": "Erfan Noury", "authors": "Erfan Noury, Suria S. Mannil, Robert T. Chang, An Ran Ran, Carol Y.\n  Cheung, Suman S. Thapa, Harsha L. Rao, Srilakshmi Dasari, Mohammed\n  Riyazuddin, Dolly Chang, Sriharsha Nagaraj, Clement C. Tham, Reza Zadeh", "title": "Finding New Diagnostic Information for Detecting Glaucoma using Neural\n  Networks", "comments": "28 pages, 12 figures, 15 tables, title changed, new authors added", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new approach to automated Glaucoma detection in 3D Spectral\nDomain Optical Coherence Tomography (OCT) optic nerve scans. First, we gathered\na unique and diverse multi-ethnic dataset of OCT scans consisting of glaucoma\nand non-glaucomatous cases obtained from four tertiary care eye hospitals\nlocated in four different countries. Using this longitudinal data, we achieved\nstate-of-the-art results for automatically detecting Glaucoma from a single raw\nOCT using a 3D Deep Learning system. These results are close to human doctors\nin a variety of settings across heterogeneous datasets and scanning\nenvironments. To verify correctness and interpretability of the automated\ncategorization, we used saliency maps to find areas of focus for the model.\nMatching human doctor behavior, the model predictions indeed correlated with\nthe conventional diagnostic parameters in the OCT printouts, such as the\nretinal nerve fiber layer. We further used our model to find new areas in the\n3D data that are presently not being identified as a diagnostic parameter to\ndetect glaucoma by human doctors. Namely, we found that the Lamina Cribrosa\n(LC) region can be a valuable source of helpful diagnostic information\npreviously unavailable to doctors during routine clinical care because it lacks\na quantitative printout. Our model provides such volumetric quantification of\nthis region. We found that even when a majority of the RNFL is removed, the LC\nregion can distinguish glaucoma. This is clinically relevant in high myopes,\nwhen the RNFL is already reduced, and thus the LC region may help differentiate\nglaucoma in this confounding situation. We further generalize this approach to\ncreate a new algorithm called DiagFind that provides a recipe for finding new\ndiagnostic information in medical imagery that may have been previously\nunusable by doctors.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 17:29:17 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 02:36:09 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Noury", "Erfan", ""], ["Mannil", "Suria S.", ""], ["Chang", "Robert T.", ""], ["Ran", "An Ran", ""], ["Cheung", "Carol Y.", ""], ["Thapa", "Suman S.", ""], ["Rao", "Harsha L.", ""], ["Dasari", "Srilakshmi", ""], ["Riyazuddin", "Mohammed", ""], ["Chang", "Dolly", ""], ["Nagaraj", "Sriharsha", ""], ["Tham", "Clement C.", ""], ["Zadeh", "Reza", ""]]}, {"id": "1910.06315", "submitter": "Badri Narayana Patro", "authors": "Soumik Dasgupta, Badri N. Patro, and Vinay P. Namboodiri", "title": "Dynamic Attention Networks for Task Oriented Grounding", "comments": "Accepted ICCV 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In order to successfully perform tasks specified by natural language\ninstructions, an artificial agent operating in a visual world needs to map\nwords, concepts, and actions from the instruction to visual elements in its\nenvironment. This association is termed as Task-Oriented Grounding. In this\nwork, we propose a novel Dynamic Attention Network architecture for the\nefficient multi-modal fusion of text and visual representations which can\ngenerate a robust definition of state for the policy learner. Our model assumes\nno prior knowledge from visual and textual domains and is an end to end\ntrainable. For a 3D visual world where the observation changes continuously,\nthe attention on the visual elements tends to be highly co-related from a\none-time step to the next. We term this as \"Dynamic Attention\". In this work,\nwe show that Dynamic Attention helps in achieving grounding and also aids in\nthe policy learning objective. Since most practical robotic applications take\nplace in the real world where the observation space is continuous, our\nframework can be used as a generalized multi-modal fusion unit for robotic\ncontrol through natural language. We show the effectiveness of using 1D\nconvolution over Gated Attention Hadamard product on the rate of convergence of\nthe network. We demonstrate that the cell-state of a Long Short Term Memory\n(LSTM) is a natural choice for modeling Dynamic Attention and shows through\nvisualization that the generated attention is very close to how humans tend to\nfocus on the environment.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 17:57:21 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Dasgupta", "Soumik", ""], ["Patro", "Badri N.", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1910.06316", "submitter": "Yichao Zhou", "authors": "Yichao Zhou and Haozhi Qi and Jingwei Huang and Yi Ma", "title": "NeurVPS: Neural Vanishing Point Scanning via Conic Convolution", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet effective end-to-end trainable deep network with\ngeometry-inspired convolutional operators for detecting vanishing points in\nimages. Traditional convolutional neural networks rely on aggregating edge\nfeatures and do not have mechanisms to directly exploit the geometric\nproperties of vanishing points as the intersections of parallel lines. In this\nwork, we identify a canonical conic space in which the neural network can\neffectively compute the global geometric information of vanishing points\nlocally, and we propose a novel operator named conic convolution that can be\nimplemented as regular convolutions in this space. This new operator explicitly\nenforces feature extractions and aggregations along the structural lines and\nyet has the same number of parameters as the regular 2D convolution. Our\nextensive experiments on both synthetic and real-world datasets show that the\nproposed operator significantly improves the performance of vanishing point\ndetection over traditional methods. The code and dataset have been made\npublicly available at https://github.com/zhou13/neurvps.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 17:58:23 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 07:24:29 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 03:03:02 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhou", "Yichao", ""], ["Qi", "Haozhi", ""], ["Huang", "Jingwei", ""], ["Ma", "Yi", ""]]}, {"id": "1910.06391", "submitter": "Chaofeng Wang", "authors": "Qian Yu, Chaofeng Wang, Barbaros Cetiner, Stella X. Yu, Frank Mckenna,\n  Ertugrul Taciroglu, Kincho H. Law", "title": "Building Information Modeling and Classification by Visual Learning At A\n  City Scale", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": "10.5281/zenodo.3996808", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide two case studies to demonstrate how artificial\nintelligence can empower civil engineering. In the first case, a machine\nlearning-assisted framework, BRAILS, is proposed for city-scale building\ninformation modeling. Building information modeling (BIM) is an efficient way\nof describing buildings, which is essential to architecture, engineering, and\nconstruction. Our proposed framework employs deep learning technique to extract\nvisual information of buildings from satellite/street view images. Further, a\nnovel machine learning (ML)-based statistical tool, SURF, is proposed to\ndiscover the spatial patterns in building metadata.\n  The second case focuses on the task of soft-story building classification.\nSoft-story buildings are a type of buildings prone to collapse during a\nmoderate or severe earthquake. Hence, identifying and retrofitting such\nbuildings is vital in the current earthquake preparedness efforts. For this\ntask, we propose an automated deep learning-based procedure for identifying\nsoft-story buildings from street view images at a regional scale. We also\ncreate a large-scale building image database and a semi-automated image\nlabeling approach that effectively annotates new database entries. Through\nextensive computational experiments, we demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 19:44:25 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 00:59:50 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Yu", "Qian", ""], ["Wang", "Chaofeng", ""], ["Cetiner", "Barbaros", ""], ["Yu", "Stella X.", ""], ["Mckenna", "Frank", ""], ["Taciroglu", "Ertugrul", ""], ["Law", "Kincho H.", ""]]}, {"id": "1910.06407", "submitter": "Jigar Doshi", "authors": "Jigar Doshi, Dominic Garcia, Cliff Massey, Pablo Llueca, Nicolas\n  Borensztein, Michael Baird, Matthew Cook, Devaki Raj", "title": "FireNet: Real-time Segmentation of Fire Perimeter from Aerial Video", "comments": "Published at NeurIPS 2019; Workshop on Artificial Intelligence for\n  Humanitarian Assistance and Disaster Response(AI+HADR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we share our approach to real-time segmentation of fire\nperimeter from aerial full-motion infrared video. We start by describing the\nproblem from a humanitarian aid and disaster response perspective.\nSpecifically, we explain the importance of the problem, how it is currently\nresolved, and how our machine learning approach improves it. To test our models\nwe annotate a large-scale dataset of 400,000 frames with guidance from domain\nexperts. Finally, we share our approach currently deployed in production with\ninference speed of 20 frames per second and an accuracy of 92 (F1 Score).\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 20:19:15 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Doshi", "Jigar", ""], ["Garcia", "Dominic", ""], ["Massey", "Cliff", ""], ["Llueca", "Pablo", ""], ["Borensztein", "Nicolas", ""], ["Baird", "Michael", ""], ["Cook", "Matthew", ""], ["Raj", "Devaki", ""]]}, {"id": "1910.06425", "submitter": "Haonan Peng", "authors": "Haonan Peng, Xingjian Yang, Yun-Hsuan Su, Blake Hannaford", "title": "Real-time Data Driven Precision Estimator for RAVEN-II Surgical Robot\n  End Effector Position", "comments": "6 pages, 10 figures, ICRA2020(under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical robots have been introduced to operating rooms over the past few\ndecades due to their high sensitivity, small size, and remote controllability.\nThe cable-driven nature of many surgical robots allows the systems to be\ndexterous and lightweight, with diameters as low as 5mm. However, due to the\nslack and stretch of the cables and the backlash of the gears, inevitable\nuncertainties are brought into the kinematics calculation. Since the reported\nend effector position of surgical robots like RAVEN-II is directly calculated\nusing the motor encoder measurements and forward kinematics, it may contain\nrelatively large error up to 10mm, whereas semi-autonomous functions being\nintroduced into abdominal surgeries require position inaccuracy of at most 1mm.\nTo resolve the problem, a cost-effective, real-time and data-driven pipeline\nfor robot end effector position precision estimation is proposed and tested on\nRAVEN-II. Analysis shows an improved end effector position error of around 1mm\nRMS traversing through the entire robot workspace without high-resolution\nmotion tracker.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 21:17:52 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Peng", "Haonan", ""], ["Yang", "Xingjian", ""], ["Su", "Yun-Hsuan", ""], ["Hannaford", "Blake", ""]]}, {"id": "1910.06426", "submitter": "Feng Xu", "authors": "Shuangjie Xu, Feng Xu, Yu Cheng, Pan Zhou", "title": "Tell-the-difference: Fine-grained Visual Descriptor via a Discriminating\n  Referee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate a novel problem of telling the difference\nbetween image pairs in natural language. Compared to previous approaches for\nsingle image captioning, it is challenging to fetch linguistic representation\nfrom two independent visual information. To this end, we have proposed an\neffective encoder-decoder caption framework based on Hyper Convolution Net. In\naddition, a series of novel feature fusing techniques for pairwise visual\ninformation fusing are introduced and a discriminating referee is proposed to\nevaluate the pipeline. Because of the lack of appropriate datasets to support\nthis task, we have collected and annotated a large new dataset with Amazon\nMechanical Turk (AMT) for generating captions in a pairwise manner (with 14764\nimages and 26710 image pairs in total). The dataset is the first one on the\nrelative difference caption task that provides descriptions in free language.\nWe evaluate the effectiveness of our model on two datasets in the field and it\noutperforms the state-of-the-art approach by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 21:21:03 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Xu", "Shuangjie", ""], ["Xu", "Feng", ""], ["Cheng", "Yu", ""], ["Zhou", "Pan", ""]]}, {"id": "1910.06428", "submitter": "Soheil Ghafurian", "authors": "Bairavi Venkatesh, Tosha Shah, Antong Chen, Soheil Ghafurian", "title": "Restoration of marker occluded hematoxylin and eosin stained whole slide\n  histology images using generative adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for pathologists to annotate specific regions of the tissue,\nsuch as tumor, directly on the glass slide with markers. Although this practice\nwas helpful prior to the advent of histology whole slide digitization, it often\noccludes important details which are increasingly relevant to immuno-oncology\ndue to recent advancements in digital pathology imaging techniques. The current\nwork uses a generative adversarial network with cycle loss to remove these\nannotations while still maintaining the underlying structure of the tissue by\nsolving an image-to-image translation problem. We train our network on up to\n300 whole slide images with marker inks and show that 70% of the corrected\nimage patches are indistinguishable from originally uncontaminated image tissue\nto a human expert. This portion increases 97% when we replace the human expert\nwith a deep residual network. We demonstrated the fidelity of the method to the\noriginal image by calculating the correlation between image gradient\nmagnitudes. We observed a revival of up to 94,000 nuclei per slide in our\ndataset, the majority of which were located on tissue border.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 21:22:54 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Venkatesh", "Bairavi", ""], ["Shah", "Tosha", ""], ["Chen", "Antong", ""], ["Ghafurian", "Soheil", ""]]}, {"id": "1910.06444", "submitter": "Joseph Xu", "authors": "Joseph Z. Xu, Wenhan Lu, Zebo Li, Pranav Khaitan, Valeriya Zaytseva", "title": "Building Damage Detection in Satellite Imagery Using Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In all types of disasters, from earthquakes to armed conflicts, aid workers\nneed accurate and timely data such as damage to buildings and population\ndisplacement to mount an effective response. Remote sensing provides this data\nat an unprecedented scale, but extracting operationalizable information from\nsatellite images is slow and labor-intensive. In this work, we use machine\nlearning to automate the detection of building damage in satellite imagery. We\ncompare the performance of four different convolutional neural network models\nin detecting damaged buildings in the 2010 Haiti earthquake. We also quantify\nhow well the models will generalize to future disasters by training and testing\nmodels on different disaster events.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 22:03:49 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Xu", "Joseph Z.", ""], ["Lu", "Wenhan", ""], ["Li", "Zebo", ""], ["Khaitan", "Pranav", ""], ["Zaytseva", "Valeriya", ""]]}, {"id": "1910.06466", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Andrew Hryniowski, Francis Li, Zhong Qiu Lin,\n  and Alexander Wong", "title": "State of Compact Architecture Search For Deep Neural Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of compact deep neural networks is a crucial task to enable\nwidespread adoption of deep neural networks in the real-world, particularly for\nedge and mobile scenarios. Due to the time-consuming and challenging nature of\nmanually designing compact deep neural networks, there has been significant\nrecent research interest into algorithms that automatically search for compact\nnetwork architectures. A particularly interesting class of compact architecture\nsearch algorithms are those that are guided by baseline network architectures.\nSuch algorithms have been shown to be significantly more computationally\nefficient than unguided methods. In this study, we explore the current state of\ncompact architecture search for deep neural networks through both theoretical\nand empirical analysis of four different state-of-the-art compact architecture\nsearch algorithms: i) group lasso regularization, ii) variational dropout, iii)\nMorphNet, and iv) Generative Synthesis. We examine these methods in detail\nbased on a number of different factors such as efficiency, effectiveness, and\nscalability. Furthermore, empirical evaluations are conducted to compare the\nefficacy of these compact architecture search algorithms across three\nwell-known benchmark datasets. While by no means an exhaustive exploration, we\nhope that this study helps provide insights into the interesting state of this\nrelatively new area of research in terms of diversity and real, tangible gains\nalready achieved in architecture design improvements. Furthermore, the hope is\nthat this study would help in pushing the conversation forward towards a deeper\ntheoretical and empirical understanding where the research community currently\nstands in the landscape of compact architecture search for deep neural\nnetworks, and the practical challenges and considerations in leveraging such\napproaches for operational usage.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 00:16:52 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Hryniowski", "Andrew", ""], ["Li", "Francis", ""], ["Lin", "Zhong Qiu", ""], ["Wong", "Alexander", ""]]}, {"id": "1910.06474", "submitter": "Jinzheng Cai", "authors": "Jinzheng Cai, Yingda Xia, Dong Yang, Daguang Xu, Lin Yang and Holger\n  Roth", "title": "End-to-End Adversarial Shape Learning for Abdomen Organ Deep\n  Segmentation", "comments": "Accepted to International Workshop on Machine Learning in Medical\n  Imaging (MLMI2019)", "journal-ref": null, "doi": "10.1007/978-3-030-32692-0_15", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of abdomen organs using medical imaging has many\npotential applications in clinical workflows. Recently, the state-of-the-art\nperformance for organ segmentation has been achieved by deep learning models,\ni.e., convolutional neural network (CNN). However, it is challenging to train\nthe conventional CNN-based segmentation models that aware of the shape and\ntopology of organs. In this work, we tackle this problem by introducing a novel\nend-to-end shape learning architecture -- organ point-network. It takes deep\nlearning features as inputs and generates organ shape representations as points\nthat located on organ surface. We later present a novel adversarial shape\nlearning objective function to optimize the point-network to capture shape\ninformation better. We train the point-network together with a CNN-based\nsegmentation model in a multi-task fashion so that the shared network\nparameters can benefit from both shape learning and segmentation tasks. We\ndemonstrate our method with three challenging abdomen organs including liver,\nspleen, and pancreas. The point-network generates surface points with\nfine-grained details and it is found critical for improving organ segmentation.\nConsequently, the deep segmentation model is improved by the introduced shape\nlearning as significantly better Dice scores are observed for spleen and\npancreas segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 01:48:29 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Cai", "Jinzheng", ""], ["Xia", "Yingda", ""], ["Yang", "Dong", ""], ["Xu", "Daguang", ""], ["Yang", "Lin", ""], ["Roth", "Holger", ""]]}, {"id": "1910.06475", "submitter": "Mingde Zhao", "authors": "Hongwei Ge and Zehang Yan and Kai Zhang and Mingde Zhao and Liang Sun", "title": "Exploring Overall Contextual Information for Image Captioning in\n  Human-Like Cognitive Style", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is a research hotspot where encoder-decoder models combining\nconvolutional neural network (CNN) and long short-term memory (LSTM) achieve\npromising results. Despite significant progress, these models generate\nsentences differently from human cognitive styles. Existing models often\ngenerate a complete sentence from the first word to the end, without\nconsidering the influence of the following words on the whole sentence\ngeneration. In this paper, we explore the utilization of a human-like cognitive\nstyle, i.e., building overall cognition for the image to be described and the\nsentence to be constructed, for enhancing computer image understanding. This\npaper first proposes a Mutual-aid network structure with Bidirectional LSTMs\n(MaBi-LSTMs) for acquiring overall contextual information. In the training\nprocess, the forward and backward LSTMs encode the succeeding and preceding\nwords into their respective hidden states by simultaneously constructing the\nwhole sentence in a complementary manner. In the captioning process, the LSTM\nimplicitly utilizes the subsequent semantic information contained in its hidden\nstates. In fact, MaBi-LSTMs can generate two sentences in forward and backward\ndirections. To bridge the gap between cross-domain models and generate a\nsentence with higher quality, we further develop a cross-modal attention\nmechanism to retouch the two sentences by fusing their salient parts as well as\nthe salient areas of the image. Experimental results on the Microsoft COCO\ndataset show that the proposed model improves the performance of\nencoder-decoder models and achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 01:49:19 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Ge", "Hongwei", ""], ["Yan", "Zehang", ""], ["Zhang", "Kai", ""], ["Zhao", "Mingde", ""], ["Sun", "Liang", ""]]}, {"id": "1910.06514", "submitter": "Takashi Matsubara", "authors": "Takashi Matsubara", "title": "Target-Oriented Deformation of Visual-Semantic Embedding Space", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal embedding is a crucial research topic for cross-modal\nunderstanding, data mining, and translation. Many studies have attempted to\nextract representations from given entities and align them in a shared\nembedding space. However, because entities in different modalities exhibit\ndifferent abstraction levels and modality-specific information, it is\ninsufficient to embed related entities close to each other. In this study, we\npropose the Target-Oriented Deformation Network (TOD-Net), a novel module that\ncontinuously deforms the embedding space into a new space under a given\ncondition, thereby adjusting similarities between entities. Unlike methods\nbased on cross-modal attention, TOD-Net is a post-process applied to the\nembedding space learned by existing embedding systems and improves their\nperformances of retrieval. In particular, when combined with cutting-edge\nmodels, TOD-Net gains the state-of-the-art cross-modal retrieval model\nassociated with the MSCOCO dataset. Qualitative analysis reveals that TOD-Net\nsuccessfully emphasizes entity-specific concepts and retrieves diverse targets\nvia handling higher levels of diversity than existing models.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 03:54:27 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Matsubara", "Takashi", ""]]}, {"id": "1910.06528", "submitter": "Yin Zhou", "authors": "Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang Gao, Tom\n  Ouyang, James Guo, Jiquan Ngiam, Vijay Vasudevan", "title": "End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point\n  Clouds", "comments": "CoRL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on 3D object detection advocates point cloud voxelization in\nbirds-eye view, where objects preserve their physical dimensions and are\nnaturally separable. When represented in this view, however, point clouds are\nsparse and have highly variable point density, which may cause detectors\ndifficulties in detecting distant or small objects (pedestrians, traffic signs,\netc.). On the other hand, perspective view provides dense observations, which\ncould allow more favorable feature encoding for such cases. In this paper, we\naim to synergize the birds-eye view and the perspective view and propose a\nnovel end-to-end multi-view fusion (MVF) algorithm, which can effectively learn\nto utilize the complementary information from both. Specifically, we introduce\ndynamic voxelization, which has four merits compared to existing voxelization\nmethods, i) removing the need of pre-allocating a tensor with fixed size; ii)\novercoming the information loss due to stochastic point/voxel dropout; iii)\nyielding deterministic voxel embeddings and more stable detection outcomes; iv)\nestablishing the bi-directional relationship between points and voxels, which\npotentially lays a natural foundation for cross-view feature fusion. By\nemploying dynamic voxelization, the proposed feature fusion architecture\nenables each point to learn to fuse context information from different views.\nMVF operates on points and can be naturally extended to other approaches using\nLiDAR point clouds. We evaluate our MVF model extensively on the newly released\nWaymo Open Dataset and on the KITTI dataset and demonstrate that it\nsignificantly improves detection accuracy over the comparable single-view\nPointPillars baseline.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 05:13:13 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 21:39:25 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Zhou", "Yin", ""], ["Sun", "Pei", ""], ["Zhang", "Yu", ""], ["Anguelov", "Dragomir", ""], ["Gao", "Jiyang", ""], ["Ouyang", "Tom", ""], ["Guo", "James", ""], ["Ngiam", "Jiquan", ""], ["Vasudevan", "Vijay", ""]]}, {"id": "1910.06540", "submitter": "Jasper Sebastiaan Wijnands", "authors": "Jasper S. Wijnands, Jason Thompson, Kerry A. Nice, Gideon D. P. A.\n  Aschwanden, Mark Stevenson", "title": "Real-time monitoring of driver drowsiness on mobile platforms using 3D\n  neural networks", "comments": "13 pages, 2 figures, 'Online First' version. For associated mp4\n  files, see journal website", "journal-ref": "Neural Computing and Applications (2019)", "doi": "10.1007/s00521-019-04506-0", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Driver drowsiness increases crash risk, leading to substantial road trauma\neach year. Drowsiness detection methods have received considerable attention,\nbut few studies have investigated the implementation of a detection approach on\na mobile phone. Phone applications reduce the need for specialised hardware and\nhence, enable a cost-effective roll-out of the technology across the driving\npopulation. While it has been shown that three-dimensional (3D) operations are\nmore suitable for spatiotemporal feature learning, current methods for\ndrowsiness detection commonly use frame-based, multi-step approaches. However,\ncomputationally expensive techniques that achieve superior results on action\nrecognition benchmarks (e.g. 3D convolutions, optical flow extraction) create\nbottlenecks for real-time, safety-critical applications on mobile devices.\nHere, we show how depthwise separable 3D convolutions, combined with an early\nfusion of spatial and temporal information, can achieve a balance between high\nprediction accuracy and real-time inference requirements. In particular,\nincreased accuracy is achieved when assessment requires motion information, for\nexample, when sunglasses conceal the eyes. Further, a custom TensorFlow-based\nsmartphone application shows the true impact of various approaches on inference\ntimes and demonstrates the effectiveness of real-time monitoring based on\nout-of-sample data to alert a drowsy driver. Our model is pre-trained on\nImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness\nDetection dataset. Fine-tuning on large naturalistic driving datasets could\nfurther improve accuracy to obtain robust in-vehicle performance. Overall, our\nresearch is a step towards practical deep learning applications, potentially\npreventing micro-sleeps and reducing road trauma.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 05:44:28 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Wijnands", "Jasper S.", ""], ["Thompson", "Jason", ""], ["Nice", "Kerry A.", ""], ["Aschwanden", "Gideon D. P. A.", ""], ["Stevenson", "Mark", ""]]}, {"id": "1910.06548", "submitter": "Zissis Poulos", "authors": "Zissis Poulos, Ali Nouri, Andreas Moshovos", "title": "Training CNNs faster with Dynamic Input and Kernel Downsampling", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reduce training time in convolutional networks (CNNs) with a method that,\nfor some of the mini-batches: a) scales down the resolution of input images via\ndownsampling, and b) reduces the forward pass operations via pooling on the\nconvolution filters. Training is performed in an interleaved fashion; some\nbatches undergo the regular forward and backpropagation passes with original\nnetwork parameters, whereas others undergo a forward pass with pooled filters\nand downsampled inputs. Since pooling is differentiable, the gradients of the\npooled filters propagate to the original network parameters for a standard\nparameter update. The latter phase requires fewer floating point operations and\nless storage due to the reduced spatial dimensions in feature maps and filters.\nThe key idea is that this phase leads to smaller and approximate updates and\nthus slower learning, but at significantly reduced cost, followed by passes\nthat use the original network parameters as a refinement stage. Deciding how\noften and for which batches the downsmapling occurs can be done either\nstochastically or deterministically, and can be defined as a training\nhyperparameter itself. Experiments on residual architectures show that we can\nachieve up to 23% reduction in training time with minimal loss in validation\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 06:18:29 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Poulos", "Zissis", ""], ["Nouri", "Ali", ""], ["Moshovos", "Andreas", ""]]}, {"id": "1910.06573", "submitter": "Cheng-En Wu", "authors": "Cheng-En Wu, Yi-Ming Chan, Chien-Hung Chen, Wen-Cheng Chen, Chu-Song\n  Chen", "title": "IMMVP: An Efficient Daytime and Nighttime On-Road Object Detector", "comments": "Accepted at IEEE 21st International Workshop on Multimedia Signal\n  Processing (MMSP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is hard to detect on-road objects under various lighting conditions. To\nimprove the quality of the classifier, three techniques are used. We define\nsubclasses to separate daytime and nighttime samples. Then we skip similar\nsamples in the training set to prevent overfitting. With the help of the\noutside training samples, the detection accuracy is also improved. To detect\nobjects in an edge device, Nvidia Jetson TX2 platform, we exert the lightweight\nmodel ResNet-18 FPN as the backbone feature extractor. The FPN (Feature Pyramid\nNetwork) generates good features for detecting objects over various scales.\nWith Cascade R-CNN technique, the bounding boxes are iteratively refined for\nbetter results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 07:46:03 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 02:16:59 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 05:00:49 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wu", "Cheng-En", ""], ["Chan", "Yi-Ming", ""], ["Chen", "Chien-Hung", ""], ["Chen", "Wen-Cheng", ""], ["Chen", "Chu-Song", ""]]}, {"id": "1910.06583", "submitter": "Xiaoli Liu", "authors": "Xiaoli Liu, Jianqin Yin, Jin Liu, Pengxiang Ding, Jun Liu, Huaping Liu", "title": "TrajectoryNet: a new spatio-temporal feature learning network for human\n  motion prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction is an increasingly interesting topic in computer\nvision and robotics. In this paper, we propose a new 2D CNN based network,\nTrajectoryNet, to predict future poses in the trajectory space. Compared with\nmost existing methods, our model focuses on modeling the motion dynamics with\ncoupled spatio-temporal features, local-global spatial features and global\ntemporal co-occurrence features of the previous pose sequence. Specifically,\nthe coupled spatio-temporal features describe the spatial and temporal\nstructure information hidden in the natural human motion sequence, which can be\nmined by covering the space and time dimensions of the input pose sequence with\nthe convolutional filters. The local-global spatial features that encode\ndifferent correlations of different joints of the human body (e.g. strong\ncorrelations between joints of one limb, weak correlations between joints of\ndifferent limbs) are captured hierarchically by enlarging the receptive field\nlayer by layer and residual connections from the lower layers to the deeper\nlayers in our proposed convolutional network. And the global temporal\nco-occurrence features represent the co-occurrence relationship that different\nsubsequences in a complex motion sequence are appeared simultaneously, which\ncan be obtained automatically with our proposed TrajectoryNet by reorganizing\nthe temporal information as the depth dimension of the input tensor. Finally,\nfuture poses are approximated based on the captured motion dynamics features.\nExtensive experiments show that our method achieves state-of-the-art\nperformance on three challenging benchmarks (e.g. Human3.6M, G3D, and FNTU),\nwhich demonstrates the effectiveness of our proposed method. The code will be\navailable if the paper is accepted.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 08:14:28 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 12:39:21 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Liu", "Xiaoli", ""], ["Yin", "Jianqin", ""], ["Liu", "Jin", ""], ["Ding", "Pengxiang", ""], ["Liu", "Jun", ""], ["Liu", "Huaping", ""]]}, {"id": "1910.06607", "submitter": "Zhao Qing", "authors": "Qing Zhao, Bin Luo, and Yun Zhang", "title": "Stereo-based Multi-motion Visual Odometry for Mobile Robots", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of computer vision, visual odometry is adopted by more\nand more mobile robots. However, we found that not only its own pose, but the\nposes of other moving objects are also crucial for the decision of the robot.\nIn addition, the visual odometry will be greatly disturbed when a significant\nmoving object appears. In this letter, a stereo-based multi-motion visual\nodometry method is proposed to acquire the poses of the robot and other moving\nobjects. In order to obtain the poses simultaneously, a continuous motion\nsegmentation module and a coordinate conversion module are applied to the\ntraditional visual odometry pipeline. As a result, poses of all moving objects\ncan be acquired and transformed into the ground coordinate system. The\nexperimental results show that the proposed multi-motion visual odometry can\neffectively eliminate the influence of moving objects on the visual odometry,\nas well as achieve 10 cm in position and 3{\\deg} in orientation RMSE (Root Mean\nSquare Error) of each moving object.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 09:10:56 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Zhao", "Qing", ""], ["Luo", "Bin", ""], ["Zhang", "Yun", ""]]}, {"id": "1910.06613", "submitter": "Mingjie Wu", "authors": "Mingjie Wu, Yongfei Zhang, Tianyu Zhang, Wenqi Zhang", "title": "Background Segmentation for Vehicle Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (Re-ID) is very important in intelligent\ntransportation and video surveillance.Prior works focus on extracting\ndiscriminative features from visual appearance of vehicles or using\nvisual-spatio-temporal information.However, background interference in vehicle\nre-identification have not been explored.In the actual large-scale\nspatio-temporal scenes, the same vehicle usually appears in different\nbackgrounds while different vehicles might appear in the same background, which\nwill seriously affect the re-identification performance. To the best of our\nknowledge, this paper is the first to consider the background interference\nproblem in vehicle re-identification. We construct a vehicle segmentation\ndataset and develop a vehicle Re-ID framework with a background interference\nremoval (BIR) mechanism to improve the vehicle Re-ID performance as well as\nrobustness against complex background in large-scale spatio-temporal scenes.\nExtensive experiments demonstrate the effectiveness of our proposed framework,\nwith an average 9% gain on mAP over state-of-the-art vehicle Re-ID algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 09:25:31 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Wu", "Mingjie", ""], ["Zhang", "Yongfei", ""], ["Zhang", "Tianyu", ""], ["Zhang", "Wenqi", ""]]}, {"id": "1910.06621", "submitter": "Arpan Garai", "authors": "Arpan Garai, Samit Biswas, Sekhar Mandal and Bidyut. B. Chaudhuri", "title": "A Method to Generate Synthetically Warped Document Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digital camera captured document images may often be warped and distorted\ndue to different camera angles or document surfaces. A robust technique is\nneeded to solve this kind of distortion. The research on dewarping of the\ndocument suffers due to the limited availability of benchmark public dataset.\nIn recent times, deep learning based approaches are used to solve the problems\naccurately. To train most of the deep neural networks a large number of\ndocument images is required and generating such a large volume of document\nimages manually is difficult. In this paper, we propose a technique to generate\na synthetic warped image from a flat-bedded scanned document image. It is done\nby calculating warping factors for each pixel position using two warping\nposition parameters (WPP) and eight warping control parameters (WCP). These\nparameters can be specified as needed depending upon the desired warping. The\nresults are compared with similar real captured images both qualitative and\nquantitative way.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 09:54:48 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Garai", "Arpan", ""], ["Biswas", "Samit", ""], ["Mandal", "Sekhar", ""], ["Chaudhuri", "Bidyut. B.", ""]]}, {"id": "1910.06632", "submitter": "Nan Yang", "authors": "Eunah Jung, Nan Yang, Daniel Cremers", "title": "Multi-Frame GAN: Image Enhancement for Stereo Visual Odometry in Low\n  Light", "comments": "Accepted by the 3rd Conference on Robot Learning, Osaka, Japan (CoRL\n  2019). The first two authors contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the concept of a multi-frame GAN (MFGAN) and demonstrate its\npotential as an image sequence enhancement for stereo visual odometry in low\nlight conditions. We base our method on an invertible adversarial network to\ntransfer the beneficial features of brightly illuminated scenes to the sequence\nin poor illumination without costly paired datasets. In order to preserve the\ncoherent geometric cues for the translated sequence, we present a novel network\narchitecture as well as a novel loss term combining temporal and stereo\nconsistencies based on optical flow estimation. We demonstrate that the\nenhanced sequences improve the performance of state-of-the-art feature-based\nand direct stereo visual odometry methods on both synthetic and real datasets\nin challenging illumination. We also show that MFGAN outperforms other\nstate-of-the-art image enhancement and style transfer methods by a large margin\nin terms of visual odometry.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 10:11:10 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Jung", "Eunah", ""], ["Yang", "Nan", ""], ["Cremers", "Daniel", ""]]}, {"id": "1910.06635", "submitter": "Mari\\\"elle Jansen", "authors": "Mari\\\"elle J.A. Jansen, Hugo J. Kuijf, Maarten Niekel, Wouter B.\n  Veldhuis, Frank J. Wessels, Max A. Viergever, Josien P.W. Pluim", "title": "Liver segmentation and metastases detection in MR images using\n  convolutional neural networks", "comments": null, "journal-ref": "J. Med. Imag. 6(4), 044003 (2019)", "doi": "10.1117/1.JMI.6.4.044003", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primary tumors have a high likelihood of developing metastases in the liver\nand early detection of these metastases is crucial for patient outcome. We\npropose a method based on convolutional neural networks (CNN) to detect liver\nmetastases. First, the liver was automatically segmented using the six phases\nof abdominal dynamic contrast enhanced (DCE) MR images. Next, DCE-MR and\ndiffusion weighted (DW) MR images are used for metastases detection within the\nliver mask. The liver segmentations have a median Dice similarity coefficient\nof 0.95 compared with manual annotations. The metastases detection method has a\nsensitivity of 99.8% with a median of 2 false positives per image. The\ncombination of the two MR sequences in a dual pathway network is proven\nvaluable for the detection of liver metastases. In conclusion, a high quality\nliver segmentation can be obtained in which we can successfully detect liver\nmetastases.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 10:17:59 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Jansen", "Mari\u00eblle J. A.", ""], ["Kuijf", "Hugo J.", ""], ["Niekel", "Maarten", ""], ["Veldhuis", "Wouter B.", ""], ["Wessels", "Frank J.", ""], ["Viergever", "Max A.", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "1910.06658", "submitter": "Linas Petkevicius", "authors": "Povilas Daniusis, Shubham Juneja, Lukas Valatka, Linas Petkevicius", "title": "Topological Navigation Graph Framework", "comments": null, "journal-ref": null, "doi": "10.1007/s10514-021-09980-x", "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the utilisation of reactive trajectory imitation controllers for\ngoal-directed mobile robot navigation. We propose a topological navigation\ngraph (TNG) - an imitation-learning-based framework for navigating through\nenvironments with intersecting trajectories. The TNG framework represents the\nenvironment as a directed graph composed of deep neural networks. Each vertex\nof the graph corresponds to a trajectory and is represented by a trajectory\nidentification classifier and a trajectory imitation controller. For trajectory\nfollowing, we propose the novel use of neural object detection architectures.\nThe edges of TNG correspond to intersections between trajectories and are all\nrepresented by a classifier. We provide empirical evaluation of the proposed\nnavigation framework and its components in simulated and real-world\nenvironments, demonstrating that TNG allows us to utilise non-goal-directed,\nimitation-learning methods for goal-directed autonomous navigation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 11:19:00 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 18:02:46 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Daniusis", "Povilas", ""], ["Juneja", "Shubham", ""], ["Valatka", "Lukas", ""], ["Petkevicius", "Linas", ""]]}, {"id": "1910.06673", "submitter": "Tessa Van Der Heiden", "authors": "Tessa van der Heiden, Naveen Shankar Nagaraja, Christian Weiss,\n  Efstratios Gavves", "title": "SafeCritic: Collision-Aware Trajectory Prediction", "comments": "To Appear as workshop paper for the British Machine Vision Conference\n  (BMVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Navigating complex urban environments safely is a key to realize fully\nautonomous systems. Predicting future locations of vulnerable road users, such\nas pedestrians and cyclists, thus, has received a lot of attention in the\nrecent years. While previous works have addressed modeling interactions with\nthe static (obstacles) and dynamic (humans) environment agents, we address an\nimportant gap in trajectory prediction. We propose SafeCritic, a model that\nsynergizes generative adversarial networks for generating multiple \"real\"\ntrajectories with reinforcement learning to generate \"safe\" trajectories. The\nDiscriminator evaluates the generated candidates on whether they are consistent\nwith the observed inputs. The Critic network is environmentally aware to prune\ntrajectories that are in collision or are in violation with the environment.\nThe auto-encoding loss stabilizes training and prevents mode-collapse. We\ndemonstrate results on two large scale data sets with a considerable\nimprovement over state-of-the-art. We also show that the Critic is able to\nclassify the safety of trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 12:15:19 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["van der Heiden", "Tessa", ""], ["Nagaraja", "Naveen Shankar", ""], ["Weiss", "Christian", ""], ["Gavves", "Efstratios", ""]]}, {"id": "1910.06690", "submitter": "Dario Dotti", "authors": "Dario Dotti, Mirela Popa, Stylianos Asteriadis", "title": "Being the center of attention: A Person-Context CNN framework for\n  Personality Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a novel study on personality recognition using video data\nfrom different scenarios. Our goal is to jointly model nonverbal behavioral\ncues with contextual information for a robust, multi-scenario, personality\nrecognition system. Therefore, we build a novel multi-stream Convolutional\nNeural Network framework (CNN), which considers multiple sources of\ninformation. From a given scenario, we extract spatio-temporal motion\ndescriptors from every individual in the scene, spatio-temporal motion\ndescriptors encoding social group dynamics, and proxemics descriptors to encode\nthe interaction with the surrounding context. All the proposed descriptors are\nmapped to the same feature space facilitating the overall learning effort.\nExperiments on two public datasets demonstrate the effectiveness of jointly\nmodeling the mutual Person-Context information, outperforming the state-of-the\nart-results for personality recognition in two different scenarios. Lastly, we\npresent CNN class activation maps for each personality trait, shedding light on\nbehavioral patterns linked with personality attributes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 12:47:11 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Dotti", "Dario", ""], ["Popa", "Mirela", ""], ["Asteriadis", "Stylianos", ""]]}, {"id": "1910.06693", "submitter": "Alejandro Cartas", "authors": "Alejandro Cartas and Jordi Luque and Petia Radeva and Carlos Segura\n  and Mariella Dimiccoli", "title": "Seeing and Hearing Egocentric Actions: How Much Can We Learn?", "comments": "Accepted for the Fifth International Workshop on Egocentric\n  Perception, Interaction and Computing (EPIC) at the International Conference\n  on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our interaction with the world is an inherently multimodal experience.\nHowever, the understanding of human-to-object interactions has historically\nbeen addressed focusing on a single modality. In particular, a limited number\nof works have considered to integrate the visual and audio modalities for this\npurpose. In this work, we propose a multimodal approach for egocentric action\nrecognition in a kitchen environment that relies on audio and visual\ninformation. Our model combines a sparse temporal sampling strategy with a late\nfusion of audio, spatial, and temporal streams. Experimental results on the\nEPIC-Kitchens dataset show that multimodal integration leads to better\nperformance than unimodal approaches. In particular, we achieved a 5.18%\nimprovement over the state of the art on verb classification.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 12:55:49 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Cartas", "Alejandro", ""], ["Luque", "Jordi", ""], ["Radeva", "Petia", ""], ["Segura", "Carlos", ""], ["Dimiccoli", "Mariella", ""]]}, {"id": "1910.06699", "submitter": "Cesar Roberto De Souza", "authors": "C\\'esar Roberto de Souza, Adrien Gaidon, Yohann Cabon, Naila Murray,\n  Antonio Manuel L\\'opez", "title": "Generating Human Action Videos by Coupling 3D Game Engines and\n  Probabilistic Graphical Models", "comments": "Pre-print of the article accepted for publication in the Special\n  Issue on Generating Realistic Visual Data of Human Behavior of the\n  International Journal of Computer Vision (IJCV). arXiv admin note:\n  substantial text overlap with arXiv:1612.00881", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep video action recognition models have been highly successful in recent\nyears but require large quantities of manually annotated data, which are\nexpensive and laborious to obtain. In this work, we investigate the generation\nof synthetic training data for video action recognition, as synthetic data have\nbeen successfully used to supervise models for a variety of other computer\nvision tasks. We propose an interpretable parametric generative model of human\naction videos that relies on procedural generation, physics models and other\ncomponents of modern game engines. With this model we generate a diverse,\nrealistic, and physically plausible dataset of human action videos, called PHAV\nfor \"Procedural Human Action Videos\". PHAV contains a total of 39,982 videos,\nwith more than 1,000 examples for each of 35 action categories. Our video\ngeneration approach is not limited to existing motion capture sequences: 14 of\nthese 35 categories are procedurally defined synthetic actions. In addition,\neach video is represented with 6 different data modalities, including RGB,\noptical flow and pixel-level semantic labels. These modalities are generated\nalmost simultaneously using the Multiple Render Targets feature of modern GPUs.\nIn order to leverage PHAV, we introduce a deep multi-task (i.e. that considers\naction classes from multiple datasets) representation learning architecture\nthat is able to simultaneously learn from synthetic and real video datasets,\neven when their action categories differ. Our experiments on the UCF-101 and\nHMDB-51 benchmarks suggest that combining our large set of synthetic videos\nwith small real-world datasets can boost recognition performance. Our approach\nalso significantly outperforms video representations produced by fine-tuning\nstate-of-the-art unsupervised generative models of videos.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 11:51:24 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["de Souza", "C\u00e9sar Roberto", ""], ["Gaidon", "Adrien", ""], ["Cabon", "Yohann", ""], ["Murray", "Naila", ""], ["L\u00f3pez", "Antonio Manuel", ""]]}, {"id": "1910.06705", "submitter": "YoungJoon Yoo", "authors": "YoungJoon Yoo, Sanghyuk Chun, Sangdoo Yun, Jung-Woo Ha, Jaejun Yoo", "title": "Neural Approximation of an Auto-Regressive Process through Confidence\n  Guided Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic confidence-based approximation that can be plugged in\nand simplify the auto-regressive generation process with a proved convergence.\nWe first assume that the priors of future samples can be generated in an\nindependently and identically distributed (i.i.d.) manner using an efficient\npredictor. Given the past samples and future priors, the mother AR model can\npost-process the priors while the accompanied confidence predictor decides\nwhether the current sample needs a resampling or not. Thanks to the i.i.d.\nassumption, the post-processing can update each sample in a parallel way, which\nremarkably accelerates the mother model. Our experiments on different data\ndomains including sequences and images show that the proposed method can\nsuccessfully capture the complex structures of the data and generate the\nmeaningful future samples with lower computational cost while preserving the\nsequential relationship of the data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 13:11:24 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Yoo", "YoungJoon", ""], ["Chun", "Sanghyuk", ""], ["Yun", "Sangdoo", ""], ["Ha", "Jung-Woo", ""], ["Yoo", "Jaejun", ""]]}, {"id": "1910.06727", "submitter": "Yan Xu", "authors": "Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun Bao, Hongsheng\n  Li", "title": "Depth Completion from Sparse LiDAR Data with Depth-Normal Constraints", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion aims to recover dense depth maps from sparse depth\nmeasurements. It is of increasing importance for autonomous driving and draws\nincreasing attention from the vision community. Most of existing methods\ndirectly train a network to learn a mapping from sparse depth inputs to dense\ndepth maps, which has difficulties in utilizing the 3D geometric constraints\nand handling the practical sensor noises. In this paper, to regularize the\ndepth completion and improve the robustness against noise, we propose a unified\nCNN framework that 1) models the geometric constraints between depth and\nsurface normal in a diffusion module and 2) predicts the confidence of sparse\nLiDAR measurements to mitigate the impact of noise. Specifically, our\nencoder-decoder backbone predicts surface normals, coarse depth and confidence\nof LiDAR inputs simultaneously, which are subsequently inputted into our\ndiffusion refinement module to obtain the final completion results. Extensive\nexperiments on KITTI depth completion dataset and NYU-Depth-V2 dataset\ndemonstrate that our method achieves state-of-the-art performance. Further\nablation study and analysis give more insights into the proposed method and\ndemonstrate the generalization capability and stability of our model.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 13:34:18 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Xu", "Yan", ""], ["Zhu", "Xinge", ""], ["Shi", "Jianping", ""], ["Zhang", "Guofeng", ""], ["Bao", "Hujun", ""], ["Li", "Hongsheng", ""]]}, {"id": "1910.06734", "submitter": "Priyam Shah Mr", "authors": "Aliasgar Haji, Priyam Shah, Srinivas Bijoor", "title": "Self Driving RC Car using Behavioral Cloning", "comments": "4 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self Driving Car technology is a vehicle that guides itself without human\nconduction. The first truly autonomous cars appeared in the 1980s with projects\nfunded by DARPA( Defense Advance Research Project Agency ). Since then a lot\nhas changed with the improvements in the fields of Computer Vision and Machine\nLearning. We have used the concept of behavioral cloning to convert a normal RC\nmodel car into an autonomous car using Deep Learning technology\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 19:03:37 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Haji", "Aliasgar", ""], ["Shah", "Priyam", ""], ["Bijoor", "Srinivas", ""]]}, {"id": "1910.06737", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Yida Zhao, Yuqing Song, Qin Jin, Qi Wu", "title": "Integrating Temporal and Spatial Attentions for VATEX Video Captioning\n  Challenge 2019", "comments": "ICCV 2019 VATEX challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This notebook paper presents our model in the VATEX video captioning\nchallenge. In order to capture multi-level aspects in the video, we propose to\nintegrate both temporal and spatial attentions for video captioning. The\ntemporal attentive module focuses on global action movements while spatial\nattentive module enables to describe more fine-grained objects. Considering\nthese two types of attentive modules are complementary, we thus fuse them via a\nlate fusion strategy. The proposed model significantly outperforms baselines\nand achieves 73.4 CIDEr score on the testing set which ranks the second place\nat the VATEX video captioning challenge leaderboard 2019.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 13:45:30 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Chen", "Shizhe", ""], ["Zhao", "Yida", ""], ["Song", "Yuqing", ""], ["Jin", "Qin", ""], ["Wu", "Qi", ""]]}, {"id": "1910.06745", "submitter": "Yundong Zhang", "authors": "Yundong Zhang, Hang Wu, Huiye Liu, Li Tong and May D Wang", "title": "Improve Model Generalization and Robustness to Dataset Bias with\n  Bias-regularized Learning and Domain-guided Augmentation", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has thrived on the emergence of biomedical big data. However,\nmedical datasets acquired at different institutions have inherent bias caused\nby various confounding factors such as operation policies, machine protocols,\ntreatment preference and etc. As the result, models trained on one dataset,\nregardless of volume, cannot be confidently utilized for the others. In this\nstudy, we investigated model robustness to dataset bias using three large-scale\nChest X-ray datasets: first, we assessed the dataset bias using vanilla\ntraining baseline; second, we proposed a novel multi-source domain\ngeneralization model by (a) designing a new bias-regularized loss function; and\n(b) synthesizing new data for domain augmentation. We showed that our model\nsignificantly outperformed the baseline and other approaches on data from\nunseen domain in terms of accuracy and various bias measures, without\nretraining or finetuning. Our method is generally applicable to other\nbiomedical data, providing new algorithms for training models robust to bias\nfor big data analysis and applications. Demo training code is publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 18:15:20 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 02:13:57 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 20:04:08 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Zhang", "Yundong", ""], ["Wu", "Hang", ""], ["Liu", "Huiye", ""], ["Tong", "Li", ""], ["Wang", "May D", ""]]}, {"id": "1910.06789", "submitter": "Surya Karthik Mukkavilli", "authors": "Caleb Hoyne, S. Karthik Mukkavilli and David Meger", "title": "Deep learning for Aerosol Forecasting", "comments": "Machine Learning and the Physical Sciences Workshop at the 33rd\n  Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.ao-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reanalysis datasets combining numerical physics models and limited\nobservations to generate a synthesised estimate of variables in an Earth\nsystem, are prone to biases against ground truth. Biases identified with the\nNASA Modern-Era Retrospective Analysis for Research and Applications, Version 2\n(MERRA-2) aerosol optical depth (AOD) dataset, against the Aerosol Robotic\nNetwork (AERONET) ground measurements in previous studies, motivated the\ndevelopment of a deep learning based AOD prediction model globally. This study\ncombines a convolutional neural network (CNN) with MERRA-2, tested against all\nAERONET sites. The new hybrid CNN-based model provides better estimates\nvalidated versus AERONET ground truth, than only using MERRA-2 reanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 17:35:08 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Hoyne", "Caleb", ""], ["Mukkavilli", "S. Karthik", ""], ["Meger", "David", ""]]}, {"id": "1910.06808", "submitter": "Luca Calatroni", "authors": "Marcelo Bertalm\\'io, Luca Calatroni, Valentina Franceschi, Benedetta\n  Franceschiello, Dario Prandi", "title": "Cortical-inspired Wilson-Cowan-type equations for orientation-dependent\n  contrast perception modelling", "comments": "This is the revised extended invited journal version of the SSVM 2019\n  conference proceeding arXiv:1812.07425", "journal-ref": "Journal of Mathematical Imaging and Vision 2020", "doi": "10.1007/s10851-020-00960-x", "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the evolution model proposed in [9, 6] to describe illusory\ncontrast perception phenomena induced by surrounding orientations. Firstly, we\nhighlight its analogies and differences with the widely used Wilson-Cowan\nequations [48], mainly in terms of efficient representation properties. Then,\nin order to explicitly encode local directional information, we exploit the\nmodel of the primary visual cortex (V1) proposed in [20] and largely used over\nthe last years for several image processing problems [24,38,28]. The resulting\nmodel is thus defined in the space of positions and orientation and it is\ncapable to describe assimilation and contrast visual bias at the same time. We\nreport several numerical tests showing the ability of the model to reproduce,\nin particular, orientation-dependent phenomena such as grating induction and a\nmodified version of the Poggendorff illusion. For this latter example, we\nempirically show the existence of a set of threshold parameters differentiating\nfrom inpainting to perception-type reconstructions and describing long-range\nconnectivity between different hypercolumns in V1.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 14:30:55 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 06:38:24 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Bertalm\u00edo", "Marcelo", ""], ["Calatroni", "Luca", ""], ["Franceschi", "Valentina", ""], ["Franceschiello", "Benedetta", ""], ["Prandi", "Dario", ""]]}, {"id": "1910.06809", "submitter": "Xihui Liu", "authors": "Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, Hongsheng Li", "title": "Learning to Predict Layout-to-image Conditional Convolutions for\n  Semantic Image Synthesis", "comments": "Accepted by NeurIPS 2019. Code is available soon at\n  https://github.com/xh-liu/CC-FPSE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image synthesis aims at generating photorealistic images from\nsemantic layouts. Previous approaches with conditional generative adversarial\nnetworks (GAN) show state-of-the-art performance on this task, which either\nfeed the semantic label maps as inputs to the generator, or use them to\nmodulate the activations in normalization layers via affine transformations. We\nargue that convolutional kernels in the generator should be aware of the\ndistinct semantic labels at different locations when generating images. In\norder to better exploit the semantic layout for the image generator, we propose\nto predict convolutional kernels conditioned on the semantic label map to\ngenerate the intermediate feature maps from the noise maps and eventually\ngenerate the images. Moreover, we propose a feature pyramid semantics-embedding\ndiscriminator, which is more effective in enhancing fine details and semantic\nalignments between the generated images and the input semantic layouts than\nprevious multi-scale discriminators. We achieve state-of-the-art results on\nboth quantitative metrics and subjective evaluation on various semantic\nsegmentation datasets, demonstrating the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 14:33:07 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 08:09:58 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 15:29:28 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Liu", "Xihui", ""], ["Yin", "Guojun", ""], ["Shao", "Jing", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1910.06827", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang", "title": "Learning Generalisable Omni-Scale Representations for Person\n  Re-Identification", "comments": "TPAMI 2021. Journal extension of arXiv:1905.00953. Updates: added\n  appendix. arXiv admin note: text overlap with arXiv:1905.00953", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective person re-identification (re-ID) model should learn feature\nrepresentations that are both discriminative, for distinguishing\nsimilar-looking people, and generalisable, for deployment across datasets\nwithout any adaptation. In this paper, we develop novel CNN architectures to\naddress both challenges. First, we present a re-ID CNN termed omni-scale\nnetwork (OSNet) to learn features that not only capture different spatial\nscales but also encapsulate a synergistic combination of multiple scales,\nnamely omni-scale features. The basic building block consists of multiple\nconvolutional streams, each detecting features at a certain scale. For\nomni-scale feature learning, a unified aggregation gate is introduced to\ndynamically fuse multi-scale features with channel-wise weights. OSNet is\nlightweight as its building blocks comprise factorised convolutions. Second, to\nimprove generalisable feature learning, we introduce instance normalisation\n(IN) layers into OSNet to cope with cross-dataset discrepancies. Further, to\ndetermine the optimal placements of these IN layers in the architecture, we\nformulate an efficient differentiable architecture search algorithm. Extensive\nexperiments show that, in the conventional same-dataset setting, OSNet achieves\nstate-of-the-art performance, despite being much smaller than existing re-ID\nmodels. In the more challenging yet practical cross-dataset setting, OSNet\nbeats most recent unsupervised domain adaptation methods without using any\ntarget data. Our code and models are released at\n\\texttt{https://github.com/KaiyangZhou/deep-person-reid}.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 14:44:16 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 10:48:32 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 18:03:28 GMT"}, {"version": "v4", "created": "Thu, 25 Mar 2021 03:24:00 GMT"}, {"version": "v5", "created": "Thu, 29 Apr 2021 14:41:52 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Yang", "Yongxin", ""], ["Cavallaro", "Andrea", ""], ["Xiang", "Tao", ""]]}, {"id": "1910.06840", "submitter": "Marvin Chanc\\'an", "authors": "Marvin Chanc\\'an, Luis Hernandez-Nunez, Ajay Narendra, Andrew B.\n  Barron, Michael Milford", "title": "A Hybrid Compact Neural Architecture for Visual Place Recognition", "comments": "Preprint version of article published in IEEE Robotics and Automation\n  Letters", "journal-ref": "IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 993-1000,\n  April 2020", "doi": "10.1109/LRA.2020.2967324", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  State-of-the-art algorithms for visual place recognition, and related visual\nnavigation systems, can be broadly split into two categories:\ncomputer-science-oriented models including deep learning or image\nretrieval-based techniques with minimal biological plausibility, and\nneuroscience-oriented dynamical networks that model temporal properties\nunderlying spatial navigation in the brain. In this letter, we propose a new\ncompact and high-performing place recognition model that bridges this divide\nfor the first time. Our approach comprises two key neural models of these\ncategories: (1) FlyNet, a compact, sparse two-layer neural network inspired by\nbrain architectures of fruit flies, Drosophila melanogaster, and (2) a\none-dimensional continuous attractor neural network (CANN). The resulting\nFlyNet+CANN network incorporates the compact pattern recognition capabilities\nof our FlyNet model with the powerful temporal filtering capabilities of an\nequally compact CANN, replicating entirely in a hybrid neural implementation\nthe functionality that yields high performance in algorithmic localization\napproaches like SeqSLAM. We evaluate our model, and compare it to three\nstate-of-the-art methods, on two benchmark real-world datasets with small\nviewpoint variations and extreme environmental changes - achieving 87% AUC\nresults under day to night transitions compared to 60% for Multi-Process\nFusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times\nfaster, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 14:58:54 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 10:21:22 GMT"}, {"version": "v3", "created": "Sun, 19 Jan 2020 09:18:47 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Chanc\u00e1n", "Marvin", ""], ["Hernandez-Nunez", "Luis", ""], ["Narendra", "Ajay", ""], ["Barron", "Andrew B.", ""], ["Milford", "Michael", ""]]}, {"id": "1910.06849", "submitter": "Guohao Li", "authors": "Guohao Li, Matthias M\\\"uller, Guocheng Qian, Itzel C. Delgadillo,\n  Abdulellah Abualshour, Ali Thabet, Bernard Ghanem", "title": "DeepGCNs: Making GCNs Go as Deep as CNNs", "comments": "Accepted at TPAMI. This work is a journal extension of our ICCV'19\n  paper arXiv:1904.03751. The first three authors contributed equally", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3074057", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been very successful at solving a\nvariety of computer vision tasks such as object classification and detection,\nsemantic segmentation, activity understanding, to name just a few. One key\nenabling factor for their great performance has been the ability to train very\ndeep networks. Despite their huge success in many tasks, CNNs do not work well\nwith non-Euclidean data, which is prevalent in many real-world applications.\nGraph Convolutional Networks (GCNs) offer an alternative that allows for\nnon-Eucledian data input to a neural network. While GCNs already achieve\nencouraging results, they are currently limited to architectures with a\nrelatively small number of layers, primarily due to vanishing gradients during\ntraining. This work transfers concepts such as residual/dense connections and\ndilated convolutions from CNNs to GCNs in order to successfully train very deep\nGCNs. We show the benefit of using deep GCNs (with as many as 112 layers)\nexperimentally across various datasets and tasks. Specifically, we achieve very\npromising performance in part segmentation and semantic segmentation on point\nclouds and in node classification of protein functions across biological\nprotein-protein interaction (PPI) graphs. We believe that the insights in this\nwork will open avenues for future research on GCNs and their application to\nfurther tasks not explored in this paper. The source code for this work is\navailable at https://github.com/lightaime/deep_gcns_torch and\nhttps://github.com/lightaime/deep_gcns for PyTorch and TensorFlow\nimplementation respectively.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:10:34 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 19:38:05 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 21:35:58 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Guohao", ""], ["M\u00fcller", "Matthias", ""], ["Qian", "Guocheng", ""], ["Delgadillo", "Itzel C.", ""], ["Abualshour", "Abdulellah", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1910.06864", "submitter": "Yuzhe Ou", "authors": "Xujiang Zhao, Yuzhe Ou, Lance Kaplan, Feng Chen, Jin-Hee Cho", "title": "Quantifying Classification Uncertainty using Regularized Evidential\n  Neural Networks", "comments": "Presented at AAAI FSS-19: Artificial Intelligence in Government and\n  Public Sector, Arlington, Virginia, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional deep neural nets (NNs) have shown the state-of-the-art\nperformance in the task of classification in various applications. However, NNs\nhave not considered any types of uncertainty associated with the class\nprobabilities to minimize risk due to misclassification under uncertainty in\nreal life. Unlike Bayesian neural nets indirectly infering uncertainty through\nweight uncertainties, evidential neural networks (ENNs) have been recently\nproposed to support explicit modeling of the uncertainty of class\nprobabilities. It treats predictions of an NN as subjective opinions and learns\nthe function by collecting the evidence leading to these opinions by a\ndeterministic NN from data. However, an ENN is trained as a black box without\nexplicitly considering different types of inherent data uncertainty, such as\nvacuity (uncertainty due to a lack of evidence) or dissonance (uncertainty due\nto conflicting evidence). This paper presents a new approach, called a {\\em\nregularized ENN}, that learns an ENN based on regularizations related to\ndifferent characteristics of inherent data uncertainty. Via the experiments\nwith both synthetic and real-world datasets, we demonstrate that the proposed\nregularized ENN can better learn of an ENN modeling different types of\nuncertainty in the class probabilities for classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:26:20 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Zhao", "Xujiang", ""], ["Ou", "Yuzhe", ""], ["Kaplan", "Lance", ""], ["Chen", "Feng", ""], ["Cho", "Jin-Hee", ""]]}, {"id": "1910.06934", "submitter": "Hichem Sahbi", "authors": "Ahmed Mazari and Hichem Sahbi", "title": "Human Action Recognition with Multi-Laplacian Graph Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are nowadays witnessing a major success in\ndifferent pattern recognition problems. These learning models were basically\ndesigned to handle vectorial data such as images but their extension to\nnon-vectorial and semi-structured data (namely graphs with variable sizes,\ntopology, etc.) remains a major challenge, though a few interesting solutions\nare currently emerging. In this paper, we introduce MLGCN; a novel spectral\nMulti-Laplacian Graph Convolutional Network. The main contribution of this\nmethod resides in a new design principle that learns graph-laplacians as convex\ncombinations of other elementary laplacians each one dedicated to a particular\ntopology of the input graphs. We also introduce a novel pooling operator, on\ngraphs, that proceeds in two steps: context-dependent node expansion is\nachieved, followed by a global average pooling; the strength of this two-step\nprocess resides in its ability to preserve the discrimination power of nodes\nwhile achieving permutation invariance. Experiments conducted on SBU and\nUCF-101 datasets, show the validity of our method for the challenging task of\naction recognition.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:15:55 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Mazari", "Ahmed", ""], ["Sahbi", "Hichem", ""]]}, {"id": "1910.06943", "submitter": "Weijie J. Su", "authors": "Hangfeng He and Weijie J. Su", "title": "The Local Elasticity of Neural Networks", "comments": "To appear in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a phenomenon in neural networks that we refer to as\n\\textit{local elasticity}. Roughly speaking, a classifier is said to be locally\nelastic if its prediction at a feature vector $\\bx'$ is \\textit{not}\nsignificantly perturbed, after the classifier is updated via stochastic\ngradient descent at a (labeled) feature vector $\\bx$ that is\n\\textit{dissimilar} to $\\bx'$ in a certain sense. This phenomenon is shown to\npersist for neural networks with nonlinear activation functions through\nextensive simulations on real-life and synthetic datasets, whereas this is not\nobserved in linear classifiers. In addition, we offer a geometric\ninterpretation of local elasticity using the neural tangent kernel\n\\citep{jacot2018neural}. Building on top of local elasticity, we obtain\npairwise similarity measures between feature vectors, which can be used for\nclustering in conjunction with $K$-means. The effectiveness of the clustering\nalgorithm on the MNIST and CIFAR-10 datasets in turn corroborates the\nhypothesis of local elasticity of neural networks on real-life data. Finally,\nwe discuss some implications of local elasticity to shed light on several\nintriguing aspects of deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:35:30 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 02:04:50 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["He", "Hangfeng", ""], ["Su", "Weijie J.", ""]]}, {"id": "1910.06961", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo", "title": "Tiny Video Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding is a challenging problem with great impact on the\nabilities of autonomous agents working in the real-world. Yet, solutions so far\nhave been computationally intensive, with the fastest algorithms running for\nmore than half a second per video snippet on powerful GPUs. We propose a novel\nidea on video architecture learning - Tiny Video Networks - which automatically\ndesigns highly efficient models for video understanding. The tiny video models\nrun with competitive performance for as low as 37 milliseconds per video on a\nCPU and 10 milliseconds on a standard GPU.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:55:37 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 14:15:55 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 01:25:26 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Angelova", "Anelia", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1910.06962", "submitter": "Jyh-Jing Hwang", "authors": "Jyh-Jing Hwang, Stella X. Yu, Jianbo Shi, Maxwell D. Collins, Tien-Ju\n  Yang, Xiao Zhang, Liang-Chieh Chen", "title": "SegSort: Segmentation by Discriminative Sorting of Segments", "comments": "In ICCV 2019. Webpage & Code:\n  https://jyhjinghwang.github.io/projects/segsort.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all existing deep learning approaches for semantic segmentation tackle\nthis task as a pixel-wise classification problem. Yet humans understand a scene\nnot in terms of pixels, but by decomposing it into perceptual groups and\nstructures that are the basic building blocks of recognition. This motivates us\nto propose an end-to-end pixel-wise metric learning approach that mimics this\nprocess. In our approach, the optimal visual representation determines the\nright segmentation within individual images and associates segments with the\nsame semantic classes across images. The core visual learning problem is\ntherefore to maximize the similarity within segments and minimize the\nsimilarity between segments. Given a model trained this way, inference is\nperformed consistently by extracting pixel-wise embeddings and clustering, with\nthe semantic label determined by the majority vote of its nearest neighbors\nfrom an annotated set.\n  As a result, we present the SegSort, as a first attempt using deep learning\nfor unsupervised semantic segmentation, achieving $76\\%$ performance of its\nsupervised counterpart. When supervision is available, SegSort shows consistent\nimprovements over conventional approaches based on pixel-wise softmax training.\nAdditionally, our approach produces more precise boundaries and consistent\nregion predictions. The proposed SegSort further produces an interpretable\nresult, as each choice of label can be easily understood from the retrieved\nnearest segments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:58:20 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 17:43:04 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Hwang", "Jyh-Jing", ""], ["Yu", "Stella X.", ""], ["Shi", "Jianbo", ""], ["Collins", "Maxwell D.", ""], ["Yang", "Tien-Ju", ""], ["Zhang", "Xiao", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "1910.06988", "submitter": "Rogerio Bonatti", "authors": "Rogerio Bonatti and Wenshan Wang and Cherie Ho and Aayush Ahuja and\n  Mirko Gschwindt and Efe Camci and Erdal Kayacan and Sanjiban Choudhury and\n  Sebastian Scherer", "title": "Autonomous Aerial Cinematography In Unstructured Environments With\n  Learned Artistic Decision-Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial cinematography is revolutionizing industries that require live and\ndynamic camera viewpoints such as entertainment, sports, and security. However,\nsafely piloting a drone while filming a moving target in the presence of\nobstacles is immensely taxing, often requiring multiple expert human operators.\nHence, there is demand for an autonomous cinematographer that can reason about\nboth geometry and scene context in real-time. Existing approaches do not\naddress all aspects of this problem; they either require high-precision\nmotion-capture systems or GPS tags to localize targets, rely on prior maps of\nthe environment, plan for short time horizons, or only follow artistic\nguidelines specified before flight.\n  In this work, we address the problem in its entirety and propose a complete\nsystem for real-time aerial cinematography that for the first time combines:\n(1) vision-based target estimation; (2) 3D signed-distance mapping for\nocclusion estimation; (3) efficient trajectory optimization for long\ntime-horizon camera motion; and (4) learning-based artistic shot selection. We\nextensively evaluate our system both in simulation and in field experiments by\nfilming dynamic targets moving through unstructured environments. Our results\nindicate that our system can operate reliably in the real world without\nrestrictive assumptions. We also provide in-depth analysis and discussions for\neach module, with the hope that our design tradeoffs can generalize to other\nrelated applications. Videos of the complete system can be found at:\nhttps://youtu.be/ookhHnqmlaU.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 18:17:58 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Bonatti", "Rogerio", ""], ["Wang", "Wenshan", ""], ["Ho", "Cherie", ""], ["Ahuja", "Aayush", ""], ["Gschwindt", "Mirko", ""], ["Camci", "Efe", ""], ["Kayacan", "Erdal", ""], ["Choudhury", "Sanjiban", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1910.07038", "submitter": "Avi Ben-Cohen", "authors": "Hussam Lawen, Avi Ben-Cohen, Matan Protter, Itamar Friedman, Lihi\n  Zelnik-Manor", "title": "Compact Network Training for Person ReID", "comments": null, "journal-ref": null, "doi": "10.1145/3372278.3390686", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of person re-identification (ReID) has attracted growing attention\nin recent years leading to improved performance, albeit with little focus on\nreal-world applications. Most SotA methods are based on heavy pre-trained\nmodels, e.g. ResNet50 (~25M parameters), which makes them less practical and\nmore tedious to explore architecture modifications. In this study, we focus on\na small-sized randomly initialized model that enables us to easily introduce\narchitecture and training modifications suitable for person ReID. The outcomes\nof our study are a compact network and a fitting training regime. We show the\nrobustness of the network by outperforming the SotA on both Market1501 and\nDukeMTMC. Furthermore, we show the representation power of our ReID network via\nSotA results on a different task of multi-object tracking.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 20:16:55 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 15:08:35 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 08:17:01 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Lawen", "Hussam", ""], ["Ben-Cohen", "Avi", ""], ["Protter", "Matan", ""], ["Friedman", "Itamar", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1910.07042", "submitter": "Mayoore Jaiswal", "authors": "Mayoore S. Jaiswal, Bumsoo Kang, Jinho Lee, Minsik Cho", "title": "MUTE: Data-Similarity Driven Multi-hot Target Encoding for Neural\n  Network Design", "comments": "NeurIPS Workshop 2019 - Learning with Rich Experience: Integration of\n  Learning Paradigms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target encoding is an effective technique to deliver better performance for\nconventional machine learning methods, and recently, for deep neural networks\nas well. However, the existing target encoding approaches require significant\nincrease in the learning capacity, thus demand higher computation power and\nmore training data. In this paper, we present a novel and efficient target\nencoding scheme, MUTE to improve both generalizability and robustness of a\ntarget model by understanding the inter-class characteristics of a target\ndataset. By extracting the confusion level between the target classes in a\ndataset, MUTE strategically optimizes the Hamming distances among target\nencoding. Such optimized target encoding offers higher classification strength\nfor neural network models with negligible computation overhead and without\nincreasing the model size. When MUTE is applied to the popular image\nclassification networks and datasets, our experimental results show that MUTE\noffers better generalization and defense against the noises and adversarial\nattacks over the existing solutions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 20:23:06 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Jaiswal", "Mayoore S.", ""], ["Kang", "Bumsoo", ""], ["Lee", "Jinho", ""], ["Cho", "Minsik", ""]]}, {"id": "1910.07048", "submitter": "Ke Lei", "authors": "Ke Lei, Morteza Mardani, John M. Pauly, Shreyas S. Vasanawala", "title": "Wasserstein GANs for MR Imaging: from Paired to Unpaired Training", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2020.3022968", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lack of ground-truth MR images impedes the common supervised training of\nneural networks for image reconstruction. To cope with this challenge, this\npaper leverages unpaired adversarial training for reconstruction networks,\nwhere the inputs are undersampled k-space and naively reconstructed images from\none dataset, and the labels are high-quality images from another dataset. The\nreconstruction networks consist of a generator which suppresses the input image\nartifacts, and a discriminator using a pool of (unpaired) labels to adjust the\nreconstruction quality. The generator is an unrolled neural network -- a\ncascade of convolutional and data consistency layers. The discriminator is also\na multilayer CNN that plays the role of a critic scoring the quality of\nreconstructed images based on the Wasserstein distance. Our experiments with\nknee MRI datasets demonstrate that the proposed unpaired training enables\ndiagnostic-quality reconstruction when high-quality image labels are not\navailable for the input types of interest, or when the amount of labels is\nsmall. In addition, our adversarial training scheme can achieve better image\nquality (as rated by expert radiologists) compared with the paired training\nschemes with pixel-wise loss.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 20:47:34 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 05:34:43 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2020 00:50:12 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Lei", "Ke", ""], ["Mardani", "Morteza", ""], ["Pauly", "John M.", ""], ["Vasanawala", "Shreyas S.", ""]]}, {"id": "1910.07067", "submitter": "Mikhail Pautov", "authors": "Mikhail Pautov, Grigorii Melnikov, Edgar Kaziakhmedov, Klim Kireev,\n  Aleksandr Petiushko", "title": "On adversarial patches: real-world attack on ArcFace-100 face\n  recognition system", "comments": null, "journal-ref": "2019 International Multi-Conference on Engineering, Computer and\n  Information Sciences (SIBIRCON)", "doi": "10.1109/SIBIRCON48586.2019.8958134", "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works showed the vulnerability of image classifiers to adversarial\nattacks in the digital domain. However, the majority of attacks involve adding\nsmall perturbation to an image to fool the classifier. Unfortunately, such\nprocedures can not be used to conduct a real-world attack, where adding an\nadversarial attribute to the photo is a more practical approach. In this paper,\nwe study the problem of real-world attacks on face recognition systems. We\nexamine security of one of the best public face recognition systems,\nLResNet100E-IR with ArcFace loss, and propose a simple method to attack it in\nthe physical world. The method suggests creating an adversarial patch that can\nbe printed, added as a face attribute and photographed; the photo of a person\nwith such attribute is then passed to the classifier such that the classifier's\nrecognized class changes from correct to the desired one. Proposed generating\nprocedure allows projecting adversarial patches not only on different areas of\nthe face, such as nose or forehead but also on some wearable accessory, such as\neyeglasses.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 21:49:56 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 12:35:59 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 23:14:52 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Pautov", "Mikhail", ""], ["Melnikov", "Grigorii", ""], ["Kaziakhmedov", "Edgar", ""], ["Kireev", "Klim", ""], ["Petiushko", "Aleksandr", ""]]}, {"id": "1910.07070", "submitter": "Wenqian Ronny Huang", "authors": "W. Ronny Huang, Yike Qi, Qianqian Li, Jonathan Degange", "title": "DeepErase: Weakly Supervised Ink Artifact Removal in Document Text\n  Images", "comments": "Conference paper at WACV 2020. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paper-intensive industries like insurance, law, and government have long\nleveraged optical character recognition (OCR) to automatically transcribe\nhordes of scanned documents into text strings for downstream processing. Even\nin 2019, there are still many scanned documents and mail that come into\nbusinesses in non-digital format. Text to be extracted from real world\ndocuments is often nestled inside rich formatting, such as tabular structures\nor forms with fill-in-the-blank boxes or underlines whose ink often touches or\neven strikes through the ink of the text itself. Further, the text region could\nhave random ink smudges or spurious strokes. Such ink artifacts can severely\ninterfere with the performance of recognition algorithms or other downstream\nprocessing tasks. In this work, we propose DeepErase, a neural-based\npreprocessor to erase ink artifacts from text images. We devise a method to\nprogrammatically assemble real text images and real artifacts into\nrealistic-looking \"dirty\" text images, and use them to train an artifact\nsegmentation network in a weakly supervised manner, since pixel-level\nannotations are automatically obtained during the assembly process. In addition\nto high segmentation accuracy, we show that our cleansed images achieve a\nsignificant boost in recognition accuracy by popular OCR software such as\nTesseract 4.0. Finally, we test DeepErase on out-of-distribution datasets (NIST\nSDB) of scanned IRS tax return forms and achieve double-digit improvements in\naccuracy. All experiments are performed on both printed and handwritten text.\nCode for all experiments is available at https://github.com/yikeqicn/DeepErase\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 21:57:04 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 00:50:27 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 05:35:49 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Huang", "W. Ronny", ""], ["Qi", "Yike", ""], ["Li", "Qianqian", ""], ["Degange", "Jonathan", ""]]}, {"id": "1910.07113", "submitter": "Matthias Plappert", "authors": "OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz\n  Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn\n  Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter\n  Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, Lei Zhang", "title": "Solving Rubik's Cube with a Robot Hand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that models trained only in simulation can be used to solve a\nmanipulation problem of unprecedented complexity on a real robot. This is made\npossible by two key components: a novel algorithm, which we call automatic\ndomain randomization (ADR) and a robot platform built for machine learning. ADR\nautomatically generates a distribution over randomized environments of\never-increasing difficulty. Control policies and vision state estimators\ntrained with ADR exhibit vastly improved sim2real transfer. For control\npolicies, memory-augmented models trained on an ADR-generated distribution of\nenvironments show clear signs of emergent meta-learning at test time. The\ncombination of ADR with our custom robot platform allows us to solve a Rubik's\ncube with a humanoid robot hand, which involves both control and state\nestimation problems. Videos summarizing our results are available:\nhttps://openai.com/blog/solving-rubiks-cube/\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 00:59:05 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["OpenAI", "", ""], ["Akkaya", "Ilge", ""], ["Andrychowicz", "Marcin", ""], ["Chociej", "Maciek", ""], ["Litwin", "Mateusz", ""], ["McGrew", "Bob", ""], ["Petron", "Arthur", ""], ["Paino", "Alex", ""], ["Plappert", "Matthias", ""], ["Powell", "Glenn", ""], ["Ribas", "Raphael", ""], ["Schneider", "Jonas", ""], ["Tezak", "Nikolas", ""], ["Tworek", "Jerry", ""], ["Welinder", "Peter", ""], ["Weng", "Lilian", ""], ["Yuan", "Qiming", ""], ["Zaremba", "Wojciech", ""], ["Zhang", "Lei", ""]]}, {"id": "1910.07122", "submitter": "Hui Xue PhD", "authors": "Hui Xue, Ethan Tseng, Kristopher D Knott, Tushar Kotecha, Louise\n  Brown, Sven Plein, Marianna Fontana, James C Moon, Peter Kellman", "title": "Automated Detection of Left Ventricle in Arterial Input Function Images\n  for Inline Perfusion Mapping using Deep Learning: A study of 15,000 Patients", "comments": "Accepted by Magnetic Resonance in Medicine on March 30, 2020", "journal-ref": null, "doi": "10.1002/mrm.27954", "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of myocardial perfusion has the potential to improve detection\nof regional and global flow reduction. Significant effort has been made to\nautomate the workflow, where one essential step is the arterial input function\n(AIF) extraction. Since failure here invalidates quantification, high accuracy\nis required. For this purpose, this study presents a robust AIF detection\nmethod using the convolutional neural net (CNN) model. CNN models were trained\nby assembling 25,027 scans (N=12,984 patients) from three hospitals, seven\nscanners. A test set of 5,721 scans (N=2,805 patients) evaluated model\nperformance. The 2D+T AIF time series was inputted into CNN. Two variations\nwere investigated: a) Two Classes (2CS) for background and foreground (LV\nmask); b) Three Classes (3CS) for background, foreground LV and RV. Final model\nwas deployed on MR scanners via the Gadgetron InlineAI. Model loading time on\nMR scanner was ~340ms and applying it took ~180ms. The 3CS model successfully\ndetect LV for 99.98% of all test cases (1 failed out of 5,721 cases). The mean\nDice ratio for 3CS was 0.87+/-0.08 with 92.0% of all test cases having Dice\nratio >0.75, while the 2CS model gave lower Dice of 0.82+/-0.22 (P<1e-5).\nExtracted AIF signals using CNN were further compared to manual ground-truth\nfor foot-time, peak-time, first-pass duration, peak value and area-under-curve.\nNo significant differences were found for all features (P>0.2). This study\nproposed, validated, and deployed a robust CNN solution to detect the LV for\nthe extraction of the AIF signal used in fully automated perfusion flow\nmapping. A very large data cohort was assembled and resulting models were\ndeployed to MR scanners for fully inline AI in clinical hospitals.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 01:22:38 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 21:41:36 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Xue", "Hui", ""], ["Tseng", "Ethan", ""], ["Knott", "Kristopher D", ""], ["Kotecha", "Tushar", ""], ["Brown", "Louise", ""], ["Plein", "Sven", ""], ["Fontana", "Marianna", ""], ["Moon", "James C", ""], ["Kellman", "Peter", ""]]}, {"id": "1910.07129", "submitter": "Masanari Kimura", "authors": "Masanari Kimura", "title": "Large-Scale Landslides Detection from Satellite Images with Incomplete\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earthquakes and tropical cyclones cause the suffering of millions of people\naround the world every year. The resulting landslides exacerbate the effects of\nthese disasters. Landslide detection is, therefore, a critical task for the\nprotection of human life and livelihood in mountainous areas. To tackle this\nproblem, we propose a combination of satellite technology and Deep Neural\nNetworks (DNNs). We evaluate the performance of multiple DNN-based methods for\nlandslide detection on actual satellite images of landslide damage. Our\nanalysis demonstrates the potential for a meaningful social impact in terms of\ndisasters and rescue.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 02:04:10 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Kimura", "Masanari", ""]]}, {"id": "1910.07133", "submitter": "Vasil Kolev", "authors": "Vasil Kolev, Todor Cooklev, Fritz Keinert", "title": "Design of a Simple Orthogonal Multiwavelet Filter by Matrix Spectral\n  Factorization", "comments": "This is a preprint of a paper whose final and definite form is\n  published in Circuits, Systems, and Signal Processing,, Springer,\n  https://link.springer.com/article/10.1007/s00034-019-01240-9, ISSN 0278-081X\n  (print), ISSN 1531-5878 (Online)", "journal-ref": null, "doi": "10.1007/s00034-019-01240-9", "report-no": null, "categories": "cs.CV cs.CE cs.NA cs.SY eess.SY math.NA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider the design of an orthogonal symmetric/antisymmetric multiwavelet\nfrom its matrix product filter by matrix spectral factorization (MSF). As a\ntest problem, we construct a simple matrix product filter with desirable\nproperties, and factor it using Bauer's method, which in this case can be done\nin closed form. The corresponding orthogonal multiwavelet function is derived\nusing algebraic techniques which allow symmetry to be considered. This leads to\nthe known orthogonal multiwavelet SA1, which can also be derived directly. We\nalso give a lifting scheme for SA1, investigate the influence of the number of\nsignificant digits in the calculations, and show some numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 02:21:52 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Kolev", "Vasil", ""], ["Cooklev", "Todor", ""], ["Keinert", "Fritz", ""]]}, {"id": "1910.07153", "submitter": "Mingfei Gao", "authors": "Mingfei Gao, Zizhao Zhang, Guo Yu, Sercan O. Arik, Larry S. Davis,\n  Tomas Pfister", "title": "Consistency-based Semi-supervised Active Learning: Towards Minimizing\n  Labeling Cost", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning (AL) combines data labeling and model training to minimize\nthe labeling cost by prioritizing the selection of high value data that can\nbest improve model performance. In pool-based active learning, accessible\nunlabeled data are not used for model training in most conventional methods.\nHere, we propose to unify unlabeled sample selection and model training towards\nminimizing labeling cost, and make two contributions towards that end. First,\nwe exploit both labeled and unlabeled data using semi-supervised learning (SSL)\nto distill information from unlabeled data during the training stage. Second,\nwe propose a consistency-based sample selection metric that is coherent with\nthe training objective such that the selected samples are effective at\nimproving model performance. We conduct extensive experiments on image\nclassification tasks. The experimental results on CIFAR-10, CIFAR-100 and\nImageNet demonstrate the superior performance of our proposed method with\nlimited labeled data, compared to the existing methods and the alternative AL\nand SSL combinations. Additionally, we study an important yet under-explored\nproblem -- \"When can we start learning-based AL selection?\". We propose a\nmeasure that is empirically correlated with the AL target loss and is\npotentially useful for determining the proper starting point of learning-based\nAL methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 03:31:53 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 04:21:15 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Gao", "Mingfei", ""], ["Zhang", "Zizhao", ""], ["Yu", "Guo", ""], ["Arik", "Sercan O.", ""], ["Davis", "Larry S.", ""], ["Pfister", "Tomas", ""]]}, {"id": "1910.07169", "submitter": "Lanlan Liu", "authors": "Lanlan Liu, Michael Muelly, Jia Deng, Tomas Pfister, Li-Jia Li", "title": "Generative Modeling for Small-Data Object Detection", "comments": "Published in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores object detection in the small data regime, where only a\nlimited number of annotated bounding boxes are available due to data rarity and\nannotation expense. This is a common challenge today with machine learning\nbeing applied to many new tasks where obtaining training data is more\nchallenging, e.g. in medical images with rare diseases that doctors sometimes\nonly see once in their life-time. In this work we explore this problem from a\ngenerative modeling perspective by learning to generate new images with\nassociated bounding boxes, and using these for training an object detector. We\nshow that simply training previously proposed generative models does not yield\nsatisfactory performance due to them optimizing for image realism rather than\nobject detection accuracy. To this end we develop a new model with a novel\nunrolling mechanism that jointly optimizes the generative model and a detector\nsuch that the generated images improve the performance of the detector. We show\nthis method outperforms the state of the art on two challenging datasets,\ndisease detection and small data pedestrian detection, improving the average\nprecision on NIH Chest X-ray by a relative 20% and localization accuracy by a\nrelative 50%.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 04:57:25 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Liu", "Lanlan", ""], ["Muelly", "Michael", ""], ["Deng", "Jia", ""], ["Pfister", "Tomas", ""], ["Li", "Li-Jia", ""]]}, {"id": "1910.07192", "submitter": "Yuki Endo", "authors": "Yuki Endo, Yoshihiro Kanamori, Shigeru Kuriyama", "title": "Animating Landscape: Self-Supervised Learning of Decoupled Motion and\n  Appearance for Single-Image Video Synthesis", "comments": "Published at SIGGRAPH Asia 2019 (ACM Transactions on Graphics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic generation of a high-quality video from a single image remains a\nchallenging task despite the recent advances in deep generative models. This\npaper proposes a method that can create a high-resolution, long-term animation\nusing convolutional neural networks (CNNs) from a single landscape image where\nwe mainly focus on skies and waters. Our key observation is that the motion\n(e.g., moving clouds) and appearance (e.g., time-varying colors in the sky) in\nnatural scenes have different time scales. We thus learn them separately and\npredict them with decoupled control while handling future uncertainty in both\npredictions by introducing latent codes. Unlike previous methods that infer\noutput frames directly, our CNNs predict spatially-smooth intermediate data,\ni.e., for motion, flow fields for warping, and for appearance, color transfer\nmaps, via self-supervised learning, i.e., without explicitly-provided ground\ntruth. These intermediate data are applied not to each previous output frame,\nbut to the input image only once for each output frame. This design is crucial\nto alleviate error accumulation in long-term predictions, which is the\nessential problem in previous recurrent approaches. The output frames can be\nlooped like cinemagraph, and also be controlled directly by specifying latent\ncodes or indirectly via visual annotations. We demonstrate the effectiveness of\nour method through comparisons with the state-of-the-arts on video prediction\nas well as appearance manipulation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 07:20:58 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Endo", "Yuki", ""], ["Kanamori", "Yoshihiro", ""], ["Kuriyama", "Shigeru", ""]]}, {"id": "1910.07234", "submitter": "Anis Koubaa", "authors": "Adel Ammar, Anis Koubaa, Mohanned Ahmed, Abdulrahman Saad", "title": "Aerial Images Processing for Car Detection using Convolutional Neural\n  Networks: Comparison between Faster R-CNN and YoloV3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of car detection from aerial images\nusing Convolutional Neural Networks (CNN). This problem presents additional\nchallenges as compared to car (or any object) detection from ground images\nbecause features of vehicles from aerial images are more difficult to discern.\nTo investigate this issue, we assess the performance of two state-of-the-art\nCNN algorithms, namely Faster R-CNN, which is the most popular region-based\nalgorithm, and YOLOv3, which is known to be the fastest detection algorithm. We\nanalyze two datasets with different characteristics to check the impact of\nvarious factors, such as UAV's altitude, camera resolution, and object size. A\ntotal of 39 training experiments were conducted to account for the effect of\ndifferent hyperparameter values. The objective of this work is to conduct the\nmost robust and exhaustive comparison between these two cutting-edge algorithms\non the specific domain of aerial images. By using a variety of metrics, we show\nthat YOLOv3 yields better performance in most configurations, except that it\nexhibits a lower recall and less confident detections when object sizes and\nscales in the testing dataset differ largely from those in the training\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 09:25:35 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 20:11:47 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ammar", "Adel", ""], ["Koubaa", "Anis", ""], ["Ahmed", "Mohanned", ""], ["Saad", "Abdulrahman", ""]]}, {"id": "1910.07236", "submitter": "Nikolay Jetchev", "authors": "Nikolay Jetchev, Urs Bergmann, G\\\"okhan Yildirim", "title": "Transform the Set: Memory Attentive Generation of Guided and Unguided\n  Image Collages", "comments": "To be presented at the NeurIPS 2019 workshop on Creativity and AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cutting and pasting image segments feels intuitive: the choice of source\ntemplates gives artists flexibility in recombining existing source material.\nFormally, this process takes an image set as input and outputs a collage of the\nset elements. Such selection from sets of source templates does not fit easily\nin classical convolutional neural models requiring inputs of fixed size.\nInspired by advances in attention and set-input machine learning, we present a\nnovel architecture that can generate in one forward pass image collages of\nsource templates using set-structured representations. This paper has the\nfollowing contributions: (i) a novel framework for image generation called\nMemory Attentive Generation of Image Collages (MAGIC) which gives artists new\nways to create digital collages; (ii) from the machine-learning perspective, we\nshow a novel Generative Adversarial Networks (GAN) architecture that uses\nSet-Transformer layers and set-pooling to blend sets of random image samples -\na hybrid non-parametric approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 09:28:40 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 10:57:00 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Jetchev", "Nikolay", ""], ["Bergmann", "Urs", ""], ["Yildirim", "G\u00f6khan", ""]]}, {"id": "1910.07328", "submitter": "Anastasia Ingacheva", "authors": "V. Kokhan, M. Grigoriev, A. Buzmakov, V. Uvarov, A. Ingacheva, E.\n  Shvets, M. Chukalina", "title": "Segmentation Criteria in the Problem of Porosity Determination based on\n  CT Scans", "comments": "ICMV 2019, 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Porous materials are widely used in different applications, in particular\nthey are used to create various filters. Their quality depends on parameters\nthat characterize the internal structure such as porosity, permeability and so\non. Computed tomography (CT) allows one to see the internal structure of a\nporous object without destroying it. The result of tomography is a gray image.\nTo evaluate the desired parameters, the image should be segmented. Traditional\nintensity threshold approaches did not reliably produce correct results due to\nlimitations with CT images quality. Errors in the evaluation of characteristics\nof porous materials based on segmented images can lead to the incorrect\nestimation of their quality and consequently to the impossibility of\nexploitation, financial losses and even to accidents. It is difficult to\nperform correctly segmentation due to the strong difference in voxel\nintensities of the reconstructed object and the presence of noise. Image\nfiltering as a preprocessing procedure is used to improve the quality of\nsegmentation. Nevertheless, there is a problem of choosing an optimal filter.\nIn this work, a method for selecting an optimal filter based on attributive\nindicator of porous objects (should be free from 'levitating stones' inside of\npores) is proposed. In this paper, we use real data where beam hardening\nartifacts are removed, which allows us to focus on the noise reduction process\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 13:01:25 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Kokhan", "V.", ""], ["Grigoriev", "M.", ""], ["Buzmakov", "A.", ""], ["Uvarov", "V.", ""], ["Ingacheva", "A.", ""], ["Shvets", "E.", ""], ["Chukalina", "M.", ""]]}, {"id": "1910.07331", "submitter": "Tianchu Guo", "authors": "Tianchu Guo, Yongchao Liu, Hui Zhang, Xiabing Liu, Youngjun Kwak,\n  Byung In Yoo, Jae-Joon Han, Changkyu Choi", "title": "A Generalized and Robust Method Towards Practical Gaze Estimation on\n  Smart Phone", "comments": "Accepted by ICCV 2019 Workshop. Fix the error of the Figure 1 in the\n  camera ready file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze estimation for ordinary smart phone, e.g. estimating where the user is\nlooking at on the phone screen, can be applied in various applications.\nHowever, the widely used appearance-based CNN methods still have two issues for\npractical adoption. First, due to the limited dataset, gaze estimation is very\nlikely to suffer from over-fitting, leading to poor accuracy at run time.\nSecond, the current methods are usually not robust, i.e. their prediction\nresults having notable jitters even when the user is performing gaze fixation,\nwhich degrades user experience greatly. For the first issue, we propose a new\ntolerant and talented (TAT) training scheme, which is an iterative random\nknowledge distillation framework enhanced with cosine similarity pruning and\naligned orthogonal initialization. The knowledge distillation is a tolerant\nteaching process providing diverse and informative supervision. The enhanced\npruning and initialization is a talented learning process prompting the network\nto escape from the local minima and re-born from a better start. For the second\nissue, we define a new metric to measure the robustness of gaze estimator, and\npropose an adversarial training based Disturbance with Ordinal loss (DwO)\nmethod to improve it. The experimental results show that our TAT method\nachieves state-of-the-art performance on GazeCapture dataset, and that our DwO\nmethod improves the robustness while keeping comparable accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 13:15:57 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Guo", "Tianchu", ""], ["Liu", "Yongchao", ""], ["Zhang", "Hui", ""], ["Liu", "Xiabing", ""], ["Kwak", "Youngjun", ""], ["Yoo", "Byung In", ""], ["Han", "Jae-Joon", ""], ["Choi", "Changkyu", ""]]}, {"id": "1910.07344", "submitter": "Micha{\\l} Stypu{\\l}kowski", "authors": "Micha{\\l} Stypu{\\l}kowski, Maciej Zamorski, Maciej Zi\\k{e}ba, Jan\n  Chorowski", "title": "Conditional Invertible Flow for Point Cloud Generation", "comments": "Published in Sets & Partitions Workshop at NeurIPS 2019\n  (https://www.sets.parts/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a novel generative approach for 3D point clouds that\nmakes use of invertible flow-based models. The main idea of the method is to\ntreat a point cloud as a probability density in 3D space that is modeled using\na cloud-specific neural network. To capture the similarity between point clouds\nwe rely on parameter sharing among networks, with each cloud having only a\nsmall embedding vector that defines it. We use invertible flows networks to\ngenerate the individual point clouds, and to regularize the embedding vectors.\nWe evaluate the generative capabilities of the model both in qualitative and\nquantitative manner.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 13:47:05 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Stypu\u0142kowski", "Micha\u0142", ""], ["Zamorski", "Maciej", ""], ["Zi\u0119ba", "Maciej", ""], ["Chorowski", "Jan", ""]]}, {"id": "1910.07360", "submitter": "Carl Chalmers", "authors": "C. Chalmers, P.Fergus, Serge Wich and Aday Curbelo Montanez", "title": "Conservation AI: Live Stream Analysis for the Detection of Endangered\n  Species Using Convolutional Neural Networks and Drone Technology", "comments": "The papaer is 10 pages and contains 11 images and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different species are adversely affected by poaching. In response to\nthis escalating crisis, efforts to stop poaching using hidden cameras, drones\nand DNA tracking have been implemented with varying degrees of success. Limited\nresources, costs and logistical limitations are often the cause of most\nunsuccessful poaching interventions. The study presented in this paper outlines\na flexible and interoperable framework for the automatic detection of animals\nand poaching activity to facilitate early intervention practices. Using a\nrobust deep learning pipeline, a convolutional neural network is trained and\nimplemented to detect rhinos and cars (considered an important tool in poaching\nfor fast access and artefact transportation in natural habitats) in the study,\nthat are found within live video streamed from drones Transfer learning with\nthe Faster RCNN Resnet 101 is performed to train a custom model with 350 images\nof rhinos and 350 images of cars. Inference is performed using a frame sampling\ntechnique to address the required trade-off control precision and processing\nspeed and maintain synchronisation with the live feed. Inference models are\nhosted on a web platform using flask web serving, OpenCV and TensorFlow 1.13.\nVideo streams are transmitted from a DJI Mavic Pro 2 drone using the Real-Time\nMessaging Protocol (RMTP). The best trained Faster RCNN model achieved a mAP of\n0.83 @IOU 0.50 and 0.69 @IOU 0.75 respectively. In comparison an\nSSD-mobilenetmodel trained under the same experimental conditions achieved a\nmAP of 0.55 @IOU .50 and 0.27 @IOU 0.75.The results demonstrate that using a\nFRCNN and off-the-shelf drones is a promising and scalable option for a range\nof conservation projects.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:11:24 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Chalmers", "C.", ""], ["Fergus", "P.", ""], ["Wich", "Serge", ""], ["Montanez", "Aday Curbelo", ""]]}, {"id": "1910.07373", "submitter": "Cristina Gonzalez-Gonzalo", "authors": "Cristina Gonz\\'alez-Gonzalo, Bart Liefers, Bram van Ginneken, Clara I.\n  S\\'anchez", "title": "Iterative augmentation of visual evidence for weakly-supervised lesion\n  localization in deep interpretability frameworks", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging. Available online 18 May 2020", "doi": "10.1109/TMI.2020.2994463", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability of deep learning (DL) systems is gaining attention in\nmedical imaging to increase experts' trust in the obtained predictions and\nfacilitate their integration in clinical settings. We propose a deep\nvisualization method to generate interpretability of DL classification tasks in\nmedical imaging by means of visual evidence augmentation. The proposed method\niteratively unveils abnormalities based on the prediction of a classifier\ntrained only with image-level labels. For each image, initial visual evidence\nof the prediction is extracted with a given visual attribution technique. This\nprovides localization of abnormalities that are then removed through selective\ninpainting. We iteratively apply this procedure until the system considers the\nimage as normal. This yields augmented visual evidence, including less\ndiscriminative lesions which were not detected at first but should be\nconsidered for final diagnosis. We apply the method to grading of two retinal\ndiseases in color fundus images: diabetic retinopathy (DR) and age-related\nmacular degeneration (AMD). We evaluate the generated visual evidence and the\nperformance of weakly-supervised localization of different types of DR and AMD\nabnormalities, both qualitatively and quantitatively. We show that the\naugmented visual evidence of the predictions highlights the biomarkers\nconsidered by the experts for diagnosis and improves the final localization\nperformance. It results in a relative increase of 11.2$\\pm$2.0% per image\nregarding average sensitivity per average 10 false positives, when applied to\ndifferent classification tasks, visual attribution techniques and network\narchitectures. This makes the proposed method a useful tool for exhaustive\nvisual support of DL classifiers in medical imaging.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:30:47 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Gonz\u00e1lez-Gonzalo", "Cristina", ""], ["Liefers", "Bart", ""], ["van Ginneken", "Bram", ""], ["S\u00e1nchez", "Clara I.", ""]]}, {"id": "1910.07395", "submitter": "Azadeh Nazemi", "authors": "Azadeh Nazemi, Niloofar Tavakolian, Donal Fitzpatrick, Chandrik a\n  Fernando, Ching Y. Suen", "title": "Offline handwritten mathematical symbol recognition utilising deep\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes an approach for offline recognition of handwritten\nmathematical symbols. The process of symbol recognition in this paper includes\nsymbol segmentation and accurate classification for over 300 classes. Many\nmultidimensional mathematical symbols need both horizontal and vertical\nprojection to be segmented. However, some symbols do not permit to be projected\nand stop segmentation, such as the root symbol. Besides, many mathematical\nsymbols are structurally similar, specifically in handwritten such as 0 and\nnull. There are more than 300 Mathematical symbols. Therefore, designing an\naccurate classifier for more than 300 classes is required. This paper initially\naddresses the issue regarding segmentation using Simple Linear Iterative\nClustering (SLIC). Experimental results indicate that the accuracy of the\ndesigned kNN classifier is 84% for salient, 57% Histogram of Oriented Gradient\n(HOG), 53% for Linear Binary Pattern (LBP) and finally 43% for pixel intensity\nof raw image for 66 classes. 87 classes using modified LeNet represents 90%\naccuracy. Finally, for 101 classes, SqueezeNet ac\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:02:07 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Nazemi", "Azadeh", ""], ["Tavakolian", "Niloofar", ""], ["Fitzpatrick", "Donal", ""], ["Fernando", "Chandrik a", ""], ["Suen", "Ching Y.", ""]]}, {"id": "1910.07416", "submitter": "Sadaf Gulshad", "authors": "Sadaf Gulshad, Zeynep Akata, Jan Hendrik Metzen, and Arnold Smeulders", "title": "Understanding Misclassifications by Attributes", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.08279", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to understand and explain the decisions of deep neural\nnetworks by studying the behavior of predicted attributes when adversarial\nexamples are introduced. We study the changes in attributes for clean as well\nas adversarial images in both standard and adversarially robust networks. We\npropose a metric to quantify the robustness of an adversarially robust network\nagainst adversarial attacks. In a standard network, attributes predicted for\nadversarial images are consistent with the wrong class, while attributes\npredicted for the clean images are consistent with the true class. In an\nadversarially robust network, the attributes predicted for adversarial images\nclassified correctly are consistent with the true class. Finally, we show that\nthe ability to robustify a network varies for different datasets. For the fine\ngrained dataset, it is higher as compared to the coarse-grained dataset.\nAdditionally, the ability to robustify a network increases with the increase in\nadversarial noise.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 09:36:23 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Gulshad", "Sadaf", ""], ["Akata", "Zeynep", ""], ["Metzen", "Jan Hendrik", ""], ["Smeulders", "Arnold", ""]]}, {"id": "1910.07423", "submitter": "Vishnu Naresh Boddeti", "authors": "Bashir Sadeghi, Runyi Yu, Vishnu Naresh Boddeti", "title": "On the Global Optima of Kernelized Adversarial Representation Learning", "comments": "Accepted for publication at ICCV 2019. This version includes\n  additional theoretical and experimental analysis. Minor update to the GMM\n  experiment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial representation learning is a promising paradigm for obtaining\ndata representations that are invariant to certain sensitive attributes while\nretaining the information necessary for predicting target attributes. Existing\napproaches solve this problem through iterative adversarial minimax\noptimization and lack theoretical guarantees. In this paper, we first study the\n\"linear\" form of this problem i.e., the setting where all the players are\nlinear functions. We show that the resulting optimization problem is both\nnon-convex and non-differentiable. We obtain an exact closed-form expression\nfor its global optima through spectral learning and provide performance\nguarantees in terms of analytical bounds on the achievable utility and\ninvariance. We then extend this solution and analysis to non-linear functions\nthrough kernel representation. Numerical experiments on UCI, Extended Yale B\nand CIFAR-100 datasets indicate that, (a) practically, our solution is ideal\nfor \"imparting\" provable invariance to any biased pre-trained data\nrepresentation, and (b) empirically, the trade-off between utility and\ninvariance provided by our solution is comparable to iterative minimax\noptimization of existing deep neural network based approaches. Code is\navailable at https://github.com/human-analysis/Kernel-ARL\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:37:14 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 16:41:09 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Sadeghi", "Bashir", ""], ["Yu", "Runyi", ""], ["Boddeti", "Vishnu Naresh", ""]]}, {"id": "1910.07428", "submitter": "Weixing Chen", "authors": "W.X. Chen, X.Y. Cui, J. Zheng, J.M. Zhang, S. Chen and Y.D. Yao", "title": "Gaze Gestures and Their Applications in human-computer interaction with\n  a head-mounted display", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A head-mounted display (HMD) is a portable and interactive display device.\nWith the development of 5G technology, it may become a general-purpose\ncomputing platform in the future. Human-computer interaction (HCI) technology\nfor HMDs has also been of significant interest in recent years. In addition to\ntracking gestures and speech, tracking human eyes as a means of interaction is\nhighly effective. In this paper, we propose two UnityEyes-based convolutional\nneural network models, UEGazeNet and UEGazeNet*, which can be used for input\nimages with low resolution and high resolution, respectively. These models can\nperform rapid interactions by classifying gaze trajectories (GTs), and a\nGTgestures dataset containing data for 10,200 \"eye-painting gestures\" collected\nfrom 15 individuals is established with our gaze-tracking method. We evaluated\nthe performance both indoors and outdoors and the UEGazeNet can obtaine results\n52\\% and 67\\% better than those of state-of-the-art networks. The\ngeneralizability of our GTgestures dataset using a variety of gaze-tracking\nmodels is evaluated, and an average recognition rate of 96.71\\% is obtained by\nour method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:42:10 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Chen", "W. X.", ""], ["Cui", "X. Y.", ""], ["Zheng", "J.", ""], ["Zhang", "J. M.", ""], ["Chen", "S.", ""], ["Yao", "Y. D.", ""]]}, {"id": "1910.07470", "submitter": "Syed Anwar", "authors": "Syed Muhammad Anwar, Tooba Altaf, Khola Rafique, Harish RaviPrakash,\n  Hassan Mohy-ud-Din and Ulas Bagci", "title": "A Survey on Recent Advancements for AI Enabled Radiomics in\n  Neuro-Oncology", "comments": "Accepted in MICCAI RNO workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) enabled radiomics has evolved immensely\nespecially in the field of oncology. Radiomics provide assistancein diagnosis\nof cancer, planning of treatment strategy, and predictionof survival. Radiomics\nin neuro-oncology has progressed significantly inthe recent past. Deep learning\nhas outperformed conventional machinelearning methods in most image-based\napplications. Convolutional neu-ral networks (CNNs) have seen some popularity\nin radiomics, since theydo not require hand-crafted features and can\nautomatically extract fea-tures during the learning process. In this regard, it\nis observed that CNNbased radiomics could provide state-of-the-art results in\nneuro-oncology,similar to the recent success of such methods in a wide spectrum\nofmedical image analysis applications. Herein we present a review of the most\nrecent best practices and establish the future trends for AI enabled radiomics\nin neuro-oncology.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:50:02 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Anwar", "Syed Muhammad", ""], ["Altaf", "Tooba", ""], ["Rafique", "Khola", ""], ["RaviPrakash", "Harish", ""], ["Mohy-ud-Din", "Hassan", ""], ["Bagci", "Ulas", ""]]}, {"id": "1910.07486", "submitter": "Jonas J\\\"ager", "authors": "Jonas J\\\"ager, Gereon Reus, Joachim Denzler, Viviane Wolff, Klaus\n  Fricke-Neuderth", "title": "LOST: A flexible framework for semi-automatic image annotation", "comments": "Under review at: Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art computer vision approaches rely on huge amounts of annotated\ndata. The collection of such data is a time consuming process since it is\nmainly performed by humans. The literature shows that semi-automatic annotation\napproaches can significantly speed up the annotation process by the automatic\ngeneration of annotation proposals to support the annotator. In this paper we\npresent a framework that allows for a quick and flexible design of\nsemi-automatic annotation pipelines. We show that a good design of the process\nwill speed up the collection of annotations. Our contribution is a new approach\nto image annotation that allows for the combination of different annotation\ntools and machine learning algorithms in one process. We further present\npotential applications of our approach. The source code of our framework called\nLOST (Label Objects and Save Time) is available at:\nhttps://github.com/l3p-cv/lost.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:34:27 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 09:27:34 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["J\u00e4ger", "Jonas", ""], ["Reus", "Gereon", ""], ["Denzler", "Joachim", ""], ["Wolff", "Viviane", ""], ["Fricke-Neuderth", "Klaus", ""]]}, {"id": "1910.07604", "submitter": "Jacob Pfau", "authors": "Jacob Pfau, Albert T. Young, Maria L. Wei, Michael J. Keiser", "title": "Global Saliency: Aggregating Saliency Maps to Assess Dataset Artefact\n  Bias", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-stakes applications of machine learning models, interpretability\nmethods provide guarantees that models are right for the right reasons. In\nmedical imaging, saliency maps have become the standard tool for determining\nwhether a neural model has learned relevant robust features, rather than\nartefactual noise. However, saliency maps are limited to local model\nexplanation because they interpret predictions on an image-by-image basis. We\npropose aggregating saliency globally, using semantic segmentation masks, to\nprovide quantitative measures of model bias across a dataset. To evaluate\nglobal saliency methods, we propose two metrics for quantifying the validity of\nsaliency explanations. We apply the global saliency method to skin lesion\ndiagnosis to determine the effect of artefacts, such as ink, on model bias.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 20:45:19 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 00:14:28 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Pfau", "Jacob", ""], ["Young", "Albert T.", ""], ["Wei", "Maria L.", ""], ["Keiser", "Michael J.", ""]]}, {"id": "1910.07615", "submitter": "Junha Roh", "authors": "Junha Roh, Chris Paxton, Andrzej Pronobis, Ali Farhadi, Dieter Fox", "title": "Conditional Driving from Natural Language Instructions", "comments": "Accepted by the 3rd Conference on Robot Learning, Osaka, Japan (CoRL\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widespread adoption of self-driving cars will depend not only on their safety\nbut largely on their ability to interact with human users. Just like human\ndrivers, self-driving cars will be expected to understand and safely follow\nnatural-language directions that suddenly alter the pre-planned route according\nto user's preference or in presence of ambiguities, particularly in locations\nwith poor or outdated map coverage. To this end, we propose a language-grounded\ndriving agent implementing a hierarchical policy using recurrent layers and\ngated attention. The hierarchical approach enables us to reason both in terms\nof high-level language instructions describing long time horizons and\nlow-level, complex, continuous state/action spaces required for real-time\ncontrol of a self-driving car. We train our policy with conditional imitation\nlearning from realistic language data collected from human drivers and\nnavigators. Through quantitative and interactive experiments within the CARLA\nframework, we show that our model can successfully interpret language\ninstructions and follow them safely, even when generalizing to previously\nunseen environments. Code and video are available at\nhttps://sites.google.com/view/language-grounded-driving.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 21:14:08 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Roh", "Junha", ""], ["Paxton", "Chris", ""], ["Pronobis", "Andrzej", ""], ["Farhadi", "Ali", ""], ["Fox", "Dieter", ""]]}, {"id": "1910.07636", "submitter": "Ruei-Sung Lin", "authors": "Oliver Zhang, Ruei-Sung Lin, Yuchuan Gou", "title": "Optimal Transport Based Generative Autoencoders", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of deep generative modeling is dominated by generative adversarial\nnetworks (GANs). However, the training of GANs often lacks stability, fails to\nconverge, and suffers from model collapse. It takes an assortment of tricks to\nsolve these problems, which may be difficult to understand for those seeking to\napply generative modeling. Instead, we propose two novel generative\nautoencoders, AE-OTtrans and AE-OTgen, which rely on optimal transport instead\nof adversarial training. AE-OTtrans and AEOTgen, unlike VAE and WAE, preserve\nthe manifold of the data; they do not force the latent distribution to match a\nnormal distribution, resulting in greater quality images. AEOTtrans and\nAE-OTgen also produce images of higher diversity compared to their predecessor,\nAE-OT. We show that AE-OTtrans and AE-OTgen surpass GANs in the MNIST and\nFashionMNIST datasets. Furthermore, We show that AE-OTtrans and AE-OTgen do\nstate of the art on the MNIST, FashionMNIST, and CelebA image sets comapred to\nother non-adversarial generative models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 22:01:51 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Zhang", "Oliver", ""], ["Lin", "Ruei-Sung", ""], ["Gou", "Yuchuan", ""]]}, {"id": "1910.07638", "submitter": "Peng Liu", "authors": "Peng Liu, Bin Kong, Zhongyu Li, Shaoting Zhang, Ruogu Fang", "title": "CFEA: Collaborative Feature Ensembling Adaptation for Domain Adaptation\n  in Unsupervised Optic Disc and Cup Segmentation", "comments": null, "journal-ref": "the 22nd International Conference on Medical Image Computing and\n  Computer Assisted Intervention (MICCAI 2019)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks have demonstrated comparable and even better\nperformance with board-certified ophthalmologists in well-annotated datasets.\nHowever, the diversity of retinal imaging devices poses a significant\nchallenge: domain shift, which leads to performance degradation when applying\nthe deep learning models to new testing domains. In this paper, we propose a\nnovel unsupervised domain adaptation framework, called Collaborative Feature\nEnsembling Adaptation (CFEA), to effectively overcome this challenge. Our\nproposed CFEA is an interactive paradigm which presents an exquisite of\ncollaborative adaptation through both adversarial learning and ensembling\nweights. In particular, we simultaneously achieve domain-invariance and\nmaintain an exponential moving average of the historical predictions, which\nachieves a better prediction for the unlabeled data, via ensembling weights\nduring training. Without annotating any sample from the target domain, multiple\nadversarial losses in encoder and decoder layers guide the extraction of\ndomain-invariant features to confuse the domain classifier and meanwhile\nbenefit the ensembling of smoothing weights. Comprehensive experimental results\ndemonstrate that our CFEA model can overcome performance degradation and\noutperform the state-of-the-art methods in segmenting retinal optic disc and\ncup from fundus images. \\textit{Code is available at\n\\url{https://github.com/cswin/AWC}}.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 22:11:16 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Liu", "Peng", ""], ["Kong", "Bin", ""], ["Li", "Zhongyu", ""], ["Zhang", "Shaoting", ""], ["Fang", "Ruogu", ""]]}, {"id": "1910.07640", "submitter": "Yingxin Cao", "authors": "Yeeleng S. Vang, Yingxin Cao, Xiaohui Xie", "title": "A Combined Deep Learning-Gradient Boosting Machine Framework for Fluid\n  Intelligence Prediction", "comments": "Challenge in Adolescent Brain Cognitive Development Neurocognitive\n  Prediction", "journal-ref": "In: Pohl K., Thompson W., Adeli E., Linguraru M. (eds) Adolescent\n  Brain Cognitive Development Neurocognitive Prediction. ABCD-NP 2019. Lecture\n  Notes in Computer Science, vol 11791. Springer, Cham (2019)", "doi": "10.1007/978-3-030-31901-4_1", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ABCD Neurocognitive Prediction Challenge is a community driven\ncompetition asking competitors to develop algorithms to predict fluid\nintelligence score from T1-w MRIs. In this work, we propose a deep learning\ncombined with gradient boosting machine framework to solve this task. We train\na convolutional neural network to compress the high dimensional MRI data and\nlearn meaningful image features by predicting the 123 continuous-valued derived\ndata provided with each MRI. These extracted features are then used to train a\ngradient boosting machine that predicts the residualized fluid intelligence\nscore. Our approach achieved mean square error (MSE) scores of 18.4374,\n68.7868, and 96.1806 for the training, validation, and test set respectively.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 22:32:13 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Vang", "Yeeleng S.", ""], ["Cao", "Yingxin", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1910.07641", "submitter": "Yanjun Fu", "authors": "Wenqiang Xu, Yanjun Fu, Yuchen Luo, Chang Liu, Cewu Lu", "title": "RGB-D Individual Segmentation", "comments": "We found some significant errors which could influence the\n  correctness of this paper, and we still need more time to find our solutions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained recognition task deals with sub-category classification problem,\nwhich is important for real-world applications. In this work, we are\nparticularly interested in the segmentation task on the \\emph{finest-grained}\nlevel, which is specifically named \"individual segmentation\". In other words,\nthe individual-level category has no sub-category under it. Segmentation\nproblem in the individual level reveals some new properties, limited training\ndata for single individual object, unknown background, and difficulty for the\nuse of depth. To address these new problems, we propose a \"Context Less-Aware\"\n(CoLA) pipeline, which produces RGB-D object-predominated images that have less\nbackground context, and enables a scale-aware training and testing with 3D\ninformation. Extensive experiments show that the proposed CoLA strategy largely\noutperforms baseline methods on YCB-Video dataset and our proposed\nSupermarket-10K dataset. Code, trained model and new dataset will be published\nwith this paper.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 22:41:16 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 04:18:39 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Xu", "Wenqiang", ""], ["Fu", "Yanjun", ""], ["Luo", "Yuchen", ""], ["Liu", "Chang", ""], ["Lu", "Cewu", ""]]}, {"id": "1910.07655", "submitter": "Kumar Abhishek", "authors": "Saeid Asgari Taghanaki, Kumar Abhishek, Joseph Paul Cohen, Julien\n  Cohen-Adad, Ghassan Hamarneh", "title": "Deep Semantic Segmentation of Natural and Medical Images: A Review", "comments": "45 pages, 16 figures. Accepted for publication in Springer Artificial\n  Intelligence Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic image segmentation task consists of classifying each pixel of an\nimage into an instance, where each instance corresponds to a class. This task\nis a part of the concept of scene understanding or better explaining the global\ncontext of an image. In the medical image analysis domain, image segmentation\ncan be used for image-guided interventions, radiotherapy, or improved\nradiological diagnostics. In this review, we categorize the leading deep\nlearning-based medical and non-medical image segmentation solutions into six\nmain groups of deep architectural, data synthesis-based, loss function-based,\nsequenced models, weakly supervised, and multi-task methods and provide a\ncomprehensive review of the contributions in each of these groups. Further, for\neach group, we analyze each variant of these groups and discuss the limitations\nof the current approaches and present potential future research directions for\nsemantic image segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 06:35:50 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 18:18:01 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 22:20:36 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Abhishek", "Kumar", ""], ["Cohen", "Joseph Paul", ""], ["Cohen-Adad", "Julien", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1910.07676", "submitter": "Jie Su", "authors": "Jie Su", "title": "Wasserstein Distance Guided Cross-Domain Learning", "comments": "47 pages, Master Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims to generalise a high-performance learner on target\ndomain (non-labelled data) by leveraging the knowledge from source domain (rich\nlabelled data) which comes from a different but related distribution. Assuming\nthe source and target domains data(e.g. images) come from a joint distribution\nbut follow on different marginal distributions, the domain adaptation work aims\nto infer the joint distribution from the source and target domain to learn the\ndomain invariant features. Therefore, in this study, I extend the existing\nstate-of-the-art approach to solve the domain adaptation problem. In\nparticular, I propose a new approach to infer the joint distribution of images\nfrom different distributions, namely Wasserstein Distance Guided Cross-Domain\nLearning (WDGCDL). WDGCDL applies the Wasserstein distance to estimate the\ndivergence between the source and target distribution which provides good\ngradient property and promising generalisation bound. Moreover, to tackle the\ntraining difficulty of the proposed framework, I propose two different training\nschemes for stable training. Qualitative results show that this new approach is\nsuperior to the existing state-of-the-art methods in the standard domain\nadaptation benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:37:09 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Su", "Jie", ""]]}, {"id": "1910.07677", "submitter": "Ruibing Hou", "authors": "Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen", "title": "Cross Attention Network for Few-shot Classification", "comments": "12 pages, 4 figures. NeurIPS 2019 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification aims to recognize unlabeled samples from unseen\nclasses given only few labeled samples. The unseen classes and low-data problem\nmake few-shot classification very challenging. Many existing approaches\nextracted features from labeled and unlabeled samples independently, as a\nresult, the features are not discriminative enough. In this work, we propose a\nnovel Cross Attention Network to address the challenging problems in few-shot\nclassification. Firstly, Cross Attention Module is introduced to deal with the\nproblem of unseen classes. The module generates cross attention maps for each\npair of class feature and query sample feature so as to highlight the target\nobject regions, making the extracted feature more discriminative. Secondly, a\ntransductive inference algorithm is proposed to alleviate the low-data problem,\nwhich iteratively utilizes the unlabeled query set to augment the support set,\nthereby making the class features more representative. Extensive experiments on\ntwo benchmarks show our method is a simple, effective and computationally\nefficient framework and outperforms the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 01:45:58 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Hou", "Ruibing", ""], ["Chang", "Hong", ""], ["Ma", "Bingpeng", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1910.07688", "submitter": "Alireza Tavakkoli", "authors": "Prithul Aniruddha and Nasif Zaman and Alireza Tavakkoli and Stewart\n  Zuckerbrod", "title": "A Parametric Perceptual Deficit Modeling and Diagnostics Framework for\n  Retina Damage using Mixed Reality", "comments": null, "journal-ref": "LNSC. 11845, pp. 1-12 (2019)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age-related Macular Degeneration (AMD) is a progressive visual impairment\naffecting millions of individuals. Since there is no current treatment for the\ndisease, the only means of improving the lives of individuals suffering from\nthe disease is via assistive technologies. In this paper we propose a novel and\neffective methodology to accurately generate a parametric model for the\nperceptual deficit caused by the physiological deterioration of a patient's\nretina due to AMD. Based on the parameters of the model, a mechanism is\ndeveloped to simulate the patient's perception as a result of the disease. This\nsimulation can effectively deliver the perceptual impact and its progression to\nthe patient's eye doctor. In addition, we propose a mixed-reality apparatus and\ninterface to allow the patient recover functional vision and to compensate for\nthe perceptual loss caused by the physiological damage. The results obtained by\nthe proposed approach show the superiority of our framework over the\nstate-of-the-art low-vision systems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 02:54:26 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Aniruddha", "Prithul", ""], ["Zaman", "Nasif", ""], ["Tavakkoli", "Alireza", ""], ["Zuckerbrod", "Stewart", ""]]}, {"id": "1910.07721", "submitter": "Tiancai Wang", "authors": "Tiancai Wang, Rao Muhammad Anwer, Muhammad Haris Khan, Fahad Shahbaz\n  Khan, Yanwei Pang, Ling Shao, Jorma Laaksonen", "title": "Deep Contextual Attention for Human-Object Interaction Detection", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-object interaction detection is an important and relatively new class\nof visual relationship detection tasks, essential for deeper scene\nunderstanding. Most existing approaches decompose the problem into object\nlocalization and interaction recognition. Despite showing progress, these\napproaches only rely on the appearances of humans and objects and overlook the\navailable context information, crucial for capturing subtle interactions\nbetween them. We propose a contextual attention framework for human-object\ninteraction detection. Our approach leverages context by learning\ncontextually-aware appearance features for human and object instances. The\nproposed attention module then adaptively selects relevant instance-centric\ncontext information to highlight image regions likely to contain human-object\ninteractions. Experiments are performed on three benchmarks: V-COCO, HICO-DET\nand HCVRD. Our approach outperforms the state-of-the-art on all datasets. On\nthe V-COCO dataset, our method achieves a relative gain of 4.4% in terms of\nrole mean average precision ($mAP_{role}$), compared to the existing best\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 05:44:46 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Wang", "Tiancai", ""], ["Anwer", "Rao Muhammad", ""], ["Khan", "Muhammad Haris", ""], ["Khan", "Fahad Shahbaz", ""], ["Pang", "Yanwei", ""], ["Shao", "Ling", ""], ["Laaksonen", "Jorma", ""]]}, {"id": "1910.07762", "submitter": "Zengyi Li", "authors": "Zengyi Li, Yubei Chen, Friedrich T. Sommer", "title": "Learning Energy-Based Models in High-Dimensional Spaces with Multi-scale\n  Denoising Score Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-Based Models (EBMs) assign unnormalized log-probability to data\nsamples. This functionality has a variety of applications, such as sample\nsynthesis, data denoising, sample restoration, outlier detection, Bayesian\nreasoning, and many more. But training of EBMs using standard maximum\nlikelihood is extremely slow because it requires sampling from the model\ndistribution. Score matching potentially alleviates this problem. In\nparticular, denoising score matching \\citep{vincent2011connection} has been\nsuccessfully used to train EBMs. Using noisy data samples with one fixed noise\nlevel, these models learn fast and yield good results in data denoising\n\\citep{saremi2019neural}. However, demonstrations of such models in high\nquality sample synthesis of high dimensional data were lacking. Recently,\n\\citet{song2019generative} have shown that a generative model trained by\ndenoising score matching accomplishes excellent sample synthesis, when trained\nwith data samples corrupted with multiple levels of noise. Here we provide\nanalysis and empirical evidence showing that training with multiple noise\nlevels is necessary when the data dimension is high. Leveraging this insight,\nwe propose a novel EBM trained with multi-scale denoising score matching. Our\nmodel exhibits data generation performance comparable to state-of-the-art\ntechniques such as GANs, and sets a new baseline for EBMs. The proposed model\nalso provides density information and performs well in an image inpainting\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 08:21:17 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 04:58:04 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Li", "Zengyi", ""], ["Chen", "Yubei", ""], ["Sommer", "Friedrich T.", ""]]}, {"id": "1910.07766", "submitter": "Sagar Verma", "authors": "Sagar Verma and Pravin Nagar and Divam Gupta and Chetan Arora", "title": "Making Third Person Techniques Recognize First-Person Actions in\n  Egocentric Videos", "comments": "5 pages, ICIP2018,\n  code:https://github.com/sagarverma/ego_action_recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We focus on first-person action recognition from egocentric videos. Unlike\nthird person domain, researchers have divided first-person actions into two\ncategories: involving hand-object interactions and the ones without, and\ndeveloped separate techniques for the two action categories. Further, it has\nbeen argued that traditional cues used for third person action recognition do\nnot suffice, and egocentric specific features, such as head motion and handled\nobjects have been used for such actions. Unlike the state-of-the-art\napproaches, we show that a regular two stream Convolutional Neural Network\n(CNN) with Long Short-Term Memory (LSTM) architecture, having separate streams\nfor objects and motion, can generalize to all categories of first-person\nactions. The proposed approach unifies the feature learned by all action\ncategories, making the proposed architecture much more practical. In an\nimportant observation, we note that the size of the objects visible in the\negocentric videos is much smaller. We show that the performance of the proposed\nmodel improves after cropping and resizing frames to make the size of objects\ncomparable to the size of ImageNet's objects. Our experiments on the standard\ndatasets: GTEA, EGTEA Gaze+, HUJI, ADL, UTE, and Kitchen, proves that our model\nsignificantly outperforms various state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 08:26:23 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Verma", "Sagar", ""], ["Nagar", "Pravin", ""], ["Gupta", "Divam", ""], ["Arora", "Chetan", ""]]}, {"id": "1910.07770", "submitter": "Xingbo Dong", "authors": "Xingbo Dong and Zhe Jin and Andrew Beng Jin Teoh and Massimo\n  Tistarelli and KokSheik Wong", "title": "On the Security Risk of Cancelable Biometrics", "comments": "Submit to PR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, a number of biometric template protection schemes, primarily\nbased on the notion of \"cancelable biometrics\" (CB) have been proposed. An\nideal cancelable biometric algorithm possesses four criteria, i.e.,\nirreversibility, revocability, unlinkability, and performance preservation.\nCancelable biometrics employed an irreversible but distance preserving\ntransform to convert the original biometric templates to the protected\ntemplates. Matching in the transformed domain can be accomplished due to the\nproperty of distance preservation. However, the distance preservation property\ninvites security issues, which are often neglected. In this paper, we analyzed\nthe property of distance preservation in cancelable biometrics, and\nsubsequently, a pre-image attack is launched to break the security of\ncancelable biometrics under the Kerckhoffs's assumption, where the cancelable\nbiometrics algorithm and parameters are known to the attackers. Furthermore, we\nproposed a framework based on mutual information to measure the information\nleakage incurred by the distance preserving transform, and demonstrated that\ninformation leakage is theoretically inevitable. The results examined on face,\niris, and fingerprint revealed that the risks origin from the matching score\ncomputed from the distance/similarity of two cancelable templates jeopardize\nthe security of cancelable biometrics schemes greatly. At the end, we discussed\nthe security and accuracy trade-off and made recommendations against pre-image\nattacks in order to design a secure biometric system.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 08:37:59 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 08:53:35 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 16:56:03 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Dong", "Xingbo", ""], ["Jin", "Zhe", ""], ["Teoh", "Andrew Beng Jin", ""], ["Tistarelli", "Massimo", ""], ["Wong", "KokSheik", ""]]}, {"id": "1910.07778", "submitter": "Sagar Verma", "authors": "Maria Papadomanolaki and Sagar Verma and Maria Vakalopoulou and\n  Siddharth Gupta and Konstantinos Karantzalos", "title": "Detecting Urban Changes with Recurrent Neural Networks from\n  Multitemporal Sentinel-2 Data", "comments": "4 pages, IGARSS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  \\begin{abstract} The advent of multitemporal high resolution data, like the\nCopernicus Sentinel-2, has enhanced significantly the potential of monitoring\nthe earth's surface and environmental dynamics. In this paper, we present a\nnovel deep learning framework for urban change detection which combines\nstate-of-the-art fully convolutional networks (similar to U-Net) for feature\nrepresentation and powerful recurrent networks (such as LSTMs) for temporal\nmodeling. We report our results on the recently publicly available bi-temporal\nOnera Satellite Change Detection (OSCD) Sentinel-2 dataset, enhancing the\ntemporal information with additional images of the same region on different\ndates. Moreover, we evaluate the performance of the recurrent networks as well\nas the use of the additional dates on the unseen test-set using an ensemble\ncross-validation strategy. All the developed models during the validation phase\nhave scored an overall accuracy of more than 95%, while the use of LSTMs and\nfurther temporal information, boost the F1 rate of the change class by an\nadditional 1.5%.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:15:43 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Papadomanolaki", "Maria", ""], ["Verma", "Sagar", ""], ["Vakalopoulou", "Maria", ""], ["Gupta", "Siddharth", ""], ["Karantzalos", "Konstantinos", ""]]}, {"id": "1910.07787", "submitter": "Houwang Zhang", "authors": "Houwang Zhang, Yuan Zhu and Hanying Zheng", "title": "NAMF: A Non-local Adaptive Mean Filter for Salt-and-Pepper Noise Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel algorithm called a non-local adaptive mean filter\n(NAMF) for removing salt-and-pepper (SAP) noise from corrupted images is\npresented. We employ an efficient window detector with adaptive size to detect\nthe noise, the noisy pixel will be replaced by the combination of its\nneighboring pixels, and finally we use a SAP noise based non-local mean filter\nto reconstruct the intensity values of noisy pixels. Extensive experimental\nresults demonstrate that NAMF can obtain better performance in terms of quality\nfor restoring images at all levels of SAP noise.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:31:07 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 15:48:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhang", "Houwang", ""], ["Zhu", "Yuan", ""], ["Zheng", "Hanying", ""]]}, {"id": "1910.07800", "submitter": "Fan-Yun Sun", "authors": "Kuan-Lun Tseng, Winston Hsu, Chun-ting Wu, Ya-Fang Shih, Fan-Yun Sun", "title": "Organ At Risk Segmentation with Multiple Modality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of image segmentation in computer vision, biomedical\nimage segmentation have achieved remarkable progress on brain tumor\nsegmentation and Organ At Risk (OAR) segmentation. However, most of the\nresearch only uses single modality such as Computed Tomography (CT) scans while\nin real world scenario doctors often use multiple modalities to get more\naccurate result. To better leverage different modalities, we have collected a\nlarge dataset consists of 136 cases with CT and MR images which diagnosed with\nnasopharyngeal cancer. In this paper, we propose to use Generative Adversarial\nNetwork to perform CT to MR transformation to synthesize MR images instead of\naligning two modalities. The synthesized MR can be jointly trained with CT to\nachieve better performance. In addition, we use instance segmentation model to\nextend the OAR segmentation task to segment both organs and tumor region. The\ncollected dataset will be made public soon.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 10:01:11 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Tseng", "Kuan-Lun", ""], ["Hsu", "Winston", ""], ["Wu", "Chun-ting", ""], ["Shih", "Ya-Fang", ""], ["Sun", "Fan-Yun", ""]]}, {"id": "1910.07831", "submitter": "Nicolas Pielawski", "authors": "Nicolas Pielawski and Carolina W\\\"ahlby", "title": "Introducing Hann windows for reducing edge-effects in patch-based image\n  segmentation", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0229839", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a limitation in the size of an image that can be processed using\ncomputationally demanding methods such as e.g. Convolutional Neural Networks\n(CNNs). Some imaging modalities - notably biological and medical - can result\nin images up to a few gigapixels in size, meaning that they have to be divided\ninto smaller parts, or patches, for processing. However, when performing image\nsegmentation, this may lead to undesirable artefacts, such as edge effects in\nthe final re-combined image. We introduce windowing methods from signal\nprocessing to effectively reduce such edge effects. With the assumption that\nthe central part of an image patch often holds richer contextual information\nthan its sides and corners, we reconstruct the prediction by overlapping\npatches that are being weighted depending on 2-dimensional windows. We compare\nthe results of four different windows: Hann, Bartlett-Hann, Triangular and a\nrecently proposed window by Cui et al., and show that the cosine-based Hann\nwindow achieves the best improvement as measured by the Structural Similarity\nIndex (SSIM). The proposed windowing method can be used together with any CNN\nmodel for segmentation without any modification and significantly improves\nnetwork predictions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 11:39:16 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Pielawski", "Nicolas", ""], ["W\u00e4hlby", "Carolina", ""]]}, {"id": "1910.07860", "submitter": "Raghav Brahmadesam Venkataramaiyer", "authors": "Raghav Brahmadesam Venkataramaiyer, Subham Kumar, Vinay P. Namboodiri", "title": "Can I teach a robot to replicate a line art", "comments": "9 pages, Accepted for the 2020 Winter Conference on Applications of\n  Computer Vision (WACV '20); Supplementary Video: https://youtu.be/nMt5Dw04XhY", "journal-ref": null, "doi": "10.1109/WACV45572.2020.9093434", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Line art is arguably one of the fundamental and versatile modes of\nexpression. We propose a pipeline for a robot to look at a grayscale line art\nand redraw it. The key novel elements of our pipeline are: a) we propose a\nnovel task of mimicking line drawings, b) to solve the pipeline we modify the\nQuick-draw dataset to obtain supervised training for converting a line drawing\ninto a series of strokes c) we propose a multi-stage segmentation and graph\ninterpretation pipeline for solving the problem. The resultant method has also\nbeen deployed on a CNC plotter as well as a robotic arm. We have trained\nseveral variations of the proposed methods and evaluate these on a dataset\nobtained from Quick-draw. Through the best methods we observe an accuracy of\naround 98% for this task, which is a significant improvement over the baseline\narchitecture we adapted from. This therefore allows for deployment of the\nmethod on robots for replicating line art in a reliable manner. We also show\nthat while the rule-based vectorization methods do suffice for simple drawings,\nit fails for more complicated sketches, unlike our method which generalizes\nwell to more complicated distributions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 12:40:15 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Venkataramaiyer", "Raghav Brahmadesam", ""], ["Kumar", "Subham", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1910.07861", "submitter": "Tom Runia", "authors": "Tom F.H. Runia, Kirill Gavrilyuk, Cees G.M. Snoek, Arnold W.M.\n  Smeulders", "title": "Go with the Flow: Perception-refined Physics Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many of the physical phenomena around us, we have developed sophisticated\nmodels explaining their behavior. Nevertheless, inferring specifics from visual\nobservations is challenging due to the high number of causally underlying\nphysical parameters -- including material properties and external forces. This\npaper addresses the problem of inferring such latent physical properties from\nobservations. Our solution is an iterative refinement procedure with simulation\nat its core. The algorithm gradually updates the physical model parameters by\nrunning a simulation of the observed phenomenon and comparing the current\nsimulation to a real-world observation. The physical similarity is computed\nusing an embedding function that maps physically similar examples to nearby\npoints. As a tangible example, we concentrate on flags curling in the wind -- a\nseemingly simple phenomenon but physically highly involved. Based on its\nunderlying physical model and visual manifestation, we propose an instantiation\nof the embedding function. For this mapping, modeled as a deep network, we\nintroduce a spectral decomposition layer that decomposes a video volume into\nits temporal spectral power and corresponding frequencies. In experiments, we\ndemonstrate our method's ability to recover intrinsic and extrinsic physical\nparameters from both simulated and real-world video.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 12:41:34 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Runia", "Tom F. H.", ""], ["Gavrilyuk", "Kirill", ""], ["Snoek", "Cees G. M.", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1910.07882", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Shuran Song, Hod Lipson, Carl Vondrick", "title": "Visual Hide and Seek", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train embodied agents to play Visual Hide and Seek where a prey must\nnavigate in a simulated environment in order to avoid capture from a predator.\nWe place a variety of obstacles in the environment for the prey to hide behind,\nand we only give the agents partial observations of their environment using an\negocentric perspective. Although we train the model to play this game from\nscratch, experiments and visualizations suggest that the agent learns to\npredict its own visibility in the environment. Furthermore, we quantitatively\nanalyze how agent weaknesses, such as slower speed, effect the learned policy.\nOur results suggest that, although agent weaknesses make the learning problem\nmore challenging, they also cause more useful features to be learned. Our\nproject website is available at: http://www.cs.columbia.edu/\n~bchen/visualhideseek/.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 01:27:09 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Chen", "Boyuan", ""], ["Song", "Shuran", ""], ["Lipson", "Hod", ""], ["Vondrick", "Carl", ""]]}, {"id": "1910.07895", "submitter": "Huiyu Li", "authors": "Huiyu Li, Xiabi Liu, Said Boumaraf, Weihua Liu, Xiaopeng Gong,\n  Xiaohong Ma", "title": "A New Three-stage Curriculum Learning Approach to Deep Network Based\n  Liver Tumor Segmentation", "comments": "5 pages, 3 figures, 1 table, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of liver tumors in medical images is crucial for the\ncomputer-aided diagnosis and therapy. It is a challenging task, since the\ntumors are notoriously small against the background voxels. This paper proposes\na new three-stage curriculum learning approach for training deep networks to\ntackle this small object segmentation problem. The learning in the first stage\nis performed on the whole input to obtain an initial deep network for tumor\nsegmenta-tion. Then the second stage of learning focuses the strength-ening of\ntumor specific features by continuing training the network on the tumor\npatches. Finally, we retrain the net-work on the whole input in the third\nstage, in order that the tumor specific features and the global context can be\ninte-grated ideally under the segmentation objective. Benefitting from the\nproposed learning approach, we only need to em-ploy one single network to\nsegment the tumors directly. We evaluated our approach on the 2017 MICCAI Liver\nTumor Segmentation challenge dataset. In the experiments, our approach exhibits\nsignificant improvement compared with the commonly used cascaded counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 13:29:42 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Li", "Huiyu", ""], ["Liu", "Xiabi", ""], ["Boumaraf", "Said", ""], ["Liu", "Weihua", ""], ["Gong", "Xiaopeng", ""], ["Ma", "Xiaohong", ""]]}, {"id": "1910.07948", "submitter": "Oier Mees", "authors": "Oier Mees, Maxim Tatarchenko, Thomas Brox, Wolfram Burgard", "title": "Self-supervised 3D Shape and Viewpoint Estimation from Single Images for\n  Robotics", "comments": "Accepted at the 2019 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS). Video at\n  https://www.youtube.com/watch?v=oQgHG9JdMP4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convolutional neural network for joint 3D shape prediction and\nviewpoint estimation from a single input image. During training, our network\ngets the learning signal from a silhouette of an object in the input image - a\nform of self-supervision. It does not require ground truth data for 3D shapes\nand the viewpoints. Because it relies on such a weak form of supervision, our\napproach can easily be applied to real-world data. We demonstrate that our\nmethod produces reasonable qualitative and quantitative results on natural\nimages for both shape estimation and viewpoint prediction. Unlike previous\napproaches, our method does not require multiple views of the same object\ninstance in the dataset, which significantly expands the applicability in\npractical robotics scenarios. We showcase it by using the hallucinated shapes\nto improve the performance on the task of grasping real-world objects both in\nsimulation and with a PR2 robot.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 14:55:21 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Mees", "Oier", ""], ["Tatarchenko", "Maxim", ""], ["Brox", "Thomas", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1910.07954", "submitter": "Weilin Huang", "authors": "Linjie Xing and Zhi Tian and Weilin Huang and Matthew R. Scott", "title": "Convolutional Character Networks", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress has been made on developing a unified framework for joint\ntext detection and recognition in natural images, but existing joint models\nwere mostly built on two-stage framework by involving ROI pooling, which can\ndegrade the performance on recognition task. In this work, we propose\nconvolutional character networks, referred as CharNet, which is an one-stage\nmodel that can process two tasks simultaneously in one pass. CharNet directly\noutputs bounding boxes of words and characters, with corresponding character\nlabels. We utilize character as basic element, allowing us to overcome the main\ndifficulty of existing approaches that attempted to optimize text detection\njointly with a RNN-based recognition branch. In addition, we develop an\niterative character detection approach able to transform the ability of\ncharacter detection learned from synthetic data to real-world images. These\ntechnical improvements result in a simple, compact, yet powerful one-stage\nmodel that works reliably on multi-orientation and curved text. We evaluate\nCharNet on three standard benchmarks, where it consistently outperforms the\nstate-of-the-art approaches [25, 24] by a large margin, e.g., with improvements\nof 65.33%->71.08% (with generic lexicon) on ICDAR 2015, and 54.0%->69.23% on\nTotal-Text, on end-to-end text recognition. Code is available at:\nhttps://github.com/MalongTech/research-charnet.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 15:01:00 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Xing", "Linjie", ""], ["Tian", "Zhi", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""]]}, {"id": "1910.07972", "submitter": "Max Argus", "authors": "Lukas Hermann, Max Argus, Andreas Eitel, Artemij Amiranashvili,\n  Wolfram Burgard, Thomas Brox", "title": "Adaptive Curriculum Generation from Demonstrations for Sim-to-Real\n  Visuomotor Control", "comments": "Accepted at the 2020 IEEE International Conference on Robotics and\n  Automation (ICRA). Project page see\n  https://lmb.informatik.uni-freiburg.de/projects/curriculum/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Adaptive Curriculum Generation from Demonstrations (ACGD) for\nreinforcement learning in the presence of sparse rewards. Rather than designing\nshaped reward functions, ACGD adaptively sets the appropriate task difficulty\nfor the learner by controlling where to sample from the demonstration\ntrajectories and which set of simulation parameters to use. We show that\ntraining vision-based control policies in simulation while gradually increasing\nthe difficulty of the task via ACGD improves the policy transfer to the real\nworld. The degree of domain randomization is also gradually increased through\nthe task difficulty. We demonstrate zero-shot transfer for two real-world\nmanipulation tasks: pick-and-stow and block stacking. A video showing the\nresults can be found at\nhttps://lmb.informatik.uni-freiburg.de/projects/curriculum/\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 15:33:03 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 10:49:36 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 15:44:10 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Hermann", "Lukas", ""], ["Argus", "Max", ""], ["Eitel", "Andreas", ""], ["Amiranashvili", "Artemij", ""], ["Burgard", "Wolfram", ""], ["Brox", "Thomas", ""]]}, {"id": "1910.08041", "submitter": "Ajay Jain", "authors": "Ajay Jain, Sergio Casas, Renjie Liao, Yuwen Xiong, Song Feng, Sean\n  Segal, Raquel Urtasun", "title": "Discrete Residual Flow for Probabilistic Pedestrian Behavior Prediction", "comments": "CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving vehicles plan around both static and dynamic objects, applying\npredictive models of behavior to estimate future locations of the objects in\nthe environment. However, future behavior is inherently uncertain, and models\nof motion that produce deterministic outputs are limited to short timescales.\nParticularly difficult is the prediction of human behavior. In this work, we\npropose the discrete residual flow network (DRF-Net), a convolutional neural\nnetwork for human motion prediction that captures the uncertainty inherent in\nlong-range motion forecasting. In particular, our learned network effectively\ncaptures multimodal posteriors over future human motion by predicting and\nupdating a discretized distribution over spatial locations. We compare our\nmodel against several strong competitors and show that our model outperforms\nall baselines.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:10:28 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Jain", "Ajay", ""], ["Casas", "Sergio", ""], ["Liao", "Renjie", ""], ["Xiong", "Yuwen", ""], ["Feng", "Song", ""], ["Segal", "Sean", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1910.08051", "submitter": "Yogesh Balaji", "authors": "Yogesh Balaji, Tom Goldstein, Judy Hoffman", "title": "Instance adaptive adversarial training: Improved accuracy tradeoffs in\n  neural nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is by far the most successful strategy for improving\nrobustness of neural networks to adversarial attacks. Despite its success as a\ndefense mechanism, adversarial training fails to generalize well to unperturbed\ntest set. We hypothesize that this poor generalization is a consequence of\nadversarial training with uniform perturbation radius around every training\nsample. Samples close to decision boundary can be morphed into a different\nclass under a small perturbation budget, and enforcing large margins around\nthese samples produce poor decision boundaries that generalize poorly.\nMotivated by this hypothesis, we propose instance adaptive adversarial training\n-- a technique that enforces sample-specific perturbation margins around every\ntraining sample. We show that using our approach, test accuracy on unperturbed\nsamples improve with a marginal drop in robustness. Extensive experiments on\nCIFAR-10, CIFAR-100 and Imagenet datasets demonstrate the effectiveness of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:24:22 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Balaji", "Yogesh", ""], ["Goldstein", "Tom", ""], ["Hoffman", "Judy", ""]]}, {"id": "1910.08055", "submitter": "Neeraj Matiyali", "authors": "Neeraj Matiyali, Gaurav Sharma", "title": "Video Person Re-Identification using Learned Clip Similarity Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging task of video-based person re-identification.\nRecent works have shown that splitting the video sequences into clips and then\naggregating clip based similarity is appropriate for the task. We show that\nusing a learned clip similarity aggregation function allows filtering out hard\nclip pairs, e.g. where the person is not clearly visible, is in a challenging\npose, or where the poses in the two clips are too different to be informative.\nThis allows the method to focus on clip-pairs which are more informative for\nthe task. We also introduce the use of 3D CNNs for video-based\nre-identification and show their effectiveness by performing equivalent to\nprevious works, which use optical flow in addition to RGB, while using RGB\ninputs only. We give quantitative results on three challenging public\nbenchmarks and show better or competitive performance. We also validate our\nmethod qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:34:27 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Matiyali", "Neeraj", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1910.08060", "submitter": "Luiz Gustavo Hafemann", "authors": "Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira", "title": "Meta-learning for fast classifier adaptation to new users of Signature\n  Verification systems", "comments": "Accepted for the IEEE Transactions on Information Forensics and\n  Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline Handwritten Signature verification presents a challenging Pattern\nRecognition problem, where only knowledge of the positive class is available\nfor training. While classifiers have access to a few genuine signatures for\ntraining, during generalization they also need to discriminate forgeries. This\nis particularly challenging for skilled forgeries, where a forger practices\nimitating the user's signature, and often is able to create forgeries visually\nclose to the original signatures. Most work in the literature address this\nissue by training for a surrogate objective: discriminating genuine signatures\nof a user and random forgeries (signatures from other users). In this work, we\npropose a solution for this problem based on meta-learning, where there are two\nlevels of learning: a task-level (where a task is to learn a classifier for a\ngiven user) and a meta-level (learning across tasks). In particular, the\nmeta-learner guides the adaptation (learning) of a classifier for each user,\nwhich is a lightweight operation that only requires genuine signatures. The\nmeta-learning procedure learns what is common for the classification across\ndifferent users. In a scenario where skilled forgeries from a subset of users\nare available, the meta-learner can guide classifiers to be discriminative of\nskilled forgeries even if the classifiers themselves do not use skilled\nforgeries for learning. Experiments conducted on the GPDS-960 dataset show\nimproved performance compared to Writer-Independent systems, and achieve\nresults comparable to state-of-the-art Writer-Dependent systems in the regime\nof few samples per user (5 reference signatures).\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:46:03 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Hafemann", "Luiz G.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1910.08071", "submitter": "Shadrokh Samavi", "authors": "Mahdi Ahmadi, Nader Karimi, Shadrokh Samavi", "title": "Context-Aware Saliency Detection for Image Retargeting Using\n  Convolutional Neural Networks", "comments": "20 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retargeting is the task of making images capable of being displayed on\nscreens with different sizes. This work should be done so that high-level\nvisual information and low-level features such as texture remain as intact as\npossible to the human visual system, while the output image may have different\ndimensions. Thus, simple methods such as scaling and cropping are not adequate\nfor this purpose. In recent years, researchers have tried to improve the\nexisting retargeting methods and introduce new ones. However, a specific method\ncannot be utilized to retarget all types of images. In other words, different\nimages require different retargeting methods. Image retargeting has a close\nrelationship to image saliency detection, which is relatively a new image\nprocessing task. Earlier saliency detection methods were based on local and\nglobal but low-level image information. These methods are called bottom-up\nmethods. On the other hand, newer approaches are top-down and mixed methods\nthat consider the high level and semantic information of the image too. In this\npaper, we introduce the proposed methods in both saliency detection and\nretargeting. For the saliency detection, the use of image context and semantic\nsegmentation are examined, and a novel mixed bottom-up, and top-down saliency\ndetection method is introduced. After saliency detection, a modified version of\nan existing retargeting method is utilized for retargeting the images. The\nresults suggest that the proposed image retargeting pipeline has excellent\nperformance compared to other tested methods. Also, the subjective evaluations\non the Pascal dataset can be used as a retargeting quality assessment dataset\nfor further research.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:59:46 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Ahmadi", "Mahdi", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1910.08103", "submitter": "Alex Georges", "authors": "Jacek Cyranka, Alexander Georges, David Meyer", "title": "Mapper Based Classifier", "comments": "12 pages, accepted to IEEE ICMLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis aims to extract topological quantities from data,\nwhich tend to focus on the broader global structure of the data rather than\nlocal information. The Mapper method, specifically, generalizes clustering\nmethods to identify significant global mathematical structures, which are out\nof reach of many other approaches. We propose a classifier based on applying\nthe Mapper algorithm to data projected onto a latent space. We obtain the\nlatent space by using PCA or autoencoders. Notably, a classifier based on the\nMapper method is immune to any gradient based attack, and improves robustness\nover traditional CNNs (convolutional neural networks). We report theoretical\njustification and some numerical experiments that confirm our claims.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:28:01 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 15:32:25 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Cyranka", "Jacek", ""], ["Georges", "Alexander", ""], ["Meyer", "David", ""]]}, {"id": "1910.08108", "submitter": "Anindya Sarkar", "authors": "Anindya Sarkar, Nikhil Kumar Gupta and Raghu Iyengar", "title": "Enforcing Linearity in DNN succours Robustness and Adversarial Image\n  Generation", "comments": "Adversarial Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on the adversarial vulnerability of neural networks have shown\nthat models trained with the objective of minimizing an upper bound on the\nworst-case loss over all possible adversarial perturbations improve robustness\nagainst adversarial attacks. Beside exploiting adversarial training framework,\nwe show that by enforcing a Deep Neural Network (DNN) to be linear in\ntransformed input and feature space improves robustness significantly. We also\ndemonstrate that by augmenting the objective function with Local Lipschitz\nregularizer boost robustness of the model further. Our method outperforms most\nsophisticated adversarial training methods and achieves state of the art\nadversarial accuracy on MNIST, CIFAR10 and SVHN dataset. In this paper, we also\npropose a novel adversarial image generation method by leveraging Inverse\nRepresentation Learning and Linearity aspect of an adversarially trained deep\nneural network classifier.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:38:40 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 17:00:50 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Sarkar", "Anindya", ""], ["Gupta", "Nikhil Kumar", ""], ["Iyengar", "Raghu", ""]]}, {"id": "1910.08131", "submitter": "Lukas Murmann", "authors": "Lukas Murmann, Michael Gharbi, Miika Aittala, Fredo Durand", "title": "A Dataset of Multi-Illumination Images in the Wild", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collections of images under a single, uncontrolled illumination have enabled\nthe rapid advancement of core computer vision tasks like classification,\ndetection, and segmentation. But even with modern learning techniques, many\ninverse problems involving lighting and material understanding remain too\nseverely ill-posed to be solved with single-illumination datasets. To fill this\ngap, we introduce a new multi-illumination dataset of more than 1000 real\nscenes, each captured under 25 lighting conditions. We demonstrate the richness\nof this dataset by training state-of-the-art models for three challenging\napplications: single-image illumination estimation, image relighting, and\nmixed-illuminant white balance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 19:49:11 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Murmann", "Lukas", ""], ["Gharbi", "Michael", ""], ["Aittala", "Miika", ""], ["Durand", "Fredo", ""]]}, {"id": "1910.08138", "submitter": "Helmut Mayer", "authors": "Helmut Mayer", "title": "RPBA -- Robust Parallel Bundle Adjustment Based on Covariance\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core component of all Structure from Motion (SfM) approaches is bundle\nadjustment. As the latter is a computational bottleneck for larger blocks,\nparallel bundle adjustment has become an active area of research. Particularly,\nconsensus-based optimization methods have been shown to be suitable for this\ntask. We have extended them using covariance information derived by the\nadjustment of individual three-dimensional (3D) points, i.e., \"triangulation\"\nor \"intersection\". This does not only lead to a much better convergence\nbehavior, but also avoids fiddling with the penalty parameter of standard\nconsensus-based approaches. The corresponding novel approach can also be seen\nas a variant of resection / intersection schemes, where we adjust during\nintersection a number of sub-blocks directly related to the number of threads\navailable on a computer each containing a fraction of the cameras of the block.\nWe show that our novel approach is suitable for robust parallel bundle\nadjustment and demonstrate its capabilities in comparison to the basic\nconsensus-based approach as well as a state-of-the-art parallel implementation\nof bundle adjustment. Code for our novel approach is available on GitHub:\nhttps://github.com/helmayer/RPBA\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 20:00:32 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Mayer", "Helmut", ""]]}, {"id": "1910.08157", "submitter": "Thomas Booth", "authors": "Thomas Booth", "title": "An Update on Machine Learning in Neuro-oncology Diagnostics", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.07440", "journal-ref": null, "doi": "10.1007/978-3-030-11723-8_4", "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging biomarkers in neuro-oncology are used for diagnosis, prognosis and\ntreatment response monitoring. Magnetic resonance imaging is typically used\nthroughout the patient pathway because routine structural imaging provides\ndetailed anatomical and pathological information and advanced techniques\nprovide additional physiological detail. Following image feature extraction,\nmachine learning allows accurate classification in a variety of scenarios.\nMachine learning also enables image feature extraction de novo although the low\nprevalence of brain tumours makes such approaches challenging. Much research is\napplied to determining molecular profiles, histological tumour grade and\nprognosis at the time that patients first present with a brain tumour.\nFollowing treatment, differentiating a treatment response from a post-treatment\nrelated effect is clinically important and also an area of study. Most of the\nevidence is low level having been obtained retrospectively and in single\ncentres.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 08:57:51 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Booth", "Thomas", ""]]}, {"id": "1910.08168", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro", "title": "Deep Sub-Ensembles for Fast Uncertainty Estimation in Image\n  Classification", "comments": "7 pages, 8 figures, Bayesian Deep Learning Workshop 2019 @ NeurIPS\n  2019, camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast estimates of model uncertainty are required for many robust robotics\napplications. Deep Ensembles provides state of the art uncertainty without\nrequiring Bayesian methods, but still it is computationally expensive. In this\npaper we propose deep sub-ensembles, an approximation to deep ensembles where\nthe core idea is to ensemble only the layers close to the output, and not the\nwhole model. With ResNet-20 on the CIFAR10 dataset, we obtain 1.5-2.5 speedup\nover a Deep Ensemble, with a small increase in error and NLL, and similarly up\nto 5-15 speedup with a VGG-like network on the SVHN dataset. Our results show\nthat this idea enables a trade-off between error and uncertainty quality versus\ncomputational performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 21:07:40 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 12:23:45 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Valdenegro-Toro", "Matias", ""]]}, {"id": "1910.08173", "submitter": "Gnana Praveen Rajasekar", "authors": "Gnana Praveen R, Eric Granger, Patrick Cardinal", "title": "Deep Weakly-Supervised Domain Adaptation for Pain Localization in Videos", "comments": "Accepted in FG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic pain assessment has an important potential diagnostic value for\npopulations that are incapable of articulating their pain experiences. As one\nof the dominating nonverbal channels for eliciting pain expression events,\nfacial expressions has been widely investigated for estimating the pain\nintensity of individual. However, using state-of-the-art deep learning (DL)\nmodels in real-world pain estimation applications poses several challenges\nrelated to the subjective variations of facial expressions, operational capture\nconditions, and lack of representative training videos with labels. Given the\ncost of annotating intensity levels for every video frame, we propose a\nweakly-supervised domain adaptation (WSDA) technique that allows for training\n3D CNNs for spatio-temporal pain intensity estimation using weakly labeled\nvideos, where labels are provided on a periodic basis. In particular, WSDA\nintegrates multiple instance learning into an adversarial deep domain\nadaptation framework to train an Inflated 3D-CNN (I3D) model such that it can\naccurately estimate pain intensities in the target operational domain. The\ntraining process relies on weak target loss, along with domain loss and source\nloss for domain adaptation of the I3D model. Experimental results obtained\nusing labeled source domain RECOLA videos and weakly-labeled target domain\nUNBC-McMaster videos indicate that the proposed deep WSDA approach can achieve\nsignificantly higher level of sequence (bag)-level and frame (instance)-level\npain localization accuracy than related state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 21:32:06 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 17:40:42 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["R", "Gnana Praveen", ""], ["Granger", "Eric", ""], ["Cardinal", "Patrick", ""]]}, {"id": "1910.08206", "submitter": "Huibin Chang", "authors": "Jie Zhang, Yuping Duan, Yue Lu, Michael K. Ng, and Huibin Chang", "title": "Bilinear Constraint based ADMM for Mixed Poisson-Gaussian Noise Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose new operator-splitting algorithms for the total\nvariation regularized infimal convolution (TV-IC) model [4] in order to remove\nmixed Poisson-Gaussian(MPG) noise. In the existing splitting algorithm for\nTV-IC, an inner loop by Newton method had to be adopted for one nonlinear\noptimization subproblem, which increased the computation cost per outer loop.\nBy introducing a new bilinear constraint and applying the alternating direction\nmethod of multipliers (ADMM), all subproblems of the proposed algorithms named\nas BCA (short for Bilinear Constraint based ADMM algorithm) and BCAf(short for\na variant of BCA with fully splitting form) can be very efficiently solved;\nespecially for the proposed BCAf, they can be calculated without any inner\niterations. Under mild conditions, the convergence of the proposed BCA is\ninvestigated. Numerically, compared to existing primal-dual algorithms for the\nTV-IC model, the proposed algorithms, with fewer tunable parameters, converge\nmuch faster and produce comparable results meanwhile.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 00:38:50 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 20:16:42 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Zhang", "Jie", ""], ["Duan", "Yuping", ""], ["Lu", "Yue", ""], ["Ng", "Michael K.", ""], ["Chang", "Huibin", ""]]}, {"id": "1910.08207", "submitter": "Kaveh Hassani", "authors": "Kaveh Hassani and Mike Haley", "title": "Unsupervised Multi-Task Feature Learning on Point Clouds", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an unsupervised multi-task model to jointly learn point and\nshape features on point clouds. We define three unsupervised tasks including\nclustering, reconstruction, and self-supervised classification to train a\nmulti-scale graph-based encoder. We evaluate our model on shape classification\nand segmentation benchmarks. The results suggest that it outperforms prior\nstate-of-the-art unsupervised models: In the ModelNet40 classification task, it\nachieves an accuracy of 89.1% and in ShapeNet segmentation task, it achieves an\nmIoU of 68.2 and accuracy of 88.6%.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 00:43:29 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Hassani", "Kaveh", ""], ["Haley", "Mike", ""]]}, {"id": "1910.08215", "submitter": "Alireza Rezvanifar", "authors": "Alireza Rezvanifar, Tunai Porto Marques, Melissa Cote, Alexandra\n  Branzan Albu, Alex Slonimer, Thomas Tolhurst, Kaan Ersahin, Todd Mudge,\n  Stephane Gauthier", "title": "A Deep Learning-based Framework for the Detection of Schools of Herring\n  in Echograms", "comments": "Accepted to NeurIPS 2019 workshop on Tackling Climate Change with\n  Machine Learning, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tracking the abundance of underwater species is crucial for understanding the\neffects of climate change on marine ecosystems. Biologists typically monitor\nunderwater sites with echosounders and visualize data as 2D images (echograms);\nthey interpret these data manually or semi-automatically, which is\ntime-consuming and prone to inconsistencies. This paper proposes a deep\nlearning framework for the automatic detection of schools of herring from\nechograms. Experiments demonstrated that our approach outperforms a traditional\nmachine learning algorithm using hand-crafted features. Our framework could\neasily be expanded to detect more species of interest to sustainable fisheries.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 01:12:46 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Rezvanifar", "Alireza", ""], ["Marques", "Tunai Porto", ""], ["Cote", "Melissa", ""], ["Albu", "Alexandra Branzan", ""], ["Slonimer", "Alex", ""], ["Tolhurst", "Thomas", ""], ["Ersahin", "Kaan", ""], ["Mudge", "Todd", ""], ["Gauthier", "Stephane", ""]]}, {"id": "1910.08223", "submitter": "Haozhe Xie", "authors": "Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping Zhang, Xiaoshuai\n  Sun, Wenxiu Sun", "title": "Toward 3D Object Reconstruction from Stereo Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Inferring the 3D shape of an object from an RGB image has shown impressive\nresults, however, existing methods rely primarily on recognizing the most\nsimilar 3D model from the training set to solve the problem. These methods\nsuffer from poor generalization and may lead to low-quality reconstructions for\nunseen objects. Nowadays, stereo cameras are pervasive in emerging devices such\nas dual-lens smartphones and robots, which enables the use of the two-view\nnature of stereo images to explore the 3D structure and thus improve the\nreconstruction performance. In this paper, we propose a new deep learning\nframework for reconstructing the 3D shape of an object from a pair of stereo\nimages, which reasons about the 3D structure of the object by taking\nbidirectional disparities and feature correspondences between the two views\ninto account. Besides, we present a large-scale synthetic benchmarking dataset,\nnamely StereoShapeNet, containing 1,052,976 pairs of stereo images rendered\nfrom ShapeNet along with the corresponding bidirectional depth and disparity\nmaps. Experimental results on the StereoShapeNet benchmark demonstrate that the\nproposed framework outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 01:53:39 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 11:43:43 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xie", "Haozhe", ""], ["Yao", "Hongxun", ""], ["Zhou", "Shangchen", ""], ["Zhang", "Shengping", ""], ["Sun", "Xiaoshuai", ""], ["Sun", "Wenxiu", ""]]}, {"id": "1910.08233", "submitter": "Sergio Casas", "authors": "Sergio Casas, Cole Gulino, Renjie Liao, Raquel Urtasun", "title": "Spatially-Aware Graph Neural Networks for Relational Behavior\n  Forecasting from Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of relational behavior forecasting from\nsensor data. Towards this goal, we propose a novel spatially-aware graph neural\nnetwork (SpAGNN) that models the interactions between agents in the scene.\nSpecifically, we exploit a convolutional neural network to detect the actors\nand compute their initial states. A graph neural network then iteratively\nupdates the actor states via a message passing process. Inspired by Gaussian\nbelief propagation, we design the messages to be spatially-transformed\nparameters of the output distributions from neighboring agents. Our model is\nfully differentiable, thus enabling end-to-end training. Importantly, our\nprobabilistic predictions can model uncertainty at the trajectory level. We\ndemonstrate the effectiveness of our approach by achieving significant\nimprovements over the state-of-the-art on two real-world self-driving datasets:\nATG4D and nuScenes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 03:14:10 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Casas", "Sergio", ""], ["Gulino", "Cole", ""], ["Liao", "Renjie", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1910.08237", "submitter": "Kartik Gupta", "authors": "Thalaiyasingam Ajanthan, Kartik Gupta, Philip H. S. Torr, Richard\n  Hartley, Puneet K. Dokania", "title": "Mirror Descent View for Neural Network Quantization", "comments": "This paper was accepted at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantizing large Neural Networks (NN) while maintaining the performance is\nhighly desirable for resource-limited devices due to reduced memory and time\ncomplexity. It is usually formulated as a constrained optimization problem and\noptimized via a modified version of gradient descent. In this work, by\ninterpreting the continuous parameters (unconstrained) as the dual of the\nquantized ones, we introduce a Mirror Descent (MD) framework for NN\nquantization. Specifically, we provide conditions on the projections (i.e.,\nmapping from continuous to quantized ones) which would enable us to derive\nvalid mirror maps and in turn the respective MD updates. Furthermore, we\npresent a numerically stable implementation of MD that requires storing an\nadditional set of auxiliary variables (unconstrained), and show that it is\nstrikingly analogous to the Straight Through Estimator (STE) based method which\nis typically viewed as a \"trick\" to avoid vanishing gradients issue. Our\nexperiments on CIFAR-10/100, TinyImageNet, and ImageNet classification datasets\nwith VGG-16, ResNet-18, and MobileNetV2 architectures show that our MD variants\nobtain quantized networks with state-of-the-art performance. Code is available\nat https://github.com/kartikgupta-at-anu/md-bnn.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 03:19:21 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 07:20:30 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 05:13:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ajanthan", "Thalaiyasingam", ""], ["Gupta", "Kartik", ""], ["Torr", "Philip H. S.", ""], ["Hartley", "Richard", ""], ["Dokania", "Puneet K.", ""]]}, {"id": "1910.08242", "submitter": "Risheng Liu", "authors": "Risheng Liu, Pan Mu, Jian Chen, Xin Fan and Zhongxuan Luo", "title": "Investigating Task-driven Latent Feasibility for Nonconvex Image\n  Modeling", "comments": "11 pages, Accepted at IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2020.3004733", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Properly modeling latent image distributions plays an important role in a\nvariety of image-related vision problems. Most exiting approaches aim to\nformulate this problem as optimization models (e.g., Maximum A Posterior, MAP)\nwith handcrafted priors. In recent years, different CNN modules are also\nconsidered as deep priors to regularize the image modeling process. However,\nthese explicit regularization techniques require deep understandings on the\nproblem and elaborately mathematical skills. In this work, we provide a new\nperspective, named Task-driven Latent Feasibility (TLF), to incorporate\nspecific task information to narrow down the solution space for the\noptimization-based image modeling problem. Thanks to the flexibility of TLF,\nboth designed and trained constraints can be embedded into the optimization\nprocess. By introducing control mechanisms based on the monotonicity and\nboundedness conditions, we can also strictly prove the convergence of our\nproposed inference process. We demonstrate that different types of image\nmodeling problems, such as image deblurring and rain streaks removals, can all\nbe appropriately addressed within our TLF framework. Extensive experiments also\nverify the theoretical results and show the advantages of our method against\nexisting state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 03:36:00 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 10:21:55 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 02:52:51 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Liu", "Risheng", ""], ["Mu", "Pan", ""], ["Chen", "Jian", ""], ["Fan", "Xin", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1910.08250", "submitter": "Yiping Tang", "authors": "Yiping Tang and Chuang Niu and Minghao Dong and Shenghan Ren and Jimin\n  Liang", "title": "AFO-TAD: Anchor-free One-Stage Detector for Temporal Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action detection is a fundamental yet challenging task in video\nunderstanding. Many of the state-of-the-art methods predict the boundaries of\naction instances based on predetermined anchors akin to the two-dimensional\nobject detection detectors. However, it is hard to detect all the action\ninstances with predetermined temporal scales because the durations of instances\nin untrimmed videos can vary from few seconds to several minutes. In this\npaper, we propose a novel action detection architecture named anchor-free\none-stage temporal action detector (AFO-TAD). AFO-TAD achieves better\nperformance for detecting action instances with arbitrary lengths and high\ntemporal resolution, which can be attributed to two aspects. First, we design a\nreceptive field adaption module which dynamically adjusts the receptive field\nfor precise action detection. Second, AFO-TAD directly predicts the categories\nand boundaries at every temporal locations without predetermined anchors.\nExtensive experiments show that AFO-TAD improves the state-of-the-art\nperformance on THUMOS'14.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 03:57:05 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Tang", "Yiping", ""], ["Niu", "Chuang", ""], ["Dong", "Minghao", ""], ["Ren", "Shenghan", ""], ["Liang", "Jimin", ""]]}, {"id": "1910.08259", "submitter": "Haotian Zhang", "authors": "Haotian Zhang, Gaoang Wang, Zhichao Lei, Jenq-Neng Hwang", "title": "Eye in the Sky: Drone-Based Object Tracking and 3D Localization", "comments": "Accepted to ACMMM2019", "journal-ref": null, "doi": "10.1145/3343031.3350933", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones, or general UAVs, equipped with a single camera have been widely\ndeployed to a broad range of applications, such as aerial photography, fast\ngoods delivery and most importantly, surveillance. Despite the great progress\nachieved in computer vision algorithms, these algorithms are not usually\noptimized for dealing with images or video sequences acquired by drones, due to\nvarious challenges such as occlusion, fast camera motion and pose variation. In\nthis paper, a drone-based multi-object tracking and 3D localization scheme is\nproposed based on the deep learning based object detection. We first combine a\nmulti-object tracking method called TrackletNet Tracker (TNT) which utilizes\ntemporal and appearance information to track detected objects located on the\nground for UAV applications. Then, we are also able to localize the tracked\nground objects based on the group plane estimated from the Multi-View Stereo\ntechnique. The system deployed on the drone can not only detect and track the\nobjects in a scene, but can also localize their 3D coordinates in meters with\nrespect to the drone camera. The experiments have proved our tracker can\nreliably handle most of the detected objects captured by drones and achieve\nfavorable 3D localization performance when compared with the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 04:44:44 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Zhang", "Haotian", ""], ["Wang", "Gaoang", ""], ["Lei", "Zhichao", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "1910.08263", "submitter": "Keifer Lee", "authors": "Keifer Lee, Jun Jet Tai, Swee King Phang", "title": "BOBBY2: Buffer Based Robust High-Speed Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a novel high-speed single object tracker that is robust against\nnon-semantic distractor exemplars is introduced; dubbed BOBBY2. It incorporates\na novel exemplar buffer module that sparsely caches the target's appearance\nacross time, enabling it to adapt to potential target deformation. As for\ntraining, an augmented ImageNet-VID dataset was used in conjunction with the\none cycle policy, enabling it to reach convergence with less than 2 epoch worth\nof data. For validation, the model was benchmarked on the GOT-10k dataset and\non an additional small, albeit challenging custom UAV dataset collected with\nthe TU-3 UAV. We demonstrate that the exemplar buffer is capable of providing\nredundancies in case of unintended target drifts, a desirable trait in any\nmiddle to long term tracking. Even when the buffer is predominantly filled with\ndistractors instead of valid exemplars, BOBBY2 is capable of maintaining a\nnear-optimal level of accuracy. BOBBY2 manages to achieve a very competitive\nresult on the GOT-10k dataset and to a lesser degree on the challenging custom\nTU-3 dataset, without fine-tuning, demonstrating its generalizability. In terms\nof speed, BOBBY2 utilizes a stripped down AlexNet as feature extractor with 63%\nless parameters than a vanilla AlexNet, thus being able to run at a competitive\n85 FPS.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 05:10:33 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Lee", "Keifer", ""], ["Tai", "Jun Jet", ""], ["Phang", "Swee King", ""]]}, {"id": "1910.08287", "submitter": "Hehe Fan", "authors": "Hehe Fan and Yi Yang", "title": "PointRNN: Point Recurrent Neural Network for Moving Point Cloud\n  Processing", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a Point Recurrent Neural Network (PointRNN) for\nmoving point cloud processing. At each time step, PointRNN takes point\ncoordinates $\\boldsymbol{P} \\in \\mathbb{R}^{n \\times 3}$ and point features\n$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$ as input ($n$ and $d$ denote the\nnumber of points and the number of feature channels, respectively). The state\nof PointRNN is composed of point coordinates $\\boldsymbol{P}$ and point states\n$\\boldsymbol{S} \\in \\mathbb{R}^{n \\times d'}$ ($d'$ denotes the number of state\nchannels). Similarly, the output of PointRNN is composed of $\\boldsymbol{P}$\nand new point features $\\boldsymbol{Y} \\in \\mathbb{R}^{n \\times d''}$ ($d''$\ndenotes the number of new feature channels). Since point clouds are orderless,\npoint features and states from two time steps can not be directly operated.\nTherefore, a point-based spatiotemporally-local correlation is adopted to\naggregate point features and states according to point coordinates. We further\npropose two variants of PointRNN, i.e., Point Gated Recurrent Unit (PointGRU)\nand Point Long Short-Term Memory (PointLSTM). We apply PointRNN, PointGRU and\nPointLSTM to moving point cloud prediction, which aims to predict the future\ntrajectories of points in a set given their history movements. Experimental\nresults show that PointRNN, PointGRU and PointLSTM are able to produce correct\npredictions on both synthetic and real-world datasets, demonstrating their\nability to model point cloud sequences. The code has been released at\n\\url{https://github.com/hehefan/PointRNN}.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 07:21:37 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 12:26:29 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Fan", "Hehe", ""], ["Yang", "Yi", ""]]}, {"id": "1910.08292", "submitter": "Sagar Verma", "authors": "Sagar Verma and Sukhad Anand and Chetan Arora and Atul Rai", "title": "Diversity in Fashion Recommendation using Semantic Parsing", "comments": "5 pages, ICIP2018, code:\n  https://github.com/sagarverma/fashion_recommendation_stlstm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing recommendation system for fashion images is challenging due to the\ninherent ambiguity associated with what criterion a user is looking at.\nSuggesting multiple images where each output image is similar to the query\nimage on the basis of a different feature or part is one way to mitigate the\nproblem. Existing works for fashion recommendation have used Siamese or Triplet\nnetwork to learn features between a similar pair and a similar-dissimilar\ntriplet respectively. However, these methods do not provide basic information\nsuch as, how two clothing images are similar, or which parts present in the two\nimages make them similar. In this paper, we propose to recommend images by\nexplicitly learning and exploiting part based similarity. We propose a novel\napproach of learning discriminative features from weakly-supervised data by\nusing visual attention over the parts and a texture encoding network. We show\nthat the learned features surpass the state-of-the-art in retrieval task on\nDeepFashion dataset. We then use the proposed model to recommend fashion images\nhaving an explicit variation with respect to similarity of any of the parts.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 07:47:41 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Verma", "Sagar", ""], ["Anand", "Sukhad", ""], ["Arora", "Chetan", ""], ["Rai", "Atul", ""]]}, {"id": "1910.08313", "submitter": "Bin Zhang", "authors": "Bin Zhang, Shenyao Jin, Yili Xia, Yongming Huang, and Zixiang Xiong", "title": "Attention Mechanism Enhanced Kernel Prediction Networks for Denoising of\n  Burst Images", "comments": "accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based image denoising methods have been extensively\ninvestigated. In this paper, attention mechanism enhanced kernel prediction\nnetworks (AME-KPNs) are proposed for burst image denoising, in which, nearly\ncost-free attention modules are adopted to first refine the feature maps and to\nfurther make a full use of the inter-frame and intra-frame redundancies within\nthe whole image burst. The proposed AME-KPNs output per-pixel\nspatially-adaptive kernels, residual maps and corresponding weight maps, in\nwhich, the predicted kernels roughly restore clean pixels at their\ncorresponding locations via an adaptive convolution operation, and\nsubsequently, residuals are weighted and summed to compensate the limited\nreceptive field of predicted kernels. Simulations and real-world experiments\nare conducted to illustrate the robustness of the proposed AME-KPNs in burst\nimage denoising.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 09:06:53 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 06:03:09 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Zhang", "Bin", ""], ["Jin", "Shenyao", ""], ["Xia", "Yili", ""], ["Huang", "Yongming", ""], ["Xiong", "Zixiang", ""]]}, {"id": "1910.08320", "submitter": "Iman Marivani", "authors": "Iman Marivani, Evaggelia Tsiligianni, Bruno Cornelis, Nikos\n  Deligiannis", "title": "Multimodal Image Super-resolution via Deep Unfolding with Side\n  Information", "comments": "5 pages, 5 figures, 3 tables, EUSIPCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have been successfully applied to various computer\nvision tasks. However, existing neural network architectures do not per se\nincorporate domain knowledge about the addressed problem, thus, understanding\nwhat the model has learned is an open research topic. In this paper, we rely on\nthe unfolding of an iterative algorithm for sparse approximation with side\ninformation, and design a deep learning architecture for multimodal image\nsuper-resolution that incorporates sparse priors and effectively utilizes\ninformation from another image modality. We develop two deep models performing\nreconstruction of a high-resolution image of a target image modality from its\nlow-resolution variant with the aid of a high-resolution image from a second\nmodality. We apply the proposed models to super-resolve near-infrared images\nusing as side information high-resolution RGB\\ images. Experimental results\ndemonstrate the superior performance of the proposed models against\nstate-of-the-art methods including unimodal and multimodal approaches.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 09:32:24 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Marivani", "Iman", ""], ["Tsiligianni", "Evaggelia", ""], ["Cornelis", "Bruno", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "1910.08328", "submitter": "Florian Lemarchand", "authors": "Florian Lemarchand, Eduardo Fernandes Montesuma, Maxime Pelcat, Erwan\n  Nogues", "title": "OpenDenoising: an Extensible Benchmark for Building Comparative Studies\n  of Image Denoisers", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053937", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising has recently taken a leap forward due to machine learning.\nHowever, image denoisers, both expert-based and learning-based, are mostly\ntested on well-behaved generated noises (usually Gaussian) rather than on\nreal-life noises, making performance comparisons difficult in real-world\nconditions. This is especially true for learning-based denoisers which\nperformance depends on training data. Hence, choosing which method to use for a\nspecific denoising problem is difficult.\n  This paper proposes a comparative study of existing denoisers, as well as an\nextensible open tool that makes it possible to reproduce and extend the study.\nMWCNN is shown to outperform other methods when trained for a real-world image\ninterception noise, and additionally is the second least compute hungry of the\ntested methods. To evaluate the robustness of conclusions, three test sets are\ncompared. A Kendall's Tau correlation of only 60% is obtained on methods\nranking between noise types, demonstrating the need for a benchmarking tool.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 09:51:38 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Lemarchand", "Florian", ""], ["Montesuma", "Eduardo Fernandes", ""], ["Pelcat", "Maxime", ""], ["Nogues", "Erwan", ""]]}, {"id": "1910.08336", "submitter": "Noemi Montobbio", "authors": "Noemi Montobbio, Laurent Bonnasse-Gahot, Giovanna Citti, Alessandro\n  Sarti", "title": "KerCNNs: biologically inspired lateral connections for classification of\n  corrupted images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of the art in many computer vision tasks is represented by\nConvolutional Neural Networks (CNNs). Although their hierarchical organization\nand local feature extraction are inspired by the structure of primate visual\nsystems, the lack of lateral connections in such architectures critically\ndistinguishes their analysis from biological object processing. The idea of\nenriching CNNs with recurrent lateral connections of convolutional type has\nbeen put into practice in recent years, in the form of learned recurrent\nkernels with no geometrical constraints. In the present work, we introduce\nbiologically plausible lateral kernels encoding a notion of correlation between\nthe feedforward filters of a CNN: at each layer, the associated kernel acts as\na transition kernel on the space of activations. The lateral kernels are\ndefined in terms of the filters, thus providing a parameter-free approach to\nassess the geometry of horizontal connections based on the feedforward\nstructure. We then test this new architecture, which we call KerCNN, on a\ngeneralization task related to global shape analysis and pattern completion:\nonce trained for performing basic image classification, the network is\nevaluated on corrupted testing images. The image perturbations examined are\ndesigned to undermine the recognition of the images via local features, thus\nrequiring an integration of context information - which in biological vision is\ncritically linked to lateral connectivity. Our KerCNNs turn out to be far more\nstable than CNNs and recurrent CNNs to such degradations, thus validating this\nbiologically inspired approach to reinforce object recognition under\nchallenging conditions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 10:31:06 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Montobbio", "Noemi", ""], ["Bonnasse-Gahot", "Laurent", ""], ["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""]]}, {"id": "1910.08343", "submitter": "Tiexin Qin", "authors": "Yinghuan Shi, Tiexin Qin, Yong Liu, Jiwen Lu, Yang Gao and Dinggang\n  Shen", "title": "Automatic Data Augmentation by Learning the Deterministic Policy", "comments": "Sorry for withdrawing our paper, there exists a mistake in the\n  experiment, and we will reupload once we have fixed it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming to produce sufficient and diverse training samples, data augmentation\nhas been demonstrated for its effectiveness in training deep models. Regarding\nthat the criterion of the best augmentation is challenging to define, we in\nthis paper present a novel learning-based augmentation method termed as\nDeepAugNet, which formulates the final augmented data as a collection of\nseveral sequentially augmented subsets. Specifically, the current augmented\nsubset is required to maximize the performance improvement compared with the\nlast augmented subset by learning the deterministic augmentation policy using\ndeep reinforcement learning. By introducing an unified optimization goal,\nDeepAugNet intends to combine the data augmentation and the deep model training\nin an end-to-end training manner which is realized by simultaneously training a\nhybrid architecture of dueling deep Q-learning algorithm and a surrogate deep\nmodel. We extensively evaluated our proposed DeepAugNet on various benchmark\ndatasets including Fashion MNIST, CUB, CIFAR-100 and WebCaricature. Compared\nwith the current state-of-the-arts, our method can achieve a significant\nimprovement in small-scale datasets, and a comparable performance in\nlarge-scale datasets. Code will be available soon.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 11:22:32 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 01:04:59 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Shi", "Yinghuan", ""], ["Qin", "Tiexin", ""], ["Liu", "Yong", ""], ["Lu", "Jiwen", ""], ["Gao", "Yang", ""], ["Shen", "Dinggang", ""]]}, {"id": "1910.08364", "submitter": "Peng Liu", "authors": "Peng Liu, Ruogu Fang", "title": "SDCNet: Smoothed Dense-Convolution Network for Restoring Low-Dose\n  Cerebral CT Perfusion", "comments": null, "journal-ref": "The IEEE International Symposium on Biomedical Imaging (ISBI 2018)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With substantial public concerns on potential cancer risks and health hazards\ncaused by the accumulated radiation exposure in medical imaging, reducing\nradiation dose in X-ray based medical imaging such as Computed Tomography\nPerfusion (CTP) has raised significant research interests. In this paper, we\nembrace the deep Convolutional Neural Networks (CNN) based approaches and\nintroduce Smoothed Dense-Convolution Neural Network (SDCNet) to recover\nhigh-dose quality CTP images from low-dose ones. SDCNet is composed of\nsub-network blocks cascaded by skip-connections to infer the noise\n(differentials) from paired low/high-dose CT scans. SDCNet can effectively\nremove the noise in real low-dose CT scans and enhance the quality of medical\nimages. We evaluate the proposed architecture on thousands of CT perfusion\nframes for both reconstructed image denoising and perfusion map quantification\nincluding cerebral blood flow (CBF) and cerebral blood volume (CBV). SDCNet\nachieves high performance in both visual and quantitative results with\npromising computational efficiency, comparing favorably with state-of-the-art\napproaches. \\textit{The code is available at\n\\url{https://github.com/cswin/RC-Nets}}.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 12:12:42 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Liu", "Peng", ""], ["Fang", "Ruogu", ""]]}, {"id": "1910.08373", "submitter": "Beomjun Kim", "authors": "Beomjun Kim, Jean Ponce, Bumsub Ham", "title": "Deformable Kernel Networks for Joint Image Filtering", "comments": "International Journal of Computer Vision (2020). arXiv admin note:\n  substantial text overlap with arXiv:1903.11286", "journal-ref": null, "doi": "10.1007/s11263-020-01386-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint image filters are used to transfer structural details from a guidance\npicture used as a prior to a target image, in tasks such as enhancing spatial\nresolution and suppressing noise. Previous methods based on convolutional\nneural networks (CNNs) combine nonlinear activations of spatially-invariant\nkernels to estimate structural details and regress the filtering result. In\nthis paper, we instead learn explicitly sparse and spatially-variant kernels.\nWe propose a CNN architecture and its efficient implementation, called the\ndeformable kernel network (DKN), that outputs sets of neighbors and the\ncorresponding weights adaptively for each pixel. The filtering result is then\ncomputed as a weighted average. We also propose a fast version of DKN that runs\nabout seventeen times faster for an image of size 640 x 480. We demonstrate the\neffectiveness and flexibility of our models on the tasks of depth map\nupsampling, saliency map upsampling, cross-modality image restoration, texture\nremoval, and semantic segmentation. In particular, we show that the weighted\naveraging process with sparsely sampled 3 x 3 kernels outperforms the state of\nthe art by a significant margin in all cases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 06:55:40 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 01:29:38 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 01:55:17 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Kim", "Beomjun", ""], ["Ponce", "Jean", ""], ["Ham", "Bumsub", ""]]}, {"id": "1910.08375", "submitter": "Yin Yin", "authors": "Z. Ma, L. Song, X. Feng, G. Yang, W.Zhu, J. Liu, Y. Zhang, X. Yang and\n  Y. Yin", "title": "Detecting intracranial aneurysm rupture from 3D surfaces using a novel\n  GraphNet approach", "comments": "Submitted to ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intracranial aneurysm (IA) is a life-threatening blood spot in human's brain\nif it ruptures and causes cerebral hemorrhage. It is challenging to detect\nwhether an IA has ruptured from medical images. In this paper, we propose a\nnovel graph based neural network named GraphNet to detect IA rupture from 3D\nsurface data. GraphNet is based on graph convolution network (GCN) and is\ndesigned for graph-level classification and node-level segmentation. The\nnetwork uses GCN blocks to extract surface local features and pools to global\nfeatures. 1250 patient data including 385 ruptured and 865 unruptured IAs were\ncollected from clinic for experiments. The performance on randomly selected 234\ntest patient data was reported. The experiment with the proposed GraphNet\nachieved accuracy of 0.82, area-under-curve (AUC) of receiver operating\ncharacteristic (ROC) curve 0.82 in the classification task, significantly\noutperforming the baseline approach without using graph based networks. The\nsegmentation output of the model achieved mean graph-node-based dice\ncoefficient (DSC) score 0.88.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 07:15:41 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Ma", "Z.", ""], ["Song", "L.", ""], ["Feng", "X.", ""], ["Yang", "G.", ""], ["Zhu", "W.", ""], ["Liu", "J.", ""], ["Zhang", "Y.", ""], ["Yang", "X.", ""], ["Yin", "Y.", ""]]}, {"id": "1910.08386", "submitter": "Zhunxuan Wang", "authors": "Zhunxuan Wang, Zipei Wang, Qiqi Li, Hakan Bilen", "title": "Image Deconvolution with Deep Image and Kernel Priors", "comments": "In Proceedings of the 2019 IEEE International Conference on Computer\n  Vision Workshops (ICCVW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deconvolution is the process of recovering convolutional degraded\nimages, which is always a hard inverse problem because of its mathematically\nill-posed property. On the success of the recently proposed deep image prior\n(DIP), we build an image deconvolution model with deep image and kernel priors\n(DIKP). DIP is a learning-free representation which uses neural net structures\nto express image prior information, and it showed great success in many\nenergy-based models, e.g. denoising, super-resolution, inpainting. Instead, our\nDIKP model uses such priors in image deconvolution to model not only images but\nalso kernels, combining the ideas of traditional learning-free deconvolution\nmethods with neural nets. In this paper, we show that DIKP improve the\nperformance of learning-free image deconvolution, and we experimentally\ndemonstrate this on the standard benchmark of six standard test images in terms\nof PSNR and visual effects.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 12:44:31 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Wang", "Zhunxuan", ""], ["Wang", "Zipei", ""], ["Li", "Qiqi", ""], ["Bilen", "Hakan", ""]]}, {"id": "1910.08439", "submitter": "Houwang Zhang", "authors": "Houwang Zhang, Chong Wu, Le Zhang and Hanying Zheng", "title": "A novel centroid update approach for clustering-based superpixel methods\n  and superpixel-based edge detection", "comments": "This paper has been accepted by ICIP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel is widely used in image processing. And among the methods for\nsuperpixel generation, clustering-based methods have a high speed and a good\nperformance at the same time. However, most clustering-based superpixel methods\nare sensitive to noise. To solve these problems, in this paper, we first\nanalyze the features of noise. Then according to the statistical features of\nnoise, we propose a novel centroid update approach to enhance the robustness of\nclustering-based superpixel methods. Besides, we propose a novel\nsuperpixel-based edge detection method. The experiments on BSD500 dataset show\nthat our approach can significantly enhance the performance of clustering-based\nsuperpixel methods in noisy environment. Moreover, we also show that our\nproposed edge detection method outperforms other classical methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 14:31:52 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 05:39:21 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zhang", "Houwang", ""], ["Wu", "Chong", ""], ["Zhang", "Le", ""], ["Zheng", "Hanying", ""]]}, {"id": "1910.08470", "submitter": "Dimitrios Sakkos", "authors": "Dimitrios Sakkos, Hubert P. H. Shum and Edmond S. L. Ho", "title": "Illumination-Based Data Augmentation for Robust Background Subtraction", "comments": "SKIMA 2019 - Best Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core challenge in background subtraction (BGS) is handling videos with\nsudden illumination changes in consecutive frames. In this paper, we tackle the\nproblem from a data point-of-view using data augmentation. Our method performs\ndata augmentation that not only creates endless data on the fly, but also\nfeatures semantic transformations of illumination which enhance the\ngeneralisation of the model. It successfully simulates flashes and shadows by\napplying the Euclidean distance transform over a binary mask that is randomly\ngenerated. Such data allows us to effectively train an illumination-invariant\ndeep learning model for BGS. Experimental results demonstrate the contribution\nof the synthetics in the ability of the models to perform BGS even when\nsignificant illumination changes take place. The source code of the project is\nmade publicly available at\nhttps://github.com/dksakkos/illumination_augmentation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:28:59 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Sakkos", "Dimitrios", ""], ["Shum", "Hubert P. H.", ""], ["Ho", "Edmond S. L.", ""]]}, {"id": "1910.08485", "submitter": "Ruth Fong", "authors": "Ruth Fong, Mandela Patrick, Andrea Vedaldi", "title": "Understanding Deep Networks via Extremal Perturbations and Smooth Masks", "comments": "Accepted at ICCV 2019 as oral; supp mat at\n  http://ruthcfong.github.io/files/fong19_extremal_supps.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of attribution is concerned with identifying the parts of an\ninput that are responsible for a model's output. An important family of\nattribution methods is based on measuring the effect of perturbations applied\nto the input. In this paper, we discuss some of the shortcomings of existing\napproaches to perturbation analysis and address them by introducing the concept\nof extremal perturbations, which are theoretically grounded and interpretable.\nWe also introduce a number of technical innovations to compute extremal\nperturbations, including a new area constraint and a parametric family of\nsmooth perturbations, which allow us to remove all tunable hyper-parameters\nfrom the optimization problem. We analyze the effect of perturbations as a\nfunction of their area, demonstrating excellent sensitivity to the spatial\nproperties of the deep neural network under stimulation. We also extend\nperturbation analysis to the intermediate layers of a network. This application\nallows us to identify the salient channels necessary for classification, which,\nwhen visualized using feature inversion, can be used to elucidate model\nbehavior. Lastly, we introduce TorchRay, an interpretability library built on\nPyTorch.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 16:02:01 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Fong", "Ruth", ""], ["Patrick", "Mandela", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1910.08515", "submitter": "Odysseas Kechagias-Stamatis", "authors": "Odysseas Kechagias-Stamatis, Nabil Aouf, Mark A. Richardson", "title": "Single and Cross-Dimensional Feature Detection and Description: An\n  Evaluation", "comments": null, "journal-ref": null, "doi": "10.1049/iet-ipr.2019.1523", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Three-dimensional local feature detection and description techniques are\nwidely used for object registration and recognition applications. Although\nseveral evaluations of 3D local feature detection and description methods have\nalready been published, these are constrained in a single dimensional scheme,\ni.e. either 3D or 2D methods that are applied onto multiple projections of the\n3D data. However, cross-dimensional (mixed 2D and 3D) feature detection and\ndescription has yet to be investigated. Here, we evaluated the performance of\nboth single and cross-dimensional feature detection and description methods on\nseveral 3D datasets and demonstrated the superiority of cross-dimensional over\nsingle-dimensional schemes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:21:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Kechagias-Stamatis", "Odysseas", ""], ["Aouf", "Nabil", ""], ["Richardson", "Mark A.", ""]]}, {"id": "1910.08519", "submitter": "Sam Ringer", "authors": "Sam Ringer, Will Williams, Tom Ash, Remi Francis, David MacLeod", "title": "Texture Bias Of CNNs Limits Few-Shot Classification Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate image classification given small amounts of labelled data (few-shot\nclassification) remains an open problem in computer vision. In this work we\nexamine how the known texture bias of Convolutional Neural Networks (CNNs)\naffects few-shot classification performance. Although texture bias can help in\nstandard image classification, in this work we show it significantly harms\nfew-shot classification performance. After correcting this bias we demonstrate\nstate-of-the-art performance on the competitive miniImageNet task using a\nmethod far simpler than the current best performing few-shot learning\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:30:11 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Ringer", "Sam", ""], ["Williams", "Will", ""], ["Ash", "Tom", ""], ["Francis", "Remi", ""], ["MacLeod", "David", ""]]}, {"id": "1910.08521", "submitter": "Timothy Overbye", "authors": "Timothy Overbye and Srikanth Saripalli", "title": "Fast Local Planning and Mapping in Unknown Off-Road Terrain", "comments": "7 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a fast, on-line mapping and planning solution for\noperation in unknown, off-road, environments. We combine obstacle detection\nalong with a terrain gradient map to make simple and adaptable cost map. This\nmap can be created and updated at 10 Hz. An A* planner finds optimal paths over\nthe map. Finally, we take multiple samples over the control input space and do\na kinematic forward simulation to generated feasible trajectories. Then the\nmost optimal trajectory, as determined by the cost map and proximity to A*\npath, is chosen and sent to the controller. Our method allows real time\noperation at rates of 30 Hz. We demonstrate the efficiency of our method in\nvarious off-road terrain at high speed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:33:43 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Overbye", "Timothy", ""], ["Saripalli", "Srikanth", ""]]}, {"id": "1910.08536", "submitter": "Zirui Xu", "authors": "Zirui Xu, Fuxun Yu, Xiang Chen", "title": "LanCe: A Comprehensive and Lightweight CNN Defense Methodology against\n  Physical Adversarial Attacks on Embedded Multimedia Applications", "comments": "6 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:1905.08790", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, adversarial attacks can be applied to the physical world, causing\npractical issues to various Convolutional Neural Networks (CNNs) powered\napplications. Most existing physical adversarial attack defense works only\nfocus on eliminating explicit perturbation patterns from inputs, ignoring\ninterpretation to CNN's intrinsic vulnerability. Therefore, they lack the\nexpected versatility to different attacks and thereby depend on considerable\ndata processing costs. In this paper, we propose LanCe -- a comprehensive and\nlightweight CNN defense methodology against different physical adversarial\nattacks. By interpreting CNN's vulnerability, we find that non-semantic\nadversarial perturbations can activate CNN with significantly abnormal\nactivations and even overwhelm other semantic input patterns' activations. We\nimprove the CNN recognition process by adding a self-verification stage to\ndetect the potential adversarial input with only one CNN inference cost. Based\non the detection result, we further propose a data recovery methodology to\ndefend the physical adversarial attacks. We apply such defense methodology into\nboth image and audio CNN recognition scenarios and analyze the computational\ncomplexity for each scenario, respectively. Experiments show that our\nmethodology can achieve an average 91% successful rate for attack detection and\n89% accuracy recovery. Moreover, it is at most 3x faster compared with the\nstate-of-the-art defense methods, making it feasible to resource-constrained\nembedded systems, such as mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 21:38:11 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xu", "Zirui", ""], ["Yu", "Fuxun", ""], ["Chen", "Xiang", ""]]}, {"id": "1910.08537", "submitter": "Jun Zhou", "authors": "Jun Zhou, Hua Huang, Bin Liu, Xiuping Liu", "title": "Normal Estimation for 3D Point Clouds via Local Plane Constraint and\n  Multi-scale Selection", "comments": "arXiv admin note: text overlap with arXiv:1710.04954,\n  arXiv:1904.07172, arXiv:1812.00709 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a normal estimation method for unstructured 3D\npoint clouds. In this method, a feature constraint mechanism called Local Plane\nFeatures Constraint (LPFC) is used and then a multi-scale selection strategy is\nintroduced. The LPEC can be used in a single-scale point network architecture\nfor a more stable normal estimation of the unstructured 3D point clouds. In\nparticular, it can partly overcome the influence of noise on a large sampling\nscale compared to the other methods which only use regression loss for normal\nestimation. For more details, a subnetwork is built after point-wise features\nextracted layers of the network and it gives more constraints to each point of\nthe local patch via a binary classifier in the end. Then we use multi-task\noptimization to train the normal estimation and local plane classification\ntasks simultaneously.Also, to integrate the advantages of multi-scale results,\na scale selection strategy is adopted, which is a data-driven approach for\nselecting the optimal scale around each point and encourages subnetwork\nspecialization. Specifically, we employed a subnetwork called Scale Estimation\nNetwork to extract scale weight information from multi-scale features. More\nanalysis is given about the relations between noise levels, local boundary, and\nscales in the experiment. These relationships can be a better guide to choosing\nparticular scales for a particular model. Besides, the experimental result\nshows that our network can distinguish the points on the fitting plane\naccurately and this can be used to guide the normal estimation and our\nmulti-scale method can improve the results well. Compared to some\nstate-of-the-art surface normal estimators, our method is robust to noise and\ncan achieve competitive results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 00:17:42 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhou", "Jun", ""], ["Huang", "Hua", ""], ["Liu", "Bin", ""], ["Liu", "Xiuping", ""]]}, {"id": "1910.08540", "submitter": "Wenyuan Li", "authors": "Wenyuan Li, Zichen Wang, Yuguang Yue, Jiayun Li, William Speier,\n  Mingyuan Zhou, Corey W. Arnold", "title": "Semi-supervised Learning using Adversarial Training with Good and Bad\n  Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate semi-supervised learning (SSL) for image\nclassification using adversarial training. Previous results have illustrated\nthat generative adversarial networks (GANs) can be used for multiple purposes.\nTriple-GAN, which aims to jointly optimize model components by incorporating\nthree players, generates suitable image-label pairs to compensate for the lack\nof labeled data in SSL with improved benchmark performance. Conversely, Bad (or\ncomplementary) GAN, optimizes generation to produce complementary data-label\npairs and force a classifier's decision boundary to lie between data manifolds.\nAlthough it generally outperforms Triple-GAN, Bad GAN is highly sensitive to\nthe amount of labeled data used for training. Unifying these two approaches, we\npresent unified-GAN (UGAN), a novel framework that enables a classifier to\nsimultaneously learn from both good and bad samples through adversarial\ntraining. We perform extensive experiments on various datasets and demonstrate\nthat UGAN: 1) achieves state-of-the-art performance among other deep generative\nmodels, and 2) is robust to variations in the amount of labeled data used for\ntraining.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 05:47:08 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Li", "Wenyuan", ""], ["Wang", "Zichen", ""], ["Yue", "Yuguang", ""], ["Li", "Jiayun", ""], ["Speier", "William", ""], ["Zhou", "Mingyuan", ""], ["Arnold", "Corey W.", ""]]}, {"id": "1910.08549", "submitter": "Achim Rettinger", "authors": "Achim Rettinger, Viktoria Bogdanova, Philipp Niemann", "title": "Towards Learning Cross-Modal Perception-Trace Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is a key element of state-of-the-art deep learning\napproaches. It enables to transform raw data into structured vector space\nembeddings. Such embeddings are able to capture the distributional semantics of\ntheir context, e.g. by word windows on natural language sentences, graph walks\non knowledge graphs or convolutions on images. So far, this context is manually\ndefined, resulting in heuristics which are solely optimized for computational\nperformance on certain tasks like link-prediction. However, such heuristic\nmodels of context are fundamentally different to how humans capture\ninformation. For instance, when reading a multi-modal webpage (i) humans do not\nperceive all parts of a document equally: Some words and parts of images are\nskipped, others are revisited several times which makes the perception trace\nhighly non-sequential; (ii) humans construct meaning from a document's content\nby shifting their attention between text and image, among other things, guided\nby layout and design elements. In this paper we empirically investigate the\ndifference between human perception and context heuristics of basic embedding\nmodels. We conduct eye tracking experiments to capture the underlying\ncharacteristics of human perception of media documents containing a mixture of\ntext and images. Based on that, we devise a prototypical computational\nperception-trace model, called CMPM. We evaluate empirically how CMPM can\nimprove a basic skip-gram embedding approach. Our results suggest, that even\nwith a basic human-inspired computational perception model, there is a huge\npotential for improving embeddings since such a model does inherently capture\nmultiple modalities, as well as layout and design elements.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:20:38 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Rettinger", "Achim", ""], ["Bogdanova", "Viktoria", ""], ["Niemann", "Philipp", ""]]}, {"id": "1910.08593", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra", "title": "Generative Adversarial Networks And Domain Adaptation For Training Data\n  Independent Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image registration is an important task in automated analysis of\nmulti-modal images and temporal data involving multiple patient visits.\nConventional approaches, although useful for different image types, are time\nconsuming. Of late, deep learning (DL) based image registration methods have\nbeen proposed that outperform traditional methods in terms of accuracy and\ntime. However,DL based methods are heavily dependent on training data and do\nnot generalize well when presented with images of different scanners or\nanatomies. We present a DL based approach that can perform medical image\nregistration of one image type despite being trained with images of a different\ntype. This is achieved by unsupervised domain adaptation in the registration\nprocess and allows for easier application to different datasets without\nextensive retraining.To achieve our objective we train a network that\ntransforms the given input image pair to a latent feature space vector using\nautoencoders. The resultant encoded feature space is used to generate the\nregistered images with the help of generative adversarial networks (GANs). This\nfeature transformation ensures greater invariance to the input image type.\nExperiments on chest Xray, retinal and brain MR images show that our method,\ntrained on one dataset gives better registration performance for other\ndatasets, outperforming conventional methods that do not incorporate domain\nadaptation\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 19:21:17 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 20:21:25 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Mahapatra", "Dwarikanath", ""]]}, {"id": "1910.08640", "submitter": "Simran Kaur", "authors": "Simran Kaur, Jeremy Cohen, Zachary C. Lipton", "title": "Are Perceptually-Aligned Gradients a General Property of Robust\n  Classifiers?", "comments": "To appear in the \"Science Meets Engineering of Deep Learning\"\n  Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a standard convolutional neural network, optimizing over the input pixels\nto maximize the score of some target class will generally produce a\ngrainy-looking version of the original image. However, Santurkar et al. (2019)\ndemonstrated that for adversarially-trained neural networks, this optimization\nproduces images that uncannily resemble the target class. In this paper, we\nshow that these \"perceptually-aligned gradients\" also occur under randomized\nsmoothing, an alternative means of constructing adversarially-robust\nclassifiers. Our finding supports the hypothesis that perceptually-aligned\ngradients may be a general property of robust classifiers. We hope that our\nresults will inspire research aimed at explaining this link between\nperceptually-aligned gradients and adversarial robustness.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 22:02:41 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 16:02:06 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Kaur", "Simran", ""], ["Cohen", "Jeremy", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "1910.08643", "submitter": "Murtadha Hssayeni", "authors": "Murtadha D. Hssayeni, M.S., Muayad S. Croock, Ph.D., Aymen Al-Ani,\n  Ph.D., Hassan Falah Al-khafaji, M.D., Zakaria A. Yahya, M.D. and Behnaz\n  Ghoraani, Ph.D", "title": "Intracranial Hemorrhage Segmentation Using Deep Convolutional Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traumatic brain injuries could cause intracranial hemorrhage (ICH). ICH could\nlead to disability or death if it is not accurately diagnosed and treated in a\ntime-sensitive procedure. The current clinical protocol to diagnose ICH is\nexamining Computerized Tomography (CT) scans by radiologists to detect ICH and\nlocalize its regions. However, this process relies heavily on the availability\nof an experienced radiologist. In this paper, we designed a study protocol to\ncollect a dataset of 82 CT scans of subjects with traumatic brain injury.\nLater, the ICH regions were manually delineated in each slice by a consensus\ndecision of two radiologists. Recently, fully convolutional networks (FCN) have\nshown to be successful in medical image segmentation. We developed a deep FCN,\ncalled U-Net, to segment the ICH regions from the CT scans in a fully automated\nmanner. The method achieved a Dice coefficient of 0.31 for the ICH segmentation\nbased on 5-fold cross-validation. The dataset is publicly available online at\nPhysioNet repository for future analysis and comparison.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 22:08:05 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 12:59:47 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Hssayeni", "Murtadha D.", ""], ["S.", "M.", ""], ["Croock", "Muayad S.", ""], ["D.", "Ph.", ""], ["Al-Ani", "Aymen", ""], ["D.", "Ph.", ""], ["Al-khafaji", "Hassan Falah", ""], ["D.", "M.", ""], ["Yahya", "Zakaria A.", ""], ["D.", "M.", ""], ["Ghoraani", "Behnaz", ""], ["D", "Ph.", ""]]}, {"id": "1910.08650", "submitter": "Mahdieh Abbasi", "authors": "Mahdieh Abbasi, Changjian Shui, Arezoo Rajabi, Christian Gagne, Rakesh\n  Bobba", "title": "Toward Metrics for Differentiating Out-of-Distribution Sets", "comments": "Workshop on Safety and Robustness in Decision Making, NeurIPS 2019", "journal-ref": "ECAI 2020 : 24th European Conference on Artificial Intelligence", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vanilla CNNs, as uncalibrated classifiers, suffer from classifying\nout-of-distribution (OOD) samples nearly as confidently as in-distribution\nsamples. To tackle this challenge, some recent works have demonstrated the\ngains of leveraging available OOD sets for training end-to-end calibrated CNNs.\nHowever, a critical question remains unanswered in these works: how to\ndifferentiate OOD sets for selecting the most effective one(s) that induce\ntraining such CNNs with high detection rates on unseen OOD sets? To address\nthis pivotal question, we provide a criterion based on generalization errors of\nAugmented-CNN, a vanilla CNN with an added extra class employed for rejection,\non in-distribution and unseen OOD sets. However, selecting the most effective\nOOD set by directly optimizing this criterion incurs a huge computational cost.\nInstead, we propose three novel computationally-efficient metrics for\ndifferentiating between OOD sets according to their \"protection\" level of\nin-distribution sub-manifolds. We empirically verify that the most protective\nOOD sets -- selected according to our metrics -- lead to A-CNNs with\nsignificantly lower generalization errors than the A-CNNs trained on the least\nprotective ones. We also empirically show the effectiveness of a protective OOD\nset for training well-generalized confidence-calibrated vanilla CNNs. These\nresults confirm that 1) all OOD sets are not equally effective for training\nwell-performing end-to-end models (i.e., A-CNNs and calibrated CNNs) for OOD\ndetection tasks and 2) the protection level of OOD sets is a viable factor for\nrecognizing the most effective one. Finally, across the image classification\ntasks, we exhibit A-CNN trained on the most protective OOD set can also detect\nblack-box FGS adversarial examples as their distance (measured by our metrics)\nis becoming larger from the protected sub-manifolds.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 22:26:49 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 17:33:37 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 16:15:21 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Abbasi", "Mahdieh", ""], ["Shui", "Changjian", ""], ["Rajabi", "Arezoo", ""], ["Gagne", "Christian", ""], ["Bobba", "Rakesh", ""]]}, {"id": "1910.08665", "submitter": "Abhishek Singh", "authors": "Abhishek Singh, Anubhav Garg, Jinan Zhou, Shiv Ram Dubey, Debo Dutta", "title": "NASIB: Neural Architecture Search withIn Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) represents a class of methods to generate\nthe optimal neural network architecture and typically iterate over candidate\narchitectures till convergence over some particular metric like validation\nloss. They are constrained by the available computation resources, especially\nin enterprise environments. In this paper, we propose a new approach for NAS,\ncalled NASIB, which adapts and attunes to the computation resources (budget)\navailable by varying the exploration vs. exploitation trade-off. We reduce the\nexpert bias by searching over an augmented search space induced by\nSuperkernels. The proposed method can provide the architecture search useful\nfor different computation resources and different domains beyond image\nclassification of natural images where we lack bespoke architecture motifs and\ndomain expertise. We show, on CIFAR10, that itis possible to search over a\nspace that comprises of 12x more candidate operations than the traditional\nprior art in just 1.5 GPU days, while reaching close to state of the art\naccuracy. While our method searches over an exponentially larger search space,\nit could lead to novel architectures that require lesser domain expertise,\ncompared to the majority of the existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 00:12:39 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Singh", "Abhishek", ""], ["Garg", "Anubhav", ""], ["Zhou", "Jinan", ""], ["Dubey", "Shiv Ram", ""], ["Dutta", "Debo", ""]]}, {"id": "1910.08681", "submitter": "Qing Guo", "authors": "Qing Guo, Xiaofei Xie, Felix Juefei-Xu, Lei Ma, Zhongguo Li, Wanli\n  Xue, Wei Feng, and Yang Liu", "title": "SPARK: Spatial-aware Online Incremental Attack Against Visual Tracking", "comments": "18 pages, 5 figures. This paper has been accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks of deep neural networks have been intensively studied on\nimage, audio, natural language, patch, and pixel classification tasks.\nNevertheless, as a typical, while important real-world application, the\nadversarial attacks of online video object tracking that traces an object's\nmoving trajectory instead of its category are rarely explored. In this paper,\nwe identify a new task for the adversarial attack to visual tracking: online\ngenerating imperceptible perturbations that mislead trackers along an incorrect\n(Untargeted Attack, UA) or specified trajectory (Targeted Attack, TA). To this\nend, we first propose a \\textit{spatial-aware} basic attack by adapting\nexisting attack methods, i.e., FGSM, BIM, and C&W, and comprehensively analyze\nthe attacking performance. We identify that online object tracking poses two\nnew challenges: 1) it is difficult to generate imperceptible perturbations that\ncan transfer across frames, and 2) real-time trackers require the attack to\nsatisfy a certain level of efficiency. To address these challenges, we further\npropose the spatial-aware online incremental attack (a.k.a. SPARK) that\nperforms spatial-temporal sparse incremental perturbations online and makes the\nadversarial attack less perceptible. In addition, as an optimization-based\nmethod, SPARK quickly converges to very small losses within several iterations\nby considering historical incremental perturbations, making it much more\nefficient than basic attacks. The in-depth evaluation on state-of-the-art\ntrackers (i.e., SiamRPN++ with AlexNet, MobileNetv2, and ResNet-50, and SiamDW)\non OTB100, VOT2018, UAV123, and LaSOT demonstrates the effectiveness and\ntransferability of SPARK in misleading the trackers under both UA and TA with\nminor perturbations.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 02:35:38 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 03:53:24 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 12:58:16 GMT"}, {"version": "v4", "created": "Wed, 22 Jul 2020 05:25:14 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Guo", "Qing", ""], ["Xie", "Xiaofei", ""], ["Juefei-Xu", "Felix", ""], ["Ma", "Lei", ""], ["Li", "Zhongguo", ""], ["Xue", "Wanli", ""], ["Feng", "Wei", ""], ["Liu", "Yang", ""]]}, {"id": "1910.08685", "submitter": "Deepali Aneja", "authors": "Deepali Aneja and Wilmot Li", "title": "Real-Time Lip Sync for Live 2D Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of commercial tools for real-time performance-based 2D\nanimation has enabled 2D characters to appear on live broadcasts and streaming\nplatforms. A key requirement for live animation is fast and accurate lip sync\nthat allows characters to respond naturally to other actors or the audience\nthrough the voice of a human performer. In this work, we present a deep\nlearning based interactive system that automatically generates live lip sync\nfor layered 2D characters using a Long Short Term Memory (LSTM) model. Our\nsystem takes streaming audio as input and produces viseme sequences with less\nthan 200ms of latency (including processing time). Our contributions include\nspecific design decisions for our feature definition and LSTM configuration\nthat provide a small but useful amount of lookahead to produce accurate lip\nsync. We also describe a data augmentation procedure that allows us to achieve\ngood results with a very small amount of hand-animated training data (13-20\nminutes). Extensive human judgement experiments show that our results are\npreferred over several competing methods, including those that only support\noffline (non-live) processing. Video summary and supplementary results at\nGitHub link: https://github.com/deepalianeja/CharacterLipSync2D\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 03:12:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Aneja", "Deepali", ""], ["Li", "Wilmot", ""]]}, {"id": "1910.08695", "submitter": "Yuezun Li", "authors": "Yuezun Li, Ao Luo and Siwei Lyu", "title": "Fast Portrait Segmentation with Highly Light-weight Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a fast and light-weight portrait segmentation\nmethod based on a new highly light-weight backbone (HLB) architecture. The core\nelement of HLB is a bottleneck-based factorized block (BFB) that has much fewer\nparameters than existing alternatives while keeping good learning capacity.\nConsequently, the HLB-based portrait segmentation method can run faster than\nthe existing methods yet retaining the competitive accuracy performance with\nstate-of-the-arts. Experiments conducted on two benchmark datasets demonstrate\nthe effectiveness and efficiency of our method.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 03:48:36 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 00:49:04 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 15:20:29 GMT"}, {"version": "v4", "created": "Sat, 30 May 2020 12:09:38 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Li", "Yuezun", ""], ["Luo", "Ao", ""], ["Lyu", "Siwei", ""]]}, {"id": "1910.08705", "submitter": "Jee Won Kim", "authors": "Jee Won Kim, Kinam Kwon, Byungjai Kim, and HyunWook Park", "title": "Attention Guided Metal Artifact Correction in MRI using Deep Neural\n  Networks", "comments": "6 pages, 5 figures", "journal-ref": "ICCV 2019 Workshop on Interpreting and Explaining Visual\n  Artificial Intelligence Models", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An attention guided scheme for metal artifact correction in MRI using deep\nneural network is proposed in this paper. The inputs of the networks are two\ndistorted images obtained with dual-polarity readout gradients. With MR image\ngeneration module and the additional data consistency loss to the previous work\n[1], the network is trained to estimate the frequency-shift map, off-resonance\nmap, and attention map. The attention map helps to produce better\ndistortion-corrected images by weighting on more relevant distortion-corrected\nimages where two distortion-corrected images are produced with half of the\nfrequency-shift maps. In this paper, we observed that in a real MRI\nenvironment, two distorted images obtained with opposite polarities of readout\ngradient showed artifacts in a different region. Therefore, we proved that\nusing the attention map was important in that it reduced the residual ripple\nand pile-up artifacts near metallic implants.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 04:51:35 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Kim", "Jee Won", ""], ["Kwon", "Kinam", ""], ["Kim", "Byungjai", ""], ["Park", "HyunWook", ""]]}, {"id": "1910.08711", "submitter": "Shuai Zhao", "authors": "Shuai Zhao, Boxi Wu, Wenqing Chu, Yao Hu, Deng Cai", "title": "Correlation Maximized Structural Similarity Loss for Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most semantic segmentation models treat semantic segmentation as a pixel-wise\nclassification task and use a pixel-wise classification error as their\noptimization criterions. However, the pixel-wise error ignores the strong\ndependencies among the pixels in an image, which limits the performance of the\nmodel. Several ways to incorporate the structure information of the objects\nhave been investigated, \\eg, conditional random fields (CRF), image structure\npriors based methods, and generative adversarial network (GAN). Nevertheless,\nthese methods usually require extra model branches or additional memories, and\nsome of them show limited improvements. In contrast, we propose a simple yet\neffective structural similarity loss (SSL) to encode the structure information\nof the objects, which only requires a few additional computational resources in\nthe training phase. Inspired by the widely-used structural similarity (SSIM)\nindex in image quality assessment, we use the linear correlation between two\nimages to quantify their structural similarity. And the goal of the proposed\nSSL is to pay more attention to the positions, whose associated predictions\nlead to a low degree of linear correlation between two corresponding regions in\nthe ground truth map and the predicted map. Thus the model can achieve a strong\nstructural similarity between the two maps through minimizing the SSL over the\nwhole map. The experimental results demonstrate that our method can achieve\nsubstantial and consistent improvements in performance on the PASCAL VOC 2012\nand Cityscapes datasets. The code will be released soon.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 06:33:11 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhao", "Shuai", ""], ["Wu", "Boxi", ""], ["Chu", "Wenqing", ""], ["Hu", "Yao", ""], ["Cai", "Deng", ""]]}, {"id": "1910.08728", "submitter": "Ziwen Wang", "authors": "Henry H. Yu, Xue Feng, Hao Sun and Ziwen Wang", "title": "MixModule: Mixed CNN Kernel Module for Medical Image Segmentation", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been successfully applied to\nmedical image classification, segmentation, and related tasks. Among the many\nCNNs architectures, U-Net and its improved versions based are widely used and\nachieve state-of-the-art performance these years. These improved architectures\nfocus on structural improvements and the size of the convolution kernel is\ngenerally fixed. In this paper, we propose a module that combines the benefits\nof multiple kernel sizes and we apply the proposed module to U-Net and its\nvariants. We test our module on three segmentation benchmark datasets and\nexperimental results show significant improvement.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 09:06:21 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 03:22:59 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Yu", "Henry H.", ""], ["Feng", "Xue", ""], ["Sun", "Hao", ""], ["Wang", "Ziwen", ""]]}, {"id": "1910.08732", "submitter": "Kranti Kumar Parida", "authors": "Kranti Kumar Parida, Neeraj Matiyali, Tanaya Guha, Gaurav Sharma", "title": "Coordinated Joint Multimodal Embeddings for Generalized Audio-Visual\n  Zeroshot Classification and Retrieval of Videos", "comments": "To appear in WACV 2020, Project Page:\n  https://cse.iitk.ac.in/users/kranti/avzsl.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an audio-visual multimodal approach for the task of zeroshot\nlearning (ZSL) for classification and retrieval of videos. ZSL has been studied\nextensively in the recent past but has primarily been limited to visual\nmodality and to images. We demonstrate that both audio and visual modalities\nare important for ZSL for videos. Since a dataset to study the task is\ncurrently not available, we also construct an appropriate multimodal dataset\nwith 33 classes containing 156,416 videos, from an existing large scale audio\nevent dataset. We empirically show that the performance improves by adding\naudio modality for both tasks of zeroshot classification and retrieval, when\nusing multimodal extensions of embedding learning methods. We also propose a\nnovel method to predict the `dominant' modality using a jointly learned\nmodality attention network. We learn the attention in a semi-supervised setting\nand thus do not require any additional explicit labelling for the modalities.\nWe provide qualitative validation of the modality specific attention, which\nalso successfully generalizes to unseen test classes.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 09:39:28 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Parida", "Kranti Kumar", ""], ["Matiyali", "Neeraj", ""], ["Guha", "Tanaya", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1910.08735", "submitter": "Deepak Gupta", "authors": "Deepak K. Gupta, Nathan de Bruijn, Andreas Panteli and Efstratios\n  Gavves", "title": "Tracking-Assisted Segmentation of Biological Cells", "comments": "Accepted in NeurIPS2019, Medical Imaging meets NeurIPS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  U-Net and its variants have been demonstrated to work sufficiently well in\nbiological cell tracking and segmentation. However, these methods still suffer\nin the presence of complex processes such as collision of cells, mitosis and\napoptosis. In this paper, we augment U-Net with Siamese matching-based tracking\nand propose to track individual nuclei over time. By modelling the behavioural\npattern of the cells, we achieve improved segmentation and tracking\nperformances through a re-segmentation procedure. Our preliminary\ninvestigations on the Fluo-N2DH-SIM+ and Fluo-N2DH-GOWT1 datasets demonstrate\nthat absolute improvements of up to 3.8 % and 3.4% can be obtained in\nsegmentation and tracking accuracy, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 09:49:01 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Gupta", "Deepak K.", ""], ["de Bruijn", "Nathan", ""], ["Panteli", "Andreas", ""], ["Gavves", "Efstratios", ""]]}, {"id": "1910.08737", "submitter": "Jan Klopp", "authors": "Jan P. Klopp, Liang-Gee Chen, Shao-Yi Chien", "title": "Utilising Low Complexity CNNs to Lift Non-Local Redundancies in Video\n  Coding", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital media is ubiquitous and produced in ever-growing quantities. This\nnecessitates a constant evolution of compression techniques, especially for\nvideo, in order to maintain efficient storage and transmission. In this work,\nwe aim at exploiting non-local redundancies in video data that remain difficult\nto erase for conventional video codecs. We design convolutional neural networks\nwith a particular emphasis on low memory and computational footprint. The\nparameters of those networks are trained on the fly, at encoding time, to\npredict the residual signal from the decoded video signal. After the training\nprocess has converged, the parameters are compressed and signalled as part of\nthe code of the underlying video codec. The method can be applied to any\nexisting video codec to increase coding gains while its low computational\nfootprint allows for an application under resource-constrained conditions.\nBuilding on top of High Efficiency Video Coding, we achieve coding gains\nsimilar to those of pretrained denoising CNNs while only requiring about 1% of\ntheir computational complexity. Through extensive experiments, we provide\ninsights into the effectiveness of our network design decisions. In addition,\nwe demonstrate that our algorithm delivers stable performance under conditions\nmet in practical video compression: our algorithm performs without significant\nperformance loss on very long random access segments (up to 256 frames) and\nwith moderate performance drops can even be applied to single frames in\nhigh-resolution low delay settings.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 10:09:39 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 05:53:52 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Klopp", "Jan P.", ""], ["Chen", "Liang-Gee", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "1910.08761", "submitter": "Tao Li", "authors": "Ratheesh Kalarot, Tao Li, Fatih Porikli", "title": "Component Attention Guided Face Super-Resolution Network: CAGFace", "comments": "Submitted to WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make the best use of the underlying structure of faces, the collective\ninformation through face datasets and the intermediate estimates during the\nupsampling process, here we introduce a fully convolutional multi-stage neural\nnetwork for 4$\\times$ super-resolution for face images. We implicitly impose\nfacial component-wise attention maps using a segmentation network to allow our\nnetwork to focus on face-inherent patterns. Each stage of our network is\ncomposed of a stem layer, a residual backbone, and spatial upsampling layers.\nWe recurrently apply stages to reconstruct an intermediate image, and then\nreuse its space-to-depth converted versions to bootstrap and enhance image\nquality progressively. Our experiments show that our face super-resolution\nmethod achieves quantitatively superior and perceptually pleasing results in\ncomparison to state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 12:28:14 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Kalarot", "Ratheesh", ""], ["Li", "Tao", ""], ["Porikli", "Fatih", ""]]}, {"id": "1910.08773", "submitter": "Dave Cliff", "authors": "Vansh Dassani, Jon Bird, and Dave Cliff", "title": "Automated Composition of Picture-Synched Music Soundtracks for Movies", "comments": "To be presented at the 16th ACM SIGGRAPH European Conference on\n  Visual Media Production. London, England: 17th-18th December 2019. 10 pages,\n  9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the implementation of and early results from a system that\nautomatically composes picture-synched musical soundtracks for videos and\nmovies. We use the phrase \"picture-synched\" to mean that the structure of the\nautomatically composed music is determined by visual events in the input movie,\ni.e. the final music is synchronised to visual events and features such as cut\ntransitions or within-shot key-frame events. Our system combines automated\nvideo analysis and computer-generated music-composition techniques to create\nunique soundtracks in response to the video input, and can be thought of as an\ninitial step in creating a computerised replacement for a human composer\nwriting music to fit the picture-locked edit of a movie. Working only from the\nvideo information in the movie, key features are extracted from the input\nvideo, using video analysis techniques, which are then fed into a\nmachine-learning-based music generation tool, to compose a piece of music from\nscratch. The resulting soundtrack is tied to video features, such as scene\ntransition markers and scene-level energy values, and is unique to the input\nvideo. Although the system we describe here is only a preliminary\nproof-of-concept, user evaluations of the output of the system have been\npositive.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 13:51:57 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Dassani", "Vansh", ""], ["Bird", "Jon", ""], ["Cliff", "Dave", ""]]}, {"id": "1910.08787", "submitter": "Qiang Chen", "authors": "Qiang Chen, Anda Cheng, Xiangyu He, Peisong Wang, Jian Cheng", "title": "SpatialFlow: Bridging All Tasks for Panoptic Segmentation", "comments": "Accepted to IEEE TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object location is fundamental to panoptic segmentation as it is related to\nall things and stuff in the image scene. Knowing the locations of objects in\nthe image provides clues for segmenting and helps the network better understand\nthe scene. How to integrate object location in both thing and stuff\nsegmentation is a crucial problem. In this paper, we propose spatial\ninformation flows to achieve this objective. The flows can bridge all sub-tasks\nin panoptic segmentation by delivering the object's spatial context from the\nbox regression task to others. More importantly, we design four parallel\nsub-networks to get a preferable adaptation of object spatial information in\nsub-tasks. Upon the sub-networks and the flows, we present a location-aware and\nunified framework for panoptic segmentation, denoted as SpatialFlow. We perform\na detailed ablation study on each component and conduct extensive experiments\nto prove the effectiveness of SpatialFlow. Furthermore, we achieve\nstate-of-the-art results, which are $47.9$ PQ and $62.5$ PQ respectively on\nMS-COCO and Cityscapes panoptic benchmarks. Code will be available at\nhttps://github.com/chensnathan/SpatialFlow.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 15:16:27 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 16:18:20 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 09:21:09 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Chen", "Qiang", ""], ["Cheng", "Anda", ""], ["He", "Xiangyu", ""], ["Wang", "Peisong", ""], ["Cheng", "Jian", ""]]}, {"id": "1910.08790", "submitter": "Megh Shukla", "authors": "Megh Shukla, Biplab Banerjee, Krishna Mohan Buddhiraju", "title": "LEt-SNE: A Hybrid Approach To Data Embedding and Visualization of\n  Hyperspectral Imagery", "comments": "Accepted, ICASSP 2020", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053924", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral Imagery (and Remote Sensing in general) captured from UAVs or\nsatellites are highly voluminous in nature due to the large spatial extent and\nwavelengths captured by them. Since analyzing these images requires a huge\namount of computational time and power, various dimensionality reduction\ntechniques have been used for feature reduction. Some popular techniques among\nthese falter when applied to Hyperspectral Imagery due to the famed curse of\ndimensionality. In this paper, we propose a novel approach, LEt-SNE, which\ncombines graph based algorithms like t-SNE and Laplacian Eigenmaps into a model\nparameterized by a shallow feed forward network. We introduce a new term,\nCompression Factor, that enables our method to combat the curse of\ndimensionality. The proposed algorithm is suitable for manifold visualization\nand sample clustering with labelled or unlabelled data. We demonstrate that our\nmethod is competitive with current state-of-the-art methods on hyperspectral\nremote sensing datasets in public domain.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 15:45:15 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 17:02:12 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Shukla", "Megh", ""], ["Banerjee", "Biplab", ""], ["Buddhiraju", "Krishna Mohan", ""]]}, {"id": "1910.08811", "submitter": "Juil Sock", "authors": "Juil Sock, Guillermo Garcia-Hernando and Tae-Kyun Kim", "title": "Active 6D Multi-Object Pose Estimation in Cluttered Scenarios with Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore how a strategic selection of camera movements can\nfacilitate the task of 6D multi-object pose estimation in cluttered scenarios\nwhile respecting real-world constraints important in robotics and augmented\nreality applications, such as time and distance traveled. In the proposed\nframework, a set of multiple object hypotheses is given to an agent, which is\ninferred by an object pose estimator and subsequently spatio-temporally\nselected by a fusion function that makes use of a verification score that\ncircumvents the need of ground-truth annotations. The agent reasons about these\nhypotheses, directing its attention to the object which it is most uncertain\nabout, moving the camera towards such an object. Unlike previous works that\npropose short-sighted policies, our agent is trained in simulated scenarios\nusing reinforcement learning, attempting to learn the camera moves that produce\nthe most accurate object poses hypotheses for a given temporal and spatial\nbudget, without the need of viewpoints rendering during inference. Our\nexperiments show that the proposed approach successfully estimates the 6D\nobject pose of a stack of objects in both challenging cluttered synthetic and\nreal scenarios, showing superior performance compared to strong baselines.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 17:56:43 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Sock", "Juil", ""], ["Garcia-Hernando", "Guillermo", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1910.08812", "submitter": "Marc-Andr\\'e Gardner", "authors": "Marc-Andr\\'e Gardner, Yannick Hold-Geoffroy, Kalyan Sunkavalli,\n  Christian Gagn\\'e and Jean-Fran\\c{c}ois Lalonde", "title": "Deep Parametric Indoor Lighting Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to estimate lighting from a single image of an indoor\nscene. Previous work has used an environment map representation that does not\naccount for the localized nature of indoor lighting. Instead, we represent\nlighting as a set of discrete 3D lights with geometric and photometric\nparameters. We train a deep neural network to regress these parameters from a\nsingle image, on a dataset of environment maps annotated with depth. We propose\na differentiable layer to convert these parameters to an environment map to\ncompute our loss; this bypasses the challenge of establishing correspondences\nbetween estimated and ground truth lights. We demonstrate, via quantitative and\nqualitative evaluations, that our representation and training scheme lead to\nmore accurate results compared to previous work, while allowing for more\nrealistic 3D object compositing with spatially-varying lighting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 17:57:01 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Gardner", "Marc-Andr\u00e9", ""], ["Hold-Geoffroy", "Yannick", ""], ["Sunkavalli", "Kalyan", ""], ["Gagn\u00e9", "Christian", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1910.08823", "submitter": "Sylvestre-Alvise Rebuffi", "authors": "Sylvestre-Alvise Rebuffi, Ruth Fong, Xu Ji, Hakan Bilen, Andrea\n  Vedaldi", "title": "NormGrad: Finding the Pixels that Matter for Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The different families of saliency methods, either based on contrastive\nsignals, closed-form formulas mixing gradients with activations or on\nperturbation masks, all focus on which parts of an image are responsible for\nthe model's inference. In this paper, we are rather interested by the locations\nof an image that contribute to the model's training. First, we propose a\nprincipled attribution method that we extract from the summation formula used\nto compute the gradient of the weights for a 1x1 convolutional layer. The\nresulting formula is fast to compute and can used throughout the network,\nallowing us to efficiently produce fined-grained importance maps. We will show\nhow to extend it in order to compute saliency maps at any targeted point within\nthe network. Secondly, to make the attribution really specific to the training\nof the model, we introduce a meta-learning approach for saliency methods by\nconsidering an inner optimisation step within the loss. This way, we do not aim\nat identifying the parts of an image that contribute to the model's output but\nrather the locations that are responsible for the good training of the model on\nthis image. Conversely, we also show that a similar meta-learning approach can\nbe used to extract the adversarial locations which can lead to the degradation\nof the model.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 19:16:20 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Rebuffi", "Sylvestre-Alvise", ""], ["Fong", "Ruth", ""], ["Ji", "Xu", ""], ["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1910.08845", "submitter": "Li-Heng Chen", "authors": "Li-Heng Chen, Christos G. Bampis, Zhi Li, Andrey Norkin, Alan C. Bovik", "title": "ProxIQA: A Proxy Approach to Perceptual Optimization of Learned Image\n  Compression", "comments": "12 pages, 12 figures, 5 tables", "journal-ref": null, "doi": "10.1109/TIP.2020.3036752", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of $\\ell_p$ $(p=1,2)$ norms has largely dominated the measurement of\nloss in neural networks due to their simplicity and analytical properties.\nHowever, when used to assess the loss of visual information, these simple norms\nare not very consistent with human perception. Here, we describe a different\n\"proximal\" approach to optimize image analysis networks against quantitative\nperceptual models. Specifically, we construct a proxy network, broadly termed\nProxIQA, which mimics the perceptual model while serving as a loss layer of the\nnetwork. We experimentally demonstrate how this optimization framework can be\napplied to train an end-to-end optimized image compression network. By building\non top of an existing deep image compression model, we are able to demonstrate\na bitrate reduction of as much as $31\\%$ over MSE optimization, given a\nspecified perceptual quality (VMAF) level.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 21:07:33 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 15:48:02 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Chen", "Li-Heng", ""], ["Bampis", "Christos G.", ""], ["Li", "Zhi", ""], ["Norkin", "Andrey", ""], ["Bovik", "Alan C.", ""]]}, {"id": "1910.08853", "submitter": "Peng Liu", "authors": "Peng Liu, Xiaoxiao Zhou, Junyi Yang, El Basha Mohammad D, Ruogu Fang", "title": "Image Restoration Using Deep Regulated Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the depth of convolutional neural networks has attracted substantial\nattention in the deep learning research, the width of these networks has\nrecently received greater interest. The width of networks, defined as the size\nof the receptive fields and the density of the channels, has demonstrated\ncrucial importance in low-level vision tasks such as image denoising and\nrestoration. However, the limited generalization ability, due to the increased\nwidth of networks, creates a bottleneck in designing wider networks. In this\npaper, we propose the Deep Regulated Convolutional Network (RC-Net), a deep\nnetwork composed of regulated sub-network blocks cascaded by skip-connections,\nto overcome this bottleneck. Specifically, the Regulated Convolution block\n(RC-block), featured by a combination of large and small convolution filters,\nbalances the effectiveness of prominent feature extraction and the\ngeneralization ability of the network. RC-Nets have several compelling\nadvantages: they embrace diversified features through large-small filter\ncombinations, alleviate the hazy boundary and blurred details in image\ndenoising and super-resolution problems, and stabilize the learning process.\nOur proposed RC-Nets outperform state-of-the-art approaches with significant\nperformance gains in various image restoration tasks while demonstrating\npromising generalization ability. The code is available at\nhttps://github.com/cswin/RC-Nets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 22:30:23 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Liu", "Peng", ""], ["Zhou", "Xiaoxiao", ""], ["Yang", "Junyi", ""], ["D", "El Basha Mohammad", ""], ["Fang", "Ruogu", ""]]}, {"id": "1910.08854", "submitter": "Cristian Canton Ferrer", "authors": "Brian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram, Cristian Canton\n  Ferrer", "title": "The Deepfake Detection Challenge (DFDC) Preview Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a preview of the Deepfakes Detection Challenge\n(DFDC) dataset consisting of 5K videos featuring two facial modification\nalgorithms. A data collection campaign has been carried out where participating\nactors have entered into an agreement to the use and manipulation of their\nlikenesses in our creation of the dataset. Diversity in several axes (gender,\nskin-tone, age, etc.) has been considered and actors recorded videos with\narbitrary backgrounds thus bringing visual variability. Finally, a set of\nspecific metrics to evaluate the performance have been defined and two existing\nmodels for detecting deepfakes have been tested to provide a reference\nperformance baseline. The DFDC dataset preview can be downloaded at:\ndeepfakedetectionchallenge.ai\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 22:35:52 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 18:47:35 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Dolhansky", "Brian", ""], ["Howes", "Russ", ""], ["Pflaum", "Ben", ""], ["Baram", "Nicole", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "1910.08867", "submitter": "Peng Liu", "authors": "Peng Liu, Xiaoxiao Zhou, Junyiyang Li, El Basha Mohammad D, Ruogu Fang", "title": "KRNET: Image Denoising with Kernel Regulation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One popular strategy for image denoising is to design a generalized\nregularization term that is capable of exploring the implicit prior underlying\ndata observation. Convolutional neural networks (CNN) have shown the powerful\ncapability to learn image prior information through a stack of layers defined\nby a combination of kernels (filters) on the input. However, existing CNN-based\nmethods mainly focus on synthetic gray-scale images. These methods still\nexhibit low performance when tackling multi-channel color image denoising. In\nthis paper, we optimize CNN regularization capability by developing a kernel\nregulation module. In particular, we propose a kernel regulation network-block,\nreferred to as KR-block, by integrating the merits of both large and small\nkernels, that can effectively estimate features in solving image denoising. We\nbuild a deep CNN-based denoiser, referred to as KRNET, via concatenating\nmultiple KR-blocks. We evaluate KRNET on additive white Gaussian noise (AWGN),\nmulti-channel (MC) noise, and realistic noise, where KRNET obtains significant\nperformance gains over state-of-the-art methods across a wide spectrum of noise\nlevels.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 01:10:14 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Liu", "Peng", ""], ["Zhou", "Xiaoxiao", ""], ["Li", "Junyiyang", ""], ["D", "El Basha Mohammad", ""], ["Fang", "Ruogu", ""]]}, {"id": "1910.08878", "submitter": "Jiancheng Yang", "authors": "Jiancheng Yang, Rongyao Fang, Bingbing Ni, Yamin Li, Yi Xu, Linguo Li", "title": "Probabilistic Radiomics: Ambiguous Diagnosis with Controllable Shape\n  Analysis", "comments": "MICCAI 2019 (early accept), with supplementary materials", "journal-ref": null, "doi": "10.1007/978-3-030-32226-7_73", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radiomics analysis has achieved great success in recent years. However,\nconventional Radiomics analysis suffers from insufficiently expressive\nhand-crafted features. Recently, emerging deep learning techniques, e.g.,\nconvolutional neural networks (CNNs), dominate recent research in\nComputer-Aided Diagnosis (CADx). Unfortunately, as black-box predictors, we\nargue that CNNs are \"diagnosing\" voxels (or pixels), rather than lesions; in\nother words, visual saliency from a trained CNN is not necessarily concentrated\non the lesions. On the other hand, classification in clinical applications\nsuffers from inherent ambiguities: radiologists may produce diverse diagnosis\non challenging cases. To this end, we propose a controllable and explainable\n{\\em Probabilistic Radiomics} framework, by combining the Radiomics analysis\nand probabilistic deep learning. In our framework, 3D CNN feature is extracted\nupon lesion region only, then encoded into lesion representation, by a\ncontrollable Non-local Shape Analysis Module (NSAM) based on self-attention.\nInspired from variational auto-encoders (VAEs), an Ambiguity PriorNet is used\nto approximate the ambiguity distribution over human experts. The final\ndiagnosis is obtained by combining the ambiguity prior sample and lesion\nrepresentation, and the whole network named $DenseSharp^{+}$ is end-to-end\ntrainable. We apply the proposed method on lung nodule diagnosis on LIDC-IDRI\ndatabase to validate its effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 02:41:07 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yang", "Jiancheng", ""], ["Fang", "Rongyao", ""], ["Ni", "Bingbing", ""], ["Li", "Yamin", ""], ["Xu", "Yi", ""], ["Li", "Linguo", ""]]}, {"id": "1910.08897", "submitter": "Yuwang Wang", "authors": "Junsheng Zhou, Yuwang Wang, Kaihuai Qin, Wenjun Zeng", "title": "Unsupervised High-Resolution Depth Learning From Videos With Dual\n  Networks", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised depth learning takes the appearance difference between a target\nview and a view synthesized from its adjacent frame as supervisory signal.\nSince the supervisory signal only comes from images themselves, the resolution\nof training data significantly impacts the performance. High-resolution images\ncontain more fine-grained details and provide more accurate supervisory signal.\nHowever, due to the limitation of memory and computation power, the original\nimages are typically down-sampled during training, which suffers heavy loss of\ndetails and disparity accuracy. In order to fully explore the information\ncontained in high-resolution data, we propose a simple yet effective dual\nnetworks architecture, which can directly take high-resolution images as input\nand generate high-resolution and high-accuracy depth map efficiently. We also\npropose a Self-assembled Attention (SA-Attention) module to handle low-texture\nregion. The evaluation on the benchmark KITTI and Make3D datasets demonstrates\nthat our method achieves state-of-the-art results in the monocular depth\nestimation task.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 04:50:47 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhou", "Junsheng", ""], ["Wang", "Yuwang", ""], ["Qin", "Kaihuai", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1910.08898", "submitter": "Yuwang Wang", "authors": "Junsheng Zhou, Yuwang Wang, Kaihuai Qin, Wenjun Zeng", "title": "Moving Indoor: Unsupervised Video Depth Learning in Challenging\n  Environments", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently unsupervised learning of depth from videos has made remarkable\nprogress and the results are comparable to fully supervised methods in outdoor\nscenes like KITTI. However, there still exist great challenges when directly\napplying this technology in indoor environments, e.g., large areas of\nnon-texture regions like white wall, more complex ego-motion of handheld\ncamera, transparent glasses and shiny objects. To overcome these problems, we\npropose a new optical-flow based training paradigm which reduces the difficulty\nof unsupervised learning by providing a clearer training target and handles the\nnon-texture regions. Our experimental evaluation demonstrates that the result\nof our method is comparable to fully supervised methods on the NYU Depth V2\nbenchmark. To the best of our knowledge, this is the first quantitative result\nof purely unsupervised learning method reported on indoor datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 04:56:54 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhou", "Junsheng", ""], ["Wang", "Yuwang", ""], ["Qin", "Kaihuai", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1910.08901", "submitter": "Hongxin Lin", "authors": "Zelin Xiao, Hongxin Lin, Renjie Li, Hongyang Chao, Shengyong Ding", "title": "Endowing Deep 3D Models with Rotation Invariance Based on Principal\n  Component Analysis", "comments": "8 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple yet effective method to endow deep 3D\nmodels with rotation invariance by expressing the coordinates in an intrinsic\nframe determined by the object shape itself. Key to our approach is to find\nsuch an intrinsic frame which should be unique to the identical object shape\nand consistent across different instances of the same category, e.g. the frame\naxes of desks should be all roughly along the edges. Interestingly, the\nprincipal component analysis exactly provides an effective way to define such a\nframe, i.e. setting the principal components as the frame axes. As the\nprincipal components have direction ambiguity caused by the sign-ambiguity of\neigenvector computation, there exist several intrinsic frames for each object.\nIn order to achieve absolute rotation invariance for a deep model, we adopt the\ncoordinates expressed in all intrinsic frames as inputs to obtain multiple\noutput features, which will be further aggregated as a final feature via a\nself-attention module. Our method is theoretically rotation-invariant and can\nbe flexibly embedded into the current network architectures. Comprehensive\nexperiments demonstrate that our approach can achieve near state-of-the-art\nperformance on rotation-augmented dataset for ModelNet40 classification and\noutperform other models on SHREC'17 perturbed retrieval task.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 05:10:36 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xiao", "Zelin", ""], ["Lin", "Hongxin", ""], ["Li", "Renjie", ""], ["Chao", "Hongyang", ""], ["Ding", "Shengyong", ""]]}, {"id": "1910.08914", "submitter": "Yuhang Li", "authors": "Yuhang Li, Xuejin Chen, Feng Wu, and Zheng-Jun Zha", "title": "LinesToFacePhoto: Face Photo Generation from Lines with Conditional\n  Self-Attention Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the task of generating photo-realistic face images\nfrom lines. Previous methods based on conditional generative adversarial\nnetworks (cGANs) have shown their power to generate visually plausible images\nwhen a conditional image and an output image share well-aligned structures.\nHowever, these models fail to synthesize face images with a whole set of\nwell-defined structures, e.g. eyes, noses, mouths, etc., especially when the\nconditional line map lacks one or several parts. To address this problem, we\npropose a conditional self-attention generative adversarial network (CSAGAN).\nWe introduce a conditional self-attention mechanism to cGANs to capture\nlong-range dependencies between different regions in faces. We also build a\nmulti-scale discriminator. The large-scale discriminator enforces the\ncompleteness of global structures and the small-scale discriminator encourages\nfine details, thereby enhancing the realism of generated face images. We\nevaluate the proposed model on the CelebA-HD dataset by two perceptual user\nstudies and three quantitative metrics. The experiment results demonstrate that\nour method generates high-quality facial images while preserving facial\nstructures. Our results outperform state-of-the-art methods both quantitatively\nand qualitatively.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 07:05:24 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Li", "Yuhang", ""], ["Chen", "Xuejin", ""], ["Wu", "Feng", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "1910.08926", "submitter": "Mohamed Karim Belaid", "authors": "Van Bach Nguyen, Belaid Mohamed Karim, Bao Long Vu, J\\\"org\n  Schl\\\"otterer, Michael Granitzer", "title": "Policy Learning for Malaria Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sequential decision making is a typical problem in reinforcement learning\nwith plenty of algorithms to solve it. However, only a few of them can work\neffectively with a very small number of observations. In this report, we\nintroduce the progress to learn the policy for Malaria Control as a\nReinforcement Learning problem in the KDD Cup Challenge 2019 and propose\ndiverse solutions to deal with the limited observations problem. We apply the\nGenetic Algorithm, Bayesian Optimization, Q-learning with sequence breaking to\nfind the optimal policy for five years in a row with only 20 episodes/100\nevaluations. We evaluate those algorithms and compare their performance with\nRandom Search as a baseline. Among these algorithms, Q-Learning with sequence\nbreaking has been submitted to the challenge and got ranked 7th in KDD Cup.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 08:19:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Nguyen", "Van Bach", ""], ["Karim", "Belaid Mohamed", ""], ["Vu", "Bao Long", ""], ["Schl\u00f6tterer", "J\u00f6rg", ""], ["Granitzer", "Michael", ""]]}, {"id": "1910.08930", "submitter": "Subham Banga", "authors": "Vanita Jain, Piyush Agrawal, Subham Banga, Rishabh Kapoor and Shashwat\n  Gulyani", "title": "Sketch2Code: Transformation of Sketches to UI in Real-time Using Deep\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User Interface (UI) prototyping is a necessary step in the early stages of\napplication development. Transforming sketches of a Graphical User Interface\n(UI) into a coded UI application is an uninspired but time-consuming task\nperformed by a UI designer. An automated system that can replace human efforts\nfor straightforward implementation of UI designs will greatly speed up this\nprocedure. The works that propose such a system primarily focus on using UI\nwireframes as input rather than hand-drawn sketches. In this paper, we put\nforward a novel approach wherein we employ a Deep Neural Network that is\ntrained on our custom database of such sketches to detect UI elements in the\ninput sketch. Detection of objects in sketches is a peculiar visual recognition\ntask that requires a specific solution that our deep neural network model\nattempts to provide. The output from the network is a platform-independent UI\nrepresentation object. The UI representation object is a dictionary of\nkey-value pairs to represent the UI elements recognized along with their\nproperties. This is further consumed by our UI parser which creates code for\ndifferent platforms. The intrinsic platform-independence allows the model to\ncreate a UI prototype for multiple platforms with single training. This\ntwo-step approach without the need for two trained models improves over other\nmethods giving time-efficient results (average time: 129 ms) with good\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 08:59:11 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Jain", "Vanita", ""], ["Agrawal", "Piyush", ""], ["Banga", "Subham", ""], ["Kapoor", "Rishabh", ""], ["Gulyani", "Shashwat", ""]]}, {"id": "1910.08952", "submitter": "Patrick Putzky", "authors": "Patrick Putzky, Dimitrios Karkalousos, Jonas Teuwen, Nikita Miriakov,\n  Bart Bakker, Matthan Caan, Max Welling", "title": "i-RIM applied to the fastMRI challenge", "comments": "Abstract submitted to the fastMRI challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We, team AImsterdam, summarize our submission to the fastMRI challenge\n(Zbontar et al., 2018). Our approach builds on recent advances in invertible\nlearning to infer models as presented in Putzky and Welling (2019). Both, our\nsingle-coil and our multi-coil model share the same basic architecture.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 11:32:22 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Putzky", "Patrick", ""], ["Karkalousos", "Dimitrios", ""], ["Teuwen", "Jonas", ""], ["Miriakov", "Nikita", ""], ["Bakker", "Bart", ""], ["Caan", "Matthan", ""], ["Welling", "Max", ""]]}, {"id": "1910.08963", "submitter": "Arnaud Boutillon", "authors": "Arnaud Boutillon, Bhushan Borotikar, Val\\'erie Burdin, Pierre-Henri\n  Conze", "title": "Combining Shape Priors with Conditional Adversarial Networks for\n  Improved Scapula Segmentation in MR images", "comments": null, "journal-ref": null, "doi": "10.1109/ISBI45749.2020.9098360", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an automatic method for scapula bone segmentation from\nMagnetic Resonance (MR) images using deep learning. The purpose of this work is\nto incorporate anatomical priors into a conditional adversarial framework,\ngiven a limited amount of heterogeneous annotated images. Our approach\nencourages the segmentation model to follow the global anatomical properties of\nthe underlying anatomy through a learnt non-linear shape representation while\nthe adversarial contribution refines the model by promoting realistic\ndelineations. These contributions are evaluated on a dataset of 15 pediatric\nshoulder examinations, and compared to state-of-the-art architectures including\nUNet and recent derivatives. The significant improvements achieved bring new\nperspectives for the pre-operative management of musculo-skeletal diseases.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 12:36:56 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 08:37:04 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 08:36:45 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Boutillon", "Arnaud", ""], ["Borotikar", "Bhushan", ""], ["Burdin", "Val\u00e9rie", ""], ["Conze", "Pierre-Henri", ""]]}, {"id": "1910.08967", "submitter": "Radu Tudor Ionescu", "authors": "Petru Soviany, Claudiu Ardei, Radu Tudor Ionescu, Marius Leordeanu", "title": "Image Difficulty Curriculum for Generative Adversarial Networks (CuGAN)", "comments": "Accepted at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant advances in recent years, Generative Adversarial\nNetworks (GANs) are still notoriously hard to train. In this paper, we propose\nthree novel curriculum learning strategies for training GANs. All strategies\nare first based on ranking the training images by their difficulty scores,\nwhich are estimated by a state-of-the-art image difficulty predictor. Our first\nstrategy is to divide images into gradually more difficult batches. Our second\nstrategy introduces a novel curriculum loss function for the discriminator that\ntakes into account the difficulty scores of the real images. Our third strategy\nis based on sampling from an evolving distribution, which favors the easier\nimages during the initial training stages and gradually converges to a uniform\ndistribution, in which samples are equally likely, regardless of difficulty. We\ncompare our curriculum learning strategies with the classic training procedure\non two tasks: image generation and image translation. Our experiments indicate\nthat all strategies provide faster convergence and superior results. For\nexample, our best curriculum learning strategy applied on spectrally normalized\nGANs (SNGANs) fooled human annotators in thinking that generated CIFAR-like\nimages are real in 25.0% of the presented cases, while the SNGANs trained using\nthe classic procedure fooled the annotators in only 18.4% cases. Similarly, in\nimage translation, the human annotators preferred the images produced by the\nCycle-consistent GAN (CycleGAN) trained using curriculum learning in 40.5%\ncases and those produced by CycleGAN based on classic training in only 19.8%\ncases, 39.7% cases being labeled as ties.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 13:06:26 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 22:19:01 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Soviany", "Petru", ""], ["Ardei", "Claudiu", ""], ["Ionescu", "Radu Tudor", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1910.08993", "submitter": "Albert Berenguel", "authors": "Albert Berenguel Centeno, Oriol Ramos Terrades, Josep Llad\\'os Canet,\n  Cristina Ca\\~nero Morales", "title": "Identity Document and banknote security forensics: a survey", "comments": "35 pages, 5 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfeiting and piracy are a form of theft that has been steadily growing\nin recent years. Banknotes and identity documents are two common objects of\ncounterfeiting. Aiming to detect these counterfeits, the present survey covers\na wide range of anti-counterfeiting security features, categorizing them into\nthree components: security substrate, security inks and security printing.\nrespectively. From the computer vision perspective, we present works in the\nliterature covering these three categories. Other topics, such as history of\ncounterfeiting, effects on society and document experts, counterfeiter types of\nattacks, trends among others are covered. Therefore, from non-experienced to\nprofessionals in security documents, can be introduced or deepen its knowledge\nin anti-counterfeiting measures.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 14:35:15 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Centeno", "Albert Berenguel", ""], ["Terrades", "Oriol Ramos", ""], ["Canet", "Josep Llad\u00f3s", ""], ["Morales", "Cristina Ca\u00f1ero", ""]]}, {"id": "1910.08995", "submitter": "Xinzi He", "authors": "Xinzi He, Baiying Lei, Tianfu Wang", "title": "SANet:Superpixel Attention Network for Skin Lesion Attributes Detection", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate detection of lesion attributes is meaningful for both the\ncomputeraid diagnosis system and dermatologists decisions. However, unlike\nlesion segmentation and melenoma classification, there are few deep learning\nmethods and literatures focusing on this task. Currently, the lesion attribute\ndetection still remains challenging due to the extremely unbalanced class\ndistribution and insufficient samples, as well as large intraclass and low\ninterclass variations. To solve these problems, we propose a deep learning\nframework named superpixel attention network (SANet). Firstly, we segment input\nimages into small regions and shuffle the obtained regions by the random\nshuttle mechanism (RSM). Secondly, we apply the SANet to capture discriminative\nfeatures and reconstruct input images. Specifically, SANet contains two sub\nmodules: superpixel average pooling and superpixel at tention module. We\nintroduce a superpixel average pooling to reformulate the superpixel\nclassification problem as a superpixel segmentation problem and a SAMis\nutilized to focus on discriminative superpixel regions and feature channels.\nFinally, we design a novel but effective loss, namely global balancing loss to\naddress the serious data imbalance in ISIC 2018 Task 2 lesion attributes\ndetection dataset. The proposed method achieves quite good performance on the\nISIC 2018 Task 2 challenge.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 14:42:04 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["He", "Xinzi", ""], ["Lei", "Baiying", ""], ["Wang", "Tianfu", ""]]}, {"id": "1910.09024", "submitter": "Jongmin Yu", "authors": "Jongmin Yu and Hyeontaek Oh", "title": "Boosting Network Weight Separability via Feed-Backward Reconstruction", "comments": "8 pages, 6 figures", "journal-ref": "in IEEE Access, vol. 8, pp. 214923-214931, 2020", "doi": "10.1109/ACCESS.2020.3041470", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new evaluation metric and boosting method for weight\nseparability in neural network design. In contrast to general visual\nrecognition methods designed to encourage both intra-class compactness and\ninter-class separability of latent features, we focus on estimating linear\nindependence of column vectors in weight matrix and improving the separability\nof weight vectors. To this end, we propose an evaluation metric for weight\nseparability based on semi-orthogonality of a matrix and Frobenius distance,\nand the feed-backward reconstruction loss which explicitly encourages weight\nseparability between the column vectors in the weight matrix. The experimental\nresults on image classification and face recognition demonstrate that the\nweight separability boosting via minimization of feed-backward reconstruction\nloss can improve the visual recognition performance, hence universally boosting\nthe performance on various visual recognition tasks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 17:04:40 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 14:05:15 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Yu", "Jongmin", ""], ["Oh", "Hyeontaek", ""]]}, {"id": "1910.09031", "submitter": "Tom Vercauteren", "authors": "Tom Vercauteren, Mathias Unberath, Nicolas Padoy, Nassir Navab", "title": "CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer\n  Assisted Interventions", "comments": null, "journal-ref": null, "doi": "10.1109/JPROC.2019.2946993", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven computational approaches have evolved to enable extraction of\ninformation from medical images with a reliability, accuracy and speed which is\nalready transforming their interpretation and exploitation in clinical\npractice. While similar benefits are longed for in the field of interventional\nimaging, this ambition is challenged by a much higher heterogeneity. Clinical\nworkflows within interventional suites and operating theatres are extremely\ncomplex and typically rely on poorly integrated intra-operative devices,\nsensors, and support infrastructures. Taking stock of some of the most exciting\ndevelopments in machine learning and artificial intelligence for computer\nassisted interventions, we highlight the crucial need to take context and human\nfactors into account in order to address these challenges. Contextual\nartificial intelligence for computer assisted intervention, or CAI4CAI, arises\nas an emerging opportunity feeding into the broader field of surgical data\nscience. Central challenges being addressed in CAI4CAI include how to integrate\nthe ensemble of prior knowledge and instantaneous sensory information from\nexperts, sensors and actuators; how to create and communicate a faithful and\nactionable shared representation of the surgery among a mixed human-AI actor\nteam; how to design interventional systems and associated cognitive shared\ncontrol schemes for online uncertainty-aware collaborative decision making\nultimately producing more precise and reliable interventions.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 17:41:29 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Vercauteren", "Tom", ""], ["Unberath", "Mathias", ""], ["Padoy", "Nicolas", ""], ["Navab", "Nassir", ""]]}, {"id": "1910.09055", "submitter": "Fatih Furkan Yilmaz", "authors": "Fatih Furkan Yilmaz and Reinhard Heckel", "title": "Image recognition from raw labels collected without annotators", "comments": "Version changelog: Added content on ImageNet related experiments;\n  Re-structured the document to incorporate the new content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification problems are typically addressed by first collecting\nexamples with candidate labels, second cleaning the candidate labels manually,\nand third training a deep neural network on the clean examples. The manual\nlabeling step is often the most expensive one as it requires workers to label\nmillions of images. In this paper we propose to work without any explicitly\nlabeled data by i) directly training the deep neural network on the noisy\ncandidate labels, and ii) early stopping the training to avoid overfitting.\nWith this procedure we exploit an intriguing property of standard\noverparameterized convolutional neural networks trained with (stochastic)\ngradient descent: Clean labels are fitted faster than noisy ones. We consider\ntwo classification problems, a subset of ImageNet and CIFAR-10. For both, we\nconstruct large candidate datasets without any explicit human annotations, that\nonly contain 10%-50% correctly labeled examples per class. We show that\ntraining on the candidate examples and regularizing through early stopping\ngives higher test performance for both problems than when training on the\noriginal, clean data. This is possible because the candidate datasets contain a\nhuge number of clean examples, and, as we show in this paper, the noise\ngenerated through the label collection process is not nearly as adversarial for\nlearning as the noise generated by randomly flipping labels.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 19:58:21 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 17:55:12 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 23:11:46 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Yilmaz", "Fatih Furkan", ""], ["Heckel", "Reinhard", ""]]}, {"id": "1910.09057", "submitter": "Wenlin Wang", "authors": "Wenlin Wang, Hongteng Xu, Guoyin Wang, Wenqi Wang, Lawrence Carin", "title": "Zero-Shot Recognition via Optimal Transport", "comments": "To appear in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimal transport (OT) framework for generalized zero-shot\nlearning (GZSL), seeking to distinguish samples for both seen and unseen\nclasses, with the assist of auxiliary attributes. The discrepancy between\nfeatures and attributes is minimized by solving an optimal transport problem.\n{Specifically, we build a conditional generative model to generate features\nfrom seen-class attributes, and establish an optimal transport between the\ndistribution of the generated features and that of the real features.} The\ngenerative model and the optimal transport are optimized iteratively with an\nattribute-based regularizer, that further enhances the discriminative power of\nthe generated features. A classifier is learned based on the features generated\nfor both the seen and unseen classes. In addition to generalized zero-shot\nlearning, our framework is also applicable to standard and transductive ZSL\nproblems. Experiments show that our optimal transport-based method outperforms\nstate-of-the-art methods on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 20:18:18 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 03:52:18 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Wenlin", ""], ["Xu", "Hongteng", ""], ["Wang", "Guoyin", ""], ["Wang", "Wenqi", ""], ["Carin", "Lawrence", ""]]}, {"id": "1910.09061", "submitter": "Tongda Xu", "authors": "Tongda Xu, Ziming Qiu, William Das, Chuiyu Wang, Jack Langerman, Nitin\n  Nair, Orlando Aristizabal, Jonathan Mamou, Daniel H. Turnbull, Jeffrey A.\n  Ketterling, Yao Wang", "title": "Deep Mouse: An End-to-end Auto-context Refinement Framework for Brain\n  Ventricle and Body Segmentation in Embryonic Mice Ultrasound Volumes", "comments": "Full Paper Submission to ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-frequency ultrasound (HFU) is well suited for imaging embryonic mice due\nto its noninvasive and real-time characteristics. However, manual segmentation\nof the brain ventricles (BVs) and body requires substantial time and expertise.\nThis work proposes a novel deep learning based end-to-end auto-context\nrefinement framework, consisting of two stages. The first stage produces a low\nresolution segmentation of the BV and body simultaneously. The resulting\nprobability map for each object (BV or body) is then used to crop a region of\ninterest (ROI) around the target object in both the original image and the\nprobability map to provide context to the refinement segmentation network.\nJoint training of the two stages provides significant improvement in Dice\nSimilarity Coefficient (DSC) over using only the first stage (0.818 to 0.906\nfor the BV, and 0.919 to 0.934 for the body). The proposed method significantly\nreduces the inference time (102.36 to 0.09 s/volume around 1000x faster) while\nslightly improves the segmentation accuracy over the previous methods using\nslide-window approaches.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 20:49:39 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 00:53:23 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Xu", "Tongda", ""], ["Qiu", "Ziming", ""], ["Das", "William", ""], ["Wang", "Chuiyu", ""], ["Langerman", "Jack", ""], ["Nair", "Nitin", ""], ["Aristizabal", "Orlando", ""], ["Mamou", "Jonathan", ""], ["Turnbull", "Daniel H.", ""], ["Ketterling", "Jeffrey A.", ""], ["Wang", "Yao", ""]]}, {"id": "1910.09070", "submitter": "Emre Aksan", "authors": "Emre Aksan, Manuel Kaufmann, Otmar Hilliges", "title": "Structured Prediction Helps 3D Human Motion Modelling", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human motion prediction is a challenging and important task in many computer\nvision application domains. Existing work only implicitly models the spatial\nstructure of the human skeleton. In this paper, we propose a novel approach\nthat decomposes the prediction into individual joints by means of a structured\nprediction layer that explicitly models the joint dependencies. This is\nimplemented via a hierarchy of small-sized neural networks connected\nanalogously to the kinematic chains in the human body as well as a joint-wise\ndecomposition in the loss function. The proposed layer is agnostic to the\nunderlying network and can be used with existing architectures for motion\nmodelling. Prior work typically leverages the H3.6M dataset. We show that some\nstate-of-the-art techniques do not perform well when trained and tested on\nAMASS, a recently released dataset 14 times the size of H3.6M. Our experiments\nindicate that the proposed layer increases the performance of motion\nforecasting irrespective of the base network, joint-angle representation, and\nprediction horizon. We furthermore show that the layer also improves motion\npredictions qualitatively. We make code and models publicly available at\nhttps://ait.ethz.ch/projects/2019/spl.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 22:06:14 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Aksan", "Emre", ""], ["Kaufmann", "Manuel", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1910.09077", "submitter": "Mohamed Chaabane", "authors": "Mohamed Chaabane, Ameni Trabelsi, Nathaniel Blanchard, Ross Beveridge", "title": "Looking Ahead: Anticipating Pedestrians Crossing with Future Frames\n  Prediction", "comments": null, "journal-ref": "WACV 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an end-to-end future-prediction model that focuses\non pedestrian safety. Specifically, our model uses previous video frames,\nrecorded from the perspective of the vehicle, to predict if a pedestrian will\ncross in front of the vehicle. The long term goal of this work is to design a\nfully autonomous system that acts and reacts as a defensive human driver would\n--- predicting future events and reacting to mitigate risk. We focus on\npedestrian-vehicle interactions because of the high risk of harm to the\npedestrian if their actions are miss-predicted. Our end-to-end model consists\nof two stages: the first stage is an encoder/decoder network that learns to\npredict future video frames. The second stage is a deep spatio-temporal network\nthat utilizes the predicted frames of the first stage to predict the\npedestrian's future action. Our system achieves state-of-the-art accuracy on\npedestrian behavior prediction and future frames prediction on the Joint\nAttention for Autonomous Driving (JAAD) dataset.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 23:12:13 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 22:49:17 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Chaabane", "Mohamed", ""], ["Trabelsi", "Ameni", ""], ["Blanchard", "Nathaniel", ""], ["Beveridge", "Ross", ""]]}, {"id": "1910.09085", "submitter": "Jindong Gu", "authors": "Jindong Gu, Volker Tresp", "title": "Semantics for Global and Local Interpretation of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) with high expressiveness have achieved\nstate-of-the-art performance in many tasks. However, their distributed feature\nrepresentations are difficult to interpret semantically. In this work,\nhuman-interpretable semantic concepts are associated with vectors in feature\nspace. The association process is mathematically formulated as an optimization\nproblem. The semantic vectors obtained from the optimal solution are applied to\ninterpret deep neural networks globally and locally. The global interpretations\nare useful to understand the knowledge learned by DNNs. The interpretation of\nlocal behaviors can help to understand individual decisions made by DNNs\nbetter. The empirical experiments demonstrate how to use identified semantics\nto interpret the existing DNNs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:00:17 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Gu", "Jindong", ""], ["Tresp", "Volker", ""]]}, {"id": "1910.09086", "submitter": "Jindong Gu", "authors": "Jindong Gu, Volker Tresp", "title": "Contextual Prediction Difference Analysis for Explaining Individual\n  Image Classifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much effort has been devoted to understanding the decisions of deep neural\nnetworks in recent years. A number of model-aware saliency methods were\nproposed to explain individual classification decisions by creating saliency\nmaps. However, they are not applicable when the parameters and the gradients of\nthe underlying models are unavailable. Recently, model-agnostic methods have\nalso received attention. As one of them, \\textit{Prediction Difference\nAnalysis} (PDA), a probabilistic sound methodology, was proposed. In this work,\nwe first show that PDA can suffer from saturated classifiers. The saturation\nphenomenon of classifiers exists widely in current neural network-based\nclassifiers. To explain the decisions of saturated classifiers better, we\nfurther propose Contextual PDA, which runs hundreds of times faster than PDA.\nThe experiments show the superiority of our method by explaining image\nclassifications of the state-of-the-art deep convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:04:22 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 00:41:19 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Gu", "Jindong", ""], ["Tresp", "Volker", ""]]}, {"id": "1910.09090", "submitter": "Jinwei Zhao", "authors": "Jinwei Zhao, Qizhou Wang, Fuqiang Zhang, Wanli Qiu, Yufei Wang, Yu\n  Liu, Guo Xie, Weigang Ma, Bin Wang, Xinhong Hei", "title": "A game method for improving the interpretability of convolution neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real artificial intelligence always has been focused on by many machine\nlearning researchers, especially in the area of deep learning. However deep\nneural network is hard to be understood and explained, and sometimes, even\nmetaphysics. The reason is, we believe that: the network is essentially a\nperceptual model. Therefore, we believe that in order to complete complex\nintelligent activities from simple perception, it is necessary to con-struct\nanother interpretable logical network to form accurate and reasonable responses\nand explanations to external things. Researchers like Bolei Zhou and Quanshi\nZhang have found many explanatory rules for deep feature extraction aimed at\nthe feature extraction stage of convolution neural network. However, although\nresearchers like Marco Gori have also made great efforts to improve the\ninterpretability of the fully connected layers of the network, the problem is\nalso very difficult. This paper firstly analyzes its reason. Then a method of\nconstructing logical network based on the fully connected layers and extracting\nlogical relation between input and output of the layers is proposed. The game\nprocess between perceptual learning and logical abstract cognitive learning is\nimplemented to improve the interpretable performance of deep learning process\nand deep learning model. The benefits of our approach are illustrated on\nbenchmark data sets and in real-world experiments.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:32:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhao", "Jinwei", ""], ["Wang", "Qizhou", ""], ["Zhang", "Fuqiang", ""], ["Qiu", "Wanli", ""], ["Wang", "Yufei", ""], ["Liu", "Yu", ""], ["Xie", "Guo", ""], ["Ma", "Weigang", ""], ["Wang", "Bin", ""], ["Hei", "Xinhong", ""]]}, {"id": "1910.09094", "submitter": "Florent Chiaroni", "authors": "Sid Ali Hamideche, Florent Chiaroni, Mohamed-Cherif Rahal", "title": "Self-supervised classification of dynamic obstacles using the temporal\n  information provided by videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, autonomous driving systems can detect, segment, and classify the\nsurrounding obstacles using a monocular camera. However, state-of-the-art\nmethods solving these tasks generally perform a fully supervised learning\nprocess and require a large amount of training labeled data. On another note,\nsome self-supervised learning approaches can deal with detection and\nsegmentation of dynamic obstacles using the temporal information available in\nvideo sequences. In this work, we propose to classify the detected obstacles\ndepending on their motion pattern. We present a novel self-supervised framework\nconsisting of learning offline clusters from temporal patch sequences and\nconsidering these clusters as labeled sets to train a real-time image\nclassifier. The presented model outperforms state-of-the-art unsupervised image\nclassification methods on large-scale diverse driving video dataset BDD100K.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:48:14 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 18:41:56 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Hamideche", "Sid Ali", ""], ["Chiaroni", "Florent", ""], ["Rahal", "Mohamed-Cherif", ""]]}, {"id": "1910.09116", "submitter": "Burhaneddin Yaman", "authors": "Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Jutta\n  Ellermann, K\\^amil U\\v{g}urbil, Mehmet Ak\\c{c}akaya", "title": "Self-Supervised Physics-Based Deep Learning MRI Reconstruction Without\n  Fully-Sampled Data", "comments": "5 Pages, 5 Figures", "journal-ref": "Proceedings of IEEE ISBI, 2020", "doi": "10.1109/ISBI45749.2020.9098514", "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has emerged as a tool for improving accelerated MRI\nreconstruction. A common strategy among DL methods is the physics-based\napproach, where a regularized iterative algorithm alternating between data\nconsistency and a regularizer is unrolled for a finite number of iterations.\nThis unrolled network is then trained end-to-end in a supervised manner, using\nfully-sampled data as ground truth for the network output. However, in a number\nof scenarios, it is difficult to obtain fully-sampled datasets, due to\nphysiological constraints such as organ motion or physical constraints such as\nsignal decay. In this work, we tackle this issue and propose a self-supervised\nlearning strategy that enables physics-based DL reconstruction without\nfully-sampled data. Our approach is to divide the acquired sub-sampled points\nfor each scan into training and validation subsets. During training, data\nconsistency is enforced over the training subset, while the validation subset\nis used to define the loss function. Results show that the proposed\nself-supervised learning method successfully reconstructs images without\nfully-sampled data, performing similarly to the supervised approach that is\ntrained with fully-sampled references. This has implications for physics-based\ninverse problem approaches for other settings, where fully-sampled data is not\navailable or possible to acquire.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 02:20:15 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Yaman", "Burhaneddin", ""], ["Hosseini", "Seyed Amir Hossein", ""], ["Moeller", "Steen", ""], ["Ellermann", "Jutta", ""], ["U\u01e7urbil", "K\u00e2mil", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "1910.09119", "submitter": "Fei Deng", "authors": "Fei Deng, Zhuo Zhi, Sungjin Ahn", "title": "Generative Hierarchical Models for Parts, Objects, and Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional structures between parts and objects are inherent in natural\nscenes. Modeling such compositional hierarchies via unsupervised learning can\nbring various benefits such as interpretability and transferability, which are\nimportant in many downstream tasks. In this paper, we propose the first deep\nlatent variable model, called RICH, for learning Representation of\nInterpretable Compositional Hierarchies. At the core of RICH is a latent scene\ngraph representation that organizes the entities of a scene into a tree\nstructure according to their compositional relationships. During inference,\ntaking top-down approach, RICH is able to use higher-level representation to\nguide lower-level decomposition. This avoids the difficult problem of routing\nbetween parts and objects that is faced by bottom-up approaches. In experiments\non images containing multiple objects with different part compositions, we\ndemonstrate that RICH is able to learn the latent compositional hierarchy and\ngenerate imaginary scenes.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 02:28:16 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Deng", "Fei", ""], ["Zhi", "Zhuo", ""], ["Ahn", "Sungjin", ""]]}, {"id": "1910.09122", "submitter": "Chris Cannella", "authors": "Chris Cannella, Jie Ding, Mohammadreza Soltani, Vahid Tarokh", "title": "Perception-Distortion Trade-off with Restricted Boltzmann Machines", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a new procedure for applying Restricted Boltzmann\nMachines (RBMs) to missing data inference tasks, based on linearization of the\neffective energy function governing the distribution of observations. We\ncompare the performance of our proposed procedure with those obtained using\nexisting reconstruction procedures trained on incomplete data. We place these\nperformance comparisons within the context of the perception-distortion\ntrade-off observed in other data reconstruction tasks, which has, until now,\nremained unexplored in tasks relying on incomplete training data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 02:39:28 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Cannella", "Chris", ""], ["Ding", "Jie", ""], ["Soltani", "Mohammadreza", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1910.09134", "submitter": "Jiaying Lu", "authors": "Jiaying Lu, Xin Ye, Yi Ren, Yezhou Yang", "title": "Good, Better, Best: Textual Distractors Generation for Multi-Choice VQA\n  via Policy Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of studies have investigated the decision-making process\nof VQA models. Many of these studies focus on the reason behind the correct\nanswer chosen by a model. Yet, the reason why the distracting answer chose by a\nmodel has rarely been studied. To this end, we introduce a novel task called\n\\textit{textual Distractors Generation for VQA} (DG-VQA) that explaining the\ndecision boundaries of existing VQA models. The goal of DG-VQA is to generate\nthe most confusing set of textual distractors in multi-choice VQA tasks which\nexpose the vulnerability of existing models (i.e. to generate distractors that\nlure existing models to fail). We show that DG-VQA can be formulated as a\nMarkov Decision Process, and present a reinforcement learning solution to come\nup with distractors in an unsupervised manner. The solution addresses the lack\nof large annotated corpus issues in previous distractor generation methods. Our\nproposed model receives reward signals from fully-trained multi-choice VQA\nmodels and updates its parameters via policy gradient. The empirical results\nshow that the generated textual distractors can successfully attack several\npopular VQA models with an average $20\\%$ accuracy drop from $64\\%$.\nFurthermore, we conduct adversarial training to improve the robustness of VQA\nmodels by incorporating the generated distractors. Empirical results validate\nthe effectiveness of adversarial training by showing a performance improvement\nof $27\\%$ for the multi-choice VQA task.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 03:32:17 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 21:01:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lu", "Jiaying", ""], ["Ye", "Xin", ""], ["Ren", "Yi", ""], ["Yang", "Yezhou", ""]]}, {"id": "1910.09139", "submitter": "Polina Zablotskaia", "authors": "Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, Leonid Sigal", "title": "DwNet: Dense warp-based network for pose-guided human video generation", "comments": "Accepted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generation of realistic high-resolution videos of human subjects is a\nchallenging and important task in computer vision. In this paper, we focus on\nhuman motion transfer - generation of a video depicting a particular subject,\nobserved in a single image, performing a series of motions exemplified by an\nauxiliary (driving) video. Our GAN-based architecture, DwNet, leverages dense\nintermediate pose-guided representation and refinement process to warp the\nrequired subject appearance, in the form of the texture, from a source image\ninto a desired pose. Temporal consistency is maintained by further conditioning\nthe decoding process within a GAN on the previously generated frame. In this\nway a video is generated in an iterative and recurrent fashion. We illustrate\nthe efficacy of our approach by showing state-of-the-art quantitative and\nqualitative performance on two benchmark datasets: TaiChi and Fashion Modeling.\nThe latter is collected by us and will be made publicly available to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 03:56:51 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zablotskaia", "Polina", ""], ["Siarohin", "Aliaksandr", ""], ["Zhao", "Bo", ""], ["Sigal", "Leonid", ""]]}, {"id": "1910.09165", "submitter": "Xingyu Liu", "authors": "Xingyu Liu, Mengyuan Yan, Jeannette Bohg", "title": "MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences", "comments": "ICCV 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding dynamic 3D environment is crucial for robotic agents and many\nother applications. We propose a novel neural network architecture called\n$MeteorNet$ for learning representations for dynamic 3D point cloud sequences.\nDifferent from previous work that adopts a grid-based representation and\napplies 3D or 4D convolutions, our network directly processes point clouds. We\npropose two ways to construct spatiotemporal neighborhoods for each point in\nthe point cloud sequence. Information from these neighborhoods is aggregated to\nlearn features per point. We benchmark our network on a variety of 3D\nrecognition tasks including action recognition, semantic segmentation and scene\nflow estimation. MeteorNet shows stronger performance than previous grid-based\nmethods while achieving state-of-the-art performance on Synthia. MeteorNet also\noutperforms previous baseline methods that are able to process at most two\nconsecutive point clouds. To the best of our knowledge, this is the first work\non deep learning for dynamic raw point cloud sequences.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 06:31:48 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 00:22:14 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liu", "Xingyu", ""], ["Yan", "Mengyuan", ""], ["Bohg", "Jeannette", ""]]}, {"id": "1910.09170", "submitter": "Sangwoo Mo", "authors": "Sangwoo Mo, Chiheon Kim, Sungwoong Kim, Minsu Cho, Jinwoo Shin", "title": "Mining GOLD Samples for Conditional GANs", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional generative adversarial networks (cGANs) have gained a\nconsiderable attention in recent years due to its class-wise controllability\nand superior quality for complex generation tasks. We introduce a simple yet\neffective approach to improving cGANs by measuring the discrepancy between the\ndata distribution and the model distribution on given samples. The proposed\nmeasure, coined the gap of log-densities (GOLD), provides an effective\nself-diagnosis for cGANs while being efficienty computed from the\ndiscriminator. We propose three applications of the GOLD: example re-weighting,\nrejection sampling, and active learning, which improve the training, inference,\nand data selection of cGANs, respectively. Our experimental results demonstrate\nthat the proposed methods outperform corresponding baselines for all three\napplications on different image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 06:49:32 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Mo", "Sangwoo", ""], ["Kim", "Chiheon", ""], ["Kim", "Sungwoong", ""], ["Cho", "Minsu", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1910.09182", "submitter": "Shen Chen", "authors": "Shen Chen, Liujuan Cao, Mingbao Lin, Yan Wang, Xiaoshuai Sun, Chenglin\n  Wu, Jingfei Qiu and Rongrong Ji", "title": "Hadamard Codebook Based Deep Hashing", "comments": "8 pages, 7 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an approximate nearest neighbor search technique, hashing has been widely\napplied in large-scale image retrieval due to its excellent efficiency. Most\nsupervised deep hashing methods have similar loss designs with embedding\nlearning, while quantizing the continuous high-dim feature into compact binary\nspace. We argue that the existing deep hashing schemes are defective in two\nissues that seriously affect the performance, i.e., bit independence and bit\nbalance. The former refers to hash codes of different classes should be\nindependent of each other, while the latter means each bit should have a\nbalanced distribution of +1s and -1s. In this paper, we propose a novel\nsupervised deep hashing method, termed Hadamard Codebook based Deep Hashing\n(HCDH), which solves the above two problems in a unified formulation.\nSpecifically, we utilize an off-the-shelf algorithm to generate a binary\nHadamard codebook to satisfy the requirement of bit independence and bit\nbalance, which subsequently serves as the desired outputs of the hash functions\nlearning. We also introduce a projection matrix to solve the inconsistency\nbetween the order of Hadamard matrix and the number of classes. Besides, the\nproposed HCDH further exploits the supervised labels by constructing a\nclassifier on top of the outputs of hash functions. Extensive experiments\ndemonstrate that HCDH can yield discriminative and balanced binary codes, which\nwell outperforms many state-of-the-arts on three widely-used benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 07:33:42 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Chen", "Shen", ""], ["Cao", "Liujuan", ""], ["Lin", "Mingbao", ""], ["Wang", "Yan", ""], ["Sun", "Xiaoshuai", ""], ["Wu", "Chenglin", ""], ["Qiu", "Jingfei", ""], ["Ji", "Rongrong", ""]]}, {"id": "1910.09185", "submitter": "Zhuang Liu", "authors": "Zhuang Liu, Tinghui Zhou, Hung-Ju Wang, Zhiqiang Shen, Bingyi Kang,\n  Evan Shelhamer, Trevor Darrell", "title": "Transferable Recognition-Aware Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in image recognition has stimulated the deployment of vision\nsystems at an unprecedented scale. As a result, visual data are now often\nconsumed not only by humans but also by machines. Existing image processing\nmethods only optimize for better human perception, yet the resulting images may\nnot be accurately recognized by machines. This can be undesirable, e.g., the\nimages can be improperly handled by search engines or recommendation systems.\nIn this work, we propose simple approaches to improve machine interpretability\nof processed images: optimizing the recognition loss directly on the image\nprocessing network or through an intermediate transforming model.\nInterestingly, the processing model's ability to enhance recognition quality\ncan transfer when evaluated on models of different architectures, recognized\ncategories, tasks and training datasets. This makes the solutions applicable\neven when we do not have the knowledge of future recognition models, e.g., if\nwe upload processed images to the Internet. We conduct experiments on multiple\nimage processing tasks, with ImageNet classification and PASCAL VOC detection\nas recognition tasks. With our simple methods, substantial accuracy gain can be\nachieved with strong transferability and minimal image quality loss. Through a\nuser study we further show that the accuracy gain can transfer to a black-box,\nthird-party cloud model. Finally, we try to explain this transferability\nphenomenon by demonstrating the similarities of different models' decision\nboundaries. Code is available at https://github.com/liuzhuang13/Transferable_RA .\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 07:36:15 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 14:32:36 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Liu", "Zhuang", ""], ["Zhou", "Tinghui", ""], ["Wang", "Hung-Ju", ""], ["Shen", "Zhiqiang", ""], ["Kang", "Bingyi", ""], ["Shelhamer", "Evan", ""], ["Darrell", "Trevor", ""]]}, {"id": "1910.09188", "submitter": "Jialiang Zhang", "authors": "Jialiang Zhang, Lixiang Lin, Yang Li, Yun-chen Chen, Jianke Zhu, Yao\n  Hu, Steven C.H. Hoi", "title": "Attribute-aware Pedestrian Detection in a Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is an initial step to perform outdoor scene analysis,\nwhich plays an essential role in many real-world applications. Although having\nenjoyed the merits of deep learning frameworks from the generic object\ndetectors, pedestrian detection is still a very challenging task due to heavy\nocclusion and highly crowded group. Generally, the conventional detectors are\nunable to differentiate individuals from each other effectively under such a\ndense environment. To tackle this critical problem, we propose an\nattribute-aware pedestrian detector to explicitly model people's semantic\nattributes in a high-level feature detection fashion. Besides the typical\nsemantic features, center position, target's scale and offset, we introduce a\npedestrian-oriented attribute feature to encode the high-level semantic\ndifferences among the crowd. Moreover, a novel attribute-feature-based\nNon-Maximum Suppression~(NMS) is proposed to distinguish the person from a\nhighly overlapped group by adaptively rejecting the false-positive results in a\nvery crowd settings. Furthermore, a novel ground truth target is designed to\nalleviate the difficulties caused by the attribute configuration and extremely\nclass imbalance issues during training. Finally, we evaluate our proposed\nattribute-aware pedestrian detector on two benchmark datasets including\nCityPersons and CrowdHuman. The experimental results show that our approach\noutperforms state-of-the-art methods at a large margin on pedestrian detection.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 07:44:26 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 02:58:29 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zhang", "Jialiang", ""], ["Lin", "Lixiang", ""], ["Li", "Yang", ""], ["Chen", "Yun-chen", ""], ["Zhu", "Jianke", ""], ["Hu", "Yao", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1910.09198", "submitter": "He Tang", "authors": "He Tang, Xiaobing Pei, Shilong Huang, Xin Li, Chao Liu", "title": "Automatic Lumbar Spinal CT Image Segmentation with a Dual Densely\n  Connected U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clinical treatment of degenerative and developmental lumbar spinal\nstenosis (LSS) is different. Computed tomography (CT) is helpful in\ndistinguishing degenerative and developmental LSS due to its advantage in\nimaging of osseous and calcified tissues. However, boundaries of the vertebral\nbody, spinal canal and dural sac have low contrast and hard to identify in a CT\nimage, so the diagnosis depends heavily on the knowledge of expert surgeons and\nradiologists. In this paper, we develop an automatic lumbar spinal CT image\nsegmentation method to assist LSS diagnosis. The main contributions of this\npaper are the following: 1) a new lumbar spinal CT image dataset is constructed\nthat contains 2393 axial CT images collected from 279 patients, with the ground\ntruth of pixel-level segmentation labels; 2) a dual densely connected U-shaped\nneural network (DDU-Net) is used to segment the spinal canal, dural sac and\nvertebral body in an end-to-end manner; 3) DDU-Net is capable of segmenting\ntissues with large scale-variant, inconspicuous edges (e.g., spinal canal) and\nextremely small size (e.g., dural sac); and 4) DDU-Net is practical, requiring\nno image preprocessing such as contrast enhancement, registration and\ndenoising, and the running time reaches 12 FPS. In the experiment, we achieve\nstate-of-the-art performance on the lumbar spinal image segmentation task. We\nexpect that the technique will increase both radiology workflow efficiency and\nthe perceived value of radiology reports for referring clinicians and patients.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 08:20:51 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 10:07:05 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Tang", "He", ""], ["Pei", "Xiaobing", ""], ["Huang", "Shilong", ""], ["Li", "Xin", ""], ["Liu", "Chao", ""]]}, {"id": "1910.09212", "submitter": "Yusuke Hosoya", "authors": "Yusuke Hosoya, Masanori Suganuma, Takayuki Okatani", "title": "Analysis and a Solution of Momentarily Missed Detection for Anchor-based\n  Object Detectors", "comments": "Accepted to WACV 2020, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The employment of convolutional neural networks has led to significant\nperformance improvement on the task of object detection. However, when applying\nexisting detectors to continuous frames in a video, we often encounter\nmomentary miss-detection of objects, that is, objects are undetected\nexceptionally at a few frames, although they are correctly detected at all\nother frames. In this paper, we analyze the mechanism of how such\nmiss-detection occurs. For the most popular class of detectors that are based\non anchor boxes, we show the followings: i) besides apparent causes such as\nmotion blur, occlusions, background clutters, etc., the majority of remaining\nmiss-detection can be explained by an improper behavior of the detectors at\nboundaries of the anchor boxes; and ii) this can be rectified by improving the\nway of choosing positive samples from candidate anchor boxes when training the\ndetectors.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 08:57:37 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 15:05:08 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Hosoya", "Yusuke", ""], ["Suganuma", "Masanori", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1910.09217", "submitter": "Bingyi Kang", "authors": "Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo,\n  Jiashi Feng, Yannis Kalantidis", "title": "Decoupling Representation and Classifier for Long-Tailed Recognition", "comments": null, "journal-ref": "Published as a conference paper at ICLR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long-tail distribution of the visual world poses great challenges for\ndeep learning based classification models on how to handle the class imbalance\nproblem. Existing solutions usually involve class-balancing strategies, e.g.,\nby loss re-weighting, data re-sampling, or transfer learning from head- to\ntail-classes, but most of them adhere to the scheme of jointly learning\nrepresentations and classifiers. In this work, we decouple the learning\nprocedure into representation learning and classification, and systematically\nexplore how different balancing strategies affect them for long-tailed\nrecognition. The findings are surprising: (1) data imbalance might not be an\nissue in learning high-quality representations; (2) with representations\nlearned with the simplest instance-balanced (natural) sampling, it is also\npossible to achieve strong long-tailed recognition ability by adjusting only\nthe classifier. We conduct extensive experiments and set new state-of-the-art\nperformance on common long-tailed benchmarks like ImageNet-LT, Places-LT and\niNaturalist, showing that it is possible to outperform carefully designed\nlosses, sampling strategies, even complex modules with memory, by using a\nstraightforward approach that decouples representation and classification. Our\ncode is available at https://github.com/facebookresearch/classifier-balancing.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:03:19 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 15:51:25 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Kang", "Bingyi", ""], ["Xie", "Saining", ""], ["Rohrbach", "Marcus", ""], ["Yan", "Zhicheng", ""], ["Gordo", "Albert", ""], ["Feng", "Jiashi", ""], ["Kalantidis", "Yannis", ""]]}, {"id": "1910.09230", "submitter": "Karim Armanious", "authors": "Karim Armanious, Vijeth Kumar, Sherif Abdulatif, Tobias Hepp, Sergios\n  Gatidis, Bin Yang", "title": "ipA-MedGAN: Inpainting of Arbitrary Regions in Medical Imaging", "comments": "Submitted to IEEE ICIP 2020", "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9191207", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local deformations in medical modalities are common phenomena due to a\nmultitude of factors such as metallic implants or limited field of views in\nmagnetic resonance imaging (MRI). Completion of the missing or distorted\nregions is of special interest for automatic image analysis frameworks to\nenhance post-processing tasks such as segmentation or classification. In this\nwork, we propose a new generative framework for medical image inpainting,\ntitled ipA-MedGAN. It bypasses the limitations of previous frameworks by\nenabling inpainting of arbitrary shaped regions without a prior localization of\nthe regions of interest. Thorough qualitative and quantitative comparisons with\nother inpainting and translational approaches have illustrated the superior\nperformance of the proposed framework for the task of brain MR inpainting.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:34:13 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 16:33:17 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Armanious", "Karim", ""], ["Kumar", "Vijeth", ""], ["Abdulatif", "Sherif", ""], ["Hepp", "Tobias", ""], ["Gatidis", "Sergios", ""], ["Yang", "Bin", ""]]}, {"id": "1910.09233", "submitter": "Arpita Dutta", "authors": "Arpita Dutta, Samit Biswas", "title": "CNN based Extraction of Panels/Characters from Bengali Comic Book Page\n  Images", "comments": "6 pages, 3 tables and 3 figures. Accepted at GREC 2019 in conjunction\n  with ICDAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peoples nowadays prefer to use digital gadgets like cameras or mobile phones\nfor capturing documents. Automatic extraction of panels/characters from the\nimages of a comic document is challenging due to the wide variety of drawing\nstyles adopted by writers, beneficial for readers to read them on mobile\ndevices at any time and useful for automatic digitization. Most of the methods\nfor localization of panel/character rely on the connected component analysis or\npage background mask and are applicable only for a limited comic dataset. This\nwork proposes a panel/character localization architecture based on the features\nof YOLO and CNN for extraction of both panels and characters from comic book\nimages. The method achieved remarkable results on Bengali Comic Book Image\ndataset (BCBId) consisting of total $4130$ images, developed by us as well as\non a variety of publicly available comic datasets in other languages, i.e.\neBDtheque, Manga 109 and DCM dataset.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:37:46 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Dutta", "Arpita", ""], ["Biswas", "Samit", ""]]}, {"id": "1910.09234", "submitter": "Tobias Alt", "authors": "Tobias Alt and Joachim Weickert", "title": "Learning a Generic Adaptive Wavelet Shrinkage Function for Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of machine learning in image processing has created a gap between\ntrainable data-driven and classical model-driven approaches: While\nlearning-based models often show superior performance, classical ones are often\nmore transparent. To reduce this gap, we introduce a generic wavelet shrinkage\nfunction for denoising which is adaptive to both the wavelet scales as well as\nthe noise standard deviation. It is inferred from trained results of a tightly\nparametrised function which is inherited from nonlinear diffusion. Our proposed\nshrinkage function is smooth and compact while only using two parameters. In\ncontrast to many existing shrinkage functions, it is able to enhance image\nstructures by amplifying wavelet coefficients. Experiments show that it\noutperforms classical shrinkage functions by a significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:38:19 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 07:18:23 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2020 08:30:31 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Alt", "Tobias", ""], ["Weickert", "Joachim", ""]]}, {"id": "1910.09244", "submitter": "Jiabo Huang", "authors": "Jiabo Huang, Xiaohua Xie, Wei-Shi Zheng", "title": "Batch Face Alignment using a Low-rank GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of aligning a set of face images of the same\nindividual into a normalized image while removing the outliers like partial\nocclusion, extreme facial expression as well as significant illumination\nvariation. Our model seeks an optimal image domain transformation such that the\nmatrix of misaligned images can be decomposed as the sum of a sparse matrix of\nnoise and a rank-one matrix of aligned images. The image transformation is\nlearned in an unsupervised manner, which means that ground-truth aligned images\nare unnecessary for our model. Specifically, we make use of the remarkable\nnon-linear transforming ability of generative adversarial network(GAN) and\nguide it with low-rank generation as well as sparse noise constraint to achieve\nthe face alignment. We verify the efficacy of the proposed model with extensive\nexperiments on real-world face databases, demonstrating higher accuracy and\nefficiency than existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 10:00:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Huang", "Jiabo", ""], ["Xie", "Xiaohua", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1910.09308", "submitter": "Dominik M\\\"uller", "authors": "Dominik M\\\"uller and Frank Kramer", "title": "MIScnn: A Framework for Medical Image Segmentation with Convolutional\n  Neural Networks and Deep Learning", "comments": "Open-source Python framework available on GitHub\n  (https://github.com/frankkramer-lab/MIScnn) and PyPI (miscnn). 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased availability and usage of modern medical imaging induced a\nstrong need for automatic medical image segmentation. Still, current image\nsegmentation platforms do not provide the required functionalities for plain\nsetup of medical image segmentation pipelines. Already implemented pipelines\nare commonly standalone software, optimized on a specific public data set.\nTherefore, this paper introduces the open-source Python library MIScnn. The aim\nof MIScnn is to provide an intuitive API allowing fast building of medical\nimage segmentation pipelines including data I/O, preprocessing, data\naugmentation, patch-wise analysis, metrics, a library with state-of-the-art\ndeep learning models and model utilization like training, prediction, as well\nas fully automatic evaluation (e.g. cross-validation). Similarly, high\nconfigurability and multiple open interfaces allow full pipeline customization.\nRunning a cross-validation with MIScnn on the Kidney Tumor Segmentation\nChallenge 2019 data set (multi-class semantic segmentation with 300 CT scans)\nresulted into a powerful predictor based on the standard 3D U-Net model. With\nthis experiment, we could show that the MIScnn framework enables researchers to\nrapidly set up a complete medical image segmentation pipeline by using just a\nfew lines of code. The source code for MIScnn is available in the Git\nrepository: https://github.com/frankkramer-lab/MIScnn.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:43:25 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["M\u00fcller", "Dominik", ""], ["Kramer", "Frank", ""]]}, {"id": "1910.09318", "submitter": "Shimiao Jiang", "authors": "Ke Zhan, Shimiao Jiang, Yu Bai, Yi Li, Xu Liu and Zhuoran Xu", "title": "Directed-Weighting Group Lasso for Eltwise Blocked CNN Pruning", "comments": null, "journal-ref": "Proceedings of the British Machine Vision Conference (BMVC), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eltwise layer is a commonly used structure in the multi-branch deep learning\nnetwork. In a filter-wise pruning procedure, due to the specific operation of\nthe eltwise layer, all its previous convolutional layers should vote for which\nfilters by index should be pruned. Since only an intersection of the voted\nfilters is pruned, the compression rate is limited. This work proposes a method\ncalled Directed-Weighting Group Lasso (DWGL), which enforces an index-wise\nincremental (directed) coefficient on the filterlevel group lasso items, so\nthat the low index filters getting high activation tend to be kept while the\nhigh index ones tend to be pruned. When using DWGL, much fewer filters are\nretained during the voting process and the compression rate can be boosted. The\npaper test the proposed method on the ResNet series networks. On CIFAR-10, it\nachieved a 75.34% compression rate on ResNet-56 with a 0.94% error increment,\nand a 52.06% compression rate on ResNet-20 with a 0.72% error increment. On\nImageNet, it achieved a 53% compression rate with ResNet-50 with a 0.6% error\nincrement, speeding up the network by 2.23 times. Furthermore, it achieved a\n75% compression rate on ResNet-50 with a 1.2% error increment, speeding up the\nnetwork by 4 times.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:51:11 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhan", "Ke", ""], ["Jiang", "Shimiao", ""], ["Bai", "Yu", ""], ["Li", "Yi", ""], ["Liu", "Xu", ""], ["Xu", "Zhuoran", ""]]}, {"id": "1910.09399", "submitter": "Haicheng Tao", "authors": "Jorge Agnese, Jonathan Herrera, Haicheng Tao, Xingquan Zhu", "title": "A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image\n  Synthesis", "comments": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery.\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-image synthesis refers to computational methods which translate human\nwritten textual descriptions, in the form of keywords or sentences, into images\nwith similar semantic meaning to the text. In earlier research, image synthesis\nrelied mainly on word to image correlation analysis combined with supervised\nmethods to find best alignment of the visual content matching to the text.\nRecent progress in deep learning (DL) has brought a new set of unsupervised\ndeep learning methods, particularly deep generative models which are able to\ngenerate realistic visual images using suitably trained neural network models.\nIn this paper, we review the most recent development in the text-to-image\nsynthesis research domain. Our survey first introduces image synthesis and its\nchallenges, and then reviews key concepts such as generative adversarial\nnetworks (GANs) and deep convolutional encoder-decoder neural networks (DCNN).\nAfter that, we propose a taxonomy to summarize GAN based text-to-image\nsynthesis into four major categories: Semantic Enhancement GANs, Resolution\nEnhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs. We\nelaborate the main objective of each group, and further review typical GAN\narchitectures in each group. The taxonomy and the review outline the techniques\nand the evolution of different approaches, and eventually provide a clear\nroadmap to summarize the list of contemporaneous solutions that utilize GANs\nand DCNNs to generate enthralling results in categories such as human faces,\nbirds, flowers, room interiors, object reconstruction from edge maps (games)\netc. The survey will conclude with a comparison of the proposed solutions,\nchallenges that remain unresolved, and future developments in the text-to-image\nsynthesis domain.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:23:14 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Agnese", "Jorge", ""], ["Herrera", "Jonathan", ""], ["Tao", "Haicheng", ""], ["Zhu", "Xingquan", ""]]}, {"id": "1910.09405", "submitter": "Zixin Xie", "authors": "Jingwen Yan, Zixin Xie, Jingyao Chen, Yinan Liu, Lei Liu", "title": "Hyperspectral Image Classification Based on Adaptive Sparse Deep Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse model is widely used in hyperspectral image classification.However,\ndifferent of sparsity and regularization parameters has great influence on the\nclassification results.In this paper, a novel adaptive sparse deep network\nbased on deep architecture is proposed, which can construct the optimal sparse\nrepresentation and regularization parameters by deep network.Firstly, a data\nflow graph is designed to represent each update iteration based on Alternating\nDirection Method of Multipliers (ADMM) algorithm.Forward network and\nBack-Propagation network are deduced.All parameters are updated by gradient\ndescent in Back-Propagation.Then we proposed an Adaptive Sparse Deep\nNetwork.Comparing with several traditional classifiers or other algorithm for\nsparse model, experiment results indicate that our method achieves great\nimprovement in HSI classification.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:31:33 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yan", "Jingwen", ""], ["Xie", "Zixin", ""], ["Chen", "Jingyao", ""], ["Liu", "Yinan", ""], ["Liu", "Lei", ""]]}, {"id": "1910.09420", "submitter": "Antoine Rivail", "authors": "Antoine Rivail, Ursula Schmidt-Erfurth, Wolf-Dieter Vogl, Sebastian M.\n  Waldstein, Sophie Riedl, Christoph Grechenig, Zhichao Wu, Hrvoje Bogunovi\\'c", "title": "Modeling Disease Progression In Retinal OCTs With Longitudinal\n  Self-Supervised Learning", "comments": "Accepted for publication in the MICCAI 2019 PRIME workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal imaging is capable of capturing the static ana\\-to\\-mi\\-cal\nstructures and the dynamic changes of the morphology resulting from aging or\ndisease progression. Self-supervised learning allows to learn new\nrepresentation from available large unlabelled data without any expert\nknowledge. We propose a deep learning self-supervised approach to model disease\nprogression from longitudinal retinal optical coherence tomography (OCT). Our\nself-supervised model takes benefit from a generic time-related task, by\nlearning to estimate the time interval between pairs of scans acquired from the\nsame patient. This task is (i) easy to implement, (ii) allows to use\nirregularly sampled data, (iii) is tolerant to poor registration, and (iv) does\nnot rely on additional annotations. This novel method learns a representation\nthat focuses on progression specific information only, which can be transferred\nto other types of longitudinal problems. We transfer the learnt representation\nto a clinically highly relevant task of predicting the onset of an advanced\nstage of age-related macular degeneration within a given time interval based on\na single OCT scan. The boost in prediction accuracy, in comparison to a network\nlearned from scratch or transferred from traditional tasks, demonstrates that\nour pretrained self-supervised representation learns a clinically meaningful\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:53:37 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 11:58:32 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 07:30:39 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Rivail", "Antoine", ""], ["Schmidt-Erfurth", "Ursula", ""], ["Vogl", "Wolf-Dieter", ""], ["Waldstein", "Sebastian M.", ""], ["Riedl", "Sophie", ""], ["Grechenig", "Christoph", ""], ["Wu", "Zhichao", ""], ["Bogunovi\u0107", "Hrvoje", ""]]}, {"id": "1910.09430", "submitter": "Oier Mees", "authors": "Oier Mees, Markus Merklinger, Gabriel Kalweit, Wolfram Burgard", "title": "Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video", "comments": "Accepted at the 2020 IEEE International Conference on Robotics and\n  Automation (ICRA). Video at https://www.youtube.com/watch?v=z8gG1k9kSqA\n  Project page at http://robotskills.cs.uni-freiburg.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key challenges for the deployment of reinforcement learning (RL) agents in\nthe real world are the discovery, representation and reuse of skills in the\nabsence of a reward function. To this end, we propose a novel approach to learn\na task-agnostic skill embedding space from unlabeled multi-view videos. Our\nmethod learns a general skill embedding independently from the task context by\nusing an adversarial loss. We combine a metric learning loss, which utilizes\ntemporal video coherence to learn a state representation, with an entropy\nregularized adversarial skill-transfer loss. The metric learning loss learns a\ndisentangled representation by attracting simultaneous viewpoints of the same\nobservations and repelling visually similar frames from temporal neighbors. The\nadversarial skill-transfer loss enhances re-usability of learned skill\nembeddings over multiple task domains. We show that the learned embedding\nenables training of continuous control policies to solve novel tasks that\nrequire the interpolation of previously seen skills. Our extensive evaluation\nwith both simulation and real world data demonstrates the effectiveness of our\nmethod in learning transferable skills from unlabeled interaction videos and\ncomposing them for new tasks. Code, pretrained models and dataset are available\nat http://robotskills.cs.uni-freiburg.de\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:06:03 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 16:28:34 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Mees", "Oier", ""], ["Merklinger", "Markus", ""], ["Kalweit", "Gabriel", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1910.09433", "submitter": "Alex Lamb", "authors": "Tarin Clanuwat, Alex Lamb, Asanobu Kitamoto", "title": "KuroNet: Pre-Modern Japanese Kuzushiji Character Recognition with Deep\n  Learning", "comments": "International Conference on Document Recognition (ICDAR) 2019 [oral]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kuzushiji, a cursive writing style, had been used in Japan for over a\nthousand years starting from the 8th century. Over 3 millions books on a\ndiverse array of topics, such as literature, science, mathematics and even\ncooking are preserved. However, following a change to the Japanese writing\nsystem in 1900, Kuzushiji has not been included in regular school curricula.\nTherefore, most Japanese natives nowadays cannot read books written or printed\njust 150 years ago. Museums and libraries have invested a great deal of effort\ninto creating digital copies of these historical documents as a safeguard\nagainst fires, earthquakes and tsunamis. The result has been datasets with\nhundreds of millions of photographs of historical documents which can only be\nread by a small number of specially trained experts. Thus there has been a\ngreat deal of interest in using Machine Learning to automatically recognize\nthese historical texts and transcribe them into modern Japanese characters.\nNevertheless, several challenges in Kuzushiji recognition have made the\nperformance of existing systems extremely poor. To tackle these challenges, we\npropose KuroNet, a new end-to-end model which jointly recognizes an entire page\nof text by using a residual U-Net architecture which predicts the location and\nidentity of all characters given a page of text (without any pre-processing).\nThis allows the model to handle long range context, large vocabularies, and\nnon-standardized character layouts. We demonstrate that our system is able to\nsuccessfully recognize a large fraction of pre-modern Japanese documents, but\nalso explore areas where our system is limited and suggest directions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:09:13 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Clanuwat", "Tarin", ""], ["Lamb", "Alex", ""], ["Kitamoto", "Asanobu", ""]]}, {"id": "1910.09447", "submitter": "Shuai Tang", "authors": "Mao-Chuang Yeh, Shuai Tang, Anand Bhattad, Chuhang Zou, David Forsyth", "title": "Improving Style Transfer with Calibrated Metrics", "comments": "updated conference camera ready version. arXiv admin note: text\n  overlap with arXiv:1804.00118", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer methods produce a transferred image which is a rendering of a\ncontent image in the manner of a style image. We seek to understand how to\nimprove style transfer.\n  To do so requires quantitative evaluation procedures, but the current\nevaluation is qualitative, mostly involving user studies. We describe a novel\nquantitative evaluation procedure. Our procedure relies on two statistics: the\nEffectiveness (E) statistic measures the extent that a given style has been\ntransferred to the target, and the Coherence (C) statistic measures the extent\nto which the original image's content is preserved. Our statistics are\ncalibrated to human preference: targets with larger values of E (resp C) will\nreliably be preferred by human subjects in comparisons of style (resp.\ncontent).\n  We use these statistics to investigate the relative performance of a number\nof Neural Style Transfer(NST) methods, revealing several intriguing properties.\nAdmissible methods lie on a Pareto frontier (i.e. improving E reduces C or vice\nversa). Three methods are admissible: Universal style transfer produces very\ngood C but weak E; modifying the optimization used for Gatys' loss produces a\nmethod with strong E and strong C; and a modified cross-layer method has\nslightly better E at strong cost in C. While the histogram loss improves the E\nstatistics of Gatys' method, it does not make the method admissible.\nSurprisingly, style weights have relatively little effect in improving EC\nscores, and most variability in the transfer is explained by the style itself\n(meaning experimenters can be misguided by selecting styles).\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:26:05 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 17:26:52 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Yeh", "Mao-Chuang", ""], ["Tang", "Shuai", ""], ["Bhattad", "Anand", ""], ["Zou", "Chuhang", ""], ["Forsyth", "David", ""]]}, {"id": "1910.09450", "submitter": "Estephe Arnaud", "authors": "Estephe Arnaud, Arnaud Dapogny, Kevin Bailly", "title": "Tree-gated Deep Mixture-of-Experts For Pose-robust Face Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment consists of aligning a shape model on a face image. It is an\nactive domain in computer vision as it is a preprocessing for a number of face\nanalysis and synthesis applications. Current state-of-the-art methods already\nperform well on \"easy\" datasets, with moderate head pose variations, but may\nnot be robust for \"in-the-wild\" data with poses up to 90{\\deg}. In order to\nincrease robustness to an ensemble of factors of variations (e.g. head pose or\nocclusions), a given layer (e.g. a regressor or an upstream CNN layer) can be\nreplaced by a Mixture of Experts (MoE) layer that uses an ensemble of experts\ninstead of a single one. The weights of this mixture can be learned as gating\nfunctions to jointly learn the experts and the corresponding weights. In this\npaper, we propose to use tree-structured gates which allows a hierarchical\nweighting of the experts (Tree-MoE). We investigate the use of Tree-MoE layers\nin different contexts in the frame of face alignment with cascaded regression,\nfirstly for emphasizing relevant, more specialized feature extractors depending\nof a high-level semantic information such as head pose (Pose-Tree-MoE), and\nsecondly as an overall more robust regression layer. We perform extensive\nexperiments on several challenging face alignment datasets, demonstrating that\nour approach outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:30:20 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Arnaud", "Estephe", ""], ["Dapogny", "Arnaud", ""], ["Bailly", "Kevin", ""]]}, {"id": "1910.09455", "submitter": "Yihui He", "authors": "Yihui He, Jianing Qian, Jianren Wang", "title": "Depth-wise Decomposition for Accelerating Separable Convolutions in\n  Efficient Convolutional Neural Networks", "comments": "CVPR 2019 workshop, Efficient Deep Learning for Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep convolutional neural networks (CNNs) have been firmly established\nas the primary methods for many computer vision tasks. However, most\nstate-of-the-art CNNs are large, which results in high inference latency.\nRecently, depth-wise separable convolution has been proposed for image\nrecognition tasks on computationally limited platforms such as robotics and\nself-driving cars. Though it is much faster than its counterpart, regular\nconvolution, accuracy is sacrificed. In this paper, we propose a novel\ndecomposition approach based on SVD, namely depth-wise decomposition, for\nexpanding regular convolutions into depthwise separable convolutions while\nmaintaining high accuracy. We show our approach can be further generalized to\nthe multi-channel and multi-layer cases, based on Generalized Singular Value\nDecomposition (GSVD) [59]. We conduct thorough experiments with the latest\nShuffleNet V2 model [47] on both random synthesized dataset and a large-scale\nimage recognition dataset: ImageNet [10]. Our approach outperforms channel\ndecomposition [73] on all datasets. More importantly, our approach improves the\nTop-1 accuracy of ShuffleNet V2 by ~2%.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:37:53 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["He", "Yihui", ""], ["Qian", "Jianing", ""], ["Wang", "Jianren", ""]]}, {"id": "1910.09458", "submitter": "Geoffrey Roman Jimenez", "authors": "Geoffrey Roman-Jimenez, Patrice Guyot, Thierry Malon, Sylvie Chambon,\n  Vincent Charvillat, Alain Crouzil, Andr\\'e P\\'eninou, Julien Pinquier,\n  Florence Sedes, Christine S\\'enac", "title": "Improving Vehicle Re-Identification using CNN Latent Spaces: Metrics\n  Comparison and Track-to-track Extension", "comments": "This paper is a postprint of a paper submitted to and accepted for\n  publication in the journal IET Computer Vision and is subject to Institution\n  of Engineering and Technology Copyright. The copy of record is available at\n  the IET Digital Library", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of vehicle re-identification using distance\ncomparison of images in CNN latent spaces.\n  Firstly, we study the impact of the distance metrics, comparing performances\nobtained with different metrics: the minimal Euclidean distance (MED), the\nminimal cosine distance (MCD), and the residue of the sparse coding\nreconstruction (RSCR). These metrics are applied using features extracted from\nfive different CNN architectures, namely ResNet18, AlexNet, VGG16, InceptionV3\nand DenseNet201. We use the specific vehicle re-identification dataset VeRi to\nfine-tune these CNNs and evaluate results. In overall, independently of the CNN\nused, MCD outperforms MED, commonly used in the literature. These results are\nconfirmed on other vehicle retrieval datasets. Secondly, we extend the\nstate-of-the-art image-to-track process (I2TP) to a track-to-track process\n(T2TP). The three distance metrics are extended to measure distance between\ntracks, enabling T2TP. We compared T2TP with I2TP using the same CNN models.\nResults show that T2TP outperforms I2TP for MCD and RSCR. T2TP combining\nDenseNet201 and MCD-based metrics exhibits the best performances, outperforming\nthe state-of-the-art I2TP-based models. Finally, experiments highlight two main\nresults: i) the impact of metric choice in vehicle re-identification, and ii)\nT2TP improves the performances compared to I2TP, especially when coupled with\nMCD-based metrics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:41:59 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 08:39:58 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Roman-Jimenez", "Geoffrey", ""], ["Guyot", "Patrice", ""], ["Malon", "Thierry", ""], ["Chambon", "Sylvie", ""], ["Charvillat", "Vincent", ""], ["Crouzil", "Alain", ""], ["P\u00e9ninou", "Andr\u00e9", ""], ["Pinquier", "Julien", ""], ["Sedes", "Florence", ""], ["S\u00e9nac", "Christine", ""]]}, {"id": "1910.09469", "submitter": "Enrique Sanchez", "authors": "Enrique Sanchez and Georgios Tzimiropoulos", "title": "Object landmark discovery through unsupervised adaptation", "comments": "NeurIPS 2019. Code is available\n  https://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to ease the unsupervised learning of object\nlandmark detectors. Similarly to previous methods, our approach is fully\nunsupervised in a sense that it does not require or make any use of annotated\nlandmarks for the target object category. Contrary to previous works, we do\nhowever assume that a landmark detector, which has already learned a structured\nrepresentation for a given object category in a fully supervised manner, is\navailable. Under this setting, our main idea boils down to adapting the given\npre-trained network to the target object categories in a fully unsupervised\nmanner. To this end, our method uses the pre-trained network as a core which\nremains frozen and does not get updated during training, and learns, in an\nunsupervised manner, only a projection matrix to perform the adaptation to the\ntarget categories. By building upon an existing structured representation\nlearned in a supervised manner, the optimization problem solved by our method\nis much more constrained with significantly less parameters to learn which\nseems to be important for the case of unsupervised learning. We show that our\nmethod surpasses fully unsupervised techniques trained from scratch as well as\na strong baseline based on fine-tuning, and produces state-of-the-art results\non several datasets. Code can be found at\nhttps://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019 .\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:58:57 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Sanchez", "Enrique", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1910.09470", "submitter": "Rae Jeong", "authors": "Rae Jeong, Yusuf Aytar, David Khosid, Yuxiang Zhou, Jackie Kay, Thomas\n  Lampe, Konstantinos Bousmalis, Francesco Nori", "title": "Self-Supervised Sim-to-Real Adaptation for Visual Robotic Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting and automatically obtaining reward signals from real robotic\nvisual data for the purposes of training reinforcement learning algorithms can\nbe quite challenging and time-consuming. Methods for utilizing unlabeled data\ncan have a huge potential to further accelerate robotic learning. We consider\nhere the problem of performing manipulation tasks from pixels. In such tasks,\nchoosing an appropriate state representation is crucial for planning and\ncontrol. This is even more relevant with real images where noise, occlusions\nand resolution affect the accuracy and reliability of state estimation. In this\nwork, we learn a latent state representation implicitly with deep reinforcement\nlearning in simulation, and then adapt it to the real domain using unlabeled\nreal robot data. We propose to do so by optimizing sequence-based self\nsupervised objectives. These exploit the temporal nature of robot experience,\nand can be common in both the simulated and real domains, without assuming any\nalignment of underlying states in simulated and unlabeled real images. We\npropose Contrastive Forward Dynamics loss, which combines dynamics model\nlearning with time-contrastive techniques. The learned state representation\nthat results from our methods can be used to robustly solve a manipulation task\nin simulation and to successfully transfer the learned skill on a real system.\nWe demonstrate the effectiveness of our approaches by training a vision-based\nreinforcement learning agent for cube stacking. Agents trained with our method,\nusing only 5 hours of unlabeled real robot data for adaptation, shows a clear\nimprovement over domain randomization, and standard visual domain adaptation\ntechniques for sim-to-real transfer.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:00:53 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Jeong", "Rae", ""], ["Aytar", "Yusuf", ""], ["Khosid", "David", ""], ["Zhou", "Yuxiang", ""], ["Kay", "Jackie", ""], ["Lampe", "Thomas", ""], ["Bousmalis", "Konstantinos", ""], ["Nori", "Francesco", ""]]}, {"id": "1910.09488", "submitter": "Tomas Werner", "authors": "Tom\\'a\\v{s} Werner, Daniel Pr\\r{u}\\v{s}a", "title": "Relative Interior Rule in Block-Coordinate Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (Block-)coordinate minimization is an iterative optimization method which in\nevery iteration finds a global minimum of the objective over a variable or a\nsubset of variables, while keeping the remaining variables constant. While for\nsome problems, coordinate minimization converges to a global minimum (e.g.,\nconvex differentiable objective), for general (non-differentiable) convex\nproblems this may not be the case. Despite this drawback, (block-)coordinate\nminimization can be an acceptable option for large-scale non-differentiable\nconvex problems; an example is methods to solve the linear programming\nrelaxation of the discrete energy minimization problem (MAP inference in\ngraphical models). When block-coordinate minimization is applied to a general\nconvex problem, in every iteration the minimizer over the current coordinate\nblock need not be unique and therefore a single minimizer must be chosen. We\npropose that this minimizer be chosen from the relative interior of the set of\nall minimizers over the current block. We show that this rule is not worse, in\na certain precise sense, than any other rule.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:25:11 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Werner", "Tom\u00e1\u0161", ""], ["Pr\u016f\u0161a", "Daniel", ""]]}, {"id": "1910.09495", "submitter": "Saeed Reza Kheradpisheh", "authors": "Saeed Reza Kheradpisheh and Timoth\\'ee Masquelier", "title": "S4NN: temporal backpropagation for spiking neural networks with one\n  spike per neuron", "comments": null, "journal-ref": "International Journal of Neural Systems 2020", "doi": "10.1142/S0129065720500276", "report-no": null, "categories": "cs.NE cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new supervised learning rule for multilayer spiking neural\nnetworks (SNNs) that use a form of temporal coding known as rank-order-coding.\nWith this coding scheme, all neurons fire exactly one spike per stimulus, but\nthe firing order carries information. In particular, in the readout layer, the\nfirst neuron to fire determines the class of the stimulus. We derive a new\nlearning rule for this sort of network, named S4NN, akin to traditional error\nbackpropagation, yet based on latencies. We show how approximated error\ngradients can be computed backward in a feedforward network with any number of\nlayers. This approach reaches state-of-the-art performance with supervised\nmulti fully-connected layer SNNs: test accuracy of 97.4% for the MNIST dataset,\nand 99.2% for the Caltech Face/Motorbike dataset. Yet, the neuron model that we\nuse, non-leaky integrate-and-fire, is much simpler than the one used in all\nprevious works. The source codes of the proposed S4NN are publicly available at\nhttps://github.com/SRKH/S4NN.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:39:42 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 15:43:30 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 09:23:11 GMT"}, {"version": "v4", "created": "Sat, 13 Jun 2020 10:33:19 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Kheradpisheh", "Saeed Reza", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1910.09505", "submitter": "Frederic Sala", "authors": "Frederic Sala, Paroma Varma, Jason Fries, Daniel Y. Fu, Shiori Sagawa,\n  Saelig Khattar, Ashwini Ramamoorthy, Ke Xiao, Kayvon Fatahalian, James\n  Priest, Christopher R\\'e", "title": "Multi-Resolution Weak Supervision for Sequential Data", "comments": "NeurIPS 2019 (Conference on Neural Information Processing Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since manually labeling training data is slow and expensive, recent\nindustrial and scientific research efforts have turned to weaker or noisier\nforms of supervision sources. However, existing weak supervision approaches\nfail to model multi-resolution sources for sequential data, like video, that\ncan assign labels to individual elements or collections of elements in a\nsequence. A key challenge in weak supervision is estimating the unknown\naccuracies and correlations of these sources without using labeled data.\nMulti-resolution sources exacerbate this challenge due to complex correlations\nand sample complexity that scales in the length of the sequence. We propose\nDugong, the first framework to model multi-resolution weak supervision sources\nwith complex correlations to assign probabilistic labels to training data.\nTheoretically, we prove that Dugong, under mild conditions, can uniquely\nrecover the unobserved accuracy and correlation parameters and use parameter\nsharing to improve sample complexity. Our method assigns clinician-validated\nlabels to population-scale biomedical video repositories, helping outperform\ntraditional supervision by 36.8 F1 points and addressing a key use case where\nmachine learning has been severely limited by the lack of expert labeled data.\nOn average, Dugong improves over traditional supervision by 16.0 F1 points and\nexisting weak supervision approaches by 24.2 F1 points across several video and\nsensor classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:48:18 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Sala", "Frederic", ""], ["Varma", "Paroma", ""], ["Fries", "Jason", ""], ["Fu", "Daniel Y.", ""], ["Sagawa", "Shiori", ""], ["Khattar", "Saelig", ""], ["Ramamoorthy", "Ashwini", ""], ["Xiao", "Ke", ""], ["Fatahalian", "Kayvon", ""], ["Priest", "James", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1910.09507", "submitter": "Hamid Behjat", "authors": "Hamid Behjat, Martin Larsson", "title": "Spectral Characterization of functional MRI data on voxel-resolution\n  cortical graphs", "comments": "Fixed two typos in the equations; (1) definition of L in section 2.1,\n  paragraph 1. (2) signal de-meaning and normalization in section 2.4,\n  paragraph 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human cortical layer exhibits a convoluted morphology that is unique to\neach individual. Conventional volumetric fMRI processing schemes take for\ngranted the rich information provided by the underlying anatomy. We present a\nmethod to study fMRI data on subject-specific cerebral hemisphere cortex (CHC)\ngraphs, which encode the cortical morphology at the resolution of voxels in\n3-D. We study graph spectral energy metrics associated to fMRI data of 100\nsubjects from the Human Connectome Project database, across seven tasks.\nExperimental results signify the strength of CHC graphs' Laplacian eigenvector\nbases in capturing subtle spatial patterns specific to different functional\nloads as well as experimental conditions within each task.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:54:45 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 15:25:10 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 00:22:34 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Behjat", "Hamid", ""], ["Larsson", "Martin", ""]]}, {"id": "1910.09524", "submitter": "Naser Damer", "authors": "Naser Damer, Fadi Boutros, Khawla Mallat, Florian Kirchbuchner,\n  Jean-Luc Dugelay, Arjan Kuijper", "title": "Cascaded Generation of High-quality Color Visible Face Images from\n  Thermal Captures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating visible-like face images from thermal images is essential to\nperform manual and automatic cross-spectrum face recognition. We successfully\npropose a solution based on cascaded refinement network that, unlike previous\nworks, produces high quality generated color images without the need for face\nalignment, large databases, data augmentation, polarimetric sensors,\ncomputationally-intense training, or unrealistic restriction on the generated\nresolution. The training of our solution is based on the contextual loss,\nmaking it inherently scale (face area) and rotation invariant. We present\ngenerated image samples of unknown individuals under different poses and\nocclusion conditions.We also prove the high similarity in image quality between\nground-truth images and generated ones by comparing seven quality metrics. We\ncompare our results with two state-of-the-art approaches proving the\nsuperiority of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 17:37:44 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Damer", "Naser", ""], ["Boutros", "Fadi", ""], ["Mallat", "Khawla", ""], ["Kirchbuchner", "Florian", ""], ["Dugelay", "Jean-Luc", ""], ["Kuijper", "Arjan", ""]]}, {"id": "1910.09570", "submitter": "Joseph Paul Cohen", "authors": "Shawn Tan and Guillaume Androz and Ahmad Chamseddine and Pierre\n  Fecteau and Aaron Courville and Yoshua Bengio and Joseph Paul Cohen", "title": "Icentia11K: An Unsupervised Representation Learning Dataset for\n  Arrhythmia Subtype Discovery", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We release the largest public ECG dataset of continuous raw signals for\nrepresentation learning containing 11 thousand patients and 2 billion labelled\nbeats. Our goal is to enable semi-supervised ECG models to be made as well as\nto discover unknown subtypes of arrhythmia and anomalous ECG signal events. To\nthis end, we propose an unsupervised representation learning task, evaluated in\na semi-supervised fashion. We provide a set of baselines for different feature\nextractors that can be built upon. Additionally, we perform qualitative\nevaluations on results from PCA embeddings, where we identify some clustering\nof known subtypes indicating the potential for representation learning in\narrhythmia sub-type discovery.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:02:36 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Tan", "Shawn", ""], ["Androz", "Guillaume", ""], ["Chamseddine", "Ahmad", ""], ["Fecteau", "Pierre", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "1910.09616", "submitter": "Siddharth Roheda", "authors": "Siddharth Roheda, Hamid Krim", "title": "Conquering the CNN Over-Parameterization Dilemma: A Volterra Filtering\n  Approach for Action Recognition", "comments": "Accepted at AAAI 2020", "journal-ref": null, "doi": "10.1609/aaai.v34i07.6870", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of inference in Machine Learning (ML) has led to an explosive\nnumber of different proposals in ML, and particularly in Deep Learning. In an\nattempt to reduce the complexity of Convolutional Neural Networks, we propose a\nVolterra filter-inspired Network architecture. This architecture introduces\ncontrolled non-linearities in the form of interactions between the delayed\ninput samples of data. We propose a cascaded implementation of Volterra\nFiltering so as to significantly reduce the number of parameters required to\ncarry out the same classification task as that of a conventional Neural\nNetwork. We demonstrate an efficient parallel implementation of this Volterra\nNeural Network (VNN), along with its remarkable performance while retaining a\nrelatively simpler and potentially more tractable structure. Furthermore, we\nshow a rather sophisticated adaptation of this network to nonlinearly fuse the\nRGB (spatial) information and the Optical Flow (temporal) information of a\nvideo sequence for action recognition. The proposed approach is evaluated on\nUCF-101 and HMDB-51 datasets for action recognition, and is shown to outperform\nstate of the art CNN approaches.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 19:22:38 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 21:48:35 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 15:10:55 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Roheda", "Siddharth", ""], ["Krim", "Hamid", ""]]}, {"id": "1910.09638", "submitter": "Hammad Ayyubi", "authors": "Hammad A. Ayyubi", "title": "GANspection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have been used extensively and quite\nsuccessfully for unsupervised learning. As GANs don't approximate an explicit\nprobability distribution, it's an interesting study to inspect the latent space\nrepresentations learned by GANs. The current work seeks to push the boundaries\nof such inspection methods to further understand in more detail the manifold\nbeing learned by GANs. Various interpolation and extrapolation techniques along\nwith vector arithmetic is used to understand the learned manifold. We show\nthrough experiments that GANs indeed learn a data probability distribution\nrather than memorize images/data. Further, we prove that GANs encode\nsemantically relevant information in the learned probability distribution. The\nexperiments have been performed on two publicly available datasets - Large\nScale Scene Understanding (LSUN) and CelebA.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 20:26:54 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Ayyubi", "Hammad A.", ""]]}, {"id": "1910.09642", "submitter": "Rafael Henrique Vareto Mr.", "authors": "Rafael Henrique Vareto, Araceli Marcia Sandanha, William Robson\n  Schwartz", "title": "The SWAX Benchmark: Attacking Biometric Systems with Wax Figures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A face spoofing attack occurs when an intruder attempts to impersonate\nsomeone who carries a gainful authentication clearance. It is a trending topic\ndue to the increasing demand for biometric authentication on mobile devices,\nhigh-security areas, among others. This work introduces a new database named\nSense Wax Attack dataset (SWAX), comprised of real human and wax figure images\nand videos that endorse the problem of face spoofing detection. The dataset\nconsists of more than 1800 face images and 110 videos of 55 people/waxworks,\narranged in training, validation and test sets with a large range in\nexpression, illumination and pose variations. Experiments performed with\nbaseline methods show that despite the progress in recent years, advanced\nspoofing methods are still vulnerable to high-quality violation attempts.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 20:40:54 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Vareto", "Rafael Henrique", ""], ["Sandanha", "Araceli Marcia", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1910.09643", "submitter": "Pratik Mazumder", "authors": "Pratik Mazumder, Pravendra Singh, Vinay Namboodiri", "title": "CPWC: Contextual Point Wise Convolution for Object Recognition", "comments": "Accepted in ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional layers are a major driving force behind the successes of deep\nlearning. Pointwise convolution (PWC) is a 1x1 convolutional filter that is\nprimarily used for parameter reduction. However, the PWC ignores the spatial\ninformation around the points it is processing. This design is by choice, in\norder to reduce the overall parameters and computations. However, we\nhypothesize that this shortcoming of PWC has a significant impact on the\nnetwork performance. We propose an alternative design for pointwise\nconvolution, which uses spatial information from the input efficiently. Our\ndesign significantly improves the performance of the networks without\nsubstantially increasing the number of parameters and computations. We\nexperimentally show that our design results in significant improvement in the\nperformance of the network for classification as well as detection.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 20:41:22 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 11:57:36 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Mazumder", "Pratik", ""], ["Singh", "Pravendra", ""], ["Namboodiri", "Vinay", ""]]}, {"id": "1910.09664", "submitter": "Valts Blukis", "authors": "Valts Blukis, Yannick Terme, Eyvind Niklasson, Ross A. Knepper, Yoav\n  Artzi", "title": "Learning to Map Natural Language Instructions to Physical Quadcopter\n  Control using Simulated Flight", "comments": "Conference on Robot Learning (CoRL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a joint simulation and real-world learning framework for mapping\nnavigation instructions and raw first-person observations to continuous\ncontrol. Our model estimates the need for environment exploration, predicts the\nlikelihood of visiting environment positions during execution, and controls the\nagent to both explore and visit high-likelihood positions. We introduce\nSupervised Reinforcement Asynchronous Learning (SuReAL). Learning uses both\nsimulation and real environments without requiring autonomous flight in the\nphysical environment during training, and combines supervised learning for\npredicting positions to visit and reinforcement learning for continuous\ncontrol. We evaluate our approach on a natural language instruction-following\ntask with a physical quadcopter, and demonstrate effective execution and\nexploration behavior.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 21:19:33 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Blukis", "Valts", ""], ["Terme", "Yannick", ""], ["Niklasson", "Eyvind", ""], ["Knepper", "Ross A.", ""], ["Artzi", "Yoav", ""]]}, {"id": "1910.09703", "submitter": "Qiujia Li", "authors": "Qiujia Li, Florian L. Kreyssig, Chao Zhang, and Philip C. Woodland", "title": "Discriminative Neural Clustering for Speaker Diarisation", "comments": "Accepted as a conference paper at the 8th IEEE Spoken Language\n  Technology Workshop (SLT 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Discriminative Neural Clustering (DNC) that\nformulates data clustering with a maximum number of clusters as a supervised\nsequence-to-sequence learning problem. Compared to traditional unsupervised\nclustering algorithms, DNC learns clustering patterns from training data\nwithout requiring an explicit definition of a similarity measure. An\nimplementation of DNC based on the Transformer architecture is shown to be\neffective on a speaker diarisation task using the challenging AMI dataset.\nSince AMI contains only 147 complete meetings as individual input sequences,\ndata scarcity is a significant issue for training a Transformer model for DNC.\nAccordingly, this paper proposes three data augmentation schemes: sub-sequence\nrandomisation, input vector randomisation, and Diaconis augmentation, which\ngenerates new data samples by rotating the entire input sequence of\nL2-normalised speaker embeddings. Experimental results on AMI show that DNC\nachieves a reduction in speaker error rate (SER) of 29.4% relative to spectral\nclustering.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 00:09:22 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 15:32:03 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Li", "Qiujia", ""], ["Kreyssig", "Florian L.", ""], ["Zhang", "Chao", ""], ["Woodland", "Philip C.", ""]]}, {"id": "1910.09705", "submitter": "Jimin Tan", "authors": "Jimin Tan, Anastasios Noulas, Diego S\\'aez, Rossano Schifanella", "title": "Mobile Recognition of Wikipedia Featured Sites using Deep Learning and\n  Crowd-sourced Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering Wikipedia content through mobile and augmented reality mediums can\nenable new forms of interaction in urban-focused user communities facilitating\nlearning, communication and knowledge exchange. With this objective in mind, in\nthis work we develop a mobile application that allows for the recognition of\nnotable sites featured on Wikipedia. The application is powered by a deep\nneural network that has been trained on crowd-sourced imagery describing sites\nof interest, such as buildings, statues, museums or other physical entities\nthat are present and visually accessible in an urban environment. We describe\nan end-to-end pipeline that describes data collection, model training and\nevaluation of our application considering online and real world scenarios. We\nidentify a number of challenges in the site recognition task which arise due to\nvisual similarities amongst the classified sites as well as due to noise\nintroduce by the surrounding built environment. We demonstrate how using mobile\ncontextual information, such as user location, orientation and attention\npatterns can significantly alleviate such challenges. Moreover, we present an\nunsupervised learning technique to de-noise crowd-sourced imagery which\nimproves classification performance further.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 00:17:55 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 18:15:39 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Tan", "Jimin", ""], ["Noulas", "Anastasios", ""], ["S\u00e1ez", "Diego", ""], ["Schifanella", "Rossano", ""]]}, {"id": "1910.09716", "submitter": "Mohammad Sadegh Norouzzadeh", "authors": "Mohammad Sadegh Norouzzadeh, Dan Morris, Sara Beery, Neel Joshi,\n  Nebojsa Jojic, and Jeff Clune", "title": "A deep active learning system for species identification and counting in\n  camera trap images", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biodiversity conservation depends on accurate, up-to-date information about\nwildlife population distributions. Motion-activated cameras, also known as\ncamera traps, are a critical tool for population surveys, as they are cheap and\nnon-intrusive. However, extracting useful information from camera trap images\nis a cumbersome process: a typical camera trap survey may produce millions of\nimages that require slow, expensive manual review. Consequently, critical\ninformation is often lost due to resource limitations, and critical\nconservation questions may be answered too slowly to support decision-making.\nComputer vision is poised to dramatically increase efficiency in image-based\nbiodiversity surveys, and recent studies have harnessed deep learning\ntechniques for automatic information extraction from camera trap images.\nHowever, the accuracy of results depends on the amount, quality, and diversity\nof the data available to train models, and the literature has focused on\nprojects with millions of relevant, labeled training images. Many camera trap\nprojects do not have a large set of labeled images and hence cannot benefit\nfrom existing machine learning techniques. Furthermore, even projects that do\nhave labeled data from similar ecosystems have struggled to adopt deep learning\nmethods because image classification models overfit to specific image\nbackgrounds (i.e., camera locations). In this paper, we focus not on automating\nthe labeling of camera trap images, but on accelerating this process. We\ncombine the power of machine intelligence and human intelligence to build a\nscalable, fast, and accurate active learning system to minimize the manual work\nrequired to identify and count animals in camera trap images. Our proposed\nscheme can match the state of the art accuracy on a 3.2 million image dataset\nwith as few as 14,100 manual labels, which means decreasing manual labeling\neffort by over 99.5%.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 01:03:33 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Norouzzadeh", "Mohammad Sadegh", ""], ["Morris", "Dan", ""], ["Beery", "Sara", ""], ["Joshi", "Neel", ""], ["Jojic", "Nebojsa", ""], ["Clune", "Jeff", ""]]}, {"id": "1910.09717", "submitter": "Chaitanya Kaul", "authors": "Chaitanya Kaul, Nick Pears, Hang Dai, Roderick Murray-Smith, Suresh\n  Manandhar", "title": "Penalizing small errors using an Adaptive Logarithmic Loss", "comments": "Published at AIHA 2020 (ICPR 2020 Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss functions are error metrics that quantify the difference between a\nprediction and its corresponding ground truth. Fundamentally, they define a\nfunctional landscape for traversal by gradient descent. Although numerous loss\nfunctions have been proposed to date in order to handle various machine\nlearning problems, little attention has been given to enhancing these functions\nto better traverse the loss landscape. In this paper, we simultaneously and\nsignificantly mitigate two prominent problems in medical image segmentation\nnamely: i) class imbalance between foreground and background pixels and ii)\npoor loss function convergence. To this end, we propose an adaptive logarithmic\nloss function. We compare this loss function with the existing state-of-the-art\non the ISIC 2018 dataset, the nuclei segmentation dataset as well as the DRIVE\nretinal vessel segmentation dataset. We measure the performance of our\nmethodology on benchmark metrics and demonstrate state-of-the-art performance.\nMore generally, we show that our system can be used as a framework for better\ntraining of deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 01:24:41 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 22:58:36 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kaul", "Chaitanya", ""], ["Pears", "Nick", ""], ["Dai", "Hang", ""], ["Murray-Smith", "Roderick", ""], ["Manandhar", "Suresh", ""]]}, {"id": "1910.09722", "submitter": "Jongmin Yu", "authors": "Jongmin Yu, Sangwoo Park, Sangwook Lee, and Moongu Jeon", "title": "Drivers Drowsiness Detection using Condition-Adaptive Representation\n  Learning Framework", "comments": "IEEE Transactions on Intelligent Transportation Systems publication\n  information (2018)", "journal-ref": null, "doi": "10.1109/TITS.2018.2883823", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a condition-adaptive representation learning framework for the\ndriver drowsiness detection based on 3D-deep convolutional neural network. The\nproposed framework consists of four models: spatio-temporal representation\nlearning, scene condition understanding, feature fusion, and drowsiness\ndetection. The spatio-temporal representation learning extracts features that\ncan describe motions and appearances in video simultaneously. The scene\ncondition understanding classifies the scene conditions related to various\nconditions about the drivers and driving situations such as statuses of wearing\nglasses, illumination condition of driving, and motion of facial elements such\nas head, eye, and mouth. The feature fusion generates a condition-adaptive\nrepresentation using two features extracted from above models. The detection\nmodel recognizes drivers drowsiness status using the condition-adaptive\nrepresentation. The condition-adaptive representation learning framework can\nextract more discriminative features focusing on each scene condition than the\ngeneral representation so that the drowsiness detection method can provide more\naccurate results for the various driving situations. The proposed framework is\nevaluated with the NTHU Drowsy Driver Detection video dataset. The experimental\nresults show that our framework outperforms the existing drowsiness detection\nmethods based on visual analysis.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 01:51:43 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Yu", "Jongmin", ""], ["Park", "Sangwoo", ""], ["Lee", "Sangwook", ""], ["Jeon", "Moongu", ""]]}, {"id": "1910.09728", "submitter": "Zhizhe Liu", "authors": "Zhizhe Liu, Xingxing Zhang, Zhenfeng Zhu, Shuai Zheng, Yao Zhao, Jian\n  Cheng", "title": "Convolutional Prototype Learning for Zero-Shot Recognition", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) has received increasing attention in recent years\nespecially in areas of fine-grained object recognition, retrieval, and image\ncaptioning. The key to ZSL is to transfer knowledge from the seen to the unseen\nclasses via auxiliary class attribute vectors. However, the popularly learned\nprojection functions in previous works cannot generalize well since they assume\nthe distribution consistency between seen and unseen domains at\nsample-level.Besides, the provided non-visual and unique class attributes can\nsignificantly degrade the recognition performance in semantic space. In this\npaper, we propose a simple yet effective convolutional prototype learning (CPL)\nframework for zero-shot recognition. By assuming distribution consistency at\ntask-level, our CPL is capable of transferring knowledge smoothly to recognize\nunseen samples.Furthermore, inside each task, discriminative visual prototypes\nare learned via a distance based training mechanism. Consequently, we can\nperform recognition in visual space, instead of semantic space. An extensive\ngroup of experiments are then carefully designed and presented, demonstrating\nthat CPL obtains more favorable effectiveness, over currently available\nalternatives under various settings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 02:15:46 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 06:53:51 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 02:56:56 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Liu", "Zhizhe", ""], ["Zhang", "Xingxing", ""], ["Zhu", "Zhenfeng", ""], ["Zheng", "Shuai", ""], ["Zhao", "Yao", ""], ["Cheng", "Jian", ""]]}, {"id": "1910.09734", "submitter": "Chun-Na Li", "authors": "Chun-Na Li, Yuan-Hai Shao, Huajun Wang, Yu-Ting Zhao, Ling-Wei Huang,\n  Naihua Xiu and Nai-Yang Deng", "title": "Single and Union Non-parallel Support Vector Machine Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the classification problem, we summarize the nonparallel support\nvector machines with the nonparallel hyperplanes to two types of frameworks.\nThe first type constructs the hyperplanes separately. It solves a series of\nsmall optimization problems to obtain a series of hyperplanes, but is hard to\nmeasure the loss of each sample. The other type constructs all the hyperplanes\nsimultaneously, and it solves one big optimization problem with the ascertained\nloss of each sample. We give the characteristics of each framework and compare\nthem carefully. In addition, based on the second framework, we construct a\nmax-min distance-based nonparallel support vector machine for multiclass\nclassification problem, called NSVM. It constructs hyperplanes with large\ndistance margin by solving an optimization problem. Experimental results on\nbenchmark data sets show the advantages of our NSVM.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 02:34:34 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 00:03:40 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 06:56:34 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Li", "Chun-Na", ""], ["Shao", "Yuan-Hai", ""], ["Wang", "Huajun", ""], ["Zhao", "Yu-Ting", ""], ["Huang", "Ling-Wei", ""], ["Xiu", "Naihua", ""], ["Deng", "Nai-Yang", ""]]}, {"id": "1910.09758", "submitter": "Andre Barczak", "authors": "Andre Barczak and Napoleon Reyes and Teo Susnjak", "title": "Assessment of the Local Tchebichef Moments Method for Texture\n  Classification by Fine Tuning Extraction Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use machine learning to study the application of Local\nTchebichef Moments (LTM) to the problem of texture classification. The original\nLTM method was proposed by Mukundan (2014).\n  The LTM method can be used for texture analysis in many different ways,\neither using the moment values directly, or more simply creating a relationship\nbetween the moment values of different orders, producing a histogram similar to\nthose of Local Binary Pattern (LBP) based methods. The original method was not\nfully tested with large datasets, and there are several parameters that should\nbe characterised for performance. Among these parameters are the kernel size,\nthe moment orders and the weights for each moment.\n  We implemented the LTM method in a flexible way in order to allow for the\nmodification of the parameters that can affect its performance. Using four\nsubsets from the Outex dataset (a popular benchmark for texture analysis), we\nused Random Forests to create models and to classify texture images, recording\nthe standard metrics for each classifier. We repeated the process using several\nvariations of the LBP method for comparison. This allowed us to find the best\ncombination of orders and weights for the LTM method for texture\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 04:13:34 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Barczak", "Andre", ""], ["Reyes", "Napoleon", ""], ["Susnjak", "Teo", ""]]}, {"id": "1910.09761", "submitter": "Shaoze You", "authors": "Shaoze You, Hua Zhu, Menggang Li, Yutan Li", "title": "A Review of Visual Trackers and Analysis of its Application to Mobile\n  Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has received a significant attention in recent year, which is\none of the important parts for robots to obtain information about the external\nenvironment. Visual trackers can provide the necessary physical and\nenvironmental parameters for the mobile robot, and their performance is related\nto the actual application of the robot. This study provides a comprehensive\nsurvey on visual trackers. Following a brief introduction, we first analyzed\nthe basic framework and difficulties of visual trackers. Then the structure of\ngenerative and discriminative methods is introduced, and summarized the feature\ndescriptors, modeling methods, and learning methods which be used in tracker.\nLater we reviewed and evaluated the state-of-the-art progress on discriminative\ntrackers from three directions: correlation filter, deep learning and\nconvolutional features. Finally, we analyzed the research direction of visual\ntracker used in mobile robot, as well as outlined the future trends for visual\ntracker on mobile robot.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 04:30:48 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["You", "Shaoze", ""], ["Zhu", "Hua", ""], ["Li", "Menggang", ""], ["Li", "Yutan", ""]]}, {"id": "1910.09768", "submitter": "Qiulei Dong", "authors": "Qiulei Dong and Jiayin Sun and Zhanyi Hu", "title": "Face representation by deep learning: a linear encoding in a parameter\n  space?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Convolutional Neural Networks (CNNs) have achieved tremendous\nperformances on face recognition, and one popular perspective regarding CNNs'\nsuccess is that CNNs could learn discriminative face representations from face\nimages with complex image feature encoding. However, it is still unclear what\nis the intrinsic mechanism of face representation in CNNs. In this work, we\ninvestigate this problem by formulating face images as points in a\nshape-appearance parameter space, and our results demonstrate that: (i) The\nencoding and decoding of the neuron responses (representations) to face images\nin CNNs could be achieved under a linear model in the parameter space, in\nagreement with the recent discovery in primate IT face neurons, but different\nfrom the aforementioned perspective on CNNs' face representation with complex\nimage feature encoding; (ii) The linear model for face encoding and decoding in\nthe parameter space could achieve close or even better performances on face\nrecognition and verification than state-of-the-art CNNs, which might provide\nnew lights on the design strategies for face recognition systems; (iii) The\nneuron responses to face images in CNNs could not be adequately modelled by the\naxis model, a model recently proposed on face modelling in primate IT cortex.\nAll these results might shed some lights on the often complained blackbox\nnature behind CNNs' tremendous performances on face recognition.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 05:01:28 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Dong", "Qiulei", ""], ["Sun", "Jiayin", ""], ["Hu", "Zhanyi", ""]]}, {"id": "1910.09773", "submitter": "Yalong Liu", "authors": "Yalong Liu, Jie Li, Miaomiao Wang, Zhicheng Jiao, Jian Yang, and\n  Xianjun Li", "title": "Trident Segmentation CNN: A Spatiotemporal Transformation CNN for\n  Punctate White Matter Lesions Segmentation in Preterm Neonates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of punctate white matter lesions (PWML) in preterm\nneonates by an automatic algorithm can better assist doctors in diagnosis.\nHowever, the existing algorithms have many limitations, such as low detection\naccuracy and large resource consumption. In this paper, a novel spatiotemporal\ntransformation deep learning method called Trident Segmentation CNN (TS-CNN) is\nproposed to segment PWML in MR images. It can convert spatial information into\ntemporal information, which reduces the consumption of computing resources.\nFurthermore, a new improved training loss called Self-balancing Focal Loss\n(SBFL) is proposed to balance the loss during the training process. The whole\nmodel is evaluated on a dataset of 704 MR images. Overall the method achieves\nmedian DSC, sensitivity, specificity, and Hausdorff distance of 0.6355, 0.7126,\n0.9998, and 24.5836 mm which outperforms the state-of-the-art algorithm. (The\ncode is now available on https://github.com/YalongLiu/Trident-Segmentation-CNN)\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 05:36:56 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Liu", "Yalong", ""], ["Li", "Jie", ""], ["Wang", "Miaomiao", ""], ["Jiao", "Zhicheng", ""], ["Yang", "Jian", ""], ["Li", "Xianjun", ""]]}, {"id": "1910.09777", "submitter": "Peike Li", "authors": "Peike Li, Yunqiu Xu, Yunchao Wei, Yi Yang", "title": "Self-Correction for Human Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Labeling pixel-level masks for fine-grained semantic segmentation tasks, e.g.\nhuman parsing, remains a challenging task. The ambiguous boundary between\ndifferent semantic parts and those categories with similar appearance usually\nare confusing, leading to unexpected noises in ground truth masks. To tackle\nthe problem of learning with label noises, this work introduces a purification\nstrategy, called Self-Correction for Human Parsing (SCHP), to progressively\npromote the reliability of the supervised labels as well as the learned models.\nIn particular, starting from a model trained with inaccurate annotations as\ninitialization, we design a cyclically learning scheduler to infer more\nreliable pseudo-masks by iteratively aggregating the current learned model with\nthe former optimal one in an online manner. Besides, those correspondingly\ncorrected labels can in turn to further boost the model performance. In this\nway, the models and the labels will reciprocally become more robust and\naccurate during the self-correction learning cycles. Benefiting from the\nsuperiority of SCHP, we achieve the best performance on two popular\nsingle-person human parsing benchmarks, including LIP and Pascal-Person-Part\ndatasets. Our overall system ranks 1st in CVPR2019 LIP Challenge. Code is\navailable at https://github.com/PeikeLi/Self-Correction-Human-Parsing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 05:49:54 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Li", "Peike", ""], ["Xu", "Yunqiu", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""]]}, {"id": "1910.09783", "submitter": "Alexandre Cunha", "authors": "Fidel A. Guerrero Pe\\~na, Pedro D. Marrero Fernandez, Paul T. Tarr,\n  Tsang Ing Ren, Elliot M. Meyerowitz, Alexandre Cunha", "title": "J Regularization Improves Imbalanced Multiclass Segmentation", "comments": "Submitted to ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new loss formulation to further advance the multiclass\nsegmentation of cluttered cells under weakly supervised conditions.\n  We improve the separation of touching and immediate cells, obtaining sharp\nsegmentation boundaries with high adequacy, when we add Youden's $J$ statistic\nregularization term to the cross entropy loss. This regularization\nintrinsically supports class imbalance thus eliminating the necessity of\nexplicitly using weights to balance training. Simulations demonstrate this\ncapability and show how the regularization leads to better results by helping\nadvancing the optimization when cross entropy stalls.\n  We build upon our previous work on multiclass segmentation by adding yet\nanother training class representing gaps between adjacent cells.\n  This addition helps the classifier identify narrow gaps as background and no\nlonger as touching regions.\n  We present results of our methods for 2D and 3D images, from bright field to\nconfocal stacks containing different types of cells, and we show that they\naccurately segment individual cells after training with a limited number of\nannotated images, some of which are poorly annotated.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 06:25:06 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Pe\u00f1a", "Fidel A. Guerrero", ""], ["Fernandez", "Pedro D. Marrero", ""], ["Tarr", "Paul T.", ""], ["Ren", "Tsang Ing", ""], ["Meyerowitz", "Elliot M.", ""], ["Cunha", "Alexandre", ""]]}, {"id": "1910.09792", "submitter": "Jisoo Lee", "authors": "Jisoo Lee, Sae-Young Chung", "title": "Robust Training with Ensemble Consensus", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since deep neural networks are over-parameterized, they can memorize noisy\nexamples. We address such a memorization issue in the presence of label noise.\nFrom the fact that deep neural networks cannot generalize to neighborhoods of\nmemorized features, we hypothesize that noisy examples do not consistently\nincur small losses on the network under a certain perturbation. Based on this,\nwe propose a novel training method called Learning with Ensemble Consensus\n(LEC) that prevents overfitting to noisy examples by removing them based on the\nconsensus of an ensemble of perturbed networks. One of the proposed LECs, LTEC\noutperforms the current state-of-the-art methods on noisy MNIST, CIFAR-10, and\nCIFAR-100 in an efficient manner.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 06:58:10 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 03:33:50 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 09:59:16 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Lee", "Jisoo", ""], ["Chung", "Sae-Young", ""]]}, {"id": "1910.09798", "submitter": "Shruti Jadon", "authors": "Shruti Jadon, Aditya Acrot Srinivasan", "title": "Improving Siamese Networks for One Shot Learning using Kernel Based\n  Activation functions", "comments": "15 pages, 8 figures", "journal-ref": "Advances in Intelligent Systems and Computing book series (AISC,\n  volume 1175) Springer 2020", "doi": "10.1007/978-981-15-5619-7_25", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The lack of a large amount of training data has always been the constraining\nfactor in solving a lot of problems in machine learning, making One Shot\nLearning one of the most intriguing ideas in machine learning. It aims to learn\ninformation about object categories from one, or only a few training examples.\nThis process of learning in deep learning is usually accomplished by proper\nobjective function, i.e; loss function and embeddings extraction i.e;\narchitecture. In this paper, we discussed about metrics based deep learning\narchitectures for one shot learning such as Siamese neural networks and present\na method to improve on their accuracy using Kafnets (kernel-based\nnon-parametric activation functions for neural networks) by learning proper\nembeddings with relatively less number of epochs. Using kernel activation\nfunctions, we are able to achieve strong results which exceed those of ReLU\nbased deep learning models in terms of embeddings structure, loss convergence,\nand accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 07:17:07 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Jadon", "Shruti", ""], ["Srinivasan", "Aditya Acrot", ""]]}, {"id": "1910.09806", "submitter": "Andres Ussa Caycedo", "authors": "Andres Ussa, Luca Della Vedova, Vandana Reddy Padala, Deepak Singla,\n  Jyotibdha Acharya, Charles Zhang Lei, Garrick Orchard, Arindam Basu and\n  Bharath Ramesh", "title": "A low-power end-to-end hybrid neuromorphic framework for surveillance\n  applications", "comments": "12 pages, 3 figures, pre-print to BMVC workshops 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of deep learning, object recognition systems that can be\ndeployed for real-world applications are becoming commonplace. However,\ninference that needs to largely take place on the `edge' (not processed on\nservers), is a highly computational and memory intensive workload, making it\nintractable for low-power mobile nodes and remote security applications. To\naddress this challenge, this paper proposes a low-power (5W) end-to-end\nneuromorphic framework for object tracking and classification using event-based\ncameras that possess desirable properties such as low power consumption (5-14\nmW) and high dynamic range (120 dB). Nonetheless, unlike traditional approaches\nof using event-by-event processing, this work uses a mixed frame and event\napproach to get energy savings with high performance. Using a frame-based\nregion proposal method based on the density of foreground events, a\nhardware-friendly object tracking is implemented using the apparent object\nvelocity while tackling occlusion scenarios. For low-power classification of\nthe tracked objects, the event camera is interfaced to IBM TrueNorth, which is\ntime-multiplexed to tackle up to eight instances for a traffic monitoring\napplication. The frame-based object track input is converted back to spikes for\nTruenorth classification via the energy efficient deep network (EEDN) pipeline.\nUsing originally collected datasets, we train the TrueNorth model on the\nhardware track outputs, instead of using ground truth object locations as\ncommonly done, and demonstrate the efficacy of our system to handle practical\nsurveillance scenarios. Finally, we compare the proposed methodologies to\nstate-of-the-art event-based systems for object tracking and classification,\nand demonstrate the use case of our neuromorphic approach for low-power\napplications without sacrificing on performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 07:51:27 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 09:01:54 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 08:44:37 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Ussa", "Andres", ""], ["Della Vedova", "Luca", ""], ["Padala", "Vandana Reddy", ""], ["Singla", "Deepak", ""], ["Acharya", "Jyotibdha", ""], ["Lei", "Charles Zhang", ""], ["Orchard", "Garrick", ""], ["Basu", "Arindam", ""], ["Ramesh", "Bharath", ""]]}, {"id": "1910.09821", "submitter": "Dan Peng", "authors": "Dan Peng, Zizhan Zheng, Linhao Luo, Xiaofeng Zhang", "title": "Structure Matters: Towards Generating Transferable Adversarial Images", "comments": "accepted to ECAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on adversarial examples for image classification focus on\ndirectly modifying pixels with minor perturbations. The small perturbation\nrequirement is imposed to ensure the generated adversarial examples being\nnatural and realistic to humans, which, however, puts a curb on the attack\nspace thus limiting the attack ability and transferability especially for\nsystems protected by a defense mechanism. In this paper, we propose the novel\nconcepts of structure patterns and structure-aware perturbations that relax the\nsmall perturbation constraint while still keeping images natural. The key idea\nof our approach is to allow perceptible deviation in adversarial examples while\nkeeping structure patterns that are central to a human classifier. Built upon\nthese concepts, we propose a \\emph{structure-preserving attack (SPA)} for\ngenerating natural adversarial examples with extremely high transferability.\nEmpirical results on the MNIST and the CIFAR10 datasets show that SPA exhibits\nstrong attack ability in both the white-box and black-box setting even defenses\nare applied. Moreover, with the integration of PGD or CW attack, its attack\nability escalates sharply under the white-box setting, without losing the\noutstanding transferability inherited from SPA.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 08:20:00 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 15:21:15 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 10:33:59 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Peng", "Dan", ""], ["Zheng", "Zizhan", ""], ["Luo", "Linhao", ""], ["Zhang", "Xiaofeng", ""]]}, {"id": "1910.09830", "submitter": "Yuanxin Zhu", "authors": "Yuanxin Zhu, Zhao Yang, Li Wang, Sai Zhao, Xiao Hu, Dapeng Tao", "title": "Hetero-Center Loss for Cross-Modality Person Re-Identification", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modality person re-identification is a challenging problem which\nretrieves a given pedestrian image in RGB modality among all the gallery images\nin infrared modality. The task can address the limitation of RGB-based person\nRe-ID in dark environments. Existing researches mainly focus on enlarging\ninter-class differences of feature to solve the problem. However, few studies\ninvestigate improving intra-class cross-modality similarity, which is important\nfor this issue. In this paper, we propose a novel loss function, called\nHetero-Center loss (HC loss) to reduce the intra-class cross-modality\nvariations. Specifically, HC loss can supervise the network learning the\ncross-modality invariant information by constraining the intra-class center\ndistance between two heterogenous modalities. With the joint supervision of\nCross-Entropy (CE) loss and HC loss, the network is trained to achieve two\nvital objectives, inter-class discrepancy and intra-class cross-modality\nsimilarity as much as possible. Besides, we propose a simple and\nhigh-performance network architecture to learn local feature representations\nfor cross-modality person re-identification, which can be a baseline for future\nresearch. Extensive experiments indicate the effectiveness of the proposed\nmethods, which outperform state-of-the-art methods by a wide margin.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 08:35:45 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Zhu", "Yuanxin", ""], ["Yang", "Zhao", ""], ["Wang", "Li", ""], ["Zhao", "Sai", ""], ["Hu", "Xiao", ""], ["Tao", "Dapeng", ""]]}, {"id": "1910.09840", "submitter": "Sebastian Lapuschkin", "authors": "Maximilian Kohlbrenner, Alexander Bauer, Shinichi Nakajima, Alexander\n  Binder, Wojciech Samek, Sebastian Lapuschkin", "title": "Towards Best Practice in Explaining Neural Network Decisions with LRP", "comments": "7 pages, 4 figures, 1 table. fixed table row compared to v2.\n  Presented virtually at IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Within the last decade, neural network based predictors have demonstrated\nimpressive - and at times super-human - capabilities. This performance is often\npaid for with an intransparent prediction process and thus has sparked numerous\ncontributions in the novel field of explainable artificial intelligence (XAI).\nIn this paper, we focus on a popular and widely used method of XAI, the\nLayer-wise Relevance Propagation (LRP). Since its initial proposition LRP has\nevolved as a method, and a best practice for applying the method has tacitly\nemerged, based however on humanly observed evidence alone. In this paper we\ninvestigate - and for the first time quantify - the effect of this current best\npractice on feedforward neural networks in a visual object detection setting.\nThe results verify that the layer-dependent approach to LRP applied in recent\nliterature better represents the model's reasoning, and at the same time\nincreases the object localization and class discriminativity of LRP.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 08:58:54 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 14:06:45 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 20:00:09 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Kohlbrenner", "Maximilian", ""], ["Bauer", "Alexander", ""], ["Nakajima", "Shinichi", ""], ["Binder", "Alexander", ""], ["Samek", "Wojciech", ""], ["Lapuschkin", "Sebastian", ""]]}, {"id": "1910.09858", "submitter": "Juntao Guan", "authors": "Juntao Guan, Rui Lai, Ai Xiong, Zesheng Liu, Lin Gu", "title": "Fixed Pattern Noise Reduction for Infrared Images Based on Cascade\n  Residual Attention CNN", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2019.10.054", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing fixed pattern noise reduction (FPNR) methods are easily affected by\nthe motion state of the scene and working condition of the image sensor, which\nleads to over smooth effects, ghosting artifacts as well as slow convergence\nrate. To address these issues, we design an innovative cascade convolution\nneural network (CNN) model with residual skip connections to realize single\nframe blind FPNR operation without any parameter tuning. Moreover, a\ncoarse-fine convolution (CF-Conv) unit is introduced to extract complementary\nfeatures in various scales and fuse them to pick more spatial information.\nInspired by the success of the visual attention mechanism, we further propose a\nparticular spatial-channel noise attention unit (SCNAU) to separate the scene\ndetails from fixed pattern noise more thoroughly and recover the real scene\nmore accurately. Experimental results on test data demonstrate that the\nproposed cascade CNN-FPNR method outperforms the existing FPNR methods in both\nof visual effect and quantitative assessment.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 09:31:46 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Guan", "Juntao", ""], ["Lai", "Rui", ""], ["Xiong", "Ai", ""], ["Liu", "Zesheng", ""], ["Gu", "Lin", ""]]}, {"id": "1910.09900", "submitter": "Junyu Liu", "authors": "Jiwei Liu, Junyu Liu, Yang Liu, Rui Yang, Dongjun Lv, Zhengting Cai\n  and Jingjing Cui", "title": "A Locating Model for Pulmonary Tuberculosis Diagnosis in Radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: We propose an end-to-end CNN-based locating model for pulmonary\ntuberculosis (TB) diagnosis in radiographs. This model makes full use of chest\nradiograph (X-ray) for its improved accessibility, reduced cost and high\naccuracy for TB disease. Methods: Several specialized improvements are proposed\nfor detection task in medical field. A false positive (FP) restrictor head is\nintroduced for FP reduction. Anchor-oriented network heads is proposed in the\nposition regression section. An optimization of loss function is designed for\nhard example mining. Results: The experimental results show that when the\nthreshold of intersection over union (IoU) is set to 0.3, the average precision\n(AP) of two test data sets provided by different hospitals reaches 0.9023 and\n0.9332. Ablation experiments shows that hard example mining and change of\nregressor heads contribute most in this work, but FP restriction is necessary\nin a CAD diagnose system. Conclusion: The results prove the high precision and\ngood generalization ability of our proposed model comparing to previous works.\nSignificance: We first make full use of the feature extraction ability of CNNs\nin TB diagnostic field and make exploration in localization of TB, when the\nprevious works focus on the weaker task of healthy-sick subject classification.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 11:42:06 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Liu", "Jiwei", ""], ["Liu", "Junyu", ""], ["Liu", "Yang", ""], ["Yang", "Rui", ""], ["Lv", "Dongjun", ""], ["Cai", "Zhengting", ""], ["Cui", "Jingjing", ""]]}, {"id": "1910.09910", "submitter": "Mohamed Ibrahim", "authors": "Mohamed R. Ibrahim, James Haworth and Tao Cheng", "title": "WeatherNet: Recognising weather and visual conditions from street-level\n  images using deep residual learning", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting information related to weather and visual conditions at a given\ntime and space is indispensable for scene awareness, which strongly impacts our\nbehaviours, from simply walking in a city to riding a bike, driving a car, or\nautonomous drive-assistance. Despite the significance of this subject, it is\nstill not been fully addressed by the machine intelligence relying on deep\nlearning and computer vision to detect the multi-labels of weather and visual\nconditions with a unified method that can be easily used for practice. What has\nbeen achieved to-date is rather sectorial models that address limited number of\nlabels that do not cover the wide spectrum of weather and visual conditions.\nNonetheless, weather and visual conditions are often addressed individually. In\nthis paper, we introduce a novel framework to automatically extract this\ninformation from street-level images relying on deep learning and computer\nvision using a unified method without any pre-defined constraints in the\nprocessed images. A pipeline of four deep Convolutional Neural Network (CNN)\nmodels, so-called the WeatherNet, is trained, relying on residual learning\nusing ResNet50 architecture, to extract various weather and visual conditions\nsuch as Dawn/dusk, day and night for time detection, and glare for lighting\nconditions, and clear, rainy, snowy, and foggy for weather conditions. The\nWeatherNet shows strong performance in extracting this information from\nuser-defined images or video streams that can be used not limited to:\nautonomous vehicles and drive-assistance systems, tracking behaviours,\nsafety-related research, or even for better understanding cities through images\nfor policy-makers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 12:03:44 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Ibrahim", "Mohamed R.", ""], ["Haworth", "James", ""], ["Cheng", "Tao", ""]]}, {"id": "1910.09920", "submitter": "Farnoosh Heidarivincheh", "authors": "Farnoosh Heidarivincheh, Majid Mirmehdi, Dima Damen", "title": "Weakly-Supervised Completion Moment Detection using Temporal Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the progression of an action towards completion offers fine\ngrained insight into the actor's behaviour. In this work, we target detecting\nthe completion moment of actions, that is the moment when the action's goal has\nbeen successfully accomplished. This has potential applications from\nsurveillance to assistive living and human-robot interactions. Previous effort\nrequired human annotations of the completion moment for training (i.e. full\nsupervision). In this work, we present an approach for moment detection from\nweak video-level labels. Given both complete and incomplete sequences, of the\nsame action, we learn temporal attention, along with accumulated completion\nprediction from all frames in the sequence. We also demonstrate how the\napproach can be used when completion moment supervision is available. We\nevaluate and compare our approach on actions from three datasets, namely HMDB,\nUCF101 and RGBD-AC, and show that temporal attention improves detection in both\nweakly-supervised and fully-supervised settings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 12:31:07 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Heidarivincheh", "Farnoosh", ""], ["Mirmehdi", "Majid", ""], ["Damen", "Dima", ""]]}, {"id": "1910.09931", "submitter": "Andrew Brown", "authors": "Andrew Brown, Pascal Mettes, Marcel Worring", "title": "4-Connected Shift Residual Networks", "comments": "ICCV Neural Architects Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shift operation was recently introduced as an alternative to spatial\nconvolutions. The operation moves subsets of activations horizontally and/or\nvertically. Spatial convolutions are then replaced with shift operations\nfollowed by point-wise convolutions, significantly reducing computational\ncosts. In this work, we investigate how shifts should best be applied to high\naccuracy CNNs. We apply shifts of two different neighbourhood groups to ResNet\non ImageNet: the originally introduced 8-connected (8C) neighbourhood shift and\nthe less well studied 4-connected (4C) neighbourhood shift. We find that when\nreplacing ResNet's spatial convolutions with shifts, both shift neighbourhoods\ngive equal ImageNet accuracy, showing the sufficiency of small neighbourhoods\nfor large images. Interestingly, when incorporating shifts to all point-wise\nconvolutions in residual networks, 4-connected shifts outperform 8-connected\nshifts. Such a 4-connected shift setup gives the same accuracy as full residual\nnetworks while reducing the number of parameters and FLOPs by over 40%. We then\nhighlight that without spatial convolutions, ResNet's downsampling/upsampling\nbottleneck channel structure is no longer needed. We show a new, 4C shift-based\nresidual network, much shorter than the original ResNet yet with a higher\naccuracy for the same computational cost. This network is the highest accuracy\nshift-based network yet shown, demonstrating the potential of shifting in deep\nneural networks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 12:46:31 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Brown", "Andrew", ""], ["Mettes", "Pascal", ""], ["Worring", "Marcel", ""]]}, {"id": "1910.09972", "submitter": "Yuki Saito", "authors": "Yuki Saito, Takuma Nakamura, Hirotaka Hachiya, Kenji Fukumizu", "title": "Exchangeable deep neural networks for set-to-set matching and learning", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58520-4_37", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching two different sets of items, called heterogeneous set-to-set\nmatching problem, has recently received attention as a promising problem. The\ndifficulties are to extract features to match a correct pair of different sets\nand also preserve two types of exchangeability required for set-to-set\nmatching: the pair of sets, as well as the items in each set, should be\nexchangeable. In this study, we propose a novel deep learning architecture to\naddress the abovementioned difficulties and also an efficient training\nframework for set-to-set matching. We evaluate the methods through experiments\nbased on two industrial applications: fashion set recommendation and group\nre-identification. In these experiments, we show that the proposed method\nprovides significant improvements and results compared with the\nstate-of-the-art methods, thereby validating our architecture for the\nheterogeneous set matching problem.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 13:42:39 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 09:34:59 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Saito", "Yuki", ""], ["Nakamura", "Takuma", ""], ["Hachiya", "Hirotaka", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1910.09976", "submitter": "Ruben H\\\"uhnerbein", "authors": "Ruben H\\\"uhnerbein, Fabrizio Savarino, Stefania Petra, Christoph\n  Schn\\\"orr", "title": "Learning Adaptive Regularization for Image Labeling Using Geometric\n  Assignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the inverse problem of model parameter learning for pixelwise image\nlabeling, using the linear assignment flow and training data with ground truth.\nThis is accomplished by a Riemannian gradient flow on the manifold of\nparameters that determine the regularization properties of the assignment flow.\nUsing the symplectic partitioned Runge--Kutta method for numerical integration,\nit is shown that deriving the sensitivity conditions of the parameter learning\nproblem and its discretization commute. A convenient property of our approach\nis that learning is based on exact inference. Carefully designed experiments\ndemonstrate the performance of our approach, the expressiveness of the\nmathematical model as well as its limitations, from the viewpoint of\nstatistical learning and optimal control.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 13:45:42 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 15:55:45 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["H\u00fchnerbein", "Ruben", ""], ["Savarino", "Fabrizio", ""], ["Petra", "Stefania", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1910.10006", "submitter": "Nicholas Marshall", "authors": "Nicholas F. Marshall, Ti-Yen Lan, Tamir Bendory, Amit Singer", "title": "Image recovery from rotational and translational invariants", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for recovering an image from its rotationally and\ntranslationally invariant features based on autocorrelation analysis. This work\nis an instance of the multi-target detection statistical model, which is mainly\nused to study the mathematical and computational properties of single-particle\nreconstruction using cryo-electron microscopy (cryo-EM) at low signal-to-noise\nratios. We demonstrate with synthetic numerical experiments that an image can\nbe reconstructed from rotationally and translationally invariant features and\nshow that the reconstruction is robust to noise. These results constitute an\nimportant step towards the goal of structure determination of small\nbiomolecules using cryo-EM.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 14:33:59 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Marshall", "Nicholas F.", ""], ["Lan", "Ti-Yen", ""], ["Bendory", "Tamir", ""], ["Singer", "Amit", ""]]}, {"id": "1910.10017", "submitter": "Minh-Tan Pham", "authors": "Alice Froidevaux, Andr\\'ea Julier, Agustin Lifschitz, Minh-Tan Pham,\n  Romain Dambreville, S\\'ebastien Lef\\`evre, Pierre Lassalle, Thanh-Long Huynh", "title": "Vehicle detection and counting from VHR satellite images: efforts and\n  open issues", "comments": "4 pages, planned for a conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of new infrastructures (commercial, logistics, industrial or\nresidential) from satellite images constitutes a proven method to investigate\nand follow economic and urban growth. The level of activities or exploitation\nof these sites may be hardly determined by building inspection, but could be\ninferred from vehicle presence from nearby streets and parking lots. We present\nin this paper two deep learning-based models for vehicle counting from optical\nsatellite images coming from the Pleiades sensor at 50-cm spatial resolution.\nBoth segmentation (Tiramisu) and detection (YOLO) architectures were\ninvestigated. These networks were adapted, trained and validated on a data set\nincluding 87k vehicles, annotated using an interactive semi-automatic tool\ndeveloped by the authors. Experimental results show that both segmentation and\ndetection models could achieve a precision rate higher than 85% with a recall\nrate also high (76.4% and 71.9% for Tiramisu and YOLO respectively).\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 14:53:04 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 15:21:51 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Froidevaux", "Alice", ""], ["Julier", "Andr\u00e9a", ""], ["Lifschitz", "Agustin", ""], ["Pham", "Minh-Tan", ""], ["Dambreville", "Romain", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""], ["Lassalle", "Pierre", ""], ["Huynh", "Thanh-Long", ""]]}, {"id": "1910.10026", "submitter": "Alina Marcu M.Sc", "authors": "Alina Marcu, Dragos Costea, Vlad Licaret and Marius Leordeanu", "title": "Towards Automatic Annotation for Semantic Segmentation in Drone Videos", "comments": "7 pages, 6 figures, submitted at the International Conference on\n  Robotics and Automation (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a crucial task for robot navigation and safety.\nHowever, it requires huge amounts of pixelwise annotations to yield accurate\nresults. While recent progress in computer vision algorithms has been heavily\nboosted by large ground-level datasets, the labeling time has hampered progress\nin low altitude UAV applications, mostly due to the difficulty imposed by large\nobject scales and pose variations. Motivated by the lack of a large video\naerial dataset, we introduce a new one, with high resolution (4K) images and\nmanually-annotated dense labels every 50 frames. To help the video labeling\nprocess, we make an important step towards automatic annotation and propose\nSegProp, an iterative flow-based method with geometric constrains to propagate\nthe semantic labels to frames that lack human annotations. This results in a\ndataset with more than 50k annotated frames - the largest of its kind, to the\nbest of our knowledge. Our experiments show that SegProp surpasses current\nstate-of-the-art label propagation methods by a significant margin.\nFurthermore, when training a semantic segmentation deep neural net using the\nautomatically annotated frames, we obtain a compelling overall performance\nboost at test time of 16.8% mean F-measure over a baseline trained only with\nmanually-labeled frames.\n  Our Ruralscapes dataset, the label propagation code and a fast segmentation\ntool are available at our website:\nhttps://sites.google.com/site/aerialimageunderstanding/\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 15:01:58 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Marcu", "Alina", ""], ["Costea", "Dragos", ""], ["Licaret", "Vlad", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1910.10027", "submitter": "Waqas Sultani", "authors": "Waqas Sultani and Mubarak Shah", "title": "Human Action Recognition in Drone Videos using a Few Aerial Training\n  Examples", "comments": "CVIU, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones are enabling new forms of human actions surveillance due to their low\ncost and fast mobility. However, using deep neural networks for automatic\naerial action recognition is difficult due to the need for a large number of\ntraining aerial human action videos. Collecting a large number of human action\naerial videos is costly, time-consuming, and difficult. In this paper, we\nexplore two alternative data sources to improve aerial action classification\nwhen only a few training aerial examples are available. As a first data source,\nwe resort to video games. We collect plenty of aerial game action videos using\ntwo gaming engines. For the second data source, we leverage conditional\nWasserstein Generative Adversarial Networks to generate aerial features from\nground videos. Given that both data sources have some limitations, e.g. game\nvideos are biased towards specific actions categories (fighting, shooting,\netc.,), and it is not easy to generate good discriminative GAN-generated\nfeatures for all types of actions, we need to efficiently integrate two dataset\nsources with few available real aerial training videos. To address this\nchallenge of the heterogeneous nature of the data, we propose to use a disjoint\nmultitask learning framework. We feed the network with real and game, or real\nand GAN-generated data in an alternating fashion to obtain an improved action\nclassifier. We validate the proposed approach on two aerial action datasets and\ndemonstrate that features from aerial game videos and those generated from GAN\ncan be extremely useful for an improved action recognition in real aerial\nvideos when only a few real aerial training examples are available.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 15:02:36 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 08:35:04 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 19:44:21 GMT"}, {"version": "v4", "created": "Fri, 2 Apr 2021 12:33:37 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sultani", "Waqas", ""], ["Shah", "Mubarak", ""]]}, {"id": "1910.10035", "submitter": "Shahab Aslani", "authors": "Shahab Aslani, Vittorio Murino, Michael Dayan, Roger Tam, Diego Sona,\n  Ghassan Hamarneh", "title": "Scanner Invariant Multiple Sclerosis Lesion Segmentation from MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple and effective generalization method for magnetic\nresonance imaging (MRI) segmentation when data is collected from multiple MRI\nscanning sites and as a consequence is affected by (site-)domain shifts. We\npropose to integrate a traditional encoder-decoder network with a\nregularization network. This added network includes an auxiliary loss term\nwhich is responsible for the reduction of the domain shift problem and for the\nresulting improved generalization. The proposed method was evaluated on\nmultiple sclerosis lesion segmentation from MRI data. We tested the proposed\nmodel on an in-house clinical dataset including 117 patients from 56 different\nscanning sites. In the experiments, our method showed better generalization\nperformance than other baseline networks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 15:11:18 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Aslani", "Shahab", ""], ["Murino", "Vittorio", ""], ["Dayan", "Michael", ""], ["Tam", "Roger", ""], ["Sona", "Diego", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1910.10051", "submitter": "Amit Moscovich", "authors": "Ye Zhou, Amit Moscovich, Tamir Bendory, Alberto Bartesaghi", "title": "Unsupervised particle sorting for high-resolution single-particle\n  cryo-EM", "comments": "12 pages, 7 figures", "journal-ref": "Inverse Problems 36:4 (2020)", "doi": "10.1088/1361-6420/ab5ec8", "report-no": null, "categories": "cs.CV eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-particle cryo-Electron Microscopy (EM) has become a popular technique\nfor determining the structure of challenging biomolecules that are inaccessible\nto other technologies. Recent advances in automation, both in data collection\nand data processing, have significantly lowered the barrier for non-expert\nusers to successfully execute the structure determination workflow. Many\ncritical data processing steps, however, still require expert user intervention\nin order to converge to the correct high-resolution structure. In particular,\nstrategies to identify homogeneous populations of particles rely heavily on\nsubjective criteria that are not always consistent or reproducible among\ndifferent users. Here, we explore the use of unsupervised strategies for\nparticle sorting that are compatible with the autonomous operation of the image\nprocessing pipeline. More specifically, we show that particles can be\nsuccessfully sorted based on a simple statistical model for the distribution of\nscores assigned during refinement. This represents an important step towards\nthe development of automated workflows for protein structure determination\nusing single-particle cryo-EM.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 15:45:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhou", "Ye", ""], ["Moscovich", "Amit", ""], ["Bendory", "Tamir", ""], ["Bartesaghi", "Alberto", ""]]}, {"id": "1910.10053", "submitter": "Anurag Ranjan", "authors": "Anurag Ranjan and Joel Janai and Andreas Geiger and Michael J. Black", "title": "Attacking Optical Flow", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural nets achieve state-of-the-art performance on the problem of\noptical flow estimation. Since optical flow is used in several safety-critical\napplications like self-driving cars, it is important to gain insights into the\nrobustness of those techniques. Recently, it has been shown that adversarial\nattacks easily fool deep neural networks to misclassify objects. The robustness\nof optical flow networks to adversarial attacks, however, has not been studied\nso far. In this paper, we extend adversarial patch attacks to optical flow\nnetworks and show that such attacks can compromise their performance. We show\nthat corrupting a small patch of less than 1% of the image size can\nsignificantly affect optical flow estimates. Our attacks lead to noisy flow\nestimates that extend significantly beyond the region of the attack, in many\ncases even completely erasing the motion of objects in the scene. While\nnetworks using an encoder-decoder architecture are very sensitive to these\nattacks, we found that networks using a spatial pyramid architecture are less\naffected. We analyse the success and failure of attacking both architectures by\nvisualizing their feature maps and comparing them to classical optical flow\ntechniques which are robust to these attacks. We also demonstrate that such\nattacks are practical by placing a printed pattern into real scenes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 15:47:56 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Ranjan", "Anurag", ""], ["Janai", "Joel", ""], ["Geiger", "Andreas", ""], ["Black", "Michael J.", ""]]}, {"id": "1910.10056", "submitter": "Xia Huang", "authors": "Xia Huang, Hossein Mousavi and Gemma Roig", "title": "Predictive Coding Networks Meet Action Recognition", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Action recognition is a key problem in computer vision that labels videos\nwith a set of predefined actions. Capturing both, semantic content and motion,\nalong the video frames is key to achieve high accuracy performance on this\ntask. Most of the state-of-the-art methods rely on RGB frames for extracting\nthe semantics and pre-computed optical flow fields as a motion cue. Then, both\nare combined using deep neural networks. Yet, it has been argued that such\nmodels are not able to leverage the motion information extracted from the\noptical flow, but instead the optical flow allows for better recognition of\npeople and objects in the video. This urges the need to explore different cues\nor models that can extract motion in a more informative fashion. To tackle this\nissue, we propose to explore the predictive coding network, so called PredNet,\na recurrent neural network that propagates predictive coding errors across\nlayers and time steps. We analyze whether PredNet can better capture motions in\nvideos by estimating over time the representations extracted from pre-trained\nnetworks for action recognition. In this way, the model only relies on the\nvideo frames, and does not need pre-processed optical flows as input. We report\nthe effectiveness of our proposed model on UCF101 and HMDB51 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 15:53:03 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Huang", "Xia", ""], ["Mousavi", "Hossein", ""], ["Roig", "Gemma", ""]]}, {"id": "1910.10088", "submitter": "Adri\\`a Recasens", "authors": "Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Matusik,\n  Antonio Torralba", "title": "Gaze360: Physically Unconstrained Gaze Estimation in the Wild", "comments": "International Conference in Computer Vision, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding where people are looking is an informative social cue. In this\nwork, we present Gaze360, a large-scale gaze-tracking dataset and method for\nrobust 3D gaze estimation in unconstrained images. Our dataset consists of 238\nsubjects in indoor and outdoor environments with labelled 3D gaze across a wide\nrange of head poses and distances. It is the largest publicly available dataset\nof its kind by both subject and variety, made possible by a simple and\nefficient collection method. Our proposed 3D gaze model extends existing models\nto include temporal information and to directly output an estimate of gaze\nuncertainty. We demonstrate the benefits of our model via an ablation study,\nand show its generalization performance via a cross-dataset evaluation against\nother recent gaze benchmark datasets. We furthermore propose a simple\nself-supervised approach to improve cross-dataset domain adaptation. Finally,\nwe demonstrate an application of our model for estimating customer attention in\na supermarket setting. Our dataset and models are available at\nhttp://gaze360.csail.mit.edu .\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:30:55 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Kellnhofer", "Petr", ""], ["Recasens", "Adria", ""], ["Stent", "Simon", ""], ["Matusik", "Wojciech", ""], ["Torralba", "Antonio", ""]]}, {"id": "1910.10093", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Tao Xiang", "title": "Torchreid: A Library for Deep Learning Person Re-Identification in\n  Pytorch", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID), which aims to re-identify people across\ndifferent camera views, has been significantly advanced by deep learning in\nrecent years, particularly with convolutional neural networks (CNNs). In this\npaper, we present Torchreid, a software library built on PyTorch that allows\nfast development and end-to-end training and evaluation of deep re-ID models.\nAs a general-purpose framework for person re-ID research, Torchreid provides\n(1) unified data loaders that support 15 commonly used re-ID benchmark datasets\ncovering both image and video domains, (2) streamlined pipelines for quick\ndevelopment and benchmarking of deep re-ID models, and (3) implementations of\nthe latest re-ID CNN architectures along with their pre-trained models to\nfacilitate reproducibility as well as future research. With a high-level\nmodularity in its design, Torchreid offers a great flexibility to allow easy\nextension to new datasets, CNN models and loss functions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:33:05 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Xiang", "Tao", ""]]}, {"id": "1910.10095", "submitter": "Chao Pan", "authors": "Chao Pan, S. M. Hossein Tabatabaei Yazdi, S Kasra Tabatabaei, Alvaro\n  G. Hernandez, Charles Schroeder, Olgica Milenkovic", "title": "Image processing in DNA", "comments": "5 pages, revision of ICASSP version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The main obstacles for the practical deployment of DNA-based data storage\nplatforms are the prohibitively high cost of synthetic DNA and the large number\nof errors introduced during synthesis. In particular, synthetic DNA products\ncontain both individual oligo (fragment) symbol errors as well as missing DNA\noligo errors, with rates that exceed those of modern storage systems by orders\nof magnitude. These errors can be corrected either through the use of a large\nnumber of redundant oligos or through cycles of writing, reading, and rewriting\nof information that eliminate the errors. Both approaches add to the overall\nstorage cost and are hence undesirable. Here we propose the first method for\nstoring quantized images in DNA that uses signal processing and machine\nlearning techniques to deal with error and cost issues without resorting to the\nuse of redundant oligos or rewriting. Our methods rely on decoupling the RGB\nchannels of images, performing specialized quantization and compression on the\nindividual color channels, and using new discoloration detection and image\ninpainting techniques. We demonstrate the performance of our approach\nexperimentally on a collection of movie posters stored in DNA.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:34:04 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 00:50:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pan", "Chao", ""], ["Yazdi", "S. M. Hossein Tabatabaei", ""], ["Tabatabaei", "S Kasra", ""], ["Hernandez", "Alvaro G.", ""], ["Schroeder", "Charles", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1910.10100", "submitter": "Junqi Tang", "authors": "Junqi Tang, Karen Egiazarian, Mohammad Golbabaee, Mike Davies", "title": "The Practicality of Stochastic Optimization in Imaging Inverse Problems", "comments": null, "journal-ref": "Published in IEEE Transactions on Computational Imaging, Vol. 6,\n  2020", "doi": "10.1109/TCI.2020.3032101", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the practicality of stochastic gradient descent\nand recently introduced variants with variance-reduction techniques in imaging\ninverse problems. Such algorithms have been shown in the machine learning\nliterature to have optimal complexities in theory, and provide great\nimprovement empirically over the deterministic gradient methods. Surprisingly,\nin some tasks such as image deblurring, many of such methods fail to converge\nfaster than the accelerated deterministic gradient methods, even in terms of\nepoch counts. We investigate this phenomenon and propose a theory-inspired\nmechanism for the practitioners to efficiently characterize whether it is\nbeneficial for an inverse problem to be solved by stochastic optimization\ntechniques or not. Using standard tools in numerical linear algebra, we derive\nconditions on the spectral structure of the inverse problem for being a\nsuitable application of stochastic gradient methods. Particularly, we show\nthat, for an imaging inverse problem, if and only if its Hessain matrix has a\nfast-decaying eigenspectrum, then the stochastic gradient methods can be more\nadvantageous than deterministic methods for solving such a problem. Our results\nalso provide guidance on choosing appropriately the partition minibatch\nschemes, showing that a good minibatch scheme typically has relatively low\ncorrelation within each of the minibatches. Finally, we propose an accelerated\nprimal-dual SGD algorithm in order to tackle another key bottleneck of\nstochastic optimization which is the heavy computation of proximal operators.\nThe proposed method has fast convergence rate in practice, and is able to\nefficiently handle non-smooth regularization terms which are coupled with\nlinear operators.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:39:00 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 17:24:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tang", "Junqi", ""], ["Egiazarian", "Karen", ""], ["Golbabaee", "Mohammad", ""], ["Davies", "Mike", ""]]}, {"id": "1910.10111", "submitter": "Jianyuan Guo", "authors": "Jianyuan Guo, Yuhui Yuan, Lang Huang, Chao Zhang, Jinge Yao and Kai\n  Han", "title": "Beyond Human Parts: Dual Part-Aligned Representations for Person\n  Re-Identification", "comments": "Accepted in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is a challenging task due to various complex\nfactors. Recent studies have attempted to integrate human parsing results or\nexternally defined attributes to help capture human parts or important object\nregions. On the other hand, there still exist many useful contextual cues that\ndo not fall into the scope of predefined human parts or attributes. In this\npaper, we address the missed contextual cues by exploiting both the accurate\nhuman parts and the coarse non-human parts. In our implementation, we apply a\nhuman parsing model to extract the binary human part masks \\emph{and} a\nself-attention mechanism to capture the soft latent (non-human) part masks. We\nverify the effectiveness of our approach with new state-of-the-art performances\non three challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03. Our\nimplementation is available at https://github.com/ggjy/P2Net.pytorch.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:53:41 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Guo", "Jianyuan", ""], ["Yuan", "Yuhui", ""], ["Huang", "Lang", ""], ["Zhang", "Chao", ""], ["Yao", "Jinge", ""], ["Han", "Kai", ""]]}, {"id": "1910.10143", "submitter": "Alexandra Luccioni", "authors": "Sharon Zhou, Alexandra Luccioni, Gautier Cosne, Michael S. Bernstein,\n  Yoshua Bengio", "title": "Establishing an Evaluation Metric to Quantify Climate Change Image\n  Realism", "comments": "Accepted to the NeurIPS 2019 Workshop, Tackling Climate Change with\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With success on controlled tasks, generative models are being increasingly\napplied to humanitarian applications [1,2]. In this paper, we focus on the\nevaluation of a conditional generative model that illustrates the consequences\nof climate change-induced flooding to encourage public interest and awareness\non the issue. Because metrics for comparing the realism of different modes in a\nconditional generative model do not exist, we propose several automated and\nhuman-based methods for evaluation. To do this, we adapt several existing\nmetrics, and assess the automated metrics against gold standard human\nevaluation. We find that using Fr\\'echet Inception Distance (FID) with\nembeddings from an intermediary Inception-V3 layer that precedes the auxiliary\nclassifier produces results most correlated with human realism. While\ninsufficient alone to establish a human-correlated automatic evaluation metric,\nwe believe this work begins to bridge the gap between human and automated\ngenerative evaluation procedures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 17:59:38 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Zhou", "Sharon", ""], ["Luccioni", "Alexandra", ""], ["Cosne", "Gautier", ""], ["Bernstein", "Michael S.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1910.10187", "submitter": "Robert Grupp", "authors": "Robert Grupp, Ryan Murphy, Rachel Hegeman, Clayton Alexander, Mathias\n  Unberath, Yoshito Otake, Benjamin McArthur, Mehran Armand, Russell Taylor", "title": "Fast and Automatic Periacetabular Osteotomy Fragment Pose Estimation\n  Using Intraoperatively Implanted Fiducials and Single-View Fluoroscopy", "comments": "Revised article to address reviewer comments. Under review for\n  Physics in Medicine and Biology. Supplementary video at\n  https://youtu.be/0E0U9G81q8g", "journal-ref": "2020 Phys. Med. Biol. 65 245019", "doi": "10.1088/1361-6560/aba089", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and consistent mental interpretation of fluoroscopy to determine the\nposition and orientation of acetabular bone fragments in 3D space is difficult.\nWe propose a computer assisted approach that uses a single fluoroscopic view\nand quickly reports the pose of an acetabular fragment without any user input\nor initialization. Intraoperatively, but prior to any osteotomies, two\nconstellations of metallic ball-bearings (BBs) are injected into the wing of a\npatient's ilium and lateral superior pubic ramus. One constellation is located\non the expected acetabular fragment, and the other is located on the remaining,\nlarger, pelvis fragment. The 3D locations of each BB are reconstructed using\nthree fluoroscopic views and 2D/3D registrations to a preoperative CT scan of\nthe pelvis. The relative pose of the fragment is established by estimating the\nmovement of the two BB constellations using a single fluoroscopic view taken\nafter osteotomy and fragment relocation. BB detection and inter-view\ncorrespondences are automatically computed throughout the processing pipeline.\nThe proposed method was evaluated on a multitude of fluoroscopic images\ncollected from six cadaveric surgeries performed bilaterally on three\nspecimens. Mean fragment rotation error was 2.4 +/- 1.0 degrees, mean\ntranslation error was 2.1 +/- 0.6 mm, and mean 3D lateral center edge angle\nerror was 1.0 +/- 0.5 degrees. The average runtime of the single-view pose\nestimation was 0.7 +/- 0.2 seconds. The proposed method demonstrates accuracy\nsimilar to other state of the art systems which require optical tracking\nsystems or multiple-view 2D/3D registrations with manual input. The errors\nreported on fragment poses and lateral center edge angles are within the\nmargins required for accurate intraoperative evaluation of femoral head\ncoverage.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 18:15:44 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 14:56:53 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 19:47:18 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Grupp", "Robert", ""], ["Murphy", "Ryan", ""], ["Hegeman", "Rachel", ""], ["Alexander", "Clayton", ""], ["Unberath", "Mathias", ""], ["Otake", "Yoshito", ""], ["McArthur", "Benjamin", ""], ["Armand", "Mehran", ""], ["Taylor", "Russell", ""]]}, {"id": "1910.10209", "submitter": "Amey Chaware", "authors": "Amey Chaware, Colin L. Cooke, Kanghyun Kim, Roarke Horstmeyer", "title": "Towards an Intelligent Microscope: adaptively learned illumination for\n  optimal sample classification", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent machine learning techniques have dramatically changed how we process\ndigital images. However, the way in which we capture images is still largely\ndriven by human intuition and experience. This restriction is in part due to\nthe many available degrees of freedom that alter the image acquisition process\n(lens focus, exposure, filtering, etc). Here we focus on one such degree of\nfreedom - illumination within a microscope - which can drastically alter\ninformation captured by the image sensor. We present a reinforcement learning\nsystem that adaptively explores optimal patterns to illuminate specimens for\nimmediate classification. The agent uses a recurrent latent space to encode a\nlarge set of variably-illuminated samples and illumination patterns. We train\nour agent using a reward that balances classification confidence with image\nacquisition cost. By synthesizing knowledge over multiple snapshots, the agent\ncan classify on the basis of all previous images with higher accuracy than from\nnaively illuminated images, thus demonstrating a smarter way to physically\ncapture task-specific information.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 19:49:06 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 22:26:55 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Chaware", "Amey", ""], ["Cooke", "Colin L.", ""], ["Kim", "Kanghyun", ""], ["Horstmeyer", "Roarke", ""]]}, {"id": "1910.10223", "submitter": "Patrick Esser", "authors": "Patrick Esser, Johannes Haux and Bj\\\"orn Ommer", "title": "Unsupervised Robust Disentangling of Latent Characteristics for Image\n  Synthesis", "comments": "ICCV 2019. Project page at\n  https://compvis.github.io/robust-disentangling/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models come with the promise to learn an explainable\nrepresentation for visual objects that allows image sampling, synthesis, and\nselective modification. The main challenge is to learn to properly model the\nindependent latent characteristics of an object, especially its appearance and\npose. We present a novel approach that learns disentangled representations of\nthese characteristics and explains them individually. Training requires only\npairs of images depicting the same object appearance, but no pose annotations.\nWe propose an additional classifier that estimates the minimal amount of\nregularization required to enforce disentanglement. Thus both representations\ntogether can completely explain an image while being independent of each other.\nPrevious methods based on adversarial approaches fail to enforce this\nindependence, while methods based on variational approaches lead to\nuninformative representations. In experiments on diverse object categories, the\napproach successfully recombines pose and appearance to reconstruct and\nretarget novel synthesized images. We achieve significant improvements over\nstate-of-the-art methods which utilize the same level of supervision, and reach\nperformances comparable to those of pose-supervised approaches. However, we can\nhandle the vast body of articulated object classes for which no pose\nmodels/annotations are available.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 20:48:44 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Esser", "Patrick", ""], ["Haux", "Johannes", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1910.10264", "submitter": "Andrew Lensen", "authors": "Andrew Lensen, Bing Xue, Mengjie Zhang", "title": "Genetic Programming for Evolving Similarity Functions for Clustering:\n  Representations and Analysis", "comments": "29 pages, accepted by Evolutionary Computation (Journal), MIT Press", "journal-ref": null, "doi": "10.1162/evco_a_00264", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a difficult and widely-studied data mining task, with many\nvarieties of clustering algorithms proposed in the literature. Nearly all\nalgorithms use a similarity measure such as a distance metric (e.g. Euclidean\ndistance) to decide which instances to assign to the same cluster. These\nsimilarity measures are generally pre-defined and cannot be easily tailored to\nthe properties of a particular dataset, which leads to limitations in the\nquality and the interpretability of the clusters produced. In this paper, we\npropose a new approach to automatically evolving similarity functions for a\ngiven clustering algorithm by using genetic programming. We introduce a new\ngenetic programming-based method which automatically selects a small subset of\nfeatures (feature selection) and then combines them using a variety of\nfunctions (feature construction) to produce dynamic and flexible similarity\nfunctions that are specifically designed for a given dataset. We demonstrate\nhow the evolved similarity functions can be used to perform clustering using a\ngraph-based representation. The results of a variety of experiments across a\nrange of large, high-dimensional datasets show that the proposed approach can\nachieve higher and more consistent performance than the benchmark methods. We\nfurther extend the proposed approach to automatically produce multiple\ncomplementary similarity functions by using a multi-tree approach, which gives\nfurther performance improvements. We also analyse the interpretability and\nstructure of the automatically evolved similarity functions to provide insight\ninto how and why they are superior to standard distance metrics.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 22:45:19 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Lensen", "Andrew", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1910.10317", "submitter": "Antonia Lovjer", "authors": "Antonia Lovjer, Minsu Yeom, Benedikt D. Schifferer, Iddo Drori", "title": "Using Segmentation Masks in the ICCV 2019 Learning to Drive Challenge", "comments": null, "journal-ref": "ICCV Autonomous Driving Workshop, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we predict vehicle speed and steering angle given camera image\nframes. Our key contribution is using an external pre-trained neural network\nfor segmentation. We augment the raw images with their segmentation masks and\nmirror images. We ensemble three diverse neural network models (i) a CNN using\na single image and its segmentation mask, (ii) a stacked CNN taking as input a\nsequence of images and segmentation masks, and (iii) a bidirectional GRU,\nextracting image features using a pre-trained ResNet34, DenseNet121 and our own\nCNN single image model. We achieve the second best performance for MSE angle\nand second best performance overall, to win 2nd place in the ICCV Learning to\nDrive challenge. We make our models and code publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 02:24:28 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Lovjer", "Antonia", ""], ["Yeom", "Minsu", ""], ["Schifferer", "Benedikt D.", ""], ["Drori", "Iddo", ""]]}, {"id": "1910.10318", "submitter": "Michael Diodato", "authors": "Michael Diodato, Yu Li, Manik Goyal, Iddo Drori", "title": "Winning the ICCV 2019 Learning to Drive Challenge", "comments": null, "journal-ref": "ICCV Autonomous Driving Workshop, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving has a significant impact on society. Predicting vehicle\ntrajectories, specifically, angle and speed, is important for safe and\ncomfortable driving. This work focuses on fusing inputs from camera sensors and\nvisual map data which lead to significant improvement in performance and plays\na key role in winning the challenge. We use pre-trained CNN's for processing\nimage frames, a neural network for fusing the image representation with visual\nmap data, and train a sequence model for time series prediction. We demonstrate\nthe best performing MSE angle and best performance overall, to win the ICCV\n2019 Learning to Drive challenge. We make our models and code publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 02:31:18 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Diodato", "Michael", ""], ["Li", "Yu", ""], ["Goyal", "Manik", ""], ["Drori", "Iddo", ""]]}, {"id": "1910.10320", "submitter": "Shuhan Tan", "authors": "Shuhan Tan, Xingchao Peng, Kate Saenko", "title": "Class-imbalanced Domain Adaptation: An Empirical Odyssey", "comments": "ECCV 2020 Workshops - TASK-CV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation is a promising way to generalize deep models\nto novel domains. However, the current literature assumes that the label\ndistribution is domain-invariant and only aligns the feature distributions or\nvice versa. In this work, we explore the more realistic task of\nClass-imbalanced Domain Adaptation: How to align feature distributions across\ndomains while the label distributions of the two domains are also different?\nTaking a practical step towards this problem, we constructed the first\nbenchmark with 22 cross-domain tasks from 6real-image datasets. We conducted\ncomprehensive experiments on 10 recent domain adaptation methods and find most\nof them are very fragile in the face of coexisting feature and label\ndistribution shift. Towards a better solution, we further proposed a feature\nand label distribution CO-ALignment (COAL) model with a novel combination of\nexisting ideas. COAL is empirically shown to outperform the most recent domain\nadaptation methods on our benchmarks. We believe the provided benchmarks,\nempirical analysis results, and the COAL baseline could stimulate and\nfacilitate future research towards this important problem.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 02:35:46 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 07:58:59 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Tan", "Shuhan", ""], ["Peng", "Xingchao", ""], ["Saenko", "Kate", ""]]}, {"id": "1910.10323", "submitter": "Zhilei Liu", "authors": "Zhilei Liu, Diyi Liu, Yunpeng Wu", "title": "Region Based Adversarial Synthesis of Facial Action Units", "comments": "Accepted by MMM2020", "journal-ref": null, "doi": "10.1007/978-3-030-37734-2_42", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression synthesis or editing has recently received increasing\nattention in the field of affective computing and facial expression modeling.\nHowever, most existing facial expression synthesis works are limited in paired\ntraining data, low resolution, identity information damaging, and so on. To\naddress those limitations, this paper introduces a novel Action Unit (AU) level\nfacial expression synthesis method called Local Attentive Conditional\nGenerative Adversarial Network (LAC-GAN) based on face action units\nannotations. Given desired AU labels, LAC-GAN utilizes local AU regional rules\nto control the status of each AU and attentive mechanism to combine several of\nthem into the whole photo-realistic facial expressions or arbitrary facial\nexpressions. In addition, unpaired training data is utilized in our proposed\nmethod to train the manipulation module with the corresponding AU labels, which\nlearns a mapping between a facial expression manifold. Extensive qualitative\nand quantitative evaluations are conducted on the commonly used BP4D dataset to\nverify the effectiveness of our proposed AU synthesis method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 02:43:57 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Liu", "Zhilei", ""], ["Liu", "Diyi", ""], ["Wu", "Yunpeng", ""]]}, {"id": "1910.10328", "submitter": "Jiahao Li", "authors": "Jiahao Li, Changhao Zhang, Ziyao Xu, Hangning Zhou, Chi Zhang", "title": "Iterative Distance-Aware Similarity Matrix Convolution with\n  Mutual-Supervised Point Elimination for Efficient Point Cloud Registration", "comments": null, "journal-ref": "European Conference on Computer Vision (ECCV) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel learning-based pipeline for partially\noverlapping 3D point cloud registration. The proposed model includes an\niterative distance-aware similarity matrix convolution module to incorporate\ninformation from both the feature and Euclidean space into the pairwise point\nmatching process. These convolution layers learn to match points based on joint\ninformation of the entire geometric features and Euclidean offset for each\npoint pair, overcoming the disadvantage of matching by simply taking the inner\nproduct of feature vectors. Furthermore, a two-stage learnable point\nelimination technique is presented to improve computational efficiency and\nreduce false positive correspondence pairs. A novel mutual-supervision loss is\nproposed to train the model without extra annotations of keypoints. The\npipeline can be easily integrated with both traditional (e.g. FPFH) and\nlearning-based features. Experiments on partially overlapping and noisy point\ncloud registration show that our method outperforms the current\nstate-of-the-art, while being more computationally efficient. Code is publicly\navailable at https://github.com/jiahaowork/idam.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 03:26:49 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 17:45:48 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 03:08:34 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Li", "Jiahao", ""], ["Zhang", "Changhao", ""], ["Xu", "Ziyao", ""], ["Zhou", "Hangning", ""], ["Zhang", "Chi", ""]]}, {"id": "1910.10330", "submitter": "Yuyang Xue", "authors": "Shaojin Cai, Yuyang Xue3 Qinquan Gao, Min Du, Gang Chen, Hejun Zhang,\n  and Tong Tong", "title": "Stain Style Transfer using Transitive Adversarial Networks", "comments": "MICCAI 2019 MLMIR Workshop, Oral Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitized pathological diagnosis has been in increasing demand recently. It\nis well known that color information is critical to the automatic and visual\nanalysis of pathological slides. However, the color variations due to various\nfactors not only have negative impact on pathologist's diagnosis, but also will\nreduce the robustness of the algorithms. The factors that cause the color\ndifferences are not only in the process of making the slices, but also in the\nprocess of digitization. Different strategies have been proposed to alleviate\nthe color variations. Most of such techniques rely on collecting color\nstatistics to perform color matching across images and highly dependent on a\nreference template slide. Since the pathological slides between hospitals are\nusually unpaired, these methods do not yield good matching results. In this\nwork, we propose a novel network that we refer to as Transitive Adversarial\nNetworks (TAN) to transfer the color information among slides from different\nhospitals or centers. It is not necessary for an expert to pick a\nrepresentative reference slide in the proposed TAN method. We compare the\nproposed method with the state-of-the-art methods quantitatively and\nqualitatively. Compared with the state-of-the-art methods, our method yields an\nimprovement of 0.87dB in terms of PSNR, demonstrating the effectiveness of the\nproposed TAN method in stain style transfer.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 03:29:56 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Cai", "Shaojin", ""], ["Gao", "Yuyang Xue3 Qinquan", ""], ["Du", "Min", ""], ["Chen", "Gang", ""], ["Zhang", "Hejun", ""], ["Tong", "Tong", ""]]}, {"id": "1910.10334", "submitter": "Zhilei Liu", "authors": "Zhilei Liu, Jiahui Dong, Cuicui Zhang, Longbiao Wang, Jianwu Dang", "title": "Relation Modeling with Graph Convolutional Networks for Facial Action\n  Unit Detection", "comments": "Accepted by MMM2020", "journal-ref": null, "doi": "10.1007/978-3-030-37734-2_40", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing AU detection works considering AU relationships are relying on\nprobabilistic graphical models with manually extracted features. This paper\nproposes an end-to-end deep learning framework for facial AU detection with\ngraph convolutional network (GCN) for AU relation modeling, which has not been\nexplored before. In particular, AU related regions are extracted firstly,\nlatent representations full of AU information are learned through an\nauto-encoder. Moreover, each latent representation vector is feed into GCN as a\nnode, the connection mode of GCN is determined based on the relationships of\nAUs. Finally, the assembled features updated through GCN are concatenated for\nAU detection. Extensive experiments on BP4D and DISFA benchmarks demonstrate\nthat our framework significantly outperforms the state-of-the-art methods for\nfacial AU detection. The proposed framework is also validated through a series\nof ablation studies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 03:57:52 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Liu", "Zhilei", ""], ["Dong", "Jiahui", ""], ["Zhang", "Cuicui", ""], ["Wang", "Longbiao", ""], ["Dang", "Jianwu", ""]]}, {"id": "1910.10344", "submitter": "Zhilei Liu", "authors": "Zhilei Liu, Le Li, Yunpeng Wu, Cuicui Zhang", "title": "Facial Expression Restoration Based on Improved Graph Convolutional\n  Networks", "comments": "Accepted by MMM2020", "journal-ref": null, "doi": "10.1007/978-3-030-37734-2_43", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression analysis in the wild is challenging when the facial image\nis with low resolution or partial occlusion. Considering the correlations among\ndifferent facial local regions under different facial expressions, this paper\nproposes a novel facial expression restoration method based on generative\nadversarial network by integrating an improved graph convolutional network\n(IGCN) and region relation modeling block (RRMB). Unlike conventional graph\nconvolutional networks taking vectors as input features, IGCN can use tensors\nof face patches as inputs. It is better to retain the structure information of\nface patches. The proposed RRMB is designed to address facial generative tasks\nincluding inpainting and super-resolution with facial action units detection,\nwhich aims to restore facial expression as the ground-truth. Extensive\nexperiments conducted on BP4D and DISFA benchmarks demonstrate the\neffectiveness of our proposed method through quantitative and qualitative\nevaluations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 04:27:40 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Liu", "Zhilei", ""], ["Li", "Le", ""], ["Wu", "Yunpeng", ""], ["Zhang", "Cuicui", ""]]}, {"id": "1910.10345", "submitter": "Yu Tian", "authors": "Yuyuan Liu, Yu Tian, Gabriel Maicas, Leonardo Z.C.T. Pu, Rajvinder\n  Singh, Johan W. Verjans, Gustavo Carneiro", "title": "Unsupervised Dual Adversarial Learning for Anomaly Detection in\n  Colonoscopy Video Frames", "comments": "Accepted by ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic detection of frames containing polyps from a colonoscopy video\nsequence is an important first step for a fully automated colonoscopy analysis\ntool. Typically, such detection system is built using a large annotated data\nset of frames with and without polyps, which is expensive to be obtained. In\nthis paper, we introduce a new system that detects frames containing polyps as\nanomalies from a distribution of frames from exams that do not contain any\npolyps. The system is trained using a one-class training set consisting of\ncolonoscopy frames without polyps -- such training set is considerably less\nexpensive to obtain, compared to the 2-class data set mentioned above. During\ninference, the system is only able to reconstruct frames without polyps, and\nwhen it tries to reconstruct a frame with polyp, it automatically removes\n(i.e., photoshop) it from the frame -- the difference between the input and\nreconstructed frames is used to detect frames with polyps. We name our proposed\nmodel as anomaly detection generative adversarial network (ADGAN), comprising a\ndual GAN with two generators and two discriminators. We show that our proposed\napproach achieves the state-of-the-art result on this data set, compared with\nrecently proposed anomaly detection systems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 04:30:19 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 10:42:14 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Liu", "Yuyuan", ""], ["Tian", "Yu", ""], ["Maicas", "Gabriel", ""], ["Pu", "Leonardo Z. C. T.", ""], ["Singh", "Rajvinder", ""], ["Verjans", "Johan W.", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1910.10356", "submitter": "Kartikeya Bhardwaj", "authors": "Kartikeya Bhardwaj, Naveen Suda, Radu Marculescu", "title": "EdgeAI: A Vision for Deep Learning in IoT Era", "comments": "To appear in IEEE Design and Test", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant computational requirements of deep learning present a major\nbottleneck for its large-scale adoption on hardware-constrained IoT-devices.\nHere, we envision a new paradigm called EdgeAI to address major impediments\nassociated with deploying deep networks at the edge. Specifically, we discuss\nthe existing directions in computation-aware deep learning and describe two new\nchallenges in the IoT era: (1) Data-independent deployment of learning, and (2)\nCommunication-aware distributed inference. We further present new directions\nfrom our recent research to alleviate the latter two challenges. Overcoming\nthese challenges is crucial for rapid adoption of learning on IoT-devices in\norder to truly enable EdgeAI.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:16:32 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Bhardwaj", "Kartikeya", ""], ["Suda", "Naveen", ""], ["Marculescu", "Radu", ""]]}, {"id": "1910.10369", "submitter": "Oluwafemi Azeez", "authors": "Azeez Oluwafemi, Yang Zou, B.V.K. Vijaya Kumar", "title": "Deep Classification Network for Monocular Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular Depth Estimation is usually treated as a supervised and regression\nproblem when it actually is very similar to semantic segmentation task since\nthey both are fundamentally pixel-level classification tasks. We applied depth\nincrements that increases with depth in discretizing depth values and then\napplied Deeplab v2 and the result was higher accuracy. We were able to achieve\na state-of-the-art result on the KITTI dataset and outperformed existing\narchitecture by an 8% margin.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:50:04 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Oluwafemi", "Azeez", ""], ["Zou", "Yang", ""], ["Kumar", "B. V. K. Vijaya", ""]]}, {"id": "1910.10371", "submitter": "Gabriel Maicas", "authors": "Saskia Glaser, Gabriel Maicas, Sergei Bedrikovetski, Tarik Sammour,\n  Gustavo Carneiro", "title": "Semi-supervised Multi-domain Multi-task Training for Metastatic Colon\n  Lymph Node Diagnosis From Abdominal CT", "comments": "Under review at ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diagnosis of the presence of metastatic lymph nodes from abdominal\ncomputed tomography (CT) scans is an essential task performed by radiologists\nto guide radiation and chemotherapy treatment. State-of-the-art deep learning\nclassifiers trained for this task usually rely on a training set containing CT\nvolumes and their respective image-level (i.e., global) annotation. However,\nthe lack of annotations for the localisation of the regions of interest (ROIs)\ncontaining lymph nodes can limit classification accuracy due to the small size\nof the relevant ROIs in this problem. The use of lymph node ROIs together with\nglobal annotations in a multi-task training process has the potential to\nimprove classification accuracy, but the high cost involved in obtaining the\nROI annotation for the same samples that have global annotations is a roadblock\nfor this alternative. We address this limitation by introducing a new training\nstrategy from two data sets: one containing the global annotations, and another\n(publicly available) containing only the lymph node ROI localisation. We term\nour new strategy semi-supervised multi-domain multi-task training, where the\ngoal is to improve the diagnosis accuracy on the globally annotated data set by\nincorporating the ROI annotations from a different domain. Using a private data\nset containing global annotations and a public data set containing lymph node\nROI localisation, we show that our proposed training mechanism improves the\narea under the ROC curve for the classification task compared to several\ntraining method baselines.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:54:08 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Glaser", "Saskia", ""], ["Maicas", "Gabriel", ""], ["Bedrikovetski", "Sergei", ""], ["Sammour", "Tarik", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1910.10397", "submitter": "Heung-Chang Lee", "authors": "Heung-Chang Lee, Do-Guk Kim, Bohyung Han", "title": "Efficient Decoupled Neural Architecture Search by Structure and\n  Operation Sampling", "comments": null, "journal-ref": "IEEE ICASSP 2020", "doi": "10.1109/ICASSP40776.2020.9053197", "report-no": "9053197", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural architecture search algorithm via reinforcement\nlearning by decoupling structure and operation search processes. Our approach\nsamples candidate models from the multinomial distribution on the policy\nvectors defined on the two search spaces independently. The proposed technique\nimproves the efficiency of architecture search process significantly compared\nto the conventional methods based on reinforcement learning with the RNN\ncontrollers while achieving competitive accuracy and model size in target\ntasks. Our policy vectors are easily interpretable throughout the training\nprocedure, which allows to analyze the search progress and the discovered\narchitectures; the black-box characteristics of the RNN controllers hamper\nunderstanding training progress in terms of policy parameter updates. Our\nexperiments demonstrate outstanding performance compared to the\nstate-of-the-art methods with a fraction of search cost.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:00:22 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lee", "Heung-Chang", ""], ["Kim", "Do-Guk", ""], ["Han", "Bohyung", ""]]}, {"id": "1910.10398", "submitter": "Christoph Angermann", "authors": "Christoph Angermann and Markus Haltmeier", "title": "Random 2.5D U-net for Fully 3D Segmentation", "comments": "Submission for joint MICCAI-Workshops on Computing and Visualization\n  for Intravascular Imaging and Computer Assisted Stenting (CVII-STENT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are state-of-the-art for various segmentation\ntasks. While for 2D images these networks are also computationally efficient,\n3D convolutions have huge storage requirements and therefore, end-to-end\ntraining is limited by GPU memory and data size. To overcome this issue, we\nintroduce a network structure for volumetric data without 3D convolution\nlayers. The main idea is to include projections from different directions to\ntransform the volumetric data to a sequence of images, where each image\ncontains information of the full data. We then apply 2D convolutions to these\nprojection images and lift them again to volumetric data using a trainable\nreconstruction algorithm. The proposed architecture can be applied end-to-end\nto very large data volumes without cropping or sliding-window techniques. For a\ntested sparse binary segmentation task, it outperforms already known standard\napproaches and is more resistant to generation of artefacts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:02:09 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Angermann", "Christoph", ""], ["Haltmeier", "Markus", ""]]}, {"id": "1910.10404", "submitter": "Firas Laakom", "authors": "Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, Jarno Nikkanen\n  and Moncef Gabbouj", "title": "INTEL-TAU: A Color Constancy Dataset", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a new large dataset for illumination estimation.\nThis dataset, called INTEL-TAU, contains 7022 images in total, which makes it\nthe largest available high-resolution dataset for illumination estimation\nresearch. The variety of scenes captured using three different camera models,\nnamely Canon 5DSR, Nikon D810, and Sony IMX135, makes the dataset appropriate\nfor evaluating the camera and scene invariance of the different illumination\nestimation techniques. Privacy masking is done for sensitive information, e.g.,\nfaces. Thus, the dataset is coherent with the new General Data Protection\nRegulation (GDPR). Furthermore, the effect of color shading for mobile images\ncan be evaluated with INTEL-TAU dataset, as both corrected and uncorrected\nversions of the raw data are provided. Furthermore, this paper benchmarks\nseveral color constancy approaches on the proposed dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:21:51 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 14:26:40 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 08:50:19 GMT"}, {"version": "v4", "created": "Wed, 6 May 2020 11:13:04 GMT"}, {"version": "v5", "created": "Wed, 23 Dec 2020 14:40:01 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Laakom", "Firas", ""], ["Raitoharju", "Jenni", ""], ["Iosifidis", "Alexandros", ""], ["Nikkanen", "Jarno", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1910.10414", "submitter": "Chenglang Yuan", "authors": "Chenglang Yuan, Cheng Bian, Hongjian Kang, Shu Liang, Kai Ma, and\n  Yefeng Zheng", "title": "Identification of primary angle-closure on AS-OCT images with\n  Convolutional Neural Networks", "comments": "The third place in angle-closure glaucoma evaluation (AGE) Challenge,\n  MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primary angle-closure disease (PACD) is a severe retinal disease, which might\ncause irreversible vision loss. In clinic, accurate identification of\nangle-closure and localization of the scleral spur's position on anterior\nsegment optical coherence tomography (AS-OCT) is essential for the diagnosis of\nPACD. However, manual delineation might confine in low accuracy and low\nefficiency. In this paper, we propose an efficient and accurate end-to-end\narchitecture for angle-closure classification and scleral spur localization.\nSpecifically, we utilize a revised ResNet152 as our backbone to improve the\naccuracy of the angle-closure identification. For scleral spur localization, we\nadopt EfficientNet as encoder because of its powerful feature extraction\npotential. By combining the skip-connect module and pyramid pooling module, the\nnetwork is able to collect semantic cues in feature maps from multiple\ndimensions and scales. Afterward, we propose a novel keypoint registration loss\nto constrain the model's attention to the intensity and location of the scleral\nspur area. Several experiments are extensively conducted to evaluate our method\non the angle-closure glaucoma evaluation (AGE) Challenge dataset. The results\nshow that our proposed architecture ranks the first place of the classification\ntask on the test dataset and achieves the average Euclidean distance error of\n12.00 pixels in the scleral spur localization task.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:47:29 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Yuan", "Chenglang", ""], ["Bian", "Cheng", ""], ["Kang", "Hongjian", ""], ["Liang", "Shu", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1910.10455", "submitter": "Zhiwu Huang", "authors": "Zhiwu Huang, Danda Pani Paudel, Guanju Li, Jiqing Wu, Radu Timofte,\n  Luc Van Gool", "title": "Divide-and-Conquer Adversarial Learning for High-Resolution Image and\n  Video Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a divide-and-conquer inspired adversarial learning\n(DACAL) approach for photo enhancement. The key idea is to decompose the photo\nenhancement process into hierarchically multiple sub-problems, which can be\nbetter conquered from bottom to up. On the top level, we propose a\nperception-based division to learn additive and multiplicative components,\nrequired to translate a low-quality image or video into its high-quality\ncounterpart. On the intermediate level, we use a frequency-based division with\ngenerative adversarial network (GAN) to weakly supervise the photo enhancement\nprocess. On the lower level, we design a dimension-based division that enables\nthe GAN model to better approximates the distribution distance on multiple\nindependent one-dimensional data to train the GAN model. While considering all\nthree hierarchies, we develop multiscale and recurrent training approaches to\noptimize the image and video enhancement process in a weakly-supervised manner.\nBoth quantitative and qualitative results clearly demonstrate that the proposed\nDACAL achieves the state-of-the-art performance for high-resolution image and\nvideo enhancement.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 11:00:51 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Huang", "Zhiwu", ""], ["Paudel", "Danda Pani", ""], ["Li", "Guanju", ""], ["Wu", "Jiqing", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1910.10467", "submitter": "Jeffrey Ede BSc MPhys", "authors": "Jeffrey M. Ede", "title": "Deep Learning Supersampled Scanning Transmission Electron Microscopy", "comments": "19 pages, 21 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cond-mat.mtrl-sci cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compressed sensing can increase resolution, and decrease electron dose and\nscan time of electron microscope point-scan systems with minimal information\nloss. Building on a history of successful deep learning applications in\ncompressed sensing, we have developed a two-stage multiscale generative\nadversarial network to supersample scanning transmission electron micrographs\nwith point-scan coverage reduced to 1/16, 1/25, ..., 1/100 px. We propose a\nnovel non-adversarial learning policy to train a unified generator for multiple\ncoverages and introduce an auxiliary network to homogenize prioritization of\ntraining data with varied signal-to-noise ratios. This achieves root mean\nsquare errors of 3.23% and 4.54% at 1/16 px and 1/100 px coverage,\nrespectively; within 1% of errors for networks trained for each coverage\nindividually. Detailed error distributions are presented for unified and\nindividual coverage generators, including errors per output pixel. In addition,\nwe present a baseline one-stage network for a single coverage and investigate\nnumerical precision for web serving. Source code, training data, and pretrained\nmodels are publicly available at https://github.com/Jeffrey-Ede/DLSS-STEM\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 11:30:25 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 09:39:05 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Ede", "Jeffrey M.", ""]]}, {"id": "1910.10469", "submitter": "Alexander Schaefer", "authors": "Alexander Schaefer, Lukas Luft, Wolfram Burgard", "title": "An Analytical Lidar Sensor Model Based on Ray Path Information", "comments": "8 pages", "journal-ref": "IEEE Robotics and Automation Letters (Volume: 2, Issue: 3, July\n  2017)", "doi": "10.1109/LRA.2017.2669376", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two core competencies of a mobile robot are to build a map of the environment\nand to estimate its own pose on the basis of this map and incoming sensor\nreadings. To account for the uncertainties in this process, one typically\nemploys probabilistic state estimation approaches combined with a model of the\nspecific sensor. Over the past years, lidar sensors have become a popular\nchoice for mapping and localization. However, many common lidar models perform\npoorly in unstructured, unpredictable environments, they lack a consistent\nphysical model for both mapping and localization, and they do not exploit all\nthe information the sensor provides, e.g. out-of-range measurements. In this\npaper, we introduce a consistent physical model that can be applied to mapping\nas well as to localization. It naturally deals with unstructured environments\nand makes use of both out-of-range measurements and information about the ray\npath. The approach can be seen as a generalization of the well-established\nreflection model, but in addition to counting ray reflections and traversals in\na specific map cell, it considers the distances that all rays travel inside\nthis cell. We prove that the resulting map maximizes the data likelihood and\ndemonstrate that our model outperforms state-of-the-art sensor models in\nextensive real-world experiments.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 11:32:22 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Schaefer", "Alexander", ""], ["Luft", "Lukas", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1910.10470", "submitter": "Hans Pinckaers", "authors": "Hans Pinckaers, Geert Litjens", "title": "Neural Ordinary Differential Equations for Semantic Segmentation of\n  Individual Colon Glands", "comments": "Accepted to 'Medical Imaging meets NeurIPS' workshop at NeurIPS 2019.\n  Source code available at:\n  https://github.com/DIAGNijmegen/neural-odes-segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated medical image segmentation plays a key role in quantitative\nresearch and diagnostics. Convolutional neural networks based on the U-Net\narchitecture are the state-of-the-art. A key disadvantage is the hard-coding of\nthe receptive field size, which requires architecture optimization for each\nsegmentation task. Furthermore, increasing the receptive field results in an\nincreasing number of weights. Recently, Neural Ordinary Differential Equations\n(NODE) have been proposed, a new type of continuous depth deep neural network.\nThis framework allows for a dynamic receptive field at a fixed memory cost and\na smaller amount of parameters. We show on a colon gland segmentation dataset\n(GlaS) that these NODEs can be used within the U-Net framework to improve\nsegmentation results while reducing memory load and parameter counts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 11:35:19 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Pinckaers", "Hans", ""], ["Litjens", "Geert", ""]]}, {"id": "1910.10493", "submitter": "Alexander Schaefer", "authors": "Lukas Luft, Alexander Schaefer, Tobias Schubert, Wolfram Burgard", "title": "Closed-Form Full Map Posteriors for Robot Localization with Lidar\n  Sensors", "comments": "7 pages", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems, Vancouver, BC, 2017, pp. 6678-6684", "doi": "10.1109/IROS.2017.8206583", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular class of lidar-based grid mapping algorithms computes for each map\ncell the probability that it reflects an incident laser beam. These algorithms\ntypically determine the map as the set of reflection probabilities that\nmaximizes the likelihood of the underlying laser data and do not compute the\nfull posterior distribution over all possible maps. Thereby, they discard\ncrucial information about the confidence of the estimate. The approach\npresented in this paper preserves this information by determining the full map\nposterior. In general, this problem is hard because distributions over\nreal-valued quantities can possess infinitely many dimensions. However, for two\nstate-of-the-art beam-based lidar models, our approach yields closed-form map\nposteriors that possess only two parameters per cell. Even better, these\nposteriors come for free, in the sense that they use the same parameters as the\ntraditional approaches, without the need for additional computations. An\nimportant use case for grid maps is robot localization, which we formulate as\nBayesian filtering based on the closed-form map posterior rather than based on\na single map. The resulting measurement likelihoods can also be expressed in\nclosed form. In simulations and extensive real-world experiments, we show that\nleveraging the full map posterior improves the localization accuracy compared\nto approaches that use the most likely map.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 11:53:06 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Luft", "Lukas", ""], ["Schaefer", "Alexander", ""], ["Schubert", "Tobias", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1910.10534", "submitter": "Beril Sirmacek", "authors": "Beril Sirmacek, Max Kivits", "title": "Semantic Segmentation of Skin Lesions using a Small Data Set", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of melanoma is difficult for the human eye but a crucial step\ntowards reducing its death rate. Computerized detection of these melanoma and\nother skin lesions is necessary. The central research question in this paper is\n\"How to segment skin lesion images using a neural network with low available\ndata?\". This question is divided into three sub questions regarding best\nperforming network structure, training data and training method. First theory\nassociated with these questions is discussed. Literature states that U-net CNN\nstructures have excellent performances on the segmentation task, more training\ndata increases network performance and utilizing transfer learning enables\nnetworks to generalize to new data better.\n  To validate these findings in the literature two experiments are conducted.\nThe first experiment trains a network on data sets of different size. The\nsecond experiment proposes twelve network structures and trains them on the\nsame data set. The experimental results support the findings in the literature.\nThe FCN16 and FCN32 networks perform best in the accuracy, intersection over\nunion and mean BF1 Score metric. Concluding from these results the skin lesion\nsegmentation network is a fully convolutional structure with a skip\narchitecture and an encoder depth of either one or two. Weights of this network\nshould be initialized using transfer learning from the pre trained VGG16\nnetwork. Training data should be cropped to reduce complexity and augmented\nduring training to reduce the likelihood of overfitting.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 12:54:35 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Sirmacek", "Beril", ""], ["Kivits", "Max", ""]]}, {"id": "1910.10542", "submitter": "Kibrom Berihu Girum Mr.", "authors": "Kibrom Berihu Girum, Gilles Cr\\'ehange, Raabid Hussain, Paul Michael\n  Walker, Alain Lalande", "title": "Deep generative model-driven multimodal prostate segmentation in\n  radiotherapy", "comments": "8 pages, camera ready paper, accepted for Artificial Intelligence in\n  Radiation Therapy (AIRT), in conjunction with MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32486-5_15", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown unprecedented success in a variety of applications,\nsuch as computer vision and medical image analysis. However, there is still\npotential to improve segmentation in multimodal images by embedding prior\nknowledge via learning-based shape modeling and registration to learn the\nmodality invariant anatomical structure of organs. For example, in radiotherapy\nautomatic prostate segmentation is essential in prostate cancer diagnosis,\ntherapy, and post-therapy assessment from T2-weighted MR or CT images. In this\npaper, we present a fully automatic deep generative model-driven multimodal\nprostate segmentation method using convolutional neural network (DGMNet). The\nnovelty of our method comes with its embedded generative neural network for\nlearning-based shape modeling and its ability to adapt for different imaging\nmodalities via learning-based registration. The proposed method includes a\nmulti-task learning framework that combines a convolutional feature extraction\nand an embedded regression and classification based shape modeling. This\nenables the network to predict the deformable shape of an organ. We show that\ngenerative neural networkbased shape modeling trained on a reliable contrast\nimaging modality (such as MRI) can be directly applied to low contrast imaging\nmodality (such as CT) to achieve accurate prostate segmentation. The method was\nevaluated on MRI and CT datasets acquired from different clinical centers with\nlarge variations in contrast and scanning protocols. Experimental results\nreveal that our method can be used to automatically and accurately segment the\nprostate gland in different imaging modalities.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 13:08:12 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Girum", "Kibrom Berihu", ""], ["Cr\u00e9hange", "Gilles", ""], ["Hussain", "Raabid", ""], ["Walker", "Paul Michael", ""], ["Lalande", "Alain", ""]]}, {"id": "1910.10549", "submitter": "Jiahao Lu", "authors": "Jiahao Lu, Nata\\v{s}a Sladoje, Christina Runow Stark, Eva Darai\n  Ramqvist, Jan-Micha\\'el Hirsch, Joakim Lindblad", "title": "A Deep Learning based Pipeline for Efficient Oral Cancer Screening on\n  Whole Slide Images", "comments": "Accepted to ICIAR 2020", "journal-ref": "In Proceedings of the 17th International Conference on Image\n  Analysis and Recognition (ICIAR), LNCS-12132, pp. 249-261, P\\'ovoa de Varzim,\n  Portugal, June 2020", "doi": "10.1007/978-3-030-50516-5_22", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oral cancer incidence is rapidly increasing worldwide. The most important\ndeterminant factor in cancer survival is early diagnosis. To facilitate large\nscale screening, we propose a fully automated pipeline for oral cancer\ndetection on whole slide cytology images. The pipeline consists of fully\nconvolutional regression-based nucleus detection, followed by per-cell focus\nselection, and CNN based classification. Our novel focus selection step\nprovides fast per-cell focus decisions at human-level accuracy. We demonstrate\nthat the pipeline provides efficient cancer classification of whole slide\ncytology images, improving over previous results both in terms of accuracy and\nfeasibility. The complete source code is available at\nhttps://github.com/MIDA-group/OralScreen.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 13:16:14 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 18:25:47 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 10:54:55 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Lu", "Jiahao", ""], ["Sladoje", "Nata\u0161a", ""], ["Stark", "Christina Runow", ""], ["Ramqvist", "Eva Darai", ""], ["Hirsch", "Jan-Micha\u00e9l", ""], ["Lindblad", "Joakim", ""]]}, {"id": "1910.10550", "submitter": "Alexander Schaefer", "authors": "Alexander Schaefer, Daniel B\\\"uscher, Johan Vertens, Lukas Luft,\n  Wolfram Burgard", "title": "Long-Term Urban Vehicle Localization Using Pole Landmarks Extracted from\n  3-D Lidar Scans", "comments": "9 pages", "journal-ref": "European Conference on Mobile Robots, Prague, Czech Republic,\n  2019, pp. 1-7", "doi": "10.1109/ECMR.2019.8870928", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their ubiquity and long-term stability, pole-like objects are well\nsuited to serve as landmarks for vehicle localization in urban environments. In\nthis work, we present a complete mapping and long-term localization system\nbased on pole landmarks extracted from 3-D lidar data. Our approach features a\nnovel pole detector, a mapping module, and an online localization module, each\nof which are described in detail, and for which we provide an open-source\nimplementation at www.github.com/acschaefer/polex. In extensive experiments, we\ndemonstrate that our method improves on the state of the art with respect to\nlong-term reliability and accuracy: First, we prove reliability by tasking the\nsystem with localizing a mobile robot over the course of 15~months in an urban\narea based on an initial map, confronting it with constantly varying routes,\ndiffering weather conditions, seasonal changes, and construction sites. Second,\nwe show that the proposed approach clearly outperforms a recently published\nmethod in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 13:21:05 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Schaefer", "Alexander", ""], ["B\u00fcscher", "Daniel", ""], ["Vertens", "Johan", ""], ["Luft", "Lukas", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1910.10563", "submitter": "Fabio Pizzati", "authors": "Fabio Pizzati, Raoul de Charette, Michela Zaccaria, Pietro Cerri", "title": "Domain Bridge for Unpaired Image-to-Image Translation and Unsupervised\n  Domain Adaptation", "comments": "WACV 20 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-to-image translation architectures may have limited effectiveness in\nsome circumstances. For example, while generating rainy scenarios, they may\nfail to model typical traits of rain as water drops, and this ultimately\nimpacts the synthetic images realism. With our method, called domain bridge,\nweb-crawled data are exploited to reduce the domain gap, leading to the\ninclusion of previously ignored elements in the generated images. We make use\nof a network for clear to rain translation trained with the domain bridge to\nextend our work to Unsupervised Domain Adaptation (UDA). In that context, we\nintroduce an online multimodal style-sampling strategy, where image translation\nmultimodality is exploited at training time to improve performances. Finally, a\nnovel approach for self-supervised learning is presented, and used to further\nalign the domains. With our contributions, we simultaneously increase the\nrealism of the generated images, while reaching on par performances with\nrespect to the UDA state-of-the-art, with a simpler approach.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 13:51:40 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 12:40:19 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2020 16:18:04 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Pizzati", "Fabio", ""], ["de Charette", "Raoul", ""], ["Zaccaria", "Michela", ""], ["Cerri", "Pietro", ""]]}, {"id": "1910.10603", "submitter": "Zhuoqing Chang", "authors": "Zhuoqing Chang, Matias Di Martino, Qiang Qiu, Steven Espinosa, and\n  Guillermo Sapiro", "title": "SalGaze: Personalizing Gaze Estimation Using Visual Saliency", "comments": "Accepted by ICCV 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional gaze estimation methods typically require explicit user\ncalibration to achieve high accuracy. This process is cumbersome and\nrecalibration is often required when there are changes in factors such as\nillumination and pose. To address this challenge, we introduce SalGaze, a\nframework that utilizes saliency information in the visual content to\ntransparently adapt the gaze estimation algorithm to the user without explicit\nuser calibration. We design an algorithm to transform a saliency map into a\ndifferentiable loss map that can be used for the optimization of CNN-based\nmodels. SalGaze is also able to greatly augment standard point calibration data\nwith implicit video saliency calibration data using a unified framework. We\nshow accuracy improvements over 24% using our technique on existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 15:11:08 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Chang", "Zhuoqing", ""], ["Di Martino", "Matias", ""], ["Qiu", "Qiang", ""], ["Espinosa", "Steven", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1910.10651", "submitter": "Ruth Fong", "authors": "Ruth Fong, Andrea Vedaldi", "title": "Occlusions for Effective Data Augmentation in Image Classification", "comments": "Accepted to 2019 ICCV Workshop on Interpreting and Explaining Visual\n  Artificial Intelligence Models (v2: corrected references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks for visual recognition are known to leverage \"easy to\nrecognise\" portions of objects such as faces and distinctive texture patterns.\nThe lack of a holistic understanding of objects may increase fragility and\noverfitting. In recent years, several papers have proposed to address this\nissue by means of occlusions as a form of data augmentation. However, successes\nhave been limited to tasks such as weak localization and model interpretation,\nbut no benefit was demonstrated on image classification on large-scale\ndatasets. In this paper, we show that, by using a simple technique based on\nbatch augmentation, occlusions as data augmentation can result in better\nperformance on ImageNet for high-capacity models (e.g., ResNet50). We also show\nthat varying amounts of occlusions used during training can be used to study\nthe robustness of different neural network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 16:19:22 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 15:25:57 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Fong", "Ruth", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1910.10652", "submitter": "Fei Xu", "authors": "Fei Xu, Yingtao Zhang, Min Xian, H. D. Cheng, Boyu Zhang, Jianrui\n  Ding, Chunping Ning, Ying Wang", "title": "Breast Anatomy Enriched Tumor Saliency Estimation", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer investigation is of great significance, and developing tumor\ndetection methodologies is a critical need. However, it is a challenging task\nfor breast ultrasound due to the complicated breast structure and poor quality\nof the images. In this paper, we propose a novel tumor saliency estimation\nmodel guided by enriched breast anatomy knowledge to localize the tumor.\nFirstly, the breast anatomy layers are generated by a deep neural network. Then\nwe refine the layers by integrating a non-semantic breast anatomy model to\nsolve the problems of incomplete mammary layers. Meanwhile, a new background\nmap generation method weighted by the semantic probability and spatial distance\nis proposed to improve the performance. The experiment demonstrates that the\nproposed method with the new background map outperforms four state-of-the-art\nTSE models with increasing 10% of F_meansure on the BUS public dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 16:19:26 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Xu", "Fei", ""], ["Zhang", "Yingtao", ""], ["Xian", "Min", ""], ["Cheng", "H. D.", ""], ["Zhang", "Boyu", ""], ["Ding", "Jianrui", ""], ["Ning", "Chunping", ""], ["Wang", "Ying", ""]]}, {"id": "1910.10653", "submitter": "Pedro Castro", "authors": "Pedro Castro, Anil Armagan, Tae-Kyun Kim", "title": "Accurate 6D Object Pose Estimation by Pose Conditioned Mesh\n  Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053627", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current 6D object pose methods consist of deep CNN models fully optimized for\na single object but with its architecture standardized among objects with\ndifferent shapes. In contrast to previous works, we explicitly exploit each\nobject's distinct topological information i.e. 3D dense meshes in the pose\nestimation model, with an automated process and prior to any post-processing\nrefinement stage. In order to achieve this, we propose a learning framework in\nwhich a Graph Convolutional Neural Network reconstructs a pose conditioned 3D\nmesh of the object. A robust estimation of the allocentric orientation is\nrecovered by computing, in a differentiable manner, the Procrustes' alignment\nbetween the canonical and reconstructed dense 3D meshes. 6D egocentric pose is\nthen lifted using additional mask and 2D centroid projection estimations. Our\nmethod is capable of self validating its pose estimation by measuring the\nquality of the reconstructed mesh, which is invaluable in real life\napplications. In our experiments on the LINEMOD, OCCLUSION and YCB-Video\nbenchmarks, the proposed method outperforms state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 16:23:12 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Castro", "Pedro", ""], ["Armagan", "Anil", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1910.10672", "submitter": "Krishna Murthy Jatavallabhula", "authors": "Krishna Murthy Jatavallabhula, Soroush Saryazdi, Ganesh Iyer, Liam\n  Paull", "title": "gradSLAM: Automagically differentiable SLAM", "comments": "Video: https://youtu.be/2ygtSJTmo08 . Project page and code:\n  https://gradslam.github.io This tech report is an extended version of the\n  ICRA 2020 paper \"gradSLAM: Dense SLAM meets automatic differentiation\". The\n  first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blending representation learning approaches with simultaneous localization\nand mapping (SLAM) systems is an open question, because of their highly modular\nand complex nature. Functionally, SLAM is an operation that transforms raw\nsensor inputs into a distribution over the state(s) of the robot and the\nenvironment. If this transformation (SLAM) were expressible as a differentiable\nfunction, we could leverage task-based error signals to learn representations\nthat optimize task performance. However, several components of a typical dense\nSLAM system are non-differentiable. In this work, we propose gradSLAM, a\nmethodology for posing SLAM systems as differentiable computational graphs,\nwhich unifies gradient-based learning and SLAM. We propose differentiable\ntrust-region optimizers, surface measurement and fusion schemes, and\nraycasting, without sacrificing accuracy. This amalgamation of dense SLAM with\ncomputational graphs enables us to backprop all the way from 3D maps to 2D\npixels, opening up new possibilities in gradient-based learning for SLAM.\n  TL;DR: We leverage the power of automatic differentiation frameworks to make\ndense SLAM differentiable.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:13:41 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 18:55:05 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 18:53:59 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Jatavallabhula", "Krishna Murthy", ""], ["Saryazdi", "Soroush", ""], ["Iyer", "Ganesh", ""], ["Paull", "Liam", ""]]}, {"id": "1910.10679", "submitter": "Leslie Smith", "authors": "Leslie N. Smith", "title": "A Useful Taxonomy for Adversarial Robustness of Neural Networks", "comments": "NRL Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks and defenses are currently active areas of research for\nthe deep learning community. A recent review paper divided the defense\napproaches into three categories; gradient masking, robust optimization, and\nadversarial example detection. We divide gradient masking and robust\noptimization differently: (1) increasing intra-class compactness and\ninter-class separation of the feature vectors improves adversarial robustness,\nand (2) marginalization or removal of non-robust image features also improves\nadversarial robustness. By reframing these topics differently, we provide a\nfresh perspective that provides insight into the underlying factors that enable\ntraining more robust networks and can help inspire novel solutions. In\naddition, there are several papers in the literature of adversarial defenses\nthat claim there is a cost for adversarial robustness, or a trade-off between\nrobustness and accuracy but, under this proposed taxonomy, we hypothesis that\nthis is not universal. We follow up on our taxonomy with several challenges to\nthe deep learning research community that builds on the connections and\ninsights in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:33:15 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Smith", "Leslie N.", ""]]}, {"id": "1910.10699", "submitter": "Yonglong Tian", "authors": "Yonglong Tian, Dilip Krishnan, Phillip Isola", "title": "Contrastive Representation Distillation", "comments": "ICLR 2020. Project Page: http://hobbitlong.github.io/CRD/, Code:\n  http://github.com/HobbitLong/RepDistiller", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Often we wish to transfer representational knowledge from one neural network\nto another. Examples include distilling a large network into a smaller one,\ntransferring knowledge from one sensory modality to a second, or ensembling a\ncollection of models into a single estimator. Knowledge distillation, the\nstandard approach to these problems, minimizes the KL divergence between the\nprobabilistic outputs of a teacher and student network. We demonstrate that\nthis objective ignores important structural knowledge of the teacher network.\nThis motivates an alternative objective by which we train a student to capture\nsignificantly more information in the teacher's representation of the data. We\nformulate this objective as contrastive learning. Experiments demonstrate that\nour resulting new objective outperforms knowledge distillation and other\ncutting-edge distillers on a variety of knowledge transfer tasks, including\nsingle model compression, ensemble distillation, and cross-modal transfer. Our\nmethod sets a new state-of-the-art in many transfer tasks, and sometimes even\noutperforms the teacher network when combined with knowledge distillation.\nCode: http://github.com/HobbitLong/RepDistiller.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:59:18 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 10:09:39 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Tian", "Yonglong", ""], ["Krishnan", "Dilip", ""], ["Isola", "Phillip", ""]]}, {"id": "1910.10706", "submitter": "Noa Garcia", "authors": "Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima", "title": "KnowIT VQA: Answering Knowledge-Based Questions about Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel video understanding task by fusing knowledge-based and\nvideo question answering. First, we introduce KnowIT VQA, a video dataset with\n24,282 human-generated question-answer pairs about a popular sitcom. The\ndataset combines visual, textual and temporal coherence reasoning together with\nknowledge-based questions, which need of the experience obtained from the\nviewing of the series to be answered. Second, we propose a video understanding\nmodel by combining the visual and textual video content with specific knowledge\nabout the show. Our main findings are: (i) the incorporation of knowledge\nproduces outstanding improvements for VQA in video, and (ii) the performance on\nKnowIT VQA still lags well behind human accuracy, indicating its usefulness for\nstudying current video modelling limitations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 01:44:12 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 00:37:24 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2019 04:13:21 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Garcia", "Noa", ""], ["Otani", "Mayu", ""], ["Chu", "Chenhui", ""], ["Nakashima", "Yuta", ""]]}, {"id": "1910.10750", "submitter": "Chen Wang", "authors": "Chen Wang, Roberto Mart\\'in-Mart\\'in, Danfei Xu, Jun Lv, Cewu Lu, Li\n  Fei-Fei, Silvio Savarese, Yuke Zhu", "title": "6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present 6-PACK, a deep learning approach to category-level 6D object pose\ntracking on RGB-D data. Our method tracks in real-time novel object instances\nof known object categories such as bowls, laptops, and mugs. 6-PACK learns to\ncompactly represent an object by a handful of 3D keypoints, based on which the\ninterframe motion of an object instance can be estimated through keypoint\nmatching. These keypoints are learned end-to-end without manual supervision in\norder to be most effective for tracking. Our experiments show that our method\nsubstantially outperforms existing methods on the NOCS category-level 6D pose\nestimation benchmark and supports a physical robot to perform simple\nvision-based closed-loop manipulation tasks. Our code and video are available\nat https://sites.google.com/view/6packtracking.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 18:16:53 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Wang", "Chen", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Xu", "Danfei", ""], ["Lv", "Jun", ""], ["Lu", "Cewu", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""], ["Zhu", "Yuke", ""]]}, {"id": "1910.10793", "submitter": "Carianne Martinez", "authors": "Tyler LaBonte, Carianne Martinez, Scott A. Roberts", "title": "We Know Where We Don't Know: 3D Bayesian CNNs for Credible Geometric\n  Uncertainty", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": "SAND2020-3269 R", "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been successfully applied to the segmentation of 3D\nComputed Tomography (CT) scans. Establishing the credibility of these\nsegmentations requires uncertainty quantification (UQ) to identify\nuntrustworthy predictions. Recent UQ architectures include Monte Carlo dropout\nnetworks (MCDNs), which approximate deep Gaussian processes, and Bayesian\nneural networks (BNNs), which learn the distribution of the weight space. BNNs\nare advantageous over MCDNs for UQ but are thought to be computationally\ninfeasible in high dimension, and neither architecture has produced\ninterpretable geometric uncertainty maps. We propose a novel 3D Bayesian\nconvolutional neural network (BCNN), the first deep learning method which\ngenerates statistically credible geometric uncertainty maps and scales for\napplication to 3D data. We present experimental results on CT scans of graphite\nelectrodes and laser-welded metals and show that our BCNN outperforms an MCDN\nin recent uncertainty metrics. The geometric uncertainty maps generated by our\nBCNN capture distributions of sigmoid values that are interpretable as\nconfidence intervals, critical for applications that rely on deep learning for\nhigh-consequence decisions. Code available at\nhttps://github.com/sandialabs/bcnn.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:07:24 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 00:35:12 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["LaBonte", "Tyler", ""], ["Martinez", "Carianne", ""], ["Roberts", "Scott A.", ""]]}, {"id": "1910.10797", "submitter": "Oscar Leong", "authors": "Oscar Leong and Wesam Sakla", "title": "Low Shot Learning with Untrained Neural Networks for Imaging Inverse\n  Problems", "comments": "Deep Inverse NeurIPS 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employing deep neural networks as natural image priors to solve inverse\nproblems either requires large amounts of data to sufficiently train expressive\ngenerative models or can succeed with no data via untrained neural networks.\nHowever, very few works have considered how to interpolate between these no- to\nhigh-data regimes. In particular, how can one use the availability of a small\namount of data (even $5-25$ examples) to one's advantage in solving these\ninverse problems and can a system's performance increase as the amount of data\nincreases as well? In this work, we consider solving linear inverse problems\nwhen given a small number of examples of images that are drawn from the same\ndistribution as the image of interest. Comparing to untrained neural networks\nthat use no data, we show how one can pre-train a neural network with a few\ngiven examples to improve reconstruction results in compressed sensing and\nsemantic image recovery problems such as colorization. Our approach leads to\nimproved reconstruction as the amount of available data increases and is on par\nwith fully trained generative models, while requiring less than $1 \\%$ of the\ndata needed to train a generative model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:23:22 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Leong", "Oscar", ""], ["Sakla", "Wesam", ""]]}, {"id": "1910.10822", "submitter": "Erdem Varol", "authors": "Erdem Varol, Amin Nejatbakhsh", "title": "Wasserstein total variation filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we expand upon the theory of trend filtering by introducing\nthe use of the Wasserstein metric as a means to control the amount of\nspatiotemporal variation in filtered time series data. While trend filtering\nutilizes regularization to produce signal estimates that are piecewise linear,\nin the case of $\\ell_1$ regularization, or temporally smooth, in the case of\n$\\ell_2$ regularization, it ignores the topology of the spatial distribution of\nsignal. By incorporating the information about the underlying metric space of\nthe pixel layout, the Wasserstein metric is an attractive choice as a\nregularizer to undercover spatiotemporal trends in time series data. We\nintroduce a globally optimal algorithm for efficiently estimating the filtered\nsignal under a Wasserstein finite differences operator. The efficacy of the\nproposed algorithm in preserving spatiotemporal trends in time series video is\ndemonstrated in both simulated and fluorescent microscopy videos of the\nnematode caenorhabditis elegans and compared against standard trend filtering\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 22:03:53 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Varol", "Erdem", ""], ["Nejatbakhsh", "Amin", ""]]}, {"id": "1910.10825", "submitter": "Ming Y. Lu", "authors": "Ming Y. Lu, Richard J. Chen, Jingwen Wang, Debora Dillon and Faisal\n  Mahmood", "title": "Semi-Supervised Histology Classification using Deep Multiple Instance\n  Learning and Contrastive Predictive Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks can be trained to perform histology slide\nclassification using weak annotations with multiple instance learning (MIL).\nHowever, given the paucity of labeled histology data, direct application of MIL\ncan easily suffer from overfitting and the network is unable to learn rich\nfeature representations due to the weak supervisory signal. We propose to\novercome such limitations with a two-stage semi-supervised approach that\ncombines the power of data-efficient self-supervised feature learning via\ncontrastive predictive coding (CPC) and the interpretability and flexibility of\nregularized attention-based MIL. We apply our two-stage CPC + MIL\nsemi-supervised pipeline to the binary classification of breast cancer\nhistology images. Across five random splits, we report state-of-the-art\nperformance with a mean validation accuracy of 95% and an area under the ROC\ncurve of 0.968. We further evaluate the quality of features learned via CPC\nrelative to simple transfer learning and show that strong classification\nperformance using CPC features can be efficiently leveraged under the MIL\nframework even with the feature encoder frozen.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 22:12:57 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 19:19:43 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 16:41:26 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Lu", "Ming Y.", ""], ["Chen", "Richard J.", ""], ["Wang", "Jingwen", ""], ["Dillon", "Debora", ""], ["Mahmood", "Faisal", ""]]}, {"id": "1910.10845", "submitter": "Shuai Zhang", "authors": "Eyasu Mequanint, Shuai Zhang, Bijan Forutanpour, Yingyong Qi, Ning Bi", "title": "Weakly-Supervised Degree of Eye-Closeness Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following recent technological advances there is a growing interest in\nbuilding non-intrusive methods that help us communicate with computing devices.\nIn this regard, accurate information from eye is a promising input medium\nbetween a user and computing devices. In this paper we propose a method that\ncaptures the degree of eye closeness. Although many methods exist for detection\nof eyelid openness, they are inherently unable to satisfactorily perform in\nreal world applications. Detailed eye state estimation is more important, in\nextracting meaningful information, than estimating whether eyes are open or\nclosed. However, learning reliable eye state estimator requires accurate\nannotations which is cost prohibitive. In this work, we leverage synthetic face\nimages which can be generated via computer graphics rendering techniques and\nautomatically annotated with different levels of eye openness. These\nsynthesized training data images, however, have a domain shift from real-world\ndata. To alleviate this issue, we propose a weakly-supervised method which\nutilizes the accurate annotation from the synthetic data set, to learn accurate\ndegree of eye openness, and the weakly labeled (open or closed) real world eye\ndata set to control the domain shift. We introduce a data set of 1.3M synthetic\nface images with detail eye openness and eye gaze information, and 21k\nreal-world images with open/closed annotation. The dataset will be released\nonline upon acceptance. Extensive experiments validate the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:14:28 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Mequanint", "Eyasu", ""], ["Zhang", "Shuai", ""], ["Forutanpour", "Bijan", ""], ["Qi", "Yingyong", ""], ["Bi", "Ning", ""]]}, {"id": "1910.10853", "submitter": "Chunlei Liu", "authors": "Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu,\n  Jianzhuang Liu, Rongrong Ji, David Doermann", "title": "Circulant Binary Convolutional Networks: Enhancing the Performance of\n  1-bit DCNNs with Circulant Back Propagation", "comments": "Published in CVPR2019", "journal-ref": "]Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition. 2019: 2691-2699", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly decreasing computation and memory cost has recently driven the\nsuccess of many applications in the field of deep learning. Practical\napplications of deep learning in resource-limited hardware, such as embedded\ndevices and smart phones, however, remain challenging. For binary convolutional\nnetworks, the reason lies in the degraded representation caused by binarizing\nfull-precision filters. To address this problem, we propose new circulant\nfilters (CiFs) and a circulant binary convolution (CBConv) to enhance the\ncapacity of binarized convolutional features via our circulant back propagation\n(CBP). The CiFs can be easily incorporated into existing deep convolutional\nneural networks (DCNNs), which leads to new Circulant Binary Convolutional\nNetworks (CBCNs). Extensive experiments confirm that the performance gap\nbetween the 1-bit and full-precision DCNNs is minimized by increasing the\nfilter diversity, which further increases the representational ability in our\nnetworks. Our experiments on ImageNet show that CBCNs achieve 61.4% top-1\naccuracy with ResNet18. Compared to the state-of-the-art such as XNOR, CBCNs\ncan achieve up to 10% higher top-1 accuracy with more powerful representational\nability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:24:30 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Liu", "Chunlei", ""], ["Ding", "Wenrui", ""], ["Xia", "Xin", ""], ["Zhang", "Baochang", ""], ["Gu", "Jiaxin", ""], ["Liu", "Jianzhuang", ""], ["Ji", "Rongrong", ""], ["Doermann", "David", ""]]}, {"id": "1910.10859", "submitter": "Chunlei Liu", "authors": "Chunlei Liu, Wenrui Ding, Jinyu Yang, Vittorio Murino, Baochang Zhang,\n  Jungong Han, Guodong Guo", "title": "Aggregation Signature for Small Object Tracking", "comments": "IEEE Transactions on Image Processing, 2019", "journal-ref": null, "doi": "10.1109/TIP.2019.2940477", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small object tracking becomes an increasingly important task, which however\nhas been largely unexplored in computer vision. The great challenges stem from\nthe facts that: 1) small objects show extreme vague and variable appearances,\nand 2) they tend to be lost easier as compared to normal-sized ones due to the\nshaking of lens. In this paper, we propose a novel aggregation signature\nsuitable for small object tracking, especially aiming for the challenge of\nsudden and large drift. We make three-fold contributions in this work. First,\ntechnically, we propose a new descriptor, named aggregation signature, based on\nsaliency, able to represent highly distinctive features for small objects.\nSecond, theoretically, we prove that the proposed signature matches the\nforeground object more accurately with a high probability. Third,\nexperimentally, the aggregation signature achieves a high performance on\nmultiple datasets, outperforming the state-of-the-art methods by large margins.\nMoreover, we contribute with two newly collected benchmark datasets, i.e.,\nsmall90 and small112, for visually small object tracking. The datasets will be\navailable in https://github.com/bczhangbczhang/.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:41:13 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Liu", "Chunlei", ""], ["Ding", "Wenrui", ""], ["Yang", "Jinyu", ""], ["Murino", "Vittorio", ""], ["Zhang", "Baochang", ""], ["Han", "Jungong", ""], ["Guo", "Guodong", ""]]}, {"id": "1910.10892", "submitter": "Zhiwei Xu", "authors": "Zhiwei Xu, Thalaiyasingam Ajanthan, Richard Hartley", "title": "Fast and Differentiable Message Passing on Pairwise Markov Random Fields", "comments": "Asian Conference on Computer Vision (ACCV), 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the availability of many Markov Random Field (MRF) optimization\nalgorithms, their widespread usage is currently limited due to imperfect MRF\nmodelling arising from hand-crafted model parameters and the selection of\ninferior inference algorithm. In addition to differentiability, the two main\naspects that enable learning these model parameters are the forward and\nbackward propagation time of the MRF optimization algorithm and its inference\ncapabilities. In this work, we introduce two fast and differentiable message\npassing algorithms, namely, Iterative Semi-Global Matching Revised (ISGMR) and\nParallel Tree-Reweighted Message Passing (TRWP) which are greatly sped up on a\nGPU by exploiting massive parallelism. Specifically, ISGMR is an iterative and\nrevised version of the standard SGM for general pairwise MRFs with improved\noptimization effectiveness, and TRWP is a highly parallel version of Sequential\nTRW (TRWS) for faster optimization. Our experiments on the standard stereo and\ndenoising benchmarks demonstrated that ISGMR and TRWP achieve much lower\nenergies than SGM and Mean-Field (MF), and TRWP is two orders of magnitude\nfaster than TRWS without losing effectiveness in optimization. We further\ndemonstrated the effectiveness of our algorithms on end-to-end learning for\nsemantic segmentation. Notably, our CUDA implementations are at least $7$ and\n$700$ times faster than PyTorch GPU implementations for forward and backward\npropagation respectively, enabling efficient end-to-end learning with message\npassing.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 02:48:11 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 12:19:42 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 05:13:03 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Xu", "Zhiwei", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Hartley", "Richard", ""]]}, {"id": "1910.10895", "submitter": "Qiang Wang", "authors": "Zhao Yang, Qiang Wang, Luca Bertinetto, Weiming Hu, Song Bai, Philip\n  H.S. Torr", "title": "Anchor Diffusion for Unsupervised Video Object Segmentation", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised video object segmentation has often been tackled by methods\nbased on recurrent neural networks and optical flow. Despite their complexity,\nthese kinds of approaches tend to favour short-term temporal dependencies and\nare thus prone to accumulating inaccuracies, which cause drift over time.\nMoreover, simple (static) image segmentation models, alone, can perform\ncompetitively against these methods, which further suggests that the way\ntemporal dependencies are modelled should be reconsidered. Motivated by these\nobservations, in this paper we explore simple yet effective strategies to model\nlong-term temporal dependencies. Inspired by the non-local operators of [70],\nwe introduce a technique to establish dense correspondences between pixel\nembeddings of a reference \"anchor\" frame and the current one. This allows the\nlearning of pairwise dependencies at arbitrarily long distances without\nconditioning on intermediate frames. Without online supervision, our approach\ncan suppress the background and precisely segment the foreground object even in\nchallenging scenarios, while maintaining consistent performance over time. With\na mean IoU of $81.7\\%$, our method ranks first on the DAVIS-2016 leaderboard of\nunsupervised methods, while still being competitive against state-of-the-art\nonline semi-supervised approaches. We further evaluate our method on the FBMS\ndataset and the ViSal video saliency dataset, showing results competitive with\nthe state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 03:10:07 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Yang", "Zhao", ""], ["Wang", "Qiang", ""], ["Bertinetto", "Luca", ""], ["Hu", "Weiming", ""], ["Bai", "Song", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1910.10896", "submitter": "Yuanliu Liu", "authors": "Haiming Yu and Yin Fan and Keyu Chen and He Yan and Xiangju Lu and\n  Junhui Liu and Danming Xie", "title": "Unknown Identity Rejection Loss: Utilizing Unlabeled Data for Face\n  Recognition", "comments": "8 pages, 2 figures, Workshop paper accepted by Lightweight Face\n  Recognition Challenge & Workshop (ICCV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has advanced considerably with the availability of\nlarge-scale labeled datasets. However, how to further improve the performance\nwith the easily accessible unlabeled dataset remains a challenge. In this\npaper, we propose the novel Unknown Identity Rejection (UIR) loss to utilize\nthe unlabeled data. We categorize identities in unconstrained environment into\nthe known set and the unknown set. The former corresponds to the identities\nthat appear in the labeled training dataset while the latter is its\ncomplementary set. Besides training the model to accurately classify the known\nidentities, we also force the model to reject unknown identities provided by\nthe unlabeled dataset via our proposed UIR loss. In order to 'reject' faces of\nunknown identities, centers of the known identities are forced to keep enough\nmargin from centers of unknown identities which are assumed to be approximated\nby the features of their samples. By this means, the discriminativeness of the\nface representations can be enhanced. Experimental results demonstrate that our\napproach can provide obvious performance improvement by utilizing the unlabeled\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 03:18:14 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Yu", "Haiming", ""], ["Fan", "Yin", ""], ["Chen", "Keyu", ""], ["Yan", "He", ""], ["Lu", "Xiangju", ""], ["Liu", "Junhui", ""], ["Xie", "Danming", ""]]}, {"id": "1910.10916", "submitter": "Brian Wandell", "authors": "Zhenyi Liu, Trisha Lian, Joyce Farrell, and Brian Wandell", "title": "Soft Prototyping Camera Designs for Car Detection Based on a\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imaging systems are increasingly used as input to convolutional neural\nnetworks (CNN) for object detection; we would like to design cameras that are\noptimized for this purpose. It is impractical to build different cameras and\nthen acquire and label the necessary data for every potential camera design;\ncreating software simulations of the camera in context (soft prototyping) is\nthe only realistic approach. We implemented soft-prototyping tools that can\nquantitatively simulate image radiance and camera designs to create realistic\nimages that are input to a convolutional neural network for car detection. We\nused these methods to quantify the effect that critical hardware components\n(pixel size), sensor control (exposure algorithms) and image processing (gamma\nand demosaicing algorithms) have upon average precision of car detection. We\nquantify (a) the relationship between pixel size and the ability to detect cars\nat different distances, (b) the penalty for choosing a poor exposure duration,\nand (c) the ability of the CNN to perform car detection for a variety of\npost-acquisition processing algorithms. These results show that the optimal\nchoices for car detection are not constrained by the same metrics used for\nimage quality in consumer photography. It is better to evaluate camera designs\nfor CNN applications using soft prototyping with task-specific metrics rather\nthan consumer photography metrics.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 05:23:07 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Liu", "Zhenyi", ""], ["Lian", "Trisha", ""], ["Farrell", "Joyce", ""], ["Wandell", "Brian", ""]]}, {"id": "1910.10930", "submitter": "Yu Qin", "authors": "Yu Qin, Yuxing Li, Zhiwen Liu, Chuyang Ye", "title": "Knowledge Transfer between Datasets for Learning-based Tissue\n  Microstructure Estimation", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based approaches, especially those based on deep networks, have\nenabled high-quality estimation of tissue microstructure from low-quality\ndiffusion magnetic resonance imaging (dMRI) scans, which are acquired with a\nlimited number of diffusion gradients and a relatively poor spatial resolution.\nThese learning-based approaches to tissue microstructure estimation require\nacquisitions of training dMRI scans with high-quality diffusion signals, which\nare densely sampled in the q-space and have a high spatial resolution. However,\nthe acquisition of training scans may not be available for all datasets.\nTherefore, we explore knowledge transfer between different dMRI datasets so\nthat learning-based tissue microstructure estimation can be applied for\ndatasets where training scans are not acquired. Specifically, for a target\ndataset of interest, where only low-quality diffusion signals are acquired\nwithout training scans, we exploit the information in a source dMRI dataset\nacquired with high-quality diffusion signals. We interpolate the diffusion\nsignals in the source dataset in the q-space using a dictionary-based signal\nrepresentation, so that the interpolated signals match the acquisition scheme\nof the target dataset. Then, the interpolated signals are used together with\nthe high-quality tissue microstructure computed from the source dataset to\ntrain deep networks that perform tissue microstructure estimation for the\ntarget dataset. Experiments were performed on brain dMRI scans with low-quality\ndiffusion signals, where the benefit of the proposed strategy is demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 06:25:00 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Qin", "Yu", ""], ["Li", "Yuxing", ""], ["Liu", "Zhiwen", ""], ["Ye", "Chuyang", ""]]}, {"id": "1910.10949", "submitter": "M\\'arton Szemenyei", "authors": "Marton Szemenyei and Vladimir Estivill-Castro", "title": "ROBO: Robust, Fully Neural Object Detection for Robot Soccer", "comments": "Presented at the 2019 RoboCup Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has become exceptionally popular in the last few years due to\nits success in computer vision and other fields of AI. However, deep neural\nnetworks are computationally expensive, which limits their application in low\npower embedded systems, such as mobile robots. In this paper, an efficient\nneural network architecture is proposed for the problem of detecting relevant\nobjects in robot soccer environments. The ROBO model's increase in efficiency\nis achieved by exploiting the peculiarities of the environment. Compared to the\nstate-of-the-art Tiny YOLO model, the proposed network provides approximately\n35 times decrease in run time, while achieving superior average precision,\nalthough at the cost of slightly worse localization accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 07:24:58 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Szemenyei", "Marton", ""], ["Estivill-Castro", "Vladimir", ""]]}, {"id": "1910.10986", "submitter": "Xin Yao", "authors": "Xin Yao, Tianchi Huang, Chenglei Wu, Rui-Xiao Zhang, Lifeng Sun", "title": "Adversarial Feature Alignment: Avoid Catastrophic Forgetting in\n  Incremental Task Lifelong Learning", "comments": null, "journal-ref": "Neural Computation, Volume 31, Issue 11, November 2019,\n  p.2266-2291", "doi": "10.1162/neco_a_01232", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings are able to master a variety of knowledge and skills with\nongoing learning. By contrast, dramatic performance degradation is observed\nwhen new tasks are added to an existing neural network model. This phenomenon,\ntermed as \\emph{Catastrophic Forgetting}, is one of the major roadblocks that\nprevent deep neural networks from achieving human-level artificial\nintelligence. Several research efforts, e.g. \\emph{Lifelong} or\n\\emph{Continual} learning algorithms, have been proposed to tackle this\nproblem. However, they either suffer from an accumulating drop in performance\nas the task sequence grows longer, or require to store an excessive amount of\nmodel parameters for historical memory, or cannot obtain competitive\nperformance on the new tasks. In this paper, we focus on the incremental\nmulti-task image classification scenario. Inspired by the learning process of\nhuman students, where they usually decompose complex tasks into easier goals,\nwe propose an adversarial feature alignment method to avoid catastrophic\nforgetting. In our design, both the low-level visual features and high-level\nsemantic features serve as soft targets and guide the training process in\nmultiple stages, which provide sufficient supervised information of the old\ntasks and help to reduce forgetting. Due to the knowledge distillation and\nregularization phenomenons, the proposed method gains even better performance\nthan finetuning on the new tasks, which makes it stand out from other methods.\nExtensive experiments in several typical lifelong learning scenarios\ndemonstrate that our method outperforms the state-of-the-art methods in both\naccuracies on new tasks and performance preservation on old tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 09:23:02 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Yao", "Xin", ""], ["Huang", "Tianchi", ""], ["Wu", "Chenglei", ""], ["Zhang", "Rui-Xiao", ""], ["Sun", "Lifeng", ""]]}, {"id": "1910.10994", "submitter": "Xingxing Zhang", "authors": "Xingxing Zhang, Shupeng Gui, Zhenfeng Zhu, Yao Zhao, Ji Liu", "title": "ATZSL: Defensive Zero-Shot Recognition in the Presence of Adversaries", "comments": "14 pages, 9 figures, 10 tables, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) has received extensive attention recently especially\nin areas of fine-grained object recognition, retrieval, and image captioning.\nDue to the complete lack of training samples and high requirement of defense\ntransferability, the ZSL model learned is particularly vulnerable against\nadversarial attacks. Recent work also showed adversarially robust\ngeneralization requires more data. This may significantly affect the robustness\nof ZSL. However, very few efforts have been devoted towards this direction. In\nthis paper, we take an initial attempt, and propose a generic formulation to\nprovide a systematical solution (named ATZSL) for learning a robust ZSL model.\nIt is capable of achieving better generalization on various adversarial objects\nrecognition while only losing a negligible performance on clean images for\nunseen classes, by casting ZSL into a min-max optimization problem. To address\nit, we design a defensive relation prediction network, which can bridge the\nseen and unseen class domains via attributes to generalize prediction and\ndefense strategy. Additionally, our framework can be extended to deal with the\npoisoned scenario of unseen class attributes. An extensive group of experiments\nare then presented, demonstrating that ATZSL obtains remarkably more favorable\ntrade-off between model transferability and robustness, over currently\navailable alternatives under various settings.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 09:36:11 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 08:18:08 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Zhang", "Xingxing", ""], ["Gui", "Shupeng", ""], ["Zhu", "Zhenfeng", ""], ["Zhao", "Yao", ""], ["Liu", "Ji", ""]]}, {"id": "1910.10997", "submitter": "Ziwei Liu", "authors": "Hang Zhou, Ziwei Liu, Xudong Xu, Ping Luo, Xiaogang Wang", "title": "Vision-Infused Deep Audio Inpainting", "comments": "To appear in ICCV 2019. Code, models, dataset and video results are\n  available at the project page:\n  https://hangz-nju-cuhk.github.io/projects/AudioInpainting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modality perception is essential to develop interactive intelligence.\nIn this work, we consider a new task of visual information-infused audio\ninpainting, \\ie synthesizing missing audio segments that correspond to their\naccompanying videos. We identify two key aspects for a successful inpainter:\n(1) It is desirable to operate on spectrograms instead of raw audios. Recent\nadvances in deep semantic image inpainting could be leveraged to go beyond the\nlimitations of traditional audio inpainting. (2) To synthesize visually\nindicated audio, a visual-audio joint feature space needs to be learned with\nsynchronization of audio and video. To facilitate a large-scale study, we\ncollect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo\n(MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that\nour framework is capable of inpainting realistic and varying audio segments\nwith or without visual contexts. More importantly, our synthesized audio\nsegments are coherent with their video counterparts, showing the effectiveness\nof our proposed Vision-Infused Audio Inpainter (VIAI). Code, models, dataset\nand video results are available at\nhttps://hangz-nju-cuhk.github.io/projects/AudioInpainting\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 09:41:44 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Zhou", "Hang", ""], ["Liu", "Ziwei", ""], ["Xu", "Xudong", ""], ["Luo", "Ping", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1910.11006", "submitter": "Dongxu Li", "authors": "Dongxu Li, Cristian Rodriguez Opazo, Xin Yu, Hongdong Li", "title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale\n  Dataset and Methods Comparison", "comments": "Accepted by WACV2020, First Round, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based sign language recognition aims at helping deaf people to\ncommunicate with others. However, most existing sign language datasets are\nlimited to a small number of words. Due to the limited vocabulary size, models\nlearned from those datasets cannot be applied in practice. In this paper, we\nintroduce a new large-scale Word-Level American Sign Language (WLASL) video\ndataset, containing more than 2000 words performed by over 100 signers. This\ndataset will be made publicly available to the research community. To our\nknowledge, it is by far the largest public ASL dataset to facilitate word-level\nsign recognition research.\n  Based on this new large-scale dataset, we are able to experiment with several\ndeep learning methods for word-level sign recognition and evaluate their\nperformances in large scale scenarios. Specifically we implement and compare\ntwo different models,i.e., (i) holistic visual appearance-based approach, and\n(ii) 2D human pose based approach. Both models are valuable baselines that will\nbenefit the community for method benchmarking. Moreover, we also propose a\nnovel pose-based temporal graph convolution networks (Pose-TGCN) that models\nspatial and temporal dependencies in human pose trajectories simultaneously,\nwhich has further boosted the performance of the pose-based method. Our results\nshow that pose-based and appearance-based models achieve comparable\nperformances up to 66% at top-10 accuracy on 2,000 words/glosses, demonstrating\nthe validity and challenges of our dataset. Our dataset and baseline deep\nmodels are available at \\url{https://dxli94.github.io/WLASL/}.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 10:04:29 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 00:24:44 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Li", "Dongxu", ""], ["Opazo", "Cristian Rodriguez", ""], ["Yu", "Xin", ""], ["Li", "Hongdong", ""]]}, {"id": "1910.11009", "submitter": "Yu Xiong", "authors": "Yu Xiong, Qingqiu Huang, Lingfeng Guo, Hang Zhou, Bolei Zhou, Dahua\n  Lin", "title": "A Graph-Based Framework to Bridge Movies and Synopses", "comments": "Accepted by ICCV 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the remarkable advances in video analytics, research teams are\nstepping towards a greater ambition -- movie understanding. However, compared\nto those activity videos in conventional datasets, movies are significantly\ndifferent. Generally, movies are much longer and consist of much richer\ntemporal structures. More importantly, the interactions among characters play a\ncentral role in expressing the underlying story. To facilitate the efforts\nalong this direction, we construct a dataset called Movie Synopses Associations\n(MSA) over 327 movies, which provides a synopsis for each movie, together with\nannotated associations between synopsis paragraphs and movie segments. On top\nof this dataset, we develop a framework to perform matching between movie\nsegments and synopsis paragraphs. This framework integrates different aspects\nof a movie, including event dynamics and character interactions, and allows\nthem to be matched with parsed paragraphs, based on a graph-based formulation.\nOur study shows that the proposed framework remarkably improves the matching\naccuracy over conventional feature-based methods. It also reveals the\nimportance of narrative structures and character interactions in movie\nunderstanding.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 10:07:51 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Xiong", "Yu", ""], ["Huang", "Qingqiu", ""], ["Guo", "Lingfeng", ""], ["Zhou", "Hang", ""], ["Zhou", "Bolei", ""], ["Lin", "Dahua", ""]]}, {"id": "1910.11010", "submitter": "Xingxing Zhang", "authors": "Xingxing Zhang, Zhenfeng Zhu, Yao Zhao", "title": "ProLFA: Representative Prototype Selection for Local Feature Aggregation", "comments": "9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of hand-crafted local features, acquiring a global representation\nvia aggregation is a promising technique to boost computational efficiency and\nimprove task performance. Existing feature aggregation (FA) approaches,\nincluding Bag of Words and Fisher Vectors, usually fail to capture the desired\ninformation due to their pipeline mode. In this paper, we propose a generic\nformulation to provide a systematical solution (named ProLFA) to aggregate\nlocal descriptors. It is capable of producing compact yet interpretable\nrepresentations by selecting representative prototypes from numerous\ndescriptors, under relaxed exclusivity constraint. Meanwhile, to strengthen the\ndiscriminability of the aggregated representation, we rationally enforce the\ndomain-invariant projection of bundled descriptors along a task-specific\ndirection. Furthermore, ProLFA is also provided with a powerful generalization\nability to deal flexibly with the semi-supervised and fully supervised\nscenarios in local feature aggregation. Experimental results on various\ndescriptors and tasks demonstrate that the proposed ProLFA is considerably\nsuperior over currently available alternatives about feature aggregation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 10:08:10 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Zhang", "Xingxing", ""], ["Zhu", "Zhenfeng", ""], ["Zhao", "Yao", ""]]}, {"id": "1910.11012", "submitter": "Xuesong Niu", "authors": "Xuesong Niu and Hu Han and Shiguang Shan and Xilin Chen", "title": "Multi-label Co-regularization for Semi-supervised Facial Action Unit\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action units (AUs) recognition is essential for emotion analysis and\nhas been widely applied in mental state analysis. Existing work on AU\nrecognition usually requires big face dataset with AU labels; however, manual\nAU annotation requires expertise and can be time-consuming. In this work, we\npropose a semi-supervised approach for AU recognition utilizing a large number\nof web face images without AU labels and a relatively small face dataset with\nAU annotations inspired by the co-training methods. Unlike traditional\nco-training methods that require provided multi-view features and model\nre-training, we propose a novel co-training method, namely multi-label\nco-regularization, for semi-supervised facial AU recognition. Two deep neural\nnetworks are utilized to generate multi-view features for both labeled and\nunlabeled face images, and a multi-view loss is designed to enforce the two\nfeature generators to get conditional independent representations. In order to\nconstrain the prediction consistency of the two views, we further propose a\nmulti-label co-regularization loss by minimizing the distance of the predicted\nAU probability distributions of two views. In addition, prior knowledge of the\nrelationship between individual AUs is embedded through a graph convolutional\nnetwork (GCN) for exploiting useful information from the big unlabeled dataset.\nExperiments on several benchmarks show that the proposed approach can\neffectively leverage large datasets of face images without AU labels to improve\nthe AU recognition accuracy and outperform the state-of-the-art semi-supervised\nAU recognition methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 10:11:20 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 05:12:54 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Niu", "Xuesong", ""], ["Han", "Hu", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1910.11016", "submitter": "Tarun Yenamandra", "authors": "Tarun Yenamandra, Florian Bernard, Jiayi Wang, Franziska Mueller,\n  Christian Theobalt", "title": "Convex Optimisation for Inverse Kinematics", "comments": null, "journal-ref": null, "doi": "10.1109/3DV.2019.00043", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inverse kinematics (IK), where one wants to find\nthe parameters of a given kinematic skeleton that best explain a set of\nobserved 3D joint locations. The kinematic skeleton has a tree structure, where\neach node is a joint that has an associated geometric transformation that is\npropagated to all its child nodes. The IK problem has various applications in\nvision and graphics, for example for tracking or reconstructing articulated\nobjects, such as human hands or bodies. Most commonly, the IK problem is\ntackled using local optimisation methods. A major downside of these approaches\nis that, due to the non-convex nature of the problem, such methods are prone to\nconverge to unwanted local optima and therefore require a good initialisation.\nIn this paper we propose a convex optimisation approach for the IK problem\nbased on semidefinite programming, which admits a polynomial-time algorithm\nthat globally solves (a relaxation of) the IK problem. Experimentally, we\ndemonstrate that the proposed method significantly outperforms local\noptimisation methods using different real-world skeletons.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 10:25:23 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Yenamandra", "Tarun", ""], ["Bernard", "Florian", ""], ["Wang", "Jiayi", ""], ["Mueller", "Franziska", ""], ["Theobalt", "Christian", ""]]}, {"id": "1910.11030", "submitter": "Tu  Nguyen", "authors": "Tu Nguyen", "title": "Spatiotemporal Tile-based Attention-guided LSTMs for Traffic Video\n  Prediction", "comments": "Neurips 2019 Traffic4Cast Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This extended abstract describes our solution for the Traffic4Cast Challenge\n2019. The key problem we addressed is to properly model both low-level (pixel\nbased) and high-level spatial information while still preserve the temporal\nrelations among the frames. Our approach is inspired by the recent adoption of\nconvolutional features into a recurrent neural networks such as LSTM to jointly\ncapture the spatio-temporal dependency. While this approach has been proven to\nsurpass the traditional stacked CNNs (using 2D or 3D kernels) in action\nrecognition, we observe suboptimal performance in traffic prediction setting.\nTherefore, we apply a number of adaptations in the frame encoder-decoder layers\nand in sampling procedure to better capture the high-resolution trajectories,\nand to increase the training efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:05:22 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 19:16:00 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2020 22:06:32 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Nguyen", "Tu", ""]]}, {"id": "1910.11033", "submitter": "Tristan Hascoet", "authors": "Tristan Hascoet, Xuejiao Deng, Kiyoto Tai, Mari Sugiyama, Yuji Adachi,\n  Sachiko Nakamura, Yasuo Ariki, Tomoko Hayashi, Tetusya Takiguchi", "title": "Assisting human experts in the interpretation of their visual process: A\n  case study on assessing copper surface adhesive potency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks are often though to lack interpretability due to the\ndistributed nature of their internal representations. In contrast, humans can\ngenerally justify, in natural language, for their answer to a visual question\nwith simple common sense reasoning. However, human introspection abilities have\ntheir own limits as one often struggles to justify for the recognition process\nbehind our lowest level feature recognition ability: for instance, it is\ndifficult to precisely explain why a given texture seems more characteristic of\nthe surface of a finger nail rather than a plastic bottle. In this paper, we\nshowcase an application in which deep learning models can actually help human\nexperts justify for their own low-level visual recognition process: We study\nthe problem of assessing the adhesive potency of copper sheets from microscopic\npictures of their surface. Although highly trained material experts are able to\nqualitatively assess the surface adhesive potency, they are often unable to\nprecisely justify for their decision process. We present a model that, under\ncareful design considerations, is able to provide visual clues for human\nexperts to understand and justify for their own recognition process. Not only\ncan our model assist human experts in their interpretation of the surface\ncharacteristics, we show how this model can be used to test different\nhypothesis of the copper surface response to different manufacturing processes.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:23:58 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Hascoet", "Tristan", ""], ["Deng", "Xuejiao", ""], ["Tai", "Kiyoto", ""], ["Sugiyama", "Mari", ""], ["Adachi", "Yuji", ""], ["Nakamura", "Sachiko", ""], ["Ariki", "Yasuo", ""], ["Hayashi", "Tomoko", ""], ["Takiguchi", "Tetusya", ""]]}, {"id": "1910.11065", "submitter": "Anthony Bourached", "authors": "Anthony Bourached, Parashkev Nachev", "title": "Unsupervised Videographic Analysis of Rodent Behaviour", "comments": "Resubmission with fixed typos and updated data source information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal behaviour is complex and the amount of data in the form of video, if\nextracted, is copious. Manual analysis of behaviour is massively limited by two\ninsurmountable obstacles, the complexity of the behavioural patterns and human\nbias. Automated visual analysis has the potential to eliminate both of these\nissues and also enable continuous analysis allowing a much higher bandwidth of\ndata collection which is vital to capture complex behaviour at many different\ntime scales. Behaviour is not confined to a finite set modules and thus we can\nonly model it by inferring the generative distribution. In this way\nunpredictable, anomalous behaviour may be considered. Here we present a method\nof unsupervised behavioural analysis from nothing but high definition video\nrecordings taken from a single, fixed perspective. We demonstrate that the\nidentification of stereotyped rodent behaviour can be extracted in this way.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 14:44:55 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 01:06:20 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Bourached", "Anthony", ""], ["Nachev", "Parashkev", ""]]}, {"id": "1910.11067", "submitter": "Cat Le", "authors": "Cat P. Le, Yi Zhou, Jie Ding, Vahid Tarokh", "title": "Supervised Encoding for Discrete Representation Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9054118", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical supervised classification tasks search for a nonlinear mapping that\nmaps each encoded feature directly to a probability mass over the labels. Such\na learning framework typically lacks the intuition that encoded features from\nthe same class tend to be similar and thus has little interpretability for the\nlearned features. In this paper, we propose a novel supervised learning model\nnamed Supervised-Encoding Quantizer (SEQ). The SEQ applies a quantizer to\ncluster and classify the encoded features. We found that the quantizer provides\nan interpretable graph where each cluster in the graph represents a class of\ndata samples that have a particular style. We also trained a decoder that can\ndecode convex combinations of the encoded features from similar and different\nclusters and provide guidance on style transfer between sub-classes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 02:42:10 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Le", "Cat P.", ""], ["Zhou", "Yi", ""], ["Ding", "Jie", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1910.11072", "submitter": "Kyu Beom Lee", "authors": "Kyu-Beom Lee and Hyu-Soung Shin", "title": "Self-enhancement of automatic tunnel accident detection (TAD) on CCTV by\n  AI deep-learning", "comments": "13 pages, 8 figures, 4 tables, The 2019 International Conference on\n  Tunnels and Underground Spaces(ICTUS 19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep-learning-based tunnel accident detection (TAD) system (Lee 2019) has\ninstalled a system capable of monitoring 9 CCTVs at XX site in November, 2018.\nThe initial deep-learning training was started by studying 70,914 labeled\nimages and label data. However, sunlight, the tail light of a vehicle, and the\nwarning light of the working vehicle were recognized as a fire, and many\npedestrians were detected in the lane of the tunnel or a black elongated black\nobject. To solve these problems, as shown in Fig. 1, the false detection data\ndetected in the field were trained with labeled data and reapplied in the\nfield. As a result, false detection of pedestrians and fire could be\nsignificantly reduced.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 04:19:56 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Lee", "Kyu-Beom", ""], ["Shin", "Hyu-Soung", ""]]}, {"id": "1910.11073", "submitter": "Geoffroy Chaussonnet", "authors": "Geoffroy Chaussonnet, Christian Lieber, Yan Yikang, Wenda Gu, Andreas\n  Bartschat, Markus Reischl, Rainer Koch, Ralf Mikut and Hans-J\\\"org Bauer", "title": "Towards DeepSpray: Using Convolutional Neural Network to post-process\n  Shadowgraphy Images of Liquid Atomization", "comments": "Technical report, 22 pages, 29 figures", "journal-ref": null, "doi": "10.5445/IR/1000097897/v3", "report-no": null, "categories": "cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report investigates the potential of Convolutional Neural\nNetworks to post-process images from primary atomization. Three tasks are\ninvestigated. First, the detection and segmentation of liquid droplets in\ndegraded optical conditions. Second, the detection of overlapping ellipses and\nthe prediction of their geometrical characteristics. This task corresponds to\nextrapolate the hidden contour of an ellipse with reduced visual information.\nThird, several features of the liquid surface during primary breakup\n(ligaments, bags, rims) are manually annotated on 15 experimental images. The\ndetector is trained on this minimal database using simple data augmentation and\nthen applied to other images from numerical simulation and from other\nexperiment. In these three tasks, models from the literature based on\nConvolutional Neural Networks showed very promising results, thus demonstrating\nthe high potential of Deep Learning to post-process liquid atomization. The\nnext step is to embed these models into a unified framework DeepSpray.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 03:00:52 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Chaussonnet", "Geoffroy", ""], ["Lieber", "Christian", ""], ["Yikang", "Yan", ""], ["Gu", "Wenda", ""], ["Bartschat", "Andreas", ""], ["Reischl", "Markus", ""], ["Koch", "Rainer", ""], ["Mikut", "Ralf", ""], ["Bauer", "Hans-J\u00f6rg", ""]]}, {"id": "1910.11086", "submitter": "Ethan Harris", "authors": "Ethan Harris, Daniela Mihai, Jonathon Hare", "title": "Spatial and Colour Opponency in Anatomically Constrained Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colour vision has long fascinated scientists, who have sought to understand\nboth the physiology of the mechanics of colour vision and the psychophysics of\ncolour perception. We consider representations of colour in anatomically\nconstrained convolutional deep neural networks. Following ideas from\nneuroscience, we classify cells in early layers into groups relating to their\nspectral and spatial functionality. We show the emergence of single and double\nopponent cells in our networks and characterise how the distribution of these\ncells changes under the constraint of a retinal bottleneck. Our experiments not\nonly open up a new understanding of how deep networks process spatial and\ncolour information, but also provide new tools to help understand the black box\nof deep learning. The code for all experiments is avaialable at\n\\url{https://github.com/ecs-vlc/opponency}.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:28:44 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Harris", "Ethan", ""], ["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "1910.11088", "submitter": "Wei Wang", "authors": "Wei Wang, Muhamad Risqi U. Saputra, Peijun Zhao, Pedro Gusmao, Bo\n  Yang, Changhao Chen, Andrew Markham, and Niki Trigoni", "title": "DeepPCO: End-to-End Point Cloud Odometry through Deep Parallel Neural\n  Network", "comments": "To appear in IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Odometry is of key importance for localization in the absence of a map. There\nis considerable work in the area of visual odometry (VO), and recent advances\nin deep learning have brought novel approaches to VO, which directly learn\nsalient features from raw images. These learning-based approaches have led to\nmore accurate and robust VO systems. However, they have not been well applied\nto point cloud data yet. In this work, we investigate how to exploit deep\nlearning to estimate point cloud odometry (PCO), which may serve as a critical\ncomponent in point cloud-based downstream tasks or learning-based systems.\nSpecifically, we propose a novel end-to-end deep parallel neural network called\nDeepPCO, which can estimate the 6-DOF poses using consecutive point clouds. It\nconsists of two parallel sub-networks to estimate 3-D translation and\norientation respectively rather than a single neural network. We validate our\napproach on KITTI Visual Odometry/SLAM benchmark dataset with different\nbaselines. Experiments demonstrate that the proposed approach achieves good\nperformance in terms of pose accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 20:59:12 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 01:43:20 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Wang", "Wei", ""], ["Saputra", "Muhamad Risqi U.", ""], ["Zhao", "Peijun", ""], ["Gusmao", "Pedro", ""], ["Yang", "Bo", ""], ["Chen", "Changhao", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1910.11089", "submitter": "Jiahuan Luo", "authors": "Jiahuan Luo, Xueyang Wu, Yun Luo, Anbu Huang, Yunfeng Huang, Yang Liu\n  and Qiang Yang", "title": "Real-World Image Datasets for Federated Learning", "comments": "This paper is published at the 2nd International Workshop on\n  Federated Learning for Data Privacy and Confidentiality, in Conjunction with\n  NeurIPS 2019 (FL-NeurIPS 19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a new machine learning paradigm which allows data\nparties to build machine learning models collaboratively while keeping their\ndata secure and private. While research efforts on federated learning have been\ngrowing tremendously in the past two years, most existing works still depend on\npre-existing public datasets and artificial partitions to simulate data\nfederations due to the lack of high-quality labeled data generated from\nreal-world edge applications. Consequently, advances on benchmark and model\nevaluations for federated learning have been lagging behind. In this paper, we\nintroduce a real-world image dataset. The dataset contains more than 900 images\ngenerated from 26 street cameras and 7 object categories annotated with\ndetailed bounding box. The data distribution is non-IID and unbalanced,\nreflecting the characteristic real-world federated learning scenarios. Based on\nthis dataset, we implemented two mainstream object detection algorithms (YOLO\nand Faster R-CNN) and provided an extensive benchmark on model performance,\nefficiency, and communication in a federated learning setting. Both the dataset\nand algorithms are made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:33:26 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 09:22:56 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 06:31:32 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Luo", "Jiahuan", ""], ["Wu", "Xueyang", ""], ["Luo", "Yun", ""], ["Huang", "Anbu", ""], ["Huang", "Yunfeng", ""], ["Liu", "Yang", ""], ["Yang", "Qiang", ""]]}, {"id": "1910.11090", "submitter": "Dimitrios Kollias", "authors": "Aritra Banerjee and Dimitrios Kollias", "title": "Emotion Generation and Recognition: A StarGAN Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main idea of this ISO is to use StarGAN (A type of GAN model) to perform\ntraining and testing on an emotion dataset resulting in a emotion recognition\nwhich can be generated by the valence arousal score of the 7 basic expressions.\nWe have created an entirely new dataset consisting of 4K videos. This dataset\nconsists of all the basic 7 types of emotions: Happy, Sad, Angry, Surprised,\nFear, Disgust, Neutral. We have performed face detection and alignment followed\nby annotating basic valence arousal values to the frames/images in the dataset\ndepending on the emotions manually. Then the existing StarGAN model is trained\non our created dataset after which some manual subjects were chosen to test the\nefficiency of the trained StarGAN model.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 16:24:46 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Banerjee", "Aritra", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.11091", "submitter": "Li Xiao", "authors": "Li Xiao, Chunlong Luo, Tianqi Yu, Yufan Luo, Manqing Wang, Fuhai Yu,\n  Yinhao Li, Chan Tian, Jie Qiao", "title": "DeepACEv2: Automated Chromosome Enumeration in Metaphase Cell Images\n  Using Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chromosome enumeration is an essential but tedious procedure in karyotyping\nanalysis. To automate the enumeration process, we develop a chromosome\nenumeration framework, DeepACEv2, based on the region based object detection\nscheme. The framework is developed following three steps. Firstly, we take the\nclassical ResNet-101 as the backbone and attach the Feature Pyramid Network\n(FPN) to the backbone. The FPN takes full advantage of the multiple level\nfeatures, and we only output the level of feature map that most of the\nchromosomes are assigned to. Secondly, we enhance the region proposal network's\nability by adding a newly proposed Hard Negative Anchors Sampling to extract\nunapparent but essential information about highly confusing partial\nchromosomes. Next, to alleviate serious occlusion problems, besides the\ntraditional detection branch, we novelly introduce an isolated Template Module\nbranch to extract unique embeddings of each proposal by utilizing the\nchromosome's geometric information. The embeddings are further incorporated\ninto the No Maximum Suppression (NMS) procedure to improve the detection of\noverlapping chromosomes. Finally, we design a Truncated Normalized Repulsion\nLoss and add it to the loss function to avoid inaccurate localization caused by\nocclusion. In the newly collected 1375 metaphase images that came from a\nclinical laboratory, a series of ablation studies validate the effectiveness of\neach proposed module. Combining them, the proposed DeepACEv2 outperforms all\nthe previous methods, yielding the Whole Correct Ratio(WCR)(%) with respect to\nimages as 71.39, and the Average Error Ratio(AER)(%) with respect to\nchromosomes as about 1.17.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 13:36:32 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 07:05:32 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 23:12:53 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 12:30:57 GMT"}, {"version": "v5", "created": "Sun, 19 Jul 2020 12:51:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Xiao", "Li", ""], ["Luo", "Chunlong", ""], ["Yu", "Tianqi", ""], ["Luo", "Yufan", ""], ["Wang", "Manqing", ""], ["Yu", "Fuhai", ""], ["Li", "Yinhao", ""], ["Tian", "Chan", ""], ["Qiao", "Jie", ""]]}, {"id": "1910.11093", "submitter": "Ivan Sosnovik", "authors": "Ivan Sosnovik, Micha{\\l} Szmaja, Arnold Smeulders", "title": "Scale-Equivariant Steerable Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of Convolutional Neural Networks (CNNs) has been\nsubstantially attributed to their built-in property of translation\nequivariance. However, CNNs do not have embedded mechanisms to handle other\ntypes of transformations. In this work, we pay attention to scale changes,\nwhich regularly appear in various tasks due to the changing distances between\nthe objects and the camera. First, we introduce the general theory for building\nscale-equivariant convolutional networks with steerable filters. We develop\nscale-convolution and generalize other common blocks to be scale-equivariant.\nWe demonstrate the computational efficiency and numerical stability of the\nproposed method. We compare the proposed models to the previously developed\nmethods for scale equivariance and local scale invariance. We demonstrate\nstate-of-the-art results on MNIST-scale dataset and on STL-10 dataset in the\nsupervised learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:46:34 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 14:09:17 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Sosnovik", "Ivan", ""], ["Szmaja", "Micha\u0142", ""], ["Smeulders", "Arnold", ""]]}, {"id": "1910.11094", "submitter": "Kyu Beom Lee", "authors": "Kyu-Beom Lee and Hyu-Soung Shin", "title": "An application of a deep learning algorithm for automatic detection of\n  unexpected accidents under bad CCTV monitoring conditions in tunnels", "comments": "10 pages, 5 figures, 2019 International Conference on Deep Learning\n  and Machine Learning in Emerging Applications (Deep-ML)", "journal-ref": "978-1-7281-2914-3/19/$31.00 \\c{opyright}2019 IEEE", "doi": "10.1109/Deep-ML.2019.00010", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, Object Detection and Tracking System (ODTS) in combination\nwith a well-known deep learning network, Faster Regional Convolution Neural\nNetwork (Faster R-CNN), for Object Detection and Conventional Object Tracking\nalgorithm will be introduced and applied for automatic detection and monitoring\nof unexpected events on CCTVs in tunnels, which are likely to (1) Wrong-Way\nDriving (WWD), (2) Stop, (3) Person out of vehicle in tunnel (4) Fire. ODTS\naccepts a video frame in time as an input to obtain Bounding Box (BBox) results\nby Object Detection and compares the BBoxs of the current and previous video\nframes to assign a unique ID number to each moving and detected object. This\nsystem makes it possible to track a moving object in time, which is not usual\nto be achieved in conventional object detection frameworks. A deep learning\nmodel in ODTS was trained with a dataset of event images in tunnels to Average\nPrecision (AP) values of 0.8479, 0.7161 and 0.9085 for target objects: Car,\nPerson, and Fire, respectively. Then, based on trained deep learning model, the\nODTS based Tunnel CCTV Accident Detection System was tested using four accident\nvideos which including each accident. As a result, the system can detect all\naccidents within 10 seconds. The more important point is that the detection\ncapacity of ODTS could be enhanced automatically without any changes in the\nprogram codes as the training dataset becomes rich.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 04:28:44 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Lee", "Kyu-Beom", ""], ["Shin", "Hyu-Soung", ""]]}, {"id": "1910.11097", "submitter": "Neofytos Dimitriou", "authors": "Neofytos Dimitriou, Ognjen Arandjelovi\\'c, Peter D Caie", "title": "Deep Learning for Whole Slide Image Analysis: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread adoption of whole slide imaging has increased the demand for\neffective and efficient gigapixel image analysis. Deep learning is at the\nforefront of computer vision, showcasing significant improvements over previous\nmethodologies on visual understanding. However, whole slide images have\nbillions of pixels and suffer from high morphological heterogeneity as well as\nfrom different types of artefacts. Collectively, these impede the conventional\nuse of deep learning. For the clinical translation of deep learning solutions\nto become a reality, these challenges need to be addressed. In this paper, we\nreview work on the interdisciplinary attempt of training deep neural networks\nusing whole slide images, and highlight the different ideas underlying these\nmethodologies.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:53:42 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Dimitriou", "Neofytos", ""], ["Arandjelovi\u0107", "Ognjen", ""], ["Caie", "Peter D", ""]]}, {"id": "1910.11099", "submitter": "Kaidi Xu", "authors": "Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge\n  Chen, Pin-Yu Chen, Yanzhi Wang, Xue Lin", "title": "Adversarial T-shirt! Evading Person Detectors in A Physical World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that deep neural networks (DNNs) are vulnerable to adversarial\nattacks. The so-called physical adversarial examples deceive DNN-based\ndecisionmakers by attaching adversarial patches to real objects. However, most\nof the existing works on physical adversarial attacks focus on static objects\nsuch as glass frames, stop signs and images attached to cardboard. In this\nwork, we proposed adversarial T-shirts, a robust physical adversarial example\nfor evading person detectors even if it could undergo non-rigid deformation due\nto a moving person's pose changes. To the best of our knowledge, this is the\nfirst work that models the effect of deformation for designing physical\nadversarial examples with respect to-rigid objects such as T-shirts. We show\nthat the proposed method achieves74% and 57% attack success rates in the\ndigital and physical worlds respectively against YOLOv2. In contrast, the\nstate-of-the-art physical attack method to fool a person detector only achieves\n18% attack success rate. Furthermore, by leveraging min-max optimization, we\nextend our method to the ensemble attack setting against two object detectors\nYOLO-v2 and Faster R-CNN simultaneously.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 02:20:17 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 00:37:43 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 03:06:26 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Xu", "Kaidi", ""], ["Zhang", "Gaoyuan", ""], ["Liu", "Sijia", ""], ["Fan", "Quanfu", ""], ["Sun", "Mengshu", ""], ["Chen", "Hongge", ""], ["Chen", "Pin-Yu", ""], ["Wang", "Yanzhi", ""], ["Lin", "Xue", ""]]}, {"id": "1910.11100", "submitter": "Dennis N\\'u\\~nez Fern\\'andez", "authors": "Dennis N\\'u\\~nez Fern\\'andez", "title": "Development of a hand pose recognition system on an embedded computer\n  using CNNs", "comments": "LatinX in AI Research at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand of hand pose recognition systems are growing in the last years in\ntechnologies like human-machine interfaces. This work suggests an approach for\nhand pose recognition in embedded computers using hand tracking and CNNs.\nResults show a fast time response with an accuracy of 94.50% and low power\nconsumption.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 08:09:23 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Fern\u00e1ndez", "Dennis N\u00fa\u00f1ez", ""]]}, {"id": "1910.11102", "submitter": "Yao Peng", "authors": "Xinxin Zhu, Longteng Guo, Peng Yao, Shichen Lu, Wei Liu, Jing Liu", "title": "Vatex Video Captioning Challenge 2020: Multi-View Features and Hybrid\n  Reward Strategies for Video Captioning", "comments": "4 pages,2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes our solution for the VATEX Captioning Challenge 2020,\nwhich requires generating descriptions for the videos in both English and\nChinese languages. We identified three crucial factors that improve the\nperformance, namely: multi-view features, hybrid reward, and diverse ensemble.\nBased on our method of VATEX 2019 challenge, we achieved significant\nimprovements this year with more advanced model architectures, combination of\nappearance and motion features, and careful hyper-parameters tuning. Our method\nachieves very competitive results on both of the Chinese and English video\ncaptioning tracks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 13:52:49 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 01:47:18 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 04:13:30 GMT"}, {"version": "v4", "created": "Wed, 24 Jun 2020 03:42:09 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Zhu", "Xinxin", ""], ["Guo", "Longteng", ""], ["Yao", "Peng", ""], ["Lu", "Shichen", ""], ["Liu", "Wei", ""], ["Liu", "Jing", ""]]}, {"id": "1910.11103", "submitter": "Yue Niu", "authors": "Yue Niu, Hanqing Zeng, Ajitesh Srivastava, Kartik Lakhotia, Rajgopal\n  Kannan, Yanzhi Wang, Viktor Prasanna", "title": "SPEC2: SPECtral SParsE CNN Accelerator on FPGAs", "comments": "This is a 10-page conference paper in 26TH IEEE International\n  Conference On High Performance Computing, Data, and Analytics (HiPC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accelerate inference of Convolutional Neural Networks (CNNs), various\ntechniques have been proposed to reduce computation redundancy. Converting\nconvolutional layers into frequency domain significantly reduces the\ncomputation complexity of the sliding window operations in space domain. On the\nother hand, weight pruning techniques address the redundancy in model\nparameters by converting dense convolutional kernels into sparse ones. To\nobtain high-throughput FPGA implementation, we propose SPEC2 -- the first work\nto prune and accelerate spectral CNNs. First, we propose a systematic pruning\nalgorithm based on Alternative Direction Method of Multipliers (ADMM). The\noffline pruning iteratively sets the majority of spectral weights to zero,\nwithout using any handcrafted heuristics. Then, we design an optimized pipeline\narchitecture on FPGA that has efficient random access into the sparse kernels\nand exploits various dimensions of parallelism in convolutional layers.\nOverall, SPEC2 achieves high inference throughput with extremely low\ncomputation complexity and negligible accuracy degradation. We demonstrate\nSPEC2 by pruning and implementing LeNet and VGG16 on the Xilinx Virtex\nplatform. After pruning 75% of the spectral weights, SPEC2 achieves 0% accuracy\nloss for LeNet, and <1% accuracy loss for VGG16. The resulting accelerators\nachieve up to 24x higher throughput, compared with the state-of-the-art FPGA\nimplementations for VGG16.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 23:30:22 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Niu", "Yue", ""], ["Zeng", "Hanqing", ""], ["Srivastava", "Ajitesh", ""], ["Lakhotia", "Kartik", ""], ["Kannan", "Rajgopal", ""], ["Wang", "Yanzhi", ""], ["Prasanna", "Viktor", ""]]}, {"id": "1910.11104", "submitter": "Lucas C. Uzal", "authors": "Facundo Tuesca, Lucas C. Uzal", "title": "Exploiting video sequences for unsupervised disentangling in generative\n  adversarial networks", "comments": "This preprint is the result of the work done for the undergraduate\n  dissertation of F. Tuesca supervised by L.C. Uzal and presented in June 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present an adversarial training algorithm that exploits\ncorrelations in video to learn --without supervision-- an image generator model\nwith a disentangled latent space. The proposed methodology requires only a few\nmodifications to the standard algorithm of Generative Adversarial Networks\n(GAN) and involves training with sets of frames taken from short videos. We\ntrain our model over two datasets of face-centered videos which present\ndifferent people speaking or moving the head: VidTIMIT and YouTube Faces\ndatasets. We found that our proposal allows us to split the generator latent\nspace into two subspaces. One of them controls content attributes, those that\ndo not change along short video sequences. For the considered datasets, this is\nthe identity of the generated face. The other subspace controls motion\nattributes, those attributes that are observed to change along short videos. We\nobserved that these motion attributes are face expressions, head orientation,\nlips and eyes movement. The presented experiments provide quantitative and\nqualitative evidence supporting that the proposed methodology induces a\ndisentangling of this two kinds of attributes in the latent space.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:37:43 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Tuesca", "Facundo", ""], ["Uzal", "Lucas C.", ""]]}, {"id": "1910.11105", "submitter": "Lior Wolf", "authors": "Barak Battash, Lior Wolf", "title": "Adaptive and Iteratively Improving Recurrent Lateral Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current leading computer vision models are typically feed forward neural\nmodels, in which the output of one computational block is passed to the next\none sequentially. This is in sharp contrast to the organization of the primate\nvisual cortex, in which feedback and lateral connections are abundant. In this\nwork, we propose a computational model for the role of lateral connections in a\ngiven block, in which the weights of the block vary dynamically as a function\nof its activations, and the input from the upstream blocks is iteratively\nreintroduced. We demonstrate how this novel architectural modification can lead\nto sizable gains in performance, when applied to visual action recognition\nwithout pretraining and that it outperforms the literature architectures with\nrecurrent feedback processing on ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:58:26 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Battash", "Barak", ""], ["Wolf", "Lior", ""]]}, {"id": "1910.11106", "submitter": "David Donahue", "authors": "David Donahue", "title": "Label-Conditioned Next-Frame Video Generation with Neural Flows", "comments": "Computer Vision class project, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art video generation systems employ Generative\nAdversarial Networks (GANs) or Variational Autoencoders (VAEs) to produce novel\nvideos. However, VAE models typically produce blurry outputs when faced with\nsub-optimal conditioning of the input, and GANs are known to be unstable for\nlarge output sizes. In addition, the output videos of these models are\ndifficult to evaluate, partly because the GAN loss function is not an accurate\nmeasure of convergence. In this work, we propose using a state-of-the-art\nneural flow generator called Glow to generate videos conditioned on a textual\nlabel, one frame at a time. Neural flow models are more stable than standard\nGANs, as they only optimize a single cross entropy loss function, which is\nmonotonic and avoids the circular convergence issues of the GAN minimax\nobjective. In addition, we also show how to condition Glow on external context,\nwhile still preserving the invertible nature of each \"flow\" layer. Finally, we\nevaluate the proposed Glow model by calculating cross entropy on a held-out\nvalidation set of videos, in order to compare multiple versions of the proposed\nmodel via an ablation study. We show generated videos and discuss future\nimprovements.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:08:28 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Donahue", "David", ""]]}, {"id": "1910.11107", "submitter": "Sergey Tarasenko", "authors": "Sergey Tarasenko and Fumihiko Takahashi", "title": "Streaming Networks: Enable A Robust Classification of Noise-Corrupted\n  Images", "comments": "10 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolution neural nets (conv nets) have achieved a state-of-the-art\nperformance in many applications of image and video processing. The most recent\nstudies illustrate that the conv nets are fragile in terms of recognition\naccuracy to various image distortions such as noise, scaling, rotation, etc. In\nthis study we focus on the problem of robust recognition accuracy of random\nnoise distorted images. A common solution to this problem is either to add a\nlot of noisy images into a training dataset, which can be very costly, or use\nsophisticated loss function and denoising techniques. We introduce a novel conv\nnet architecture with multiple streams. Each stream is taking a certain\nintensity slice of the original image as an input, and stream parameters are\ntrained independently. We call this novel network a \"Streaming Net\". Our\nresults indicate that Streaming Net outperforms 1-stream conv net (employed as\na single stream) and 1-stream wide conv net (employs the same number of filters\nas Streaming Net) in recognition accuracy of noise-corrupted images, while\nproducing the same or higher recognition accuracy of no noise images in almost\nall of the tests. Thus, we introduce a new simple method to increase robustness\nof recognition of noisy images without using data generation or sophisticated\ntraining techniques.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:32:32 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Tarasenko", "Sergey", ""], ["Takahashi", "Fumihiko", ""]]}, {"id": "1910.11109", "submitter": "Zhen-Liang Ni", "authors": "Zhen-Liang Ni, Gui-Bin Bian, Zeng-Guang Hou, Xiao-Hu Zhou, Xiao-Liang\n  Xie, Zhen Li", "title": "Attention-Guided Lightweight Network for Real-Time Segmentation of\n  Robotic Surgical Instruments", "comments": "Accepted by ICRA2020; Camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real-time segmentation of surgical instruments plays a crucial role in\nrobot-assisted surgery. However, it is still a challenging task to implement\ndeep learning models to do real-time segmentation for surgical instruments due\nto their high computational costs and slow inference speed. In this paper, we\npropose an attention-guided lightweight network (LWANet), which can segment\nsurgical instruments in real-time. LWANet adopts encoder-decoder architecture,\nwhere the encoder is the lightweight network MobileNetV2, and the decoder\nconsists of depthwise separable convolution, attention fusion block, and\ntransposed convolution. Depthwise separable convolution is used as the basic\nunit to construct the decoder, which can reduce the model size and\ncomputational costs. Attention fusion block captures global contexts and\nencodes semantic dependencies between channels to emphasize target regions,\ncontributing to locating the surgical instrument. Transposed convolution is\nperformed to upsample feature maps for acquiring refined edges. LWANet can\nsegment surgical instruments in real-time while takes little computational\ncosts. Based on 960*544 inputs, its inference speed can reach 39 fps with only\n3.39 GFLOPs. Also, it has a small model size and the number of parameters is\nonly 2.06 M. The proposed network is evaluated on two datasets. It achieves\nstate-of-the-art performance 94.10% mean IOU on Cata7 and obtains a new record\non EndoVis 2017 with a 4.10% increase on mean IOU.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:48:52 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 09:25:52 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 14:55:16 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ni", "Zhen-Liang", ""], ["Bian", "Gui-Bin", ""], ["Hou", "Zeng-Guang", ""], ["Zhou", "Xiao-Hu", ""], ["Xie", "Xiao-Liang", ""], ["Li", "Zhen", ""]]}, {"id": "1910.11111", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Viktoriia Sharmanska and Stefanos Zafeiriou", "title": "Face Behavior a la carte: Expressions, Affect and Action Units in a\n  Single Network", "comments": "filed as a patent", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic facial behavior analysis has a long history of studies in the\nintersection of computer vision, physiology and psychology. However it is only\nrecently, with the collection of large-scale datasets and powerful machine\nlearning methods such as deep neural networks, that automatic facial behavior\nanalysis started to thrive. Three of its iconic tasks are automatic recognition\nof basic expressions (e.g. happy, sad, surprised), estimation of continuous\nemotions (e.g., valence and arousal), and detection of facial action units\n(activations of e.g. upper/inner eyebrows, nose wrinkles). Up until now these\ntasks have been mostly studied independently collecting a dataset for the task.\nWe present the first and the largest study of all facial behaviour tasks\nlearned jointly in a single multi-task, multi-domain and multi-label network,\nwhich we call FaceBehaviorNet. For this we utilize all publicly available\ndatasets in the community (around 5M images) that study facial behaviour tasks\nin-the-wild. We demonstrate that training jointly an end-to-end network for all\ntasks has consistently better performance than training each of the single-task\nnetworks. Furthermore, we propose two simple strategies for coupling the tasks\nduring training, co-annotation and distribution matching, and show the\nadvantages of this approach. Finally we show that FaceBehaviorNet has learned\nfeatures that encapsulate all aspects of facial behaviour, and can be\nsuccessfully applied to perform tasks (compound emotion recognition) beyond the\nones that it has been trained in a zero- and few-shot learning setting.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:45:41 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 23:35:29 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 02:35:49 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Sharmanska", "Viktoriia", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1910.11113", "submitter": "Li-Heng Chen", "authors": "Ching-Da Wu and Li-Heng Chen", "title": "Facial Emotion Recognition Using Deep Learning", "comments": "5 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to construct a system that captures real-world facial images through\nthe front camera on a laptop. The system is capable of processing/recognizing\nthe captured image and predict a result in real-time. In this system, we\nexploit the power of deep learning technique to learn a facial emotion\nrecognition (FER) model based on a set of labeled facial images. Finally,\nexperiments are conducted to evaluate our model using largely used public\ndatabase.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 14:54:11 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Wu", "Ching-Da", ""], ["Chen", "Li-Heng", ""]]}, {"id": "1910.11116", "submitter": "Tim \\\"Ubelh\\\"or", "authors": "Tim \\\"Ubelh\\\"or, Jonas Gesenhues, Nassim Ayoub, Ali Modabber and Dirk\n  Abel", "title": "Depth Camera Based Particle Filter for Robotic Osteotomy Navigation", "comments": "6 pages, submitted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active surgical robots lack acceptance in clinical practice, because they do\nnot offer the flexibility and usability required for a versatile usage: the\nsystems require a large installation space or a complicated registration step,\nwhere the preoperative plan is aligned to the patient and transformed to the\nbase frame of the robot. In this paper, a navigation system for robotic\nosteotomies is designed, which uses the raw depth images from a camera mounted\non the flange of a lightweight robot arm. Consequently, the system does not\nrequire any rigid attachment of the robot or fiducials to the bone and the\ntime-consuming registration step is eliminated. Instead, only a coarse\ninitialization is required which improves the usability in surgery. The full\nsix dimensional pose of the iliac crest bone is estimated with a particle\nfilter at a maximum rate of 90 Hz. The presented method is robust against\nchanging lighting conditions, blood or tissue on the bone surface and partial\nocclusions caused by the surgeons. Proof of the usability in a clinical\nenvironment is successfully provided in a corpse study, where surgeons used an\naugmented reality osteotomy template, which was aligned to bone via the\nparticle filters pose estimates for the resection of transplants from the iliac\ncrest.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:53:21 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["\u00dcbelh\u00f6r", "Tim", ""], ["Gesenhues", "Jonas", ""], ["Ayoub", "Nassim", ""], ["Modabber", "Ali", ""], ["Abel", "Dirk", ""]]}, {"id": "1910.11118", "submitter": "Kyle Robinson", "authors": "Kyle Robinson, Dan Brown", "title": "Shallow Art: Art Extension Through Simple Machine Learning", "comments": "5 pages, 9 figures, presented at the 10th International Conference on\n  Computational Creativity (ICCC 2019)", "journal-ref": "Proceedings of the 10th International Conference on Computational\n  Creativity (2019) 316-320 [ISBN: 978-989-54160-1-1]", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shallow Art presents, implements, and tests the use of simple single-output\nclassification and regression models for the purpose of art generation. Various\nmachine learning algorithms are trained on collections of computer generated\nimages, artworks from Vincent van Gogh, and artworks from Rembrandt van Rijn.\nThese models are then provided half of an image and asked to complete the\nmissing side. The resulting images are displayed, and we explore implications\nfor computational creativity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:47:06 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Robinson", "Kyle", ""], ["Brown", "Dan", ""]]}, {"id": "1910.11119", "submitter": "Jianri Li", "authors": "Jianri Li, Jae-whan Lee, Woo-sang Song, Ki-young Shin, Byung-hyun Go", "title": "Designovel's system description for Fashion-IQ challenge 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Designovel's systems which are submitted to the Fashion\nIQ Challenge 2019. Goal of the challenge is building an image retrieval system\nwhere input query is a candidate image plus two text phrases describe user's\nfeedback about visual differences between the candidate image and the search\ntarget. We built the systems by combining methods from recent work on deep\nmetric learning, multi-modal retrieval and natual language processing. First,\nwe encode both candidate and target images with CNNs into high-level\nrepresentations, and encode text descriptions to a single text vector using\nTransformer-based encoder. Then we compose candidate image vector and text\nrepresentation into a single vector which is exptected to be biased toward\ntarget image vector. Finally, we compute cosine similarities between composed\nvector and encoded vectors of whole dataset, and rank them in desceding order\nto get ranked list. We experimented with Fashion IQ 2019 dataset in various\nsettings of hyperparameters, achieved 39.12% average recall by a single model\nand 43.67% average recall by an ensemble of 16 models on test dataset.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:06:26 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Li", "Jianri", ""], ["Lee", "Jae-whan", ""], ["Song", "Woo-sang", ""], ["Shin", "Ki-young", ""], ["Go", "Byung-hyun", ""]]}, {"id": "1910.11121", "submitter": "Iqbal Nouyed", "authors": "Mohammad Iqbal Nouyed, Guodong Guo", "title": "Face Detection on Surveillance Images", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In last few decades, a lot of progress has been made in the field of face\ndetection. Various face detection methods have been proposed by numerous\nresearchers working in this area. The two well-known benchmarking platform: the\nFDDB and WIDER face detection provide quite challenging scenarios to assess the\nefficacy of the detection methods. These benchmarking data sets are mostly\ncreated using images from the public network ie. the Internet. A recent, face\ndetection and open-set recognition challenge has shown that those same face\ndetection algorithms produce high false alarms for images taken in surveillance\nscenario. This shows the difficult nature of the surveillance environment. Our\nproposed body pose based face detection method was one of the top performers in\nthis competition. In this paper, we perform a comparative performance analysis\nof some of the well known face detection methods including the few used in that\ncompetition, and, compare them to our proposed body pose based face detection\nmethod. Experiment results show that, our proposed method that leverages body\ninformation to detect faces, is the most realistic approach in terms of\naccuracy, false alarms and average detection time, when surveillance scenario\nis in consideration.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 02:10:58 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Nouyed", "Mohammad Iqbal", ""], ["Guo", "Guodong", ""]]}, {"id": "1910.11122", "submitter": "Sheng Zou", "authors": "Sheng Zou, Yu-Chien Tseng, Alina Zare, Diane Rowland, Barry Tillman,\n  Seung-Chul Yoon", "title": "Peanut Maturity Classification using Hyperspectral Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seed maturity in peanut (Arachis hypogaea L.) determines economic return to a\nproducer because of its impact on seed weight (yield), and critically\ninfluences seed vigor and other quality characteristics. During seed\ndevelopment, the inner mesocarp layer of the pericarp (hull) transitions in\ncolor from white to black as the seed matures. The maturity assessment process\ninvolves the removal of the exocarp of the hull and visually categorizing the\nmesocarp color into varying color classes from immature (white, yellow, orange)\nto mature (brown, and black). This visual color classification is time\nconsuming because the exocarp must be manually removed. In addition, the visual\nclassification process involves human assessment of colors, which leads to\nlarge variability of color classification from observer to observer. A more\nobjective, digital imaging approach to peanut maturity is needed, optimally\nwithout the requirement of removal of the hull's exocarp. This study examined\nthe use of a hyperspectral imaging (HSI) process to determine pod maturity with\nintact pericarps. The HSI method leveraged spectral differences between mature\nand immature pods within a classification algorithm to identify the mature and\nimmature pods. The results showed a high classification accuracy with\nconsistency using samples from different years and cultivars. In addition, the\nproposed method was capable of estimating a continuous-valued, pixel-level\nmaturity value for individual peanut pods, allowing for a valuable tool that\ncan be utilized in seed quality research. This new method solves issues of\nlabor intensity and subjective error that all current methods of peanut\nmaturity determination have.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 22:51:49 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 00:33:46 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Zou", "Sheng", ""], ["Tseng", "Yu-Chien", ""], ["Zare", "Alina", ""], ["Rowland", "Diane", ""], ["Tillman", "Barry", ""], ["Yoon", "Seung-Chul", ""]]}, {"id": "1910.11123", "submitter": "Hamed Majidifard", "authors": "Hamed Majidifard, Peng Jin, Yaw Adu-Gyamfi, William G. Buttlar", "title": "Pavement Image Datasets: A New Benchmark Dataset to Classify and Densify\n  Pavement Distresses", "comments": null, "journal-ref": null, "doi": "10.1177/0361198120907283", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated pavement distresses detection using road images remains a\nchallenging topic in the computer vision research community. Recent\ndevelopments in deep learning has led to considerable research activity\ndirected towards improving the efficacy of automated pavement distress\nidentification and rating. Deep learning models require a large ground truth\ndata set, which is often not readily available in the case of pavements. In\nthis study, a labeled dataset approach is introduced as a first step towards a\nmore robust, easy-to-deploy pavement condition assessment system. The technique\nis termed herein as the Pavement Image Dataset (PID) method. The dataset\nconsists of images captured from two camera views of an identical pavement\nsegment, i.e., a wide-view and a top-down view. The wide-view images were used\nto classify the distresses and to train the deep learning frameworks, while the\ntop-down view images allowed calculation of distress density, which will be\nused in future studies aimed at automated pavement rating. For the wide view\ngroup dataset, 7,237 images were manually annotated and distresses classified\ninto nine categories. Images were extracted using the Google Application\nProgramming Interface (API), selecting street-view images using a python-based\ncode developed for this project. The new dataset was evaluated using two\nmainstream deep learning frameworks: You Only Look Once (YOLO v2) and Faster\nRegion Convolution Neural Network (Faster R-CNN). Accuracy scores using the F1\nindex were found to be 0.84 for YOLOv2 and 0.65 for the Faster R-CNN model\nruns; both quite acceptable considering the convenience of utilizing Google\nmaps images.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 04:55:37 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 06:10:33 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Majidifard", "Hamed", ""], ["Jin", "Peng", ""], ["Adu-Gyamfi", "Yaw", ""], ["Buttlar", "William G.", ""]]}, {"id": "1910.11124", "submitter": "Hammad Ayyubi", "authors": "Hammad A. Ayyubi, Md. Mehrab Tanjim and David J. Kriegman", "title": "Enforcing Reasoning in Visual Commonsense Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of Visual Commonsense Reasoning is extremely challenging in the\nsense that the model has to not only be able to answer a question given an\nimage, but also be able to learn to reason. The baselines introduced in this\ntask are quite limiting because two networks are trained for predicting answers\nand rationales separately. Question and image is used as input to train answer\nprediction network while question, image and correct answer are used as input\nin the rationale prediction network. As rationale is conditioned on the correct\nanswer, it is based on the assumption that we can solve Visual Question\nAnswering task without any error - which is over ambitious. Moreover, such an\napproach makes both answer and rationale prediction two completely independent\nVQA tasks rendering cognition task meaningless. In this paper, we seek to\naddress these issues by proposing an end-to-end trainable model which considers\nboth answers and their reasons jointly. Specifically, we first predict the\nanswer for the question and then use the chosen answer to predict the\nrationale. However, a trivial design of such a model becomes non-differentiable\nwhich makes it difficult to train. We solve this issue by proposing four\napproaches - softmax, gumbel-softmax, reinforcement learning based sampling and\ndirect cross entropy against all pairs of answers and rationales. We\ndemonstrate through experiments that our model performs competitively against\ncurrent state-of-the-art. We conclude with an analysis of presented approaches\nand discuss avenues for further work.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 02:33:18 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 10:09:58 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ayyubi", "Hammad A.", ""], ["Tanjim", "Md. Mehrab", ""], ["Kriegman", "David J.", ""]]}, {"id": "1910.11126", "submitter": "Enea Ceolini", "authors": "Enea Ceolini, Gemma Taverni, Lyes Khacef, Melika Payvand, Elisa Donati", "title": "Sensor fusion using EMG and vision for hand gesture classification in\n  mobile applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrimination of human gestures using wearable solutions is extremely\nimportant as a supporting technique for assisted living, healthcare of the\nelderly and neurorehabilitation. This paper presents a mobile electromyography\n(EMG) analysis framework to be an auxiliary component in physiotherapy sessions\nor as a feedback for neuroprosthesis calibration. We implemented a framework\nthat allows the integration of multisensors, EMG and visual information, to\nperform sensor fusion and to improve the accuracy of hand gesture recognition\ntasks. In particular, we used an event-based camera adapted to run on the\nlimited computational resources of mobile phones. We introduced a new publicly\navailable dataset of sensor fusion for hand gesture recognition recorded from\n10 subjects and used it to train the recognition models offline. We compare the\nonline results of the hand gesture recognition using the fusion approach with\nthe individual sensors with an improvement in the accuracy of 13% and 11%, for\nEMG and vision respectively, reaching 85%.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 00:02:48 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Ceolini", "Enea", ""], ["Taverni", "Gemma", ""], ["Khacef", "Lyes", ""], ["Payvand", "Melika", ""], ["Donati", "Elisa", ""]]}, {"id": "1910.11127", "submitter": "Tristan Hascoet", "authors": "Tristan Hascoet, Quentin Febvre, Yasuo Ariki, Tetsuya Takiguchi", "title": "Reversible designs for extreme memory cost reduction of CNN training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Convolutional Neural Networks (CNN) is a resource intensive task\nthat requires specialized hardware for efficient computation. One of the most\nlimiting bottleneck of CNN training is the memory cost associated with storing\nthe activation values of hidden layers needed for the computation of the\nweights gradient during the backward pass of the backpropagation algorithm.\nRecently, reversible architectures have been proposed to reduce the memory cost\nof training large CNN by reconstructing the input activation values of hidden\nlayers from their output during the backward pass, circumventing the need to\naccumulate these activations in memory during the forward pass. In this paper,\nwe push this idea to the extreme and analyze reversible network designs\nyielding minimal training memory footprint. We investigate the propagation of\nnumerical errors in long chains of invertible operations and analyze their\neffect on training. We introduce the notion of pixel-wise memory cost to\ncharacterize the memory footprint of model training, and propose a new model\narchitecture able to efficiently train arbitrarily deep neural networks with a\nminimum memory cost of 352 bytes per input pixel. This new kind of architecture\nenables training large neural networks on very limited memory, opening the door\nfor neural network training on embedded devices or non-specialized hardware.\nFor instance, we demonstrate training of our model to 93.3% accuracy on the\nCIFAR10 dataset within 67 minutes on a low-end Nvidia GTX750 GPU with only 1GB\nof memory.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:59:23 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Hascoet", "Tristan", ""], ["Febvre", "Quentin", ""], ["Ariki", "Yasuo", ""], ["Takiguchi", "Tetsuya", ""]]}, {"id": "1910.11146", "submitter": "Alexander Schaefer", "authors": "Alexander Schaefer, Johan Vertens, Daniel B\\\"uscher, Wolfram Burgard", "title": "A Maximum Likelihood Approach to Extract Finite Planes from 3-D Laser\n  Scans", "comments": null, "journal-ref": "International Conference on Robotics and Automation , Montreal,\n  QC, Canada, 2019, pp. 72-78", "doi": "10.1109/ICRA.2019.8794318", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether it is object detection, model reconstruction, laser odometry, or\npoint cloud registration: Plane extraction is a vital component of many robotic\nsystems. In this paper, we propose a strictly probabilistic method to detect\nfinite planes in organized 3-D laser range scans. An agglomerative hierarchical\nclustering technique, our algorithm builds planes from bottom up, always\nextending a plane by the point that decreases the measurement likelihood of the\nscan the least. In contrast to most related methods, which rely on heuristics\nlike orthogonal point-to-plane distance, we leverage the ray path information\nto compute the measurement likelihood. We evaluate our approach not only on the\npopular SegComp benchmark, but also provide a challenging synthetic dataset\nthat overcomes SegComp's deficiencies. Both our implementation and the\nsuggested dataset are available at www.github.com/acschaefer/ppe.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 13:06:34 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Schaefer", "Alexander", ""], ["Vertens", "Johan", ""], ["B\u00fcscher", "Daniel", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1910.11147", "submitter": "Alexander Schaefer", "authors": "Alexander Schaefer, Lukas Luft, Wolfram Burgard", "title": "DCT Maps: Compact Differentiable Lidar Maps Based on the Cosine\n  Transform", "comments": "8 pages", "journal-ref": "IEEE Robotics and Automation Letters (Volume: 3, Issue: 2, April\n  2018)", "doi": "10.1109/LRA.2018.2794602", "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most robot mapping techniques for lidar sensors tessellate the environment\ninto pixels or voxels and assume uniformity of the environment within them.\nAlthough intuitive, this representation entails disadvantages: The resulting\ngrid maps exhibit aliasing effects and are not differentiable. In the present\npaper, we address these drawbacks by introducing a novel mapping technique that\ndoes neither rely on tessellation nor on the assumption of piecewise uniformity\nof the space, without increasing memory requirements. Instead of representing\nthe map in the position domain, we store the map parameters in the discrete\nfrequency domain and leverage the continuous extension of the inverse discrete\ncosine transform to convert them to a continuously differentiable scalar field\nin the position domain, which we call DCT map. A DCT map assigns to each point\nin space a lidar decay rate, which models the local permeability of the space\nfor laser rays. In this way, the map can describe objects of different laser\npermeabilities, from completely opaque to completely transparent. DCT maps\nrepresent lidar measurements significantly more accurate than grid maps,\nGaussian process occupancy maps, and Hilbert maps, all with the same memory\nrequirements, as demonstrated in our real-world experiments.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 12:07:12 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Schaefer", "Alexander", ""], ["Luft", "Lukas", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1910.11148", "submitter": "Qiegen Liu", "authors": "Zhuonan He, Jinjie Zhou, Dong Liang, Yuhao Wang, Qiegen Liu", "title": "Learning Priors in High-frequency Domain for Inverse Imaging\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ill-posed inverse problems in imaging remain an active research topic in\nseveral decades, with new approaches constantly emerging. Recognizing that the\npopular dictionary learning and convolutional sparse coding are both\nessentially modeling the high-frequency component of an image, which convey\nmost of the semantic information such as texture details, in this work we\npropose a novel multi-profile high-frequency transform-guided denoising\nautoencoder as prior (HF-DAEP). To achieve this goal, we first extract a set of\nmulti-profile high-frequency components via a specific transformation and add\nthe artificial Gaussian noise to these high-frequency components as training\nsamples. Then, as the high-frequency prior information is learned, we\nincorporate it into classical iterative reconstruction process by proximal\ngradient descent technique. Preliminary results on highly under-sampled\nmagnetic resonance imaging and sparse-view computed tomography reconstruction\ndemonstrate that the proposed method can efficiently reconstruct feature\ndetails and present advantages over state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:15:42 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["He", "Zhuonan", ""], ["Zhou", "Jinjie", ""], ["Liang", "Dong", ""], ["Wang", "Yuhao", ""], ["Liu", "Qiegen", ""]]}, {"id": "1910.11174", "submitter": "Zheng Lian", "authors": "Zheng Lian, Ya Li, Jianhua Tao, Jian Huang", "title": "Speech Emotion Recognition via Contrastive Loss under Siamese Networks", "comments": "ASMMC-MMAC 2018 Proceedings of the Joint Workshop of the 4th Workshop\n  on Affective Social Multimedia Computing and first Multi-Modal Affective\n  Computing of Large-Scale Multimedia Data", "journal-ref": null, "doi": "10.1145/3267935.3267946", "report-no": null, "categories": "cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech emotion recognition is an important aspect of human-computer\ninteraction. Prior work proposes various end-to-end models to improve the\nclassification performance. However, most of them rely on the cross-entropy\nloss together with softmax as the supervision component, which does not\nexplicitly encourage discriminative learning of features. In this paper, we\nintroduce the contrastive loss function to encourage intra-class compactness\nand inter-class separability between learnable features. Furthermore, multiple\nfeature selection methods and pairwise sample selection methods are evaluated.\nTo verify the performance of the proposed system, we conduct experiments on The\nInteractive Emotional Dyadic Motion Capture (IEMOCAP) database, a common\nevaluation corpus. Experimental results reveal the advantages of the proposed\nmethod, which reaches 62.19% in the weighted accuracy and 63.21% in the\nunweighted accuracy. It outperforms the baseline system that is optimized\nwithout the contrastive loss function with 1.14% and 2.55% in the weighted\naccuracy and the unweighted accuracy, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 15:43:42 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Lian", "Zheng", ""], ["Li", "Ya", ""], ["Tao", "Jianhua", ""], ["Huang", "Jian", ""]]}, {"id": "1910.11185", "submitter": "Mohamed Hamidi", "authors": "Mohamed Hamidi, Mohamed El Haziti, Hocine Cherifi, Driss Aboutajdine", "title": "A blind Robust Image Watermarking Approach exploiting the DFT Magnitude", "comments": "6 pages, 4 Figures, published in : (2015) IEEE/ACS 12th International\n  Conference of Computer Systems and Applications (AICCSA)", "journal-ref": null, "doi": "10.1109/AICCSA.2015.7507124", "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the current progress in Internet, digital contents (video, audio and\nimages) are widely used. Distribution of multimedia contents is now faster and\nit allows for easy unauthorized reproduction of information. Digital\nwatermarking came up while trying to solve this problem. Its main idea is to\nembed a watermark into a host digital content without affecting its quality.\nMoreover, watermarking can be used in several applications such as\nauthentication, copy control, indexation, Copyright protection, etc. In this\npaper, we propose a blind robust image watermarking approach as a solution to\nthe problem of copyright protection of digital images. The underlying concept\nof our method is to apply a discrete cosine transform (DCT) to the magnitude\nresulting from a discrete Fourier transform (DFT) applied to the original\nimage. Then, the watermark is embedded by modifying the coefficients of the DCT\nusing a secret key to increase security. Experimental results show the\nrobustness of the proposed technique to a wide range of common attacks, e.g.,\nLow-Pass Gaussian Filtering, JPEG compression, Gaussian noise, salt & pepper\nnoise, Gaussian Smoothing and Histogram equalization. The proposed method\nachieves a Peak signal-to-noise-ration (PSNR) value greater than 66 (dB) and\nensures a perfect watermark extraction.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 00:26:13 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Hamidi", "Mohamed", ""], ["Haziti", "Mohamed El", ""], ["Cherifi", "Hocine", ""], ["Aboutajdine", "Driss", ""]]}, {"id": "1910.11215", "submitter": "Sudeep Dasari", "authors": "Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette\n  Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, Chelsea Finn", "title": "RoboNet: Large-Scale Multi-Robot Learning", "comments": "accepted at the Conference on Robot Learning (CoRL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot learning has emerged as a promising tool for taming the complexity and\ndiversity of the real world. Methods based on high-capacity models, such as\ndeep networks, hold the promise of providing effective generalization to a wide\nrange of open-world environments. However, these same methods typically require\nlarge amounts of diverse training data to generalize effectively. In contrast,\nmost robotic learning experiments are small-scale, single-domain, and\nsingle-robot. This leads to a frequent tension in robotic learning: how can we\nlearn generalizable robotic controllers without having to collect impractically\nlarge amounts of data for each separate experiment? In this paper, we propose\nRoboNet, an open database for sharing robotic experience, which provides an\ninitial pool of 15 million video frames, from 7 different robot platforms, and\nstudy how it can be used to learn generalizable models for vision-based robotic\nmanipulation. We combine the dataset with two different learning algorithms:\nvisual foresight, which uses forward video prediction models, and supervised\ninverse models. Our experiments test the learned algorithms' ability to work\nacross new objects, new tasks, new scenes, new camera viewpoints, new grippers,\nor even entirely new robots. In our final experiment, we find that by\npre-training on RoboNet and fine-tuning on data from a held-out Franka or Kuka\nrobot, we can exceed the performance of a robot-specific training approach that\nuses 4x-20x more data. For videos and data, see the project webpage:\nhttps://www.robonet.wiki/\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:20:03 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 06:26:37 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Dasari", "Sudeep", ""], ["Ebert", "Frederik", ""], ["Tian", "Stephen", ""], ["Nair", "Suraj", ""], ["Bucher", "Bernadette", ""], ["Schmeckpeper", "Karl", ""], ["Singh", "Siddharth", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1910.11222", "submitter": "Shuming Jiao", "authors": "Shuming Jiao, Dongfang Zhang, Chonglei Zhang, Yang Gao, Ting Lei,\n  Xiaocong Yuan", "title": "Data hiding in complex-amplitude modulation using a digital micromirror\n  device", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A digital micromirror device (DMD) is an amplitude-type spatial light\nmodulator. However, a complex-amplitude light modulation with a DMD can be\nachieved using the superpixel scheme. In the superpixel scheme, we notice that\nmultiple different DMD local block patterns may correspond to the same complex\nsuperpixel value. Based on this inherent encoding redundancy, a large amount of\nexternal data can be embedded into the DMD pattern without extra cost.\nMeanwhile, the original complex light field information carried by the DMD\npattern is fully preserved. This proposed scheme is favorable for applications\nsuch as secure information transmission and copyright protection.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:26:38 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Jiao", "Shuming", ""], ["Zhang", "Dongfang", ""], ["Zhang", "Chonglei", ""], ["Gao", "Yang", ""], ["Lei", "Ting", ""], ["Yuan", "Xiaocong", ""]]}, {"id": "1910.11250", "submitter": "Kaylen Pfisterer", "authors": "Kaylen J Pfisterer, Robert Amelard, Audrey G Chung, Braeden Syrnyk,\n  Alexander MacLean, Heather H Keller, Alexander Wong", "title": "When Segmentation is Not Enough: Rectifying Visual-Volume Discordance\n  Through Multisensor Depth-Refined Semantic Segmentation for Food Intake\n  Tracking in Long-Term Care", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malnutrition is a multidomain problem affecting 54% of older adults in\nlong-term care (LTC). Monitoring nutritional intake in LTC is laborious and\nsubjective, limiting clinical inference capabilities. Recent advances in\nautomatic image-based food estimation have not yet been evaluated in LTC\nsettings. Here, we describe a fully automatic imaging system for quantifying\nfood intake. We propose a novel deep convolutional encoder-decoder food network\nwith depth-refinement (EDFN-D) using an RGB-D camera for quantifying a plate's\nremaining food volume relative to reference portions in whole and modified\ntexture foods. We trained and validated the network on the pre-labelled\nUNIMIB2016 food dataset and tested on our two novel LTC-inspired plate datasets\n(689 plate images, 36 unique foods). EDFN-D performed comparably to\ndepth-refined graph cut on IOU (0.879 vs. 0.887), with intake errors well below\ntypical 50% (mean percent intake error: -4.2%). We identify how standard\nsegmentation metrics are insufficient due to visual-volume discordance, and\ninclude volume disparity analysis to facilitate system trust. This system\nprovides improved transparency, approximates human assessors with enhanced\nobjectivity, accuracy, and precision while avoiding hefty semi-automatic method\ntime requirements. This may help address short-comings currently limiting\nutility of automated early malnutrition detection in resource-constrained LTC\nand hospital settings.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:50:20 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 19:56:38 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Pfisterer", "Kaylen J", ""], ["Amelard", "Robert", ""], ["Chung", "Audrey G", ""], ["Syrnyk", "Braeden", ""], ["MacLean", "Alexander", ""], ["Keller", "Heather H", ""], ["Wong", "Alexander", ""]]}, {"id": "1910.11272", "submitter": "Enlai Guo", "authors": "Enlai Guo, Shuo Zhu, Yan Sun, Lianfa Bai, Jing Han", "title": "Learning-based real-time method to looking through scattering medium\n  beyond the memory effect", "comments": "15 pages with 9 figures", "journal-ref": null, "doi": "10.1364/OE.383911", "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strong scattering medium brings great difficulties to optical imaging, which\nis also a problem in medical imaging and many other fields. Optical memory\neffect makes it possible to image through strong random scattering medium.\nHowever, this method also has the limitation of limited angle field-of-view\n(FOV), which prevents it from being applied in practice. In this paper, a kind\nof practical convolutional neural network called PDSNet is proposed, which\neffectively breaks through the limitation of optical memory effect on FOV.\nExperiments is conducted to prove that the scattered pattern can be\nreconstructed accurately in real-time by PDSNet, and it is widely applicable to\nretrieve complex objects of random scales and different scattering media.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 03:56:26 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 07:25:29 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Guo", "Enlai", ""], ["Zhu", "Shuo", ""], ["Sun", "Yan", ""], ["Bai", "Lianfa", ""], ["Han", "Jing", ""]]}, {"id": "1910.11276", "submitter": "Qian Zheng Mr", "authors": "Qian Zheng", "title": "Emotion recognition with 4kresolution database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying the human emotion through facial expressions is a big topic in\nboth the Computer Vision and Deep learning fields. Human emotion can be\nclassified as one of the basic emotion types like being angry, happy or\ndimensional emotion with valence and arousal values. There are a lot of related\nchallenges in this topic, one of the most famous challenges is called the\n'Affect-in-the-wild Challenge'(Aff-Wild Challenge). It is the first challenge\non the estimation of valence and arousal in-the-wild. This project is an\nextension of the Aff-wild Challenge. Aff-wild database was created using images\nwith a mean resolution of 607*359, I and Dimitrios sought to find out the\nperformance of the model that is trained on a database that contains4K\nresolution in-the-wild images. Since there is no existing database to satisfy\nthe requirement, I built this database from scratch with help from Dimitrios\nand trained neural network models with different hyperparameters on this\ndatabase. I used network models likeVGG16, AlexNet, ResNet and also some\npre-trained models like Ima-geNet VGG. I compared the results of the different\nnetwork models alongside the results from the Aff-wild database to exploit the\noptimal model for my database.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 16:44:11 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Zheng", "Qian", ""]]}, {"id": "1910.11285", "submitter": "Xudong Lin", "authors": "Xudong Lin, Zheng Shou, Shih-Fu Chang", "title": "Towards Train-Test Consistency for Semi-supervised Temporal Action\n  Localization", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Weakly-supervised Temporal Action Localization (WTAL) has been\ndensely studied but there is still a large gap between weakly-supervised models\nand fully-supervised models. It is practical and intuitive to annotate temporal\nboundaries of a few examples and utilize them to help WTAL models better detect\nactions. However, the train-test discrepancy of action localization strategy\nprevents WTAL models from leveraging semi-supervision for further improvement.\nAt training time, attention or multiple instance learning is used to aggregate\npredictions of each snippet for video-level classification; at test time, they\nfirst obtain action score sequences over time, then truncate segments of scores\nhigher than a fixed threshold, and post-process action segments. The\ninconsistent strategy makes it hard to explicitly supervise the action\nlocalization model with temporal boundary annotations at training time. In this\npaper, we propose a Train-Test Consistent framework, TTC-Loc. In both training\nand testing time, our TTC-Loc localizes actions by comparing scores of action\nclasses and predicted threshold, which enables it to be trained with\nsemi-supervision. By fixing the train-test discrepancy, our TTC-Loc\nsignificantly outperforms the state-of-the-art performance on THUMOS'14,\nActivityNet 1.2 and 1.3 when only video-level labels are provided for training.\nWith full annotations of only one video per class and video-level labels for\nthe other videos, our TTC-Loc further boosts the performance and achieves\n33.4\\% mAP (IoU threshold 0.5) on THUMOS's 14.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:00:14 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 01:16:21 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 02:56:39 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Lin", "Xudong", ""], ["Shou", "Zheng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1910.11296", "submitter": "Kelvin Wong", "authors": "Kelvin Wong, Shenlong Wang, Mengye Ren, Ming Liang, and Raquel Urtasun", "title": "Identifying Unknown Instances for Autonomous Driving", "comments": "3rd Conference on Robot Learning (CoRL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, we have seen great progress in perception algorithms,\nparticular through the use of deep learning. However, most existing approaches\nfocus on a few categories of interest, which represent only a small fraction of\nthe potential categories that robots need to handle in the real-world. Thus,\nidentifying objects from unknown classes remains a challenging yet crucial\ntask. In this paper, we develop a novel open-set instance segmentation\nalgorithm for point clouds which can segment objects from both known and\nunknown classes in a holistic way. Our method uses a deep convolutional neural\nnetwork to project points into a category-agnostic embedding space in which\nthey can be clustered into instances irrespective of their semantics.\nExperiments on two large-scale self-driving datasets validate the effectiveness\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:24:43 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Wong", "Kelvin", ""], ["Wang", "Shenlong", ""], ["Ren", "Mengye", ""], ["Liang", "Ming", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1910.11301", "submitter": "An Yan", "authors": "An Yan, Xin Eric Wang, Jiangtao Feng, Lei Li, William Yang Wang", "title": "Cross-Lingual Vision-Language Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commanding a robot to navigate with natural language instructions is a\nlong-term goal for grounded language understanding and robotics. But the\ndominant language is English, according to previous studies on vision-language\nnavigation (VLN). To go beyond English and serve people speaking different\nlanguages, we collect a bilingual Room-to-Room (BL-R2R) dataset, extending the\noriginal benchmark with new Chinese instructions. Based on this newly\nintroduced dataset, we study how an agent can be trained on existing English\ninstructions but navigate effectively with another language under a zero-shot\nlearning scenario. Without any training data of the target language, our model\nshows competitive results even compared to a model with full access to the\ntarget language training data. Moreover, we investigate the transferring\nability of our model when given a certain amount of target language training\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:32:38 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 05:48:48 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2020 02:48:07 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Yan", "An", ""], ["Wang", "Xin Eric", ""], ["Feng", "Jiangtao", ""], ["Li", "Lei", ""], ["Wang", "William Yang", ""]]}, {"id": "1910.11306", "submitter": "Joao Carreira", "authors": "Jean-Baptiste Alayrac, Jo\\~ao Carreira, Relja Arandjelovi\\'c and\n  Andrew Zisserman", "title": "Controllable Attention for Structured Layered Video Decomposition", "comments": "In ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to be able to separate a video into its\nnatural layers, and to control which of the separated layers to attend to. For\nexample, to be able to separate reflections, transparency or object motion. We\nmake the following three contributions: (i) we introduce a new structured\nneural network architecture that explicitly incorporates layers (as spatial\nmasks) into its design. This improves separation performance over previous\ngeneral purpose networks for this task; (ii) we demonstrate that we can augment\nthe architecture to leverage external cues such as audio for controllability\nand to help disambiguation; and (iii) we experimentally demonstrate the\neffectiveness of our approach and training procedure with controlled\nexperiments while also showing that the proposed model can be successfully\napplied to real-word applications such as reflection removal and action\nrecognition in cluttered scenes.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:36:40 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Alayrac", "Jean-Baptiste", ""], ["Carreira", "Jo\u00e3o", ""], ["Arandjelovi\u0107", "Relja", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1910.11319", "submitter": "Yi-Hsuan Tsai", "authors": "Han-Kai Hsu, Chun-Han Yao, Yi-Hsuan Tsai, Wei-Chih Hung, Hung-Yu\n  Tseng, Maneesh Singh, Ming-Hsuan Yang", "title": "Progressive Domain Adaptation for Object Detection", "comments": "Accepted in WACV'20. Code and models will be available at\n  https://github.com/kevinhkhsu/DA_detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning methods for object detection rely on a large amount of\nbounding box annotations. Collecting these annotations is laborious and costly,\nyet supervised models do not generalize well when testing on images from a\ndifferent distribution. Domain adaptation provides a solution by adapting\nexisting labels to the target testing data. However, a large gap between\ndomains could make adaptation a challenging task, which leads to unstable\ntraining processes and sub-optimal results. In this paper, we propose to bridge\nthe domain gap with an intermediate domain and progressively solve easier\nadaptation subtasks. This intermediate domain is constructed by translating the\nsource images to mimic the ones in the target domain. To tackle the\ndomain-shift problem, we adopt adversarial learning to align distributions at\nthe feature level. In addition, a weighted task loss is applied to deal with\nunbalanced image quality in the intermediate domain. Experimental results show\nthat our method performs favorably against the state-of-the-art method in terms\nof the performance on the target domain.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:55:04 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Hsu", "Han-Kai", ""], ["Yao", "Chun-Han", ""], ["Tsai", "Yi-Hsuan", ""], ["Hung", "Wei-Chih", ""], ["Tseng", "Hung-Yu", ""], ["Singh", "Maneesh", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1910.11322", "submitter": "Georgios Pavlakos", "authors": "Georgios Pavlakos, Nikos Kolotouros, Kostas Daniilidis", "title": "TexturePose: Supervising Human Mesh Estimation with Texture Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of model-based human pose estimation. Recent\napproaches have made significant progress towards regressing the parameters of\nparametric human body models directly from images. Because of the absence of\nimages with 3D shape ground truth, relevant approaches rely on 2D annotations\nor sophisticated architecture designs. In this work, we advocate that there are\nmore cues we can leverage, which are available for free in natural images,\ni.e., without getting more annotations, or modifying the network architecture.\nWe propose a natural form of supervision, that capitalizes on the appearance\nconstancy of a person among different frames (or viewpoints). This seemingly\ninsignificant and often overlooked cue goes a long way for model-based pose\nestimation. The parametric model we employ allows us to compute a texture map\nfor each frame. Assuming that the texture of the person does not change\ndramatically between frames, we can apply a novel texture consistency loss,\nwhich enforces that each point in the texture map has the same texture value\nacross all frames. Since the texture is transferred in this common texture map\nspace, no camera motion computation is necessary, or even an assumption of\nsmoothness among frames. This makes our proposed supervision applicable in a\nvariety of settings, ranging from monocular video, to multi-view images. We\nbenchmark our approach against strong baselines that require the same or even\nmore annotations that we do and we consistently outperform them.\nSimultaneously, we achieve state-of-the-art results among model-based pose\nestimation approaches in different benchmarks. The project website with videos,\nresults, and code can be found at\nhttps://seas.upenn.edu/~pavlakos/projects/texturepose.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:55:31 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Pavlakos", "Georgios", ""], ["Kolotouros", "Nikos", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1910.11328", "submitter": "Jia-Bin Huang", "authors": "Badour AlBahar and Jia-Bin Huang", "title": "Guided Image-to-Image Translation with Bi-Directional Feature\n  Transformation", "comments": "ICCV 2019 Code: https://github.com/vt-vl-lab/Guided-pix2pix Project\n  page: https://filebox.ece.vt.edu/~Badour/guided_pix2pix.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of guided image-to-image translation where we\ntranslate an input image into another while respecting the constraints provided\nby an external, user-provided guidance image. Various conditioning methods for\nleveraging the given guidance image have been explored, including input\nconcatenation , feature concatenation, and conditional affine transformation of\nfeature activations. All these conditioning mechanisms, however, are\nuni-directional, i.e., no information flow from the input image back to the\nguidance. To better utilize the constraints of the guidance image, we present a\nbi-directional feature transformation (bFT) scheme. We show that our bFT scheme\noutperforms other conditioning schemes and has comparable results to\nstate-of-the-art methods on different tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:58:47 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["AlBahar", "Badour", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "1910.11334", "submitter": "Yifei Xing", "authors": "Rudrasis Chakraborty, Yifei Xing, Stella Yu", "title": "SurReal: Complex-Valued Learning as Principled Transformations on a\n  Scaling and Rotation Manifold", "comments": "12 pages, accepted to TNNLS journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex-valued data is ubiquitous in signal and image processing\napplications, and complex-valued representations in deep learning have\nappealing theoretical properties. While these aspects have long been\nrecognized, complex-valued deep learning continues to lag far behind its\nreal-valued counterpart.\n  We propose a principled geometric approach to complex-valued deep learning.\nComplex-valued data could often be subject to arbitrary complex-valued scaling;\nas a result, real and imaginary components could co-vary. Instead of treating\ncomplex values as two independent channels of real values, we recognize their\nunderlying geometry: We model the space of complex numbers as a product\nmanifold of non-zero scaling and planar rotations. Arbitrary complex-valued\nscaling naturally becomes a group of transitive actions on this manifold.\n  We propose to extend the property instead of the form of real-valued\nfunctions to the complex domain. We define convolution as weighted Fr\\'echet\nmean on the manifold that is equivariant to the group of scaling/rotation\nactions, and define distance transform on the manifold that is invariant to the\naction group. The manifold perspective also allows us to define nonlinear\nactivation functions such as tangent ReLU and G-transport, as well as residual\nconnections on the manifold-valued data.\n  We dub our model SurReal, as our experiments on MSTAR and RadioML deliver\nhigh performance with only a fractional size of real-valued and complex-valued\nbaseline models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 20:36:29 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 15:40:49 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2020 19:05:06 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chakraborty", "Rudrasis", ""], ["Xing", "Yifei", ""], ["Yu", "Stella", ""]]}, {"id": "1910.11336", "submitter": "Orly Liba", "authors": "Orly Liba, Kiran Murthy, Yun-Ta Tsai, Tim Brooks, Tianfan Xue, Nikhil\n  Karnad, Qiurui He, Jonathan T. Barron, Dillon Sharlet, Ryan Geiss, Samuel W.\n  Hasinoff, Yael Pritch, Marc Levoy", "title": "Handheld Mobile Photography in Very Low Light", "comments": "22 pages, 27 figures", "journal-ref": "ACM Trans. Graph.38, 6, Article 164 (November 2019)", "doi": "10.1145/3355089.3356508", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Taking photographs in low light using a mobile phone is challenging and\nrarely produces pleasing results. Aside from the physical limits imposed by\nread noise and photon shot noise, these cameras are typically handheld, have\nsmall apertures and sensors, use mass-produced analog electronics that cannot\neasily be cooled, and are commonly used to photograph subjects that move, like\nchildren and pets. In this paper we describe a system for capturing clean,\nsharp, colorful photographs in light as low as 0.3~lux, where human vision\nbecomes monochromatic and indistinct. To permit handheld photography without\nflash illumination, we capture, align, and combine multiple frames. Our system\nemploys \"motion metering\", which uses an estimate of motion magnitudes (whether\ndue to handshake or moving objects) to identify the number of frames and the\nper-frame exposure times that together minimize both noise and motion blur in a\ncaptured burst. We combine these frames using robust alignment and merging\ntechniques that are specialized for high-noise imagery. To ensure accurate\ncolors in such low light, we employ a learning-based auto white balancing\nalgorithm. To prevent the photographs from looking like they were shot in\ndaylight, we use tone mapping techniques inspired by illusionistic painting:\nincreasing contrast, crushing shadows to black, and surrounding the scene with\ndarkness. All of these processes are performed using the limited computational\nresources of a mobile device. Our system can be used by novice photographers to\nproduce shareable pictures in a few seconds based on a single shutter press,\neven in environments so dim that humans cannot see clearly.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:21:08 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Liba", "Orly", ""], ["Murthy", "Kiran", ""], ["Tsai", "Yun-Ta", ""], ["Brooks", "Tim", ""], ["Xue", "Tianfan", ""], ["Karnad", "Nikhil", ""], ["He", "Qiurui", ""], ["Barron", "Jonathan T.", ""], ["Sharlet", "Dillon", ""], ["Geiss", "Ryan", ""], ["Hasinoff", "Samuel W.", ""], ["Pritch", "Yael", ""], ["Levoy", "Marc", ""]]}, {"id": "1910.11363", "submitter": "Vickram Rajendran", "authors": "Vickram Rajendran, William LeVine", "title": "Accurate Layerwise Interpretable Competence Estimation", "comments": "Proceedings of the 33rd Conference in Neural Information Processing\n  Systems (2019), 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating machine learning performance 'in the wild' is both an important\nand unsolved problem. In this paper, we seek to examine, understand, and\npredict the pointwise competence of classification models. Our contributions\nare twofold: First, we establish a statistically rigorous definition of\ncompetence that generalizes the common notion of classifier confidence; second,\nwe present the ALICE (Accurate Layerwise Interpretable Competence Estimation)\nScore, a pointwise competence estimator for any classifier. By considering\ndistributional, data, and model uncertainty, ALICE empirically shows accurate\ncompetence estimation in common failure situations such as class-imbalanced\ndatasets, out-of-distribution datasets, and poorly trained models. Our\ncontributions allow us to accurately predict the competence of any\nclassification model given any input and error function. We compare our score\nwith state-of-the-art confidence estimators such as model confidence and Trust\nScore, and show significant improvements in competence prediction over these\nmethods on datasets such as DIGITS, CIFAR10, and CIFAR100.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:10:35 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Rajendran", "Vickram", ""], ["LeVine", "William", ""]]}, {"id": "1910.11367", "submitter": "Sri Kalyan Yarlagadda", "authors": "Sri Kalyan Yarlagadda, Sriram Baireddy, David G\\\"uera, Carol J.\n  Boushey, Deborah A. Kerr, Fengqing Zhu", "title": "Learning eating environments through scene clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that dietary habits have a significant influence on health.\nWhile many studies have been conducted to understand this relationship, little\nis known about the relationship between eating environments and health. Yet\nresearchers and health agencies around the world have recognized the eating\nenvironment as a promising context for improving diet and health. In this\npaper, we propose an image clustering method to automatically extract the\neating environments from eating occasion images captured during a community\ndwelling dietary study. Specifically, we are interested in learning how many\ndifferent environments an individual consumes food in. Our method clusters\nimages by extracting features at both global and local scales using a deep\nneural network. The variation in the number of clusters and images captured by\ndifferent individual makes this a very challenging problem. Experimental\nresults show that our method performs significantly better compared to several\nexisting clustering approaches.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:16:11 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 01:19:56 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Yarlagadda", "Sri Kalyan", ""], ["Baireddy", "Sriram", ""], ["G\u00fcera", "David", ""], ["Boushey", "Carol J.", ""], ["Kerr", "Deborah A.", ""], ["Zhu", "Fengqing", ""]]}, {"id": "1910.11375", "submitter": "Gregory Meyer", "authors": "Gregory P. Meyer and Niranjan Thakurdesai", "title": "Learning an Uncertainty-Aware Object Detector for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capability to detect objects is a core part of autonomous driving. Due to\nsensor noise and incomplete data, perfectly detecting and localizing every\nobject is infeasible. Therefore, it is important for a detector to provide the\namount of uncertainty in each prediction. Providing the autonomous system with\nreliable uncertainties enables the vehicle to react differently based on the\nlevel of uncertainty. Previous work has estimated the uncertainty in a\ndetection by predicting a probability distribution over object bounding boxes.\nIn this work, we propose a method to improve the ability to learn the\nprobability distribution by considering the potential noise in the ground-truth\nlabeled data. Our proposed approach improves not only the accuracy of the\nlearned distribution but also the object detection performance.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:52:28 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 16:29:08 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Meyer", "Gregory P.", ""], ["Thakurdesai", "Niranjan", ""]]}, {"id": "1910.11414", "submitter": "Mario Malav\\'e", "authors": "Mario O. Malav\\'e, Corey A. Baron, Srivathsan P. Koundinyan,\n  Christopher M. Sandino, Frank Ong, Joseph Y. Cheng, and Dwight G. Nishimura", "title": "Reconstruction of Undersampled 3D Non-Cartesian Image-Based Navigators\n  for Coronary MRA Using an Unrolled Deep Learning Model", "comments": "34 pages, 5 figures, 1 table, 6 supporting figures, 1 supporting\n  table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To rapidly reconstruct undersampled 3D non-Cartesian image-based\nnavigators (iNAVs) using an unrolled deep learning (DL) model for non-rigid\nmotion correction in coronary magnetic resonance angiography (CMRA).\n  Methods: An unrolled network is trained to reconstruct beat-to-beat 3D iNAVs\nacquired as part of a CMRA sequence. The unrolled model incorporates a\nnon-uniform FFT operator to perform the data consistency operation, and the\nregularization term is learned by a convolutional neural network (CNN) based on\nthe proximal gradient descent algorithm. The training set includes 6,000 3D\niNAVs acquired from 7 different subjects and 11 scans using a variable-density\n(VD) cones trajectory. For testing, 3D iNAVs from 4 additional subjects are\nreconstructed using the unrolled model. To validate reconstruction accuracy,\nglobal and localized motion estimates from DL model-based 3D iNAVs are compared\nwith those extracted from 3D iNAVs reconstructed with $\\textit{l}_{1}$-ESPIRiT.\nThen, the high-resolution coronary MRA images motion corrected with\nautofocusing using the $\\textit{l}_{1}$-ESPIRiT and DL model-based 3D iNAVs are\nassessed for differences.\n  Results: 3D iNAVs reconstructed using the DL model-based approach and\nconventional $\\textit{l}_{1}$-ESPIRiT generate similar global and localized\nmotion estimates and provide equivalent coronary image quality. Reconstruction\nwith the unrolled network completes in a fraction of the time compared to CPU\nand GPU implementations of $\\textit{l}_{1}$-ESPIRiT (20x and 3x speed\nincreases, respectively).\n  Conclusion: We have developed a deep neural network architecture to\nreconstruct undersampled 3D non-Cartesian VD cones iNAVs. Our approach\ndecreases reconstruction time for 3D iNAVs, while preserving the accuracy of\nnon-rigid motion information offered by them for correction.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 20:27:59 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Malav\u00e9", "Mario O.", ""], ["Baron", "Corey A.", ""], ["Koundinyan", "Srivathsan P.", ""], ["Sandino", "Christopher M.", ""], ["Ong", "Frank", ""], ["Cheng", "Joseph Y.", ""], ["Nishimura", "Dwight G.", ""]]}, {"id": "1910.11432", "submitter": "Chengshu Li", "authors": "Chengshu Li, Fei Xia, Roberto Martin-Martin, Silvio Savarese", "title": "HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation\n  with Mobile Manipulators", "comments": "Conference on Robot Learning (CoRL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most common navigation tasks in human environments require auxiliary arm\ninteractions, e.g. opening doors, pressing buttons and pushing obstacles away.\nThis type of navigation tasks, which we call Interactive Navigation, requires\nthe use of mobile manipulators: mobile bases with manipulation capabilities.\nInteractive Navigation tasks are usually long-horizon and composed of\nheterogeneous phases of pure navigation, pure manipulation, and their\ncombination. Using the wrong part of the embodiment is inefficient and hinders\nprogress. We propose HRL4IN, a novel Hierarchical RL architecture for\nInteractive Navigation tasks. HRL4IN exploits the exploration benefits of HRL\nover flat RL for long-horizon tasks thanks to temporally extended commitments\ntowards subgoals. Different from other HRL solutions, HRL4IN handles the\nheterogeneous nature of the Interactive Navigation task by creating subgoals in\ndifferent spaces in different phases of the task. Moreover, HRL4IN selects\ndifferent parts of the embodiment to use for each phase, improving energy\nefficiency. We evaluate HRL4IN against flat PPO and HAC, a state-of-the-art HRL\nalgorithm, on Interactive Navigation in two environments - a 2D grid-world\nenvironment and a 3D environment with physics simulation. We show that HRL4IN\nsignificantly outperforms its baselines in terms of task performance and energy\nefficiency. More information is available at\nhttps://sites.google.com/view/hrl4in.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 21:34:29 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Li", "Chengshu", ""], ["Xia", "Fei", ""], ["Martin-Martin", "Roberto", ""], ["Savarese", "Silvio", ""]]}, {"id": "1910.11443", "submitter": "Abhineet Singh", "authors": "Abhineet Singh, Marcin Pietrasik, Gabriell Natha, Nehla Ghouaiel, Ken\n  Brizel and Nilanjan Ray", "title": "Animal Detection in Man-made Environments", "comments": "to appear in to WACV 2020, supplementary:\n  [http://webdocs.cs.ualberta.ca/~vis/asingh1/docs/animal_detection_supp.pdf],\n  demo: [https://youtu.be/ZkjcP8s0QVQ]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of animals that have strayed into human inhabited areas\nhas important security and road safety applications. This paper attempts to\nsolve this problem using deep learning techniques from a variety of computer\nvision fields including object detection, tracking, segmentation and edge\ndetection. Several interesting insights into transfer learning are elicited\nwhile adapting models trained on benchmark datasets for real world deployment.\nEmpirical evidence is presented to demonstrate the inability of detectors to\ngeneralize from training images of animals in their natural habitats to\ndeployment scenarios of man-made environments. A solution is also proposed\nusing semi-automated synthetic data generation for domain specific training.\nCode and data used in the experiments are made available to facilitate further\nwork in this domain.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 22:17:24 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 18:10:57 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Singh", "Abhineet", ""], ["Pietrasik", "Marcin", ""], ["Natha", "Gabriell", ""], ["Ghouaiel", "Nehla", ""], ["Brizel", "Ken", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1910.11456", "submitter": "Xi Fang", "authors": "Xi Fang, Bo Du, Sheng Xu, Bradford J. Wood, and Pingkun Yan", "title": "Unified Multi-scale Feature Abstraction for Medical Image Segmentation", "comments": "Abstract of SPIE Medical Imaging (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic medical image segmentation, an essential component of medical image\nanalysis, plays an importantrole in computer-aided diagnosis. For example,\nlocating and segmenting the liver can be very helpful in livercancer diagnosis\nand treatment. The state-of-the-art models in medical image segmentation are\nvariants ofthe encoder-decoder architecture such as fully convolutional network\n(FCN) and U-Net.1A major focus ofthe FCN based segmentation methods has been on\nnetwork structure engineering by incorporating the latestCNN structures such as\nResNet2and DenseNet.3In addition to exploring new network structures for\nefficientlyabstracting high level features, incorporating structures for\nmulti-scale image feature extraction in FCN hashelped to improve performance in\nsegmentation tasks. In this paper, we design a new multi-scale\nnetworkarchitecture, which takes multi-scale inputs with dedicated\nconvolutional paths to efficiently combine featuresfrom different scales to\nbetter utilize the hierarchical information.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 23:19:05 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Fang", "Xi", ""], ["Du", "Bo", ""], ["Xu", "Sheng", ""], ["Wood", "Bradford J.", ""], ["Yan", "Pingkun", ""]]}, {"id": "1910.11475", "submitter": "Weijiang Yu", "authors": "Weijiang Yu, Jingwen Zhou, Weihao Yu, Xiaodan Liang, Nong Xiao", "title": "Heterogeneous Graph Learning for Visual Commonsense Reasoning", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual commonsense reasoning task aims at leading the research field into\nsolving cognition-level reasoning with the ability of predicting correct\nanswers and meanwhile providing convincing reasoning paths, resulting in three\nsub-tasks i.e., Q->A, QA->R and Q->AR. It poses great challenges over the\nproper semantic alignment between vision and linguistic domains and knowledge\nreasoning to generate persuasive reasoning paths. Existing works either resort\nto a powerful end-to-end network that cannot produce interpretable reasoning\npaths or solely explore intra-relationship of visual objects (homogeneous\ngraph) while ignoring the cross-domain semantic alignment among visual concepts\nand linguistic words. In this paper, we propose a new Heterogeneous Graph\nLearning (HGL) framework for seamlessly integrating the intra-graph and\ninter-graph reasoning in order to bridge vision and language domain. Our HGL\nconsists of a primal vision-to-answer heterogeneous graph (VAHG) module and a\ndual question-to-answer heterogeneous graph (QAHG) module to interactively\nrefine reasoning paths for semantic agreement. Moreover, our HGL integrates a\ncontextual voting module to exploit a long-range visual context for better\nglobal reasoning. Experiments on the large-scale Visual Commonsense Reasoning\nbenchmark demonstrate the superior performance of our proposed modules on three\ntasks (improving 5% accuracy on Q->A, 3.5% on QA->R, 5.8% on Q->AR)\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 01:04:46 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Yu", "Weijiang", ""], ["Zhou", "Jingwen", ""], ["Yu", "Weihao", ""], ["Liang", "Xiaodan", ""], ["Xiao", "Nong", ""]]}, {"id": "1910.11481", "submitter": "Lingzhi Zhang", "authors": "Lingzhi Zhang, Jiancong Wang, Jianbo Shi", "title": "Multimodal Image Outpainting With Regularized Normalized Diversification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of generating a set ofrealistic and\ndiverse backgrounds when given only a smallforeground region. We refer to this\ntask as image outpaint-ing. The technical challenge of this task is to\nsynthesize notonly plausible but also diverse image outputs.\nTraditionalgenerative adversarial networks suffer from mode collapse.While\nrecent approaches propose to maximize orpreserve the pairwise distance between\ngenerated sampleswith respect to their latent distance, they do not\nexplicitlyprevent the diverse samples of different conditional inputsfrom\ncollapsing. Therefore, we propose a new regulariza-tion method to encourage\ndiverse sampling in conditionalsynthesis. In addition, we propose a feature\npyramid dis-criminator to improve the image quality. Our experimen-tal results\nshow that our model can produce more diverseimages without sacrificing visual\nquality compared to state-of-the-arts approaches in both the CelebA face\ndataset and the Cityscape scene dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 01:24:45 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Zhang", "Lingzhi", ""], ["Wang", "Jiancong", ""], ["Shi", "Jianbo", ""]]}, {"id": "1910.11482", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad, Naimul Khan", "title": "Human Action Recognition Using Deep Multilevel Multimodal (M2) Fusion of\n  Depth and Inertial Sensors", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal fusion frameworks for Human Action Recognition (HAR) using depth\nand inertial sensor data have been proposed over the years. In most of the\nexisting works, fusion is performed at a single level (feature level or\ndecision level), missing the opportunity to fuse rich mid-level features\nnecessary for better classification. To address this shortcoming, in this\npaper, we propose three novel deep multilevel multimodal fusion frameworks to\ncapitalize on different fusion strategies at various stages and to leverage the\nsuperiority of multilevel fusion. At input, we transform the depth data into\ndepth images called sequential front view images (SFIs) and inertial sensor\ndata into signal images. Each input modality, depth and inertial, is further\nmade multimodal by taking convolution with the Prewitt filter. Creating\n\"modality within modality\" enables further complementary and discriminative\nfeature extraction through Convolutional Neural Networks (CNNs). CNNs are\ntrained on input images of each modality to learn low-level, high-level and\ncomplex features. Learned features are extracted and fused at different stages\nof the proposed frameworks to combine discriminative and complementary\ninformation. These highly informative features are served as input to a\nmulti-class Support Vector Machine (SVM). We evaluate the proposed frameworks\non three publicly available multimodal HAR datasets, namely, UTD Multimodal\nHuman Action Dataset (MHAD), Berkeley MHAD, and UTD-MHAD Kinect V2.\nExperimental results show the supremacy of the proposed fusion frameworks over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 01:29:58 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Khan", "Naimul", ""]]}, {"id": "1910.11492", "submitter": "Vikas Ramachandra", "authors": "Vikas Ramachandra", "title": "Causal inference for climate change events from satellite image time\n  series using computer vision and deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for causal inference using satellite image time series,\nin order to determine the treatment effects of interventions which impact\nclimate change, such as deforestation. Simply put, the aim is to quantify the\n'before versus after' effect of climate related human driven interventions,\nsuch as urbanization; as well as natural disasters, such as hurricanes and\nforest fires. As a concrete example, we focus on quantifying forest tree cover\nchange/ deforestation due to human led causes. The proposed method involves the\nfollowing steps. First, we uae computer vision and machine learning/deep\nlearning techniques to detect and quantify forest tree coverage levels over\ntime, at every time epoch. We then look at this time series to identify\nchangepoints. Next, we estimate the expected (forest tree cover) values using a\nBayesian structural causal model and projecting/forecasting the counterfactual.\nThis is compared to the values actually observed post intervention, and the\ndifference in the two values gives us the effect of the intervention (as\ncompared to the non intervention scenario, i.e. what would have possibly\nhappened without the intervention). As a specific use case, we analyze\ndeforestation levels before and after the hyperinflation event (intervention)\nin Brazil (which ended in 1993-94), for the Amazon rainforest region, around\nRondonia, Brazil. For this deforestation use case, using our causal inference\nframework can help causally attribute change/reduction in forest tree cover and\nincreasing deforestation rates due to human activities at various points in\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 02:16:15 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Ramachandra", "Vikas", ""]]}, {"id": "1910.11495", "submitter": "Lingzhi Zhang", "authors": "Lingzhi Zhang, Tarmily Wen, Jianbo Shi", "title": "Deep Image Blending", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image composition is an important operation to create visual content. Among\nimage composition tasks, image blending aims to seamlessly blend an object from\na source image onto a target image with lightly mask adjustment. A popular\napproach is Poisson image blending, which enforces the gradient domain\nsmoothness in the composite image. However, this approach only considers the\nboundary pixels of target image, and thus can not adapt to texture of target\nimage. In addition, the colors of the target image often seep through the\noriginal source object too much causing a significant loss of content of the\nsource object. We propose a Poisson blending loss that achieves the same\npurpose of Poisson image blending. In addition, we jointly optimize the\nproposed Poisson blending loss as well as the style and content loss computed\nfrom a deep network, and reconstruct the blending region by iteratively\nupdating the pixels using the L-BFGS solver. In the blending image, we not only\nsmooth out gradient domain of the blending boundary but also add consistent\ntexture into the blending region. User studies show that our method outperforms\nstrong baselines as well as state-of-the-art approaches when placing objects\nonto both paintings and real-world images.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 02:23:36 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Zhang", "Lingzhi", ""], ["Wen", "Tarmily", ""], ["Shi", "Jianbo", ""]]}, {"id": "1910.11497", "submitter": "Diego L. Guarin", "authors": "Diego L. Guarin, Yana Yunusova, Babak Taati, Joseph R Dusseldorp,\n  Suresh Mohan, Joana Tavares, Martinus M. van Veen, Emily Fortier, Tessa A.\n  Hadlock, and Nate Jowett", "title": "Toward an Automatic System for Computer-Aided Assessment in Facial Palsy", "comments": "21 pages, 4 figures, 1 table", "journal-ref": null, "doi": "10.1089/fpsam.2019.29000.gua.", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance: Machine learning (ML) approaches to facial landmark localization\ncarry great clinical potential for quantitative assessment of facial function\nas they enable high-throughput automated quantification of relevant facial\nmetrics from photographs. However, translation from research settings to\nclinical applications requires important improvements. Objective: To develop an\nML algorithm for accurate facial landmarks localization in photographs of\nfacial palsy patients, and use it as part of an automated computer-aided\ndiagnosis system. Design, Setting, and Participants: Facial landmarks were\nmanually localized in portrait photographs of eight expressions obtained from\n200 facial palsy patients and 10 controls. A novel ML model for automated\nfacial landmark localization was trained using this disease-specific database.\nModel output was compared to manual annotations and the output of a model\ntrained using a larger database consisting only of healthy subjects. Model\naccuracy was evaluated by the normalized root mean square error (NRMSE) between\nalgorithms' prediction and manual annotations. Results: Publicly available\nalgorithms provide poor results when applied to patients compared to healthy\ncontrols (NRMSE, 8.56 +/- 2.16 vs. 7.09 +/- 2.34, p << 0.01). We found\nsignificant improvement in facial landmark localization accuracy for the\nclinical population when using a model trained with a relatively small number\npatients' photographs (1440) compared to a model trained using several thousand\nmore images of healthy faces (NRMSE, 6.03 +/- 2.43 vs. 8.56 +/- 2.16, p <<\n0.01). Conclusions: Retraining a landmark detection model with a small number\nof clinical images significantly improved landmark detection performance in\nfrontal view photographs of the clinical population. These results represent\nthe first steps towards an automatic system for computer-aided assessment in\nfacial palsy.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 02:28:22 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Guarin", "Diego L.", ""], ["Yunusova", "Yana", ""], ["Taati", "Babak", ""], ["Dusseldorp", "Joseph R", ""], ["Mohan", "Suresh", ""], ["Tavares", "Joana", ""], ["van Veen", "Martinus M.", ""], ["Fortier", "Emily", ""], ["Hadlock", "Tessa A.", ""], ["Jowett", "Nate", ""]]}, {"id": "1910.11506", "submitter": "Quan Huu Cap", "authors": "Katsumasa Suwa, Quan Huu Cap, Ryunosuke Kotani, Hiroyuki Uga, Satoshi\n  Kagiwada and Hitoshi Iyatomi", "title": "A comparable study: Intrinsic difficulties of practical plant diagnosis\n  from wide-angle images", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical automated detection and diagnosis of plant disease from wide-angle\nimages (i.e. in-field images containing multiple leaves using a fixed-position\ncamera) is a very important application for large-scale farm management, in\nview of the need to ensure global food security. However, developing automated\nsystems for disease diagnosis is often difficult, because labeling a reliable\nwide-angle disease dataset from actual field images is very laborious. In\naddition, the potential similarities between the training and test data lead to\na serious problem of model overfitting. In this paper, we investigate changes\nin performance when applying disease diagnosis systems to different scenarios\ninvolving wide-angle cucumber test data captured on real farms, and propose an\neffective diagnostic strategy. We show that leading object recognition\ntechniques such as SSD and Faster R-CNN achieve excellent end-to-end disease\ndiagnostic performance only for a test dataset that is collected from the same\npopulation as the training dataset (with F1-score of 81.5% - 84.1% for\ndiagnosed cases of disease), but their performance markedly deteriorates for a\ncompletely different test dataset (with F1-score of 4.4 - 6.2%). In contrast,\nour proposed two-stage systems using independent leaf detection and leaf\ndiagnosis stages attain a promising disease diagnostic performance that is more\nthan six times higher than end-to-end systems (with F1-score of 33.4 - 38.9%)\non an unseen target dataset. We also confirm the efficiency of our proposal\nbased on visual assessment, concluding that a two-stage model is a suitable and\nreasonable choice for practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 03:03:34 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 10:46:39 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Suwa", "Katsumasa", ""], ["Cap", "Quan Huu", ""], ["Kotani", "Ryunosuke", ""], ["Uga", "Hiroyuki", ""], ["Kagiwada", "Satoshi", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "1910.11509", "submitter": "Wassim Bouachir", "authors": "Imanne El Maachi, Guillaume-Alexandre Bilodeau, Wassim Bouachir", "title": "Deep 1D-Convnet for accurate Parkinson disease detection and severity\n  prediction from gait", "comments": "Source code available at\n  https://github.com/imanneelmaachi/Parkinson-disease-detection-and-severity-prediction-from-gait", "journal-ref": "Expert Systems with Applications, 113075 (2019)", "doi": "10.1016/j.eswa.2019.113075", "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosing Parkinson's disease is a complex task that requires the evaluation\nof several motor and non-motor symptoms. During diagnosis, gait abnormalities\nare among the important symptoms that physicians should consider. However, gait\nevaluation is challenging and relies on the expertise and subjectivity of\nclinicians. In this context, the use of an intelligent gait analysis algorithm\nmay assist physicians in order to facilitate the diagnosis process. This paper\nproposes a novel intelligent Parkinson detection system based on deep learning\ntechniques to analyze gait information. We used 1D convolutional neural network\n(1D-Convnet) to build a Deep Neural Network (DNN) classifier. The proposed\nmodel processes 18 1D-signals coming from foot sensors measuring the vertical\nground reaction force (VGRF). The first part of the network consists of 18\nparallel 1D-Convnet corresponding to system inputs. The second part is a fully\nconnected network that connects the concatenated outputs of the 1D-Convnets to\nobtain a final classification. We tested our algorithm in Parkinson's detection\nand in the prediction of the severity of the disease with the Unified\nParkinson's Disease Rating Scale (UPDRS). Our experiments demonstrate the high\nefficiency of the proposed method in the detection of Parkinson disease based\non gait data. The proposed algorithm achieved an accuracy of 98.7 %. To our\nknowledge, this is the state-of-the-start performance in Parkinson's gait\nrecognition. Furthermore, we achieved an accuracy of 85.3 % in Parkinson's\nseverity prediction. To the best of our knowledge, this is the first algorithm\nto perform a severity prediction based on the UPDRS. Our results show that the\nmodel is able to learn intrinsic characteristics from gait data and to\ngeneralize to unseen subjects, which could be helpful in a clinical diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 03:14:54 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 02:03:44 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 02:23:21 GMT"}, {"version": "v4", "created": "Sat, 16 May 2020 18:50:52 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Maachi", "Imanne El", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Bouachir", "Wassim", ""]]}, {"id": "1910.11515", "submitter": "Xuesong Niu", "authors": "Xuesong Niu and Shiguang Shan and Hu Han and Xilin Chen", "title": "RhythmNet: End-to-end Heart Rate Estimation from Face via\n  Spatial-temporal Representation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2947204", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart rate (HR) is an important physiological signal that reflects the\nphysical and emotional status of a person. Traditional HR measurements usually\nrely on contact monitors, which may cause inconvenience and discomfort.\nRecently, some methods have been proposed for remote HR estimation from face\nvideos; however, most of them focus on well-controlled scenarios, their\ngeneralization ability into less-constrained scenarios (e.g., with head\nmovement, and bad illumination) are not known. At the same time, lacking\nlarge-scale HR databases has limited the use of deep models for remote HR\nestimation. In this paper, we propose an end-to-end RhythmNet for remote HR\nestimation from the face. In RyhthmNet, we use a spatial-temporal\nrepresentation encoding the HR signals from multiple ROI volumes as its input.\nThen the spatial-temporal representations are fed into a convolutional network\nfor HR estimation. We also take into account the relationship of adjacent HR\nmeasurements from a video sequence via Gated Recurrent Unit (GRU) and achieves\nefficient HR measurement. In addition, we build a large-scale multi-modal HR\ndatabase (named as VIPL-HR, available at\n'http://vipl.ict.ac.cn/view_database.php?id=15'), which contains 2,378 visible\nlight videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Our\nVIPL-HR database contains various variations such as head movements,\nillumination variations, and acquisition device changes, replicating a\nless-constrained scenario for HR estimation. The proposed approach outperforms\nthe state-of-the-art methods on both the public-domain and our VIPL-HR\ndatabases.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 04:03:41 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 06:23:47 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Niu", "Xuesong", ""], ["Shan", "Shiguang", ""], ["Han", "Hu", ""], ["Chen", "Xilin", ""]]}, {"id": "1910.11534", "submitter": "Yusuke Niitani", "authors": "Yusuke Niitani, Toru Ogawa, Shuji Suzuki, Takuya Akiba, Tommi Kerola,\n  Kohei Ozaki, Shotaro Sano", "title": "Team PFDet's Methods for Open Images Challenge 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the instance segmentation and the object detection method used by\nteam PFDet for Open Images Challenge 2019. We tackle a massive dataset size,\nhuge class imbalance and federated annotations. Using this method, the team\nPFDet achieved 3rd and 4th place in the instance segmentation and the object\ndetection track, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 05:28:36 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Niitani", "Yusuke", ""], ["Ogawa", "Toru", ""], ["Suzuki", "Shuji", ""], ["Akiba", "Takuya", ""], ["Kerola", "Tommi", ""], ["Ozaki", "Kohei", ""], ["Sano", "Shotaro", ""]]}, {"id": "1910.11535", "submitter": "Haodong Duan", "authors": "Haodong Duan, KwanYee Lin, Sheng Jin, Wentao Liu, Chen Qian, Wanli\n  Ouyang", "title": "TRB: A Novel Triplet Representation for Understanding 2D Human Body", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose and shape are two important components of 2D human body. However,\nhow to efficiently represent both of them in images is still an open question.\nIn this paper, we propose the Triplet Representation for Body (TRB) -- a\ncompact 2D human body representation, with skeleton keypoints capturing human\npose information and contour keypoints containing human shape information. TRB\nnot only preserves the flexibility of skeleton keypoint representation, but\nalso contains rich pose and human shape information. Therefore, it promises\nbroader application areas, such as human shape editing and conditional image\ngeneration. We further introduce the challenging problem of TRB estimation,\nwhere joint learning of human pose and shape is required. We construct several\nlarge-scale TRB estimation datasets, based on popular 2D pose datasets: LSP,\nMPII, COCO. To effectively solve TRB estimation, we propose a two-branch\nnetwork (TRB-net) with three novel techniques, namely X-structure (Xs),\nDirectional Convolution (DC) and Pairwise Mapping (PM), to enforce multi-level\nmessage passing for joint feature learning. We evaluate our proposed TRB-net\nand several leading approaches on our proposed TRB datasets, and demonstrate\nthe superiority of our method through extensive evaluations.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 05:35:21 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Duan", "Haodong", ""], ["Lin", "KwanYee", ""], ["Jin", "Sheng", ""], ["Liu", "Wentao", ""], ["Qian", "Chen", ""], ["Ouyang", "Wanli", ""]]}, {"id": "1910.11547", "submitter": "Yiheng Liu", "authors": "Yiheng Liu, Wengang Zhou, Jianzhuang Liu, Guojun Qi, Qi Tian, Houqiang\n  Li", "title": "An End-to-End Foreground-Aware Network for Person Re-Identification", "comments": "Accepted to IEEE Transactions on Image Processing (TIP), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is a crucial task of identifying pedestrians of\ninterest across multiple surveillance camera views. In person\nre-identification, a pedestrian is usually represented with features extracted\nfrom a rectangular image region that inevitably contains the scene background,\nwhich incurs ambiguity to distinguish different pedestrians and degrades the\naccuracy. To this end, we propose an end-to-end foreground-aware network to\ndiscriminate foreground from background by learning a soft mask for person\nre-identification. In our method, in addition to the pedestrian ID as\nsupervision for foreground, we introduce the camera ID of each pedestrian image\nfor background modeling. The foreground branch and the background branch are\noptimized collaboratively. By presenting a target attention loss, the\npedestrian features extracted from the foreground branch become more\ninsensitive to the backgrounds, which greatly reduces the negative impacts of\nchanging backgrounds on matching an identical across different camera views.\nNotably, in contrast to existing methods, our approach does not require any\nadditional dataset to train a human landmark detector or a segmentation model\nfor locating the background regions. The experimental results conducted on\nthree challenging datasets, i.e., Market-1501, DukeMTMC-reID, and MSMT17,\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 06:43:19 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 09:42:43 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liu", "Yiheng", ""], ["Zhou", "Wengang", ""], ["Liu", "Jianzhuang", ""], ["Qi", "Guojun", ""], ["Tian", "Qi", ""], ["Li", "Houqiang", ""]]}, {"id": "1910.11560", "submitter": "Qiaokang Xie", "authors": "Qiaokang Xie, Wengang Zhou, Guo-Jun Qi, Qi Tian and Houqiang Li", "title": "Progressive Unsupervised Person Re-identification by Tracklet\n  Association with Spatio-Temporal Regularization", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for person re-identification (Re-ID) are mostly based on\nsupervised learning which requires numerous manually labeled samples across all\ncamera views for training. Such a paradigm suffers the scalability issue since\nin real-world Re-ID application, it is difficult to exhaustively label abundant\nidentities over multiple disjoint camera views. To this end, we propose a\nprogressive deep learning method for unsupervised person Re-ID in the wild by\nTracklet Association with Spatio-Temporal Regularization (TASTR). In our\napproach, we first collect tracklet data within each camera by automatic person\ndetection and tracking. Then, an initial Re-ID model is trained based on\nwithin-camera triplet construction for person representation learning. After\nthat, based on the person visual feature and spatio-temporal constraint, we\nassociate cross-camera tracklets to generate cross-camera triplets and update\nthe Re-ID model. Lastly, with the refined Re-ID model, better visual feature of\nperson can be extracted, which further promote the association of cross-camera\ntracklets. The last two steps are iterated multiple times to progressively\nupgrade the Re-ID model.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 07:49:03 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Xie", "Qiaokang", ""], ["Zhou", "Wengang", ""], ["Qi", "Guo-Jun", ""], ["Tian", "Qi", ""], ["Li", "Houqiang", ""]]}, {"id": "1910.11563", "submitter": "Jian Li", "authors": "Jian Li, Yan Wang, Xiubao Zhang, Weihong Deng, Haifeng Shen", "title": "Metric Classification Network in Actual Face Recognition Scene", "comments": "arXiv admin note: text overlap with arXiv:1504.03641 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to make facial features more discriminative, some new models have\nrecently been proposed. However, almost all of these models use the traditional\nface verification method, where the cosine operation is performed using the\nfeatures of the bottleneck layer output. However, each of these models needs to\nchange a threshold each time it is operated on a different test set. This is\nvery inappropriate for application in real-world scenarios. In this paper, we\ntrain a validation classifier to normalize the decision threshold, which means\nthat the result can be obtained directly without replacing the threshold. We\nrefer to our model as validation classifier, which achieves best result on the\nstructure consisting of one convolution layer and six fully connected layers.\nTo test our approach, we conduct extensive experiments on Labeled Face in the\nWild (LFW) and Youtube Faces (YTF), and the relative error reduction is 25.37%\nand 26.60% than traditional method respectively. These experiments confirm the\neffectiveness of validation classifier on face recognition task.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 07:58:10 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Li", "Jian", ""], ["Wang", "Yan", ""], ["Zhang", "Xiubao", ""], ["Deng", "Weihong", ""], ["Shen", "Haifeng", ""]]}, {"id": "1910.11577", "submitter": "Wei Yu", "authors": "Wei Yu, Yichao Lu, Steve Easterbrook, Sanja Fidler", "title": "CrevNet: Conditionally Reversible Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying resolution-preserving blocks is a common practice to maximize\ninformation preservation in video prediction, yet their high memory consumption\ngreatly limits their application scenarios. We propose CrevNet, a Conditionally\nReversible Network that uses reversible architectures to build a bijective\ntwo-way autoencoder and its complementary recurrent predictor. Our model enjoys\nthe theoretically guaranteed property of no information loss during the feature\nextraction, much lower memory consumption and computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 08:59:32 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Yu", "Wei", ""], ["Lu", "Yichao", ""], ["Easterbrook", "Steve", ""], ["Fidler", "Sanja", ""]]}, {"id": "1910.11605", "submitter": "Koyel Mukherjee", "authors": "Koyel Mukherjee, Alind Khare, Ashish Verma", "title": "A Simple Dynamic Learning Rate Tuning Algorithm For Automated Training\n  of DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks on image datasets generally require extensive\nexperimentation to find the optimal learning rate regime. Especially, for the\ncases of adversarial training or for training a newly synthesized model, one\nwould not know the best learning rate regime beforehand. We propose an\nautomated algorithm for determining the learning rate trajectory, that works\nacross datasets and models for both natural and adversarial training, without\nrequiring any dataset/model specific tuning. It is a stand-alone,\nparameterless, adaptive approach with no computational overhead. We\ntheoretically discuss the algorithm's convergence behavior. We empirically\nvalidate our algorithm extensively. Our results show that our proposed approach\n\\emph{consistently} achieves top-level accuracy compared to SOTA baselines in\nthe literature in natural as well as adversarial training.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 10:23:12 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Mukherjee", "Koyel", ""], ["Khare", "Alind", ""], ["Verma", "Ashish", ""]]}, {"id": "1910.11609", "submitter": "Li Lyna Zhang", "authors": "Li Lyna Zhang, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, Yunxin Liu", "title": "Fast Hardware-Aware Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing accurate and efficient convolutional neural architectures for vast\namount of hardware is challenging because hardware designs are complex and\ndiverse. This paper addresses the hardware diversity challenge in Neural\nArchitecture Search (NAS). Unlike previous approaches that apply search\nalgorithms on a small, human-designed search space without considering hardware\ndiversity, we propose HURRICANE that explores the automatic hardware-aware\nsearch over a much larger search space and a two-stage search algorithm, to\nefficiently generate tailored models for different types of hardware. Extensive\nexperiments on ImageNet demonstrate that our algorithm outperforms\nstate-of-the-art hardware-aware NAS methods under the same latency constraint\non three types of hardware. Moreover, the discovered architectures achieve much\nlower latency and higher accuracy than current state-of-the-art efficient\nmodels. Remarkably, HURRICANE achieves a 76.67% top-1 accuracy on ImageNet with\na inference latency of only 16.5 ms for DSP, which is a 3.47% higher accuracy\nand a 6.35x inference speedup than FBNet-iPhoneX, respectively. For VPU, we\nachieve a 0.53% higher top-1 accuracy than Proxyless-mobile with a 1.49x\nspeedup. Even for well-studied mobile CPU, we achieve a 1.63% higher top-1\naccuracy than FBNet-iPhoneX with a comparable inference latency. HURRICANE also\nreduces the training time by 30.4% compared to SPOS.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 10:40:57 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 02:50:26 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 02:05:46 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zhang", "Li Lyna", ""], ["Yang", "Yuqing", ""], ["Jiang", "Yuhang", ""], ["Zhu", "Wenwu", ""], ["Liu", "Yunxin", ""]]}, {"id": "1910.11626", "submitter": "David Bau iii", "authors": "David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik\n  Strobelt, Bolei Zhou, Antonio Torralba", "title": "Seeing What a GAN Cannot Generate", "comments": "ICCV 2019 oral; http://ganseeing.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of Generative Adversarial Networks (GANs), mode collapse\nremains a serious issue during GAN training. To date, little work has focused\non understanding and quantifying which modes have been dropped by a model. In\nthis work, we visualize mode collapse at both the distribution level and the\ninstance level. First, we deploy a semantic segmentation network to compare the\ndistribution of segmented objects in the generated images with the target\ndistribution in the training set. Differences in statistics reveal object\nclasses that are omitted by a GAN. Second, given the identified omitted object\nclasses, we visualize the GAN's omissions directly. In particular, we compare\nspecific differences between individual photos and their approximate inversions\nby a GAN. To this end, we relax the problem of inversion and solve the\ntractable problem of inverting a GAN layer instead of the entire generator.\nFinally, we use this framework to analyze several recent GANs trained on\nmultiple datasets and identify their typical failure cases.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:56:04 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Bau", "David", ""], ["Zhu", "Jun-Yan", ""], ["Wulff", "Jonas", ""], ["Peebles", "William", ""], ["Strobelt", "Hendrik", ""], ["Zhou", "Bolei", ""], ["Torralba", "Antonio", ""]]}, {"id": "1910.11631", "submitter": "Mikel Bober-Irizar", "authors": "Mikel Bober-Irizar, Miha Skalic, David Austin", "title": "Learning to Localize Temporal Events in Large-scale Video Data", "comments": "ICCV 2019, 3rd Youtube-8M Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address temporal localization of events in large-scale video data, in the\ncontext of the Youtube-8M Segments dataset. This emerging field within video\nrecognition can enable applications to identify the precise time a specified\nevent occurs in a video, which has broad implications for video search. To\naddress this we present two separate approaches: (1) a gradient boosted\ndecision tree model on a crafted dataset and (2) a combination of deep learning\nmodels based on frame-level data, video-level data, and a localization model.\nThe combinations of these two approaches achieved 5th place in the 3rd\nYoutube-8M video recognition challenge.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 11:40:29 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Bober-Irizar", "Mikel", ""], ["Skalic", "Miha", ""], ["Austin", "David", ""]]}, {"id": "1910.11637", "submitter": "Daniele Giunchi", "authors": "Daniele Giunchi, Stuart james, Donald Degraen, Anthony Steed", "title": "Mixing realities for sketch retrieval in Virtual Reality", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing tools for Virtual Reality (VR) enable users to model 3D designs from\nwithin the virtual environment itself. These tools employ sketching and\nsculpting techniques known from desktop-based interfaces and apply them to\nhand-based controller interaction. While these techniques allow for mid-air\nsketching of basic shapes, it remains difficult for users to create detailed\nand comprehensive 3D models. In our work, we focus on supporting the user in\ndesigning the virtual environment around them by enhancing sketch-based\ninterfaces with a supporting system for interactive model retrieval. Through\nsketching, an immersed user can query a database containing detailed 3D models\nand replace them into the virtual environment. To understand supportive\nsketching within a virtual environment, we compare different methods of sketch\ninteraction, i.e., 3D mid-air sketching, 2D sketching on a virtual tablet, 2D\nsketching on a fixed virtual whiteboard, and 2D sketching on a real tablet.\n%using a 2D physical tablet, a 2D virtual tablet, a 2D virtual whiteboard, and\n3D mid-air sketching. Our results show that 3D mid-air sketching is considered\nto be a more intuitive method to search a collection of models while the\naddition of physical devices creates confusion due to the complications of\ntheir inclusion within a virtual environment. While we pose our work as a\nretrieval problem for 3D models of chairs, our results can be extrapolated to\nother sketching tasks for virtual environments.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 11:52:25 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 09:58:24 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Giunchi", "Daniele", ""], ["james", "Stuart", ""], ["Degraen", "Donald", ""], ["Steed", "Anthony", ""]]}, {"id": "1910.11645", "submitter": "Hyeonseob Nam", "authors": "Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, Donggeun Yoo", "title": "Reducing Domain Gap by Reducing Style Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) often fail to maintain their performance\nwhen they confront new test domains, which is known as the problem of domain\nshift. Recent studies suggest that one of the main causes of this problem is\nCNNs' strong inductive bias towards image styles (i.e. textures) which are\nsensitive to domain changes, rather than contents (i.e. shapes). Inspired by\nthis, we propose to reduce the intrinsic style bias of CNNs to close the gap\nbetween domains. Our Style-Agnostic Networks (SagNets) disentangle style\nencodings from class categories to prevent style biased predictions and focus\nmore on the contents. Extensive experiments show that our method effectively\nreduces the style bias and makes the model more robust under domain shift. It\nachieves remarkable performance improvements in a wide range of cross-domain\ntasks including domain generalization, unsupervised domain adaptation, and\nsemi-supervised domain adaptation on multiple datasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 12:19:11 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 12:36:15 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 10:21:05 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 02:35:47 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Nam", "Hyeonseob", ""], ["Lee", "HyunJae", ""], ["Park", "Jongchan", ""], ["Yoon", "Wonjun", ""], ["Yoo", "Donggeun", ""]]}, {"id": "1910.11656", "submitter": "Yifei Yang", "authors": "Shizhou Zhang, Yifei Yang, Peng Wang, Xiuwei Zhang and Yanning Zhang", "title": "Attend to the Difference: Cross-Modality Person Re-identification via\n  Contrastive Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of cross-modality person re-identification has been receiving\nincreasing attention recently, due to its practical significance. Motivated by\nthe fact that human usually attend to the difference when they compare two\nsimilar objects, we propose a dual-path cross-modality feature learning\nframework which preserves intrinsic spatial strictures and attends to the\ndifference of input cross-modality image pairs. Our framework is composed by\ntwo main components: a Dual-path Spatial-structure-preserving Common Space\nNetwork (DSCSN) and a Contrastive Correlation Network (CCN). The former embeds\ncross-modality images into a common 3D tensor space without losing spatial\nstructures, while the latter extracts contrastive features by dynamically\ncomparing input image pairs. Note that the representations generated for the\ninput RGB and Infrared images are mutually dependant to each other. We conduct\nextensive experiments on two public available RGB-IR ReID datasets, SYSU-MM01\nand RegDB, and our proposed method outperforms state-of-the-art algorithms by a\nlarge margin with both full and simplified evaluation modes.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 12:36:47 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Zhang", "Shizhou", ""], ["Yang", "Yifei", ""], ["Wang", "Peng", ""], ["Zhang", "Xiuwei", ""], ["Zhang", "Yanning", ""]]}, {"id": "1910.11667", "submitter": "Anurag Ranjan", "authors": "Anurag Ranjan and David T. Hoffmann and Dimitrios Tzionas and Siyu\n  Tang and Javier Romero and Michael J. Black", "title": "Learning Multi-Human Optical Flow", "comments": "arXiv admin note: text overlap with arXiv:1806.05666", "journal-ref": "International Journal of Computer Vision (IJCV) 2019", "doi": "10.1007/s11263-019-01279-w", "report-no": "2019", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optical flow of humans is well known to be useful for the analysis of\nhuman action. Recent optical flow methods focus on training deep networks to\napproach the problem. However, the training data used by them does not cover\nthe domain of human motion. Therefore, we develop a dataset of multi-human\noptical flow and train optical flow networks on this dataset. We use a 3D model\nof the human body and motion capture data to synthesize realistic flow fields\nin both single- and multi-person images. We then train optical flow networks to\nestimate human flow fields from pairs of images. We demonstrate that our\ntrained networks are more accurate than a wide range of top methods on held-out\ntest data and that they can generalize well to real image sequences. The code,\ntrained models and the dataset are available for research.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:44:46 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 10:48:45 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Ranjan", "Anurag", ""], ["Hoffmann", "David T.", ""], ["Tzionas", "Dimitrios", ""], ["Tang", "Siyu", ""], ["Romero", "Javier", ""], ["Black", "Michael J.", ""]]}, {"id": "1910.11669", "submitter": "Mia Kokic", "authors": "Mia Kokic, Danica Kragic, Jeannette Bohg", "title": "Learning Task-Oriented Grasping from Human Activity Datasets", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters 5 (2020) 3352-3359", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to leverage a real-world, human activity RGB dataset to teach a\nrobot Task-Oriented Grasping (TOG). We develop a model that takes as input an\nRGB image and outputs a hand pose and configuration as well as an object pose\nand a shape. We follow the insight that jointly estimating hand and object\nposes increases accuracy compared to estimating these quantities independently\nof each other. Given the trained model, we process an RGB dataset to\nautomatically obtain the data to train a TOG model. This model takes as input\nan object point cloud and outputs a suitable region for task-specific grasping.\nOur ablation study shows that training an object pose predictor with the hand\npose information (and vice versa) is better than training without this\ninformation. Furthermore, our results on a real-world dataset show the\napplicability and competitiveness of our method over state-of-the-art.\nExperiments with a robot demonstrate that our method can allow a robot to\npreform TOG on novel objects.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 12:52:40 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 08:06:53 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Kokic", "Mia", ""], ["Kragic", "Danica", ""], ["Bohg", "Jeannette", ""]]}, {"id": "1910.11670", "submitter": "Ashvin Nair", "authors": "Ashvin Nair, Shikhar Bahl, Alexander Khazatsky, Vitchyr Pong, Glen\n  Berseth, Sergey Levine", "title": "Contextual Imagined Goals for Self-Supervised Robotic Learning", "comments": "12 pages, to be presented at Conference on Robot Learning (CoRL)\n  2019. Project website: https://ccrig.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While reinforcement learning provides an appealing formalism for learning\nindividual skills, a general-purpose robotic system must be able to master an\nextensive repertoire of behaviors. Instead of learning a large collection of\nskills individually, can we instead enable a robot to propose and practice its\nown behaviors automatically, learning about the affordances and behaviors that\nit can perform in its environment, such that it can then repurpose this\nknowledge once a new task is commanded by the user? In this paper, we study\nthis question in the context of self-supervised goal-conditioned reinforcement\nlearning. A central challenge in this learning regime is the problem of goal\nsetting: in order to practice useful skills, the robot must be able to\nautonomously set goals that are feasible but diverse. When the robot's\nenvironment and available objects vary, as they do in most open-world settings,\nthe robot must propose to itself only those goals that it can accomplish in its\npresent setting with the objects that are at hand. Previous work only studies\nself-supervised goal-conditioned RL in a single-environment setting, where goal\nproposals come from the robot's past experience or a generative model are\nsufficient. In more diverse settings, this frequently leads to impossible goals\nand, as we show experimentally, prevents effective learning. We propose a\nconditional goal-setting model that aims to propose goals that are feasible\nfrom the robot's current state. We demonstrate that this enables\nself-supervised goal-conditioned off-policy learning with raw image\nobservations in the real world, enabling a robot to manipulate a variety of\nobjects and generalize to new objects that were not seen during training.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 18:00:18 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Nair", "Ashvin", ""], ["Bahl", "Shikhar", ""], ["Khazatsky", "Alexander", ""], ["Pong", "Vitchyr", ""], ["Berseth", "Glen", ""], ["Levine", "Sergey", ""]]}, {"id": "1910.11671", "submitter": "Xingxing Zhang", "authors": "Xingxing Zhang, Shupeng Gui, Zhenfeng Zhu, Yao Zhao, Ji Liu", "title": "Hierarchical Prototype Learning for Zero-Shot Recognition", "comments": "This manuscript has been accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) has received extensive attention and successes in\nrecent years especially in areas of fine-grained object recognition, retrieval,\nand image captioning. Key to ZSL is to transfer knowledge from the seen to the\nunseen classes via auxiliary semantic prototypes (e.g., word or attribute\nvectors). However, the popularly learned projection functions in previous works\ncannot generalize well due to non-visual components included in semantic\nprototypes. Besides, the incompleteness of provided prototypes and captured\nimages has less been considered by the state-of-the-art approaches in ZSL. In\nthis paper, we propose a hierarchical prototype learning formulation to provide\na systematical solution (named HPL) for zero-shot recognition. Specifically,\nHPL is able to obtain discriminability on both seen and unseen class domains by\nlearning visual prototypes respectively under the transductive setting. To\nnarrow the gap of two domains, we further learn the interpretable\nsuper-prototypes in both visual and semantic spaces. Meanwhile, the two spaces\nare further bridged by maximizing their structural consistency. This not only\nfacilitates the representativeness of visual prototypes, but also alleviates\nthe loss of information of semantic prototypes. An extensive group of\nexperiments are then carefully designed and presented, demonstrating that HPL\nobtains remarkably more favorable efficiency and effectiveness, over currently\navailable alternatives under various settings.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 07:26:30 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 07:27:03 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Zhang", "Xingxing", ""], ["Gui", "Shupeng", ""], ["Zhu", "Zhenfeng", ""], ["Zhao", "Yao", ""], ["Liu", "Ji", ""]]}, {"id": "1910.11713", "submitter": "Fatih Can Kurnaz", "authors": "Fatih Can Kurnaz, Burak Hocao\\u{g}lu, Mert Kaan Y{\\i}lmaz, \\.Idil\n  S\\\"ulo, and Sinan Kalkan (KOVAN Research Lab, Dept. of Computer Engineering,\n  Middle East Technical University, Ankara, Turkey)", "title": "ALET (Automated Labeling of Equipment and Tools): A Dataset, a Baseline\n  and a Usecase for Tool Detection in the Wild", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots collaborating with humans in realistic environments will need to be\nable to detect the tools that can be used and manipulated. However, there is no\navailable dataset or study that addresses this challenge in real settings. In\nthis paper, we fill this gap by providing an extensive dataset (METU-ALET) for\ndetecting farming, gardening, office, stonemasonry, vehicle, woodworking and\nworkshop tools. The scenes correspond to sophisticated environments with or\nwithout humans using the tools. The scenes we consider introduce several\nchallenges for object detection, including the small scale of the tools, their\narticulated nature, occlusion, inter-class invariance, etc. Moreover, we train\nand compare several state of the art deep object detectors (including Faster\nR-CNN, Cascade R-CNN, RepPoint and RetinaNet) on our dataset. We observe that\nthe detectors have difficulty in detecting especially small-scale tools or\ntools that are visually similar to parts of other tools. This in turn supports\nthe importance of our dataset and paper. With the dataset, the code and the\ntrained models, our work provides a basis for further research into tools and\ntheir use in robotics applications.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 13:29:10 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 20:05:29 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 15:31:44 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kurnaz", "Fatih Can", "", "KOVAN Research Lab, Dept. of Computer Engineering,\n  Middle East Technical University, Ankara, Turkey"], ["Hocao\u011flu", "Burak", "", "KOVAN Research Lab, Dept. of Computer Engineering,\n  Middle East Technical University, Ankara, Turkey"], ["Y\u0131lmaz", "Mert Kaan", "", "KOVAN Research Lab, Dept. of Computer Engineering,\n  Middle East Technical University, Ankara, Turkey"], ["S\u00fclo", "\u0130dil", "", "KOVAN Research Lab, Dept. of Computer Engineering,\n  Middle East Technical University, Ankara, Turkey"], ["Kalkan", "Sinan", "", "KOVAN Research Lab, Dept. of Computer Engineering,\n  Middle East Technical University, Ankara, Turkey"]]}, {"id": "1910.11760", "submitter": "Chuang Gan", "authors": "Chuang Gan, Hang Zhao, Peihao Chen, David Cox, Antonio Torralba", "title": "Self-supervised Moving Vehicle Tracking with Stereo Sound", "comments": "To appear at ICCV 2019. Project page:\n  http://sound-track.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are able to localize objects in the environment using both visual and\nauditory cues, integrating information from multiple modalities into a common\nreference frame. We introduce a system that can leverage unlabeled audio-visual\ndata to learn to localize objects (moving vehicles) in a visual reference\nframe, purely using stereo sound at inference time. Since it is labor-intensive\nto manually annotate the correspondences between audio and object bounding\nboxes, we achieve this goal by using the co-occurrence of visual and audio\nstreams in unlabeled videos as a form of self-supervision, without resorting to\nthe collection of ground-truth annotations. In particular, we propose a\nframework that consists of a vision \"teacher\" network and a stereo-sound\n\"student\" network. During training, knowledge embodied in a well-established\nvisual vehicle detection model is transferred to the audio domain using\nunlabeled videos as a bridge. At test time, the stereo-sound student network\ncan work independently to perform object localization us-ing just stereo audio\nand camera meta-data, without any visual input. Experimental results on a newly\ncollected Au-ditory Vehicle Tracking dataset verify that our proposed approach\noutperforms several baseline approaches. We also demonstrate that our\ncross-modal auditory localization approach can assist in the visual\nlocalization of moving vehicles under poor lighting conditions.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:28:55 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Gan", "Chuang", ""], ["Zhao", "Hang", ""], ["Chen", "Peihao", ""], ["Cox", "David", ""], ["Torralba", "Antonio", ""]]}, {"id": "1910.11761", "submitter": "Tianrui Liu", "authors": "Tianrui Liu, Jun-Jie Huang, Tianhong Dai, Guangyu Ren and Tania\n  Stathaki", "title": "Gated Multi-layer Convolutional Feature Extraction Network for Robust\n  Pedestrian Detection", "comments": null, "journal-ref": "International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP) 2020", "doi": null, "report-no": "Accepted by ICASSP'20", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection methods have been significantly improved with the\ndevelopment of deep convolutional neural networks. Nevertheless, robustly\ndetecting pedestrians with a large variant on sizes and with occlusions remains\na challenging problem. In this paper, we propose a gated multi-layer\nconvolutional feature extraction method which can adaptively generate\ndiscriminative features for candidate pedestrian regions. The proposed gated\nfeature extraction framework consists of squeeze units, gate units and a\nconcatenation layer which perform feature dimension squeezing, feature elements\nmanipulation and convolutional features combination from multiple CNN layers,\nrespectively. We proposed two different gate models which can manipulate the\nregional feature maps in a channel-wise selection manner and a spatial-wise\nselection manner, respectively. Experiments on the challenging CityPersons\ndataset demonstrate the effectiveness of the proposed method, especially on\ndetecting those small-size and occluded pedestrians.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:28:58 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 15:39:23 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Liu", "Tianrui", ""], ["Huang", "Jun-Jie", ""], ["Dai", "Tianhong", ""], ["Ren", "Guangyu", ""], ["Stathaki", "Tania", ""]]}, {"id": "1910.11764", "submitter": "Ying Liu", "authors": "Liu Ying, Heng Fan, Fuchuan Ni, Jinhai Xiang", "title": "ClsGAN: Selective Attribute Editing Model Based On Classification\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribution editing has achieved remarkable progress in recent years owing to\nthe encoder-decoder structure and generative adversarial network (GAN).\nHowever, it remains challenging in generating high-quality images with accurate\nattribute transformation. Attacking these problems, the work proposes a novel\nselective attribute editing model based on classification adversarial network\n(referred to as ClsGAN) that shows good balance between attribute transfer\naccuracy and photo-realistic images. Considering that the editing images are\nprone to be affected by original attribute due to skip-connection in\nencoder-decoder structure, an upper convolution residual network (referred to\nas Tr-resnet) is presented to selectively extract information from the source\nimage and target label. In addition, to further improve the transfer accuracy\nof generated images, an attribute adversarial classifier (referred to as\nAtta-cls) is introduced to guide the generator from the perspective of\nattribute through learning the defects of attribute transfer images.\nExperimental results on CelebA demonstrate that our ClsGAN performs favorably\nagainst state-of-the-art approaches in image quality and transfer accuracy.\nMoreover, ablation studies are also designed to verify the great performance of\nTr-resnet and Atta-cls.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:32:21 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 12:14:18 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Ying", "Liu", ""], ["Fan", "Heng", ""], ["Ni", "Fuchuan", ""], ["Xiang", "Jinhai", ""]]}, {"id": "1910.11777", "submitter": "Teresa Ara\\'ujo", "authors": "Teresa Ara\\'ujo, Guilherme Aresta, Lu\\'is Mendon\\c{c}a, Susana Penas,\n  Carolina Maia, \\^Angela Carneiro, Ana Maria Mendon\\c{c}a, Aur\\'elio Campilho", "title": "DR$\\vert$GRADUATE: uncertainty-aware deep learning-based diabetic\n  retinopathy grading in eye fundus images", "comments": "Published at Medical Image Analysis (Elsevier). Publication licensed\n  under the Creative Commons CC-BY-NC-ND 4.0 license\n  https://creativecommons.org/licenses/by-nc-nd/4.0/. Figures are compressed\n  due to file size constraints", "journal-ref": "Medical Image Analysis, Volume 63, July 2020, 101715", "doi": "10.1016/j.media.2020.101715", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) grading is crucial in determining the adequate\ntreatment and follow up of patients, but the screening process can be tiresome\nand prone to errors. Deep learning approaches have shown promising performance\nas computer-aided diagnosis(CAD) systems, but their black-box behaviour hinders\nthe clinical application. We propose DR$\\vert$GRADUATE, a novel deep\nlearning-based DR grading CAD system that supports its decision by providing a\nmedically interpretable explanation and an estimation of how uncertain that\nprediction is, allowing the ophthalmologist to measure how much that decision\nshould be trusted. We designed DR$\\vert$GRADUATE taking into account the\nordinal nature of the DR grading problem. A novel Gaussian-sampling approach\nbuilt upon a Multiple Instance Learning framework allow DR$\\vert$GRADUATE to\ninfer an image grade associated with an explanation map and a prediction\nuncertainty while being trained only with image-wise labels. DR$\\vert$GRADUATE\nwas trained on the Kaggle training set and evaluated across multiple datasets.\nIn DR grading, a quadratic-weighted Cohen's kappa (QWK) between 0.71 and 0.84\nwas achieved in five different datasets. We show that high QWK values occur for\nimages with low prediction uncertainty, thus indicating that this uncertainty\nis a valid measure of the predictions' quality. Further, bad quality images are\ngenerally associated with higher uncertainties, showing that images not\nsuitable for diagnosis indeed lead to less trustworthy predictions.\nAdditionally, tests on unfamiliar medical image data types suggest that\nDR$\\vert$GRADUATE allows outlier detection. The attention maps generally\nhighlight regions of interest for diagnosis. These results show the great\npotential of DR$\\vert$GRADUATE as a second-opinion system in DR severity\ngrading.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:56:15 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 14:55:20 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Ara\u00fajo", "Teresa", ""], ["Aresta", "Guilherme", ""], ["Mendon\u00e7a", "Lu\u00eds", ""], ["Penas", "Susana", ""], ["Maia", "Carolina", ""], ["Carneiro", "\u00c2ngela", ""], ["Mendon\u00e7a", "Ana Maria", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "1910.11791", "submitter": "Linchao Bao", "authors": "Yajing Chen, Fanzi Wu, Zeyu Wang, Yibing Song, Yonggen Ling, Linchao\n  Bao", "title": "Self-supervised Learning of Detailed 3D Face Reconstruction", "comments": "Accepted by IEEE Transactions on Image Processing (TIP)", "journal-ref": null, "doi": "10.1109/TIP.2020.3017347", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an end-to-end learning framework for detailed 3D\nface reconstruction from a single image. Our approach uses a 3DMM-based coarse\nmodel and a displacement map in UV-space to represent a 3D face. Unlike\nprevious work addressing the problem, our learning framework does not require\nsupervision of surrogate ground-truth 3D models computed with traditional\napproaches. Instead, we utilize the input image itself as supervision during\nlearning. In the first stage, we combine a photometric loss and a facial\nperceptual loss between the input face and the rendered face, to regress a\n3DMM-based coarse model. In the second stage, both the input image and the\nregressed texture of the coarse model are unwrapped into UV-space, and then\nsent through an image-toimage translation network to predict a displacement map\nin UVspace. The displacement map and the coarse model are used to render a\nfinal detailed face, which again can be compared with the original input image\nto serve as a photometric loss for the second stage. The advantage of learning\ndisplacement map in UV-space is that face alignment can be explicitly done\nduring the unwrapping, thus facial details are easier to learn from large\namount of data. Extensive experiments demonstrate the superiority of the\nproposed method over previous work.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 15:16:20 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 03:58:23 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Chen", "Yajing", ""], ["Wu", "Fanzi", ""], ["Wang", "Zeyu", ""], ["Song", "Yibing", ""], ["Ling", "Yonggen", ""], ["Bao", "Linchao", ""]]}, {"id": "1910.11792", "submitter": "Mihir Patel", "authors": "Roberto Mart\\'in-Mart\\'in, Mihir Patel, Hamid Rezatofighi, Abhijeet\n  Shenoi, JunYoung Gwak, Eric Frankel, Amir Sadeghian, Silvio Savarese", "title": "JRDB: A Dataset and Benchmark of Egocentric Robot Visual Perception of\n  Humans in Built Environments", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2021.3070543", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present JRDB, a novel egocentric dataset collected from our social mobile\nmanipulator JackRabbot. The dataset includes 64 minutes of annotated multimodal\nsensor data including stereo cylindrical 360$^\\circ$ RGB video at 15 fps, 3D\npoint clouds from two Velodyne 16 Lidars, line 3D point clouds from two Sick\nLidars, audio signal, RGB-D video at 30 fps, 360$^\\circ$ spherical image from a\nfisheye camera and encoder values from the robot's wheels. Our dataset\nincorporates data from traditionally underrepresented scenes such as indoor\nenvironments and pedestrian areas, all from the ego-perspective of the robot,\nboth stationary and navigating. The dataset has been annotated with over 2.3\nmillion bounding boxes spread over 5 individual cameras and 1.8 million\nassociated 3D cuboids around all people in the scenes totaling over 3500 time\nconsistent trajectories. Together with our dataset and the annotations, we\nlaunch a benchmark and metrics for 2D and 3D person detection and tracking.\nWith this dataset, which we plan on extending with further types of annotation\nin the future, we hope to provide a new source of data and a test-bench for\nresearch in the areas of egocentric robot vision, autonomous navigation, and\nall perceptual tasks around social robotics in human environments.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 15:16:40 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 05:07:16 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 22:38:16 GMT"}, {"version": "v4", "created": "Sat, 24 Apr 2021 07:09:03 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Patel", "Mihir", ""], ["Rezatofighi", "Hamid", ""], ["Shenoi", "Abhijeet", ""], ["Gwak", "JunYoung", ""], ["Frankel", "Eric", ""], ["Sadeghian", "Amir", ""], ["Savarese", "Silvio", ""]]}, {"id": "1910.11818", "submitter": "Bin Sun", "authors": "Bin Sun, Ming Shao, Siyu Xia, Yun Fu", "title": "Real-time Memory Efficient Large-pose Face Alignment via Deep\n  Evolutionary Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an urgent need to apply face alignment in a memory-efficient and\nreal-time manner due to the recent explosion of face recognition applications.\nHowever, impact factors such as large pose variation and computational\ninefficiency, still hinder its broad implementation. To this end, we propose a\ncomputationally efficient deep evolutionary model integrated with 3D Diffusion\nHeap Maps (DHM). First, we introduce a sparse 3D DHM to assist the initial\nmodeling process under extreme pose conditions. Afterward, a simple and\neffective CNN feature is extracted and fed to Recurrent Neural Network (RNN)\nfor evolutionary learning. To accelerate the model, we propose an efficient\nnetwork structure to accelerate the evolutionary learning process through a\nfactorization strategy. Extensive experiments on three popular alignment\ndatabases demonstrate the advantage of the proposed models over the\nstate-of-the-art, especially under large-pose conditions. Notably, the\ncomputational speed of our model is 6 times faster than the state-of-the-art on\nCPU and 14 times on GPU. We also discuss and analyze the limitations of our\nmodels and future research work.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:00:05 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 15:31:34 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Sun", "Bin", ""], ["Shao", "Ming", ""], ["Xia", "Siyu", ""], ["Fu", "Yun", ""]]}, {"id": "1910.11819", "submitter": "Qin Zou", "authors": "Yuanhao Yue, Qin Zou, Hongkai Yu, Qian Wang and Song Wang", "title": "An End-to-End Network for Co-Saliency Detection in One Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a common visual problem, co-saliency detection within a single image does\nnot attract enough attention and yet has not been well addressed. Existing\nmethods often follow a bottom-up strategy to infer co-saliency in an image,\nwhere salient regions are firstly detected using visual primitives such as\ncolor and shape, and then grouped and merged into a co-saliency map. However,\nco-saliency is intrinsically perceived in a complex manner with bottom-up and\ntop-down strategies combined in human vision. To deal with this problem, a\nnovel end-to-end trainable network is proposed in this paper, which includes a\nbackbone net and two branch nets. The backbone net uses ground-truth masks as\ntop-down guidance for saliency prediction, while the two branch nets construct\ntriplet proposals for feature organization and clustering, which drives the\nnetwork to be sensitive to co-salient regions in a bottom-up way. To evaluate\nthe proposed method, we construct a new dataset of 2,019 nature images with\nco-saliency in each image. Experimental results show that the proposed method\nachieves a state-of-the-art accuracy with a running speed of 28fps.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:00:44 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Yue", "Yuanhao", ""], ["Zou", "Qin", ""], ["Yu", "Hongkai", ""], ["Wang", "Qian", ""], ["Wang", "Song", ""]]}, {"id": "1910.11831", "submitter": "Lingxi Xie", "authors": "Kaifeng Bi, Changping Hu, Lingxi Xie, Xin Chen, Longhui Wei, Qi Tian", "title": "Stabilizing DARTS with Amended Gradient Estimation on Architectural\n  Parameters", "comments": "22 pages, 12 figures, submitted to ICML 2020, updated experiments on\n  Penn Treebank", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DARTS is a popular algorithm for neural architecture search (NAS). Despite\nits great advantage in search efficiency, DARTS often suffers weak stability,\nwhich reflects in the large variation among individual trials as well as the\nsensitivity to the hyper-parameters of the search process. This paper owes such\ninstability to an optimization gap between the super-network and its\nsub-networks, namely, improving the validation accuracy of the super-network\ndoes not necessarily lead to a higher expectation on the performance of the\nsampled sub-networks. Then, we point out that the gap is due to the inaccurate\nestimation of the architectural gradients, based on which we propose an amended\nestimation method. Mathematically, our method guarantees a bounded error from\nthe true gradients while the original estimation does not. Our approach bridges\nthe gap from two aspects, namely, amending the estimation on the architectural\ngradients, and unifying the hyper-parameter settings in the search and\nre-training stages. Experiments on CIFAR10 and ImageNet demonstrate that our\napproach largely improves search stability and, more importantly, enables\nDARTS-based approaches to explore much larger search spaces that have not been\ninvestigated before.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:31:25 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 08:59:51 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 04:22:46 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 04:19:28 GMT"}, {"version": "v5", "created": "Mon, 4 May 2020 10:19:18 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bi", "Kaifeng", ""], ["Hu", "Changping", ""], ["Xie", "Lingxi", ""], ["Chen", "Xin", ""], ["Wei", "Longhui", ""], ["Tian", "Qi", ""]]}, {"id": "1910.11844", "submitter": "Achal Dave", "authors": "Achal Dave, Pavel Tokmakov, Cordelia Schmid, Deva Ramanan", "title": "Learning to Track Any Object", "comments": "To be presented at the Holistic Video Understanding workshop at ICCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking can be formulated as \"finding the right object in a video\".\nWe observe that recent approaches for class-agnostic tracking tend to focus on\nthe \"finding\" part, but largely overlook the \"object\" part of the task,\nessentially doing a template matching over a frame in a sliding-window. In\ncontrast, class-specific trackers heavily rely on object priors in the form of\ncategory-specific object detectors. In this work, we re-purpose\ncategory-specific appearance models into a generic objectness prior. Our\napproach converts a category-specific object detector into a category-agnostic,\nobject-specific detector (i.e. a tracker) efficiently, on the fly. Moreover, at\ntest time the same network can be applied to detection and tracking, resulting\nin a unified approach for the two tasks. We achieve state-of-the-art results on\ntwo recent large-scale tracking benchmarks (OxUvA and GOT, using external\ndata). By simply adding a mask prediction branch, our approach is able to\nproduce instance segmentation masks for the tracked object. Despite only using\nbox-level information on the first frame, our method outputs high-quality\nmasks, as evaluated on the DAVIS '17 video object segmentation benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:58:42 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Dave", "Achal", ""], ["Tokmakov", "Pavel", ""], ["Schmid", "Cordelia", ""], ["Ramanan", "Deva", ""]]}, {"id": "1910.11853", "submitter": "Bin Sun", "authors": "Bin Sun, Jun Li, Ming Shao, Yun Fu", "title": "LPRNet: Lightweight Deep Network by Low-rank Pointwise Residual\n  Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become popular in recent years primarily due to the\npowerful computing device such as GPUs. However, deploying these deep models to\nend-user devices, smart phones, or embedded systems with limited resources is\nchallenging. To reduce the computation and memory costs, we propose a novel\nlightweight deep learning module by low-rank pointwise residual (LPR)\nconvolution, called LPRNet. Essentially, LPR aims at using low-rank\napproximation in pointwise convolution to further reduce the module size, while\nkeeping depthwise convolutions as the residual module to rectify the LPR\nmodule. This is critical when the low-rankness undermines the convolution\nprocess. We embody our design by replacing modules of identical input-output\ndimension in MobileNet and ShuffleNetv2. Experiments on visual recognition\ntasks including image classification and face alignment on popular benchmarks\nshow that our LPRNet achieves competitive performance but with significant\nreduction of Flops and memory cost compared to the state-of-the-art deep models\nfocusing on model compression.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 17:23:05 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 18:08:55 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 22:44:48 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Sun", "Bin", ""], ["Li", "Jun", ""], ["Shao", "Ming", ""], ["Fu", "Yun", ""]]}, {"id": "1910.11908", "submitter": "Nick Moran", "authors": "Nick Moran and Dan Schmidt and Yu Zhong and Patrick Coady", "title": "Noisier2Noise: Learning to Denoise from Unpaired Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for training a neural network to perform image denoising\nwithout access to clean training examples or access to paired noisy training\nexamples. Our method requires only a single noisy realization of each training\nexample and a statistical model of the noise distribution, and is applicable to\na wide variety of noise models, including spatially structured noise. Our model\nproduces results which are competitive with other learned methods which require\nricher training data, and outperforms traditional non-learned denoising\nmethods. We present derivations of our method for arbitrary additive noise, an\nimprovement specific to Gaussian additive noise, and an extension to\nmultiplicative Bernoulli noise.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 19:30:38 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Moran", "Nick", ""], ["Schmidt", "Dan", ""], ["Zhong", "Yu", ""], ["Coady", "Patrick", ""]]}, {"id": "1910.11949", "submitter": "Mariona Car\\'os Roca", "authors": "Mariona Caros, Maite Garolera, Petia Radeva and Xavier Giro-i-Nieto", "title": "Automatic Reminiscence Therapy for Dementia", "comments": "MSc thesis at TelecomBCN, Universitat Politecnica de Catalunya 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With people living longer than ever, the number of cases with dementia such\nas Alzheimer's disease increases steadily. It affects more than 46 million\npeople worldwide, and it is estimated that in 2050 more than 100 million will\nbe affected. While there are not effective treatments for these terminal\ndiseases, therapies such as reminiscence, that stimulate memories from the past\nare recommended. Currently, reminiscence therapy takes place in care homes and\nis guided by a therapist or a carer. In this work, we present an AI-based\nsolution to automatize the reminiscence therapy, which consists in a dialogue\nsystem that uses photos as input to generate questions. We run a usability case\nstudy with patients diagnosed of mild cognitive impairment that shows they\nfound the system very entertaining and challenging. Overall, this paper\npresents how reminiscence therapy can be automatized by using machine learning,\nand deployed to smartphones and laptops, making the therapy more accessible to\nevery person affected by dementia.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 21:47:52 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 12:26:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Caros", "Mariona", ""], ["Garolera", "Maite", ""], ["Radeva", "Petia", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1910.11960", "submitter": "Ibrahim Ali", "authors": "Ibrahim Saad Ali, Mamdouh Farouk Mohamed, Yousef Bassyouni Mahdy", "title": "Data Augmentation for Skin Lesion using Self-Attention based Progressive\n  Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) show a significant impact on medical imaging. One\nsignificant problem with adopting DNNs for skin cancer classification is that\nthe class frequencies in the existing datasets are imbalanced. This problem\nhinders the training of robust and well-generalizing models. Data Augmentation\naddresses this by using existing data more effectively. However, standard data\naugmentation implementations are manually designed and produce only limited\nreasonably alternative data. Instead, Generative Adversarial Networks (GANs) is\nutilized to generate a much broader set of augmentations. This paper proposes a\nnovel enhancement for the progressive generative adversarial networks (PGAN)\nusing self-attention mechanism. Self-attention mechanism is used to directly\nmodel the long-range dependencies in the feature maps. Accordingly,\nself-attention complements PGAN to generate fine-grained samples that comprise\nclinically-meaningful information. Moreover, the stabilization technique was\napplied to the enhanced generative model. To train the generative models, ISIC\n2018 skin lesion challenge dataset was used to synthesize highly realistic skin\nlesion samples for boosting further the classification result. We achieve an\naccuracy of 70.1% which is 2.8% better than the non-augmented one of 67.3%.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 23:15:36 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ali", "Ibrahim Saad", ""], ["Mohamed", "Mamdouh Farouk", ""], ["Mahdy", "Yousef Bassyouni", ""]]}, {"id": "1910.11968", "submitter": "Philippe Gigu\\`ere", "authors": "Charles-\\'Eric No\\\"el Laflamme, Fran\\c{c}ois Pomerleau, Philippe\n  Gigu\\`ere", "title": "Driving Datasets Literature Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report is a survey of the different autonomous driving datasets which\nhave been published up to date. The first section introduces the many sensor\ntypes used in autonomous driving datasets. The second section investigates the\ncalibration and synchronization procedure required to generate accurate data.\nThe third section describes the diverse driving tasks explored by the datasets.\nFinally, the fourth section provides comprehensive lists of datasets, mainly in\nthe form of tables.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 00:28:39 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Laflamme", "Charles-\u00c9ric No\u00ebl", ""], ["Pomerleau", "Fran\u00e7ois", ""], ["Gigu\u00e8re", "Philippe", ""]]}, {"id": "1910.11971", "submitter": "Xundong Wu", "authors": "Zhilin Yu, Chao Wang, Xin Wang, Qing Wu, Yong Zhao, Xundong Wu", "title": "Cross-Channel Intragroup Sparsity Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks rely on overparameterization to achieve\nstate-of-the-art generalization. But overparameterized models are\ncomputationally expensive. Network pruning is often employed to obtain less\ndemanding models for deployment. Fine-grained pruning removes individual\nweights in parameter tensors and can achieve a high model compression ratio\nwith little accuracy degradation. However, it introduces irregularity into the\ncomputing dataflow and often does not yield improved model inference efficiency\nin practice. Coarse-grained model pruning, while realizing satisfactory\ninference speedup through removal of network weights in groups, e.g. an entire\nfilter, often lead to significant accuracy degradation. This work introduces\nthe cross-channel intragroup (CCI) sparsity structure, which can prevent the\ninference inefficiency of fine-grained pruning while maintaining outstanding\nmodel performance. We then present a novel training algorithm designed to\nperform well under the constraint imposed by the CCI-Sparsity. Through a series\nof comparative experiments we show that our proposed CCI-Sparsity structure and\nthe corresponding pruning algorithm outperform prior art in inference\nefficiency by a substantial margin given suited hardware acceleration in the\nfuture.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 01:03:01 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 05:29:47 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Yu", "Zhilin", ""], ["Wang", "Chao", ""], ["Wang", "Xin", ""], ["Wu", "Qing", ""], ["Zhao", "Yong", ""], ["Wu", "Xundong", ""]]}, {"id": "1910.11981", "submitter": "Liang Shen", "authors": "Liang Shen, Jiahua Zhu, Chongyi Fan, Xiaotao Huang (Member, IEEE) and\n  Tian Jin", "title": "Novel Co-variant Feature Point Matching Based on Gaussian Mixture Model", "comments": "arXiv admin note: text overlap with arXiv:0905.2635 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feature frame is a key idea of feature matching problem between two\nimages. However, most of the traditional matching methods only simply employ\nthe spatial location information (the coordinates), which ignores the shape and\norientation information of the local feature. Such additional information can\nbe obtained along with coordinates using general co-variant detectors such as\nDOG, Hessian, Harris-Affine and MSER. In this paper, we develop a novel method\nconsidering all the feature center position coordinates, the local feature\nshape and orientation information based on Gaussian Mixture Model for\nco-variant feature matching. We proposed three sub-versions in our method for\nsolving the matching problem in different conditions: rigid, affine and\nnon-rigid, respectively, which all optimized by expectation maximization\nalgorithm. Due to the effective utilization of the additional shape and\norientation information, the proposed model can significantly improve the\nperformance in terms of convergence speed and recall. Besides, it is more\nrobust to the outliers.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 02:31:02 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Shen", "Liang", "", "Member, IEEE"], ["Zhu", "Jiahua", "", "Member, IEEE"], ["Fan", "Chongyi", "", "Member, IEEE"], ["Huang", "Xiaotao", "", "Member, IEEE"], ["Jin", "Tian", ""]]}, {"id": "1910.12003", "submitter": "Chanho Eom", "authors": "Chanho Eom, Bumsub Ham", "title": "Learning Disentangled Representation for Robust Person Re-identification", "comments": null, "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of person re-identification (reID), that is,\nretrieving person images from a large dataset, given a query image of the\nperson of interest. A key challenge is to learn person representations robust\nto intra-class variations, as different persons can have the same attribute and\nthe same person's appearance looks different with viewpoint changes. Recent\nreID methods focus on learning discriminative features but robust to only a\nparticular factor of variations (e.g., human pose), which requires\ncorresponding supervisory signals (e.g., pose annotations). To tackle this\nproblem, we propose to disentangle identity-related and -unrelated features\nfrom person images. Identity-related features contain information useful for\nspecifying a particular person (e.g., clothing), while identity-unrelated ones\nhold other factors (e.g., human pose, scale changes). To this end, we introduce\na new generative adversarial network, dubbed \\emph{identity shuffle GAN}\n(IS-GAN), that factorizes these features using identification labels without\nany auxiliary information. We also propose an identity-shuffling technique to\nregularize the disentangled features. Experimental results demonstrate the\neffectiveness of IS-GAN, significantly outperforming the state of the art on\nstandard reID benchmarks including the Market-1501, CUHK03 and DukeMTMC-reID.\nOur code and models are available online:\nhttps://cvlab-yonsei.github.io/projects/ISGAN/.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 05:52:11 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 07:00:05 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Eom", "Chanho", ""], ["Ham", "Bumsub", ""]]}, {"id": "1910.12008", "submitter": "Kristy Choi", "authors": "Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, Stefano Ermon", "title": "Fair Generative Modeling via Weak Supervision", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world datasets are often biased with respect to key demographic factors\nsuch as race and gender. Due to the latent nature of the underlying factors,\ndetecting and mitigating bias is especially challenging for unsupervised\nmachine learning. We present a weakly supervised algorithm for overcoming\ndataset bias for deep generative models. Our approach requires access to an\nadditional small, unlabeled reference dataset as the supervision signal, thus\nsidestepping the need for explicit labels on the underlying bias factors. Using\nthis supplementary dataset, we detect the bias in existing datasets via a\ndensity ratio technique and learn generative models which efficiently achieve\nthe twin goals of: 1) data efficiency by using training examples from both\nbiased and reference datasets for learning; and 2) data generation close in\ndistribution to the reference dataset at test time. Empirically, we demonstrate\nthe efficacy of our approach which reduces bias w.r.t. latent factors by an\naverage of up to 34.6% over baselines for comparable image generation using\ngenerative adversarial networks.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 06:40:03 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 11:11:27 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Choi", "Kristy", ""], ["Grover", "Aditya", ""], ["Singh", "Trisha", ""], ["Shu", "Rui", ""], ["Ermon", "Stefano", ""]]}, {"id": "1910.12010", "submitter": "Lei Mou", "authors": "Lei Mou, Li Chen, Jun Cheng, Zaiwang Gu, Yitian Zhao and Jiang Liu", "title": "Dense Dilated Network with Probability Regularized Walk for Vessel\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of retinal vessel is of great importance in the diagnosis and\ntreatment of many ocular diseases. Many methods have been proposed for vessel\ndetection. However, most of the algorithms neglect the connectivity of the\nvessels, which plays an important role in the diagnosis. In this paper, we\npropose a novel method for retinal vessel detection. The proposed method\nincludes a dense dilated network to get an initial detection of the vessels and\na probability regularized walk algorithm to address the fracture issue in the\ninitial detection. The dense dilated network integrates newly proposed dense\ndilated feature extraction blocks into an encoder-decoder structure to extract\nand accumulate features at different scales. A multiscale Dice loss function is\nadopted to train the network. To improve the connectivity of the segmented\nvessels, we also introduce a probability regularized walk algorithm to connect\nthe broken vessels. The proposed method has been applied on three public data\nsets: DRIVE, STARE and CHASE_DB1. The results show that the proposed method\noutperforms the state-of-the-art methods in accuracy, sensitivity, specificity\nand also are under receiver operating characteristic curve.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 06:44:11 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Mou", "Lei", ""], ["Chen", "Li", ""], ["Cheng", "Jun", ""], ["Gu", "Zaiwang", ""], ["Zhao", "Yitian", ""], ["Liu", "Jiang", ""]]}, {"id": "1910.12019", "submitter": "Huanhou Xiao", "authors": "Huanhou Xiao and Jinglun Shi", "title": "Diverse Video Captioning Through Latent Variable Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing video content with text description is challenging\nbut important task, which has been attracting a lot of attention in computer\nvision community. Previous works mainly strive for the accuracy of the\ngenerated sentences, while ignoring the sentences diversity, which is\ninconsistent with human behavior. In this paper, we aim to caption each video\nwith multiple descriptions and propose a novel framework. Concretely, for a\ngiven video, the intermediate latent variables of conventional encode-decode\nprocess are utilized as input to the conditional generative adversarial network\n(CGAN) with the purpose of generating diverse sentences. We adopt different\nConvolutional Neural Networks (CNNs) as our generator that produces\ndescriptions conditioned on latent variables and discriminator that assesses\nthe quality of generated sentences. Simultaneously, a novel DCE metric is\ndesigned to assess the diverse captions. We evaluate our method on the\nbenchmark datasets, where it demonstrates its ability to generate diverse\ndescriptions and achieves superior results against other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 08:34:20 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 02:26:11 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 01:28:08 GMT"}, {"version": "v4", "created": "Thu, 5 Mar 2020 16:58:51 GMT"}, {"version": "v5", "created": "Thu, 25 Mar 2021 15:12:08 GMT"}, {"version": "v6", "created": "Tue, 15 Jun 2021 14:50:14 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Xiao", "Huanhou", ""], ["Shi", "Jinglun", ""]]}, {"id": "1910.12023", "submitter": "Fran\\c{c}ois Waldner", "authors": "Fran\\c{c}ois Waldner, Foivos I. Diakogiannis", "title": "Deep learning on edge: extracting field boundaries from satellite images\n  with a convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Applications of digital agricultural services often require either farmers or\ntheir advisers to provide digital records of their field boundaries. Automatic\nextraction of field boundaries from satellite imagery would reduce the reliance\non manual input of these records which is time consuming and error-prone, and\nwould underpin the provision of remote products and services. The lack of\ncurrent field boundary data sets seems to indicate low uptake of existing\nmethods,presumably because of expensive image preprocessing requirements and\nlocal, often arbitrary, tuning. In this paper, we address the problem of field\nboundary extraction from satellite images as a multitask semantic segmentation\nproblem. We used ResUNet-a, a deep convolutional neural network with a fully\nconnected UNet backbone that features dilated convolutions and conditioned\ninference, to assign three labels to each pixel: 1) the probability of\nbelonging to a field; 2) the probability of being part of a boundary; and 3)\nthe distance to the closest boundary. These labels can then be combined to\nobtain closed field boundaries. Using a single composite image from Sentinel-2,\nthe model was highly accurate in mapping field extent, field boundaries, and,\nconsequently, individual fields. Replacing the monthly composite with a\nsingle-date image close to the compositing period only marginally decreased\naccuracy. We then showed in a series of experiments that our model generalised\nwell across resolutions, sensors, space and time without recalibration.\nBuilding consensus by averaging model predictions from at least four images\nacquired across the season is the key to coping with the temporal variations of\naccuracy. By minimising image preprocessing requirements and replacing local\narbitrary decisions by data-driven ones, our approach is expected to facilitate\nthe extraction of individual crop fields at scale.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 08:46:24 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 06:13:29 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Waldner", "Fran\u00e7ois", ""], ["Diakogiannis", "Foivos I.", ""]]}, {"id": "1910.12024", "submitter": "Saiprasad Ravishankar", "authors": "Zhipeng Li, Siqi Ye, Yong Long, and Saiprasad Ravishankar", "title": "SUPER Learning: A Supervised-Unsupervised Framework for Low-Dose CT\n  Image Reconstruction", "comments": "Accepted to International Conference on Computer Vision (ICCV) -\n  Learning for Computational Imaging (LCI) Workshop, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed growing interest in machine learning-based models\nand techniques for low-dose X-ray CT (LDCT) imaging tasks. The methods can\ntypically be categorized into supervised learning methods and unsupervised or\nmodel-based learning methods. Supervised learning methods have recently shown\nsuccess in image restoration tasks. However, they often rely on large training\nsets. Model-based learning methods such as dictionary or transform learning do\nnot require large or paired training sets and often have good generalization\nproperties, since they learn general properties of CT image sets. Recent works\nhave shown the promising reconstruction performance of methods such as\nPWLS-ULTRA that rely on clustering the underlying (reconstructed) image patches\ninto a learned union of transforms. In this paper, we propose a new\nSupervised-UnsuPERvised (SUPER) reconstruction framework for LDCT image\nreconstruction that combines the benefits of supervised learning methods and\n(unsupervised) transform learning-based methods such as PWLS-ULTRA that involve\nhighly image-adaptive clustering. The SUPER model consists of several layers,\neach of which includes a deep network learned in a supervised manner and an\nunsupervised iterative method that involves image-adaptive components. The\nSUPER reconstruction algorithms are learned in a greedy manner from training\ndata. The proposed SUPER learning methods dramatically outperform both the\nconstituent supervised learning-based networks and iterative algorithms for\nLDCT, and use much fewer iterations in the iterative reconstruction modules.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 09:04:45 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Li", "Zhipeng", ""], ["Ye", "Siqi", ""], ["Long", "Yong", ""], ["Ravishankar", "Saiprasad", ""]]}, {"id": "1910.12027", "submitter": "Han Zhang", "authors": "Han Zhang, Zizhao Zhang, Augustus Odena, Honglak Lee", "title": "Consistency Regularization for Generative Adversarial Networks", "comments": "ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are known to be difficult to train,\ndespite considerable research effort. Several regularization techniques for\nstabilizing training have been proposed, but they introduce non-trivial\ncomputational overheads and interact poorly with existing techniques like\nspectral normalization. In this work, we propose a simple, effective training\nstabilizer based on the notion of consistency regularization---a popular\ntechnique in the semi-supervised learning literature. In particular, we augment\ndata passing into the GAN discriminator and penalize the sensitivity of the\ndiscriminator to these augmentations. We conduct a series of experiments to\ndemonstrate that consistency regularization works effectively with spectral\nnormalization and various GAN architectures, loss functions and optimizer\nsettings. Our method achieves the best FID scores for unconditional image\ngeneration compared to other regularization methods on CIFAR-10 and CelebA.\nMoreover, Our consistency regularized GAN (CR-GAN) improves state-of-the-art\nFID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from\n8.73 to 6.66 on ImageNet-2012.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 09:06:03 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 21:43:54 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Zhang", "Han", ""], ["Zhang", "Zizhao", ""], ["Odena", "Augustus", ""], ["Lee", "Honglak", ""]]}, {"id": "1910.12028", "submitter": "Kundan Kumar", "authors": "Debojyoti Mallick, Kundan Kumar, Sumanshu Agarwal", "title": "Blood Vessel Detection using Modified Multiscale MF-FDOG Filters for\n  Diabetic Retinopathy", "comments": "5 Pages, 7 Figures, ICAML2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blindness in diabetic patients caused by retinopathy (characterized by an\nincrease in the diameter and new branches of the blood vessels inside the\nretina) is a grave concern. Many efforts have been made for the early detection\nof the disease using various image processing techniques on retinal images.\nHowever, most of the methods are plagued with the false detection of the blood\nvessel pixels. Given that, here, we propose a modified matched filter with the\nfirst derivative of Gaussian. The method uses the top-hat transform and\ncontrast limited histogram equalization. Further, we segment the modified\nmultiscale matched filter response by using a binary threshold obtained from\nthe first derivative of Gaussian. The method was assessed on a publicly\navailable database (DRIVE database). As anticipated, the proposed method\nprovides a higher accuracy compared to the literature. Moreover, a lesser false\ndetection from the existing matched filters and its variants have been\nobserved.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 09:17:10 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Mallick", "Debojyoti", ""], ["Kumar", "Kundan", ""], ["Agarwal", "Sumanshu", ""]]}, {"id": "1910.12029", "submitter": "Ju Yong Chang", "authors": "Ju Yong Chang, Gyeongsik Moon, Kyoung Mu Lee", "title": "PoseLifter: Absolute 3D human pose lifting network from a single noisy\n  2D human pose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a new network (i.e., PoseLifter) that can lift a 2D human\npose to an absolute 3D pose in a camera coordinate system. The proposed network\nestimates the absolute 3D location of a target subject and generates an\nimproved 3D relative pose estimation compared with existing pose-lifting\nmethods. Using the PoseLifter with a 2D pose estimator in a cascade fashion can\nestimate a 3D human pose from a single RGB image. In this case, we empirically\nprove that using realistic 2D poses synthesized with the real error\ndistribution of 2D body joints considerably improves the performance of our\nPoseLifter. The proposed method is applied to public datasets to achieve\nstate-of-the-art 2D-to-3D pose lifting and 3D human pose estimation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 09:18:30 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 14:46:07 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Chang", "Ju Yong", ""], ["Moon", "Gyeongsik", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1910.12032", "submitter": "Kun Zhou", "authors": "Kun Zhou, Xiaoguang Han, Nianjuan Jiang, Kui Jia, and Jiangbo Lu", "title": "HEMlets Pose: Learning Part-Centric Heatmap Triplets for Accurate 3D\n  Human Pose Estimation", "comments": "10 pages, 6 figures, to be presented at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D human pose from a single image is a challenging task. This work\nattempts to address the uncertainty of lifting the detected 2D joints to the 3D\nspace by introducing an intermediate state - Part-Centric Heatmap Triplets\n(HEMlets), which shortens the gap between the 2D observation and the 3D\ninterpretation. The HEMlets utilize three joint-heatmaps to represent the\nrelative depth information of the end-joints for each skeletal body part. In\nour approach, a Convolutional Network (ConvNet) is first trained to predict\nHEMlests from the input image, followed by a volumetric joint-heatmap\nregression. We leverage on the integral operation to extract the joint\nlocations from the volumetric heatmaps, guaranteeing end-to-end learning.\nDespite the simplicity of the network design, the quantitative comparisons show\na significant performance improvement over the best-of-grade method (by 20% on\nHuman3.6M). The proposed method naturally supports training with \"in-the-wild\"\nimages, where only weakly-annotated relative depth information of skeletal\njoints is available. This further improves the generalization ability of our\nmodel, as validated by qualitative comparisons on outdoor images.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 09:29:54 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhou", "Kun", ""], ["Han", "Xiaoguang", ""], ["Jiang", "Nianjuan", ""], ["Jia", "Kui", ""], ["Lu", "Jiangbo", ""]]}, {"id": "1910.12037", "submitter": "Shuai Zhao", "authors": "Shuai Zhao, Yang Wang, Zheng Yang, Deng Cai", "title": "Region Mutual Information Loss for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic segmentation is a fundamental problem in computer vision. It is\nconsidered as a pixel-wise classification problem in practice, and most\nsegmentation models use a pixel-wise loss as their optimization riterion.\nHowever, the pixel-wise loss ignores the dependencies between pixels in an\nimage. Several ways to exploit the relationship between pixels have been\ninvestigated, \\eg, conditional random fields (CRF) and pixel affinity based\nmethods. Nevertheless, these methods usually require additional model branches,\nlarge extra memories, or more inference time. In this paper, we develop a\nregion mutual information (RMI) loss to model the dependencies among pixels\nmore simply and efficiently. In contrast to the pixel-wise loss which treats\nthe pixels as independent samples, RMI uses one pixel and its neighbour pixels\nto represent this pixel. Then for each pixel in an image, we get a\nmulti-dimensional point that encodes the relationship between pixels, and the\nimage is cast into a multi-dimensional distribution of these high-dimensional\npoints. The prediction and ground truth thus can achieve high order consistency\nthrough maximizing the mutual information (MI) between their multi-dimensional\ndistributions. Moreover, as the actual value of the MI is hard to calculate, we\nderive a lower bound of the MI and maximize the lower bound to maximize the\nreal value of the MI. RMI only requires a few extra computational resources in\nthe training stage, and there is no overhead during testing. Experimental\nresults demonstrate that RMI can achieve substantial and consistent\nimprovements in performance on PASCAL VOC 2012 and CamVid datasets. The code is\navailable at https://github.com/ZJULearning/RMI.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 09:39:36 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhao", "Shuai", ""], ["Wang", "Yang", ""], ["Yang", "Zheng", ""], ["Cai", "Deng", ""]]}, {"id": "1910.12044", "submitter": "Xingyuan Bu", "authors": "Xingyuan Bu, Junran Peng, Changbao Wang, Cunjun Yu, Guoliang Cao", "title": "Learning an Efficient Network for Large-Scale Hierarchical Object\n  Detection with Data Imbalance: 3rd Place Solution to Open Images Challenge\n  2019", "comments": "6 pages, 3 figures, ICCV 2019 Open Images Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report details our solution to the Google AI Open Images Challenge 2019\nObject Detection Track. Based on our detailed analysis on the Open Images\ndataset, it is found that there are four typical features: large-scale,\nhierarchical tag system, severe annotation incompleteness and data imbalance.\nConsidering these characteristics, many strategies are employed, including\nlarger backbone, distributed softmax loss, class-aware sampling, expert model,\nand heavier classifier. In virtue of these effective strategies, our best\nsingle model could achieve a mAP of 61.90. After ensemble, the final mAP is\nboosted to 67.17 in the public leaderboard and 64.21 in the private\nleaderboard, which earns 3rd place in the Open Images Challenge 2019.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 10:31:35 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Bu", "Xingyuan", ""], ["Peng", "Junran", ""], ["Wang", "Changbao", ""], ["Yu", "Cunjun", ""], ["Cao", "Guoliang", ""]]}, {"id": "1910.12053", "submitter": "Lin Xu", "authors": "Lin Xu", "title": "A Preliminary Study on Optimal Placement of Cameras", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper primarily focuses on figuring out the best array of cameras, or\nvisual sensors, so that such a placement enables the maximum utilization of\nthese visual sensors. Maximizing the utilization of these cameras can convert\nto another problem that is simpler for the formulation, that is, maximizing the\ntotal coverage with these cameras. To solve the problem, the coverage problem\nis first defined subject to the capabilities and limits of cameras. Then, poses\nof cameras are analyzed for the best arrangement.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 12:06:44 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Xu", "Lin", ""]]}, {"id": "1910.12056", "submitter": "Zhijie Wu", "authors": "Chunjin Song, Zhijie Wu, Yang Zhou, Minglun Gong, Hui Huang", "title": "ETNet: Error Transition Network for Arbitrary Style Transfer", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous valuable efforts have been devoted to achieving arbitrary style\ntransfer since the seminal work of Gatys et al. However, existing\nstate-of-the-art approaches often generate insufficiently stylized results\nunder challenging cases. We believe a fundamental reason is that these\napproaches try to generate the stylized result in a single shot and hence fail\nto fully satisfy the constraints on semantic structures in the content images\nand style patterns in the style images. Inspired by the works on\nerror-correction, instead, we propose a self-correcting model to predict what\nis wrong with the current stylization and refine it accordingly in an iterative\nmanner. For each refinement, we transit the error features across both the\nspatial and scale domain and invert the processed features into a residual\nimage, with a network we call Error Transition Network (ETNet). The proposed\nmodel improves over the state-of-the-art methods with better semantic\nstructures and more adaptive style pattern details. Various qualitative and\nquantitative experiments show that the key concept of both progressive strategy\nand error-correction leads to better results. Code and models are available at\nhttps://github.com/zhijieW94/ETNet.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 12:49:00 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 17:04:01 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Song", "Chunjin", ""], ["Wu", "Zhijie", ""], ["Zhou", "Yang", ""], ["Gong", "Minglun", ""], ["Huang", "Hui", ""]]}, {"id": "1910.12060", "submitter": "Haifeng Li", "authors": "Qing Zhu, Cheng Liao, Han Hu, Xiaoming Mei, Haifeng Li", "title": "MAP-Net: Multi Attending Path Neural Network for Building Footprint\n  Extraction from Remote Sensed Imagery", "comments": "13 pages, 10 figures", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing 2020 IEEE\n  Transactions on Geoscience and Remote Sensing 2020 IEEE Transactions on\n  Geoscience and Remote Sensing 2020", "doi": "10.1109/TGRS.2020.3026051", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately and efficiently extracting building footprints from a wide range\nof remote sensed imagery remains a challenge due to their complex structure,\nvariety of scales and diverse appearances. Existing convolutional neural\nnetwork (CNN)-based building extraction methods are complained that they cannot\ndetect the tiny buildings because the spatial information of CNN feature maps\nare lost during repeated pooling operations of the CNN, and the large buildings\nstill have inaccurate segmentation edges. Moreover, features extracted by a CNN\nare always partial which restricted by the size of the respective field, and\nlarge-scale buildings with low texture are always discontinuous and holey when\nextracted. This paper proposes a novel multi attending path neural network\n(MAP-Net) for accurately extracting multiscale building footprints and precise\nboundaries. MAP-Net learns spatial localization-preserved multiscale features\nthrough a multi-parallel path in which each stage is gradually generated to\nextract high-level semantic features with fixed resolution. Then, an attention\nmodule adaptively squeezes channel-wise features from each path for\noptimization, and a pyramid spatial pooling module captures global dependency\nfor refining discontinuous building footprints. Experimental results show that\nMAP-Net outperforms state-of-the-art (SOTA) algorithms in boundary localization\naccuracy as well as continuity of large buildings. Specifically, our method\nachieved 0.68\\%, 1.74\\%, 1.46\\% precision, and 1.50\\%, 1.53\\%, 0.82\\% IoU score\nimprovement without increasing computational complexity compared with the\nlatest HRNetv2 on the Urban 3D, Deep Globe and WHU datasets, respectively. The\nTensorFlow implementation is available at https://github.com/lehaifeng/MAPNet.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 13:09:17 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 07:05:59 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Zhu", "Qing", ""], ["Liao", "Cheng", ""], ["Hu", "Han", ""], ["Mei", "Xiaoming", ""], ["Li", "Haifeng", ""]]}, {"id": "1910.12077", "submitter": "Eytan Kats", "authors": "Eytan Kats, Jacob Goldberger, Hayit Greenspan", "title": "A Soft STAPLE Algorithm Combined with Anatomical Knowledge", "comments": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning algorithms, especially in the medical domain, are\naffected by considerable ambiguity in expert markings. In this study we address\nthe case where the experts' opinion is obtained as a distribution over the\npossible values. We propose a soft version of the STAPLE algorithm for experts'\nmarkings fusion that can handle soft values. The algorithm was applied to\nobtain consensus from soft Multiple Sclerosis (MS) segmentation masks. Soft MS\nsegmentations are constructed from manual binary delineations by including\nlesion surrounding voxels in the segmentation mask with a reduced confidence\nweight. We suggest that these voxels contain additional anatomical information\nabout the lesion structure. The fused masks are utilized as ground truth mask\nto train a Fully Convolutional Neural Network (FCNN). The proposed method was\nevaluated on the MICCAI 2016 challenge dataset, and yields improved\nprecision-recall tradeoff and a higher average Dice similarity coefficient.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 14:38:27 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kats", "Eytan", ""], ["Goldberger", "Jacob", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1910.12100", "submitter": "Keqiang Sun", "authors": "Keqiang Sun, Wayne Wu, Tinghao Liu, Shuo Yang, Quan Wang, Qiang Zhou,\n  Zuochang Ye, Chen Qian", "title": "FAB: A Robust Facial Landmark Detection Framework for Motion-Blurred\n  Videos", "comments": "Accepted to ICCV 2019. Project page:\n  https://keqiangsun.github.io/projects/FAB/FAB.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, facial landmark detection algorithms have achieved remarkable\nperformance on static images. However, these algorithms are neither accurate\nnor stable in motion-blurred videos. The missing of structure information makes\nit difficult for state-of-the-art facial landmark detection algorithms to yield\ngood results. In this paper, we propose a framework named FAB that takes\nadvantage of structure consistency in the temporal dimension for facial\nlandmark detection in motion-blurred videos. A structure predictor is proposed\nto predict the missing face structural information temporally, which serves as\na geometry prior. This allows our framework to work as a virtuous circle. On\none hand, the geometry prior helps our structure-aware deblurring network\ngenerates high quality deblurred images which lead to better landmark detection\nresults. On the other hand, better landmark detection results help structure\npredictor generate better geometry prior for the next frame. Moreover, it is a\nflexible video-based framework that can incorporate any static image-based\nmethods to provide a performance boost on video datasets. Extensive experiments\non Blurred-300VW, the proposed Real-world Motion Blur (RWMB) datasets and 300VW\ndemonstrate the superior performance to the state-of-the-art methods. Datasets\nand models will be publicly available at\nhttps://keqiangsun.github.io/projects/FAB/FAB.html.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 17:00:11 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Sun", "Keqiang", ""], ["Wu", "Wayne", ""], ["Liu", "Tinghao", ""], ["Yang", "Shuo", ""], ["Wang", "Quan", ""], ["Zhou", "Qiang", ""], ["Ye", "Zuochang", ""], ["Qian", "Chen", ""]]}, {"id": "1910.12122", "submitter": "Ata Jodeiri", "authors": "Ata Jodeiri, Yoshito Otake, Reza A. Zoroofi, Yuta Hiasa, Masaki Takao,\n  Keisuke Uemura, Nobuhiko Sugano, Yoshinobu Sato", "title": "Estimation of Pelvic Sagittal Inclination from Anteroposterior\n  Radiograph Using Convolutional Neural Networks: Proof-of-Concept Study", "comments": "Best Technical Paper Award Winner of CAOS 2018\n  (https://www.caos-international.org/award-paper.php)", "journal-ref": null, "doi": "10.29007/w6t7", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alignment of the bones in standing position provides useful information in\nsurgical planning. In total hip arthroplasty (THA), pelvic sagittal inclination\n(PSI) angle in the standing position is an important factor in planning of cup\nalignment and has been estimated mainly from radiographs. Previous methods for\nPSI estimation used a patient-specific CT to create digitally reconstructed\nradiographs (DRRs) and compare them with the radiograph to estimate relative\nposition between the pelvis and the x-ray detector. In this study, we developed\na method that estimates PSI angle from a single anteroposterior radiograph\nusing two convolutional neural networks (CNNs) without requiring the\npatient-specific CT, which reduces radiation exposure of the patient and opens\nup the possibility of application in a larger number of hospitals where CT is\nnot acquired in a routine protocol.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 19:27:18 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Jodeiri", "Ata", ""], ["Otake", "Yoshito", ""], ["Zoroofi", "Reza A.", ""], ["Hiasa", "Yuta", ""], ["Takao", "Masaki", ""], ["Uemura", "Keisuke", ""], ["Sugano", "Nobuhiko", ""], ["Sato", "Yoshinobu", ""]]}, {"id": "1910.12162", "submitter": "Jong Chul Ye", "authors": "Mathews Jacob, Merry P. Mani, and Jong Chul Ye", "title": "Structured Low-Rank Algorithms: Theory, MR Applications, and Links to\n  Machine Learning", "comments": "Accepted for IEEE Signal Processing Magazine", "journal-ref": null, "doi": "10.1109/MSP.2019.2950432", "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey, we provide a detailed review of recent advances in the\nrecovery of continuous domain multidimensional signals from their few\nnon-uniform (multichannel) measurements using structured low-rank matrix\ncompletion formulation. This framework is centered on the fundamental duality\nbetween the compactness (e.g., sparsity) of the continuous signal and the rank\nof a structured matrix, whose entries are functions of the signal. This\nproperty enables the reformulation of the signal recovery as a low-rank\nstructured matrix completion, which comes with performance guarantees. We will\nalso review fast algorithms that are comparable in complexity to current\ncompressed sensing methods, which enables the application of the framework to\nlarge-scale magnetic resonance (MR) recovery problems. The remarkable\nflexibility of the formulation can be used to exploit signal properties that\nare difficult to capture by current sparse and low-rank optimization\nstrategies. We demonstrate the utility of the framework in a wide range of MR\nimaging (MRI) applications, including highly accelerated imaging,\ncalibration-free acquisition, MR artifact correction, and ungated dynamic MRI.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 01:46:02 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Jacob", "Mathews", ""], ["Mani", "Merry P.", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1910.12165", "submitter": "Yiming Li", "authors": "Jia Xu, Yiming Li, Yong Jiang, Shu-Tao Xia", "title": "Adversarial Defense via Local Flatness Regularization", "comments": "Accepted by the ICIP 2020. The first two authors contributed equally\n  to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial defense is a popular and important research area. Due to its\nintrinsic mechanism, one of the most straightforward and effective ways of\ndefending attacks is to analyze the property of loss surface in the input\nspace. In this paper, we define the local flatness of the loss surface as the\nmaximum value of the chosen norm of the gradient regarding to the input within\na neighborhood centered on the benign sample, and discuss the relationship\nbetween the local flatness and adversarial vulnerability. Based on the\nanalysis, we propose a novel defense approach via regularizing the local\nflatness, dubbed local flatness regularization (LFR). We also demonstrate the\neffectiveness of the proposed method from other perspectives, such as human\nvisual mechanism, and analyze the relationship between LFR and other related\nmethods theoretically. Experiments are conducted to verify our theory and\ndemonstrate the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 02:12:20 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 06:53:06 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 11:49:26 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2020 07:19:03 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Xu", "Jia", ""], ["Li", "Yiming", ""], ["Jiang", "Yong", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "1910.12181", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua\n  Chai, Kurt Keutzer", "title": "Multi-source Domain Adaptation for Semantic Segmentation", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-to-real domain adaptation for semantic segmentation has been\nactively studied for various applications such as autonomous driving. Existing\nmethods mainly focus on a single-source setting, which cannot easily handle a\nmore practical scenario of multiple sources with different distributions. In\nthis paper, we propose to investigate multi-source domain adaptation for\nsemantic segmentation. Specifically, we design a novel framework, termed\nMulti-source Adversarial Domain Aggregation Network (MADAN), which can be\ntrained in an end-to-end manner. First, we generate an adapted domain for each\nsource with dynamic semantic consistency while aligning at the pixel-level\ncycle-consistently towards the target. Second, we propose sub-domain\naggregation discriminator and cross-domain cycle discriminator to make\ndifferent adapted domains more closely aggregated. Finally, feature-level\nalignment is performed between the aggregated domain and target domain while\ntraining the segmentation network. Extensive experiments from synthetic GTA and\nSYNTHIA to real Cityscapes and BDDS datasets demonstrate that the proposed\nMADAN model outperforms state-of-the-art approaches. Our source code is\nreleased at: https://github.com/Luodian/MADAN.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 05:04:25 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhao", "Sicheng", ""], ["Li", "Bo", ""], ["Yue", "Xiangyu", ""], ["Gu", "Yang", ""], ["Xu", "Pengfei", ""], ["Hu", "Runbo", ""], ["Chai", "Hua", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1910.12206", "submitter": "Venkatesh Ramesh", "authors": "Venkatesh R, Anand Metha", "title": "Segmenting Ships in Satellite Imagery With Squeeze and Excitation U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ship-detection task in satellite imagery presents significant obstacles\nto even the most state of the art segmentation models due to lack of labelled\ndataset or approaches which are not able to generalize to unseen images. The\nmost common methods for semantic segmentation involve complex two-stage\nnetworks or networks which make use of a multi-scale scene parsing module. In\nthis paper, we propose a modified version of the popular U-Net architecture\ncalled Squeeze and Excitation U-Net and train it with a loss that helps in\ndirectly optimizing the intersection over union (IoU) score. Our method gives\ncomparable performance to other methods while having the additional benefit of\nbeing computationally efficient.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 08:28:51 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["R", "Venkatesh", ""], ["Metha", "Anand", ""]]}, {"id": "1910.12223", "submitter": "Jing Zhang", "authors": "Jing Zhang and Zhe Chen and Dacheng Tao", "title": "Human Keypoint Detection by Progressive Context Refinement", "comments": "Technical Report for \"Joint COCO and MapillaryWorkshop at ICCV 2019:\n  COCO Keypoint Detection Challenge Track\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human keypoint detection from a single image is very challenging due to\nocclusion, blur, illumination and scale variance of person instances. In this\npaper, we find that context information plays an important role in addressing\nthese issues, and propose a novel method named progressive context refinement\n(PCR) for human keypoint detection. First, we devise a simple but effective\ncontext-aware module (CAM) that can efficiently integrate spatial and channel\ncontext information to aid feature learning for locating hard keypoints. Then,\nwe construct the PCR model by stacking several CAMs sequentially with shortcuts\nand employ multi-task learning to progressively refine the context information\nand predictions. Besides, to maximize PCR's potential for the aforementioned\nhard case inference, we propose a hard-negative person detection mining\nstrategy together with a joint-training strategy by exploiting the unlabeled\ncoco dataset and external dataset. Extensive experiments on the COCO keypoint\ndetection benchmark demonstrate the superiority of PCR over representative\nstate-of-the-art (SOTA) methods. Our single model achieves comparable\nperformance with the winner of the 2018 COCO Keypoint Detection Challenge. The\nfinal ensemble model sets a new SOTA on this benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 09:57:08 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhang", "Jing", ""], ["Chen", "Zhe", ""], ["Tao", "Dacheng", ""]]}, {"id": "1910.12227", "submitter": "Ali Shahin Shamsabadi", "authors": "Ali Shahin Shamsabadi, Changjae Oh, Andrea Cavallaro", "title": "EdgeFool: An Adversarial Image Enhancement Filter", "comments": null, "journal-ref": "Proceedings of the 45th IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP)2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are intentionally perturbed images that mislead\nclassifiers. These images can, however, be easily detected using denoising\nalgorithms, when high-frequency spatial perturbations are used, or can be\nnoticed by humans, when perturbations are large. In this paper, we propose\nEdgeFool, an adversarial image enhancement filter that learns structure-aware\nadversarial perturbations. EdgeFool generates adversarial images with\nperturbations that enhance image details via training a fully convolutional\nneural network end-to-end with a multi-task loss function. This loss function\naccounts for both image detail enhancement and class misleading objectives. We\nevaluate EdgeFool on three classifiers (ResNet-50, ResNet-18 and AlexNet) using\ntwo datasets (ImageNet and Private-Places365) and compare it with six\nadversarial methods (DeepFool, SparseFool, Carlini-Wagner, SemanticAdv,\nNon-targeted and Private Fast Gradient Sign Methods). Code is available at\nhttps://github.com/smartcameras/EdgeFool.git.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 10:16:26 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 10:08:19 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Shamsabadi", "Ali Shahin", ""], ["Oh", "Changjae", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "1910.12243", "submitter": "Zhengxuan Ling", "authors": "Zhengxuan Ling, Xinyu Tao, Yu Zhang, Xi Chen", "title": "Solving Optimization Problems through Fully Convolutional Networks: an\n  Application to the Travelling Salesman Problem", "comments": "25pages,7figures,research article", "journal-ref": null, "doi": "10.1186/s11671-018-2831-8", "report-no": null, "categories": "cs.LG cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the new wave of artificial intelligence, deep learning is impacting\nvarious industries. As a closely related area, optimization algorithms greatly\ncontribute to the development of deep learning. But the reverse applications\nare still insufficient. Is there any efficient way to solve certain\noptimization problem through deep learning? The key is to convert the\noptimization to a representation suitable for deep learning. In this paper, a\ntraveling salesman problem (TSP) is studied. Considering that deep learning is\ngood at image processing, an image representation method is proposed to\ntransfer a TSP to an image. Based on samples of a 10 city TSP, a fully\nconvolutional network (FCN) is used to learn the mapping from a feasible region\nto an optimal solution. The training process is analyzed and interpreted\nthrough stages. A visualization method is presented to show how a FCN can\nunderstand the training task of a TSP. Once the training is completed, no\nsignificant effort is required to solve a new TSP and the prediction is\nobtained on the scale of milliseconds. The results show good performance in\nfinding the global optimal solution. Moreover, the developed FCN model has been\ndemonstrated on TSP's with different city numbers, proving excellent\ngeneralization performance.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 11:32:39 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Ling", "Zhengxuan", ""], ["Tao", "Xinyu", ""], ["Zhang", "Yu", ""], ["Chen", "Xi", ""]]}, {"id": "1910.12246", "submitter": "Junyu Liu", "authors": "Junyu Liu, Xiang Li, Jin Wang, Jiqiang Zhou, Jianxiong Shen", "title": "Prediction stability as a criterion in active learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs made by deep learning rely heavily on large number of\nannotated samples. To overcome this shortcoming, active learning is a possible\nsolution. Beside the previous active learning algorithms that only adopted\ninformation after training, we propose a new class of method based on the\ninformation during training, named sequential-based method. An specific\ncriterion of active learning called prediction stability is proposed to prove\nthe feasibility of sequential-based methods. Experiments are made on CIFAR-10\nand CIFAR-100, and the results indicates that prediction stability is effective\nand works well on fewer-labeled datasets. Prediction stability reaches the\naccuracy of traditional acquisition functions like entropy on CIFAR-10, and\nnotably outperforms them on CIFAR-100.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 12:01:37 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Liu", "Junyu", ""], ["Li", "Xiang", ""], ["Wang", "Jin", ""], ["Zhou", "Jiqiang", ""], ["Shen", "Jianxiong", ""]]}, {"id": "1910.12257", "submitter": "Peter M. Roth", "authors": "Martin Hirzer, Peter M. Roth, Vincent Lepetit", "title": "Smart Hypothesis Generation for Efficient and Robust Room Layout\n  Estimation", "comments": "Accepted: Winter Conference on Applications of Computer Vision (WACV)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to efficiently estimate the spatial layout of a\nroom from a single monocular RGB image. As existing approaches based on\nlow-level feature extraction, followed by a vanishing point estimation are very\nslow and often unreliable in realistic scenarios, we build on semantic\nsegmentation of the input image. To obtain better segmentations, we introduce a\nrobust, accurate and very efficient hypothesize-and-test scheme. The key idea\nis to use three segmentation hypotheses, each based on a different number of\nvisible walls. For each hypothesis, we predict the image locations of the room\ncorners and select the hypothesis for which the layout estimated from the room\ncorners is consistent with the segmentation. We demonstrate the efficiency and\nrobustness of our method on three challenging benchmark datasets, where we\nsignificantly outperform the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 13:10:13 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Hirzer", "Martin", ""], ["Roth", "Peter M.", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1910.12259", "submitter": "Peter M. Roth", "authors": "Mina Basirat, Peter M. Roth", "title": "L*ReLU: Piece-wise Linear Activation Functions for Deep Fine-grained\n  Visual Categorization", "comments": "Accepted: Winter Conference on Applications of Computer Vision (WACV)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks paved the way for significant improvements in image\nvisual categorization during the last years. However, even though the tasks are\nhighly varying, differing in complexity and difficulty, existing solutions\nmostly build on the same architectural decisions. This also applies to the\nselection of activation functions (AFs), where most approaches build on\nRectified Linear Units (ReLUs). In this paper, however, we show that the choice\nof a proper AF has a significant impact on the classification accuracy, in\nparticular, if fine, subtle details are of relevance. Therefore, we propose to\nmodel the degree of absence and the presence of features via the AF by using\npiece-wise linear functions, which we refer to as L*ReLU. In this way, we can\nensure the required properties, while still inheriting the benefits in terms of\ncomputational efficiency from ReLUs. We demonstrate our approach for the task\nof Fine-grained Visual Categorization (FGVC), running experiments on seven\ndifferent benchmark datasets. The results do not only demonstrate superior\nresults but also that for different tasks, having different characteristics,\ndifferent AFs are selected.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 13:15:32 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Basirat", "Mina", ""], ["Roth", "Peter M.", ""]]}, {"id": "1910.12273", "submitter": "Shyamgopal Karthik", "authors": "Shyamgopal Karthik, Abhinav Moudgil and Vineet Gandhi", "title": "Exploring 3 R's of Long-term Tracking: Re-detection, Recovery and\n  Reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have proposed several long term tracking benchmarks and\nhighlight the importance of moving towards long-duration tracking to bridge the\ngap with application requirements. The current evaluation methodologies,\nhowever, do not focus on several aspects that are crucial in a long term\nperspective like Re-detection, Recovery, and Reliability. In this paper, we\npropose novel evaluation strategies for a more in-depth analysis of trackers\nfrom a long-term perspective. More specifically, (a) we test re-detection\ncapability of the trackers in the wild by simulating virtual cuts, (b) we\ninvestigate the role of chance in the recovery of tracker after failure and (c)\nwe propose a novel metric allowing visual inference on the ability of a tracker\nto track contiguously (without any failure) at a given accuracy. We present\nseveral original insights derived from an extensive set of quantitative and\nqualitative experiments.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 14:46:08 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Karthik", "Shyamgopal", ""], ["Moudgil", "Abhinav", ""], ["Gandhi", "Vineet", ""]]}, {"id": "1910.12278", "submitter": "Kaiwei Zeng", "authors": "Kaiwei Zeng", "title": "Hierarchical Clustering with Hard-batch Triplet Loss for Person\n  Re-identification", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most unsupervised person re-identification (re-ID), people often adopt\nunsupervised domain adaptation (UDA) method. UDA often train on the labeled\nsource dataset and evaluate on the target dataset, which often focuses on\nlearning differences between the source dataset and the target dataset to\nimprove the generalization of the model. Base on these, we explore how to make\nuse of the similarity of samples to conduct a fully unsupervised method which\njust trains on the unlabeled target dataset. Concretely, we propose a\nhierarchical clustering-guided re-ID (HCR) method. We use hierarchical\nclustering to generate pseudo labels and use these pseudo labels as monitors to\nconduct the training. In order to exclude hard examples and promote the\nconvergence of the model, We use PK sampling in each iteration, which randomly\nselects a fixed number of samples from each cluster for training. We evaluate\nour model on Market-1501, DukeMTMC-reID and MSMT17. Results show that HCR gets\nthe state-of-the-arts and achieves 55.3% mAP on Market-1501 and 46.8% mAP on\nDukeMTMC-reID. Our code will be released soon.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 15:07:57 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 07:46:20 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Zeng", "Kaiwei", ""]]}, {"id": "1910.12286", "submitter": "Yi Xu", "authors": "Yi Xu, Longwen Gao, Kai Tian, Shuigeng Zhou, Huyang Sun", "title": "Non-Local ConvLSTM for Video Compression Artifact Reduction", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video compression artifact reduction aims to recover high-quality videos from\nlow-quality compressed videos. Most existing approaches use a single\nneighboring frame or a pair of neighboring frames (preceding and/or following\nthe target frame) for this task. Furthermore, as frames of high quality overall\nmay contain low-quality patches, and high-quality patches may exist in frames\nof low quality overall, current methods focusing on nearby peak-quality frames\n(PQFs) may miss high-quality details in low-quality frames. To remedy these\nshortcomings, in this paper we propose a novel end-to-end deep neural network\ncalled non-local ConvLSTM (NL-ConvLSTM in short) that exploits multiple\nconsecutive frames. An approximate non-local strategy is introduced in\nNL-ConvLSTM to capture global motion patterns and trace the spatiotemporal\ndependency in a video sequence. This approximate strategy makes the non-local\nmodule work in a fast and low space-cost way. Our method uses the preceding and\nfollowing frames of the target frame to generate a residual, from which a\nhigher quality frame is reconstructed. Experiments on two datasets show that\nNL-ConvLSTM outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 15:45:41 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Xu", "Yi", ""], ["Gao", "Longwen", ""], ["Tian", "Kai", ""], ["Zhou", "Shuigeng", ""], ["Sun", "Huyang", ""]]}, {"id": "1910.12295", "submitter": "Rongcheng Lin", "authors": "Rongcheng Lin, Jing Xiao and Jianping Fan", "title": "MOD: A Deep Mixture Model with Online Knowledge Distillation for Large\n  Scale Video Temporal Concept Localization", "comments": "ICCV 2019 YouTube8M workshop", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present and discuss a deep mixture model with online\nknowledge distillation (MOD) for large-scale video temporal concept\nlocalization, which is ranked 3rd in the 3rd YouTube-8M Video Understanding\nChallenge. Specifically, we find that by enabling knowledge sharing with online\ndistillation, fintuning a mixture model on a smaller dataset can achieve better\nevaluation performance. Based on this observation, in our final solution, we\ntrained and fintuned 12 NeXtVLAD models in parallel with a 2-layer online\ndistillation structure. The experimental results show that the proposed\ndistillation structure can effectively avoid overfitting and shows superior\ngeneralization performance. The code is publicly available at:\nhttps://github.com/linrongc/solution_youtube8m_v3\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 16:24:42 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lin", "Rongcheng", ""], ["Xiao", "Jing", ""], ["Fan", "Jianping", ""]]}, {"id": "1910.12322", "submitter": "Arda Efe Okay", "authors": "Arda Efe Okay, Manal AlGhamdi, Robert Westendorp and Mohamed\n  Abdel-Mottaleb", "title": "Multi-Resolution Overlapping Stripes Network for Person\n  Re-Identification", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the person re-identification (PReID) problem by\ncombining global and local information at multiple feature resolutions with\ndifferent loss functions. Many previous studies address this problem using\neither part-based features or global features. In case of part-based\nrepresentation, the spatial correlation between these parts is not considered,\nwhile global-based representation are not sensitive to spatial variations. This\npaper presents a part-based model with a multi-resolution network that uses\ndifferent level of features. The output of the last two conv blocks is then\npartitioned horizontally and processed in pairs with overlapping stripes to\ncover the important information that might lie between parts. We use different\nloss functions to combine local and global information for classification.\nExperimental results on a benchmark dataset demonstrate that the presented\nmethod outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 18:38:42 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Okay", "Arda Efe", ""], ["AlGhamdi", "Manal", ""], ["Westendorp", "Robert", ""], ["Abdel-Mottaleb", "Mohamed", ""]]}, {"id": "1910.12324", "submitter": "Gal Sadeh Kenigsfield", "authors": "Gal Sadeh Kenigsfield, Ran El-Yaniv", "title": "Leveraging Auxiliary Text for Deep Recognition of Unseen Visual\n  Relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most difficult tasks in scene understanding is recognizing\ninteractions between objects in an image. This task is often called visual\nrelationship detection (VRD). We consider the question of whether, given\nauxiliary textual data in addition to the standard visual data used for\ntraining VRD models, VRD performance can be improved. We present a new deep\nmodel that can leverage additional textual data. Our model relies on a shared\ntext--image representation of subject-verb-object relationships appearing in\nthe text, and object interactions in images. Our method is the first to enable\nrecognition of visual relationships missing in the visual training data and\nappearing only in the auxiliary text. We test our approach on two different\ntext sources: text originating in images and text originating in books. We test\nand validate our approach using two large-scale recognition tasks: VRD and\nScene Graph Generation. We show a surprising result: Our approach works better\nwith text originating in books, and outperforms the text originating in images\non the task of unseen relationship recognition. It is comparable to the model\nwhich utilizes text originating in images on the task of seen relationship\nrecognition.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 18:45:04 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kenigsfield", "Gal Sadeh", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1910.12325", "submitter": "Anuroop Sriram", "authors": "Anuroop Sriram, Jure Zbontar, Tullie Murrell, C. Lawrence Zitnick,\n  Aaron Defazio, Daniel K. Sodickson", "title": "GrappaNet: Combining Parallel Imaging with Deep Learning for Multi-Coil\n  MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Image (MRI) acquisition is an inherently slow process\nwhich has spurred the development of two different acceleration methods:\nacquiring multiple correlated samples simultaneously (parallel imaging) and\nacquiring fewer samples than necessary for traditional signal processing\nmethods (compressed sensing). Both methods provide complementary approaches to\naccelerating the speed of MRI acquisition. In this paper, we present a novel\nmethod to integrate traditional parallel imaging methods into deep neural\nnetworks that is able to generate high quality reconstructions even for high\nacceleration factors. The proposed method, called GrappaNet, performs\nprogressive reconstruction by first mapping the reconstruction problem to a\nsimpler one that can be solved by a traditional parallel imaging methods using\na neural network, followed by an application of a parallel imaging method, and\nfinally fine-tuning the output with another neural network. The entire network\ncan be trained end-to-end. We present experimental results on the recently\nreleased fastMRI dataset and show that GrappaNet can generate higher quality\nreconstructions than competing methods for both $4\\times$ and $8\\times$\nacceleration.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 19:11:05 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 18:05:39 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 03:23:13 GMT"}, {"version": "v4", "created": "Mon, 30 Mar 2020 23:33:06 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Sriram", "Anuroop", ""], ["Zbontar", "Jure", ""], ["Murrell", "Tullie", ""], ["Zitnick", "C. Lawrence", ""], ["Defazio", "Aaron", ""], ["Sodickson", "Daniel K.", ""]]}, {"id": "1910.12326", "submitter": "Alireza Chamanzar", "authors": "Alireza Chamanzar, Yao Nie", "title": "Weakly Supervised Multi-Task Learning for Cell Detection and\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell detection and segmentation is fundamental for all downstream analysis of\ndigital pathology images. However, obtaining the pixel-level ground truth for\nsingle cell segmentation is extremely labor intensive. To overcome this\nchallenge, we developed an end-to-end deep learning algorithm to perform both\nsingle cell detection and segmentation using only point labels. This is\nachieved through the combination of different task orientated point label\nencoding methods and a multi-task scheduler for training. We apply and validate\nour algorithm on PMS2 stained colon rectal cancer and tonsil tissue images.\nCompared to the state-of-the-art, our algorithm shows significant improvement\nin cell detection and segmentation without increasing the annotation efforts.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 19:11:39 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Chamanzar", "Alireza", ""], ["Nie", "Yao", ""]]}, {"id": "1910.12329", "submitter": "Aicha BenTaieb", "authors": "A\\\"icha BenTaieb, Ghassan Hamarneh", "title": "Deep Learning Models for Digital Pathology", "comments": "Technical report, Survey, 58 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathology images; microscopy images of stained tissue biopsies contain\nfundamental prognostic information that forms the foundation of pathological\nanalysis and diagnostic medicine. However, diagnostics from histopathology\nimages generally rely on a visual cognitive assessment of tissue slides which\nimplies an inherent element of interpretation and hence subjectivity. Access to\ndigitized histopathology images enabled the development of computational\nsystems aiming at reducing manual intervention and automating parts of\npathologists' workflow. Specifically, applications of deep learning to\nhistopathology image analysis now offer opportunities for better quantitative\nmodeling of disease appearance and hence possibly improved prediction of\ndisease aggressiveness and patient outcome. However digitized histopathology\ntissue slides are unique in a variety of ways and come with their own set of\ncomputational challenges. In this survey, we summarize the different challenges\nfacing computational systems for digital pathology and provide a review of\nstate-of-the-art works that developed deep learning-based solutions for the\npredictive modeling of histopathology images from a detection, stain\nnormalization, segmentation, and tissue classification perspective. We then\ndiscuss the challenges facing the validation and integration of such deep\nlearning-based computational systems in clinical workflow and reflect on future\nopportunities for histopathology derived image measurements and better\npredictive modeling.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 19:38:58 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 01:35:20 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["BenTaieb", "A\u00efcha", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1910.12361", "submitter": "Huaizu Jiang", "authors": "Huaizu Jiang, Deqing Sun, Varun Jampani, Zhaoyang Lv, Erik\n  Learned-Miller, Jan Kautz", "title": "SENSE: a Shared Encoder Network for Scene-flow Estimation", "comments": "ICCV 2019 Oral", "journal-ref": "International Conference on Computer Vision 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a compact network for holistic scene flow estimation, called\nSENSE, which shares common encoder features among four closely-related tasks:\noptical flow estimation, disparity estimation from stereo, occlusion\nestimation, and semantic segmentation. Our key insight is that sharing features\nmakes the network more compact, induces better feature representations, and can\nbetter exploit interactions among these tasks to handle partially labeled data.\nWith a shared encoder, we can flexibly add decoders for different tasks during\ntraining. This modular design leads to a compact and efficient model at\ninference time. Exploiting the interactions among these tasks allows us to\nintroduce distillation and self-supervised losses in addition to supervised\nlosses, which can better handle partially labeled real-world data. SENSE\nachieves state-of-the-art results on several optical flow benchmarks and runs\nas fast as networks specifically designed for optical flow. It also compares\nfavorably against the state of the art on stereo and scene flow, while\nconsuming much less memory.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 21:44:00 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Jiang", "Huaizu", ""], ["Sun", "Deqing", ""], ["Jampani", "Varun", ""], ["Lv", "Zhaoyang", ""], ["Learned-Miller", "Erik", ""], ["Kautz", "Jan", ""]]}, {"id": "1910.12363", "submitter": "Dan Oneata", "authors": "Dan Oneata, Cosmin George Alexandru, Marius Stanescu, Octavian Pascu,\n  Alexandru Magan, Adrian Postelnicu, Horia Cucu", "title": "The Quo Vadis submission at Traffic4cast 2019", "comments": "Extended abstract for the Traffic4cast competition from NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe the submission of the Quo Vadis team to the Traffic4cast\ncompetition, which was organized as part of the NeurIPS 2019 series of\nchallenges. Our system consists of a temporal regression module, implemented as\n$1\\times1$ 2d convolutions, augmented with spatio-temporal biases. We have\nfound that using biases is a straightforward and efficient way to include\nseasonal patterns and to improve the performance of the temporal regression\nmodel. Our implementation obtains a mean squared error of $9.47\\times 10^{-3}$\non the test data, placing us on the eight place team-wise. We also present our\nattempts at incorporating spatial correlations into the model; however,\ncontrary to our expectations, adding this type of auxiliary information did not\nbenefit the main system. Our code is available at\nhttps://github.com/danoneata/traffic4cast.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 21:50:30 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Oneata", "Dan", ""], ["Alexandru", "Cosmin George", ""], ["Stanescu", "Marius", ""], ["Pascu", "Octavian", ""], ["Magan", "Alexandru", ""], ["Postelnicu", "Adrian", ""], ["Cucu", "Horia", ""]]}, {"id": "1910.12384", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A. Sindagi, Rajeev Yasarla, Vishal M. Patel", "title": "Pushing the Frontiers of Unconstrained Crowd Counting: New Dataset and\n  Benchmark Method", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a novel crowd counting network that progressively\ngenerates crowd density maps via residual error estimation. The proposed method\nuses VGG16 as the backbone network and employs density map generated by the\nfinal layer as a coarse prediction to refine and generate finer density maps in\na progressive fashion using residual learning. Additionally, the residual\nlearning is guided by an uncertainty-based confidence weighting mechanism that\npermits the flow of only high-confidence residuals in the refinement path. The\nproposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is\nevaluated on recent complex datasets, and it achieves significant improvements\nin errors.\n  Furthermore, we introduce a new large scale unconstrained crowd counting\ndataset (JHU-CROWD) that is ~2.8 larger than the most recent crowd counting\ndatasets in terms of the number of images. It contains 4,250 images with 1.11\nmillion annotations. In comparison to existing datasets, the proposed dataset\nis collected under a variety of diverse scenarios and environmental conditions.\nSpecifically, the dataset includes several images with weather-based\ndegradations and illumination variations in addition to many distractor images,\nmaking it a very challenging dataset. Additionally, the dataset consists of\nrich annotations at both image-level and head-level. Several recent methods are\nevaluated and compared on this dataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 00:02:33 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Yasarla", "Rajeev", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1910.12392", "submitter": "Ehsan Nowroozi", "authors": "Mauro Barni, Ehsan Nowroozi, Benedetta Tondi, Bowen Zhang", "title": "Effectiveness of random deep feature selection for securing image\n  manipulation detectors against adversarial examples", "comments": "Submitted to the ICASSP conference to be held in 2020, Barcelona,\n  Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate if the random feature selection approach proposed in [1] to\nimprove the robustness of forensic detectors to targeted attacks, can be\nextended to detectors based on deep learning features. In particular, we study\nthe transferability of adversarial examples targeting an original CNN image\nmanipulation detector to other detectors (a fully connected neural network and\na linear SVM) that rely on a random subset of the features extracted from the\nflatten layer of the original network. The results we got by considering three\nimage manipulation detection tasks (resizing, median filtering and adaptive\nhistogram equalization), two original network architectures and three classes\nof attacks, show that feature randomization helps to hinder attack\ntransferability, even if, in some cases, simply changing the architecture of\nthe detector, or even retraining the detector is enough to prevent the\ntransferability of the attacks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 10:33:32 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 11:37:28 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Barni", "Mauro", ""], ["Nowroozi", "Ehsan", ""], ["Tondi", "Benedetta", ""], ["Zhang", "Bowen", ""]]}, {"id": "1910.12399", "submitter": "Dennis N\\'u\\~nez Fern\\'andez", "authors": "Bryan Saldivar-Espinoza, Dennis N\\'u\\~nez-Fern\\'andez, Franklin\n  Porras-Barrientos, Alicia Alva-Mantari, Lisa Suzanne Leslie, Mirko Zimic", "title": "Portable system for the prediction of anemia based on the ocular\n  conjunctiva using Artificial Intelligence", "comments": "LatinX in AI Research at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anemia is a major health burden worldwide. Examining the hemoglobin level of\nblood is an important way to achieve the diagnosis of anemia, but it requires\nblood drawing and a blood test. In this work we propose a non-invasive, fast,\nand cost-effective screening test for iron-deficiency anemia in Peruvian young\nchildren. Our initial results show promising evidence for detecting\nconjunctival pallor anemia and Artificial Intelligence techniques with photos\ntaken with a popular smartphone.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:44:03 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Saldivar-Espinoza", "Bryan", ""], ["N\u00fa\u00f1ez-Fern\u00e1ndez", "Dennis", ""], ["Porras-Barrientos", "Franklin", ""], ["Alva-Mantari", "Alicia", ""], ["Leslie", "Lisa Suzanne", ""], ["Zimic", "Mirko", ""]]}, {"id": "1910.12423", "submitter": "Yen-Chi Hsu", "authors": "Yen-Chi Hsu, Cheng-Yao Hong, Wan-Cyuan Fan, Ming-Sui Lee, Davi Geiger,\n  Tyng-Luh Liu", "title": "ACE: Adaptive Confusion Energy for Natural World Data Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep learning, standard classification problems have\nachieved good results. However, conventional classification problems are often\ntoo idealistic. Most data in the natural world usually have imbalanced\ndistribution and fine-grained characteristics. Recently, many state-of-the-art\napproaches tend to focus on one or another separately, but rarely on both. In\nthis paper, we introduce a novel and adaptive batch-wise regularization based\non the proposed Adaptive Confusion Energy (ACE) to flexibly address the nature\nworld distribution, which usually involves fine-grained and long-tailed\nproperties at the same time. ACE increases the difficulty of the training\nprocess and further alleviates the overfitting problem. Through the datasets\nwith the technical issue in fine-grained (CUB, CAR, AIR) and long-tailed\n(ImageNet-LT), or comprehensive issues (CUB-LT, iNaturalist), the result shows\nthat the ACE is not only competitive to some state-of-the-art on performance\nbut also demonstrates the effectiveness of training.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:23:56 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 07:16:19 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 06:37:12 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Hsu", "Yen-Chi", ""], ["Hong", "Cheng-Yao", ""], ["Fan", "Wan-Cyuan", ""], ["Lee", "Ming-Sui", ""], ["Geiger", "Davi", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1910.12460", "submitter": "Kyle Xiao", "authors": "Kyle Xiao, Houdong Hu, Yan Wang", "title": "Applications of Generative Adversarial Models in Visual Search\n  Reformulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query reformulation is the process by which a input search query is refined\nby the user to match documents outside the original top-n results. On average,\nroughly 50% of text search queries involve some form of reformulation, and term\nsuggestion tools are used 35% of the time when offered to users. As prevalent\nas text search queries are, however, such a feature has yet to be explored at\nscale for visual search. This is because reformulation for images presents a\nnovel challenge to seamlessly transform visual features to match user intent\nwithin the context of a typical user session. In this paper, we present methods\nof semantically transforming visual queries, such as utilizing operations in\nthe latent space of a generative adversarial model for the scenarios of fashion\nand product search.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 06:27:38 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Xiao", "Kyle", ""], ["Hu", "Houdong", ""], ["Wang", "Yan", ""]]}, {"id": "1910.12462", "submitter": "Ankur Goswami", "authors": "Ankur Goswami, Joshua McGrath, Shanan Peters, Theodoros Rekatsinas", "title": "Fine-Grained Object Detection over Scientific Document Images with\n  Region Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study the problem of object detection over scanned images of scientific\ndocuments. We consider images that contain objects of varying aspect ratios and\nsizes and range from coarse elements such as tables and figures to fine\nelements such as equations and section headers. We find that current object\ndetectors fail to produce properly localized region proposals over such page\nobjects. We revisit the original R-CNN model and present a method for\ngenerating fine-grained proposals over document elements. We also present a\nregion embedding model that uses the convolutional maps of a proposal's\nneighbors as context to produce an embedding for each proposal. This region\nembedding is able to capture the semantic relationships between a target region\nand its surrounding context. Our end-to-end model produces an embedding for\neach proposal, then classifies each proposal by using a multi-head attention\nmodel that attends to the most important neighbors of a proposal. To evaluate\nour model, we collect and annotate a dataset of publications from heterogeneous\njournals. We show that our model, referred to as Attentive-RCNN, yields a 17%\nmAP improvement compared to standard object detection models.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 06:39:02 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 17:25:24 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Goswami", "Ankur", ""], ["McGrath", "Joshua", ""], ["Peters", "Shanan", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "1910.12467", "submitter": "Hong Huy Nguyen", "authors": "Huy H. Nguyen, Junichi Yamagishi, Isao Echizen", "title": "Use of a Capsule Network to Detect Fake Images and Videos", "comments": "Fixing Table 2's scale", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The revolution in computer hardware, especially in graphics processing units\nand tensor processing units, has enabled significant advances in computer\ngraphics and artificial intelligence algorithms. In addition to their many\nbeneficial applications in daily life and business,\ncomputer-generated/manipulated images and videos can be used for malicious\npurposes that violate security systems, privacy, and social trust. The deepfake\nphenomenon and its variations enable a normal user to use his or her personal\ncomputer to easily create fake videos of anybody from a short real online\nvideo. Several countermeasures have been introduced to deal with attacks using\nsuch videos. However, most of them are targeted at certain domains and are\nineffective when applied to other domains or new attacks. In this paper, we\nintroduce a capsule network that can detect various kinds of attacks, from\npresentation attacks using printed images and replayed videos to attacks using\nfake videos created using deep learning. It uses many fewer parameters than\ntraditional convolutional neural networks with similar performance. Moreover,\nwe explain, for the first time ever in the literature, the theory behind the\napplication of capsule networks to the forensics problem through detailed\nanalysis and visualization.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 07:01:49 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 14:30:58 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Nguyen", "Huy H.", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "1910.12468", "submitter": "Assia Benbihi", "authors": "Assia Benbihi, St\\'ephanie Aravecchia, Matthieu Geist, and C\\'edric\n  Pradalier", "title": "Image-Based Place Recognition on Bucolic Environment Across Seasons From\n  Semantic Edge Description", "comments": null, "journal-ref": "2020 IEEE International Conference on Robotics and Automation\n  (ICRA)", "doi": "10.1109/ICRA40945.2020.9197529", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the research effort on image-based place recognition is designed for\nurban environments. In bucolic environments such as natural scenes with low\ntexture and little semantic content, the main challenge is to handle the\nvariations in visual appearance across time such as illumination, weather,\nvegetation state or viewpoints. The nature of the variations is different and\nthis leads to a different approach to describing a bucolic scene. We introduce\na global image descriptor computed from its semantic and topological\ninformation. It is built from the wavelet transforms of the image semantic\nedges. Matching two images is then equivalent to matching their semantic edge\ndescriptors. We show that this method reaches state-of-the-art image retrieval\nperformance on two multi-season environment-monitoring datasets: the\nCMU-Seasons and the Symphony Lake dataset. It also generalises to urban scenes\non which it is on par with the current baselines NetVLAD and DELF.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 07:06:25 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 05:41:55 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 14:55:38 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2021 13:42:43 GMT"}, {"version": "v5", "created": "Thu, 1 Apr 2021 08:10:33 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Benbihi", "Assia", ""], ["Aravecchia", "St\u00e9phanie", ""], ["Geist", "Matthieu", ""], ["Pradalier", "C\u00e9dric", ""]]}, {"id": "1910.12514", "submitter": "Jiexiang Wang", "authors": "Jiexiang Wang, Hongyu Huang, Chaoqi Chen, Wenao Ma, Yue Huang, Xinghao\n  Ding", "title": "Multi-sequence Cardiac MR Segmentation with Adversarial Domain\n  Adaptation Network", "comments": "10th Workshop on Statistical Atlases and Computational Modelling of\n  the Heart (MICCAI2019 Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic and accurate segmentation of the ventricles and myocardium from\nmulti-sequence cardiac MRI (CMR) is crucial for the diagnosis and treatment\nmanagement for patients suffering from myocardial infarction (MI). However, due\nto the existence of domain shift among different modalities of datasets, the\nperformance of deep neural networks drops significantly when the training and\ntesting datasets are distinct. In this paper, we propose an unsupervised domain\nalignment method to explicitly alleviate the domain shifts among different\nmodalities of CMR sequences, \\emph{e.g.,} bSSFP, LGE, and T2-weighted. Our\nsegmentation network is attention U-Net with pyramid pooling module, where\nmulti-level feature space and output space adversarial learning are proposed to\ntransfer discriminative domain knowledge across different datasets. Moreover,\nwe further introduce a group-wise feature recalibration module to enforce the\nfine-grained semantic-level feature alignment that matching features from\ndifferent networks but with the same class label. We evaluate our method on the\nmulti-sequence cardiac MR Segmentation Challenge 2019 datasets, which contain\nthree different modalities of MRI sequences. Extensive experimental results\nshow that the proposed methods can obtain significant segmentation improvements\ncompared with the baseline models.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 09:20:23 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Jiexiang", ""], ["Huang", "Hongyu", ""], ["Chen", "Chaoqi", ""], ["Ma", "Wenao", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""]]}, {"id": "1910.12539", "submitter": "Seong Jae Kang", "authors": "Seongjae Kang, Jaeyoon Kim, Sung-eui Yoon", "title": "Virtual Piano using Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, Piano performances have been analyzed only based on visual\ninformation. Computer vision algorithms, e.g., Hough transform and binary\nthresholding, have been applied to find where the keyboard and specific keys\nare located. At the same time, Convolutional Neural Networks(CNNs) has been\nalso utilized to find whether specific keys are pressed or not, and how much\nintensity the keys are pressed only based on visual information. Especially for\ndetecting intensity, a new method of utilizing spatial, temporal CNNs model is\ndevised. Early fusion technique is especially applied in temporal CNNs\narchitecture to analyze hand movement. We also make a new dataset for training\neach model. Especially when finding an intensity of a pressed key, both of\nvideo frames and their optical flow images are used to train models to find\neffectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 10:36:30 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kang", "Seongjae", ""], ["Kim", "Jaeyoon", ""], ["Yoon", "Sung-eui", ""]]}, {"id": "1910.12585", "submitter": "Jean-Baptiste Weibel", "authors": "Jean-Baptiste Weibel, Timothy Patten, Markus Vincze", "title": "Addressing the Sim2Real Gap in Robotic 3D Object Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object classification with 3D data is an essential component of any scene\nunderstanding method. It has gained significant interest in a variety of\ncommunities, most notably in robotics and computer graphics. While the advent\nof deep learning has progressed the field of 3D object classification, most\nwork using this data type are solely evaluated on CAD model datasets.\nConsequently, current work does not address the discrepancies existing between\nreal and artificial data. In this work, we examine this gap in a robotic\ncontext by specifically addressing the problem of classification when\ntransferring from artificial CAD models to real reconstructed objects. This is\nperformed by training on ModelNet (CAD models) and evaluating on ScanNet\n(reconstructed objects). We show that standard methods do not perform well in\nthis task. We thus introduce a method that carefully samples object parts that\nare reproducible under various transformations and hence robust. Using graph\nconvolution to classify the composed graph of parts, our method significantly\nimproves upon the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 12:27:40 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Weibel", "Jean-Baptiste", ""], ["Patten", "Timothy", ""], ["Vincze", "Markus", ""]]}, {"id": "1910.12604", "submitter": "Xiyan Liu", "authors": "Xiyan Liu, Gaofeng Meng, Shiming Xiang, Chunhong Pan", "title": "FontGAN: A Unified Generative Framework for Chinese Character\n  Stylization and De-stylization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese character synthesis involves two related aspects, i.e., style\nmaintenance and content consistency. Although some methods have achieved\nremarkable success in synthesizing a character with specified style from\nstandard font, how to map characters to a specified style domain without losing\ntheir identifiability remains very challenging. In this paper, we propose a\nnovel model named FontGAN, which integrates the character stylization and\nde-stylization into a unified framework. In our model, we decouple character\nimages into style representation and content representation, which facilitates\nmore precise control of these two types of variables, thereby improving the\nquality of the generated results. We also introduce two modules, namely, font\nconsistency module (FCM) and content prior module (CPM). FCM exploits a\ncategory guided Kullback-Leibler loss to embedding the style representation\ninto different Gaussian distributions. It constrains the characters of the same\nfont in the training set globally. On the other hand, it enables our model to\nobtain style variables through sampling in testing phase. CPM provides content\nprior for the model to guide the content encoding process and alleviates the\nproblem of stroke deficiency during de-stylization. Extensive experimental\nresults on character stylization and de-stylization have demonstrated the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 12:38:39 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Liu", "Xiyan", ""], ["Meng", "Gaofeng", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1910.12625", "submitter": "Erwei Wang", "authors": "Erwei Wang, James J. Davis, Peter Y. K. Cheung, George A.\n  Constantinides", "title": "LUTNet: Learning FPGA Configurations for Highly Efficient Neural Network\n  Inference", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.00938.\n  Accepted manuscript uploaded 02/03/20. DOA 01/03/20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has shown that deep neural networks contain significant redundancy,\nand thus that high classification accuracy can be achieved even when weights\nand activations are quantized down to binary values. Network binarization on\nFPGAs greatly increases area efficiency by replacing resource-hungry\nmultipliers with lightweight XNOR gates. However, an FPGA's fundamental\nbuilding block, the K-LUT, is capable of implementing far more than an XNOR: it\ncan perform any K-input Boolean operation. Inspired by this observation, we\npropose LUTNet, an end-to-end hardware-software framework for the construction\nof area-efficient FPGA-based neural network accelerators using the native LUTs\nas inference operators. We describe the realization of both unrolled and tiled\nLUTNet architectures, with the latter facilitating smaller, less power-hungry\ndeployment over the former while sacrificing area and energy efficiency along\nwith throughput. For both varieties, we demonstrate that the exploitation of\nLUT flexibility allows for far heavier pruning than possible in prior works,\nresulting in significant area savings while achieving comparable accuracy.\nAgainst the state-of-the-art binarized neural network implementation, we\nachieve up to twice the area efficiency for several standard network models\nwhen inferencing popular datasets. We also demonstrate that even greater energy\nefficiency improvements are obtainable.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:04:56 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 23:26:43 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Wang", "Erwei", ""], ["Davis", "James J.", ""], ["Cheung", "Peter Y. K.", ""], ["Constantinides", "George A.", ""]]}, {"id": "1910.12699", "submitter": "Olivia Wiles", "authors": "Olivia Wiles, A. Sophia Koepke, Andrew Zisserman", "title": "Self-supervised learning of class embeddings from video", "comments": "4th International Workshop on Compact and Efficient Feature\n  Representation and Learning in Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work explores how to use self-supervised learning on videos to learn a\nclass-specific image embedding that encodes pose and shape information. At\ntrain time, two frames of the same video of an object class (e.g. human upper\nbody) are extracted and each encoded to an embedding. Conditioned on these\nembeddings, the decoder network is tasked to transform one frame into another.\nTo successfully perform long range transformations (e.g. a wrist lowered in one\nimage should be mapped to the same wrist raised in another), we introduce a\nhierarchical probabilistic network decoder model. Once trained, the embedding\ncan be used for a variety of downstream tasks and domains. We demonstrate our\napproach quantitatively on three distinct deformable object classes -- human\nfull bodies, upper bodies, faces -- and show experimentally that the learned\nembeddings do indeed generalise. They achieve state-of-the-art performance in\ncomparison to other self-supervised methods trained on the same datasets, and\napproach the performance of fully supervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:18:17 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wiles", "Olivia", ""], ["Koepke", "A. Sophia", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1910.12704", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (CMM), Jesus Angulo (CMM), Dominique Jeulin (CMM),\n  Daniel Balvay (PARCC - UMR-S U970), Charles-Andr\\'e Cuenod (PARCC - UMR-S\n  U970)", "title": "Multivariate mathematical morphology for DCE-MRI image analysis in\n  angiogenesis studies", "comments": null, "journal-ref": "Image Analysis and Stereology, International Society for\n  Stereology, 2015, 34 (1), pp.1-25", "doi": "10.5566/ias.1109", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new computer aided detection framework for tumours acquired on\nDCE-MRI (Dynamic Contrast Enhanced Magnetic Resonance Imaging) series on small\nanimals. In this approach we consider DCE-MRI series as multivariate images. A\nfull multivariate segmentation method based on dimensionality reduction, noise\nfiltering, supervised classification and stochastic watershed is explained and\ntested on several data sets. The two main key-points introduced in this paper\nare noise reduction preserving contours and spatio temporal segmentation by\nstochastic watershed. Noise reduction is performed in a special way that\nselects factorial axes of Factor Correspondence Analysis in order to preserves\ncontours. Then a spatio-temporal approach based on stochastic watershed is used\nto segment tumours. The results obtained are in accordance with the diagnosis\nof the medical doctors.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:26:49 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Noyel", "Guillaume", "", "CMM"], ["Angulo", "Jesus", "", "CMM"], ["Jeulin", "Dominique", "", "CMM"], ["Balvay", "Daniel", "", "PARCC - UMR-S U970"], ["Cuenod", "Charles-Andr\u00e9", "", "PARCC - UMR-S\n  U970"]]}, {"id": "1910.12707", "submitter": "Mattia Marconcini Dr.-Ing.", "authors": "Mattia Marconcini, Annekatrin Metz-Marconcini, Soner \\\"Ureyen, Daniela\n  Palacios-Lopez, Wiebke Hanke, Felix Bachofer, Julian Zeidler, Thomas Esch,\n  Noel Gorelick, Ashwin Kakarla, Emanuele Strano", "title": "Outlining where humans live -- The World Settlement Footprint 2015", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human settlements are the cause and consequence of most environmental and\nsocietal changes on Earth; however, their location and extent is still under\ndebate. We provide here a new 10m resolution (0.32 arc sec) global map of human\nsettlements on Earth for the year 2015, namely the World Settlement Footprint\n2015 (WSF2015). The raster dataset has been generated by means of an advanced\nclassification system which, for the first time, jointly exploits open-and-free\noptical and radar satellite imagery. The WSF2015 has been validated against\n900,000 samples labelled by crowdsourcing photointerpretation of very high\nresolution Google Earth imagery and outperforms all other similar existing\nlayers; in particular, it considerably improves the detection of very small\nsettlements in rural regions and better outlines scattered suburban areas. The\ndataset can be used at any scale of observation in support to all applications\nrequiring detailed and accurate information on human presence (e.g.,\nsocioeconomic development, population distribution, risks assessment, etc.).\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:29:10 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Marconcini", "Mattia", ""], ["Metz-Marconcini", "Annekatrin", ""], ["\u00dcreyen", "Soner", ""], ["Palacios-Lopez", "Daniela", ""], ["Hanke", "Wiebke", ""], ["Bachofer", "Felix", ""], ["Zeidler", "Julian", ""], ["Esch", "Thomas", ""], ["Gorelick", "Noel", ""], ["Kakarla", "Ashwin", ""], ["Strano", "Emanuele", ""]]}, {"id": "1910.12713", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, Bryan\n  Catanzaro", "title": "Few-shot Video-to-Video Synthesis", "comments": "In NeurIPS, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-to-video synthesis (vid2vid) aims at converting an input semantic\nvideo, such as videos of human poses or segmentation masks, to an output\nphotorealistic video. While the state-of-the-art of vid2vid has advanced\nsignificantly, existing approaches share two major limitations. First, they are\ndata-hungry. Numerous images of a target human subject or a scene are required\nfor training. Second, a learned model has limited generalization capability. A\npose-to-human vid2vid model can only synthesize poses of the single person in\nthe training set. It does not generalize to other humans that are not in the\ntraining set. To address the limitations, we propose a few-shot vid2vid\nframework, which learns to synthesize videos of previously unseen subjects or\nscenes by leveraging few example images of the target at test time. Our model\nachieves this few-shot generalization capability via a novel network weight\ngeneration module utilizing an attention mechanism. We conduct extensive\nexperimental validations with comparisons to strong baselines using several\nlarge-scale video datasets including human-dancing videos, talking-head videos,\nand street-scene videos. The experimental results verify the effectiveness of\nthe proposed framework in addressing the two limitations of existing vid2vid\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:33:09 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Liu", "Ming-Yu", ""], ["Tao", "Andrew", ""], ["Liu", "Guilin", ""], ["Kautz", "Jan", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1910.12727", "submitter": "Weiwei Zhang", "authors": "Weiwei Zhang, Changsheng chen, Xuechun Wu, Jialin Gao, Di Bao, Jiwei\n  Li, Xi Zhou", "title": "Layer Pruning for Accelerating Very Deep Neural Networks", "comments": "v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an adaptive pruning method. This method can cut off\nthe channel and layer adaptively. The proportion of the layer and the channel\nto be cut is learned adaptively. The pruning method proposed in this paper can\nreduce half of the parameters, and the accuracy will not decrease or even be\nhigher than baseline.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:49:42 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhang", "Weiwei", ""], ["chen", "Changsheng", ""], ["Wu", "Xuechun", ""], ["Gao", "Jialin", ""], ["Bao", "Di", ""], ["Li", "Jiwei", ""], ["Zhou", "Xi", ""]]}, {"id": "1910.12770", "submitter": "Alaaeldin El-Nouby", "authors": "Alaaeldin El-Nouby, Shuangfei Zhai, Graham W. Taylor, Joshua M.\n  Susskind", "title": "Skip-Clip: Self-Supervised Spatiotemporal Representation Learning by\n  Future Clip Order Ranking", "comments": "Holistic Video Understanding Workshop ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks require collecting and annotating large amounts of data\nto train successfully. In order to alleviate the annotation bottleneck, we\npropose a novel self-supervised representation learning approach for\nspatiotemporal features extracted from videos. We introduce Skip-Clip, a method\nthat utilizes temporal coherence in videos, by training a deep model for future\nclip order ranking conditioned on a context clip as a surrogate objective for\nvideo future prediction. We show that features learned using our method are\ngeneralizable and transfer strongly to downstream tasks. For action recognition\non the UCF101 dataset, we obtain 51.8% improvement over random initialization\nand outperform models initialized using inflated ImageNet parameters. Skip-Clip\nalso achieves results competitive with state-of-the-art self-supervision\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 15:54:45 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["El-Nouby", "Alaaeldin", ""], ["Zhai", "Shuangfei", ""], ["Taylor", "Graham W.", ""], ["Susskind", "Joshua M.", ""]]}, {"id": "1910.12771", "submitter": "Haiping Zhu", "authors": "Haiping Zhu, Zhizhong Huang, Hongming Shan, and Junping Zhang", "title": "Look globally, age locally: Face aging with an attention mechanism", "comments": "arXiv admin note: text overlap with arXiv:1807.09251 by other authors", "journal-ref": "IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), 2020", "doi": "10.1109/ICASSP40776.2020.9054553", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face aging is of great importance for cross-age recognition and\nentertainment-related applications. Recently, conditional generative\nadversarial networks (cGANs) have achieved impressive results for face aging.\nExisting cGANs-based methods usually require a pixel-wise loss to keep the\nidentity and background consistent. However, minimizing the pixel-wise loss\nbetween the input and synthesized images likely resulting in a ghosted or\nblurry face. To address this deficiency, this paper introduces an Attention\nConditional GANs (AcGANs) approach for face aging, which utilizes attention\nmechanism to only alert the regions relevant to face aging. In doing so, the\nsynthesized face can well preserve the background information and personal\nidentity without using the pixel-wise loss, and the ghost artifacts and\nblurriness can be significantly reduced. Based on the benchmarked dataset\nMorph, both qualitative and quantitative experiment results demonstrate\nsuperior performance over existing algorithms in terms of image quality,\npersonal identity, and age accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 21:00:49 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhu", "Haiping", ""], ["Huang", "Zhizhong", ""], ["Shan", "Hongming", ""], ["Zhang", "Junping", ""]]}, {"id": "1910.12795", "submitter": "Zhiting Hu", "authors": "Zhiting Hu, Bowen Tan, Ruslan Salakhutdinov, Tom Mitchell, Eric P.\n  Xing", "title": "Learning Data Manipulation for Augmentation and Weighting", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulating data, such as weighting data examples or augmenting with new\ninstances, has been increasingly used to improve model training. Previous work\nhas studied various rule- or learning-based approaches designed for specific\ntypes of data manipulation. In this work, we propose a new method that supports\nlearning different manipulation schemes with the same gradient-based algorithm.\nOur approach builds upon a recent connection of supervised learning and\nreinforcement learning (RL), and adapts an off-the-shelf reward learning\nalgorithm from RL for joint data manipulation learning and model training.\nDifferent parameterization of the \"data reward\" function instantiates different\nmanipulation schemes. We showcase data augmentation that learns a text\ntransformation network, and data weighting that dynamically adapts the data\nsample importance. Experiments show the resulting algorithms significantly\nimprove the image and text classification performance in low data regime and\nclass-imbalance problems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:46:24 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Hu", "Zhiting", ""], ["Tan", "Bowen", ""], ["Salakhutdinov", "Ruslan", ""], ["Mitchell", "Tom", ""], ["Xing", "Eric P.", ""]]}, {"id": "1910.12800", "submitter": "Xing Zhao", "authors": "Xing Zhao, Ping Lu, Yanyan Zhang, Jianxiong Chen, and Xiaoyang Li", "title": "Attenuating Random Noise in Seismic Data by a Deep Learning Approach", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the geophysical field, seismic noise attenuation has been considered as a\ncritical and long-standing problem, especially for the pre-stack data\nprocessing. Here, we propose a model to leverage the deep-learning model for\nthis task. Rather than directly applying an existing de-noising model from\nordinary images to the seismic data, we have designed a particular\ndeep-learning model, based on residual neural networks. It is named as\nN2N-Seismic, which has a strong ability to recover the seismic signals back to\nintact condition with the preservation of primary signals. The proposed model,\nachieving with great success in attenuating noise, has been tested on two\ndifferent seismic datasets. Several metrics show that our method outperforms\nconventional approaches in terms of Signal-to-Noise-Ratio, Mean-Squared-Error,\nPhase Spectrum, etc. Moreover, robust tests in terms of effectively removing\nrandom noise from any dataset with strong and weak noises have been extensively\nscrutinized in making sure that the proposed model is able to maintain a good\nlevel of adaptation while dealing with large variations of noise\ncharacteristics and intensities.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:53:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhao", "Xing", ""], ["Lu", "Ping", ""], ["Zhang", "Yanyan", ""], ["Chen", "Jianxiong", ""], ["Li", "Xiaoyang", ""]]}, {"id": "1910.12827", "submitter": "Michael Chang", "authors": "Rishi Veerapaneni, John D. Co-Reyes, Michael Chang, Michael Janner,\n  Chelsea Finn, Jiajun Wu, Joshua B. Tenenbaum, Sergey Levine", "title": "Entity Abstraction in Visual Model-Based Reinforcement Learning", "comments": "Accepted at CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tests the hypothesis that modeling a scene in terms of entities\nand their local interactions, as opposed to modeling the scene globally,\nprovides a significant benefit in generalizing to physical tasks in a\ncombinatorial space the learner has not encountered before. We present\nobject-centric perception, prediction, and planning (OP3), which to the best of\nour knowledge is the first fully probabilistic entity-centric dynamic latent\nvariable framework for model-based reinforcement learning that acquires entity\nrepresentations from raw visual observations without supervision and uses them\nto predict and plan. OP3 enforces entity-abstraction -- symmetric processing of\neach entity representation with the same locally-scoped function -- which\nenables it to scale to model different numbers and configurations of objects\nfrom those in training. Our approach to solving the key technical challenge of\ngrounding these entity representations to actual objects in the environment is\nto frame this variable binding problem as an inference problem, and we develop\nan interactive inference algorithm that uses temporal continuity and\ninteractive feedback to bind information about object properties to the entity\nvariables. On block-stacking tasks, OP3 generalizes to novel block\nconfigurations and more objects than observed during training, outperforming an\noracle model that assumes access to object supervision and achieving two to\nthree times better accuracy than a state-of-the-art video prediction model that\ndoes not exhibit entity abstraction.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:37:46 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 12:57:33 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 23:16:31 GMT"}, {"version": "v4", "created": "Sat, 11 Apr 2020 19:50:41 GMT"}, {"version": "v5", "created": "Wed, 6 May 2020 14:51:15 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Veerapaneni", "Rishi", ""], ["Co-Reyes", "John D.", ""], ["Chang", "Michael", ""], ["Janner", "Michael", ""], ["Finn", "Chelsea", ""], ["Wu", "Jiajun", ""], ["Tenenbaum", "Joshua B.", ""], ["Levine", "Sergey", ""]]}, {"id": "1910.12861", "submitter": "Weiwei Song", "authors": "Shutao Li and Weiwei Song and Leyuan Fang and Yushi Chen and Pedram\n  Ghamisi and J\\'on Atli Benediktsson", "title": "Deep Learning for Hyperspectral Image Classification: An Overview", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no.\n  9, pp. 6690-6709, Sep. 2019", "doi": "10.1109/TGRS.2019.2907932", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) classification has become a hot topic in the field\nof remote sensing. In general, the complex characteristics of hyperspectral\ndata make the accurate classification of such data challenging for traditional\nmachine learning methods. In addition, hyperspectral imaging often deals with\nan inherently nonlinear relation between the captured spectral information and\nthe corresponding materials. In recent years, deep learning has been recognized\nas a powerful feature-extraction tool to effectively address nonlinear problems\nand widely used in a number of image processing tasks. Motivated by those\nsuccessful applications, deep learning has also been introduced to classify\nHSIs and demonstrated good performance. This survey paper presents a systematic\nreview of deep learning-based HSI classification literatures and compares\nseveral strategies for this topic. Specifically, we first summarize the main\nchallenges of HSI classification which cannot be effectively overcome by\ntraditional machine learning methods, and also introduce the advantages of deep\nlearning to handle these problems. Then, we build a framework which divides the\ncorresponding works into spectral-feature networks, spatial-feature networks,\nand spectral-spatial-feature networks to systematically review the recent\nachievements in deep learning-based HSI classification. In addition,\nconsidering the fact that available training samples in the remote sensing\nfield are usually very limited and training deep networks require a large\nnumber of samples, we include some strategies to improve classification\nperformance, which can provide some guidelines for future studies on this\ntopic. Finally, several representative deep learning-based classification\nmethods are conducted on real HSIs in our experiments.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 11:50:27 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Li", "Shutao", ""], ["Song", "Weiwei", ""], ["Fang", "Leyuan", ""], ["Chen", "Yushi", ""], ["Ghamisi", "Pedram", ""], ["Benediktsson", "J\u00f3n Atli", ""]]}, {"id": "1910.12906", "submitter": "Uttaran Bhattacharya", "authors": "Uttaran Bhattacharya and Trisha Mittal and Rohan Chandra and Tanmay\n  Randhavane and Aniket Bera and Dinesh Manocha", "title": "STEP: Spatial Temporal Graph Convolutional Networks for Emotion\n  Perception from Gaits", "comments": null, "journal-ref": "Thirty-Fourth AAAI Conference on Artificial Intelligence 2020", "doi": "10.1609/aaai.v34i02.5490", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel classifier network called STEP, to classify perceived\nhuman emotion from gaits, based on a Spatial Temporal Graph Convolutional\nNetwork (ST-GCN) architecture. Given an RGB video of an individual walking, our\nformulation implicitly exploits the gait features to classify the emotional\nstate of the human into one of four emotions: happy, sad, angry, or neutral. We\nuse hundreds of annotated real-world gait videos and augment them with\nthousands of annotated synthetic gaits generated using a novel generative\nnetwork called STEP-Gen, built on an ST-GCN based Conditional Variational\nAutoencoder (CVAE). We incorporate a novel push-pull regularization loss in the\nCVAE formulation of STEP-Gen to generate realistic gaits and improve the\nclassification accuracy of STEP. We also release a novel dataset (E-Gait),\nwhich consists of $2,177$ human gaits annotated with perceived emotions along\nwith thousands of synthetic gaits. In practice, STEP can learn the affective\nfeatures and exhibits classification accuracy of 89% on E-Gait, which is 14 -\n30% more accurate over prior methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:43:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Bhattacharya", "Uttaran", ""], ["Mittal", "Trisha", ""], ["Chandra", "Rohan", ""], ["Randhavane", "Tanmay", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1910.12945", "submitter": "Jiaxin Xu", "authors": "Jiaxin Xu, Rui Wang, Vaibhav Rakheja", "title": "Literature Review: Human Segmentation with Static Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research topic is Human segmentation with static camera. This topic can\nbe divided into three sub-tasks, which are object detection, instance\nidentification and segmentation. These sub-tasks are three closely related\nsubjects. The development of each subject has great impact on the other two\nfields. In this literature review, we will first introduce the background of\nhuman segmentation and then talk about issues related to the above three fields\nas well as how they interact with each other.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 20:02:41 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Xu", "Jiaxin", ""], ["Wang", "Rui", ""], ["Rakheja", "Vaibhav", ""]]}, {"id": "1910.12976", "submitter": "Wanyu Lin", "authors": "Wanyu Lin, Zhaolin Gao, Baochun Li", "title": "Shoestring: Graph-Based Semi-Supervised Learning with Severely Limited\n  Labeled Data", "comments": "9 pages, 5 tables, 3 figures, accepted at CVPR2020, source code will\n  be released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based semi-supervised learning has been shown to be one of the most\neffective approaches for classification tasks from a wide range of domains,\nsuch as image classification and text classification, as they can exploit the\nconnectivity patterns between labeled and unlabeled samples to improve learning\nperformance. In this work, we advance this effective learning paradigm towards\na scenario where labeled data are severely limited. More specifically, we\naddress the problem of graph-based semi-supervised learning in the presence of\nseverely limited labeled samples, and propose a new framework, called {\\em\nShoestring}, that improves the learning performance through semantic transfer\nfrom these very few labeled samples to large numbers of unlabeled samples.\n  In particular, our framework learns a metric space in which classification\ncan be performed by computing the similarity to centroid embedding of each\nclass. {\\em Shoestring} is trained in an end-to-end fashion to learn to\nleverage the semantic knowledge of limited labeled samples as well as their\nconnectivity patterns with large numbers of unlabeled samples simultaneously.\nBy combining {\\em Shoestring} with graph convolutional networks, label\npropagation and their recent label-efficient variations (IGCN and GLP), we are\nable to achieve state-of-the-art node classification performance in the\npresence of very few labeled samples. In addition, we demonstrate the\neffectiveness of our framework on image classification tasks in the few-shot\nlearning regime, with significant gains on miniImageNet ($2.57\\%\\sim3.59\\%$)\nand tieredImageNet ($1.05\\%\\sim2.70\\%$).\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 21:23:01 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 14:34:18 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Lin", "Wanyu", ""], ["Gao", "Zhaolin", ""], ["Li", "Baochun", ""]]}, {"id": "1910.13003", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Zhen Liu, James M. Rehg, Le Song", "title": "Neural Similarity Learning", "comments": "NeurIPS 2019 (v3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inner product-based convolution has been the founding stone of convolutional\nneural networks (CNNs), enabling end-to-end learning of visual representation.\nBy generalizing inner product with a bilinear matrix, we propose the neural\nsimilarity which serves as a learnable parametric similarity measure for CNNs.\nNeural similarity naturally generalizes the convolution and enhances\nflexibility. Further, we consider the neural similarity learning (NSL) in order\nto learn the neural similarity adaptively from training data. Specifically, we\npropose two different ways of learning the neural similarity: static NSL and\ndynamic NSL. Interestingly, dynamic neural similarity makes the CNN become a\ndynamic inference network. By regularizing the bilinear matrix, NSL can be\nviewed as learning the shape of kernel and the similarity measure\nsimultaneously. We further justify the effectiveness of NSL with a theoretical\nviewpoint. Most importantly, NSL shows promising performance in visual\nrecognition and few-shot learning, validating the superiority of NSL over the\ninner product-based convolution counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 23:06:56 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 16:59:32 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 10:39:39 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Liu", "Weiyang", ""], ["Liu", "Zhen", ""], ["Rehg", "James M.", ""], ["Song", "Le", ""]]}, {"id": "1910.13029", "submitter": "Anderson de Andrade", "authors": "Anderson de Andrade", "title": "Best Practices for Convolutional Neural Networks Applied to Object\n  Recognition in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research project studies the impact of convolutional neural networks\n(CNN) in image classification tasks. We explore different architectures and\ntraining configurations with the use of ReLUs, Nesterov's accelerated gradient,\ndropout and maxout networks. We work with the CIFAR-10 dataset as part of a\nKaggle competition to identify objects in images. Initial results show that\nCNNs outperform our baseline by acting as invariant feature detectors.\nComparisons between different preprocessing procedures show better results for\nglobal contrast normalization and ZCA whitening. ReLUs are much faster than\ntanh units and outperform sigmoids. We provide extensive details about our\ntraining hyperparameters, providing intuition for their selection that could\nhelp enhance learning in similar situations. We design 4 models of\nconvolutional neural networks that explore characteristics such as depth,\nnumber of feature maps, size and overlap of kernels, pooling regions, and\ndifferent subsampling techniques. Results favor models of moderate depth that\nuse an extensive number of parameters in both convolutional and dense layers.\nMaxout networks are able to outperform rectifiers on some models but introduce\ntoo much noise as the complexity of the fully-connected layers increases. The\nfinal discussion explains our results and provides additional techniques that\ncould improve performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 01:18:15 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["de Andrade", "Anderson", ""]]}, {"id": "1910.13042", "submitter": "David Ho", "authors": "David Joon Ho, Dig V. K. Yarlagadda, Timothy M. D'Alfonso, Matthew G.\n  Hanna, Anne Grabenstetter, Peter Ntiamoah, Edi Brogi, Lee K. Tan, Thomas J.\n  Fuchs", "title": "Deep Multi-Magnification Networks for Multi-Class Breast Cancer Image\n  Segmentation", "comments": "Accepted at Computerized Medical Imaging and Graphics", "journal-ref": null, "doi": "10.1016/j.compmedimag.2021.101866", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathologic analysis of surgical excision specimens for breast carcinoma is\nimportant to evaluate the completeness of surgical excision and has\nimplications for future treatment. This analysis is performed manually by\npathologists reviewing histologic slides prepared from formalin-fixed tissue.\nIn this paper, we present Deep Multi-Magnification Network trained by partial\nannotation for automated multi-class tissue segmentation by a set of patches\nfrom multiple magnifications in digitized whole slide images. Our proposed\narchitecture with multi-encoder, multi-decoder, and multi-concatenation\noutperforms other single and multi-magnification-based architectures by\nachieving the highest mean intersection-over-union, and can be used to\nfacilitate pathologists' assessments of breast cancer.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 02:25:36 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 19:05:05 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ho", "David Joon", ""], ["Yarlagadda", "Dig V. K.", ""], ["D'Alfonso", "Timothy M.", ""], ["Hanna", "Matthew G.", ""], ["Grabenstetter", "Anne", ""], ["Ntiamoah", "Peter", ""], ["Brogi", "Edi", ""], ["Tan", "Lee K.", ""], ["Fuchs", "Thomas J.", ""]]}, {"id": "1910.13046", "submitter": "Risheng Liu", "authors": "Risheng Liu and Yuxi Zhang and Shichao Cheng and Zhongxuan Luo and Xin\n  Fan", "title": "Converged Deep Framework Assembling Principled Modules for CS-MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed Sensing Magnetic Resonance Imaging (CS-MRI) significantly\naccelerates MR data acquisition at a sampling rate much lower than the Nyquist\ncriterion. A major challenge for CS-MRI lies in solving the severely ill-posed\ninverse problem to reconstruct aliasing-free MR images from the sparse k-space\ndata. Conventional methods typically optimize an energy function, producing\nreconstruction of high quality, but their iterative numerical solvers\nunavoidably bring extremely slow processing. Recent data-driven techniques are\nable to provide fast restoration by either learning direct prediction to final\nreconstruction or plugging learned modules into the energy optimizer.\nNevertheless, these data-driven predictors cannot guarantee the reconstruction\nfollowing constraints underlying the regularizers of conventional methods so\nthat the reliability of their reconstruction results are questionable. In this\npaper, we propose a converged deep framework assembling principled modules for\nCS-MRI that fuses learning strategy with the iterative solver of a conventional\nreconstruction energy. This framework embeds an optimal condition checking\nmechanism, fostering \\emph{efficient} and \\emph{reliable} reconstruction. We\nalso apply the framework to two practical tasks, \\emph{i.e.}, parallel imaging\nand reconstruction with Rician noise. Extensive experiments on both benchmark\nand manufacturer-testing images demonstrate that the proposed method reliably\nconverges to the optimal solution more efficiently and accurately than the\nstate-of-the-art in various scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 02:30:57 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Liu", "Risheng", ""], ["Zhang", "Yuxi", ""], ["Cheng", "Shichao", ""], ["Luo", "Zhongxuan", ""], ["Fan", "Xin", ""]]}, {"id": "1910.13049", "submitter": "Qiming Zhang", "authors": "Qiming Zhang, Jing Zhang, Wei Liu, Dacheng Tao", "title": "Category Anchor-Guided Unsupervised Domain Adaptation for Semantic\n  Segmentation", "comments": "11 pages, 2 figures, accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to enhance the generalization\ncapability of a certain model from a source domain to a target domain. UDA is\nof particular significance since no extra effort is devoted to annotating\ntarget domain samples. However, the different data distributions in the two\ndomains, or \\emph{domain shift/discrepancy}, inevitably compromise the UDA\nperformance. Although there has been a progress in matching the marginal\ndistributions between two domains, the classifier favors the source domain\nfeatures and makes incorrect predictions on the target domain due to\ncategory-agnostic feature alignment. In this paper, we propose a novel category\nanchor-guided (CAG) UDA model for semantic segmentation, which explicitly\nenforces category-aware feature alignment to learn shared discriminative\nfeatures and classifiers simultaneously. First, the category-wise centroids of\nthe source domain features are used as guided anchors to identify the active\nfeatures in the target domain and also assign them pseudo-labels. Then, we\nleverage an anchor-based pixel-level distance loss and a discriminative loss to\ndrive the intra-category features closer and the inter-category features\nfurther apart, respectively. Finally, we devise a stagewise training mechanism\nto reduce the error accumulation and adapt the proposed model progressively.\nExperiments on both the GTA5$\\rightarrow $Cityscapes and SYNTHIA$\\rightarrow\n$Cityscapes scenarios demonstrate the superiority of our CAG-UDA model over the\nstate-of-the-art methods. The code is available at\n\\url{https://github.com/RogerZhangzz/CAG_UDA}.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 02:40:32 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 05:35:27 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Zhang", "Qiming", ""], ["Zhang", "Jing", ""], ["Liu", "Wei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1910.13050", "submitter": "Rudrasis Chakraborty Dr.", "authors": "Liu Yang, Rudrasis Chakraborty and Stella X. Yu", "title": "POIRot: A rotation invariant omni-directional pointnet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-cloud is an efficient way to represent 3D world. Analysis of\npoint-cloud deals with understanding the underlying 3D geometric structure. But\ndue to the lack of smooth topology, and hence the lack of neighborhood\nstructure, standard correlation can not be directly applied on point-cloud. One\nof the popular approaches to do point correlation is to partition the\npoint-cloud into voxels and extract features using standard 3D correlation. But\nthis approach suffers from sparsity of point-cloud and hence results in\nmultiple empty voxels. One possible solution to deal with this problem is to\nlearn a MLP to map a point or its local neighborhood to a high dimensional\nfeature space. All these methods suffer from a large number of parameters\nrequirement and are susceptible to random rotations. A popular way to make the\nmodel \"invariant\" to rotations is to use data augmentation techniques with\nsmall rotations but the potential drawback includes \\item more training samples\n\\item susceptible to large rotations. In this work, we develop a rotation\ninvariant point-cloud segmentation and classification scheme based on the\nomni-directional camera model (dubbed as {\\bf POIRot$^1$}). Our proposed model\nis rotationally invariant and can preserve geometric shape of a 3D point-cloud.\nBecause of the inherent rotation invariant property, our proposed framework\nrequires fewer number of parameters (please see \\cite{Iandola2017SqueezeNetAA}\nand the references therein for motivation of lean models). Several experiments\nhave been performed to show that our proposed method can beat the\nstate-of-the-art algorithms in classification and part segmentation\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 02:41:19 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 02:51:50 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Yang", "Liu", ""], ["Chakraborty", "Rudrasis", ""], ["Yu", "Stella X.", ""]]}, {"id": "1910.13055", "submitter": "Rui Fan", "authors": "Rui Fan, Yuan Wang, Lei Qiao, Ruiwen Yao, Peng Han, Weidong Zhang,\n  Ioannis Pitas, Ming Liu", "title": "PT-ResNet: Perspective Transformation-Based Residual Network for\n  Semantic Road Image Segmentation", "comments": "5 pages, 5 figures, accepted by 2019 IEEE International Conference on\n  Imaging Systems and Techniques (IST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic road region segmentation is a high-level task, which paves the way\ntowards road scene understanding. This paper presents a residual network\ntrained for semantic road segmentation. Firstly, we represent the projections\nof road disparities in the v-disparity map as a linear model, which can be\nestimated by optimizing the v-disparity map using dynamic programming. This\nlinear model is then utilized to reduce the redundant information in the left\nand right road images. The right image is also transformed into the left\nperspective view, which greatly enhances the road surface similarity between\nthe two images. Finally, the processed stereo images and their disparity maps\nare concatenated to create a set of 3D images, which are then utilized to train\nour neural network. The experimental results illustrate that our network\nachieves a maximum F1-measure of approximately 91.19% when analyzing the images\nfrom the KITTI road dataset.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 02:53:46 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Fan", "Rui", ""], ["Wang", "Yuan", ""], ["Qiao", "Lei", ""], ["Yao", "Ruiwen", ""], ["Han", "Peng", ""], ["Zhang", "Weidong", ""], ["Pitas", "Ioannis", ""], ["Liu", "Ming", ""]]}, {"id": "1910.13062", "submitter": "Ziye Zhang", "authors": "Ziye Zhang, Li Sun, Zhilin Zheng and Qingli Li", "title": "Disentangling the Spatial Structure and Style in Conditional VAE", "comments": "5 pages, 3 figures", "journal-ref": "Published on ICIP 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to disentangle the latent space in cVAE into the spatial\nstructure and the style code, which are complementary to each other, with one\nof them $z_s$ being label relevant and the other $z_u$ irrelevant. The\ngenerator is built by a connected encoder-decoder and a label condition mapping\nnetwork. Depending on whether the label is related with the spatial structure,\nthe output $z_s$ from the condition mapping network is used either as a style\ncode or a spatial structure code. The encoder provides the label irrelevant\nposterior from which $z_u$ is sampled. The decoder employs $z_s$ and $z_u$ in\neach layer by adaptive normalization like SPADE or AdaIN. Extensive experiments\non two datasets with different types of labels show the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 03:14:13 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 09:02:56 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhang", "Ziye", ""], ["Sun", "Li", ""], ["Zheng", "Zhilin", ""], ["Li", "Qingli", ""]]}, {"id": "1910.13066", "submitter": "David Berga", "authors": "David Berga, Xos\\'e R. Fdez-Vidal, Xavier Otazu, Xos\\'e M. Pardo", "title": "SID4VAM: A Benchmark Dataset with Synthetic Images for Visual Attention\n  Modeling", "comments": "10 pages, 8 figures, 3 tables, conference paper (ICCV 2019),\n  http://openaccess.thecvf.com/content_ICCV_2019/papers/Berga_SID4VAM_A_Benchmark_Dataset_With_Synthetic_Images_for_Visual_Attention_ICCV_2019_paper.pdf", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV) 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A benchmark of saliency models performance with a synthetic image dataset is\nprovided. Model performance is evaluated through saliency metrics as well as\nthe influence of model inspiration and consistency with human psychophysics.\nSID4VAM is composed of 230 synthetic images, with known salient regions. Images\nwere generated with 15 distinct types of low-level features (e.g. orientation,\nbrightness, color, size...) with a target-distractor pop-out type of synthetic\npatterns. We have used Free-Viewing and Visual Search task instructions and 7\nfeature contrasts for each feature category. Our study reveals that\nstate-of-the-art Deep Learning saliency models do not perform well with\nsynthetic pattern images, instead, models with Spectral/Fourier inspiration\noutperform others in saliency metrics and are more consistent with human\npsychophysical experimentation. This study proposes a new way to evaluate\nsaliency models in the forthcoming literature, accounting for synthetic images\nwith uniquely low-level feature contexts, distinct from previous eye tracking\nimage datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 03:29:36 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Berga", "David", ""], ["Fdez-Vidal", "Xos\u00e9 R.", ""], ["Otazu", "Xavier", ""], ["Pardo", "Xos\u00e9 M.", ""]]}, {"id": "1910.13076", "submitter": "Alceu Emanuel Bissoto", "authors": "Alceu Bissoto, Eduardo Valle, Sandra Avila", "title": "The Six Fronts of the Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks fostered a newfound interest in generative\nmodels, resulting in a swelling wave of new works that new-coming researchers\nmay find formidable to surf. In this paper, we intend to help those\nresearchers, by splitting that incoming wave into six \"fronts\": Architectural\nContributions, Conditional Techniques, Normalization and Constraint\nContributions, Loss Functions, Image-to-image Translations, and Validation\nMetrics. The division in fronts organizes literature into approachable blocks,\nultimately communicating to the reader how the area is evolving. Previous\nsurveys in the area, which this works also tabulates, focus on a few of those\nfronts, leaving a gap that we propose to fill with a more integrated,\ncomprehensive overview. Here, instead of an exhaustive survey, we opt for a\nstraightforward review: our target is to be an entry point to this vast\nliterature, and also to be able to update experienced researchers to the newest\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 04:07:00 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Bissoto", "Alceu", ""], ["Valle", "Eduardo", ""], ["Avila", "Sandra", ""]]}, {"id": "1910.13077", "submitter": "Zhaoyang Zeng", "authors": "Bei Liu, Zhicheng Huang, Zhaoyang Zeng, Zheyu Chen, Jianlong Fu", "title": "Learning Rich Image Region Representation for Visual Question Answering", "comments": "Rank 2 in VQA Challenge 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to boost VQA by leveraging more powerful feature extractors by\nimproving the representation ability of both visual and text features and the\nensemble of models. For visual feature, some detection techniques are used to\nimprove the detector. For text feature, we adopt BERT as the language model and\nfind that it can significantly improve VQA performance. Our solution won the\nsecond place in the VQA Challenge 2019.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 04:10:06 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Liu", "Bei", ""], ["Huang", "Zhicheng", ""], ["Zeng", "Zhaoyang", ""], ["Chen", "Zheyu", ""], ["Fu", "Jianlong", ""]]}, {"id": "1910.13081", "submitter": "Tao Wang", "authors": "Tao Wang and Yu Li and Bingyi Kang and Junnan Li and Jun Hao Liew and\n  Sheng Tang and Steven Hoi and Jiashi Feng", "title": "Classification Calibration for Long-tail Instance Segmentation", "comments": "This report presents our winning solution to LVIS 2019 challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remarkable progress has been made in object instance detection and\nsegmentation in recent years. However, existing state-of-the-art methods are\nmostly evaluated with fairly balanced and class-limited benchmarks, such as\nMicrosoft COCO dataset [8]. In this report, we investigate the performance drop\nphenomenon of state-of-the-art two-stage instance segmentation models when\nprocessing extreme long-tail training data based on the LVIS [5] dataset, and\nfind a major cause is the inaccurate classification of object proposals. Based\non this observation, we propose to calibrate the prediction of classification\nhead to improve recognition performance for the tail classes. Without much\nadditional cost and modification of the detection model architecture, our\ncalibration method improves the performance of the baseline by a large margin\non the tail classes. Codes will be available. Importantly, after the\nsubmission, we find significant improvement can be further achieved by\nmodifying the calibration head, which we will update later.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 04:29:25 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 03:12:19 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 00:56:29 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Wang", "Tao", ""], ["Li", "Yu", ""], ["Kang", "Bingyi", ""], ["Li", "Junnan", ""], ["Liew", "Jun Hao", ""], ["Tang", "Sheng", ""], ["Hoi", "Steven", ""], ["Feng", "Jiashi", ""]]}, {"id": "1910.13089", "submitter": "Md Mahfuzur Rahman", "authors": "Md Mahfuzur Rahman, Daniel Pimentel-Alarcon", "title": "GLIMPS: A Greedy Mixed Integer Approach for Super Robust Matched\n  Subspace Detection", "comments": "8 pages, 5 figures, 57th Allerton Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to diverse nature of data acquisition and modern applications, many\ncontemporary problems involve high dimensional datum $\\x \\in \\R^\\d$ whose\nentries often lie in a union of subspaces and the goal is to find out which\nentries of $\\x$ match with a particular subspace $\\sU$, classically called\n\\emph {matched subspace detection}. Consequently, entries that match with one\nsubspace are considered as inliers w.r.t the subspace while all other entries\nare considered as outliers. Proportion of outliers relative to each subspace\nvaries based on the degree of coordinates from subspaces. This problem is a\ncombinatorial NP-hard in nature and has been immensely studied in recent years.\nExisting approaches can solve the problem when outliers are sparse. However, if\noutliers are abundant or in other words if $\\x$ contains coordinates from a\nfair amount of subspaces, this problem can't be solved with acceptable accuracy\nor within a reasonable amount of time. This paper proposes a two-stage approach\ncalled \\emph{Greedy Linear Integer Mixed Programmed Selector} (GLIMPS) for this\nabundant-outliers setting, which combines a greedy algorithm and mixed integer\nformulation and can tolerate over 80\\% outliers, outperforming the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 05:15:40 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Rahman", "Md Mahfuzur", ""], ["Pimentel-Alarcon", "Daniel", ""]]}, {"id": "1910.13093", "submitter": "Jinghuai Zhang", "authors": "Zixuan Huang, Jinghuai Zhang, Jing Liao", "title": "Style Mixer: Semantic-aware Multi-Style Transfer Network", "comments": "Pacific Graphics 2019", "journal-ref": null, "doi": "10.1111/cgf.13853", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural style transfer frameworks have obtained astonishing visual\nquality and flexibility in Single-style Transfer (SST), but little attention\nhas been paid to Multi-style Transfer (MST) which refers to simultaneously\ntransferring multiple styles to the same image. Compared to SST, MST has the\npotential to create more diverse and visually pleasing stylization results. In\nthis paper, we propose the first MST framework to automatically incorporate\nmultiple styles into one result based on regional semantics. We first improve\nthe existing SST backbone network by introducing a novel multi-level feature\nfusion module and a patch attention module to achieve better semantic\ncorrespondences and preserve richer style details. For MST, we designed a\nconceptually simple yet effective region-based style fusion module to insert\ninto the backbone. It assigns corresponding styles to content regions based on\nsemantic matching, and then seamlessly combines multiple styles together.\nComprehensive evaluations demonstrate that our framework outperforms existing\nworks of SST and MST.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 05:35:34 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Huang", "Zixuan", ""], ["Zhang", "Jinghuai", ""], ["Liao", "Jing", ""]]}, {"id": "1910.13102", "submitter": "Rui Fan", "authors": "Huaiyang Huang, Rui Fan, Yilong Zhu, Ming Liu, Ioannis Pitas", "title": "A Robust Pavement Mapping System Based on Normal-Constrained Stereo\n  Visual Odometry", "comments": "6 pages, 7 figures, 3 tables, accepted for publishing on 2019 IEEE\n  International Conference on Imaging Systems and Techniques (IST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pavement condition is crucial for civil infrastructure maintenance. This task\nusually requires efficient road damage localization, which can be accomplished\nby the visual odometry system embedded in unmanned aerial vehicles (UAVs).\nHowever, the state-of-the-art visual odometry and mapping methods suffer from\nlarge drift under the degeneration of the scene structure. To alleviate this\nissue, we integrate normal constraints into the visual odometry process, which\ngreatly helps to avoid large drift. By parameterizing the normal vector on the\ntangential plane, the normal factors are coupled with traditional reprojection\nfactors in the pose optimization procedure. The experimental results\ndemonstrate the effectiveness of the proposed system. The overall absolute\ntrajectory error is improved by approximately 20%, which indicates that the\nestimated trajectory is much more accurate than that obtained using other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 06:17:09 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Huang", "Huaiyang", ""], ["Fan", "Rui", ""], ["Zhu", "Yilong", ""], ["Liu", "Ming", ""], ["Pitas", "Ioannis", ""]]}, {"id": "1910.13113", "submitter": "Kazuhiro Fukui", "authors": "Kazuhiro Fukui, Naoya Sogi, Takumi Kobayashi, Jing-Hao Xue, Atsuto\n  Maki", "title": "Discriminant analysis based on projection onto generalized difference\n  subspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a new type of discriminant analysis based on the\northogonal projection of data onto a generalized difference subspace (GDS). In\nour previous work, we have demonstrated that GDS projection works as the\nquasi-orthogonalization of class subspaces, which is an effective feature\nextraction for subspace based classifiers. Interestingly, GDS projection also\nworks as a discriminant feature extraction through a similar mechanism to the\nFisher discriminant analysis (FDA). A direct proof of the connection between\nGDS projection and FDA is difficult due to the significant difference in their\nformulations. To avoid the difficulty, we first introduce geometrical Fisher\ndiscriminant analysis (gFDA) based on a simplified Fisher criterion. Our\nsimplified Fisher criterion is derived from a heuristic yet practically\nplausible principle: the direction of the sample mean vector of a class is in\nmost cases almost equal to that of the first principal component vector of the\nclass, under the condition that the principal component vectors are calculated\nby applying the principal component analysis (PCA) without data centering. gFDA\ncan work stably even under few samples, bypassing the small sample size (SSS)\nproblem of FDA. Next, we prove that gFDA is equivalent to GDS projection with a\nsmall correction term. This equivalence ensures GDS projection to inherit the\ndiscriminant ability from FDA via gFDA. Furthermore, to enhance the\nperformances of gFDA and GDS projection, we normalize the projected vectors on\nthe discriminant spaces. Extensive experiments using the extended Yale B+\ndatabase and the CMU face database show that gFDA and GDS projection have\nequivalent or better performance than the original FDA and its extensions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 06:56:17 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 03:19:56 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Fukui", "Kazuhiro", ""], ["Sogi", "Naoya", ""], ["Kobayashi", "Takumi", ""], ["Xue", "Jing-Hao", ""], ["Maki", "Atsuto", ""]]}, {"id": "1910.13136", "submitter": "Haoyu Ma", "authors": "Haoyu Ma, Qingmin Liao, Juncheng Zhang, Shaojun Liu, Jing-Hao Xue", "title": "An {\\alpha}-Matte Boundary Defocus Model Based Cascaded Network for\n  Multi-focus Image Fusion", "comments": "10 pages, 8 figures, journal Unfortunately, I cannot spell one of the\n  authors' name coorectly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing an all-in-focus image with a single camera is difficult since the\ndepth of field of the camera is usually limited. An alternative method to\nobtain the all-in-focus image is to fuse several images focusing at different\ndepths. However, existing multi-focus image fusion methods cannot obtain clear\nresults for areas near the focused/defocused boundary (FDB). In this paper, a\nnovel {\\alpha}-matte boundary defocus model is proposed to generate realistic\ntraining data with the defocus spread effect precisely modeled, especially for\nareas near the FDB. Based on this {\\alpha}-matte defocus model and the\ngenerated data, a cascaded boundary aware convolutional network termed MMF-Net\nis proposed and trained, aiming to achieve clearer fusion results around the\nFDB. More specifically, the MMF-Net consists of two cascaded sub-nets for\ninitial fusion and boundary fusion, respectively; these two sub-nets are\ndesigned to first obtain a guidance map of FDB and then refine the fusion near\nthe FDB. Experiments demonstrate that with the help of the new {\\alpha}-matte\nboundary defocus model, the proposed MMF-Net outperforms the state-of-the-art\nmethods both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:02:45 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 01:01:13 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Ma", "Haoyu", ""], ["Liao", "Qingmin", ""], ["Zhang", "Juncheng", ""], ["Liu", "Shaojun", ""], ["Xue", "Jing-Hao", ""]]}, {"id": "1910.13140", "submitter": "Neo Christopher Chung <", "authors": "Lennart Brocki, Neo Christopher Chung", "title": "Concept Saliency Maps to Visualize Relevant Features in Deep Generative\n  Models", "comments": "18th IEEE International Conference on Machine Learning and\n  Applications (ICMLA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating, explaining, and visualizing high-level concepts in generative\nmodels, such as variational autoencoders (VAEs), is challenging in part due to\na lack of known prediction classes that are required to generate saliency maps\nin supervised learning. While saliency maps may help identify relevant features\n(e.g., pixels) in the input for classification tasks of deep neural networks,\nsimilar frameworks are understudied in unsupervised learning. Therefore, we\nintroduce a new method of obtaining saliency maps for latent representations of\nknown or novel high-level concepts, often called concept vectors in generative\nmodels. Concept scores, analogous to class scores in classification tasks, are\ndefined as dot products between concept vectors and encoded input data, which\ncan be readily used to compute the gradients. The resulting concept saliency\nmaps are shown to highlight input features deemed important for high-level\nconcepts. Our method is applied to the VAE's latent space of CelebA dataset in\nwhich known attributes such as \"smiles\" and \"hats\" are used to elucidate\nrelevant facial features. Furthermore, our application to spatial\ntranscriptomic (ST) data of a mouse olfactory bulb demonstrates the potential\nof latent representations of morphological layers and molecular features in\nadvancing our understanding of complex biological systems. By extending the\npopular method of saliency maps to generative models, the proposed concept\nsaliency maps help improve interpretability of latent variable models in deep\nlearning.\n  Codes to reproduce and to implement concept saliency maps:\nhttps://github.com/lenbrocki/concept-saliency-maps\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:15:25 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Brocki", "Lennart", ""], ["Chung", "Neo Christopher", ""]]}, {"id": "1910.13141", "submitter": "Atsushi Yaguchi", "authors": "Atsushi Yaguchi, Taiji Suzuki, Shuhei Nitta, Yukinobu Sakata, Akiyuki\n  Tanizawa", "title": "Decomposable-Net: Scalable Low-Rank Compression for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing deep neural networks (DNNs) is important for real-world\napplications operating on resource-constrained devices. However, it is not\nstraightforward to change the model size (i.e., computational complexity) once\ntraining and compression are completed, calling for retraining to construct\nmodels suitable for different devices. In this paper, we propose a novel\nmethod, Decomposable-Net (the network decomposable in any size), which allows\nflexible changes to model size without retraining. We decompose weight matrices\nin the DNNs via singular value decomposition and adjust ranks according to the\ntarget model size. Unlike the existing methods, (1) we propose a learning\nmethod that explicitly minimizes losses for both of full-rank and low-rank\nnetworks, which is designed not only to maintain the performance of a full-rank\nnetwork but also to improve multiple low-rank networks in a single model. (2)\nWe also provide a mathematical analysis for the scalability of the\napproximation error with respect to the rank in each layer. Moreover, on the\nbasis of the analysis, (3) we introduce a simple criterion for rank selection\nthat effectively suppresses approximation error. In experiments on\nimage-classification tasks on CIFAR-10/100 and ImageNet datasets,\nDecomposable-Net yields favorable performance in a broader range of compressed\nmodels. In particular, Decomposable-Net achieves the top-1 accuracy of $73.2\\%$\nwith $0.27\\times$MACs on the ImageNet classification task with ResNet-50,\ncompared to low-rank tensor (Tucker) decomposition ($67.4\\% / 0.30\\times$) and\nuniversally slimmable networks ($70.6\\% / 0.26\\times$).\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:15:40 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 07:32:42 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Yaguchi", "Atsushi", ""], ["Suzuki", "Taiji", ""], ["Nitta", "Shuhei", ""], ["Sakata", "Yukinobu", ""], ["Tanizawa", "Akiyuki", ""]]}, {"id": "1910.13144", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro, Mariela De Lucas Alvarez, Mariia Dmitrieva,\n  Bilal Wehbe, Georgios Salavasidis, Shahab Heshmati-Alamdari, Juan F.\n  Fuentes-P\\'erez, Veronika Yordanova, Klemen Isteni\\v{c}, Thomas Guerneve", "title": "Results from the Robocademy ITN: Autonomy, Disturbance Rejection and\n  Perception for Advanced Marine Robotics", "comments": "19 pages, 20 figures, initial preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marine and Underwater resources are important part of the economy of many\ncountries. This requires significant financial resources into their\nconstruction and maintentance. Robotics is expected to fill this void, by\nautomating and/or removing humans from hostile environments in order to easily\nperform maintenance tasks. The Robocademy Marie Sklodowska-Curie Initial\nTraining Network was funded by the European Union's FP7 research program in\norder to train 13 Fellows into world-leading researchers in Marine and\nUnderwater Robotics. The fellows developed guided research into three areas of\nkey importance: Autonomy, Disturbance Rejection, and Perception. This paper\npresents a summary of the fellows' research in the three action lines. 71\nscientific publications were the primary result of this project, with many\nother publications currently in the pipeline. Most of the fellows have found\nemployment in Europe, which shows the high demand for this kind of experts. We\nbelieve the results from this project are already having an impact in the\nmarine robotics industry, as key technologies are being adopted already.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:28:04 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Valdenegro-Toro", "Matias", ""], ["Alvarez", "Mariela De Lucas", ""], ["Dmitrieva", "Mariia", ""], ["Wehbe", "Bilal", ""], ["Salavasidis", "Georgios", ""], ["Heshmati-Alamdari", "Shahab", ""], ["Fuentes-P\u00e9rez", "Juan F.", ""], ["Yordanova", "Veronika", ""], ["Isteni\u010d", "Klemen", ""], ["Guerneve", "Thomas", ""]]}, {"id": "1910.13157", "submitter": "Eran Treister", "authors": "Jonathan Ephrath, Moshe Eliasof, Lars Ruthotto, Eldad Haber and Eran\n  Treister", "title": "LeanConvNets: Low-cost Yet Effective Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2020.2972775", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have become indispensable for solving\nmachine learning tasks in speech recognition, computer vision, and other areas\nthat involve high-dimensional data. A CNN filters the input feature using a\nnetwork containing spatial convolution operators with compactly supported\nstencils. In practice, the input data and the hidden features consist of a\nlarge number of channels, which in most CNNs are fully coupled by the\nconvolution operators. This coupling leads to immense computational cost in the\ntraining and prediction phase. In this paper, we introduce LeanConvNets that\nare derived by sparsifying fully-coupled operators in existing CNNs. Our goal\nis to improve the efficiency of CNNs by reducing the number of weights,\nfloating point operations and latency times, with minimal loss of accuracy. Our\nlean convolution operators involve tuning parameters that controls the\ntrade-off between the network's accuracy and computational costs. These\nconvolutions can be used in a wide range of existing networks, and we exemplify\ntheir use in residual networks (ResNets). Using a range of benchmark problems\nfrom image classification and semantic segmentation, we demonstrate that the\nresulting LeanConvNet's accuracy is close to state-of-the-art networks while\nbeing computationally less expensive. In our tests, the lean versions of ResNet\nin most cases outperform comparable reduced architectures such as MobileNets\nand ShuffleNets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:51:10 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 10:50:12 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Ephrath", "Jonathan", ""], ["Eliasof", "Moshe", ""], ["Ruthotto", "Lars", ""], ["Haber", "Eldad", ""], ["Treister", "Eran", ""]]}, {"id": "1910.13222", "submitter": "Li Chen", "authors": "Li Chen, Guowei Zhu, Qi Li, Haifeng Li", "title": "Adversarial Example in Remote Sensing Image Recognition", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the wide application of remote sensing technology in various fields, the\naccuracy and security requirements for remote sensing images (RSIs) recognition\nare also increasing. In recent years, due to the rapid development of deep\nlearning in the field of image recognition, RSI recognition models based on\ndeep convolution neural networks (CNNs) outperform traditional hand-craft\nfeature techniques. However, CNNs also pose security issues when they show\ntheir capability of accurate classification. By adding a very small variation\nof the adversarial perturbation to the input image, the CNN model can be caused\nto produce erroneous results with extremely high confidence, and the\nmodification of the image is not perceived by the human eye. This added\nadversarial perturbation image is called an adversarial example, which poses a\nserious security problem for systems based on CNN model recognition results.\nThis paper, for the first time, analyzes adversarial example problem of RSI\nrecognition under CNN models. In the experiments, we used different attack\nalgorithms to fool multiple high-accuracy RSI recognition models trained on\nmultiple RSI datasets. The results show that RSI recognition models are also\nvulnerable to adversarial examples, and the models with different structures\ntrained on the same RSI dataset also have different vulnerabilities. For each\nRSI dataset, the number of features also affects the vulnerability of the\nmodel. Many features are good for defensive adversarial examples. Further, we\nfind that the attacked class of RSI has an attack selectivity property. The\nmisclassification of adversarial examples of the RSIs are related to the\nsimilarity of the original classes in the CNN feature space. In addition,\nadversarial examples in RSI recognition are of great significance for the\nsecurity of remote sensing applications, showing a huge potential for future\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 12:26:04 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 04:39:58 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Chen", "Li", ""], ["Zhu", "Guowei", ""], ["Li", "Qi", ""], ["Li", "Haifeng", ""]]}, {"id": "1910.13231", "submitter": "Ata Jodeiri", "authors": "Ata Jodeiri, Reza A. Zoroofi, Yuta Hiasa, Masaki Takao, Nobuhiko\n  Sugano, Yoshinobu Sato, Yoshito Otake", "title": "Region-based Convolution Neural Network Approach for Accurate\n  Segmentation of Pelvic Radiograph", "comments": "Accepted at ICBME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing usage of radiograph images as a most common medical\nimaging system for diagnosis, treatment planning, and clinical studies, it is\nincreasingly becoming a vital factor to use machine learning-based systems to\nprovide reliable information for surgical pre-planning. Segmentation of pelvic\nbone in radiograph images is a critical preprocessing step for some\napplications such as automatic pose estimation and disease detection. However,\nthe encoder-decoder style network known as U-Net has demonstrated limited\nresults due to the challenging complexity of the pelvic shapes, especially in\nsevere patients. In this paper, we propose a novel multi-task segmentation\nmethod based on Mask R-CNN architecture. For training, the network weights were\ninitialized by large non-medical dataset and fine-tuned with radiograph images.\nFurthermore, in the training process, augmented data was generated to improve\nnetwork performance. Our experiments show that Mask R-CNN utilizing multi-task\nlearning, transfer learning, and data augmentation techniques achieve 0.96 DICE\ncoefficient, which significantly outperforms the U-Net. Notably, for a fair\ncomparison, the same transfer learning and data augmentation techniques have\nbeen used for U-net training.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 12:45:21 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 12:49:11 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Jodeiri", "Ata", ""], ["Zoroofi", "Reza A.", ""], ["Hiasa", "Yuta", ""], ["Takao", "Masaki", ""], ["Sugano", "Nobuhiko", ""], ["Sato", "Yoshinobu", ""], ["Otake", "Yoshito", ""]]}, {"id": "1910.13232", "submitter": "Hanhe Lin", "authors": "Felix Wilhelm Siebert, Hanhe Lin", "title": "Detecting motorcycle helmet use with deep learning", "comments": null, "journal-ref": "Accident Analysis and Prevention 134 (2020) 105319", "doi": "10.1016/j.aap.2019.105319", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous motorization of traffic has led to a sustained increase in the\nglobal number of road related fatalities and injuries. To counter this,\ngovernments are focusing on enforcing safe and law-abiding behavior in traffic.\nHowever, especially in developing countries where the motorcycle is the main\nform of transportation, there is a lack of comprehensive data on the\nsafety-critical behavioral metric of motorcycle helmet use. This lack of data\nprohibits targeted enforcement and education campaigns which are crucial for\ninjury prevention. Hence, we have developed an algorithm for the automated\nregistration of motorcycle helmet usage from video data, using a deep learning\napproach. Based on 91,000 annotated frames of video data, collected at multiple\nobservation sites in 7 cities across the country of Myanmar, we trained our\nalgorithm to detect active motorcycles, the number and position of riders on\nthe motorcycle, as well as their helmet use. An analysis of the algorithm's\naccuracy on an annotated test data set, and a comparison to available\nhuman-registered helmet use data reveals a high accuracy of our approach. Our\nalgorithm registers motorcycle helmet use rates with an accuracy of -4.4% and\n+2.1% in comparison to a human observer, with minimal training for individual\nobservation sites. Without observation site specific training, the accuracy of\nhelmet use detection decreases slightly, depending on a number of factors. Our\napproach can be implemented in existing roadside traffic surveillance\ninfrastructure and can facilitate targeted data-driven injury prevention\ncampaigns with real-time speed. Implications of the proposed method, as well as\nmeasures that can further improve detection accuracy are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 12:50:40 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Siebert", "Felix Wilhelm", ""], ["Lin", "Hanhe", ""]]}, {"id": "1910.13249", "submitter": "Florian Golemo", "authors": "Martin Weiss, Simon Chamorro, Roger Girgis, Margaux Luck, Samira E.\n  Kahou, Joseph P. Cohen, Derek Nowrouzezahrai, Doina Precup, Florian Golemo,\n  Chris Pal", "title": "Navigation Agents for the Visually Impaired: A Sidewalk Simulator and\n  Experiments", "comments": "Accepted at CoRL2019. Code & video available at\n  https://mweiss17.github.io/SEVN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of blind and visually-impaired (BVI) people navigate urban\nenvironments every day, using smartphones for high-level path-planning and\nwhite canes or guide dogs for local information. However, many BVI people still\nstruggle to travel to new places. In our endeavor to create a navigation\nassistant for the BVI, we found that existing Reinforcement Learning (RL)\nenvironments were unsuitable for the task. This work introduces SEVN, a\nsidewalk simulation environment and a neural network-based approach to creating\na navigation agent. SEVN contains panoramic images with labels for house\nnumbers, doors, and street name signs, and formulations for several navigation\ntasks. We study the performance of an RL algorithm (PPO) in this setting. Our\npolicy model fuses multi-modal observations in the form of variable resolution\nimages, visible text, and simulated GPS data to navigate to a goal door. We\nhope that this dataset, simulator, and experimental results will provide a\nfoundation for further research into the creation of agents that can assist\nmembers of the BVI community with outdoor navigation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 13:23:02 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Weiss", "Martin", ""], ["Chamorro", "Simon", ""], ["Girgis", "Roger", ""], ["Luck", "Margaux", ""], ["Kahou", "Samira E.", ""], ["Cohen", "Joseph P.", ""], ["Nowrouzezahrai", "Derek", ""], ["Precup", "Doina", ""], ["Golemo", "Florian", ""], ["Pal", "Chris", ""]]}, {"id": "1910.13268", "submitter": "Kush Varshney", "authors": "Newton M. Kinyanjui, Timothy Odonga, Celia Cintas, Noel C. F. Codella,\n  Rameswar Panda, Prasanna Sattigeri, and Kush R. Varshney", "title": "Estimating Skin Tone and Effects on Classification Performance in\n  Dermatology Datasets", "comments": "NeurIPS 2019 Workshop on Fair ML for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision and deep learning have led to\nbreakthroughs in the development of automated skin image analysis. In\nparticular, skin cancer classification models have achieved performance higher\nthan trained expert dermatologists. However, no attempt has been made to\nevaluate the consistency in performance of machine learning models across\npopulations with varying skin tones. In this paper, we present an approach to\nestimate skin tone in benchmark skin disease datasets, and investigate whether\nmodel performance is dependent on this measure. Specifically, we use individual\ntypology angle (ITA) to approximate skin tone in dermatology datasets. We look\nat the distribution of ITA values to better understand skin color\nrepresentation in two benchmark datasets: 1) the ISIC 2018 Challenge dataset, a\ncollection of dermoscopic images of skin lesions for the detection of skin\ncancer, and 2) the SD-198 dataset, a collection of clinical images capturing a\nwide variety of skin diseases. To estimate ITA, we first develop segmentation\nmodels to isolate non-diseased areas of skin. We find that the majority of the\ndata in the the two datasets have ITA values between 34.5{\\deg} and 48{\\deg},\nwhich are associated with lighter skin, and is consistent with\nunder-representation of darker skinned populations in these datasets. We also\nfind no measurable correlation between performance of machine learning model\nand ITA values, though more comprehensive data is needed for further\nvalidation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 13:48:17 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Kinyanjui", "Newton M.", ""], ["Odonga", "Timothy", ""], ["Cintas", "Celia", ""], ["Codella", "Noel C. F.", ""], ["Panda", "Rameswar", ""], ["Sattigeri", "Prasanna", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1910.13296", "submitter": "Thai Son Nguyen", "authors": "Thai-Son Nguyen, Sebastian Stueker, Jan Niehues, Alex Waibel", "title": "Improving sequence-to-sequence speech recognition training with\n  on-the-fly data augmentation", "comments": "To appear in ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-Sequence (S2S) models recently started to show state-of-the-art\nperformance for automatic speech recognition (ASR). With these large and deep\nmodels overfitting remains the largest problem, outweighing performance\nimprovements that can be obtained from better architectures. One solution to\nthe overfitting problem is increasing the amount of available training data and\nthe variety exhibited by the training data with the help of data augmentation.\nIn this paper we examine the influence of three data augmentation methods on\nthe performance of two S2S model architectures. One of the data augmentation\nmethod comes from literature, while two other methods are our own development -\na time perturbation in the frequency domain and sub-sequence sampling. Our\nexperiments on Switchboard and Fisher data show state-of-the-art performance\nfor S2S models that are trained solely on the speech training data and do not\nuse additional text data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 14:38:22 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 08:12:31 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Nguyen", "Thai-Son", ""], ["Stueker", "Sebastian", ""], ["Niehues", "Jan", ""], ["Waibel", "Alex", ""]]}, {"id": "1910.13302", "submitter": "Roman Solovyev A", "authors": "Roman Solovyev, Weimin Wang, Tatiana Gabruseva", "title": "Weighted boxes fusion: Ensembling boxes from different object detection\n  models", "comments": null, "journal-ref": "Image and Vision Computing (2021): 104117", "doi": "10.1016/j.imavis.2021.104117", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel method for combining predictions of object\ndetection models: weighted boxes fusion. Our algorithm utilizes confidence\nscores of all proposed bounding boxes to constructs the averaged boxes. We\ntested method on several datasets and evaluated it in the context of the Open\nImages and COCO Object Detection tracks, achieving top results in these\nchallenges. The source code is publicly available at\nhttps://github.com/ZFTurbo/Weighted-Boxes-Fusion\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 14:45:26 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 12:21:04 GMT"}, {"version": "v3", "created": "Sat, 6 Feb 2021 14:47:37 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Solovyev", "Roman", ""], ["Wang", "Weimin", ""], ["Gabruseva", "Tatiana", ""]]}, {"id": "1910.13317", "submitter": "Zachary Serlin", "authors": "Zachary Serlin, Guang Yang, Brandon Sookraj, Calin Belta, and Roberto\n  Tron", "title": "Distributed and Consistent Multi-Image Feature Matching via QuickMatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the multi-image object matching problem, extend a\ncentralized solution of the problem to a distributed solution, and present an\nexperimental application of the centralized solution. Multi-image feature\nmatching is a keystone of many applications, including simultaneous\nlocalization and mapping, homography, object detection, and structure from\nmotion. We first review the QuickMatch algorithm for multi-image feature\nmatching. We then present a scheme for distributing sets of features across\ncomputational units (agents) that largely preserves feature match quality and\nminimizes communication between agents (avoiding, in particular, the need of\nflooding all data to all agents). Finally, we show how QuickMatch performs on\nan object matching test with low quality images. The centralized QuickMatch\nalgorithm is compared to other standard matching algorithms, while the\nDistributed QuickMatch algorithm is compared to the centralized algorithm in\nterms of preservation of match consistency. The presented experiment shows that\nQuickMatch matches features across a large number of images and features in\nlarger numbers and more accurately than standard techniques.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:26:54 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Serlin", "Zachary", ""], ["Yang", "Guang", ""], ["Sookraj", "Brandon", ""], ["Belta", "Calin", ""], ["Tron", "Roberto", ""]]}, {"id": "1910.13321", "submitter": "Tobias Hinz", "authors": "Tobias Hinz, Stefan Heinrich, Stefan Wermter", "title": "Semantic Object Accuracy for Generative Text-to-Image Synthesis", "comments": "Added a user study to verify results. Code available at\n  https://github.com/tohinz/semantic-object-accuracy-for-generative-text-to-image-synthesis", "journal-ref": "TPAMI (Early Access), 2020", "doi": "10.1109/TPAMI.2020.3021209", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks conditioned on textual image descriptions are\ncapable of generating realistic-looking images. However, current methods still\nstruggle to generate images based on complex image captions from a\nheterogeneous domain. Furthermore, quantitatively evaluating these\ntext-to-image models is challenging, as most evaluation metrics only judge\nimage quality but not the conformity between the image and its caption. To\naddress these challenges we introduce a new model that explicitly models\nindividual objects within an image and a new evaluation metric called Semantic\nObject Accuracy (SOA) that specifically evaluates images given an image\ncaption. The SOA uses a pre-trained object detector to evaluate if a generated\nimage contains objects that are mentioned in the image caption, e.g. whether an\nimage generated from \"a car driving down the street\" contains a car. We perform\na user study comparing several text-to-image models and show that our SOA\nmetric ranks the models the same way as humans, whereas other metrics such as\nthe Inception Score do not. Our evaluation also shows that models which\nexplicitly model objects outperform models which only model global image\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:35:52 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 12:25:16 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Hinz", "Tobias", ""], ["Heinrich", "Stefan", ""], ["Wermter", "Stefan", ""]]}, {"id": "1910.13323", "submitter": "Philip Smith", "authors": "Vitaliy Kurlin and Philip Smith", "title": "Resolution-independent meshes of super pixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The over-segmentation into superpixels is an important preprocessing step to\nsmartly compress the input size and speed up higher level tasks. A superpixel\nwas traditionally considered as a small cluster of square-based pixels that\nhave similar color intensities and are closely located to each other. In this\ndiscrete model the boundaries of superpixels often have irregular zigzags\nconsisting of horizontal or vertical edges from a given pixel grid. However\ndigital images represent a continuous world, hence the following continuous\nmodel in the resolution-independent formulation can be more suitable for the\nreconstruction problem.\n  Instead of uniting squares in a grid, a resolution-independent superpixel is\ndefined as a polygon that has straight edges with any possible slope at\nsubpixel resolution. The harder continuous version of the over-segmentation\nproblem is to split an image into polygons and find a best (say, constant)\ncolor of each polygon so that the resulting colored mesh well approximates the\ngiven image. Such a mesh of polygons can be rendered at any higher resolution\nwith all edges kept straight.\n  We propose a fast conversion of any traditional superpixels into polygons and\nguarantees that their straight edges do not intersect. The meshes based on the\nsuperpixels SEEDS (Superpixels Extracted via Energy-Driven Sampling) and SLIC\n(Simple Linear Iterative Clustering) are compared with past meshes based on the\nLine Segment Detector. The experiments on the Berkeley Segmentation Database\nconfirm that the new superpixels have more compact shapes than pixel-based\nsuperpixels.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:36:10 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 11:17:43 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Kurlin", "Vitaliy", ""], ["Smith", "Philip", ""]]}, {"id": "1910.13327", "submitter": "Michael Riegler", "authors": "Steven A. Hicks and Jorunn M. Andersen and Oliwia Witczak and Vajira\n  Thambawita and P{\\aa}ll Halvorsen and Hugo L. Hammer and Trine B. Haugen and\n  Michael A. Riegler", "title": "Machine Learning-Based Analysis of Sperm Videos and Participant Data for\n  Male Fertility Prediction", "comments": "Preprint, accepted by Nature Scientific Reports for publication\n  24.10.2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for automatic analysis of clinical data are usually targeted towards\na specific modality and do not make use of all relevant data available. In the\nfield of male human reproduction, clinical and biological data are not used to\nits fullest potential. Manual evaluation of a semen sample using a microscope\nis time-consuming and requires extensive training. Furthermore, the validity of\nmanual semen analysis has been questioned due to limited reproducibility, and\noften high inter-personnel variation. The existing computer-aided sperm\nanalyzer systems are not recommended for routine clinical use due to\nmethodological challenges caused by the consistency of the semen sample. Thus,\nthere is a need for an improved methodology. We use modern and classical\nmachine learning techniques together with a dataset consisting of 85 videos of\nhuman semen samples and related participant data to automatically predict sperm\nmotility. Used techniques include simple linear regression and more\nsophisticated methods using convolutional neural networks. Our results indicate\nthat sperm motility prediction based on deep learning using sperm motility\nvideos is rapid to perform and consistent. The algorithms performed worse when\nparticipant data was added. In conclusion, machine learning-based automatic\nanalysis may become a valuable tool in male infertility investigation and\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:38:47 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Hicks", "Steven A.", ""], ["Andersen", "Jorunn M.", ""], ["Witczak", "Oliwia", ""], ["Thambawita", "Vajira", ""], ["Halvorsen", "P\u00e5ll", ""], ["Hammer", "Hugo L.", ""], ["Haugen", "Trine B.", ""], ["Riegler", "Michael A.", ""]]}, {"id": "1910.13328", "submitter": "Jingwen Wang", "authors": "Jingwen Wang, Richard J. Chen, Ming Y. Lu, Alexander Baras, Faisal\n  Mahmood", "title": "Weakly Supervised Prostate TMA Classification via Graph Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histology-based grade classification is clinically important for many cancer\ntypes in stratifying patients distinct treatment groups. In prostate cancer,\nthe Gleason score is a grading system used to measure the aggressiveness of\nprostate cancer from the spatial organization of cells and the distribution of\nglands. However, the subjective interpretation of Gleason score often suffers\nfrom large interobserver and intraobserver variability. Previous work in deep\nlearning-based objective Gleason grading requires manual pixel-level\nannotation. In this work, we propose a weakly-supervised approach for grade\nclassification in tissue micro-arrays (TMA) using graph convolutional networks\n(GCNs), in which we model the spatial organization of cells as a graph to\nbetter capture the proliferation and community structure of tumor cells. As\nnode-level features in our graph representation, we learn the morphometry of\neach cell using a contrastive predictive coding (CPC)-based self-supervised\napproach. We demonstrate that on a five-fold cross validation our method can\nachieve $0.9659\\pm0.0096$ AUC using only TMA-level labels. Our method\ndemonstrates a 39.80\\% improvement over standard GCNs with texture features and\na 29.27% improvement over GCNs with VGG19 features. Our proposed pipeline can\nbe used to objectively stratify low and high risk cases, reducing inter- and\nintra-observer variability and pathologist workload.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:44:20 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 14:24:35 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Wang", "Jingwen", ""], ["Chen", "Richard J.", ""], ["Lu", "Ming Y.", ""], ["Baras", "Alexander", ""], ["Mahmood", "Faisal", ""]]}, {"id": "1910.13340", "submitter": "Rick Groenendijk", "authors": "Rick Groenendijk, Sezer Karaoglu, Theo Gevers, Thomas Mensink", "title": "On the Benefit of Adversarial Training for Monocular Depth Estimation", "comments": "11 pages, 8 tables, 5 figures, accepted at CVIU", "journal-ref": null, "doi": "10.1016/j.cviu.2019.102848", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the benefit of adding adversarial training to the\ntask of monocular depth estimation. A model can be trained in a self-supervised\nsetting on stereo pairs of images, where depth (disparities) are an\nintermediate result in a right-to-left image reconstruction pipeline. For the\nquality of the image reconstruction and disparity prediction, a combination of\ndifferent losses is used, including L1 image reconstruction losses and\nleft-right disparity smoothness. These are local pixel-wise losses, while depth\nprediction requires global consistency. Therefore, we extend the\nself-supervised network to become a Generative Adversarial Network (GAN), by\nincluding a discriminator which should tell apart reconstructed (fake) images\nfrom real images. We evaluate Vanilla GANs, LSGANs and Wasserstein GANs in\ncombination with different pixel-wise reconstruction losses. Based on extensive\nexperimental evaluation, we conclude that adversarial training is beneficial if\nand only if the reconstruction loss is not too constrained. Even though\nadversarial training seems promising because it promotes global consistency,\nnon-adversarial training outperforms (or is on par with) any method trained\nwith a GAN when a constrained reconstruction loss is used in combination with\nbatch normalisation. Based on the insights of our experimental evaluation we\nobtain state-of-the art monocular depth estimation results by using batch\nnormalisation and different output scales.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:57:24 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Groenendijk", "Rick", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""], ["Mensink", "Thomas", ""]]}, {"id": "1910.13348", "submitter": "Beril Sirmacek", "authors": "Beril Sirmacek, Nicol\\`o Botteghi, Santiago Sanchez Escalonilla Plaza", "title": "Sequential image processing methods for improving semantic video\n  segmentation algorithms", "comments": "29 pages, original work of the authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, semantic video segmentation gained high attention especially for\nsupporting autonomous driving systems. Deep learning methods made it possible\nto implement real time segmentation and object identification algorithms on\nvideos. However, most of the available approaches process each video frame\nindependently disregarding their sequential relation in time. Therefore their\nresults suddenly miss some of the object segments in some of the frames even if\nthey were detected properly in the earlier frames. Herein we propose two\nsequential probabilistic video frame analysis approaches to improve the\nsegmentation performance of the existing algorithms. Our experiments show that\nusing the information of the past frames we increase the performance and\nconsistency of the state of the art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:07:02 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Sirmacek", "Beril", ""], ["Botteghi", "Nicol\u00f2", ""], ["Plaza", "Santiago Sanchez Escalonilla", ""]]}, {"id": "1910.13351", "submitter": "Donald Wunsch", "authors": "Donald C. Wunsch", "title": "Admiring the Great Mountain: A Celebration Special Issue in Honor of\n  Stephen Grossbergs 80th Birthday", "comments": "Editorial for Special Issue of Neural Networks in honor of\n  Grossberg's 80th birthday", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This editorial summarizes selected key contributions of Prof. Stephen\nGrossberg and describes the papers in this 80th birthday special issue in his\nhonor. His productivity, creativity, and vision would each be enough to mark a\nscientist of the first caliber. In combination, they have resulted in\ncontributions that have changed the entire discipline of neural networks.\nGrossberg has been tremendously influential in engineering, dynamical systems,\nand artificial intelligence as well. Indeed, he has been one of the most\nimportant mentors and role models in my career, and has done so with\nextraordinary generosity and encouragement. All authors in this special issue\nhave taken great pleasure in hereby commemorating his extraordinary career and\ncontributions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 09:17:01 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Wunsch", "Donald C.", ""]]}, {"id": "1910.13395", "submitter": "Kuan Fang", "authors": "Kuan Fang, Yuke Zhu, Animesh Garg, Silvio Savarese, Li Fei-Fei", "title": "Dynamics Learning with Cascaded Variational Inference for Multi-Step\n  Manipulation", "comments": "CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental challenge of planning for multi-step manipulation is to find\neffective and plausible action sequences that lead to the task goal. We present\nCascaded Variational Inference (CAVIN) Planner, a model-based method that\nhierarchically generates plans by sampling from latent spaces. To facilitate\nplanning over long time horizons, our method learns latent representations that\ndecouple the prediction of high-level effects from the generation of low-level\nmotions through cascaded variational inference. This enables us to model\ndynamics at two different levels of temporal resolutions for hierarchical\nplanning. We evaluate our approach in three multi-step robotic manipulation\ntasks in cluttered tabletop environments given high-dimensional observations.\nEmpirical results demonstrate that the proposed method outperforms\nstate-of-the-art model-based methods by strategically interacting with multiple\nobjects.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:58:25 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 08:55:05 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Fang", "Kuan", ""], ["Zhu", "Yuke", ""], ["Garg", "Animesh", ""], ["Savarese", "Silvio", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1910.13439", "submitter": "Lerrel Pinto", "authors": "Yilin Wu, Wilson Yan, Thanard Kurutach, Lerrel Pinto, Pieter Abbeel", "title": "Learning to Manipulate Deformable Objects without Demonstrations", "comments": "Project website:\n  https://sites.google.com/view/alternating-pick-and-place", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of deformable object manipulation through\nmodel-free visual reinforcement learning (RL). In order to circumvent the\nsample inefficiency of RL, we propose two key ideas that accelerate learning.\nFirst, we propose an iterative pick-place action space that encodes the\nconditional relationship between picking and placing on deformable objects. The\nexplicit structural encoding enables faster learning under complex object\ndynamics. Second, instead of jointly learning both the pick and the place\nlocations, we only explicitly learn the placing policy conditioned on random\npick points. Then, by selecting the pick point that has Maximal Value under\nPlacing (MVP), we obtain our picking policy. This provides us with an informed\npicking policy during testing, while using only random pick points during\ntraining. Experimentally, this learning framework obtains an order of magnitude\nfaster learning compared to independent action-spaces on our suite of\ndeformable object manipulation tasks with visual RGB observations. Finally,\nusing domain randomization, we transfer our policies to a real PR2 robot for\nchallenging cloth and rope coverage tasks, and demonstrate significant\nimprovements over standard RL techniques on average coverage.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 17:56:56 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 21:45:54 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Wu", "Yilin", ""], ["Yan", "Wilson", ""], ["Kurutach", "Thanard", ""], ["Pinto", "Lerrel", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1910.13509", "submitter": "Mengge Chen", "authors": "Mengge Chen, Jonathan Li", "title": "Deep convolutional neural network application on rooftop detection for\n  aerial image", "comments": "4 pages, two figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most destructive disasters in the world, earthquake causes\ndeath, injuries, destruction and enormous damage to the affected area. It is\nsignificant to detect buildings after an earthquake in response to\nreconstruction and damage evaluation. In this research, we proposed an\nautomatic rooftop detection method based on the convolutional neural network\n(CNN) to extract buildings in the city of Christchurch and tuned\nhyperparameters to detect small detached houses from the aerial image. The\nexperiment result shows that our approach can effectively and accurately detect\nand segment buildings and has competitive performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 20:04:02 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Chen", "Mengge", ""], ["Li", "Jonathan", ""]]}, {"id": "1910.13523", "submitter": "Fan Yang", "authors": "Fan Yang, Jaymar Soriano, Takatomi Kubo, Kazushi Ikeda", "title": "A Hierarchical Mixture Density Network", "comments": "8 pages, 5 figures, conference", "journal-ref": "The 24th International Conference on Neural Information\n  Processing, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship among three correlated variables could be very\nsophisticated, as a result, we may not be able to find their hidden causality\nand model their relationship explicitly. However, we still can make our best\nguess for possible mappings among these variables, based on the observed\nrelationship. One of the complicated relationships among three correlated\nvariables could be a two-layer hierarchical many-to-many mapping. In this\npaper, we proposed a Hierarchical Mixture Density Network (HMDN) to model the\ntwo-layer hierarchical many-to-many mapping. We apply HMDN on an indoor\npositioning problem and show its benefit.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 11:33:07 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Yang", "Fan", ""], ["Soriano", "Jaymar", ""], ["Kubo", "Takatomi", ""], ["Ikeda", "Kazushi", ""]]}, {"id": "1910.13574", "submitter": "Amir Mosavi Prof", "authors": "Sanaz Mojrian, Gergo Pinter, Javad Hassannataj Joloudari, Imre Felde,\n  Narjes Nabipour, Laszlo Nadai, Amir Mosavi", "title": "Hybrid Machine Learning Model of Extreme Learning Machine Radial basis\n  function for Breast Cancer Detection and Diagnosis; a Multilayer Fuzzy Expert\n  System", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mammography is often used as the most common laboratory method for the\ndetection of breast cancer, yet associated with the high cost and many side\neffects. Machine learning prediction as an alternative method has shown\npromising results. This paper presents a method based on a multilayer fuzzy\nexpert system for the detection of breast cancer using an extreme learning\nmachine (ELM) classification model integrated with radial basis function (RBF)\nkernel called ELM-RBF, considering the Wisconsin dataset. The performance of\nthe proposed model is further compared with a linear-SVM model. The proposed\nmodel outperforms the linear-SVM model with RMSE, R2, MAPE equal to 0.1719,\n0.9374 and 0.0539, respectively. Furthermore, both models are studied in terms\nof criteria of accuracy, precision, sensitivity, specificity, validation, true\npositive rate (TPR), and false-negative rate (FNR). The ELM-RBF model for these\ncriteria presents better performance compared to the SVM model.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 23:33:23 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Mojrian", "Sanaz", ""], ["Pinter", "Gergo", ""], ["Joloudari", "Javad Hassannataj", ""], ["Felde", "Imre", ""], ["Nabipour", "Narjes", ""], ["Nadai", "Laszlo", ""], ["Mosavi", "Amir", ""]]}, {"id": "1910.13580", "submitter": "Qi Dou", "authors": "Qi Dou, Daniel C. Castro, Konstantinos Kamnitsas, Ben Glocker", "title": "Domain Generalization via Model-Agnostic Learning of Semantic Features", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization capability to unseen domains is crucial for machine learning\nmodels when deploying to real-world conditions. We investigate the challenging\nproblem of domain generalization, i.e., training a model on multi-domain source\ndata such that it can directly generalize to target domains with unknown\nstatistics. We adopt a model-agnostic learning paradigm with gradient-based\nmeta-train and meta-test procedures to expose the optimization to domain shift.\nFurther, we introduce two complementary losses which explicitly regularize the\nsemantic structure of the feature space. Globally, we align a derived soft\nconfusion matrix to preserve general knowledge about inter-class relationships.\nLocally, we promote domain-independent class-specific cohesion and separation\nof sample features with a metric-learning component. The effectiveness of our\nmethod is demonstrated with new state-of-the-art results on two common object\nrecognition benchmarks. Our method also shows consistent improvement on a\nmedical image segmentation task.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 23:43:01 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Dou", "Qi", ""], ["Castro", "Daniel C.", ""], ["Kamnitsas", "Konstantinos", ""], ["Glocker", "Ben", ""]]}, {"id": "1910.13646", "submitter": "Munan Xu", "authors": "Munan Xu, Junming Chen, Haiqiang Wang, Shan Liu, Ge Li, Zhiqiang Bai", "title": "C3DVQA: Full-Reference Video Quality Assessment with 3D Convolutional\n  Neural Network", "comments": "Cam ready, 5 pages, 3 figures, Accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional video quality assessment (VQA) methods evaluate localized picture\nquality and video score is predicted by temporally aggregating frame scores.\nHowever, video quality exhibits different characteristics from static image\nquality due to the existence of temporal masking effects. In this paper, we\npresent a novel architecture, namely C3DVQA, that uses Convolutional Neural\nNetwork with 3D kernels (C3D) for full-reference VQA task. C3DVQA combines\nfeature learning and score pooling into one spatiotemporal feature learning\nprocess. We use 2D convolutional layers to extract spatial features and 3D\nconvolutional layers to learn spatiotemporal features. We empirically found\nthat 3D convolutional layers are capable to capture temporal masking effects of\nvideos. We evaluated the proposed method on the LIVE and CSIQ datasets. The\nexperimental results demonstrate that the proposed method achieves the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 03:21:47 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 09:11:49 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Xu", "Munan", ""], ["Chen", "Junming", ""], ["Wang", "Haiqiang", ""], ["Liu", "Shan", ""], ["Li", "Ge", ""], ["Bai", "Zhiqiang", ""]]}, {"id": "1910.13656", "submitter": "Van Vung Pham", "authors": "Vung Pham and Tommy Dang", "title": "Outliagnostics: Visualizing Temporal Discrepancy in Outlying Signatures\n  of Data Entries", "comments": "in IEEE Visualization in Data Science (IEEE VDS) (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to analyzing two-dimensional temporal\ndatasets focusing on identifying observations that are significant in\ncalculating the outliers of a scatterplot. We also propose a prototype, called\nOutliagnostics, to guide users when interactively exploring abnormalities in\nlarge time series. Instead of focusing on detecting outliers at each time\npoint, we monitor and display the discrepant temporal signatures of each data\nentry concerning the overall distributions. Our prototype is designed to handle\nthese tasks in parallel to improve performance. To highlight the benefits and\nperformance of our approach, we illustrate and validate the use of\nOutliagnostics on real-world datasets of various sizes in different parallelism\nconfigurations. This work also discusses how to extend these ideas to handle\ntime series with a higher number of dimensions and provides a prototype for\nthis type of datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 04:24:38 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Pham", "Vung", ""], ["Dang", "Tommy", ""]]}, {"id": "1910.13671", "submitter": "Chaoyue Song", "authors": "Chaoyue Song, Yugang Chen, Shulai Zhang and Bingbing Ni", "title": "Facial Image Deformation Based on Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we use facial landmarks to make the deformation for facial\nimages more authentic. The deformation includes the expansion of eyes and the\nshrinking of noses, mouths, and cheeks. An advanced 106-point facial landmark\ndetector is utilized to provide control points for deformation. Bilinear\ninterpolation is used in the expansion and Moving Least Squares methods (MLS)\nincluding Affine Deformation, Similarity Deformation and Rigid Deformation are\nused in the shrinking. We compare the running time as well as the quality of\ndeformed images using different MLS methods. The experimental results show that\nthe Rigid Deformation which can keep other parts of the images unchanged\nperforms better even if it takes the longest time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 04:57:36 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 12:52:35 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Song", "Chaoyue", ""], ["Chen", "Yugang", ""], ["Zhang", "Shulai", ""], ["Ni", "Bingbing", ""]]}, {"id": "1910.13675", "submitter": "Kevin Zakka", "authors": "Kevin Zakka, Andy Zeng, Johnny Lee, Shuran Song", "title": "Form2Fit: Learning Shape Priors for Generalizable Assembly from\n  Disassembly", "comments": "Code, videos, and supplemental material are available at\n  https://form2fit.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to learn policies for robotic assembly that can generalize to\nnew objects? We explore this idea in the context of the kit assembly task.\nSince classic methods rely heavily on object pose estimation, they often\nstruggle to generalize to new objects without 3D CAD models or task-specific\ntraining data. In this work, we propose to formulate the kit assembly task as a\nshape matching problem, where the goal is to learn a shape descriptor that\nestablishes geometric correspondences between object surfaces and their target\nplacement locations from visual input. This formulation enables the model to\nacquire a broader understanding of how shapes and surfaces fit together for\nassembly -- allowing it to generalize to new objects and kits. To obtain\ntraining data for our model, we present a self-supervised data-collection\npipeline that obtains ground truth object-to-placement correspondences by\ndisassembling complete kits. Our resulting real-world system, Form2Fit, learns\neffective pick and place strategies for assembling objects into a variety of\nkits -- achieving $90\\%$ average success rates under different initial\nconditions (e.g. varying object and kit poses), $94\\%$ success under new\nconfigurations of multiple kits, and over $86\\%$ success with completely new\nobjects and kits.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:11:53 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 23:20:28 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zakka", "Kevin", ""], ["Zeng", "Andy", ""], ["Lee", "Johnny", ""], ["Song", "Shuran", ""]]}, {"id": "1910.13676", "submitter": "Kartik Srivastava", "authors": "Kartik Srivastava, Akash Kumar Singh and Guruprasad M. Hegde", "title": "Multi Modal Semantic Segmentation using Synthetic Data", "comments": "Accepted in 3rd Edition of Deep Learning for Automated Driving (DLAD)\n  workshop, IEEE International Conference on Intelligent Transportation Systems\n  (ITSC'19) [see\n  https://sites.google.com/view/dlad-bp-itsc2019/schedule?authuser=0#h.p_gI84BCoB0_bJ]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic understanding of scenes in three-dimensional space (3D) is a\nquintessential part of robotics oriented applications such as autonomous\ndriving as it provides geometric cues such as size, orientation and true\ndistance of separation to objects which are crucial for taking mission critical\ndecisions. As a first step, in this work we investigate the possibility of\nsemantically classifying different parts of a given scene in 3D by learning the\nunderlying geometric context in addition to the texture cues BUT in the absence\nof labelled real-world datasets. To this end we generate a large number of\nsynthetic scenes, their pixel-wise labels and corresponding 3D representations\nusing CARLA software framework. We then build a deep neural network that learns\nunderlying category specific 3D representation and texture cues from color\ninformation of the rendered synthetic scenes. Further on we apply the learned\nmodel on different real world datasets to evaluate its performance. Our\npreliminary investigation of results show that the neural network is able to\nlearn the geometric context from synthetic scenes and effectively apply this\nknowledge to classify each point of a 3D representation of a scene in\nreal-world.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:13:33 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Srivastava", "Kartik", ""], ["Singh", "Akash Kumar", ""], ["Hegde", "Guruprasad M.", ""]]}, {"id": "1910.13681", "submitter": "Wenjun Yan", "authors": "Wenjun Yan, Yuanyuan Wang, Shengjia Gu, Lu Huang, Fuhua Yan, Liming\n  Xia, and Qian Tao", "title": "The Domain Shift Problem of Medical Image Segmentation and\n  Vendor-Adaptation by Unet-GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN), in particular the Unet, is a powerful\nmethod for medical image segmentation. To date Unet has demonstrated\nstate-of-art performance in many complex medical image segmentation tasks,\nespecially under the condition when the training and testing data share the\nsame distribution (i.e. come from the same source domain). However, in clinical\npractice, medical images are acquired from different vendors and centers. The\nperformance of a U-Net trained from a particular source domain, when\ntransferred to a different target domain (e.g. different vendor, acquisition\nparameter), can drop unexpectedly. Collecting a large amount of annotation from\neach new domain to retrain the U-Net is expensive, tedious, and practically\nimpossible. In this work, we proposed a generic framework to address this\nproblem, consisting of (1) an unpaired generative adversarial network (GAN) for\nvendor-adaptation, and (2) a Unet for object segmentation. In the proposed\nUnet-GAN architecture, GAN learns from Unet at the feature level that is\nsegmentation-specific. We used cardiac cine MRI as the example, with three\nmajor vendors (Philips, Siemens, and GE) as three domains, while the\nmethodology can be extended to medical images segmentation in general. The\nproposed method showed significant improvement of the segmentation results\nacross vendors. The proposed Unet-GAN provides an annotation-free solution to\nthe cross-vendor medical image segmentation problem, potentially extending a\ntrained deep learning model to multi-center and multi-vendor use in real\nclinical scenario.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:36:21 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Yan", "Wenjun", ""], ["Wang", "Yuanyuan", ""], ["Gu", "Shengjia", ""], ["Huang", "Lu", ""], ["Yan", "Fuhua", ""], ["Xia", "Liming", ""], ["Tao", "Qian", ""]]}, {"id": "1910.13688", "submitter": "Qing Zhang", "authors": "Qing Zhang, Yongwei Nie, Wei-Shi Zheng", "title": "Dual Illumination Estimation for Robust Exposure Correction", "comments": "Computer Graphics Forum (Proceedings of Pacific Graphics 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposure correction is one of the fundamental tasks in image processing and\ncomputational photography. While various methods have been proposed, they\neither fail to produce visually pleasing results, or only work well for limited\ntypes of image (e.g., underexposed images). In this paper, we present a novel\nautomatic exposure correction method, which is able to robustly produce\nhigh-quality results for images of various exposure conditions (e.g.,\nunderexposed, overexposed, and partially under- and over-exposed). At the core\nof our approach is the proposed dual illumination estimation, where we\nseparately cast the under- and over-exposure correction as trivial illumination\nestimation of the input image and the inverted input image. By performing dual\nillumination estimation, we obtain two intermediate exposure correction results\nfor the input image, with one fixes the underexposed regions and the other one\nrestores the overexposed regions. A multi-exposure image fusion technique is\nthen employed to adaptively blend the visually best exposed parts in the two\nintermediate exposure correction images and the input image into a globally\nwell-exposed image. Experiments on a number of challenging images demonstrate\nthe effectiveness of the proposed approach and its superiority over the\nstate-of-the-art methods and popular automatic exposure correction tools.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:59:30 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Zhang", "Qing", ""], ["Nie", "Yongwei", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1910.13708", "submitter": "Yotam Gil", "authors": "Yotam Gil, Shay Elmalem, Harel Haim, Emanuel Marom, Raja Giryes", "title": "MonSter: Awakening the Mono in Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passive depth estimation is among the most long-studied fields in computer\nvision. The most common methods for passive depth estimation are either a\nstereo or a monocular system. Using the former requires an accurate calibration\nprocess, and has a limited effective range. The latter, which does not require\nextrinsic calibration but generally achieves inferior depth accuracy, can be\ntuned to achieve better results in part of the depth range. In this work, we\nsuggest combining the two frameworks. We propose a two-camera system, in which\nthe cameras are used jointly to extract a stereo depth and individually to\nprovide a monocular depth from each camera. The combination of these depth maps\nleads to more accurate depth estimation. Moreover, enforcing consistency\nbetween the extracted maps leads to a novel online self-calibration strategy.\nWe present a prototype camera that demonstrates the benefits of the proposed\ncombination, for both self-calibration and depth reconstruction in real-world\nscenes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 08:13:31 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Gil", "Yotam", ""], ["Elmalem", "Shay", ""], ["Haim", "Harel", ""], ["Marom", "Emanuel", ""], ["Giryes", "Raja", ""]]}, {"id": "1910.13740", "submitter": "Yuanhao Guo Dr.", "authors": "Yuanhao Guo, Fons J. Verbeek, Ge Yang", "title": "Probabilistic Inference for Camera Calibration in Light Microscopy under\n  Circular Motion", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and accurate camera calibration is essential for 3D reconstruction in\nlight microscopy under circular motion. Conventional methods require either\naccurate key point matching or precise segmentation of the axial-view images.\nBoth remain challenging because specimens often exhibit\ntransparency/translucency in a light microscope. To address those issues, we\npropose a probabilistic inference based method for the camera calibration that\ndoes not require sophisticated image pre-processing. Based on 3D projective\ngeometry, our method assigns a probability on each of a range of voxels that\ncover the whole object. The probability indicates the likelihood of a voxel\nbelonging to the object to be reconstructed. Our method maximizes a joint\nprobability that distinguishes the object from the background. Experimental\nresults show that the proposed method can accurately recover camera\nconfigurations in both light microscopy and natural scene imaging. Furthermore,\nthe method can be used to produce high-fidelity 3D reconstructions and accurate\n3D measurements.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 09:49:55 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Guo", "Yuanhao", ""], ["Verbeek", "Fons J.", ""], ["Yang", "Ge", ""]]}, {"id": "1910.13757", "submitter": "Juan Pablo Zuluaga", "authors": "Juan Zuluaga-Gomez, Zeina Al Masry, Khaled Benaggoune, Safa Meraghni\n  and Noureddine Zerhouni", "title": "A CNN-based methodology for breast cancer diagnosis using thermal images", "comments": "19 pages, 7 figures, 5 tables. Clinical Breast Cancer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro Abstract: A recent study from GLOBOCAN disclosed that during 2018 two\nmillion women worldwide had been diagnosed from breast cancer. This study\npresents a computer-aided diagnosis system based on convolutional neural\nnetworks as an alternative diagnosis methodology for breast cancer diagnosis\nwith thermal images. Experimental results showed that lower false-positives and\nfalse-negatives classification rates are obtained when data pre-processing and\ndata augmentation techniques are implemented in these thermal images.\nBackground: There are many types of breast cancer screening techniques such as,\nmammography, magnetic resonance imaging, ultrasound and blood sample tests,\nwhich require either, expensive devices or personal qualified. Currently, some\ncountries still lack access to these main screening techniques due to economic,\nsocial or cultural issues. The objective of this study is to demonstrate that\ncomputer-aided diagnosis(CAD) systems based on convolutional neural networks\n(CNN) are faster, reliable and robust than other techniques. Methods: We\nperformed a study of the influence of data pre-processing, data augmentation\nand database size versus a proposed set of CNN models. Furthermore, we\ndeveloped a CNN hyper-parameters fine-tuning optimization algorithm using a\ntree parzen estimator. Results: Among the 57 patients database, our CNN models\nobtained a higher accuracy (92\\%) and F1-score (92\\%) that outperforms several\nstate-of-the-art architectures such as ResNet50, SeResNet50 and Inception.\nAlso, we demonstrated that a CNN model that implements data-augmentation\ntechniques reach identical performance metrics in comparison with a CNN that\nuses a database up to 50\\% bigger. Conclusion: This study highlights the\nbenefits of data augmentation and CNNs in thermal breast images. Also, it\nmeasures the influence of the database size in the performance of CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 10:24:42 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Zuluaga-Gomez", "Juan", ""], ["Masry", "Zeina Al", ""], ["Benaggoune", "Khaled", ""], ["Meraghni", "Safa", ""], ["Zerhouni", "Noureddine", ""]]}, {"id": "1910.13796", "submitter": "Niall O' Mahony", "authors": "Niall O' Mahony, Sean Campbell, Anderson Carvalho, Suman\n  Harapanahalli, Gustavo Velasco-Hernandez, Lenka Krpalkova, Daniel Riordan,\n  Joseph Walsh", "title": "Deep Learning vs. Traditional Computer Vision", "comments": null, "journal-ref": "in Advances in Computer Vision Proceedings of the 2019 Computer\n  Vision Conference (CVC). Springer Nature Switzerland AG, pp. 128-144", "doi": "10.1007/978-3-030-17795-9", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has pushed the limits of what was possible in the domain of\nDigital Image Processing. However, that is not to say that the traditional\ncomputer vision techniques which had been undergoing progressive development in\nyears prior to the rise of DL have become obsolete. This paper will analyse the\nbenefits and drawbacks of each approach. The aim of this paper is to promote a\ndiscussion on whether knowledge of classical computer vision techniques should\nbe maintained. The paper will also explore how the two sides of computer vision\ncan be combined. Several recent hybrid methodologies are reviewed which have\ndemonstrated the ability to improve computer vision performance and to tackle\nproblems not suited to Deep Learning. For example, combining traditional\ncomputer vision techniques with Deep Learning has been popular in emerging\ndomains such as Panoramic Vision and 3D vision for which Deep Learning models\nhave not yet been fully optimised\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 12:25:10 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Mahony", "Niall O'", ""], ["Campbell", "Sean", ""], ["Carvalho", "Anderson", ""], ["Harapanahalli", "Suman", ""], ["Velasco-Hernandez", "Gustavo", ""], ["Krpalkova", "Lenka", ""], ["Riordan", "Daniel", ""], ["Walsh", "Joseph", ""]]}, {"id": "1910.13824", "submitter": "Henry Martin", "authors": "Henry Martin, Ye Hong, Dominik Bucher, Christian Rupprecht, Ren\\'e\n  Buffat", "title": "Traffic4cast-Traffic Map Movie Forecasting -- Team MIE-Lab", "comments": null, "journal-ref": null, "doi": "10.3929/ethz-b-000388707", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the IARAI competition traffic4cast was to predict the city-wide\ntraffic status within a 15-minute time window, based on information from the\nprevious hour. The traffic status was given as multi-channel images (one pixel\nroughly corresponds to 100x100 meters), where one channel indicated the traffic\nvolume, another one the average speed of vehicles, and a third one their rough\nheading. As part of our work on the competition, we evaluated many different\nnetwork architectures, analyzed the statistical properties of the given data in\ndetail, and thought about how to transform the problem to be able to take\nadditional spatio-temporal context-information into account, such as the street\nnetwork, the positions of traffic lights, or the weather. This document\nsummarizes our efforts that led to our best submission, and gives some insights\nabout which other approaches we evaluated, and why they did not work as well as\nimagined.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 20:39:14 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 10:10:48 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Martin", "Henry", ""], ["Hong", "Ye", ""], ["Bucher", "Dominik", ""], ["Rupprecht", "Christian", ""], ["Buffat", "Ren\u00e9", ""]]}, {"id": "1910.13888", "submitter": "Yudong Jiang", "authors": "Yudong Jiang, Kaixu Cui, Bo Peng, Changliang Xu", "title": "Comprehensive Video Understanding: Video summarization with\n  content-based video recommender design", "comments": "2019 International Conference on Computer Vision Workshop (ICCVW\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video summarization aims to extract keyframes/shots from a long video.\nPrevious methods mainly take diversity and representativeness of generated\nsummaries as prior knowledge in algorithm design. In this paper, we formulate\nvideo summarization as a content-based recommender problem, which should\ndistill the most useful content from a long video for users who suffer from\ninformation overload. A scalable deep neural network is proposed on predicting\nif one video segment is a useful segment for users by explicitly modelling both\nsegment and video. Moreover, we accomplish scene and action recognition in\nuntrimmed videos in order to find more correlations among different aspects of\nvideo understanding tasks. Also, our paper will discuss the effect of audio and\nvisual features in summarization task. We also extend our work by data\naugmentation and multi-task learning for preventing the model from early-stage\noverfitting. The final results of our model win the first place in ICCV 2019\nCoView Workshop Challenge Track.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:29:08 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Jiang", "Yudong", ""], ["Cui", "Kaixu", ""], ["Peng", "Bo", ""], ["Xu", "Changliang", ""]]}, {"id": "1910.13911", "submitter": "Angel Martinez", "authors": "Angel Mart\\'inez-Gonz\\'alez, Michael Villamizar, Olivier Can\\'evet and\n  Jean-Marc Odobez", "title": "Real-time Convolutional Networks for Depth-based Human Pose Estimation", "comments": "Published in IROS 2018", "journal-ref": "2018 IEEE International Conference on Intelligent Robots and\n  Systems, Madrid, Spain,", "doi": "10.1109/IROS.2018.8593383", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to combine recent Convolutional Neural Networks (CNN) models with\ndepth imaging to obtain a reliable and fast multi-person pose estimation\nalgorithm applicable to Human Robot Interaction (HRI) scenarios. Our hypothesis\nis that depth images contain less structures and are easier to process than RGB\nimages while keeping the required information for human detection and pose\ninference, thus allowing the use of simpler networks for the task. Our\ncontributions are threefold. (i) we propose a fast and efficient network based\non residual blocks (called RPM) for body landmark localization from depth\nimages; (ii) we created a public dataset DIH comprising more than 170k\nsynthetic images of human bodies with various shapes and viewpoints as well as\nreal (annotated) data for evaluation; (iii) we show that our model trained on\nsynthetic data from scratch can perform well on real data, obtaining similar\nresults to larger models initialized with pre-trained networks. It thus\nprovides a good trade-off between performance and computation. Experiments on\nreal data demonstrate the validity of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:52:46 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Mart\u00ednez-Gonz\u00e1lez", "Angel", ""], ["Villamizar", "Michael", ""], ["Can\u00e9vet", "Olivier", ""], ["Odobez", "Jean-Marc", ""]]}, {"id": "1910.13921", "submitter": "Mojtaba Bemana", "authors": "Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel", "title": "Neural View-Interpolation for Sparse Light Field Video", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest representing light field (LF) videos as \"one-off\" neural networks\n(NN), i.e., a learned mapping from view-plus-time coordinates to\nhigh-resolution color values, trained on sparse views. Initially, this sounds\nlike a bad idea for three main reasons: First, a NN LF will likely have less\nquality than a same-sized pixel basis representation. Second, only few training\ndata, e.g., 9 exemplars per frame are available for sparse LF videos. Third,\nthere is no generalization across LFs, but across view and time instead.\nConsequently, a network needs to be trained for each LF video. Surprisingly,\nthese problems can turn into substantial advantages: Other than the linear\npixel basis, a NN has to come up with a compact, non-linear i.e., more\nintelligent, explanation of color, conditioned on the sparse view and time\ncoordinates. As observed for many NN however, this representation now is\ninterpolatable: if the image output for sparse view coordinates is plausible,\nit is for all intermediate, continuous coordinates as well. Our specific\nnetwork architecture involves a differentiable occlusion-aware warping step,\nwhich leads to a compact set of trainable parameters and consequently fast\nlearning and fast execution.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 15:18:37 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 16:38:40 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 10:27:50 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Bemana", "Mojtaba", ""], ["Myszkowski", "Karol", ""], ["Seidel", "Hans-Peter", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1910.13931", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, Aparna Aketi, and Kaushik Roy", "title": "Towards Scalable, Efficient and Accurate Deep Spiking Neural Networks\n  with Backward Residual Connections, Stochastic Softmax and Hybridization", "comments": "14 pages, 7 figures, 17 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) may offer an energy-efficient alternative for\nimplementing deep learning applications. In recent years, there have been\nseveral proposals focused on supervised (conversion, spike-based gradient\ndescent) and unsupervised (spike timing dependent plasticity) training methods\nto improve the accuracy of SNNs on large-scale tasks. However, each of these\nmethods suffer from scalability, latency and accuracy limitations. In this\npaper, we propose novel algorithmic techniques of modifying the SNN\nconfiguration with backward residual connections, stochastic softmax and hybrid\nartificial-and-spiking neuronal activations to improve the learning ability of\nthe training methodologies to yield competitive accuracy, while, yielding large\nefficiency gains over their artificial counterparts. Note, artificial\ncounterparts refer to conventional deep learning/artificial neural networks.\nOur techniques apply to VGG/Residual architectures, and are compatible with all\nforms of training methodologies. Our analysis reveals that the proposed\nsolutions yield near state-of-the-art accuracy with significant\nenergy-efficiency and reduced parameter overhead translating to hardware\nimprovements on complex visual recognition tasks, such as, CIFAR10, Imagenet\ndatatsets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 15:31:15 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Aketi", "Aparna", ""], ["Roy", "Kaushik", ""]]}, {"id": "1910.13942", "submitter": "Felix Leeb", "authors": "Felix Leeb, Arunkumar Byravan, Dieter Fox", "title": "Motion-Nets: 6D Tracking of Unknown Objects in Unseen Environments using\n  RGB", "comments": "Accepted to IROS 2019 workshop on The Importance of Uncertainty in\n  Deep Learning for Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we bridge the gap between recent pose estimation and tracking\nwork to develop a powerful method for robots to track objects in their\nsurroundings. Motion-Nets use a segmentation model to segment the scene, and\nseparate translation and rotation models to identify the relative 6D motion of\nan object between two consecutive frames. We train our method with generated\ndata of floating objects, and then test on several prediction tasks, including\none with a real PR2 robot, and a toy control task with a simulated PR2 robot\nnever seen during training. Motion-Nets are able to track the pose of objects\nwith some quantitative accuracy for about 30-60 frames including occlusions and\ndistractors. Additionally, the single step prediction errors remain low even\nafter 100 frames. We also investigate an iterative correction procedure to\nimprove performance for control tasks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 15:51:23 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Leeb", "Felix", ""], ["Byravan", "Arunkumar", ""], ["Fox", "Dieter", ""]]}, {"id": "1910.13955", "submitter": "Brian Wang", "authors": "Brian H. Wang, Wei-Lun Chao, Yan Wang, Bharath Hariharan, Kilian Q.\n  Weinberger, and Mark Campbell", "title": "LDLS: 3-D Object Segmentation Through Label Diffusion From 2-D Images", "comments": "Accepted for publication in IEEE Robotics and Automation Letters with\n  presentation at IROS 2019", "journal-ref": null, "doi": "10.1109/LRA.2019.2922582", "report-no": null, "categories": "eess.IV cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object segmentation in three-dimensional (3-D) point clouds is a critical\ntask for robots capable of 3-D perception. Despite the impressive performance\nof deep learning-based approaches on object segmentation in 2-D images, deep\nlearning has not been applied nearly as successfully for 3-D point cloud\nsegmentation. Deep networks generally require large amounts of labeled training\ndata, which are readily available for 2-D images but are difficult to produce\nfor 3-D point clouds. In this letter, we present Label Diffusion Lidar\nSegmentation (LDLS), a novel approach for 3-D point cloud segmentation, which\nleverages 2-D segmentation of an RGB image from an aligned camera to avoid the\nneed for training on annotated 3-D data. We obtain 2-D segmentation predictions\nby applying Mask-RCNN to the RGB image, and then link this image to a 3-D lidar\npoint cloud by building a graph of connections among 3-D points and 2-D pixels.\nThis graph then directs a semi-supervised label diffusion process, where the\n2-D pixels act as source nodes that diffuse object label information through\nthe 3-D point cloud, resulting in a complete 3-D point cloud segmentation. We\nconduct empirical studies on the KITTI benchmark dataset and on a mobile robot,\ndemonstrating wide applicability and superior performance of LDLS compared with\nthe previous state of the art in 3-D point cloud segmentation, without any need\nfor either 3-D training data or fine tuning of the 2-D image segmentation\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:11:23 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Wang", "Brian H.", ""], ["Chao", "Wei-Lun", ""], ["Wang", "Yan", ""], ["Hariharan", "Bharath", ""], ["Weinberger", "Kilian Q.", ""], ["Campbell", "Mark", ""]]}, {"id": "1910.13988", "submitter": "Roman Goldenberg", "authors": "Dror Simon, Miriam Farber, Roman Goldenberg", "title": "Auto-Annotation Quality Prediction for Semi-Supervised Learning with\n  Ensembles", "comments": "10 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-annotation by ensemble of models is an efficient method of learning on\nunlabeled data. Wrong or inaccurate annotations generated by the ensemble may\nlead to performance degradation of the trained model. To deal with this problem\nwe propose filtering the auto-labeled data using a trained model that predicts\nthe quality of the annotation from the degree of consensus between ensemble\nmodels. Using semantic segmentation as an example, we show the advantage of the\nproposed auto-annotation filtering over training on data contaminated with\ninaccurate labels.\n  Moreover, our experimental results show that in the case of semantic\nsegmentation, the performance of a state-of-the-art model can be achieved by\ntraining it with only a fraction (30$\\%$) of the original manually labeled data\nset, and replacing the rest with the auto-annotated, quality filtered labels.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:10:21 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Simon", "Dror", ""], ["Farber", "Miriam", ""], ["Goldenberg", "Roman", ""]]}, {"id": "1910.13993", "submitter": "Litu Rout", "authors": "Litu Rout", "title": "Is Supervised Learning With Adversarial Features Provably Better Than\n  Sole Supervision?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) have shown promising results on a wide\nvariety of complex tasks. Recent experiments show adversarial training provides\nuseful gradients to the generator that helps attain better performance. In this\npaper, we intend to theoretically analyze whether supervised learning with\nadversarial features can outperform sole supervision, or not. First, we show\nthat supervised learning without adversarial features suffer from vanishing\ngradient issue in near optimal region. Second, we analyze how adversarial\nlearning augmented with supervised signal mitigates this vanishing gradient\nissue. Finally, we prove our main result that shows supervised learning with\nadversarial features can be better than sole supervision (under some mild\nassumptions). We support our main result on two fronts (i) expected empirical\nrisk and (ii) rate of convergence.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:20:45 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 14:42:18 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Rout", "Litu", ""]]}, {"id": "1910.14029", "submitter": "Stefan Engblom", "authors": "Jing Liu and Stefan Engblom and Carl Nettelblad", "title": "Flash X-ray diffraction imaging in 3D: a proposed analysis pipeline", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.390384", "report-no": null, "categories": "eess.IV cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Flash X-ray diffraction Imaging (FXI) acquires diffraction signals\nfrom single biomolecules at a high repetition rate from X-ray Free Electron\nLasers (XFELs), easily obtaining millions of 2D diffraction patterns from a\nsingle experiment. Due to the stochastic nature of FXI experiments and the\nmassive volumes of data, retrieving 3D electron densities from raw 2D\ndiffraction patterns is a challenging and time-consuming task.\n  We propose a semi-automatic data analysis pipeline for FXI experiments, which\nincludes four steps: hit finding and preliminary filtering, pattern\nclassification, 3D Fourier reconstruction, and post analysis. We also include a\nrecently developed bootstrap methodology in the post-analysis step for\nuncertainty analysis and quality control. To achieve the best possible\nresolution, we further suggest using background subtraction, signal windowing,\nand convex optimization techniques when retrieving the Fourier phases in the\npost-analysis step.\n  As an application example, we quantified the 3D electron structure of the\nPR772 virus using the proposed data-analysis pipeline. The retrieved structure\nwas above the detector-edge resolution and clearly showed the\npseudo-icosahedral capsid of the PR772.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 13:29:54 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 09:04:33 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Liu", "Jing", ""], ["Engblom", "Stefan", ""], ["Nettelblad", "Carl", ""]]}, {"id": "1910.14031", "submitter": "Harnaik Dhami", "authors": "Harnaik Dhami, Kevin Yu, Tianshu Xu, Qian Zhu, Kshitiz Dhakal, James\n  Friel, Song Li, and Pratap Tokekar", "title": "Crop Height and Plot Estimation for Phenotyping from Unmanned Aerial\n  Vehicles using 3D LiDAR", "comments": "8 pages, 10 figures, 1 table, Accepted to IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present techniques to measure crop heights using a 3D Light Detection and\nRanging (LiDAR) sensor mounted on an Unmanned Aerial Vehicle (UAV). Knowing the\nheight of plants is crucial to monitor their overall health and growth cycles,\nespecially for high-throughput plant phenotyping. We present a methodology for\nextracting plant heights from 3D LiDAR point clouds, specifically focusing on\nplot-based phenotyping environments. We also present a toolchain that can be\nused to create phenotyping farms for use in Gazebo simulations. The tool\ncreates a randomized farm with realistic 3D plant and terrain models. We\nconducted a series of simulations and hardware experiments in controlled and\nnatural settings. Our algorithm was able to estimate the plant heights in a\nfield with 112 plots with a root mean square error (RMSE) of 6.1 cm. This is\nthe first such dataset for 3D LiDAR from an airborne robot over a wheat field.\nThe developed simulation toolchain, algorithmic implementation, and datasets\ncan be found on the GitHub repository located at\nhttps://github.com/hsd1121/PointCloudProcessing.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 15:03:21 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 15:42:05 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 01:23:36 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Dhami", "Harnaik", ""], ["Yu", "Kevin", ""], ["Xu", "Tianshu", ""], ["Zhu", "Qian", ""], ["Dhakal", "Kshitiz", ""], ["Friel", "James", ""], ["Li", "Song", ""], ["Tokekar", "Pratap", ""]]}, {"id": "1910.14034", "submitter": "Ryne Roady", "authors": "Ryne Roady, Tyler L. Hayes, Ronald Kemker, Ayesha Gonzales, and\n  Christopher Kanan", "title": "Are Out-of-Distribution Detection Methods Effective on Large-Scale\n  Datasets?", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0238302", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised classification methods often assume the train and test data\ndistributions are the same and that all classes in the test set are present in\nthe training set. However, deployed classifiers often require the ability to\nrecognize inputs from outside the training set as unknowns. This problem has\nbeen studied under multiple paradigms including out-of-distribution detection\nand open set recognition. For convolutional neural networks, there have been\ntwo major approaches: 1) inference methods to separate knowns from unknowns and\n2) feature space regularization strategies to improve model robustness to\noutlier inputs. There has been little effort to explore the relationship\nbetween the two approaches and directly compare performance on anything other\nthan small-scale datasets that have at most 100 categories. Using ImageNet-1K\nand Places-434, we identify novel combinations of regularization and\nspecialized inference methods that perform best across multiple outlier\ndetection problems of increasing difficulty level. We found that input\nperturbation and temperature scaling yield the best performance on large scale\ndatasets regardless of the feature space regularization strategy. Improving the\nfeature space by regularizing against a background class can be helpful if an\nappropriate background class can be found, but this is impractical for large\nscale image classification datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:53:13 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Roady", "Ryne", ""], ["Hayes", "Tyler L.", ""], ["Kemker", "Ronald", ""], ["Gonzales", "Ayesha", ""], ["Kanan", "Christopher", ""]]}, {"id": "1910.14063", "submitter": "Yi-Ling Qiao", "authors": "Yi-Ling Qiao, Lin Gao, Jie Yang, Paul L. Rosin, Yu-Kun Lai, and Xilin\n  Chen", "title": "LaplacianNet: Learning on 3D Meshes with Laplacian Encoding and Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D models are commonly used in computer vision and graphics. With the wider\navailability of mesh data, an efficient and intrinsic deep learning approach to\nprocessing 3D meshes is in great need. Unlike images, 3D meshes have irregular\nconnectivity, requiring careful design to capture relations in the data. To\nutilize the topology information while staying robust under different\ntriangulation, we propose to encode mesh connectivity using Laplacian spectral\nanalysis, along with Mesh Pooling Blocks (MPBs) that can split the surface\ndomain into local pooling patches and aggregate global information among them.\nWe build a mesh hierarchy from fine to coarse using Laplacian spectral\nclustering, which is flexible under isometric transformation. Inside the MPBs\nthere are pooling layers to collect local information and multi-layer\nperceptrons to compute vertex features with increasing complexity. To obtain\nthe relationships among different clusters, we introduce a Correlation Net to\ncompute a correlation matrix, which can aggregate the features globally by\nmatrix multiplication with cluster features. Our network architecture is\nflexible enough to be used on meshes with different numbers of vertices. We\nconduct several experiments including shape segmentation and classification,\nand our LaplacianNet outperforms state-of-the-art algorithms for these tasks on\nShapeNet and COSEG datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:02:23 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Qiao", "Yi-Ling", ""], ["Gao", "Lin", ""], ["Yang", "Jie", ""], ["Rosin", "Paul L.", ""], ["Lai", "Yu-Kun", ""], ["Chen", "Xilin", ""]]}, {"id": "1910.14139", "submitter": "Andrew Davison", "authors": "Andrew J. Davison and Joseph Ortiz", "title": "FutureMapping 2: Gaussian Belief Propagation for Spatial AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue the case for Gaussian Belief Propagation (GBP) as a strong\nalgorithmic framework for the distributed, generic and incremental\nprobabilistic estimation we need in Spatial AI as we aim at high performance\nsmart robots and devices which operate within the constraints of real products.\nProcessor hardware is changing rapidly, and GBP has the right character to take\nadvantage of highly distributed processing and storage while estimating global\nquantities, as well as great flexibility. We present a detailed tutorial on\nGBP, relating to the standard factor graph formulation used in robotics and\ncomputer vision, and give several simulation examples with code which\ndemonstrate its properties.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 21:12:14 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Davison", "Andrew J.", ""], ["Ortiz", "Joseph", ""]]}, {"id": "1910.14184", "submitter": "Xing Wei", "authors": "Wenjie Ding, Xing Wei, Rongrong Ji, Xiaopeng Hong, Qi Tian, Yihong\n  Gong", "title": "Beyond Universal Person Re-ID Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based person re-identification (Re-ID) has made great progress\nand achieved high performance recently. In this paper, we make the first\nattempt to examine the vulnerability of current person Re-ID models against a\ndangerous attack method, \\ie, the universal adversarial perturbation (UAP)\nattack, which has been shown to fool classification models with a little\noverhead. We propose a \\emph{more universal} adversarial perturbation (MUAP)\nmethod for both image-agnostic and model-insensitive person Re-ID attack.\nFirstly, we adopt a list-wise attack objective function to disrupt the\nsimilarity ranking list directly. Secondly, we propose a model-insensitive\nmechanism for cross-model attack. Extensive experiments show that the proposed\nattack approach achieves high attack performance and outperforms other state of\nthe arts by large margin in cross-model scenario. The results also demonstrate\nthe vulnerability of current Re-ID models to MUAP and further suggest the need\nof designing more robust Re-ID models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 23:43:51 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 13:29:31 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 15:12:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ding", "Wenjie", ""], ["Wei", "Xing", ""], ["Ji", "Rongrong", ""], ["Hong", "Xiaopeng", ""], ["Tian", "Qi", ""], ["Gong", "Yihong", ""]]}, {"id": "1910.14202", "submitter": "Bishesh Khanal", "authors": "Bidur Khanal, Lavsen Dahal, Prashant Adhikari and Bishesh Khanal", "title": "Automatic Cobb Angle Detection using Vertebra Detector and Vertebra\n  Corners Regression", "comments": "Accepted to MICCAI 2019 CSI Workshop & Challenge: Computational\n  Methods and Clinical Applications for Spine Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correct evaluation and treatment of Scoliosis require accurate estimation of\nspinal curvature. Current gold standard is to manually estimate Cobb Angles in\nspinal X-ray images which is time consuming and has high inter-rater\nvariability. We propose an automatic method with a novel framework that first\ndetects vertebrae as objects followed by a landmark detector that estimates the\n4 landmark corners of each vertebra separately. Cobb Angles are calculated\nusing the slope of each vertebra obtained from the predicted landmarks. For\ninference on test data, we perform pre and post processings that include\ncropping, outlier rejection and smoothing of the predicted landmarks. The\nresults were assessed in AASCE MICCAI challenge 2019 which showed a promise\nwith a SMAPE score of 25.69 on the challenge test set.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 01:20:28 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Khanal", "Bidur", ""], ["Dahal", "Lavsen", ""], ["Adhikari", "Prashant", ""], ["Khanal", "Bishesh", ""]]}, {"id": "1910.14207", "submitter": "Anastasiia Razdaibiedina", "authors": "Anastasia Razdaibiedina, Jeevaa Velayutham, Miti Modi", "title": "Multi-defect microscopy image restoration under limited data conditions", "comments": "NeurIPS 2019 Medical Imaging workhop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods are becoming widely used for restoration of defects\nassociated with fluorescence microscopy imaging. One of the major challenges in\napplication of such methods is the availability of training data. In this work,\nwe propose a unified method for reconstruction of multi-defect fluorescence\nmicroscopy images when training data is limited. Our approach consists of two\nstages: first, we perform data augmentation using Generative Adversarial\nNetwork (GAN) with conditional instance normalization (CIN); second, we train a\nconditional GAN (cGAN) on paired ground-truth and defected images to perform\nrestoration. The experiments on three common types of imaging defects with\ndifferent amounts of training data show that the proposed method gives\ncomparable results or outperforms CARE, deblurGAN and CycleGAN in restored\nimage quality when available data is limited.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 01:55:01 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 20:14:19 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Razdaibiedina", "Anastasia", ""], ["Velayutham", "Jeevaa", ""], ["Modi", "Miti", ""]]}, {"id": "1910.14208", "submitter": "Jialin Wu", "authors": "Jialin Wu and Raymond J. Mooney", "title": "Hidden State Guidance: Improving Image Captioning using An Image\n  Conditioned Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most RNN-based image captioning models receive supervision on the output\nwords to mimic human captions. Therefore, the hidden states can only receive\nnoisy gradient signals via layers of back-propagation through time, leading to\nless accurate generated captions. Consequently, we propose a novel framework,\nHidden State Guidance (HSG), that matches the hidden states in the caption\ndecoder to those in a teacher decoder trained on an easier task of autoencoding\nthe captions conditioned on the image. During training with the REINFORCE\nalgorithm, the conventional rewards are sentence-based evaluation metrics\nequally distributed to each generated word, no matter their relevance. HSG\nprovides a word-level reward that helps the model learn better hidden\nrepresentations. Experimental results demonstrate that HSG clearly outperforms\nvarious state-of-the-art caption decoders using either raw images or detected\nobjects as inputs.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 01:56:33 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 19:21:02 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Wu", "Jialin", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1910.14216", "submitter": "Yang Wu", "authors": "Yang Wu and Xu Cai and Pengxu Wei and Guanbin Li and Liang Lin", "title": "Generalizing Energy-based Generative ConvNets from Particle Evolution\n  Perspective", "comments": "after the submission we found there is fatal error in this paper.\n  From one respect we missed cite some important reference work, from another\n  our pipeline of cooperative learning is totally wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with Generative Adversarial Networks (GAN), Energy-Based generative\nModels (EBMs) possess two appealing properties: i) they can be directly\noptimized without requiring an auxiliary network during the learning and\nsynthesizing; ii) they can better approximate underlying distribution of the\nobserved data by learning explicitly potential functions. This paper studies a\nbranch of EBMs, i.e., energy-based Generative ConvNets (GCNs), which minimize\ntheir energy function defined by a bottom-up ConvNet. From the perspective of\nparticle physics, we solve the problem of unstable energy dissipation that\nmight damage the quality of the synthesized samples during the maximum\nlikelihood learning. Specifically, we firstly establish a connection between\nclassical FRAME model [1] and dynamic physics process and generalize the GCN in\ndiscrete flow with a certain metric measure from particle perspective. To\naddress KL-vanishing issue, we then reformulate GCN from the KL discrete flow\nwith KL divergence measure to a Jordan-Kinderleher-Otto (JKO) discrete flow\nwith Wasserastein distance metric and derive a Wasserastein GCN (wGCN). Based\non these theoretical studies on GCN, we finally derive a Generalized GCN (GGCN)\nto further improve the model generalization and learning capability. GGCN\nintroduces a hidden space mapping strategy by employing a normal distribution\nfor the reference distribution to address the learning bias issue. Due to MCMC\nsampling in GCNs, it still suffers from a serious time-consuming issue when\nsampling steps increase; thus a trainable non-linear upsampling function and an\namortized learning are proposed to improve the learning efficiency. Our\nproposed GGCN is trained in a symmetrical learning manner. Our method surpass\nthe existing models in both model stability and the quality of generated\nsamples on several widely-used face and natural image datasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 02:26:20 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 08:33:25 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 06:24:34 GMT"}, {"version": "v4", "created": "Sat, 1 Feb 2020 10:59:52 GMT"}, {"version": "v5", "created": "Sat, 15 Feb 2020 16:05:12 GMT"}, {"version": "v6", "created": "Sun, 30 Aug 2020 05:55:46 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wu", "Yang", ""], ["Cai", "Xu", ""], ["Wei", "Pengxu", ""], ["Li", "Guanbin", ""], ["Lin", "Liang", ""]]}, {"id": "1910.14218", "submitter": "Meng Song", "authors": "Yuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu, Hao Su", "title": "S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in Cluttered\n  Scenes", "comments": "Accepted at the Conference on Robot Learning (CoRL) 2019. Project\n  webpage is available here: https://sites.google.com/view/s4ggrapsing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grasping is among the most fundamental and long-lasting problems in robotics\nstudy. This paper studies the problem of 6-DoF(degree of freedom) grasping by a\nparallel gripper in a cluttered scene captured using a commodity depth sensor\nfrom a single viewpoint. We address the problem in a learning-based framework.\nAt the high level, we rely on a single-shot grasp proposal network, trained\nwith synthetic data and tested in real-world scenarios. Our single-shot neural\nnetwork architecture can predict amodal grasp proposal efficiently and\neffectively. Our training data synthesis pipeline can generate scenes of\ncomplex object configuration and leverage an innovative gripper contact model\nto create dense and high-quality grasp annotations. Experiments in synthetic\nand real environments have demonstrated that the proposed approach can\noutperform state-of-the-arts by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 02:29:57 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Qin", "Yuzhe", ""], ["Chen", "Rui", ""], ["Zhu", "Hao", ""], ["Song", "Meng", ""], ["Xu", "Jing", ""], ["Su", "Hao", ""]]}, {"id": "1910.14226", "submitter": "Yuhu Shan", "authors": "Yuhu Shan", "title": "Distilling Pixel-Wise Feature Similarities for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the neural network compression techniques, knowledge distillation is an\neffective one which forces a simpler student network to mimic the output of a\nlarger teacher network. However, most of such model distillation methods focus\non the image-level classification task. Directly adapting these methods to the\ntask of semantic segmentation only brings marginal improvements. In this paper,\nwe propose a simple, yet effective knowledge representation referred to as\npixel-wise feature similarities (PFS) to tackle the challenging distillation\nproblem of semantic segmentation. The developed PFS encodes spatial structural\ninformation for each pixel location of the high-level convolutional features,\nwhich helps guide the distillation process in an easier way. Furthermore, a\nnovel weighted pixel-level soft prediction imitation approach is proposed to\nenable the student network to selectively mimic the teacher network's output,\naccording to their pixel-wise knowledge-gaps. Extensive experiments are\nconducted on the challenging datasets of Pascal VOC 2012, ADE20K and Pascal\nContext. Our approach brings significant performance improvements compared to\nseveral strong baselines and achieves new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 02:59:51 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Shan", "Yuhu", ""]]}, {"id": "1910.14241", "submitter": "Avinash Kori", "authors": "Avinash Kori, Manik Sharma", "title": "Dynamic Regularizer with an Informative Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regularization methods, specifically those which directly alter weights like\n$L_1$ and $L_2$, are an integral part of many learning algorithms. Both the\nregularizers mentioned above are formulated by assuming certain priors in the\nparameter space and these assumptions, in some cases, induce sparsity in the\nparameter space. Regularizers help in transferring beliefs one has on the\ndataset or the parameter space by introducing adequate terms in the loss\nfunction. Any kind of formulation represents a specific set of beliefs: $L_1$\nregularization conveys that the parameter space should be sparse whereas $L_2$\nregularization conveys that the parameter space should be bounded and\ncontinuous. These regularizers in turn leverage certain priors to express these\ninherent beliefs. A better understanding of how the prior affects the behavior\nof the parameters and how the priors can be updated based on the dataset can\ncontribute greatly in improving the generalization capabilities of a function\nestimator. In this work, we introduce a weakly informative prior and then\nfurther extend it to an informative prior in order to formulate a\nregularization penalty, which shows better results in terms of inducing\nsparsity experimentally, when compared to regularizers based only on Gaussian\nand Laplacian priors. Experimentally, we verify that a regularizer based on an\nadapted prior improves the generalization capabilities of any network. We\nillustrate the performance of the proposed method on the MNIST and CIFAR-10\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 03:40:03 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Kori", "Avinash", ""], ["Sharma", "Manik", ""]]}, {"id": "1910.14247", "submitter": "Fania Mokhayeri", "authors": "Fania Mokhayeri, Kaveh Kamali, Eric Granger", "title": "Cross-Domain Face Synthesis using a Controllable GAN", "comments": null, "journal-ref": "Winter Conference on Applications of Computer Vision (WACV 2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The performance of face recognition (FR) systems applied in video\nsurveillance has been shown to improve when the design data is augmented\nthrough synthetic face generation. This is true, for instance, with pair-wise\nmatchers (e.g., deep Siamese networks) that typically rely on a reference\ngallery with one still image per individual. However, generating synthetic\nimages in the source domain may not improve the performance during operations\ndue to the domain shift w.r.t. the target domain. Moreover, despite the\nemergence of Generative Adversarial Networks (GANs) for realistic synthetic\ngeneration, it is often difficult to control the conditions under which\nsynthetic faces are generated. In this paper, a cross-domain face synthesis\napproach is proposed that integrates a new Controllable GAN (C-GAN). It employs\nan off-the-shelf 3D face model as a simulator to generate face images under\nvarious poses. The simulated images and noise are input to the C-GAN for\nrealism refinement which employs an additional adversarial game as a third\nplayer to preserve the identity and specific facial attributes of the refined\nimages. This allows generating realistic synthetic face images that reflects\ncapture conditions in the target domain while controlling the GAN output to\ngenerate faces under desired pose conditions. Experiments were performed using\nvideos from the Chokepoint and COX-S2V datasets, and a deep Siamese network for\nFR with a single reference still per person. Results indicate that the proposed\napproach can provide a higher level of accuracy compared to the current\nstate-of-the-art approaches for synthetic data augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 04:16:10 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Mokhayeri", "Fania", ""], ["Kamali", "Kaveh", ""], ["Granger", "Eric", ""]]}, {"id": "1910.14255", "submitter": "Frincy Clement", "authors": "Frincy Clement, Kirtan Shah, Dhara Pancholi", "title": "A Review of methods for Textureless Object Recognition", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textureless object recognition has become a significant task in Computer\nVision with the advent of Robotics and its applications in manufacturing\nsector. It has been very challenging to get good performance because of its\nlack of discriminative features and reflectance properties. Hence, the\napproaches used for textured objects cannot be applied for textureless objects.\nA lot of work has been done in the last 20 years, especially in the recent 5\nyears after the TLess and other textureless dataset were introduced. In our\nresearch, we plan to combine image processing techniques (for feature\nenhancement) along with deep learning techniques (for object recognition). Here\nwe present an overview of the various existing work in the field of textureless\nobject recognition, which can be broadly classified into View-based,\nFeature-based and Shape-based. We have also added a review of few of the\nresearch papers submitted at the International Conference on Smart Multimedia,\n2018. Index terms: Computer Vision, Textureless object detection, Textureless\nobject recognition, Feature-based, Edge detection, Deep Learning\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 04:39:30 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Clement", "Frincy", ""], ["Shah", "Kirtan", ""], ["Pancholi", "Dhara", ""]]}, {"id": "1910.14260", "submitter": "Zehua Zhang", "authors": "Zehua Zhang, Chen Yu, David Crandall", "title": "A Self Validation Network for Object-Level Human Attention Estimation", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the foveated nature of the human vision system, people can focus their\nvisual attention on a small region of their visual field at a time, which\nusually contains only a single object. Estimating this object of attention in\nfirst-person (egocentric) videos is useful for many human-centered real-world\napplications such as augmented reality applications and driver assistance\nsystems. A straightforward solution for this problem is to pick the object\nwhose bounding box is hit by the gaze, where eye gaze point estimation is\nobtained from a traditional eye gaze estimator and object candidates are\ngenerated from an off-the-shelf object detector. However, such an approach can\nfail because it addresses the where and the what problems separately, despite\nthat they are highly related, chicken-and-egg problems. In this paper, we\npropose a novel unified model that incorporates both spatial and temporal\nevidence in identifying as well as locating the attended object in firstperson\nvideos. It introduces a novel Self Validation Module that enforces and\nleverages consistency of the where and the what concepts. We evaluate on two\npublic datasets, demonstrating that Self Validation Module significantly\nbenefits both training and testing and that our model outperforms the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 04:56:43 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 19:06:13 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zhang", "Zehua", ""], ["Yu", "Chen", ""], ["Crandall", "David", ""]]}, {"id": "1910.14303", "submitter": "Yitian Yuan", "authors": "Yitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, Wenwu Zhu", "title": "Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding\n  in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal sentence grounding in videos aims to detect and localize one target\nvideo segment, which semantically corresponds to a given sentence. Existing\nmethods mainly tackle this task via matching and aligning semantics between a\nsentence and candidate video segments, while neglect the fact that the sentence\ninformation plays an important role in temporally correlating and composing the\ndescribed contents in videos. In this paper, we propose a novel semantic\nconditioned dynamic modulation (SCDM) mechanism, which relies on the sentence\nsemantics to modulate the temporal convolution operations for better\ncorrelating and composing the sentence related video contents over time. More\nimportantly, the proposed SCDM performs dynamically with respect to the diverse\nvideo contents so as to establish a more precise matching relationship between\nsentence and video, thereby improving the temporal grounding accuracy.\nExtensive experiments on three public datasets demonstrate that our proposed\nmodel outperforms the state-of-the-arts with clear margins, illustrating the\nability of SCDM to better associate and localize relevant video contents for\ntemporal sentence grounding. Our code for this paper is available at\nhttps://github.com/yytzsy/SCDM .\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 08:33:25 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Yuan", "Yitian", ""], ["Ma", "Lin", ""], ["Wang", "Jingwen", ""], ["Liu", "Wei", ""], ["Zhu", "Wenwu", ""]]}, {"id": "1910.14325", "submitter": "Ruturaj Gavaskar", "authors": "Ruturaj G. Gavaskar and Kunal N. Chaudhury", "title": "On the Proof of Fixed-Point Convergence for Plug-and-Play ADMM", "comments": "Accepted in IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2019.2950611", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most state-of-the-art image restoration methods, the sum of a\ndata-fidelity and a regularization term is optimized using an iterative\nalgorithm such as ADMM (alternating direction method of multipliers). In recent\nyears, the possibility of using denoisers for regularization has been explored\nin several works. A popular approach is to formally replace the proximal\noperator within the ADMM framework with some powerful denoiser. However, since\nmost state-of-the-art denoisers cannot be posed as a proximal operator, one\ncannot guarantee the convergence of these so-called plug-and-play (PnP)\nalgorithms. In fact, the theoretical convergence of PnP algorithms is an active\nresearch topic. In this letter, we consider the result of Chan et al. (IEEE\nTCI, 2017), where fixed-point convergence of an ADMM-based PnP algorithm was\nestablished for a class of denoisers. We argue that the original proof is\nincomplete, since convergence is not analyzed for one of the three possible\ncases outlined in the paper. Moreover, we explain why the argument for the\nother cases does not apply in this case. We give a different analysis to fill\nthis gap, which firmly establishes the original convergence theorem.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 09:19:06 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Gavaskar", "Ruturaj G.", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "1910.14333", "submitter": "Zhirui Chen", "authors": "Zhirui Chen, Jianheng Li, Wei-Shi Zheng", "title": "Weakly Supervised Tracklet Person Re-Identification by Deep Feature-wise\n  Mutual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability problem caused by the difficulty in annotating Person\nRe-identification(Re-ID) datasets has become a crucial bottleneck in the\ndevelopment of Re-ID.To address this problem, many unsupervised Re-ID methods\nhave recently been proposed.Nevertheless, most of these models require transfer\nfrom another auxiliary fully supervised dataset, which is still expensive to\nobtain.In this work, we propose a Re-ID model based on Weakly Supervised\nTracklets(WST) data from various camera views, which can be inexpensively\nacquired by combining the fragmented tracklets of the same person in the same\ncamera view over a period of time.We formulate our weakly supervised tracklets\nRe-ID model by a novel method, named deep feature-wise mutual learning(DFML),\nwhich consists of Mutual Learning on Feature Extractors (MLFE) and Mutual\nLearning on Feature Classifiers (MLFC).We propose MLFE by leveraging two\nfeature extractors to learn from each other to extract more robust and\ndiscriminative features.On the other hand, we propose MLFC by adapting\ndiscriminative features from various camera views to each classifier. Extensive\nexperiments demonstrate the superiority of our proposed DFML over the\nstate-of-the-art unsupervised models and even some supervised models on three\nRe-ID benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 09:37:19 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Chen", "Zhirui", ""], ["Li", "Jianheng", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1910.14377", "submitter": "Alireza Ahrabian", "authors": "Alireza Ahrabian, Joao F. C. Mota, Andrew M. Wallace", "title": "Image-Guided Depth Upsampling via Hessian and TV Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that combines sparse depth (LiDAR) measurements with an\nintensity image and to produce a dense high-resolution depth image. As there\nare few, but accurate, depth measurements from the scene, our method infers the\nremaining depth values by incorporating information from the intensity image,\nnamely the magnitudes and directions of the identified edges, and by assuming\nthat the scene is composed mostly of flat surfaces. Such inference is achieved\nby solving a convex optimisation problem with properly weighted regularisers\nthat are based on the `1-norm (specifically, on total variation). We solve the\nresulting problem with a computationally efficient ADMM-based algorithm. Using\nthe SYNTHIA and KITTI datasets, our experiments show that the proposed method\nachieves a depth reconstruction performance comparable to or better than other\nmodel-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 11:19:22 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Ahrabian", "Alireza", ""], ["Mota", "Joao F. C.", ""], ["Wallace", "Andrew M.", ""]]}, {"id": "1910.14442", "submitter": "Fei Xia", "authors": "Fei Xia, William B. Shen, Chengshu Li, Priya Kasimbeg, Micael Tchapmi,\n  Alexander Toshev, Li Fei-Fei, Roberto Mart\\'in-Mart\\'in, Silvio Savarese", "title": "Interactive Gibson Benchmark: A Benchmark for Interactive Navigation in\n  Cluttered Environments", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Interactive Gibson Benchmark, the first comprehensive benchmark\nfor training and evaluating Interactive Navigation: robot navigation strategies\nwhere physical interaction with objects is allowed and even encouraged to\naccomplish a task. For example, the robot can move objects if needed in order\nto clear a path leading to the goal location. Our benchmark comprises two novel\nelements: 1) a new experimental setup, the Interactive Gibson Environment,\nwhich simulates high fidelity visuals of indoor scenes, and high fidelity\nphysical dynamics of the robot and common objects found in these scenes; 2) a\nset of Interactive Navigation metrics which allows one to study the interplay\nbetween navigation and physical interaction. We present and evaluate multiple\nlearning-based baselines in Interactive Gibson, and provide insights into\nregimes of navigation with different trade-offs between navigation path\nefficiency and disturbance of surrounding objects. We make our benchmark\npublicly available(https://sites.google.com/view/interactivegibsonenv) and\nencourage researchers from all disciplines in robotics (e.g. planning,\nlearning, control) to propose, evaluate, and compare their Interactive\nNavigation solutions in Interactive Gibson.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 01:04:37 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 07:08:01 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Xia", "Fei", ""], ["Shen", "William B.", ""], ["Li", "Chengshu", ""], ["Kasimbeg", "Priya", ""], ["Tchapmi", "Micael", ""], ["Toshev", "Alexander", ""], ["Fei-Fei", "Li", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Savarese", "Silvio", ""]]}, {"id": "1910.14453", "submitter": "Ramy Battrawy", "authors": "Ramy Battrawy, Ren\\'e Schuster, Oliver Wasenm\\\"uller, Qing Rao, Didier\n  Stricker", "title": "LiDAR-Flow: Dense Scene Flow Estimation from Sparse LiDAR and Stereo\n  Images", "comments": "Accepted in IROS19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach called LiDAR-Flow to robustly estimate a dense\nscene flow by fusing a sparse LiDAR with stereo images. We take the advantage\nof the high accuracy of LiDAR to resolve the lack of information in some\nregions of stereo images due to textureless objects, shadows, ill-conditioned\nlight environment and many more. Additionally, this fusion can overcome the\ndifficulty of matching unstructured 3D points between LiDAR-only scans. Our\nLiDAR-Flow approach consists of three main steps; each of them exploits LiDAR\nmeasurements. First, we build strong seeds from LiDAR to enhance the robustness\nof matches between stereo images. The imagery part seeks the motion matches and\nincreases the density of scene flow estimation. Then, a consistency check\nemploys LiDAR seeds to remove the possible mismatches. Finally, LiDAR\nmeasurements constraint the edge-preserving interpolation method to fill the\nremaining gaps. In our evaluation we investigate the individual processing\nsteps of our LiDAR-Flow approach and demonstrate the superior performance\ncompared to image-only approach.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 13:22:19 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 16:35:50 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Battrawy", "Ramy", ""], ["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Rao", "Qing", ""], ["Stricker", "Didier", ""]]}, {"id": "1910.14481", "submitter": "Dushyant Rao", "authors": "Dushyant Rao, Francesco Visin, Andrei A. Rusu, Yee Whye Teh, Razvan\n  Pascanu, Raia Hadsell", "title": "Continual Unsupervised Representation Learning", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to improve the ability of modern learning systems to\ndeal with non-stationary distributions, typically by attempting to learn a\nseries of tasks sequentially. Prior art in the field has largely considered\nsupervised or reinforcement learning tasks, and often assumes full knowledge of\ntask labels and boundaries. In this work, we propose an approach (CURL) to\ntackle a more general problem that we will refer to as unsupervised continual\nlearning. The focus is on learning representations without any knowledge about\ntask identity, and we explore scenarios when there are abrupt changes between\ntasks, smooth transitions from one task to another, or even when the data is\nshuffled. The proposed approach performs task inference directly within the\nmodel, is able to dynamically expand to capture new concepts over its lifetime,\nand incorporates additional rehearsal-based techniques to deal with\ncatastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised\nlearning setting with MNIST and Omniglot, where the lack of labels ensures no\ninformation is leaked about the task. Further, we demonstrate strong\nperformance compared to prior art in an i.i.d setting, or when adapting the\ntechnique to supervised tasks such as incremental class learning.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 14:18:45 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Rao", "Dushyant", ""], ["Visin", "Francesco", ""], ["Rusu", "Andrei A.", ""], ["Teh", "Yee Whye", ""], ["Pascanu", "Razvan", ""], ["Hadsell", "Raia", ""]]}, {"id": "1910.14526", "submitter": "Carmelo Sferrazza", "authors": "Camill Trueeb, Carmelo Sferrazza and Raffaello D'Andrea", "title": "Towards vision-based robotic skins: a data-driven, multi-camera tactile\n  sensor", "comments": "Accompanying video: https://youtu.be/lbavqAlKl98", "journal-ref": "Proceedings of the 2020 3rd IEEE International Conference on Soft\n  Robotics (RoboSoft), pp. 333-338", "doi": "10.1109/RoboSoft48309.2020.9116060", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design of a multi-camera optical tactile sensor that\nprovides information about the contact force distribution applied to its soft\nsurface. This information is contained in the motion of spherical particles\nspread within the surface, which deforms when subject to force. The small\nembedded cameras capture images of the different particle patterns that are\nthen mapped to the three-dimensional contact force distribution through a\nmachine learning architecture. The design proposed in this paper exhibits a\nlarger contact surface and a thinner structure than most of the existing\ncamera-based tactile sensors, without the use of additional reflecting\ncomponents such as mirrors. A modular implementation of the learning\narchitecture is discussed that facilitates the scalability to larger surfaces\nsuch as robotic skins.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 15:10:20 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 03:03:28 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 12:52:47 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Trueeb", "Camill", ""], ["Sferrazza", "Carmelo", ""], ["D'Andrea", "Raffaello", ""]]}, {"id": "1910.14532", "submitter": "Maxime Ferrera", "authors": "Maxime Ferrera (LIRMM), Vincent Creuze (LIRMM), Julien Moras, Pauline\n  Trouv\\'e-Peloux", "title": "AQUALOC: An Underwater Dataset for Visual-Inertial-Pressure Localization", "comments": "The International Journal of Robotics Research, SAGE Publications,\n  2019", "journal-ref": null, "doi": "10.1177/0278364919883346", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dataset, dedicated to the development of simultaneous\nlocalization and mapping methods for underwater vehicles navigating close to\nthe seabed. The data sequences composing this dataset are recorded in three\ndifferent environments: a harbor at a depth of a few meters, a first\narchaeological site at a depth of 270 meters and a second site at a depth of\n380 meters. The data acquisition is performed using Remotely Operated Vehicles\nequipped with a monocular monochromatic camera, a low-cost inertial measurement\nunit, a pressure sensor and a computing unit, all embedded in a single\nenclosure. The sensors' measurements are recorded synchronously on the\ncomputing unit and seventeen sequences have been created from all the acquired\ndata. These sequences are made available in the form of ROS bags and as raw\ndata. For each sequence, a trajectory has also been computed offline using a\nStructure-from-Motion library in order to allow the comparison with real-time\nlocalization methods. With the release of this dataset, we wish to provide data\ndifficult to acquire and to encourage the development of vision-based\nlocalization methods dedicated to the underwater environment. The dataset can\nbe downloaded from: http://www.lirmm.fr/aqualoc/\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 15:28:08 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Ferrera", "Maxime", "", "LIRMM"], ["Creuze", "Vincent", "", "LIRMM"], ["Moras", "Julien", ""], ["Trouv\u00e9-Peloux", "Pauline", ""]]}, {"id": "1910.14552", "submitter": "Madhu Kiran", "authors": "Madhu Kiran, Vivek Tiwari, Le Thanh Nguyen-Meidine and Eric Granger", "title": "On the Interaction Between Deep Detectors and Siamese Trackers in Video\n  Surveillance", "comments": "Presented in AVSS-2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking is an important function in many real-time video\nsurveillance applications, such as localization and spatio-temporal recognition\nof persons. In real-world applications, an object detector and tracker must\ninteract on a periodic basis to discover new objects, and thereby to initiate\ntracks. Periodic interactions with the detector can also allow the tracker to\nvalidate and/or update its object template with new bounding boxes. However,\nbounding boxes provided by a state-of-the-art detector are noisy, due to\nchanges in appearance, background and occlusion, which can cause the tracker to\ndrift. Moreover, CNN-based detectors can provide a high level of accuracy at\nthe expense of computational complexity, so interactions should be minimized\nfor real-time applications.\n  In this paper, a new approach is proposed to manage detector-tracker\ninteractions for trackers from the Siamese-FC family. By integrating a change\ndetection mechanism into a deep Siamese-FC tracker, its template can be adapted\nin response to changes in a target's appearance that lead to drifts during\ntracking. An abrupt change detection triggers an update of tracker template\nusing the bounding box produced by the detector, while in the case of a gradual\nchange, the detector is used to update an evolving set of templates for robust\nmatching.\n  Experiments were performed using state-of-the-art Siamese-FC trackers and the\nYOLOv3 detector on a subset of videos from the OTB-100 dataset that mimic video\nsurveillance scenarios. Results highlight the importance for reliable VOT of\nusing accurate detectors. They also indicate that our adaptive Siamese trackers\nare robust to noisy object detections, and can significantly improve the\nperformance of Siamese-FC tracking.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 15:52:51 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Kiran", "Madhu", ""], ["Tiwari", "Vivek", ""], ["Nguyen-Meidine", "Le Thanh", ""], ["Granger", "Eric", ""]]}, {"id": "1910.14565", "submitter": "Mehul S. Raval", "authors": "Hiren Galiyawala, Mehul S Raval, Shivansh Dave", "title": "Visual Appearance Based Person Retrieval in Unconstrained Environment\n  Videos", "comments": "11 pages", "journal-ref": "Image and Vision Computing, 2019", "doi": "10.1016/j.imavis.2019.10.002", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual appearance-based person retrieval is a challenging problem in\nsurveillance. It uses attributes like height, cloth color, cloth type and\ngender to describe a human. Such attributes are known as soft biometrics. This\npaper proposes person retrieval from surveillance video using height, torso\ncloth type, torso cloth color and gender. The approach introduces an adaptive\ntorso patch extraction and bounding box regression to improve the retrieval.\nThe algorithm uses fine-tuned Mask R-CNN and DenseNet-169 for person detection\nand attribute classification respectively. The performance is analyzed on AVSS\n2018 challenge II dataset and it achieves 11.35% improvement over\nstate-of-the-art based on average Intersection over Union measure.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:20:33 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Galiyawala", "Hiren", ""], ["Raval", "Mehul S", ""], ["Dave", "Shivansh", ""]]}, {"id": "1910.14567", "submitter": "Jevgenij Gamper", "authors": "Michael Zotov, Jevgenij Gamper", "title": "Conditional Denoising of Remote Sensing Imagery Using Cycle-Consistent\n  Deep Generative Models", "comments": "Accepted NeurIPS AI for Social Good, 14 December 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential of using remote sensing imagery for environmental modelling and\nfor providing real time support to humanitarian operations such as hurricane\nrelief efforts is well established. These applications are substantially\naffected by missing data due to non-structural noise such as clouds, shadows\nand other atmospheric effects. In this work we probe the potential of applying\na cycle-consistent latent variable deep generative model (DGM) for denoising\ncloudy Sentinel-2 observations conditioned on the information in cloud\npenetrating bands. We adapt the recently proposed Fr\\'{e}chet Distance metric\nto remote sensing images for evaluating performance of the generator,\ndemonstrate the potential of DGMs for conditional denoising, and discuss future\ndirections as well as the limitations of DGMs in Earth science and humanitarian\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:24:30 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Zotov", "Michael", ""], ["Gamper", "Jevgenij", ""]]}, {"id": "1910.14576", "submitter": "Raimon Fabregat", "authors": "Raimon Fabregat, Nelly Pustelnik, Paulo Gon\\c{c}alves and Pierre\n  Borgnat", "title": "Solving NMF with smoothness and sparsity constraints using PALM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization is a problem of dimensionality reduction\nand source separation of data that has been widely used in many fields since it\nwas studied in depth in 1999 by Lee and Seung, including in compression of\ndata, document clustering, processing of audio spectrograms and astronomy. In\nthis work we have adapted a minimization scheme for convex functions with\nnon-differentiable constraints called PALM to solve the NMF problem with\nsolutions that can be smooth and/or sparse, two properties frequently desired.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:29:59 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 09:04:43 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Fabregat", "Raimon", ""], ["Pustelnik", "Nelly", ""], ["Gon\u00e7alves", "Paulo", ""], ["Borgnat", "Pierre", ""]]}, {"id": "1910.14578", "submitter": "Minh-Tan Pham", "authors": "Minh-Tan Pham, S\\'ebastien Lef\\`evre", "title": "Very high resolution Airborne PolSAR Image Classification using\n  Convolutional Neural Networks", "comments": "5 pages, accepted in EUSAR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we exploit convolutional neural networks (CNNs) for the\nclassification of very high resolution (VHR) polarimetric SAR (PolSAR) data.\nDue to the significant appearance of heterogeneous textures within these data,\nnot only polarimetric features but also structural tensors are exploited to\nfeed CNN models. For deep networks, we use the SegNet model for semantic\nsegmentation, which corresponds to pixelwise classification in remote sensing.\nOur experiments on the airborne F-SAR data show that for VHR PolSAR images,\nSegNet could provide high accuracy for the classification task; and introducing\nstructural tensors together with polarimetric features as inputs could help the\nnetwork to focus more on geometrical information to significantly improve the\nclassification performance.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:32:10 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 21:13:23 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Pham", "Minh-Tan", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""]]}, {"id": "1910.14594", "submitter": "Vladimir Golkov", "authors": "Luca Della Libera, Vladimir Golkov, Yue Zhu, Arman Mielke, Daniel\n  Cremers", "title": "Deep Learning for 2D and 3D Rotatable Data: An Overview of Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the reasons for the success of convolutional networks is their\nequivariance/invariance under translations. However, rotatable data such as\nmolecules, living cells, everyday objects, or galaxies require processing with\nequivariance/invariance under rotations in cases where the rotation of the\ncoordinate system does not affect the meaning of the data (e.g. object\nclassification). On the other hand, estimation/processing of rotations is\nnecessary in cases where rotations are important (e.g. motion estimation).\nThere has been recent progress in methods and theory in all these regards. Here\nwe provide an overview of existing methods, both for 2D and 3D rotations (and\ntranslations), and identify commonalities and links between them, in the hope\nthat our insights will be useful for choosing and perfecting the methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:47:46 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Della Libera", "Luca", ""], ["Golkov", "Vladimir", ""], ["Zhu", "Yue", ""], ["Mielke", "Arman", ""], ["Cremers", "Daniel", ""]]}, {"id": "1910.14609", "submitter": "Jean-Benoit Delbrouck", "authors": "Jean-Benoit Delbrouck and Bastien Vanderplaetse and St\\'ephane Dupont", "title": "Can adversarial training learn image captioning ?", "comments": "Accepted to NeurIPS 2019 ViGiL workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, generative adversarial networks (GAN) have gathered a lot of\ninterest. Their efficiency in generating unseen samples of high quality,\nespecially images, has improved over the years. In the field of Natural\nLanguage Generation (NLG), the use of the adversarial setting to generate\nmeaningful sentences has shown to be difficult for two reasons: the lack of\nexisting architectures to produce realistic sentences and the lack of\nevaluation tools. In this paper, we propose an adversarial architecture related\nto the conditional GAN (cGAN) that generates sentences according to a given\nimage (also called image captioning). This attempt is the first that uses no\npre-training or reinforcement methods. We also explain why our experiment\nsettings can be safely evaluated and interpreted for further works.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:59:14 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Delbrouck", "Jean-Benoit", ""], ["Vanderplaetse", "Bastien", ""], ["Dupont", "St\u00e9phane", ""]]}, {"id": "1910.14634", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Mahdi Soltanolkotabi", "title": "Denoising and Regularization via Exploiting the Structural Bias of\n  Convolutional Generators", "comments": "final ICRL version; simplifications in the proof", "journal-ref": "International Conference on Learning Representations (ICLR) 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have emerged as highly successful tools\nfor image generation, recovery, and restoration. A major contributing factor to\nthis success is that convolutional networks impose strong prior assumptions\nabout natural images. A surprising experiment that highlights this\narchitectural bias towards natural images is that one can remove noise and\ncorruptions from a natural image without using any training data, by simply\nfitting (via gradient descent) a randomly initialized, over-parameterized\nconvolutional generator to the corrupted image. While this over-parameterized\nnetwork can fit the corrupted image perfectly, surprisingly after a few\niterations of gradient descent it generates an almost uncorrupted image. This\nintriguing phenomenon enables state-of-the-art CNN-based denoising and\nregularization of other inverse problems. In this paper, we attribute this\neffect to a particular architectural choice of convolutional networks, namely\nconvolutions with fixed interpolating filters. We then formally characterize\nthe dynamics of fitting a two-layer convolutional generator to a noisy signal\nand prove that early-stopped gradient descent denoises/regularizes. Our proof\nrelies on showing that convolutional generators fit the structured part of an\nimage significantly faster than the corrupted portion.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:22:00 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 01:49:25 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Heckel", "Reinhard", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1910.14655", "submitter": "Huan Zhang", "authors": "Huan Zhang, Minhao Cheng, Cho-Jui Hsieh", "title": "Enhancing Certifiable Robustness via a Deep Model Ensemble", "comments": "This is an extended version of ICLR 2019 Safe Machine Learning\n  Workshop (SafeML) paper, \"RobBoost: A provable approach to boost the\n  robustness of deep model ensemble\". May 6, 2019, New Orleans, LA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm to enhance certified robustness of a deep model\nensemble by optimally weighting each base model. Unlike previous works on using\nensembles to empirically improve robustness, our algorithm is based on\noptimizing a guaranteed robustness certificate of neural networks. Our proposed\nensemble framework with certified robustness, RobBoost, formulates the optimal\nmodel selection and weighting task as an optimization problem on a lower bound\nof classification margin, which can be efficiently solved using coordinate\ndescent. Experiments show that our algorithm can form a more robust ensemble\nthan naively averaging all available models using robustly trained MNIST or\nCIFAR base models. Additionally, our ensemble typically has better accuracy on\nclean (unperturbed) data. RobBoost allows us to further improve certified\nrobustness and clean accuracy by creating an ensemble of already certified\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:48:33 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Zhang", "Huan", ""], ["Cheng", "Minhao", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1910.14667", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Ser-Nam Lim, Larry Davis, Tom Goldstein", "title": "Making an Invisibility Cloak: Real World Adversarial Attacks on Object\n  Detectors", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a systematic study of adversarial attacks on state-of-the-art\nobject detection frameworks. Using standard detection datasets, we train\npatterns that suppress the objectness scores produced by a range of commonly\nused detectors, and ensembles of detectors. Through extensive experiments, we\nbenchmark the effectiveness of adversarially trained patches under both\nwhite-box and black-box settings, and quantify transferability of attacks\nbetween datasets, object classes, and detector models. Finally, we present a\ndetailed study of physical world attacks using printed posters and wearable\nclothes, and rigorously quantify the performance of such attacks with different\nmetrics.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:56:29 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 17:59:49 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Wu", "Zuxuan", ""], ["Lim", "Ser-Nam", ""], ["Davis", "Larry", ""], ["Goldstein", "Tom", ""]]}, {"id": "1910.14671", "submitter": "Jingxiang Lin", "authors": "Jingxiang Lin, Unnat Jain, Alexander G. Schwing", "title": "TAB-VCR: Tags and Attributes based Visual Commonsense Reasoning\n  Baselines", "comments": "Accepted to NeurIPS 2019. Project page:\n  https://deanplayerljx.github.io/tabvcr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning is an important ability that we learn from a very early age. Yet,\nreasoning is extremely hard for algorithms. Despite impressive recent progress\nthat has been reported on tasks that necessitate reasoning, such as visual\nquestion answering and visual dialog, models often exploit biases in datasets.\nTo develop models with better reasoning abilities, recently, the new visual\ncommonsense reasoning (VCR) task has been introduced. Not only do models have\nto answer questions, but also do they have to provide a reason for the given\nanswer. The proposed baseline achieved compelling results, leveraging a\nmeticulously designed model composed of LSTM modules and attention nets. Here\nwe show that a much simpler model obtained by ablating and pruning the existing\nintricate baseline can perform better with half the number of trainable\nparameters. By associating visual features with attribute information and\nbetter text to image grounding, we obtain further improvements for our simpler\n& effective baseline, TAB-VCR. We show that this approach results in a 5.3%,\n4.4% and 6.5% absolute improvement over the previous state-of-the-art on\nquestion answering, answer justification and holistic VCR.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:59:57 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 15:55:26 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Lin", "Jingxiang", ""], ["Jain", "Unnat", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1910.14673", "submitter": "Tiantian Fang", "authors": "Tiantian Fang and Alexander G. Schwing", "title": "Co-Generation with GANs using AIS based HMC", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring the most likely configuration for a subset of variables of a joint\ndistribution given the remaining ones - which we refer to as co-generation - is\nan important challenge that is computationally demanding for all but the\nsimplest settings. This task has received a considerable amount of attention,\nparticularly for classical ways of modeling distributions like structured\nprediction. In contrast, almost nothing is known about this task when\nconsidering recently proposed techniques for modeling high-dimensional\ndistributions, particularly generative adversarial nets (GANs). Therefore, in\nthis paper, we study the occurring challenges for co-generation with GANs. To\naddress those challenges we develop an annealed importance sampling based\nHamiltonian Monte Carlo co-generation algorithm. The presented approach\nsignificantly outperforms classical gradient based methods on a synthetic and\non the CelebA and LSUN datasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:59:59 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Fang", "Tiantian", ""], ["Schwing", "Alexander G.", ""]]}]