[{"id": "1106.0107", "submitter": "Jomy John", "authors": "John Jomy, K. V. Pramod, Balakrishnan Kannan", "title": "Handwritten Character Recognition of South Indian Scripts: A Review", "comments": "Paper presented on the \"National Conference on Indian Language\n  Computing\", Kochi, February 19-20, 2011. 6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten character recognition is always a frontier area of research in\nthe field of pattern recognition and image processing and there is a large\ndemand for OCR on hand written documents. Even though, sufficient studies have\nperformed in foreign scripts like Chinese, Japanese and Arabic characters, only\na very few work can be traced for handwritten character recognition of Indian\nscripts especially for the South Indian scripts. This paper provides an\noverview of offline handwritten character recognition in South Indian Scripts,\nnamely Malayalam, Tamil, Kannada and Telungu.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 07:22:04 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Jomy", "John", ""], ["Pramod", "K. V.", ""], ["Kannan", "Balakrishnan", ""]]}, {"id": "1106.0357", "submitter": "Mohamad Tarifi", "authors": "Mohamad Tarifi, Meera Sitharam, Jeffery Ho", "title": "Learning Hierarchical Sparse Representations using Iterative Dictionary\n  Learning and Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an elemental building block which combines Dictionary\nLearning and Dimension Reduction (DRDL). We show how this foundational element\ncan be used to iteratively construct a Hierarchical Sparse Representation (HSR)\nof a sensory stream. We compare our approach to existing models showing the\ngenerality of our simple prescription. We then perform preliminary experiments\nusing this framework, illustrating with the example of an object recognition\ntask using standard datasets. This work introduces the very first steps towards\nan integrated framework for designing and analyzing various computational tasks\nfrom learning to attention to action. The ultimate goal is building a\nmathematically rigorous, integrated theory of intelligence.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2011 02:31:04 GMT"}], "update_date": "2011-06-03", "authors_parsed": [["Tarifi", "Mohamad", ""], ["Sitharam", "Meera", ""], ["Ho", "Jeffery", ""]]}, {"id": "1106.0371", "submitter": "Ashraf Aly Ahmed", "authors": "Ashraf A. Aly, Safaai Bin Deris and Nazar Zaki", "title": "A Novel Image Segmentation Enhancement Technique based on Active Contour\n  and Topological Alignments", "comments": "7 pages", "journal-ref": "Advanced Computing: An International Journal ( ACIJ ), Vol.2,\n  No.3, May 2011", "doi": "10.5121/acij.2011.2301", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Topological alignments and snakes are used in image processing, particularly\nin locating object boundaries. Both of them have their own advantages and\nlimitations. To improve the overall image boundary detection system, we focused\non developing a novel algorithm for image processing. The algorithm we propose\nto develop will based on the active contour method in conjunction with\ntopological alignments method to enhance the image detection approach. The\nalgorithm presents novel technique to incorporate the advantages of both\nTopological Alignments and snakes. Where the initial segmentation by\nTopological Alignments is firstly transformed into the input of the snake model\nand begins its evolvement to the interested object boundary. The results show\nthat the algorithm can deal with low contrast images and shape cells,\ndemonstrate the segmentation accuracy under weak image boundaries, which\nresponsible for lacking accuracy in image detecting techniques. We have\nachieved better segmentation and boundary detecting for the image, also the\nability of the system to improve the low contrast and deal with over and under\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2011 06:31:13 GMT"}], "update_date": "2011-06-03", "authors_parsed": [["Aly", "Ashraf A.", ""], ["Deris", "Safaai Bin", ""], ["Zaki", "Nazar", ""]]}, {"id": "1106.0823", "submitter": "Oleg Kupervasser", "authors": "Oleg Kupervasser", "title": "Recovering Epipolar Geometry from Images of Smooth Surfaces", "comments": "accepted to \"Pattern Recognition and Image Analysis\" for publishing\n  in 2013, 33 pages, 19 figures", "journal-ref": "Pattern Recognition and Image Analysis, April 2013, Volume 23,\n  Issue 2, pp 236-257", "doi": "10.1134/S1054661813020107", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present four methods for recovering the epipolar geometry from images of\nsmooth surfaces. In the existing methods for recovering epipolar geometry\ncorresponding feature points are used that cannot be found in such images. The\nfirst method is based on finding corresponding characteristic points created by\nillumination (ICPM - illumination characteristic points' method (PM)). The\nsecond method is based on correspondent tangency points created by tangents\nfrom epipoles to outline of smooth bodies (OTPM - outline tangent PM). These\ntwo methods are exact and give correct results for real images, because\npositions of the corresponding illumination characteristic points and\ncorresponding outline are known with small errors. But the second method is\nlimited either to special type of scenes or to restricted camera motion. We\nalso consider two more methods which are termed CCPM (curve characteristic PM)\nand CTPM (curve tangent PM), for searching epipolar geometry for images of\nsmooth bodies based on a set of level curves with constant illumination\nintensity. The CCPM method is based on searching correspondent points on\nisophoto curves with the help of correlation of curvatures between these lines.\nThe CTPM method is based on property of the tangential to isophoto curve\nepipolar line to map into the tangential to correspondent isophoto curves\nepipolar line. The standard method (SM) based on knowledge of pairs of the\nalmost exact correspondent points. The methods have been implemented and tested\nby SM on pairs of real images. Unfortunately, the last two methods give us only\na finite subset of solutions including \"good\" solution. Exception is \"epipoles\nin infinity\". The main reason is inaccuracy of assumption of constant\nbrightness for smooth bodies. But outline and illumination characteristic\npoints are not influenced by this inaccuracy. So, the first pair of methods\ngives exact results.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2011 13:48:21 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2011 12:33:21 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2011 09:27:20 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2012 12:38:37 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Kupervasser", "Oleg", ""]]}, {"id": "1106.0962", "submitter": "Joydeep Basu", "authors": "K. Chattopadhyay, J. Basu, A. Konar", "title": "An efficient circle detection scheme in digital images using ant system\n  algorithm", "comments": "4 pages, 3 figures, Published in Proceedings of the IEEE Sponsored\n  Conference on Computational Intelligence, Control And Computer Vision In\n  Robotics & Automation, 2008", "journal-ref": "Proceedings of the 2008 IEEE Sponsored Conference on Computational\n  Intelligence, Control And Computer Vision In Robotics & Automation, Rourkela,\n  India, 2008, pages 145-148", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of geometric features in digital images is an important exercise in\nimage analysis and computer vision. The Hough Transform techniques for\ndetection of circles require a huge memory space for data processing hence\nrequiring a lot of time in computing the locations of the data space, writing\nto and searching through the memory space. In this paper we propose a novel and\nefficient scheme for detecting circles in edge-detected grayscale digital\nimages. We use Ant-system algorithm for this purpose which has not yet found\nmuch application in this field. The main feature of this scheme is that it can\ndetect both intersecting as well as non-intersecting circles with a time\nefficiency that makes it useful in real time applications. We build up an ant\nsystem of new type which finds out closed loops in the image and then tests\nthem for circles.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2011 05:52:09 GMT"}], "update_date": "2011-06-07", "authors_parsed": [["Chattopadhyay", "K.", ""], ["Basu", "J.", ""], ["Konar", "A.", ""]]}, {"id": "1106.0987", "submitter": "Junping Zhang", "authors": "Junping Zhang and Ziyu Xie and Stan Z. Li", "title": "Nearest Prime Simplicial Complex for Object Recognition", "comments": "16pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure representation of data distribution plays an important role in\nunderstanding the underlying mechanism of generating data. In this paper, we\npropose nearest prime simplicial complex approaches (NSC) by utilizing\npersistent homology to capture such structures. Assuming that each class is\nrepresented with a prime simplicial complex, we classify unlabeled samples\nbased on the nearest projection distances from the samples to the simplicial\ncomplexes. We also extend the extrapolation ability of these complexes with a\nprojection constraint term. Experiments in simulated and practical datasets\nindicate that compared with several published algorithms, the proposed NSC\napproaches achieve promising performance without losing the structure\nrepresentation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2011 08:32:16 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zhang", "Junping", ""], ["Xie", "Ziyu", ""], ["Li", "Stan Z.", ""]]}, {"id": "1106.1975", "submitter": "Khaled Masmoudi Mr.", "authors": "Khaled Masmoudi and Marc Antonini and Pierre Kornprobst", "title": "Exact Reconstruction of the Rank Order Coding using Frames Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to revisit rank order coding by proposing an original exact\ndecoding procedure for it. Rank order coding was proposed by Simon Thorpe et\nal. who stated that the retina represents the visual stimulus by the order in\nwhich its cells are activated. A classical rank order coder/decoder was then\ndesigned on this basis [1]. Though, it appeared that the decoding procedure\nemployed yields reconstruction errors that limit the model Rate/Quality\nperformances when used as an image codec. The attempts made in the literature\nto overcome this issue are time consuming and alter the coding procedure, or\nare lacking mathematical support and feasibility for standard size images. Here\nwe solve this problem in an original fashion by using the frames theory, where\na frame of a vector space designates an extension for the notion of basis.\nFirst, we prove that the analyzing filter bank considered is a frame, and then\nwe define the corresponding dual frame that is necessary for the exact image\nreconstruction. Second, to deal with the problem of memory overhead, we design\na recursive out-of-core blockwise algorithm for the computation of this dual\nframe. Our work provides a mathematical formalism for the retinal model under\nstudy and defines a simple and exact reverse transform for it with up to 270 dB\nof PSNR gain compared to [1]. Furthermore, the framework presented here can be\nextended to several models of the visual cortical areas using redundant\nrepresentations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2011 08:26:16 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2011 14:18:34 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Masmoudi", "Khaled", ""], ["Antonini", "Marc", ""], ["Kornprobst", "Pierre", ""]]}, {"id": "1106.2124", "submitter": "Ge Wang", "authors": "Ge Wang, Jie Zhang, Hao Gao, Victor Weir, Hengyong Yu, Wenxiang Cong,\n  Xiaochen Xu, Haiou Shen, James Bennett, Yue Wang, Michael Vannier", "title": "Omni-tomography/Multi-tomography -- Integrating Multiple Modalities for\n  Simultaneous Imaging", "comments": "43 pages, 15 figures, 99 references, provisional patent applications\n  filed by Virginia Tech", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current tomographic imaging systems need major improvements, especially when\nmulti-dimensional, multi-scale, multi-temporal and multi-parametric phenomena\nare under investigation. Both preclinical and clinical imaging now depend on in\nvivo tomography, often requiring separate evaluations by different imaging\nmodalities to define morphologic details, delineate interval changes due to\ndisease or interventions, and study physiological functions that have\ninterconnected aspects. Over the past decade, fusion of multimodality images\nhas emerged with two different approaches: post-hoc image registration and\ncombined acquisition on PET-CT, PET-MRI and other hybrid scanners. There are\nintrinsic limitations for both the post-hoc image analysis and dual/triple\nmodality approaches defined by registration errors and physical constraints in\nthe acquisition chain. We envision that tomography will evolve beyond current\nmodality fusion and towards grand fusion, a large scale fusion of all or many\nimaging modalities, which may be referred to as omni-tomography or\nmulti-tomography. Unlike modality fusion, grand fusion is here proposed for\ntruly simultaneous but often localized reconstruction in terms of all or many\nrelevant imaging mechanisms such as CT, MRI, PET, SPECT, US, optical, and\npossibly more. In this paper, the technical basis for omni-tomography is\nintroduced and illustrated with a top-level design of a next generation\nscanner, interior tomographic reconstructions of representative modalities, and\nanticipated applications of omni-tomography.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2011 17:19:15 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["Wang", "Ge", ""], ["Zhang", "Jie", ""], ["Gao", "Hao", ""], ["Weir", "Victor", ""], ["Yu", "Hengyong", ""], ["Cong", "Wenxiang", ""], ["Xu", "Xiaochen", ""], ["Shen", "Haiou", ""], ["Bennett", "James", ""], ["Wang", "Yue", ""], ["Vannier", "Michael", ""]]}, {"id": "1106.2233", "submitter": "Xiaowen Dong", "authors": "Xiaowen Dong, Pascal Frossard, Pierre Vandergheynst and Nikolai\n  Nefedov", "title": "Clustering with Multi-Layer Graphs: A Spectral Perspective", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 60, no. 11, pp.\n  5820-5831, November 2012", "doi": "10.1109/TSP.2012.2212886", "report-no": null, "categories": "cs.LG cs.CV cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational data usually comes with a multimodal nature, which means that\nit can be naturally represented by a multi-layer graph whose layers share the\nsame set of vertices (users) with different edges (pairwise relationships). In\nthis paper, we address the problem of combining different layers of the\nmulti-layer graph for improved clustering of the vertices compared to using\nlayers independently. We propose two novel methods, which are based on joint\nmatrix factorization and graph regularization framework respectively, to\nefficiently combine the spectrum of the multiple graph layers, namely the\neigenvectors of the graph Laplacian matrices. In each case, the resulting\ncombination, which we call a \"joint spectrum\" of multiple graphs, is used for\nclustering the vertices. We evaluate our approaches by simulations with several\nreal world social network datasets. Results demonstrate the superior or\ncompetitive performance of the proposed methods over state-of-the-art technique\nand common baseline methods, such as co-regularization and summation of\ninformation from individual graphs.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2011 12:43:18 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Dong", "Xiaowen", ""], ["Frossard", "Pascal", ""], ["Vandergheynst", "Pierre", ""], ["Nefedov", "Nikolai", ""]]}, {"id": "1106.2357", "submitter": "Nicolaie Popescu-Bodorin", "authors": "Nicolaie Popescu-Bodorin, Valentina E. Balas", "title": "Comparing Haar-Hilbert and Log-Gabor Based Iris Encoders on Bath Iris\n  Image Database", "comments": "6 pages, 4 figures, latest version: http://fmi.spiruharet.ro/bodorin/", "journal-ref": "Proc. 4th International Workshop on Soft Computing Applications,\n  pp. 191-196, IEEE Press, July 2010", "doi": "10.1109/SOFA.2010.5565599", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This papers introduces a new family of iris encoders which use 2-dimensional\nHaar Wavelet Transform for noise attenuation, and Hilbert Transform to encode\nthe iris texture. In order to prove the usefulness of the newly proposed iris\nencoding approach, the recognition results obtained by using these new encoders\nare compared to those obtained using the classical Log- Gabor iris encoder.\nTwelve tests involving single/multienrollment and conducted on Bath Iris Image\nDatabase are presented here. One of these tests achieves an Equal Error Rate\ncomparable to the lowest value reported so far for this database. New Matlab\ntools for iris image processing are also released together with this paper: a\nsecond version of the Circular Fuzzy Iris Segmentator (CFIS2), a fast Log-Gabor\nencoder and two Haar-Hilbert based encoders.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2011 23:14:05 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Popescu-Bodorin", "Nicolaie", ""], ["Balas", "Valentina E.", ""]]}, {"id": "1106.2695", "submitter": "Duc Phu Chau", "authors": "Duc Phu Chau (INRIA Sophia Antipolis), Fran\\c{c}ois Bremond (INRIA\n  Sophia Antipolis), Monique Thonnat (INRIA Sophia Antipolis), Etienne Corvee\n  (INRIA Sophia Antipolis)", "title": "Robust Mobile Object Tracking Based on Multiple Feature Similarity and\n  Trajectory Filtering", "comments": null, "journal-ref": "The International Conference on Computer Vision Theory and\n  Applications (VISAPP) (2011)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm to track mobile objects in different\nscene conditions. The main idea of the proposed tracker includes estimation,\nmulti-features similarity measures and trajectory filtering. A feature set\n(distance, area, shape ratio, color histogram) is defined for each tracked\nobject to search for the best matching object. Its best matching object and its\nstate estimated by the Kalman filter are combined to update position and size\nof the tracked object. However, the mobile object trajectories are usually\nfragmented because of occlusions and misdetections. Therefore, we also propose\na trajectory filtering, named global tracker, aims at removing the noisy\ntrajectories and fusing the fragmented trajectories belonging to a same mobile\nobject. The method has been tested with five videos of different scene\nconditions. Three of them are provided by the ETISEO benchmarking project\n(http://www-sop.inria.fr/orion/ETISEO) in which the proposed tracker\nperformance has been compared with other seven tracking algorithms. The\nadvantages of our approach over the existing state of the art ones are: (i) no\nprior knowledge information is required (e.g. no calibration and no contextual\nmodels are needed), (ii) the tracker is more reliable by combining multiple\nfeature similarities, (iii) the tracker can perform in different scene\nconditions: single/several mobile objects, weak/strong illumination,\nindoor/outdoor scenes, (iv) a trajectory filtering is defined and applied to\nimprove the tracker performance, (v) the tracker performance outperforms many\nalgorithms of the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 12:45:05 GMT"}], "update_date": "2011-06-15", "authors_parsed": [["Chau", "Duc Phu", "", "INRIA Sophia Antipolis"], ["Bremond", "Fran\u00e7ois", "", "INRIA\n  Sophia Antipolis"], ["Thonnat", "Monique", "", "INRIA Sophia Antipolis"], ["Corvee", "Etienne", "", "INRIA Sophia Antipolis"]]}, {"id": "1106.2696", "submitter": "Peter Schaffer Dr.", "authors": "Peter Schaffer, Djamila Aouada, Shishir Nagaraja", "title": "Who clicks there!: Anonymizing the photographer in a camera saturated\n  society", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, social media has played an increasingly important role in\nreporting world events. The publication of crowd-sourced photographs and videos\nin near real-time is one of the reasons behind the high impact. However, the\nuse of a camera can draw the photographer into a situation of conflict.\nExamples include the use of cameras by regulators collecting evidence of Mafia\noperations; citizens collecting evidence of corruption at a public service\noutlet; and political dissidents protesting at public rallies. In all these\ncases, the published images contain fairly unambiguous clues about the location\nof the photographer (scene viewpoint information). In the presence of adversary\noperated cameras, it can be easy to identify the photographer by also combining\nleaked information from the photographs themselves. We call this the camera\nlocation detection attack. We propose and review defense techniques against\nsuch attacks. Defenses such as image obfuscation techniques do not protect\ncamera-location information; current anonymous publication technologies do not\nhelp either. However, the use of view synthesis algorithms could be a promising\nstep in the direction of providing probabilistic privacy guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 12:49:48 GMT"}], "update_date": "2011-07-21", "authors_parsed": [["Schaffer", "Peter", ""], ["Aouada", "Djamila", ""], ["Nagaraja", "Shishir", ""]]}, {"id": "1106.2729", "submitter": "Svebor Karaman", "authors": "Svebor Karaman (LaBRI), Jenny Benois-Pineau (LaBRI), R\\'emi M\\'egret\n  (IMS)", "title": "Nested Graph Words for Object Recognition", "comments": "Preliminary version of accepted paper \"Multi-Layer Local Graph Words\n  for Object Recognition\". Leaving only final version to avoid confusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new, scalable approach for the task of object\nbased image search or object recognition. Despite the very large literature\nexisting on the scalability issues in CBIR in the sense of retrieval\napproaches, the scalability of media and scalability of features remain an\nissue. In our work we tackle the problem of scalability and structural\norganization of features. The proposed features are nested local graphs built\nupon sets of SURF feature points with Delaunay triangulation. A\nBag-of-Visual-Words (BoVW) framework is applied on these graphs, giving birth\nto a Bag-of-Graph-Words representation. The nested nature of the descriptors\nconsists in scaling from trivial Delaunay graphs - isolated feature points - by\nincreasing the number of nodes layer by layer up to graphs with maximal number\nof nodes. For each layer of graphs its proper visual dictionary is built. The\nexperiments conducted on the SIVAL data set reveal that the graph features at\ndifferent layers exhibit complementary performances on the same content. The\nnested approach, the combination of all existing layers, yields significant\nimprovement of the object recognition performance compared to single level\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 14:43:02 GMT"}, {"version": "v2", "created": "Wed, 23 Apr 2014 08:34:24 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Karaman", "Svebor", "", "LaBRI"], ["Benois-Pineau", "Jenny", "", "LaBRI"], ["M\u00e9gret", "R\u00e9mi", "", "IMS"]]}, {"id": "1106.3464", "submitter": "Debotosh Bhattacharjee", "authors": "Mrinal Kanti Bhowmik, Debotosh Bhattacharjee, Dipak Kumar Basu, and\n  Mita Nasipuri", "title": "Polar Fusion Technique Analysis for Evaluating the Performances of Image\n  Fusion of Thermal and Visual Images for Human Face Recognition", "comments": "Proceedings of IEEE Workshop on Computational Intelligence in\n  Biometrics and Identity Management (IEEE CIBIM 2011), Paris, France, April 11\n  - 15, 2011", "journal-ref": null, "doi": "10.1109/CIBIM.2011.5949220", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents a comparative study of two different methods, which are\nbased on fusion and polar transformation of visual and thermal images. Here,\ninvestigation is done to handle the challenges of face recognition, which\ninclude pose variations, changes in facial expression, partial occlusions,\nvariations in illumination, rotation through different angles, change in scale\netc. To overcome these obstacles we have implemented and thoroughly examined\ntwo different fusion techniques through rigorous experimentation. In the first\nmethod log-polar transformation is applied to the fused images obtained after\nfusion of visual and thermal images whereas in second method fusion is applied\non log-polar transformed individual visual and thermal images. After this step,\nwhich is thus obtained in one form or another, Principal Component Analysis\n(PCA) is applied to reduce dimension of the fused images. Log-polar transformed\nimages are capable of handling complicacies introduced by scaling and rotation.\nThe main objective of employing fusion is to produce a fused image that\nprovides more detailed and reliable information, which is capable to overcome\nthe drawbacks present in the individual visual and thermal face images.\nFinally, those reduced fused images are classified using a multilayer\nperceptron neural network. The database used for the experiments conducted here\nis Object Tracking and Classification Beyond Visible Spectrum (OTCBVS) database\nbenchmark thermal and visual face images. The second method has shown better\nperformance, which is 95.71% (maximum) and on an average 93.81% as correct\nrecognition rate.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 12:25:30 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Bhowmik", "Mrinal Kanti", ""], ["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1106.3466", "submitter": "Debotosh Bhattacharjee", "authors": "Mrinal Kanti Bhowmik, Gautam Majumdar, Debotosh Bhattacharjee, Dipak\n  Kumar Basu, and Mita Nasipuri", "title": "Next Level of Data Fusion for Human Face Recognition", "comments": "Keywords: Thermal Image, Visual Image, Fused Image, Data Fusion,\n  Wavelet decomposition, Decision Fusion, Classification", "journal-ref": "Proceedings of International Conference on Mathematical Modeling\n  and Applications to Industrial Problems (MMIP' 11), National Institute of\n  Technology, Calicut, India, March 28-31, 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper demonstrates two different fusion techniques at two different\nlevels of a human face recognition process. The first one is called data fusion\nat lower level and the second one is the decision fusion towards the end of the\nrecognition process. At first a data fusion is applied on visual and\ncorresponding thermal images to generate fused image. Data fusion is\nimplemented in the wavelet domain after decomposing the images through\nDaubechies wavelet coefficients (db2). During the data fusion maximum of\napproximate and other three details coefficients are merged together. After\nthat Principle Component Analysis (PCA) is applied over the fused coefficients\nand finally two different artificial neural networks namely Multilayer\nPerceptron(MLP) and Radial Basis Function(RBF) networks have been used\nseparately to classify the images. After that, for decision fusion based\ndecisions from both the classifiers are combined together using Bayesian\nformulation. For experiments, IRIS thermal/visible Face Database has been used.\nExperimental results show that the performance of multiple classifier system\nalong with decision fusion works well over the single classifier system.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 12:31:30 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Bhowmik", "Mrinal Kanti", ""], ["Majumdar", "Gautam", ""], ["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1106.3467", "submitter": "Debotosh Bhattacharjee", "authors": "Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri,\n  Mahantapas Kundu", "title": "High Performance Human Face Recognition using Independent High Intensity\n  Gabor Wavelet Responses: A Statistical Approach", "comments": "Keywords: Feature extraction; Gabor Wavelets; independent\n  high-intensity feature (IHIF); Independent Component Analysis (ICA);\n  Specificity; Sensitivity; Cosine Similarity Measure; E-ISSN: 2044-6004", "journal-ref": "International Journal of Computer Science & Emerging Technologies\n  pp 178-187, Volume 2, Issue 1, February 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we present a technique by which high-intensity feature vectors\nextracted from the Gabor wavelet transformation of frontal face images, is\ncombined together with Independent Component Analysis (ICA) for enhanced face\nrecognition. Firstly, the high-intensity feature vectors are automatically\nextracted using the local characteristics of each individual face from the\nGabor transformed images. Then ICA is applied on these locally extracted\nhigh-intensity feature vectors of the facial images to obtain the independent\nhigh intensity feature (IHIF) vectors. These IHIF forms the basis of the work.\nFinally, the image classification is done using these IHIF vectors, which are\nconsidered as representatives of the images. The importance behind implementing\nICA along with the high-intensity features of Gabor wavelet transformation is\ntwofold. On the one hand, selecting peaks of the Gabor transformed face images\nexhibit strong characteristics of spatial locality, scale, and orientation\nselectivity. Thus these images produce salient local features that are most\nsuitable for face recognition. On the other hand, as the ICA employs locally\nsalient features from the high informative facial parts, it reduces redundancy\nand represents independent features explicitly. These independent features are\nmost useful for subsequent facial discrimination and associative recall. The\nefficiency of IHIF method is demonstrated by the experiment on frontal facial\nimages dataset, selected from the FERET, FRAV2D, and the ORL database.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 12:42:26 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Kar", "Arindam", ""], ["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""], ["Nasipuri", "Mita", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1106.3517", "submitter": "Shashikumar D R", "authors": "Shashi Kumar D. R., K. B. Raja, R. K. Chhootaray, Sabyasachi Pattanaik", "title": "DWT Based Fingerprint Recognition using Non Minutiae Features", "comments": "9 pages", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 2, March 2011, 257-265", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic applications like criminal investigations, terrorist identification\nand National security issues require a strong fingerprint data base and\nefficient identification system. In this paper we propose DWT based Fingerprint\nRecognition using Non Minutiae (DWTFR) algorithm. Fingerprint image is\ndecomposed into multi resolution sub bands of LL, LH, HL and HH by applying 3\nlevel DWT. The Dominant local orientation angle {\\theta} and Coherence are\ncomputed on LL band only. The Centre Area Features and Edge Parameters are\ndetermined on each DWT level by considering all four sub bands. The comparison\nof test fingerprint with database fingerprint is decided based on the Euclidean\nDistance of all the features. It is observed that the values of FAR, FRR and\nTSR are improved compared to the existing algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 15:52:56 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["R.", "Shashi Kumar D.", ""], ["Raja", "K. B.", ""], ["Chhootaray", "R. K.", ""], ["Pattanaik", "Sabyasachi", ""]]}, {"id": "1106.3684", "submitter": "Nicolaie Popescu-Bodorin", "authors": "Nicolaie Popescu-Bodorin, Valentina E. Balas", "title": "Exploratory simulation of an Intelligent Iris Verifier Distributed\n  System", "comments": "4 pages, 2 figures, latest version: http://fmi.spiruharet.ro/bodorin/", "journal-ref": "Proc. 6th IEEE International Symposium on Applied Computational\n  Intelligence and Informatics, pp. 259 - 262, IEEE Press, June 2011", "doi": "10.1109/SACI.2011.5873010", "report-no": null, "categories": "cs.CV cs.ET cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses some topics related to the latest trends in the field of\nevolutionary approaches to iris recognition. It presents the results of an\nexploratory experimental simulation whose goal was to analyze the possibility\nof establishing an Interchange Protocol for Digital Identities evolved in\ndifferent geographic locations interconnected through and into an Intelligent\nIris Verifier Distributed System (IIVDS) based on multi-enrollment. Finding a\nlogically consistent model for the Interchange Protocol is the key factor in\ndesigning the future large-scale iris biometric networks. Therefore, the\nlogical model of such a protocol is also investigated here. All tests are made\non Bath Iris Database and prove that outstanding power of discrimination\nbetween the intra- and the inter-class comparisons can be achieved by an IIVDS,\neven when practicing 52.759.182 inter-class and 10.991.943 intra-class\ncomparisons. Still, the test results confirm that inconsistent enrollment can\nchange the logic of recognition from a fuzzified 2-valent consistent logic of\nbiometric certitudes to a fuzzified 3-valent inconsistent possibilistic logic\nof biometric beliefs justified through experimentally determined probabilities,\nor to a fuzzified 8-valent logic which is almost consistent as a biometric\ntheory - this quality being counterbalanced by an absolutely reasonable loss in\nthe user comfort level.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2011 20:16:10 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Popescu-Bodorin", "Nicolaie", ""], ["Balas", "Valentina E.", ""]]}, {"id": "1106.4632", "submitter": "Heran Yang", "authors": "Heran Yang, Tiffany Low, Matthew Cong, Ashutosh Saxena", "title": "Inferring 3D Articulated Models for Box Packaging Robot", "comments": "For: RSS 2011 Workshop on Mobile Manipulation: Learning to Manipulate", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a point cloud, we consider inferring kinematic models of 3D articulated\nobjects such as boxes for the purpose of manipulating them. While previous work\nhas shown how to extract a planar kinematic model (often represented as a\nlinear chain), such planar models do not apply to 3D objects that are composed\nof segments often linked to the other segments in cyclic configurations. We\npresent an approach for building a model that captures the relation between the\ninput point cloud features and the object segment as well as the relation\nbetween the neighboring object segments. We use a conditional random field that\nallows us to model the dependencies between different segments of the object.\nWe test our approach on inferring the kinematic structure from partial and\nnoisy point cloud data for a wide variety of boxes including cake boxes, pizza\nboxes, and cardboard cartons of several sizes. The inferred structure enables\nour robot to successfully close these boxes by manipulating the flaps.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 05:32:39 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Yang", "Heran", ""], ["Low", "Tiffany", ""], ["Cong", "Matthew", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1106.4907", "submitter": "H.R.  Chennamma", "authors": "H. R. Chennamma and Lalitha Rangarajan and Veerabhadrappa", "title": "Face Identification from Manipulated Facial Images using SIFT", "comments": "4 pages, 4 figures, IEEE 3rd International Conference on Emerging\n  Trends in Engineering & Technology (ICETET'2010), Nov 19-21, 2010, Goa, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Editing on digital images is ubiquitous. Identification of deliberately\nmodified facial images is a new challenge for face identification system. In\nthis paper, we address the problem of identification of a face or person from\nheavily altered facial images. In this face identification problem, the input\nto the system is a manipulated or transformed face image and the system reports\nback the determined identity from a database of known individuals. Such a\nsystem can be useful in mugshot identification in which mugshot database\ncontains two views (frontal and profile) of each criminal. We considered only\nfrontal view from the available database for face identification and the query\nimage is a manipulated face generated by face transformation software tool\navailable online. We propose SIFT features for efficient face identification in\nthis scenario. Further comparative analysis has been given with well known\neigenface approach. Experiments have been conducted with real case images to\nevaluate the performance of both methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 08:30:15 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Chennamma", "H. R.", ""], ["Rangarajan", "Lalitha", ""], ["Veerabhadrappa", "", ""]]}, {"id": "1106.5156", "submitter": "Mallikarjun Hangarge Dr.", "authors": "B.V.Dhandra, Mallikarjun Hangarge", "title": "Morphological Reconstruction for Word Level Script Identification", "comments": "11 Pages, 8 Figures,5 Tables; Revised: 15-06-2007,Published:\n  30-06-2007", "journal-ref": "International Journal of Computer Science and Security\n  (IJCSS),Volume (1) : Issue (1), 41 - 51, 2007", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A line of a bilingual document page may contain text words in regional\nlanguage and numerals in English. For Optical Character Recognition (OCR) of\nsuch a document page, it is necessary to identify different script forms before\nrunning an individual OCR system. In this paper, we have identified a tool of\nmorphological opening by reconstruction of an image in different directions and\nregional descriptors for script identification at word level, based on the\nobservation that every text has a distinct visual appearance. The proposed\nsystem is developed for three Indian major bilingual documents, Kannada, Telugu\nand Devnagari containing English numerals. The nearest neighbour and k-nearest\nneighbour algorithms are applied to classify new word images. The proposed\nalgorithm is tested on 2625 words with various font styles and sizes. The\nresults obtained are quite encouraging\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2011 18:16:59 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2011 13:35:35 GMT"}], "update_date": "2011-07-05", "authors_parsed": [["Dhandra", "B. V.", ""], ["Hangarge", "Mallikarjun", ""]]}, {"id": "1106.5186", "submitter": "Ula\\c{s} Ba\\u{g}ci", "authors": "Ulas Bagci, Jianhua Yao, Jesus Caban, Anthony F. Suffredini, Tara N.\n  Palmore, Daniel J. Mollura", "title": "Learning Shape and Texture Characteristics of CT Tree-in-Bud Opacities\n  for CAD Systems", "comments": "7 pages, 4 figures. Published in Proc. of Medical Image Computing and\n  Computer Assisted Interventions (MICCAI), 2011", "journal-ref": null, "doi": null, "report-no": "NIH-CIDI-MICCAI2011", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Although radiologists can employ CAD systems to characterize malignancies,\npulmonary fibrosis and other chronic diseases; the design of imaging techniques\nto quantify infectious diseases continue to lag behind. There exists a need to\ncreate more CAD systems capable of detecting and quantifying characteristic\npatterns often seen in respiratory tract infections such as influenza,\nbacterial pneumonia, or tuborculosis. One of such patterns is Tree-in-bud (TIB)\nwhich presents \\textit{thickened} bronchial structures surrounding by clusters\nof \\textit{micro-nodules}. Automatic detection of TIB patterns is a challenging\ntask because of their weak boundary, noisy appearance, and small lesion size.\nIn this paper, we present two novel methods for automatically detecting TIB\npatterns: (1) a fast localization of candidate patterns using information from\nlocal scale of the images, and (2) a M\\\"{o}bius invariant feature extraction\nmethod based on learned local shape and texture properties. A comparative\nevaluation of the proposed methods is presented with a dataset of 39 laboratory\nconfirmed viral bronchiolitis human parainfluenza (HPIV) CTs and 21 normal lung\nCTs. Experimental results demonstrate that the proposed CAD system can achieve\nhigh detection rate with an overall accuracy of 90.96%.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 03:35:08 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Bagci", "Ulas", ""], ["Yao", "Jianhua", ""], ["Caban", "Jesus", ""], ["Suffredini", "Anthony F.", ""], ["Palmore", "Tara N.", ""], ["Mollura", "Daniel J.", ""]]}, {"id": "1106.5341", "submitter": "Daniel Ly", "authors": "Daniel L. Ly and Ashutosh Saxena and Hod Lipson", "title": "Pose Estimation from a Single Depth Image for Arbitrary Kinematic\n  Skeletons", "comments": "2 pages, 2 figures, RGB-D workshop in Robotics: Science and Systems\n  (RSS 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating pose information from a single depth image\ngiven an arbitrary kinematic structure without prior training. For an arbitrary\nskeleton and depth image, an evolutionary algorithm is used to find the optimal\nkinematic configuration to explain the observed image. Results show that our\napproach can correctly estimate poses of 39 and 78 degree-of-freedom models\nfrom a single depth image, even in cases of significant self-occlusion.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2011 09:47:28 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Ly", "Daniel L.", ""], ["Saxena", "Ashutosh", ""], ["Lipson", "Hod", ""]]}, {"id": "1106.5460", "submitter": "Jeremiah Wala", "authors": "Jeremiah Wala, Sergei Fotin, Jaesung Lee, Artit Jirapatnakul, Alberto\n  Biancardi and Anthony Reeves", "title": "Automated segmentation of the pulmonary arteries in low-dose CT by\n  vessel tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automated method for top-down segmentation of the\npulmonary arterial tree in low-dose thoracic CT images. The main basal\npulmonary arteries are identified near the lung hilum by searching for\ncandidate vessels adjacent to known airways, identified by our previously\nreported airway segmentation method. Model cylinders are iteratively fit to the\nvessels to track them into the lungs. Vessel bifurcations are detected by\nmeasuring the rate of change of vessel radii, and child vessels are segmented\nby initiating new trackers at bifurcation points. Validation is accomplished\nusing our novel sparse surface (SS) evaluation metric. The SS metric was\ndesigned to quantify the magnitude of the segmentation error per vessel while\nsignificantly decreasing the manual marking burden for the human user. A total\nof 210 arteries and 205 veins were manually marked across seven test cases.\n134/210 arteries were correctly segmented, with a specificity for arteries of\n90%, and average segmentation error of 0.15 mm. This fully-automated\nsegmentation is a promising method for improving lung nodule detection in\nlow-dose CT screening scans, by separating vessels from surrounding\niso-intensity objects.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2011 17:47:23 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Wala", "Jeremiah", ""], ["Fotin", "Sergei", ""], ["Lee", "Jaesung", ""], ["Jirapatnakul", "Artit", ""], ["Biancardi", "Alberto", ""], ["Reeves", "Anthony", ""]]}, {"id": "1106.5569", "submitter": "David Prochazka", "authors": "David Prochazka and Tomas Koubek", "title": "Augmented Reality Implementation Methods in Mainstream Applications", "comments": null, "journal-ref": "Acta Universitatis agriculturae et silviculturae Mendelianae\n  Brunensis, Vol. LIX, No. 4, 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality has became an useful tool in many areas from space\nexploration to military applications. Although used theoretical principles are\nwell known for almost a decade, the augmented reality is almost exclusively\nused in high budget solutions with a special hardware. However, in last few\nyears we could see rising popularity of many projects focused on deployment of\nthe augmented reality on different mobile devices. Our article is aimed on\ndevelopers who consider development of an augmented reality application for the\nmainstream market. Such developers will be forced to keep the application\nprice, therefore also the development price, at reasonable level. Usage of\nexisting image processing software library could bring a significant cut-down\nof the development costs. In the theoretical part of the article is presented\nan overview of the augmented reality application structure. Further, an\napproach for selection appropriate library as well as the review of the\nexisting software libraries focused in this area is described. The last part of\nthe article outlines our implementation of key parts of the augmented reality\napplication using the OpenCV library.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 05:57:37 GMT"}], "update_date": "2011-06-29", "authors_parsed": [["Prochazka", "David", ""], ["Koubek", "Tomas", ""]]}, {"id": "1106.5571", "submitter": "David Prochazka", "authors": "David Prochazka, Michael Stencl, Ondrej Popelka and Jiri Stastny", "title": "Mobile Augmented Reality Applications", "comments": null, "journal-ref": "Proceedings of Mendel 2011: 17th International Conference on Soft\n  Computing, pp. 469-476, ISBN 978-80-214-4302-0", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality have undergone considerable improvement in past years. Many\nspecial techniques and hardware devices were developed, but the crucial\nbreakthrough came with the spread of intelligent mobile phones. This enabled\nmass spread of augmented reality applications. However mobile devices have\nlimited hardware capabilities, which narrows down the methods usable for scene\nanalysis. In this article we propose an augmented reality application which is\nusing cloud computing to enable using of more complex computational methods\nsuch as neural networks. Our goal is to create an affordable augmented reality\napplication suitable which will help car designers in by 'virtualizing' car\nmodifications.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 06:08:38 GMT"}], "update_date": "2011-06-29", "authors_parsed": [["Prochazka", "David", ""], ["Stencl", "Michael", ""], ["Popelka", "Ondrej", ""], ["Stastny", "Jiri", ""]]}, {"id": "1106.5737", "submitter": "Jenny Blight", "authors": "D.Bennet and Dr. S. Arumuga Perumal", "title": "Fingerprint: DWT, SVD Based Enhancement and Significant Contrast for\n  Ridges and Valleys Using Fuzzy Measures", "comments": "Submitted to Journal of Computer Science and Engineering, see\n  http://sites.google.com/site/jcseuk/volume-6-issue-1-march", "journal-ref": "Journal of Computer Science and Engineering, Volume 6, Issue 1,\n  p28-32, March 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of the Fingerprint recognition system will be more accurate\nwith respect of enhancement for the fingerprint images. In this paper we\ndevelop a novel method for Fingerprint image contrast enhancement technique\nbased on the discrete wavelet transform (DWT) and singular value decomposition\n(SVD) has been proposed. This technique is compared with conventional image\nequalization techniques such as standard general histogram equalization and\nlocal histogram equalization. An automatic histogram threshold approach based\non a fuzziness measure is presented. Then, using an index of fuzziness, a\nsimilarity process is started to find the threshold point. A significant\ncontrast between ridges and valleys of the best, medium and poor finger image\nfeatures to extract from finger images and get maximum recognition rate using\nfuzzy measures. The experimental results show the recognition of superiority of\nthe proposed method to get maximum performance up gradation to the\nimplementation of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 16:25:22 GMT"}], "update_date": "2011-06-29", "authors_parsed": [["Bennet", "D.", ""], ["Perumal", "Dr. S. Arumuga", ""]]}, {"id": "1106.5793", "submitter": "Dandan Hu", "authors": "Dandan Hu, Peter Ronhovde, Zohar Nussinov", "title": "A Replica Inference Approach to Unsupervised Multi-Scale Image\n  Segmentation", "comments": "26 pages, 22 figures", "journal-ref": "Phys. Rev. E 85, 016101 (2012)", "doi": "10.1103/PhysRevE.85.016101", "report-no": null, "categories": "cond-mat.stat-mech cs.CV physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a replica inference based Potts model method to unsupervised image\nsegmentation on multiple scales. This approach was inspired by the statistical\nmechanics problem of \"community detection\" and its phase diagram. Specifically,\nthe problem is cast as identifying tightly bound clusters (\"communities\" or\n\"solutes\") against a background or \"solvent\". Within our multiresolution\napproach, we compute information theory based correlations among multiple\nsolutions (\"replicas\") of the same graph over a range of resolutions.\nSignificant multiresolution structures are identified by replica correlations\nas manifest in information theory overlaps. With the aid of these correlations\nas well as thermodynamic measures, the phase diagram of the corresponding Potts\nmodel is analyzed both at zero and finite temperatures. Optimal parameters\ncorresponding to a sensible unsupervised segmentation correspond to the \"easy\nphase\" of the Potts model. Our algorithm is fast and shown to be at least as\naccurate as the best algorithms to date and to be especially suited to the\ndetection of camouflaged images.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 20:01:40 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Hu", "Dandan", ""], ["Ronhovde", "Peter", ""], ["Nussinov", "Zohar", ""]]}, {"id": "1106.5829", "submitter": "Geoffrey Hollinger", "authors": "Geoffrey A. Hollinger, Urbashi Mitra, Gaurav S. Sukhatme", "title": "Active Classification: Theory and Application to Underwater Inspection", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the problem in which an autonomous vehicle must classify an object\nbased on multiple views. We focus on the active classification setting, where\nthe vehicle controls which views to select to best perform the classification.\nThe problem is formulated as an extension to Bayesian active learning, and we\nshow connections to recent theoretical guarantees in this area. We formally\nanalyze the benefit of acting adaptively as new information becomes available.\nThe analysis leads to a probabilistic algorithm for determining the best views\nto observe based on information theoretic costs. We validate our approach in\ntwo ways, both related to underwater inspection: 3D polyhedra recognition in\nsynthetic depth maps and ship hull inspection with imaging sonar. These tasks\nencompass both the planning and recognition aspects of the active\nclassification problem. The results demonstrate that actively planning for\ninformative views can reduce the number of necessary views by up to 80% when\ncompared to passive methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 01:39:29 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Hollinger", "Geoffrey A.", ""], ["Mitra", "Urbashi", ""], ["Sukhatme", "Gaurav S.", ""]]}, {"id": "1106.5928", "submitter": "Gabriel Cristobal", "authors": "Salvador Gabarda and Gabriel Cristobal", "title": "Image denoising assessment using anisotropic stack filtering", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a measure of anisotropy as a quality parameter to\nestimate the amount of noise in noisy images. The anisotropy of an image can be\ndetermined through a directional measure, using an appropriate statistical\ndistribution of the information contained in the image. This new measure is\nachieved through a stack filtering paradigm. First, we define a local\ndirectional entropy, based on the distribution of 0's and 1's in the\nneigborhood of every pixel location of each stack level. Then the entropy\nvariation of this directional entropy is used to define an anisotropic measure.\nThe empirical results have shown that this measure can be regarded as an\nexcellent image noise indicator, which is particularly relevant for quality\nassessment of denoising algorithms. The method has been evaluated with\nartificial and real-world degraded images.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 13:12:56 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Gabarda", "Salvador", ""], ["Cristobal", "Gabriel", ""]]}, {"id": "1106.6242", "submitter": "Sandeep Katta", "authors": "Sandeep Katta", "title": "Visual Secret Sharing Scheme using Grayscale Images", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel expansion and the quality of the reconstructed secret image has been a\nmajor issue of visual secret sharing (VSS) schemes. A number of probabilistic\nVSS schemes with minimum pixel expansion have been proposed for black and white\n(binary) secret images. This paper presents a probabilistic (2, 3)-VSS scheme\nfor gray scale images. Its pixel expansion is larger in size but the quality of\nthe image is perfect when it's reconstructed. The construction of the shadow\nimages (transparent shares) is based on the binary OR operation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 14:25:46 GMT"}], "update_date": "2011-07-01", "authors_parsed": [["Katta", "Sandeep", ""]]}, {"id": "1106.6341", "submitter": "Oleg Kupervasser", "authors": "Ronen Lerner, Oleg Kupervasser and Ehud Rivlin", "title": "Vision-Based Navigation III: Pose and Motion from Omnidirectional\n  Optical Flow and a Digital Terrain Map", "comments": "6 pages, 9 figures", "journal-ref": "Proceedings of the 2006 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems October 9 - 15, 2006, Beijing, China", "doi": "10.1109/IROS.2006.282569", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for pose and motion estimation using corresponding features in\nomnidirectional images and a digital terrain map is proposed. In previous\npaper, such algorithm for regular camera was considered. Using a Digital\nTerrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables\nrecovering the absolute position and orientation of the camera. In order to do\nthis, the DTM is used to formulate a constraint between corresponding features\nin two consecutive frames. In this paper, these constraints are extended to\nhandle non-central projection, as is the case with many omnidirectional\nsystems. The utilization of omnidirectional data is shown to improve the\nrobustness and accuracy of the navigation algorithm. The feasibility of this\nalgorithm is established through lab experimentation with two kinds of\nomnidirectional acquisition systems. The first one is polydioptric cameras\nwhile the second is catadioptric camera.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 18:59:53 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2011 09:22:33 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Lerner", "Ronen", ""], ["Kupervasser", "Oleg", ""], ["Rivlin", "Ehud", ""]]}]