[{"id": "1806.00047", "submitter": "Valts Blukis", "authors": "Valts Blukis and Nataly Brukhim and Andrew Bennett and Ross A. Knepper\n  and Yoav Artzi", "title": "Following High-level Navigation Instructions on a Simulated Quadcopter\n  with Imitation Learning", "comments": "To appear in Robotics: Science and Systems (RSS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for following high-level navigation instructions by\nmapping directly from images, instructions and pose estimates to continuous\nlow-level velocity commands for real-time control. The Grounded Semantic\nMapping Network (GSMN) is a fully-differentiable neural network architecture\nthat builds an explicit semantic map in the world reference frame by\nincorporating a pinhole camera projection model within the network. The\ninformation stored in the map is learned from experience, while the\nlocal-to-world transformation is computed explicitly. We train the model using\nDAggerFM, a modified variant of DAgger that trades tabular convergence\nguarantees for improved training speed and memory use. We test GSMN in virtual\nenvironments on a realistic quadcopter simulator and show that incorporating an\nexplicit mapping and grounding modules allows GSMN to outperform strong neural\nbaselines and almost reach an expert policy performance. Finally, we analyze\nthe learned map representations and show that using an explicit map leads to an\ninterpretable instruction-following model.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 18:42:26 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Blukis", "Valts", ""], ["Brukhim", "Nataly", ""], ["Bennett", "Andrew", ""], ["Knepper", "Ross A.", ""], ["Artzi", "Yoav", ""]]}, {"id": "1806.00088", "submitter": "Jan Svoboda", "authors": "Jan Svoboda, Jonathan Masci, Federico Monti, Michael M. Bronstein,\n  Leonidas Guibas", "title": "PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning systems have become ubiquitous in many aspects of our lives.\nUnfortunately, it has been shown that such systems are vulnerable to\nadversarial attacks, making them prone to potential unlawful uses. Designing\ndeep neural networks that are robust to adversarial attacks is a fundamental\nstep in making such systems safer and deployable in a broader variety of\napplications (e.g. autonomous driving), but more importantly is a necessary\nstep to design novel and more advanced architectures built on new computational\nparadigms rather than marginally building on the existing ones. In this paper\nwe introduce PeerNets, a novel family of convolutional networks alternating\nclassical Euclidean convolutions with graph convolutions to harness information\nfrom a graph of peer samples. This results in a form of non-local forward\npropagation in the model, where latent features are conditioned on the global\nstructure induced by the graph, that is up to 3 times more robust to a variety\nof white- and black-box adversarial attacks compared to conventional\narchitectures with almost no drop in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 20:33:21 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Svoboda", "Jan", ""], ["Masci", "Jonathan", ""], ["Monti", "Federico", ""], ["Bronstein", "Michael M.", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1806.00094", "submitter": "Ibrahim Alsolami", "authors": "Ibrahim Alsolami and Wolfgang Heidrich", "title": "Imaging with SPADs and DMDs: Seeing through Diffraction-Photons", "comments": null, "journal-ref": "in IEEE Transactions on Image Processing, vol. 29, pp. 1440-1449,\n  2020", "doi": "10.1109/TIP.2019.2941315", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of imaging in the presence of\ndiffraction-photons. Diffraction-photons arise from the low contrast ratio of\nDMDs ($\\sim\\,1000:1$), and very much degrade the quality of images captured by\nSPAD-based systems. Herein, a joint illumination-deconvolution scheme is\ndesigned to overcome diffraction-photons, enabling the acquisition of intensity\nand depth images. Additionally, a proof-of-concept experiment is conducted to\ndemonstrate the viability of the designed scheme. It is shown that by\nco-designing the illumination and deconvolution phases of imaging, one can\nsubstantially overcome diffraction-photons.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 20:44:24 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 12:13:28 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Alsolami", "Ibrahim", ""], ["Heidrich", "Wolfgang", ""]]}, {"id": "1806.00102", "submitter": "Min Xu", "authors": "Guannan Zhao, Bo Zhou, Kaiwen Wang, Rui Jiang, Min Xu", "title": "Respond-CAM: Analyzing Deep Models for 3D Imaging Data by Visualizations", "comments": null, "journal-ref": "Medical Image Computing & Computer Assisted Intervention (MICCAI)\n  2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network (CNN) has become a powerful tool for various\nbiomedical image analysis tasks, but there is a lack of visual explanation for\nthe machinery of CNNs. In this paper, we present a novel algorithm,\nRespond-weighted Class Activation Mapping (Respond-CAM), for making CNN-based\nmodels interpretable by visualizing input regions that are important for\npredictions, especially for biomedical 3D imaging data inputs. Our method uses\nthe gradients of any target concept (e.g. the score of target class) that flows\ninto a convolutional layer. The weighted feature maps are combined to produce a\nheatmap that highlights the important regions in the image for predicting the\ntarget concept. We prove a preferable sum-to-score property of the Respond-CAM\nand verify its significant improvement on 3D images from the current\nstate-of-the-art approach. Our tests on Cellular Electron Cryo-Tomography 3D\nimages show that Respond-CAM achieves superior performance on visualizing the\nCNNs with 3D biomedical images inputs, and is able to get reasonably good\nresults on visualizing the CNNs with natural image inputs. The Respond-CAM is\nan efficient and reliable approach for visualizing the CNN machinery, and is\napplicable to a wide variety of CNN model families and image analysis tasks.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 21:18:17 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 06:11:46 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Zhao", "Guannan", ""], ["Zhou", "Bo", ""], ["Wang", "Kaiwen", ""], ["Jiang", "Rui", ""], ["Xu", "Min", ""]]}, {"id": "1806.00104", "submitter": "Yasamin Jafarian", "authors": "Yuan Yao, Yasamin Jafarian, and Hyun Soo Park", "title": "MONET: Multiview Semi-supervised Keypoint Detection via Epipolar\n  Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents MONET -- an end-to-end semi-supervised learning framework\nfor a keypoint detector using multiview image streams. In particular, we\nconsider general subjects such as non-human species where attaining a large\nscale annotated dataset is challenging. While multiview geometry can be used to\nself-supervise the unlabeled data, integrating the geometry into learning a\nkeypoint detector is challenging due to representation mismatch. We address\nthis mismatch by formulating a new differentiable representation of the\nepipolar constraint called epipolar divergence---a generalized distance from\nthe epipolar lines to the corresponding keypoint distribution. Epipolar\ndivergence characterizes when two view keypoint distributions produce zero\nreprojection error. We design a twin network that minimizes the epipolar\ndivergence through stereo rectification that can significantly alleviate\ncomputational complexity and sampling aliasing in training. We demonstrate that\nour framework can localize customized keypoints of diverse species, e.g.,\nhumans, dogs, and monkeys.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 21:27:33 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 00:14:15 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yao", "Yuan", ""], ["Jafarian", "Yasamin", ""], ["Park", "Hyun Soo", ""]]}, {"id": "1806.00111", "submitter": "Jonathan Vacher", "authors": "Jonathan Vacher, Pascal Mamassian, Ruben Coen-Cagli", "title": "Probabilistic Model of Visual Segmentation", "comments": "13 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual segmentation is a key perceptual function that partitions visual space\nand allows for detection, recognition and discrimination of objects in complex\nenvironments. The processes underlying human segmentation of natural images are\nstill poorly understood. In part, this is because we lack segmentation models\nconsistent with experimental and theoretical knowledge in visual neuroscience.\nBiological sensory systems have been shown to approximate probabilistic\ninference to interpret their inputs. This requires a generative model that\ncaptures both the statistics of the sensory inputs and expectations about the\ncauses of those inputs. Following this hypothesis, we propose a probabilistic\ngenerative model of visual segmentation that combines knowledge about 1) the\nsensitivity of neurons in the visual cortex to statistical regularities in\nnatural images; and 2) the preference of humans to form contiguous partitions\nof visual space. We develop an efficient algorithm for training and inference\nbased on expectation-maximization and validate it on synthetic data.\nImportantly, with the appropriate choice of the prior, we derive an intuitive\nclosed--form update rule for assigning pixels to segments: at each iteration,\nthe pixel assignment probabilities to segments is the sum of the evidence (i.e.\nlocal pixel statistics) and prior (i.e. the assignments of neighboring pixels)\nweighted by their relative uncertainty. The model performs competitively on\nnatural images from the Berkeley Segmentation Dataset (BSD), and we illustrate\nhow the likelihood and prior components improve segmentation relative to\ntraditional mixture models. Furthermore, our model explains some variability\nacross human subjects as reflecting local uncertainty about the number of\nsegments. Our model thus provides a viable approach to probe human visual\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 21:48:43 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 16:09:11 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 18:54:39 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Vacher", "Jonathan", ""], ["Mamassian", "Pascal", ""], ["Coen-Cagli", "Ruben", ""]]}, {"id": "1806.00153", "submitter": "Jong Chul Ye", "authors": "Juyoung Lee, Yoseob Han, Jae-Kyun Ryu, Jang-Yeon Park and Jong Chul Ye", "title": "k-Space Deep Learning for Reference-free EPI Ghost Correction", "comments": "To appear in Magnetic Resonance in Medicine", "journal-ref": null, "doi": null, "report-no": "https://doi.org/10.1002/mrm.27896", "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nyquist ghost artifacts in EPI are originated from phase mismatch between the\neven and odd echoes. However, conventional correction methods using reference\nscans often produce erroneous results especially in high-field MRI due to the\nnon-linear and time-varying local magnetic field changes. Recently, it was\nshown that the problem of ghost correction can be reformulated as k-space\ninterpolation problem that can be solved using structured low-rank Hankel\nmatrix approaches. Another recent work showed that data driven Hankel matrix\ndecomposition can be reformulated to exhibit similar structures as deep\nconvolutional neural network. By synergistically combining these findings, we\npropose a k-space deep learning approach that immediately corrects the phase\nmismatch without a reference scan in both accelerated and non-accelerated EPI\nacquisitions. To take advantage of the even and odd-phase directional\nredundancy, the k-space data is divided into two channels configured with even\nand odd phase encodings. The redundancies between coils are also exploited by\nstacking the multi-coil k-space data into additional input channels. Then, our\nk-space ghost correction network is trained to learn the interpolation kernel\nto estimate the missing virtual k-space data. For the accelerated EPI data, the\nsame neural network is trained to directly estimate the interpolation kernels\nfor missing k-space data from both ghost and subsampling. Reconstruction\nresults using 3T and 7T in-vivo data showed that the proposed method\noutperformed the image quality compared to the existing methods, and the\ncomputing time is much faster.The proposed k-space deep learning for EPI ghost\ncorrection is highly robust and fast, and can be combined with acceleration, so\nthat it can be used as a promising correction tool for high-field MRI without\nchanging the current acquisition protocol.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 01:01:27 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 07:17:27 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 00:24:07 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Lee", "Juyoung", ""], ["Han", "Yoseob", ""], ["Ryu", "Jae-Kyun", ""], ["Park", "Jang-Yeon", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1806.00178", "submitter": "Ke Sun", "authors": "Ke Sun, Mingjie Li, Dong Liu and Jingdong Wang", "title": "IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural\n  Networks", "comments": "10 pages, 2 figures, accepted by BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in building lightweight and efficient\nconvolutional neural networks. Inspired by the success of two design patterns,\ncomposition of structured sparse kernels, e.g., interleaved group convolutions\n(IGC), and composition of low-rank kernels, e.g., bottle-neck modules, we study\nthe combination of such two design patterns, using the composition of\nstructured sparse low-rank kernels, to form a convolutional kernel. Rather than\nintroducing a complementary condition over channels, we introduce a loose\ncomplementary condition, which is formulated by imposing the complementary\ncondition over super-channels, to guide the design for generating a dense\nconvolutional kernel. The resulting network is called IGCV3. We empirically\ndemonstrate that the combination of low-rank and sparse kernels boosts the\nperformance and the superiority of our proposed approach to the\nstate-of-the-arts, IGCV2 and MobileNetV2 over image classification on CIFAR and\nImageNet and object detection on COCO.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 03:18:10 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 08:45:37 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Sun", "Ke", ""], ["Li", "Mingjie", ""], ["Liu", "Dong", ""], ["Wang", "Jingdong", ""]]}, {"id": "1806.00179", "submitter": "George Philipp", "authors": "George Philipp, Jaime G. Carbonell", "title": "The Nonlinearity Coefficient - Predicting Generalization in Deep Neural\n  Networks", "comments": "Previous name: The Nonlinearity Coefficient - Predicting Overfitting\n  in Deep Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a long time, designing neural architectures that exhibit high performance\nwas considered a dark art that required expert hand-tuning. One of the few\nwell-known guidelines for architecture design is the avoidance of exploding\ngradients, though even this guideline has remained relatively vague and\ncircumstantial. We introduce the nonlinearity coefficient (NLC), a measurement\nof the complexity of the function computed by a neural network that is based on\nthe magnitude of the gradient. Via an extensive empirical study, we show that\nthe NLC is a powerful predictor of test error and that attaining a right-sized\nNLC is essential for optimal performance.\n  The NLC exhibits a range of intriguing and important properties. It is\nclosely tied to the amount of information gained from computing a single\nnetwork gradient. It is tied to the error incurred when replacing the\nnonlinearity operations in the network with linear operations. It is not\nsusceptible to the confounders of multiplicative scaling, additive bias and\nlayer width. It is stable from layer to layer. Hence, we argue that the NLC is\nthe first robust predictor of overfitting in deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 03:58:14 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 02:21:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Philipp", "George", ""], ["Carbonell", "Jaime G.", ""]]}, {"id": "1806.00183", "submitter": "Zhang Qiang", "authors": "Qiangqiang Yuan, Qiang Zhang, Jie Li, Huanfeng Shen, Liangpei Zhang", "title": "Hyperspectral Image Denoising Employing a Spatial-Spectral Deep Residual\n  Convolutional Neural Network", "comments": "Accepted by IEEE TGRS, available codes:\n  https://github.com/WHUQZhang/HSID-CNN", "journal-ref": null, "doi": "10.1109/TGRS.2018.2865197", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) denoising is a crucial preprocessing procedure to\nimprove the performance of the subsequent HSI interpretation and applications.\nIn this paper, a novel deep learning-based method for this task is proposed, by\nlearning a non-linear end-to-end mapping between the noisy and clean HSIs with\na combined spatial-spectral deep convolutional neural network (HSID-CNN). Both\nthe spatial and spectral information are simultaneously assigned to the\nproposed network. In addition, multi-scale feature extraction and multi-level\nfeature representation are respectively employed to capture both the\nmulti-scale spatial-spectral feature and fuse the feature representations with\ndifferent levels for the final restoration. The simulated and real-data\nexperiments demonstrate that the proposed HSID-CNN outperforms many of the\nmainstream methods in both the quantitative evaluation indexes, visual effects,\nand HSI classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 04:24:09 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 01:54:08 GMT"}, {"version": "v3", "created": "Sat, 11 Aug 2018 00:06:48 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Yuan", "Qiangqiang", ""], ["Zhang", "Qiang", ""], ["Li", "Jie", ""], ["Shen", "Huanfeng", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1806.00186", "submitter": "Nayyer Aafaq Mr.", "authors": "Nayyer Aafaq, Ajmal Mian, Wei Liu, Syed Zulqarnain Gilani and Mubarak\n  Shah", "title": "Video Description: A Survey of Methods, Datasets and Evaluation Metrics", "comments": "Accepted by ACM Computing Surveys", "journal-ref": "ACM Computing Surveys (CSUR) 52(6), 115 (2019)", "doi": "10.1145/3355390", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video description is the automatic generation of natural language sentences\nthat describe the contents of a given video. It has applications in human-robot\ninteraction, helping the visually impaired and video subtitling. The past few\nyears have seen a surge of research in this area due to the unprecedented\nsuccess of deep learning in computer vision and natural language processing.\nNumerous methods, datasets and evaluation metrics have been proposed in the\nliterature, calling the need for a comprehensive survey to focus research\nefforts in this flourishing new direction. This paper fills the gap by\nsurveying the state of the art approaches with a focus on deep learning models;\ncomparing benchmark datasets in terms of their domains, number of classes, and\nrepository size; and identifying the pros and cons of various evaluation\nmetrics like SPICE, CIDEr, ROUGE, BLEU, METEOR, and WMD. Classical video\ndescription approaches combined subject, object and verb detection with\ntemplate based language models to generate sentences. However, the release of\nlarge datasets revealed that these methods can not cope with the diversity in\nunconstrained open domain videos. Classical approaches were followed by a very\nshort era of statistical methods which were soon replaced with deep learning,\nthe current state of the art in video description. Our survey shows that\ndespite the fast-paced developments, video description research is still in its\ninfancy due to the following reasons. Analysis of video description models is\nchallenging because it is difficult to ascertain the contributions, towards\naccuracy or errors, of the visual features and the adopted language model in\nthe final description. Existing datasets neither contain adequate visual\ndiversity nor complexity of linguistic structures. Finally, current evaluation\nmetrics ...\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 04:31:58 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 02:49:57 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 04:10:09 GMT"}, {"version": "v4", "created": "Tue, 3 Mar 2020 02:30:57 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Aafaq", "Nayyer", ""], ["Mian", "Ajmal", ""], ["Liu", "Wei", ""], ["Gilani", "Syed Zulqarnain", ""], ["Shah", "Mubarak", ""]]}, {"id": "1806.00194", "submitter": "Chen Huang", "authors": "Chen Huang, Yining Li, Chen Change Loy, Xiaoou Tang", "title": "Deep Imbalanced Learning for Face Recognition and Attribute Prediction", "comments": "14 pages, 10 figures, 8 tables. Accepted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data for face analysis often exhibit highly-skewed class distribution, i.e.,\nmost data belong to a few majority classes, while the minority classes only\ncontain a scarce amount of instances. To mitigate this issue, contemporary deep\nlearning methods typically follow classic strategies such as class re-sampling\nor cost-sensitive training. In this paper, we conduct extensive and systematic\nexperiments to validate the effectiveness of these classic schemes for\nrepresentation learning on class-imbalanced data. We further demonstrate that\nmore discriminative deep representation can be learned by enforcing a deep\nnetwork to maintain inter-cluster margins both within and between classes. This\ntight constraint effectively reduces the class imbalance inherent in the local\ndata neighborhood, thus carving much more balanced class boundaries locally. We\nshow that it is easy to deploy angular margins between the cluster\ndistributions on a hypersphere manifold. Such learned Cluster-based Large\nMargin Local Embedding (CLMLE), when combined with a simple k-nearest cluster\nalgorithm, shows significant improvements in accuracy over existing methods on\nboth face recognition and face attribute prediction tasks that exhibit\nimbalanced class distribution.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 04:55:47 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 03:49:42 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Huang", "Chen", ""], ["Li", "Yining", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1806.00236", "submitter": "Hyunjung Shim Dr.", "authors": "Junsuk Choe, Joo Hyun Park, Hyunjung Shim", "title": "Generative Adversarial Networks for Unsupervised Object Co-localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach for unsupervised object\nco-localization using Generative Adversarial Networks (GANs). GAN is a powerful\ntool that can implicitly learn unknown data distributions in an unsupervised\nmanner. From the observation that GAN discriminator is highly influenced by\npixels where objects appear, we analyze the internal layers of discriminator\nand visualize the activated pixels. Our important finding is that high image\ndiversity of GAN, which is a main goal in GAN research, is ironically\ndisadvantageous for object localization, because such discriminators focus not\nonly on the target object, but also on the various objects, such as background\nobjects. Based on extensive evaluations and experimental studies, we show the\nimage diversity and localization performance have a negative correlation. In\naddition, our approach achieves meaningful accuracy for unsupervised object\nco-localization using publicly available benchmark datasets, even comparable to\nstate-of-the-art weakly-supervised approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 08:33:30 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 08:51:34 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Choe", "Junsuk", ""], ["Park", "Joo Hyun", ""], ["Shim", "Hyunjung", ""]]}, {"id": "1806.00264", "submitter": "Ting-Ting Liang", "authors": "Ting-Ting Liang, Satoshi Tsutsui, Liangcai Gao, Jing-Jing Lu and\n  Mengyan Sun", "title": "Combining Pyramid Pooling and Attention Mechanism for Pelvic MR Image\n  Semantic Segmentaion", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the time-consuming routine work for a radiologist is to discern\nanatomical structures from tomographic images. For assisting radiologists, this\npaper develops an automatic segmentation method for pelvic magnetic resonance\n(MR) images. The task has three major challenges 1) A pelvic organ can have\nvarious sizes and shapes depending on the axial image, which requires local\ncontexts to segment correctly. 2) Different organs often have quite similar\nappearance in MR images, which requires global context to segment. 3) The\nnumber of available annotated images are very small to use the latest\nsegmentation algorithms. To address the challenges, we propose a novel\nconvolutional neural network called Attention-Pyramid network (APNet) that\neffectively exploits both local and global contexts, in addition to a\ndata-augmentation technique that is particularly effective for MR images. In\norder to evaluate our method, we construct fine-grained (50 pelvic organs) MR\nimage segmentation dataset, and experimentally confirm the superior performance\nof our techniques over the state-of-the-art image segmentation methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 10:13:45 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 16:57:39 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Liang", "Ting-Ting", ""], ["Tsutsui", "Satoshi", ""], ["Gao", "Liangcai", ""], ["Lu", "Jing-Jing", ""], ["Sun", "Mengyan", ""]]}, {"id": "1806.00265", "submitter": "Firat Ozdemir", "authors": "Firat Ozdemir, Philipp Fuernstahl, Orcun Goksel", "title": "Learn the new, keep the old: Extending pretrained models with new\n  anatomy and images", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00937-3_42", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been widely accepted as a promising solution for medical\nimage segmentation, given a sufficiently large representative dataset of images\nwith corresponding annotations. With ever increasing amounts of annotated\nmedical datasets, it is infeasible to train a learning method always with all\ndata from scratch. This is also doomed to hit computational limits, e.g.,\nmemory or runtime feasible for training. Incremental learning can be a\npotential solution, where new information (images or anatomy) is introduced\niteratively. Nevertheless, for the preservation of the collective information,\nit is essential to keep some \"important\" (i.e. representative) images and\nannotations from the past, while adding new information. In this paper, we\nintroduce a framework for applying incremental learning for segmentation and\npropose novel methods for selecting representative data therein. We\ncomparatively evaluate our methods in different scenarios using MR images and\nvalidate the increased learning capacity with using our methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 10:14:31 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ozdemir", "Firat", ""], ["Fuernstahl", "Philipp", ""], ["Goksel", "Orcun", ""]]}, {"id": "1806.00292", "submitter": "Andrija Stajduhar", "authors": "Andrija \\v{S}tajduhar, Domagoj D\\v{z}aja, Milo\\v{s} Juda\\v{s}, Sven\n  Lon\\v{c}ari\\'c", "title": "Automatic Detection of Neurons in NeuN-stained Histological Images of\n  Human Brain", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2018.12.027", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a novel use of an anisotropic diffusion model for\nautomatic detection of neurons in histological sections of the adult human\nbrain cortex. We use a partial differential equation model to process high\nresolution images to acquire locations of neuronal bodies. We also present a\nnovel approach in model training and evaluation that considers variability\namong the human experts, addressing the issue of existence and correctness of\nthe golden standard for neuron and cell counting, used in most of relevant\npapers. Our method, trained on dataset manually labeled by three experts, has\ncorrectly distinguished over 95% of neuron bodies in test data, doing so in\ntime much shorter than other comparable methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 11:38:37 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["\u0160tajduhar", "Andrija", ""], ["D\u017eaja", "Domagoj", ""], ["Juda\u0161", "Milo\u0161", ""], ["Lon\u010dari\u0107", "Sven", ""]]}, {"id": "1806.00360", "submitter": "Mejdi Ben Dkhil", "authors": "M. Ben Dkhil, A. Wali and Adel M. Alimi", "title": "Towards a new system for drowsiness detection based on eye blinking and\n  head posture estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver drowsiness problem is considered as one of the most important reasons\nthat increases road accidents number. We propose in this paper a new approach\nfor realtime driver drowsiness in order to prevent road accidents. The system\nuses a smart video camera that takes drivers faces images and supervises the\neye blink (open and close) state and head posture to detect the different\ndrowsiness states. Face and eye detection are done by Viola and Jones\ntechnique.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 13:24:10 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Dkhil", "M. Ben", ""], ["Wali", "A.", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1806.00363", "submitter": "Ben Glocker", "authors": "Vanya V. Valindria, Ioannis Lavdas, Wenjia Bai, Konstantinos\n  Kamnitsas, Eric O. Aboagye, Andrea G. Rockall, Daniel Rueckert, Ben Glocker", "title": "Domain Adaptation for MRI Organ Segmentation using Reverse\n  Classification Accuracy", "comments": "Accepted at the International Conference on Medical Imaging with Deep\n  Learning (MIDL) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variations in multi-center data in medical imaging studies have brought\nthe necessity of domain adaptation. Despite the advancement of machine learning\nin automatic segmentation, performance often degrades when algorithms are\napplied on new data acquired from different scanners or sequences than the\ntraining data. Manual annotation is costly and time consuming if it has to be\ncarried out for every new target domain. In this work, we investigate automatic\nselection of suitable subjects to be annotated for supervised domain adaptation\nusing the concept of reverse classification accuracy (RCA). RCA predicts the\nperformance of a trained model on data from the new domain and different\nstrategies of selecting subjects to be included in the adaptation via transfer\nlearning are evaluated. We perform experiments on a two-center MR database for\nthe task of organ segmentation. We show that subject selection via RCA can\nreduce the burden of annotation of new data for the target domain.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 14:09:33 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Valindria", "Vanya V.", ""], ["Lavdas", "Ioannis", ""], ["Bai", "Wenjia", ""], ["Kamnitsas", "Konstantinos", ""], ["Aboagye", "Eric O.", ""], ["Rockall", "Andrea G.", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""]]}, {"id": "1806.00365", "submitter": "Ce Qi", "authors": "Ce Qi, Zhizhong Liu, Fei Su", "title": "Accurate and Efficient Similarity Search for Large Scale Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face verification is a relatively easy task with the help of discriminative\nfeatures from deep neural networks. However, it is still a challenge to\nrecognize faces on millions of identities while keeping high performance and\nefficiency. The challenge 2 of MS-Celeb-1M is a classification task. However,\nthe number of identities is too large and it is not that elegant to treat the\ntask as an image classification task. We treat the classification task as\nsimilarity search and do experiments on different similarity search strategies.\nSimilarity search strategy accelerates the speed of searching and boosts the\naccuracy of final results. The model used for extracting features is a single\ndeep neural network pretrained on CASIA-Webface, which is not trained on the\nbase set or novel set offered by official. Finally, we rank \\textbf{3rd}, while\nthe speed of searching is 1ms/image.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 14:13:56 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Qi", "Ce", ""], ["Liu", "Zhizhong", ""], ["Su", "Fei", ""]]}, {"id": "1806.00398", "submitter": "Zhixian Ma", "authors": "Zhixian Ma, Jie Zhu, Weitian Li, Haiguang Xu", "title": "Radio Galaxy Morphology Generation Using DNN Autoencoder and Gaussian\n  Mixture Models", "comments": "Accepted by the 14th International Conference on Signal Processing\n  (ICSP2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The morphology of a radio galaxy is highly affected by its central active\ngalactic nuclei (AGN), which is studied to reveal the evolution of the super\nmassive black hole (SMBH). In this work, we propose a morphology generation\nframework for two typical radio galaxies namely Fanaroff-Riley type-I (FRI) and\ntype-II (FRII) with deep neural network based autoencoder (DNNAE) and Gaussian\nmixture models (GMMs). The encoder and decoder subnets in the DNNAE are\nsymmetric aside a fully-connected layer namely code layer hosting the extracted\nfeature vectors. By randomly generating the feature vectors later with a\nthree-component Gaussian Mixture models, new FRI or FRII radio galaxy\nmorphologies are simulated. Experiments were demonstrated on real radio galaxy\nimages, where we discussed the length of feature vectors, selection of lost\nfunctions, and made comparisons on batch normalization and dropout techniques\nfor training the network. The results suggest a high efficiency and performance\nof our morphology generation framework. Code is available at:\nhttps://github.com/myinxd/dnnae-gmm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 15:33:16 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Ma", "Zhixian", ""], ["Zhu", "Jie", ""], ["Li", "Weitian", ""], ["Xu", "Haiguang", ""]]}, {"id": "1806.00411", "submitter": "Alberto Gomez", "authors": "Alberto Gomez and Veronika A. Zimmer and Bishesh Khanal and Nicolas\n  Toussaint and Julia A. Schnabel", "title": "Adapted and Oversegmenting Graphs: Application to Geometric Deep\n  Learning", "comments": "Submited to CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel iterative method to adapt a a graph to d-dimensional image\ndata. The method drives the nodes of the graph towards image features. The\nadaptation process naturally lends itself to a measure of feature saliency\nwhich can then be used to retain meaningful nodes and edges in the graph. From\nthe adapted graph, we also propose the computation of a dual graph, which\ninherits the saliency measure from the adapted graph, and whose edges run along\nimage features, hence producing an oversegmenting graph. The proposed method is\ncomputationally efficient and fully parallelisable. We propose two distance\nmeasures to find image saliency along graph edges, and evaluate the performance\non synthetic images and on natural images from publicly available databases. In\nboth cases, the most salient nodes of the graph achieve average boundary recall\nover 90%. We also apply our method to image classification on the MNIST\nhand-written digit dataset, using a recently proposed Deep Geometric Learning\narchitecture, and achieving state-of-the-art classification accuracy, for a\ngraph-based method, of 97.86%.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 15:56:50 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 17:43:17 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Gomez", "Alberto", ""], ["Zimmer", "Veronika A.", ""], ["Khanal", "Bishesh", ""], ["Toussaint", "Nicolas", ""], ["Schnabel", "Julia A.", ""]]}, {"id": "1806.00428", "submitter": "Aditya Vora", "authors": "Aditya Vora", "title": "A Classification approach towards Unsupervised Learning of Visual\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a technique for unsupervised learning of visual\nrepresentations. Specifically, we train a model for foreground and background\nclassification task, in the process of which it learns visual representations.\nForeground and background patches for training come af- ter mining for such\npatches from hundreds and thousands of unlabelled videos available on the web\nwhich we ex- tract using a proposed patch extraction algorithm. With- out using\nany supervision, with just using 150, 000 unla- belled videos and the PASCAL\nVOC 2007 dataset, we train a object recognition model that achieves 45.3 mAP\nwhich is close to the best performing unsupervised feature learn- ing technique\nwhereas better than many other proposed al- gorithms. The code for patch\nextraction is implemented in Matlab and available open source at the following\nlink .\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 16:35:08 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Vora", "Aditya", ""]]}, {"id": "1806.00466", "submitter": "Aneeq Zia", "authors": "Aneeq Zia, Andrew Hung, Irfan Essa, and Anthony Jarc", "title": "Surgical Activity Recognition in Robot-Assisted Radical Prostatectomy\n  using Deep Learning", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse surgical outcomes are costly to patients and hospitals. Approaches to\nbenchmark surgical care are often limited to gross measures across the entire\nprocedure despite the performance of particular tasks being largely responsible\nfor undesirable outcomes. In order to produce metrics from tasks as opposed to\nthe whole procedure, methods to recognize automatically individual surgical\ntasks are needed. In this paper, we propose several approaches to recognize\nsurgical activities in robot-assisted minimally invasive surgery using deep\nlearning. We collected a clinical dataset of 100 robot-assisted radical\nprostatectomies (RARP) with 12 tasks each and propose `RP-Net', a modified\nversion of InceptionV3 model, for image based surgical activity recognition. We\nachieve an average precision of 80.9% and average recall of 76.7% across all\ntasks using RP-Net which out-performs all other RNN and CNN based models\nexplored in this paper. Our results suggest that automatic surgical activity\nrecognition during RARP is feasible and can be the foundation for advanced\nanalytics.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 17:55:38 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Zia", "Aneeq", ""], ["Hung", "Andrew", ""], ["Essa", "Irfan", ""], ["Jarc", "Anthony", ""]]}, {"id": "1806.00523", "submitter": "Kashyap Chitta", "authors": "Kashyap Chitta", "title": "Targeted Kernel Networks: Faster Convolutions with Attentive\n  Regularization", "comments": "ECCV 2018 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Attentive Regularization (AR), a method to constrain the\nactivation maps of kernels in Convolutional Neural Networks (CNNs) to specific\nregions of interest (ROIs). Each kernel learns a location of specialization\nalong with its weights through standard backpropagation. A differentiable\nattention mechanism requiring no additional supervision is used to optimize the\nROIs. Traditional CNNs of different types and structures can be modified with\nthis idea into equivalent Targeted Kernel Networks (TKNs), while keeping the\nnetwork size nearly identical. By restricting kernel ROIs, we reduce the number\nof sliding convolutional operations performed throughout the network in its\nforward pass, speeding up both training and inference. We evaluate our proposed\narchitecture on both synthetic and natural tasks across multiple domains. TKNs\nobtain significant improvements over baselines, requiring less computation\n(around an order of magnitude) while achieving superior performance.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 19:46:16 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 20:27:47 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Chitta", "Kashyap", ""]]}, {"id": "1806.00525", "submitter": "Huda Alamri", "authors": "Huda Alamri, Vincent Cartillier, Raphael Gontijo Lopes, Abhishek Das,\n  Jue Wang, Irfan Essa, Dhruv Batra, Devi Parikh, Anoop Cherian, Tim K. Marks,\n  Chiori Hori", "title": "Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene-aware dialog systems will be able to have conversations with users\nabout the objects and events around them. Progress on such systems can be made\nby integrating state-of-the-art technologies from multiple research areas\nincluding end-to-end dialog systems visual dialog, and video description. We\nintroduce the Audio Visual Scene Aware Dialog (AVSD) challenge and dataset. In\nthis challenge, which is one track of the 7th Dialog System Technology\nChallenges (DSTC7) workshop1, the task is to build a system that generates\nresponses in a dialog about an input video\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 19:51:58 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Alamri", "Huda", ""], ["Cartillier", "Vincent", ""], ["Lopes", "Raphael Gontijo", ""], ["Das", "Abhishek", ""], ["Wang", "Jue", ""], ["Essa", "Irfan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Cherian", "Anoop", ""], ["Marks", "Tim K.", ""], ["Hori", "Chiori", ""]]}, {"id": "1806.00546", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Zhoubing Xu, Katherine Aboud, Prasanna Parvathaneni,\n  Shunxing Bao, Camilo Bermudez, Susan M. Resnick, Laurie E. Cutting, Bennett\n  A. Landman", "title": "Spatially Localized Atlas Network Tiles Enables 3D Whole Brain\n  Segmentation from Limited Data", "comments": "To appear in MICCAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole brain segmentation on a structural magnetic resonance imaging (MRI) is\nessential in non-invasive investigation for neuroanatomy. Historically,\nmulti-atlas segmentation (MAS) has been regarded as the de facto standard\nmethod for whole brain segmentation. Recently, deep neural network approaches\nhave been applied to whole brain segmentation by learning random patches or 2D\nslices. Yet, few previous efforts have been made on detailed whole brain\nsegmentation using 3D networks due to the following challenges: (1) fitting\nentire whole brain volume into 3D networks is restricted by the current GPU\nmemory, and (2) the large number of targeting labels (e.g., > 100 labels) with\nlimited number of training 3D volumes (e.g., < 50 scans). In this paper, we\npropose the spatially localized atlas network tiles (SLANT) method to\ndistribute multiple independent 3D fully convolutional networks to cover\noverlapped sub-spaces in a standard atlas space. This strategy simplifies the\nwhole brain learning task to localized sub-tasks, which was enabled by combing\ncanonical registration and label fusion techniques with deep learning. To\naddress the second challenge, auxiliary labels on 5111 initially unlabeled\nscans were created by MAS for pre-training. From empirical validation, the\nstate-of-the-art MAS method achieved mean Dice value of 0.76, 0.71, and 0.68,\nwhile the proposed method achieved 0.78, 0.73, and 0.71 on three validation\ncohorts. Moreover, the computational time reduced from > 30 hours using MAS to\n~15 minutes using the proposed method. The source code is available online\nhttps://github.com/MASILab/SLANTbrainSeg\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 21:39:47 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 04:49:05 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Huo", "Yuankai", ""], ["Xu", "Zhoubing", ""], ["Aboud", "Katherine", ""], ["Parvathaneni", "Prasanna", ""], ["Bao", "Shunxing", ""], ["Bermudez", "Camilo", ""], ["Resnick", "Susan M.", ""], ["Cutting", "Laurie E.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1806.00557", "submitter": "Shichao Yang", "authors": "Shichao Yang, Sebastian Scherer", "title": "CubeSLAM: Monocular 3D Object SLAM", "comments": "IEEE Transactions on Robotics", "journal-ref": null, "doi": "10.1109/TRO.2019.2909168", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for single image 3D cuboid object detection and\nmulti-view object SLAM in both static and dynamic environments, and demonstrate\nthat the two parts can improve each other. Firstly for single image object\ndetection, we generate high-quality cuboid proposals from 2D bounding boxes and\nvanishing points sampling. The proposals are further scored and selected based\non the alignment with image edges. Secondly, multi-view bundle adjustment with\nnew object measurements is proposed to jointly optimize poses of cameras,\nobjects and points. Objects can provide long-range geometric and scale\nconstraints to improve camera pose estimation and reduce monocular drift.\nInstead of treating dynamic regions as outliers, we utilize object\nrepresentation and motion model constraints to improve the camera pose\nestimation. The 3D detection experiments on SUN RGBD and KITTI show better\naccuracy and robustness over existing approaches. On the public TUM, KITTI\nodometry and our own collected datasets, our SLAM method achieves the\nstate-of-the-art monocular camera pose estimation and at the same time,\nimproves the 3D object detection accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 22:44:20 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 06:05:44 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Yang", "Shichao", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1806.00578", "submitter": "Cheng-Lin Liu", "authors": "Yi-Chao Wu, Fei Yin, Xu-Yao Zhang, Li Liu, Cheng-Lin Liu", "title": "SCAN: Sliding Convolutional Attention Network for Scene Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition has drawn great attentions in the community of\ncomputer vision and artificial intelligence due to its challenges and wide\napplications. State-of-the-art recurrent neural networks (RNN) based models map\nan input sequence to a variable length output sequence, but are usually applied\nin a black box manner and lack of transparency for further improvement, and the\nmaintaining of the entire past hidden states prevents parallel computation in a\nsequence. In this paper, we investigate the intrinsic characteristics of text\nrecognition, and inspired by human cognition mechanisms in reading texts, we\npropose a scene text recognition method with sliding convolutional attention\nnetwork (SCAN). Similar to the eye movement during reading, the process of SCAN\ncan be viewed as an alternation between saccades and visual fixations. Compared\nto the previous recurrent models, computations over all elements of SCAN can be\nfully parallelized during training. Experimental results on several challenging\nbenchmarks, including the IIIT5k, SVT and ICDAR 2003/2013 datasets, demonstrate\nthe superiority of SCAN over state-of-the-art methods in terms of both the\nmodel interpretability and performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 03:28:43 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Wu", "Yi-Chao", ""], ["Yin", "Fei", ""], ["Zhang", "Xu-Yao", ""], ["Liu", "Li", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1806.00580", "submitter": "Pinlong Zhao", "authors": "Pinlong Zhao, Zhouyu Fu, Ou wu, Qinghua Hu, and Jun Wang", "title": "Detecting Adversarial Examples via Key-based Network", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep neural networks have achieved state-of-the-art performance in\nvisual classification, recent studies have shown that they are all vulnerable\nto the attack of adversarial examples. Small and often imperceptible\nperturbations to the input images are sufficient to fool the most powerful deep\nneural networks. Various defense methods have been proposed to address this\nissue. However, they either require knowledge on the process of generating\nadversarial examples, or are not robust against new attacks specifically\ndesigned to penetrate the existing defense. In this work, we introduce\nkey-based network, a new detection-based defense mechanism to distinguish\nadversarial examples from normal ones based on error correcting output codes,\nusing the binary code vectors produced by multiple binary classifiers applied\nto randomly chosen label-sets as signatures to match normal images and reject\nadversarial examples. In contrast to existing defense methods, the proposed\nmethod does not require knowledge of the process for generating adversarial\nexamples and can be applied to defend against different types of attacks. For\nthe practical black-box and gray-box scenarios, where the attacker does not\nknow the encoding scheme, we show empirically that key-based network can\neffectively detect adversarial examples generated by several state-of-the-art\nattacks.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 04:13:02 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zhao", "Pinlong", ""], ["Fu", "Zhouyu", ""], ["wu", "Ou", ""], ["Hu", "Qinghua", ""], ["Wang", "Jun", ""]]}, {"id": "1806.00585", "submitter": "Chunhua Shen", "authors": "Yuanzhouhan Cao, Tianqi Zhao, Ke Xian, Chunhua Shen, Zhiguo Cao,\n  Shugong Xu", "title": "Monocular Depth Estimation with Augmented Ordinal Depth Relationships", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing algorithms for depth estimation from single monocular images\nneed large quantities of metric groundtruth depths for supervised learning. We\nshow that relative depth can be an informative cue for metric depth estimation\nand can be easily obtained from vast stereo videos. Acquiring metric depths\nfrom stereo videos is sometimes impracticable due to the absence of camera\nparameters. In this paper, we propose to improve the performance of metric\ndepth estimation with relative depths collected from stereo movie videos using\nexisting stereo matching algorithm. We introduce a new \"Relative Depth in\nStereo\" (RDIS) dataset densely labelled with relative depths. We first pretrain\na ResNet model on our RDIS dataset. Then we finetune the model on RGB-D\ndatasets with metric ground-truth depths. During our finetuning, we formulate\ndepth estimation as a classification task. This re-formulation scheme enables\nus to obtain the confidence of a depth prediction in the form of probability\ndistribution. With this confidence, we propose an information gain loss to make\nuse of the predictions that are close to ground-truth. We evaluate our approach\non both indoor and outdoor benchmark RGB-D datasets and achieve\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 05:52:32 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 02:29:02 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Cao", "Yuanzhouhan", ""], ["Zhao", "Tianqi", ""], ["Xian", "Ke", ""], ["Shen", "Chunhua", ""], ["Cao", "Zhiguo", ""], ["Xu", "Shugong", ""]]}, {"id": "1806.00593", "submitter": "Lin Yang", "authors": "Lin Yang, Yizhe Zhang, Zhuo Zhao, Hao Zheng, Peixian Liang, Michael T.\n  C. Ying, Anil T. Ahuja, Danny Z. Chen", "title": "BoxNet: Deep Learning Based Biomedical Image Segmentation Using Boxes\n  Only Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, deep learning (DL) methods have become powerful tools for\nbiomedical image segmentation. However, high annotation efforts and costs are\ncommonly needed to acquire sufficient biomedical training data for DL models.\nTo alleviate the burden of manual annotation, in this paper, we propose a new\nweakly supervised DL approach for biomedical image segmentation using boxes\nonly annotation. First, we develop a method to combine graph search (GS) and DL\nto generate fine object masks from box annotation, in which DL uses box\nannotation to compute a rough segmentation for GS and then GS is applied to\nlocate the optimal object boundaries. During the mask generation process, we\ncarefully utilize information from box annotation to filter out potential\nerrors, and then use the generated masks to train an accurate DL segmentation\nnetwork. Extensive experiments on gland segmentation in histology images, lymph\nnode segmentation in ultrasound images, and fungus segmentation in electron\nmicroscopy images show that our approach attains superior performance over the\nbest known state-of-the-art weakly supervised DL method and is able to achieve\n(1) nearly the same accuracy compared to fully supervised DL methods with far\nless annotation effort, (2) significantly better results with similar\nannotation time, and (3) robust performance in various applications.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 07:10:30 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Yang", "Lin", ""], ["Zhang", "Yizhe", ""], ["Zhao", "Zhuo", ""], ["Zheng", "Hao", ""], ["Liang", "Peixian", ""], ["Ying", "Michael T. C.", ""], ["Ahuja", "Anil T.", ""], ["Chen", "Danny Z.", ""]]}, {"id": "1806.00600", "submitter": "Cheng Chen", "authors": "Cheng Chen, Qi Dou, Hao Chen, and Pheng-Ann Heng", "title": "Semantic-Aware Generative Adversarial Nets for Unsupervised Domain\n  Adaptation in Chest X-ray Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the compelling achievements that deep neural networks (DNNs) have\nmade in medical image computing, these deep models often suffer from degraded\nperformance when being applied to new test datasets with domain shift. In this\npaper, we present a novel unsupervised domain adaptation approach for\nsegmentation tasks by designing semantic-aware generative adversarial networks\n(GANs). Specifically, we transform the test image into the appearance of source\ndomain, with the semantic structural information being well preserved, which is\nachieved by imposing a nested adversarial learning in semantic label space. In\nthis way, the segmentation DNN learned from the source domain is able to be\ndirectly generalized to the transformed test image, eliminating the need of\ntraining a new model for every new target dataset. Our domain adaptation\nprocedure is unsupervised, without using any target domain labels. The\nadversarial learning of our network is guided by a GAN loss for mapping data\ndistributions, a cycle-consistency loss for retaining pixel-level content, and\na semantic-aware loss for enhancing structural information. We validated our\nmethod on two different chest X-ray public datasets for left/right lung\nsegmentation. Experimental results show that the segmentation performance of\nour unsupervised approach is highly competitive with the upper bound of\nsupervised transfer learning.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 07:59:55 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 03:01:49 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Chen", "Cheng", ""], ["Dou", "Qi", ""], ["Chen", "Hao", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1806.00630", "submitter": "Daiki Kimura", "authors": "Daiki Kimura", "title": "DAQN: Deep Auto-encoder and Q-Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep reinforcement learning method usually requires a large number of\ntraining images and executing actions to obtain sufficient results. When it is\nextended a real-task in the real environment with an actual robot, the method\nwill be required more training images due to complexities or noises of the\ninput images, and executing a lot of actions on the real robot also becomes a\nserious problem. Therefore, we propose an extended deep reinforcement learning\nmethod that is applied a generative model to initialize the network for\nreducing the number of training trials. In this paper, we used a deep q-network\nmethod as the deep reinforcement learning method and a deep auto-encoder as the\ngenerative model. We conducted experiments on three different tasks: a\ncart-pole game, an atari game, and a real-game with an actual robot. The\nproposed method trained efficiently on all tasks than the previous method,\nespecially 2.5 times faster on a task with real environment images.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 13:09:28 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Kimura", "Daiki", ""]]}, {"id": "1806.00631", "submitter": "Zhenxing Zheng", "authors": "Gaoyun An, Wen Zhou, Yuxuan Wu, Zhenxing Zheng, Yongwen Liu", "title": "Squeeze-and-Excitation on Spatial and Temporal Deep Feature Space for\n  Action Recognition", "comments": "Need to be Revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and temporal features are two key and complementary information for\nhuman action recognition. In order to make full use of the intra-frame spatial\ncharacteristics and inter-frame temporal relationships, we propose the\nSqueeze-and-Excitation Long-term Recurrent Convolutional Networks (SE-LRCN) for\nhuman action recognition. The Squeeze and Excitation operations are used to\nimplement the feature recalibration. In SE-LRCN, Squeeze-and-Excitation\nResNet-34 (SE-ResNet-34) network is adopted to extract spatial features to\nenhance the dependencies and importance of feature channels of pixel\ngranularity. We also propose the Squeeze-and-Excitation Long Short-Term Memory\n(SE-LSTM) network to model the temporal relationship, and to enhance the\ndependencies and importance of feature channels of frame granularity. We\nevaluate the proposed model on two challenging benchmarks, HMDB51 and UCF101,\nand the proposed SE-LRCN achieves the competitive results with the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 13:09:50 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 02:14:33 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["An", "Gaoyun", ""], ["Zhou", "Wen", ""], ["Wu", "Yuxuan", ""], ["Zheng", "Zhenxing", ""], ["Liu", "Yongwen", ""]]}, {"id": "1806.00672", "submitter": "Lori Dalton", "authors": "Lori A. Dalton, Marco E. Benalc\\'azar, and Edward R. Dougherty", "title": "Optimal Clustering under Uncertainty", "comments": "19 pages, 5 eps figures, 1 table", "journal-ref": null, "doi": "10.1371/journal.pone.0204627", "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical clustering algorithms typically either lack an underlying\nprobability framework to make them predictive or focus on parameter estimation\nrather than defining and minimizing a notion of error. Recent work addresses\nthese issues by developing a probabilistic framework based on the theory of\nrandom labeled point processes and characterizing a Bayes clusterer that\nminimizes the number of misclustered points. The Bayes clusterer is analogous\nto the Bayes classifier. Whereas determining a Bayes classifier requires full\nknowledge of the feature-label distribution, deriving a Bayes clusterer\nrequires full knowledge of the point process. When uncertain of the point\nprocess, one would like to find a robust clusterer that is optimal over the\nuncertainty, just as one may find optimal robust classifiers with uncertain\nfeature-label distributions. Herein, we derive an optimal robust clusterer by\nfirst finding an effective random point process that incorporates all\nrandomness within its own probabilistic structure and from which a Bayes\nclusterer can be derived that provides an optimal robust clusterer relative to\nthe uncertainty. This is analogous to the use of effective class-conditional\ndistributions in robust classification. After evaluating the performance of\nrobust clusterers in synthetic mixtures of Gaussians models, we apply the\nframework to granular imaging, where we make use of the asymptotic\ngranulometric moment theory for granular images to relate robust clustering\ntheory to the application.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 17:07:22 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Dalton", "Lori A.", ""], ["Benalc\u00e1zar", "Marco E.", ""], ["Dougherty", "Edward R.", ""]]}, {"id": "1806.00681", "submitter": "Yunzhe Tao", "authors": "Yunzhe Tao, Qi Sun, Qiang Du, and Wei Liu", "title": "Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling", "comments": "Accepted by NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlocal neural networks have been proposed and shown to be effective in\nseveral computer vision tasks, where the nonlocal operations can directly\ncapture long-range dependencies in the feature space. In this paper, we study\nthe nature of diffusion and damping effect of nonlocal networks by doing\nspectrum analysis on the weight matrices of the well-trained networks, and then\npropose a new formulation of the nonlocal block. The new block not only learns\nthe nonlocal interactions but also has stable dynamics, thus allowing deeper\nnonlocal structures. Moreover, we interpret our formulation from the general\nnonlocal modeling perspective, where we make connections between the proposed\nnonlocal network and other nonlocal models, such as nonlocal diffusion process\nand Markov jump process.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 18:23:48 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 22:00:29 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 17:21:36 GMT"}, {"version": "v4", "created": "Fri, 25 Jan 2019 03:08:46 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Tao", "Yunzhe", ""], ["Sun", "Qi", ""], ["Du", "Qiang", ""], ["Liu", "Wei", ""]]}, {"id": "1806.00685", "submitter": "Yunzhe Tao", "authors": "Yunzhe Tao, Lin Ma, Weizhong Zhang, Jian Liu, Wei Liu, Qiang Du", "title": "Hierarchical Attention-Based Recurrent Highway Networks for Time Series\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series prediction has been studied in a variety of domains. However, it\nis still challenging to predict future series given historical observations and\npast exogenous data. Existing methods either fail to consider the interactions\namong different components of exogenous variables which may affect the\nprediction accuracy, or cannot model the correlations between exogenous data\nand target data. Besides, the inherent temporal dynamics of exogenous data are\nalso related to the target series prediction, and thus should be considered as\nwell. To address these issues, we propose an end-to-end deep learning model,\ni.e., Hierarchical attention-based Recurrent Highway Network (HRHN), which\nincorporates spatio-temporal feature extraction of exogenous variables and\ntemporal dynamics modeling of target variables into a single framework.\nMoreover, by introducing the hierarchical attention mechanism, HRHN can\nadaptively select the relevant exogenous features in different semantic levels.\nWe carry out comprehensive empirical evaluations with various methods over\nseveral datasets, and show that HRHN outperforms the state of the arts in time\nseries prediction, especially in capturing sudden changes and sudden\noscillations of time series.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 18:46:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Tao", "Yunzhe", ""], ["Ma", "Lin", ""], ["Zhang", "Weizhong", ""], ["Liu", "Jian", ""], ["Liu", "Wei", ""], ["Du", "Qiang", ""]]}, {"id": "1806.00712", "submitter": "Shiwen Shen", "authors": "Shiwen Shen, Simon X. Han, Denise R. Aberle, Alex A.T. Bui, Willliam\n  Hsu", "title": "An Interpretable Deep Hierarchical Semantic Convolutional Neural Network\n  for Lung Nodule Malignancy Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning methods are increasingly being applied to tasks such as\ncomputer-aided diagnosis, these models are difficult to interpret, do not\nincorporate prior domain knowledge, and are often considered as a \"black-box.\"\nThe lack of model interpretability hinders them from being fully understood by\ntarget users such as radiologists. In this paper, we present a novel\ninterpretable deep hierarchical semantic convolutional neural network (HSCNN)\nto predict whether a given pulmonary nodule observed on a computed tomography\n(CT) scan is malignant. Our network provides two levels of output: 1) low-level\nradiologist semantic features, and 2) a high-level malignancy prediction score.\nThe low-level semantic outputs quantify the diagnostic features used by\nradiologists and serve to explain how the model interprets the images in an\nexpert-driven manner. The information from these low-level tasks, along with\nthe representations learned by the convolutional layers, are then combined and\nused to infer the high-level task of predicting nodule malignancy. This unified\narchitecture is trained by optimizing a global loss function including both\nlow- and high-level tasks, thereby learning all the parameters within a joint\nframework. Our experimental results using the Lung Image Database Consortium\n(LIDC) show that the proposed method not only produces interpretable lung\ncancer predictions but also achieves significantly better results compared to\ncommon 3D CNN approaches.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 22:41:28 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Shen", "Shiwen", ""], ["Han", "Simon X.", ""], ["Aberle", "Denise R.", ""], ["Bui", "Alex A. T.", ""], ["Hsu", "Willliam", ""]]}, {"id": "1806.00728", "submitter": "Nisar Ahmed", "authors": "Nisar Ahmed", "title": "Data-Free/Data-Sparse Softmax Parameter Estimation with Structured Class\n  Geometries", "comments": "Final version accepted to IEEE Signal Processing Letters (double\n  column), submitted July 21, 2018", "journal-ref": null, "doi": "10.1109/LSP.2018.2860238", "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note considers softmax parameter estimation when little/no labeled\ntraining data is available, but a priori information about the relative\ngeometry of class label log-odds boundaries is available. It is shown that\n`data-free' softmax model synthesis corresponds to solving a linear system of\nparameter equations, wherein desired dominant class log-odds boundaries are\nencoded via convex polytopes that decompose the input feature space. When\nsolvable, the linear equations yield closed-form softmax parameter solution\nfamilies using class boundary polytope specifications only. This allows softmax\nparameter learning to be implemented without expensive brute force data\nsampling and numerical optimization. The linear equations can also be adapted\nto constrained maximum likelihood estimation in data-sparse settings. Since\nsolutions may also fail to exist for the linear parameter equations derived\nfrom certain polytope specifications, it is thus also shown that there exist\nprobabilistic classification problems over m convexly separable classes for\nwhich the log-odds boundaries cannot be learned using an m-class softmax model.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 02:03:32 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 03:02:03 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Ahmed", "Nisar", ""]]}, {"id": "1806.00737", "submitter": "Mengyi Liu", "authors": "Mengyi Liu, Xiaohui Xie, Hanning Zhou", "title": "Content-based Video Relevance Prediction Challenge: Data, Protocol, and\n  Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video relevance prediction is one of the most important tasks for online\nstreaming service. Given the relevance of videos and viewer feedbacks, the\nsystem can provide personalized recommendations, which will help the user\ndiscover more content of interest. In most online service, the computation of\nvideo relevance table is based on users' implicit feedback, e.g. watch and\nsearch history. However, this kind of method performs poorly for \"cold-start\"\nproblems - when a new video is added to the library, the recommendation system\nneeds to bootstrap the video relevance score with very little user behavior\nknown. One promising approach to solve it is analyzing video content itself,\ni.e. predicting video relevance by video frame, audio, subtitle and metadata.\nIn this paper, we describe a challenge on Content-based Video Relevance\nPrediction (CBVRP) that is hosted by Hulu in the ACM Multimedia Conference\n2018. In this challenge, Hulu drives the study on an open problem of exploiting\ncontent characteristics directly from original video for video relevance\nprediction. We provide massive video assets and ground truth relevance derived\nfrom our really system, to build up a common platform for algorithm development\nand performance evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 04:51:53 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Liu", "Mengyi", ""], ["Xie", "Xiaohui", ""], ["Zhou", "Hanning", ""]]}, {"id": "1806.00738", "submitter": "Diana Gonz\\'alez-Rico", "authors": "Diana Gonzalez-Rico, Gibran Fuentes-Pineda", "title": "Contextualize, Show and Tell: A Neural Visual Storyteller", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural model for generating short stories from image sequences,\nwhich extends the image description model by Vinyals et al. (Vinyals et al.,\n2015). This extension relies on an encoder LSTM to compute a context vector of\neach story from the image sequence. This context vector is used as the first\nstate of multiple independent decoder LSTMs, each of which generates the\nportion of the story corresponding to each image in the sequence by taking the\nimage embedding as the first input. Our model showed competitive results with\nthe METEOR metric and human ratings in the internal track of the Visual\nStorytelling Challenge 2018.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 05:09:54 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Gonzalez-Rico", "Diana", ""], ["Fuentes-Pineda", "Gibran", ""]]}, {"id": "1806.00746", "submitter": "Amarjot Singh", "authors": "Amarjot Singh, Devendra Patil, and SN Omkar", "title": "Eye in the Sky: Real-time Drone Surveillance System (DSS) for Violent\n  Individuals Identification using ScatterNet Hybrid Deep Learning Network", "comments": "To Appear in the Efficient Deep Learning for Computer Vision (ECV)\n  workshop at IEEE Computer Vision and Pattern Recognition (CVPR) 2018. Youtube\n  demo at this: https://www.youtube.com/watch?v=zYypJPJipYc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drone systems have been deployed by various law enforcement agencies to\nmonitor hostiles, spy on foreign drug cartels, conduct border control\noperations, etc. This paper introduces a real-time drone surveillance system to\nidentify violent individuals in public areas. The system first uses the Feature\nPyramid Network to detect humans from aerial images. The image region with the\nhuman is used by the proposed ScatterNet Hybrid Deep Learning (SHDL) network\nfor human pose estimation. The orientations between the limbs of the estimated\npose are next used to identify the violent individuals. The proposed deep\nnetwork can learn meaningful representations quickly using ScatterNet and\nstructural priors with relatively fewer labeled examples. The system detects\nthe violent individuals in real-time by processing the drone images in the\ncloud. This research also introduces the aerial violent individual dataset used\nfor training the deep network which hopefully may encourage researchers\ninterested in using deep learning for aerial surveillance. The pose estimation\nand violent individuals identification performance is compared with the\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 07:44:11 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Singh", "Amarjot", ""], ["Patil", "Devendra", ""], ["Omkar", "SN", ""]]}, {"id": "1806.00771", "submitter": "Yan Niu", "authors": "Yan Niu, Jihong Ouyang, Wanli Zuo, Fuxin Wang", "title": "Low Cost Edge Sensing for High Quality Demosaicking", "comments": "Corresponding E-mail: niuyan@jlu.edu.cn;ouyj@jlu.edu.cn", "journal-ref": null, "doi": "10.1109/TIP.2018.2883815", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital cameras that use Color Filter Arrays (CFA) entail a demosaicking\nprocedure to form full RGB images. As today's camera users generally require\nimages to be viewed instantly, demosaicking algorithms for real applications\nmust be fast. Moreover, the associated cost should be lower than the cost saved\nby using CFA. For this purpose, we revisit the classical Hamilton-Adams (HA)\nalgorithm, which outperforms many sophisticated techniques in both speed and\naccuracy. Inspired by HA's strength and weakness, we design a very low cost\nedge sensing scheme. Briefly, it guides demosaicking by a logistic functional\nof the difference between directional variations. We extensively compare our\nalgorithm with 28 demosaicking algorithms by running their open source codes on\nbenchmark datasets. Compared to methods of similar computational cost, our\nmethod achieves substantially higher accuracy, Whereas compared to methods of\nsimilar accuracy, our method has significantly lower cost. Moreover, on test\nimages of currently popular resolution, the quality of our algorithm is\ncomparable to top performers, whereas its speed is tens of times faster.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 11:17:15 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 10:45:56 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Niu", "Yan", ""], ["Ouyang", "Jihong", ""], ["Zuo", "Wanli", ""], ["Wang", "Fuxin", ""]]}, {"id": "1806.00800", "submitter": "Daniel Maurer", "authors": "Daniel Maurer, Andr\\'es Bruhn", "title": "ProFlow: Learning to Predict Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal coherence is a valuable source of information in the context of\noptical flow estimation. However, finding a suitable motion model to leverage\nthis information is a non-trivial task. In this paper we propose an\nunsupervised online learning approach based on a convolutional neural network\n(CNN) that estimates such a motion model individually for each frame. By\nrelating forward and backward motion these learned models not only allow to\ninfer valuable motion information based on the backward flow, they also help to\nimprove the performance at occlusions, where a reliable prediction is\nparticularly useful. Moreover, our learned models are spatially variant and\nhence allow to estimate non-rigid motion per construction. This, in turns,\nallows to overcome the major limitation of recent rigidity-based approaches\nthat seek to improve the estimation by incorporating additional stereo/SfM\nconstraints. Experiments demonstrate the usefulness of our new approach. They\nnot only show a consistent improvement of up to 27% for all major benchmarks\n(KITTI 2012, KITTI 2015, MPI Sintel) compared to a baseline without prediction,\nthey also show top results for the MPI Sintel benchmark -- the one of the three\nbenchmarks that contains the largest amount of non-rigid motion.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 14:38:17 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Maurer", "Daniel", ""], ["Bruhn", "Andr\u00e9s", ""]]}, {"id": "1806.00801", "submitter": "Gui-Song Xia", "authors": "Pu Jin, Gui-Song Xia, Fan Hu, Qikai Lu, Liangpei Zhang", "title": "AID++: An Updated Version of AID on Scene Classification", "comments": "IGARSS'18 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial image scene classification is a fundamental problem for understanding\nhigh-resolution remote sensing images and has become an active research task in\nthe field of remote sensing due to its important role in a wide range of\napplications. However, the limitations of existing datasets for scene\nclassification, such as the small scale and low-diversity, severely hamper the\npotential usage of the new generation deep convolutional neural networks\n(CNNs). Although huge efforts have been made in building large-scale datasets\nvery recently, e.g., the Aerial Image Dataset (AID) which contains 10,000 image\nsamples, they are still far from sufficient to fully train a high-capacity deep\nCNN model. To this end, we present a larger-scale dataset in this paper, named\nas AID++, for aerial scene classification based on the AID dataset. The\nproposed AID++ consists of more than 400,000 image samples that are\nsemi-automatically annotated by using the existing the geographical data. We\nevaluate several prevalent CNN models on the proposed dataset, and the results\nshow that our dataset can be used as a promising benchmark for scene\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 14:40:20 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Jin", "Pu", ""], ["Xia", "Gui-Song", ""], ["Hu", "Fan", ""], ["Lu", "Qikai", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1806.00804", "submitter": "Yedid Hoshen", "authors": "Yedid Hoshen and Lior Wolf", "title": "NAM: Non-Adversarial Unsupervised Domain Mapping", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods were recently proposed for the task of translating images\nbetween domains without prior knowledge in the form of correspondences. The\nexisting methods apply adversarial learning to ensure that the distribution of\nthe mapped source domain is indistinguishable from the target domain, which\nsuffers from known stability issues. In addition, most methods rely heavily on\n`cycle' relationships between the domains, which enforce a one-to-one mapping.\nIn this work, we introduce an alternative method: Non-Adversarial Mapping\n(NAM), which separates the task of target domain generative modeling from the\ncross-domain mapping task. NAM relies on a pre-trained generative model of the\ntarget domain, and aligns each source image with an image synthesized from the\ntarget domain, while jointly optimizing the domain mapping function. It has\nseveral key advantages: higher quality and resolution image translations,\nsimpler and more stable training and reusable target models. Extensive\nexperiments are presented validating the advantages of our method.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 14:53:20 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 08:59:26 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Hoshen", "Yedid", ""], ["Wolf", "Lior", ""]]}, {"id": "1806.00806", "submitter": "Jong Chul Ye", "authors": "Eunju Cha, Eung Yeop Kim, and Jong Chul Ye", "title": "k-Space Deep Learning for Parallel MRI: Application to Time-Resolved MR\n  Angiography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-resolved angiography with interleaved stochastic trajectories (TWIST)\nhas been widely used for dynamic contrast enhanced MRI (DCE-MRI). To achieve\nhighly accelerated acquisitions, TWIST combines the periphery of the k-space\ndata from several adjacent frames to reconstruct one temporal frame. However,\nthis view-sharing scheme limits the true temporal resolution of TWIST.\nMoreover, the k-space sampling patterns have been specially designed for a\nspecific generalized autocalibrating partial parallel acquisition (GRAPPA)\nfactor so that it is not possible to reduce the number of view-sharing once the\nk-data is acquired. To address these issues, this paper proposes a novel\nk-space deep learning approach for parallel MRI. In particular, we have\ndesigned our neural network so that accurate k-space interpolations are\nperformed simultaneously for multiple coils by exploiting the redundancies\nalong the coils and images. Reconstruction results using in vivo TWIST data set\nconfirm that the proposed method can immediately generate high-quality\nreconstruction results with various choices of view- sharing, allowing us to\nexploit the trade-off between spatial and temporal resolution in time-resolved\nMR angiography.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 14:56:46 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 06:51:12 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Cha", "Eunju", ""], ["Kim", "Eung Yeop", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1806.00839", "submitter": "Vinicius Vianna", "authors": "Vinicius Pavanelli Vianna", "title": "Study and development of a Computer-Aided Diagnosis system for\n  classification of chest x-ray images using convolutional neural networks\n  pre-trained for ImageNet and data augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (ConvNets) are the actual standard for image\nrecognizement and classification. On the present work we develop a Computer\nAided-Diagnosis (CAD) system using ConvNets to classify a x-rays chest images\ndataset in two groups: Normal and Pneumonia. The study uses ConvNets models\navailable on the PyTorch platform: AlexNet, SqueezeNet, ResNet and Inception.\nWe initially use three training styles: complete from scratch using random\ninitialization, using a pre-trained ImageNet model training only the last layer\nadapted to our problem (transfer learning) and a pre-trained model modified\ntraining all the classifying layers of the model (fine tuning). The last\nstrategy of training used is with data augmentation techniques that avoid over\nfitting problems on ConvNets yielding the better results on this study\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 17:31:42 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Vianna", "Vinicius Pavanelli", ""]]}, {"id": "1806.00844", "submitter": "Alexey Shvets", "authors": "Vladimir I. Iglovikov, Selim Seferbekov, Alexander V. Buslaev and\n  Alexey Shvets", "title": "TernausNetV2: Fully Convolutional Network for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The most common approaches to instance segmentation are complex and use\ntwo-stage networks with object proposals, conditional random-fields, template\nmatching or recurrent neural networks. In this work we present TernausNetV2 - a\nsimple fully convolutional network that allows extracting objects from a\nhigh-resolution satellite imagery on an instance level. The network has popular\nencoder-decoder type of architecture with skip connections but has a few\nessential modifications that allows using for semantic as well as for instance\nsegmentation tasks. This approach is universal and allows to extend any network\nthat has been successfully applied for semantic segmentation to perform\ninstance segmentation task. In addition, we generalize network encoder that was\npre-trained for RGB images to use additional input channels. It makes possible\nto use transfer learning from visual to a wider spectral range. For\nDeepGlobe-CVPR 2018 building detection sub-challenge, based on public\nleaderboard score, our approach shows superior performance in comparison to\nother methods. The source code corresponding pre-trained weights are publicly\navailable at https://github.com/ternaus/TernausNetV2\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 17:55:13 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 19:13:47 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Iglovikov", "Vladimir I.", ""], ["Seferbekov", "Selim", ""], ["Buslaev", "Alexander V.", ""], ["Shvets", "Alexey", ""]]}, {"id": "1806.00857", "submitter": "Gabriel Grand", "authors": "Gabriel Grand, Aron Szanto, Yoon Kim, Alexander Rush", "title": "On the Flip Side: Identifying Counterexamples in Visual Question\n  Answering", "comments": "KDD 2018 conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) models respond to open-ended natural language\nquestions about images. While VQA is an increasingly popular area of research,\nit is unclear to what extent current VQA architectures learn key semantic\ndistinctions between visually-similar images. To investigate this question, we\nexplore a reformulation of the VQA task that challenges models to identify\ncounterexamples: images that result in a different answer to the original\nquestion. We introduce two methods for evaluating existing VQA models against a\nsupervised counterexample prediction task, VQA-CX. While our models surpass\nexisting benchmarks on VQA-CX, we find that the multimodal representations\nlearned by an existing state-of-the-art VQA model do not meaningfully\ncontribute to performance on this task. These results call into question the\nassumption that successful performance on the VQA benchmark is indicative of\ngeneral visual-semantic reasoning abilities.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 19:31:47 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 06:12:54 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 05:05:31 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Grand", "Gabriel", ""], ["Szanto", "Aron", ""], ["Kim", "Yoon", ""], ["Rush", "Alexander", ""]]}, {"id": "1806.00868", "submitter": "Amlaan Bhoi", "authors": "Somshubra Majumdar, Amlaan Bhoi, Ganesh Jagadeesan", "title": "A Comprehensive Comparison between Neural Style Transfer and Universal\n  Style Transfer", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer aims to transfer arbitrary visual styles to content images. We\nexplore algorithms adapted from two papers that try to solve the problem of\nstyle transfer while generalizing on unseen styles or compromised visual\nquality. Majority of the improvements made focus on optimizing the algorithm\nfor real-time style transfer while adapting to new styles with considerably\nless resources and constraints. We compare these strategies and compare how\nthey measure up to produce visually appealing images. We explore two approaches\nto style transfer: neural style transfer with improvements and universal style\ntransfer. We also make a comparison between the different images produced and\nhow they can be qualitatively measured.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 20:28:04 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Majumdar", "Somshubra", ""], ["Bhoi", "Amlaan", ""], ["Jagadeesan", "Ganesh", ""]]}, {"id": "1806.00874", "submitter": "Chieh-Chi Kao", "authors": "Chieh-Chi Kao, Yuxiang Wang, Jonathan Waltman, Pradeep Sen", "title": "Patch-Based Image Hallucination for Super Resolution with Detail\n  Reconstruction from Similar Sample Images", "comments": "13 pages, 8 figures, submitted to IEEE Transactions on Multimedia,\n  under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image hallucination and super-resolution have been studied for decades, and\nmany approaches have been proposed to upsample low-resolution images using\ninformation from the images themselves, multiple example images, or large image\ndatabases. However, most of this work has focused exclusively on small\nmagnification levels because the algorithms simply sharpen the blurry edges in\nthe upsampled images - no actual new detail is typically reconstructed in the\nfinal result. In this paper, we present a patch-based algorithm for image\nhallucination which, for the first time, properly synthesizes novel high\nfrequency detail. To do this, we pose the synthesis problem as a patch-based\noptimization which inserts coherent, high-frequency detail from\ncontextually-similar images of the same physical scene/subject provided from\neither a personal image collection or a large online database. The resulting\nimage is visually plausible and contains coherent high frequency information.\nWe demonstrate the robustness of our algorithm by testing it on a large number\nof images and show that its performance is considerably superior to all\nstate-of-the-art approaches, a result that is verified to be statistically\nsignificant through a randomized user study.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 20:59:43 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Kao", "Chieh-Chi", ""], ["Wang", "Yuxiang", ""], ["Waltman", "Jonathan", ""], ["Sen", "Pradeep", ""]]}, {"id": "1806.00880", "submitter": "Mahyar Khayatkhoei", "authors": "Mahyar Khayatkhoei, Ahmed Elgammal, Maneesh Singh", "title": "Disconnected Manifold Learning for Generative Adversarial Networks", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural images may lie on a union of disjoint manifolds rather than one\nglobally connected manifold, and this can cause several difficulties for the\ntraining of common Generative Adversarial Networks (GANs). In this work, we\nfirst show that single generator GANs are unable to correctly model a\ndistribution supported on a disconnected manifold, and investigate how sample\nquality, mode dropping and local convergence are affected by this. Next, we\nshow how using a collection of generators can address this problem, providing\nnew insights into the success of such multi-generator GANs. Finally, we explain\nthe serious issues caused by considering a fixed prior over the collection of\ngenerators and propose a novel approach for learning the prior and inferring\nthe necessary number of generators without any supervision. Our proposed\nmodifications can be applied on top of any other GAN model to enable learning\nof distributions supported on disconnected manifolds. We conduct several\nexperiments to illustrate the aforementioned shortcoming of GANs, its\nconsequences in practice, and the effectiveness of our proposed modifications\nin alleviating these issues.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 21:19:48 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 21:15:13 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 22:54:41 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Khayatkhoei", "Mahyar", ""], ["Elgammal", "Ahmed", ""], ["Singh", "Maneesh", ""]]}, {"id": "1806.00890", "submitter": "Konstantinos Rematas", "authors": "Konstantinos Rematas, Ira Kemelmacher-Shlizerman, Brian Curless, Steve\n  Seitz", "title": "Soccer on Your Tabletop", "comments": "CVPR'18. Project: http://grail.cs.washington.edu/projects/soccer/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that transforms a monocular video of a soccer game into a\nmoving 3D reconstruction, in which the players and field can be rendered\ninteractively with a 3D viewer or through an Augmented Reality device. At the\nheart of our paper is an approach to estimate the depth map of each player,\nusing a CNN that is trained on 3D player data extracted from soccer video\ngames. We compare with state of the art body pose and depth estimation\ntechniques, and show results on both synthetic ground truth benchmarks, and\nreal YouTube soccer footage.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 22:51:35 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Rematas", "Konstantinos", ""], ["Kemelmacher-Shlizerman", "Ira", ""], ["Curless", "Brian", ""], ["Seitz", "Steve", ""]]}, {"id": "1806.00894", "submitter": "Xiao Chen", "authors": "Barak Oshri, Annie Hu, Peter Adelson, Xiao Chen, Pascaline Dupas,\n  Jeremy Weinstein, Marshall Burke, David Lobell and Stefano Ermon", "title": "Infrastructure Quality Assessment in Africa using Satellite Imagery and\n  Deep Learning", "comments": null, "journal-ref": "KDD 2018 Proceedings of the 24th ACM SIGKDD International\n  Conference on Knowledge Discovery & Data Mining", "doi": "10.1145/3219819.3219924", "report-no": null, "categories": "cs.CY cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The UN Sustainable Development Goals allude to the importance of\ninfrastructure quality in three of its seventeen goals. However, monitoring\ninfrastructure quality in developing regions remains prohibitively expensive\nand impedes efforts to measure progress toward these goals. To this end, we\ninvestigate the use of widely available remote sensing data for the prediction\nof infrastructure quality in Africa. We train a convolutional neural network to\npredict ground truth labels from the Afrobarometer Round 6 survey using Landsat\n8 and Sentinel 1 satellite imagery.\n  Our best models predict infrastructure quality with AUROC scores of 0.881 on\nElectricity, 0.862 on Sewerage, 0.739 on Piped Water, and 0.786 on Roads using\nLandsat 8. These performances are significantly better than models that\nleverage OpenStreetMap or nighttime light intensity on the same tasks. We also\ndemonstrate that our trained model can accurately make predictions in an unseen\ncountry after fine-tuning on a small sample of images. Furthermore, the model\ncan be deployed in regions with limited samples to predict infrastructure\noutcomes with higher performance than nearest neighbor spatial interpolation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 23:30:01 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Oshri", "Barak", ""], ["Hu", "Annie", ""], ["Adelson", "Peter", ""], ["Chen", "Xiao", ""], ["Dupas", "Pascaline", ""], ["Weinstein", "Jeremy", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "1806.00899", "submitter": "Gui-Song Xia", "authors": "Fan Hu, Gui-Song Xia, Wen Yang, Liangpei Zhang", "title": "Recent advances and opportunities in scene classification of aerial\n  images with deep models", "comments": "IGARSS'18 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene classification is a fundamental task in interpretation of remote\nsensing images, and has become an active research topic in remote sensing\ncommunity due to its important role in a wide range of applications. Over the\npast years, tremendous efforts have been made for developing powerful\napproaches for scene classification of remote sensing images, evolving from the\ntraditional bag-of-visual-words model to the new generation deep convolutional\nneural networks (CNNs). The deep CNN based methods have exhibited remarkable\nbreakthrough on performance, dramatically outperforming previous methods which\nstrongly rely on hand-crafted features. However, performance with deep CNNs has\ngradually plateaued on existing public scene datasets, due to the notable\ndrawbacks of these datasets, such as the small scale and low-diversity of\ntraining samples. Therefore, to promote the development of new methods and move\nthe scene classification task a step further, we deeply discuss the existing\nproblems in scene classification task, and accordingly present three open\ndirections. We believe these potential directions will be instructive for the\nresearchers in this field.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 00:02:17 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hu", "Fan", ""], ["Xia", "Gui-Song", ""], ["Yang", "Wen", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1806.00901", "submitter": "Gui-Song Xia", "authors": "Xin-Yi Tong, Qikai Lu, Gui-Song Xia, Liangpei Zhang", "title": "Large-scale Land Cover Classification in GaoFen-2 Satellite Imagery", "comments": "IGARSS'18 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many significant applications need land cover information of remote sensing\nimages that are acquired from different areas and times, such as change\ndetection and disaster monitoring. However, it is difficult to find a generic\nland cover classification scheme for different remote sensing images due to the\nspectral shift caused by diverse acquisition condition. In this paper, we\ndevelop a novel land cover classification method that can deal with large-scale\ndata captured from widely distributed areas and different times. Additionally,\nwe establish a large-scale land cover classification dataset consisting of 150\nGaofen-2 imageries as data support for model training and performance\nevaluation. Our experiments achieve outstanding classification accuracy\ncompared with traditional methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 00:12:00 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Tong", "Xin-Yi", ""], ["Lu", "Qikai", ""], ["Xia", "Gui-Song", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1806.00908", "submitter": "Gui-Song Xia", "authors": "Jin Huang, Gui-Song Xia, Fan Hu, Liangpei Zhang", "title": "Accurate Building Detection in VHR Remote Sensing Images using Geometric\n  Saliency", "comments": "IGRASS'18 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to address the problem of detecting buildings from remote\nsensing images with very high resolution (VHR). Inspired by the observation\nthat buildings are always more distinguishable in geometries than in texture or\nspectral, we propose a new geometric building index (GBI) for accurate building\ndetection, which relies on the geometric saliency of building structures. The\ngeometric saliency of buildings is derived from a mid-level geometric\nrepresentations based on meaningful junctions that can locally describe\nanisotropic geometrical structures of images. The resulting GBI is measured by\nintegrating the derived geometric saliency of buildings. Experiments on three\npublic datasets demonstrate that the proposed GBI achieves very promising\nperformance, and meanwhile shows impressive generalization capability.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 01:02:22 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 01:38:45 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Huang", "Jin", ""], ["Xia", "Gui-Song", ""], ["Hu", "Fan", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1806.00911", "submitter": "Trung Pham", "authors": "Trung Pham, Vijay Kumar B G, Thanh-Toan Do, Gustavo Carneiro, Ian Reid", "title": "Bayesian Semantic Instance Segmentation in Open Set World", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the semantic instance segmentation task in the open-set\nconditions, where input images can contain known and unknown object classes.\nThe training process of existing semantic instance segmentation methods\nrequires annotation masks for all object instances, which is expensive to\nacquire or even infeasible in some realistic scenarios, where the number of\ncategories may increase boundlessly. In this paper, we present a novel open-set\nsemantic instance segmentation approach capable of segmenting all known and\nunknown object classes in images, based on the output of an object detector\ntrained on known object classes. We formulate the problem using a Bayesian\nframework, where the posterior distribution is approximated with a simulated\nannealing optimization equipped with an efficient image partition sampler. We\nshow empirically that our method is competitive with state-of-the-art\nsupervised methods on known classes, but also performs well on unknown classes\nwhen compared with unsupervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 01:16:17 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 02:38:12 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Pham", "Trung", ""], ["G", "Vijay Kumar B", ""], ["Do", "Thanh-Toan", ""], ["Carneiro", "Gustavo", ""], ["Reid", "Ian", ""]]}, {"id": "1806.00921", "submitter": "Xin Yi", "authors": "Xin Yi, Scott Adams, Paul Babyn, Abdul Elnajmi", "title": "Automatic catheter detection in pediatric X-ray images using a\n  scale-recurrent network and synthetic data", "comments": "accepted to the 1st Conference on Medical Imaging with Deep Learning\n  (MIDL2018), Amsterdam, The Netherlands", "journal-ref": null, "doi": "10.1007/s10278-019-00201-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catheters are commonly inserted life supporting devices. X-ray images are\nused to assess the position of a catheter immediately after placement as\nserious complications can arise from malpositioned catheters. Previous computer\nvision approaches to detect catheters on X-ray images either relied on\nlow-level cues that are not sufficiently robust or only capable of processing a\nlimited number or type of catheters. With the resurgence of deep learning,\nsupervised training approaches are begining to showing promising results.\nHowever, dense annotation maps are required, and the work of a human annotator\nis hard to scale. In this work, we proposed a simple way of synthesizing\ncatheters on X-ray images and a scale recurrent network for catheter detection.\nBy training on adult chest X-rays, the proposed network exhibits promising\ndetection results on pediatric chest/abdomen X-rays in terms of both precision\nand recall.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 01:54:38 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Yi", "Xin", ""], ["Adams", "Scott", ""], ["Babyn", "Paul", ""], ["Elnajmi", "Abdul", ""]]}, {"id": "1806.00926", "submitter": "Fenfen Sheng", "authors": "Fenfen Sheng, Zhineng Chen, Bo Xu", "title": "NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text\n  Recognition", "comments": "6 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition has attracted a great many researches due to its\nimportance to various applications. Existing methods mainly adopt recurrence or\nconvolution based networks. Though have obtained good performance, these\nmethods still suffer from two limitations: slow training speed due to the\ninternal recurrence of RNNs, and high complexity due to stacked convolutional\nlayers for long-term feature extraction. This paper, for the first time,\nproposes a no-recurrence sequence-to-sequence text recognizer, named NRTR, that\ndispenses with recurrences and convolutions entirely. NRTR follows the\nencoder-decoder paradigm, where the encoder uses stacked self-attention to\nextract image features, and the decoder applies stacked self-attention to\nrecognize texts based on encoder output. NRTR relies solely on self-attention\nmechanism thus could be trained with more parallelization and less complexity.\nConsidering scene image has large variation in text and background, we further\ndesign a modality-transform block to effectively transform 2D input images to\n1D sequences, combined with the encoder to extract more discriminative\nfeatures. NRTR achieves state-of-the-art or highly competitive performance on\nboth regular and irregular benchmarks, while requires only a small fraction of\ntraining time compared to the best model from the literature (at least 8 times\nfaster).\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 02:10:35 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 11:30:21 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Sheng", "Fenfen", ""], ["Chen", "Zhineng", ""], ["Xu", "Bo", ""]]}, {"id": "1806.00961", "submitter": "Se Young Chun", "authors": "Magauiya Zhussip, Shakarim Soltanayev, Se Young Chun", "title": "Training deep learning based image denoisers from undersampled\n  measurements without ground truth and without image prior", "comments": "10 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing is a method to recover the original image from\nundersampled measurements. In order to overcome the ill-posedness of this\ninverse problem, image priors are used such as sparsity in the wavelet domain,\nminimum total-variation, or self-similarity. Recently, deep learning based\ncompressive image recovery methods have been proposed and have yielded\nstate-of-the-art performances. They used deep learning based data-driven\napproaches instead of hand-crafted image priors to solve the ill-posed inverse\nproblem with undersampled data. Ironically, training deep neural networks for\nthem requires \"clean\" ground truth images, but obtaining the best quality\nimages from undersampled data requires well-trained deep neural networks. To\nresolve this dilemma, we propose novel methods based on two well-grounded\ntheories: denoiser-approximate message passing and Stein's unbiased risk\nestimator. Our proposed methods were able to train deep learning based image\ndenoisers from undersampled measurements without ground truth images and\nwithout image priors, and to recover images with state-of-the-art qualities\nfrom undersampled data. We evaluated our methods for various compressive\nsensing recovery problems with Gaussian random, coded diffraction pattern, and\ncompressive sensing MRI measurement matrices. Our methods yielded\nstate-of-the-art performances for all cases without ground truth images and\nwithout image priors. They also yielded comparable performances to the methods\nwith ground truth data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 05:41:46 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 15:46:52 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Zhussip", "Magauiya", ""], ["Soltanayev", "Shakarim", ""], ["Chun", "Se Young", ""]]}, {"id": "1806.00974", "submitter": "Binghui Chen", "authors": "Binghui Chen, Weihong Deng", "title": "ALMN: Deep Embedding Learning with Geometrical Virtual Point Generating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep embedding learning becomes more attractive for discriminative feature\nlearning, but many methods still require hard-class mining, which is\ncomputationally complex and performance-sensitive. To this end, we propose\nAdaptive Large Margin N-Pair loss (ALMN) to address the aforementioned issues.\nInstead of exploring hard example-mining strategy, we introduce the concept of\nlarge margin constraint. This constraint aims at encouraging local-adaptive\nlarge angular decision margin among dissimilar samples in multimodal feature\nspace so as to significantly encourage intraclass compactness and interclass\nseparability. And it is mainly achieved by a simple yet novel geometrical\nVirtual Point Generating (VPG) method, which converts artificially setting a\nfixed margin into automatically generating a boundary training sample in\nfeature space and is an open question. We demonstrate the effectiveness of our\nmethod on several popular datasets for image retrieval and clustering tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 06:38:28 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 10:47:05 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Chen", "Binghui", ""], ["Deng", "Weihong", ""]]}, {"id": "1806.01013", "submitter": "Lichao Zhang", "authors": "Lichao Zhang, Abel Gonzalez-Garcia, Joost van de Weijer, Martin\n  Danelljan, and Fahad Shahbaz Khan", "title": "Synthetic data generation for end-to-end thermal infrared tracking", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2879249", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of both off-the-shelf and end-to-end trained deep networks have\nsignificantly improved performance of visual tracking on RGB videos. However,\nthe lack of large labeled datasets hampers the usage of convolutional neural\nnetworks for tracking in thermal infrared (TIR) images. Therefore, most state\nof the art methods on tracking for TIR data are still based on handcrafted\nfeatures. To address this problem, we propose to use image-to-image translation\nmodels. These models allow us to translate the abundantly available labeled RGB\ndata to synthetic TIR data. We explore both the usage of paired and unpaired\nimage translation models for this purpose. These methods provide us with a\nlarge labeled dataset of synthetic TIR sequences, on which we can train\nend-to-end optimal features for tracking. To the best of our knowledge we are\nthe first to train end-to-end features for TIR tracking. We perform extensive\nexperiments on VOT-TIR2017 dataset. We show that a network trained on a large\ndataset of synthetic TIR data obtains better performance than one trained on\nthe available real TIR data. Combining both data sources leads to further\nimprovement. In addition, when we combine the network with motion features we\noutperform the state of the art with a relative gain of over 10%, clearly\nshowing the efficiency of using synthetic data to train end-to-end TIR\ntrackers.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 08:52:28 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 11:58:14 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Zhang", "Lichao", ""], ["Gonzalez-Garcia", "Abel", ""], ["van de Weijer", "Joost", ""], ["Danelljan", "Martin", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "1806.01018", "submitter": "Titinunt Kitrungrotsakul", "authors": "Titinunt Kitrungrotsakul, Xian-Hau Han, Yutaro Iwamoto, Satoko\n  Takemoto, Hideo Yokota, Sari Ipponjima, Tomomi Nemoto, Xiong Wei, Yen-Wei\n  Chen", "title": "A 2.5D Cascaded Convolutional Neural Network with Temporal Information\n  for Automatic Mitotic Cell Detection in 4D Microscopic Images", "comments": "4 pages, 4 figures, conference paper (submitted to arxiv then update\n  version and submitted to conference. Finally update in arxiv for newest\n  version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, intravital skin imaging has been increasingly used in\nmammalian skin research to investigate cell behaviors. A fundamental step of\nthe investigation is mitotic cell (cell division) detection. Because of the\ncomplex backgrounds (normal cells), the majority of the existing methods cause\nseveral false positives. In this paper, we proposed a 2.5D cascaded end-to-end\nconvolutional neural network (CasDetNet) with temporal information to\naccurately detect automatic mitotic cell in 4D microscopic images with few\ntraining data. The CasDetNet consists of two 2.5D networks. The first one is\nused for detecting candidate cells with only volume information and the second\none, containing temporal information, for reducing false positive and adding\nmitotic cells that were missed in the first step. The experimental results show\nthat our CasDetNet can achieve higher precision and recall compared to other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 09:05:24 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 03:07:37 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Kitrungrotsakul", "Titinunt", ""], ["Han", "Xian-Hau", ""], ["Iwamoto", "Yutaro", ""], ["Takemoto", "Satoko", ""], ["Yokota", "Hideo", ""], ["Ipponjima", "Sari", ""], ["Nemoto", "Tomomi", ""], ["Wei", "Xiong", ""], ["Chen", "Yen-Wei", ""]]}, {"id": "1806.01023", "submitter": "Hongwei Li", "authors": "Hongwei Li, Kanru Lin, Maximilian Reichert, Lina Xu, Rickmer Braren,\n  Deliang Fu, Roland Schmid, Ji Li, Bjoern Menze and Kuangyu Shi", "title": "Differential Diagnosis for Pancreatic Cysts in CT Scans Using\n  Densely-Connected Convolutional Networks", "comments": "submitted to miccai 2017, *corresponding author: liji@huashan.org.cn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lethal nature of pancreatic ductal adenocarcinoma (PDAC) calls for early\ndifferential diagnosis of pancreatic cysts, which are identified in up to 16%\nof normal subjects, and some of which may develop into PDAC. Previous\ncomputer-aided developments have achieved certain accuracy for classification\non segmented cystic lesions in CT. However, pancreatic cysts have a large\nvariation in size and shape, and the precise segmentation of them remains\nrather challenging, which restricts the computer-aided interpretation of CT\nimages acquired for differential diagnosis. We propose a computer-aided\nframework for early differential diagnosis of pancreatic cysts without\npre-segmenting the lesions using densely-connected convolutional networks\n(Dense-Net). The Dense-Net learns high-level features from whole abnormal\npancreas and builds mappings between medical imaging appearance to different\npathological types of pancreatic cysts. To enhance the clinical applicability,\nwe integrate saliency maps in the framework to assist the physicians to\nunderstand the decision of the deep learning method. The test on a cohort of\n206 patients with 4 pathologically confirmed subtypes of pancreatic cysts has\nachieved an overall accuracy of 72.8%, which is significantly higher than the\nbaseline accuracy of 48.1%, which strongly supports the clinical potential of\nour developed method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 09:25:59 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 14:13:10 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 07:38:11 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Li", "Hongwei", ""], ["Lin", "Kanru", ""], ["Reichert", "Maximilian", ""], ["Xu", "Lina", ""], ["Braren", "Rickmer", ""], ["Fu", "Deliang", ""], ["Schmid", "Roland", ""], ["Li", "Ji", ""], ["Menze", "Bjoern", ""], ["Shi", "Kuangyu", ""]]}, {"id": "1806.01054", "submitter": "Jindong Jiang", "authors": "Jindong Jiang, Lunan Zheng, Fei Luo, and Zhijun Zhang", "title": "RedNet: Residual Encoder-Decoder Network for indoor RGB-D Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor semantic segmentation has always been a difficult task in computer\nvision. In this paper, we propose an RGB-D residual encoder-decoder\narchitecture, named RedNet, for indoor RGB-D semantic segmentation. In RedNet,\nthe residual module is applied to both the encoder and decoder as the basic\nbuilding block, and the skip-connection is used to bypass the spatial feature\nbetween the encoder and decoder. In order to incorporate the depth information\nof the scene, a fusion structure is constructed, which makes inference on RGB\nimage and depth image separately, and fuses their features over several layers.\nIn order to efficiently optimize the network's parameters, we propose a\n`pyramid supervision' training scheme, which applies supervised learning over\ndifferent layers in the decoder, to cope with the problem of gradients\nvanishing. Experiment results show that the proposed RedNet(ResNet-50) achieves\na state-of-the-art mIoU accuracy of 47.8% on the SUN RGB-D benchmark dataset.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 11:33:57 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 17:11:25 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Jiang", "Jindong", ""], ["Zheng", "Lunan", ""], ["Luo", "Fei", ""], ["Zhang", "Zhijun", ""]]}, {"id": "1806.01069", "submitter": "Benjam\\'in Guti\\'errez Becker", "authors": "Benjamin Gutierrez-Becker, Christian Wachinger", "title": "Deep Multi-Structural Shape Analysis: Application to Neuroanatomy", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep neural network for supervised learning on neuroanatomical\nshapes. The network directly operates on raw point clouds without the need for\nmesh processing or the identification of point correspondences, as spatial\ntransformer networks map the data to a canonical space. Instead of relying on\nhand-crafted shape descriptors, an optimal representation is learned in the\nend-to-end training stage of the network. The proposed network consists of\nmultiple branches, so that features for multiple structures are learned\nsimultaneously. We demonstrate the performance of our method on two\napplications: (i) the prediction of Alzheimer's disease and mild cognitive\nimpairment and (ii) the regression of the brain age. Finally, we visualize the\nimportant parts of the anatomy for the prediction by adapting the occlusion\nmethod to point clouds.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 12:22:40 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Gutierrez-Becker", "Benjamin", ""], ["Wachinger", "Christian", ""]]}, {"id": "1806.01097", "submitter": "J\\'er\\'emy Anger", "authors": "J\\'er\\'emy Anger, Mauricio Delbracio, Gabriele Facciolo", "title": "Modeling Realistic Degradations in Non-blind Deconvolution", "comments": "Accepted at the 2018 IEEE International Conference on Image\n  Processing (ICIP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Most image deblurring methods assume an over-simplistic image formation model\nand as a result are sensitive to more realistic image degradations. We propose\na novel variational framework, that explicitly handles pixel saturation, noise,\nquantization, as well as non-linear camera response function due to e.g., gamma\ncorrection. We show that accurately modeling a more realistic image acquisition\npipeline leads to significant improvements, both in terms of image quality and\nPSNR. Furthermore, we show that incorporating the non-linear response in both\nthe data and the regularization terms of the proposed energy leads to a more\ndetailed restoration than a naive inversion of the non-linear curve. The\nminimization of the proposed energy is performed using stochastic optimization.\nA dataset consisting of realistically degraded images is created in order to\nevaluate the method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 13:32:15 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Anger", "J\u00e9r\u00e9my", ""], ["Delbracio", "Mauricio", ""], ["Facciolo", "Gabriele", ""]]}, {"id": "1806.01183", "submitter": "Hui Zhou", "authors": "Hui Zhou, Wanli Ouyang, Jian Cheng, Xiaogang Wang and Hongsheng Li", "title": "Deep Continuous Conditional Random Fields with Asymmetric Inter-object\n  Constraints for Online Multi-object Tracking", "comments": "Accepted to TCSVT", "journal-ref": null, "doi": "10.1109/TCSVT.2018.2825679", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online Multi-Object Tracking (MOT) is a challenging problem and has many\nimportant applications including intelligence surveillance, robot navigation\nand autonomous driving. In existing MOT methods, individual object's movements\nand inter-object relations are mostly modeled separately and relations between\nthem are still manually tuned. In addition, inter-object relations are mostly\nmodeled in a symmetric way, which we argue is not an optimal setting. To tackle\nthose difficulties, in this paper, we propose a Deep Continuous Conditional\nRandom Field (DCCRF) for solving the online MOT problem in a track-by-detection\nframework. The DCCRF consists of unary and pairwise terms. The unary terms\nestimate tracked objects' displacements across time based on visual appearance\ninformation. They are modeled as deep Convolution Neural Networks, which are\nable to learn discriminative visual features for tracklet association. The\nasymmetric pairwise terms model inter-object relations in an asymmetric way,\nwhich encourages high-confidence tracklets to help correct errors of\nlow-confidence tracklets and not to be affected by low-confidence ones much.\nThe DCCRF is trained in an end-to-end manner for better adapting the influences\nof visual information as well as inter-object relations. Extensive experimental\ncomparisons with state-of-the-arts as well as detailed component analysis of\nour proposed DCCRF on two public benchmarks demonstrate the effectiveness of\nour proposed MOT framework.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:27:03 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zhou", "Hui", ""], ["Ouyang", "Wanli", ""], ["Cheng", "Jian", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1806.01196", "submitter": "Jianzhu Guo", "authors": "Jianzhu Guo, Xiangyu Zhu, Zhen Lei and Stan Z. Li", "title": "Face Synthesis for Eyeglass-Robust Face Recognition", "comments": "Accepted by CCBR 2018, with MeGlass released at\n  https://github.com/cleardusk/MeGlass", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the application of face recognition, eyeglasses could significantly\ndegrade the recognition accuracy. A feasible method is to collect large-scale\nface images with eyeglasses for training deep learning methods. However, it is\ndifficult to collect the images with and without glasses of the same identity,\nso that it is difficult to optimize the intra-variations caused by eyeglasses.\nIn this paper, we propose to address this problem in a virtual synthesis\nmanner. The high-fidelity face images with eyeglasses are synthesized based on\n3D face model and 3D eyeglasses. Models based on deep learning methods are then\ntrained on the synthesized eyeglass face dataset, achieving better performance\nthan previous ones. Experiments on the real face database validate the\neffectiveness of our synthesized data for improving eyeglass face recognition\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:38:45 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 13:09:00 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Guo", "Jianzhu", ""], ["Zhu", "Xiangyu", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1806.01225", "submitter": "Barbara Gris", "authors": "Gris Barbara, Chen Chong, \\\"Oktem Ozan", "title": "Image reconstruction through metamorphosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article adapts the framework of metamorphosis to solve inverse problems\nin imaging that includes joint reconstruction and image registration. The\ndeformations in question have two components, one that is a geometric\ndeformation moving intensities and the other a deformation of intensity values\nitself, which, e.g., allows for appearance of a new structure. The idea\ndeveloped here is to reconstruct an image from noisy and indirect observations\nby registering, via metamorphosis, a template to the observed data. Unlike a\nregistration with only geometrical changes, this framework gives good results\nwhen intensities of the template are poorly chosen. We show that this method is\na well-defined regularisation method (proving existence, stability and\nconvergence) and present several numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:09:54 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 08:55:03 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Barbara", "Gris", ""], ["Chong", "Chen", ""], ["Ozan", "\u00d6ktem", ""]]}, {"id": "1806.01260", "submitter": "Cl\\'ement Godard", "authors": "Cl\\'ement Godard, Oisin Mac Aodha, Michael Firman, Gabriel Brostow", "title": "Digging Into Self-Supervised Monocular Depth Estimation", "comments": "ICCV 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Per-pixel ground-truth depth data is challenging to acquire at scale. To\novercome this limitation, self-supervised learning has emerged as a promising\nalternative for training models to perform monocular depth estimation. In this\npaper, we propose a set of improvements, which together result in both\nquantitatively and qualitatively improved depth maps compared to competing\nself-supervised methods.\n  Research on self-supervised monocular training usually explores increasingly\ncomplex architectures, loss functions, and image formation models, all of which\nhave recently helped to close the gap with fully-supervised methods. We show\nthat a surprisingly simple model, and associated design choices, lead to\nsuperior predictions. In particular, we propose (i) a minimum reprojection\nloss, designed to robustly handle occlusions, (ii) a full-resolution\nmulti-scale sampling method that reduces visual artifacts, and (iii) an\nauto-masking loss to ignore training pixels that violate camera motion\nassumptions. We demonstrate the effectiveness of each component in isolation,\nand show high quality, state-of-the-art results on the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:58:05 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 19:06:28 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 01:27:58 GMT"}, {"version": "v4", "created": "Sat, 17 Aug 2019 22:57:30 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Godard", "Cl\u00e9ment", ""], ["Mac Aodha", "Oisin", ""], ["Firman", "Michael", ""], ["Brostow", "Gabriel", ""]]}, {"id": "1806.01263", "submitter": "Seung Lee", "authors": "Suyong Choi, Seung J. Lee, Maxim Perelstein", "title": "Infrared Safety of a Neural-Net Top Tagging Algorithm", "comments": "7 pages, 8 figures, final version to be published in JHEP", "journal-ref": null, "doi": "10.1007/JHEP02(2019)132", "report-no": null, "categories": "hep-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based algorithms provide a promising approach to jet\nclassification problems, such as boosted top jet tagging. To date, NN-based top\ntaggers demonstrated excellent performance in Monte Carlo studies. In this\npaper, we construct a top-jet tagger based on a Convolutional Neural Network\n(CNN), and apply it to parton-level boosted top samples, with and without an\nadditional gluon in the final state. We show that the jet observable defined by\nthe CNN obeys the canonical definition of infrared safety: it is unaffected by\nthe presence of the extra gluon, as long as it is soft or collinear with one of\nthe quarks. Our results indicate that the CNN tagger is robust with respect to\npossible mis-modeling of soft and collinear final-state radiation by Monte\nCarlo generators.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:59:51 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 16:43:37 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Choi", "Suyong", ""], ["Lee", "Seung J.", ""], ["Perelstein", "Maxim", ""]]}, {"id": "1806.01313", "submitter": "Sachin Mehta", "authors": "Sachin Mehta, Ezgi Mercan, Jamen Bartlett, Donald Weave, Joann G.\n  Elmore, Linda Shapiro", "title": "Y-Net: Joint Segmentation and Classification for Diagnosis of Breast\n  Biopsy Images", "comments": "Accepted for publication at MICCAI'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a conceptually simple network for generating\ndiscriminative tissue-level segmentation masks for the purpose of breast cancer\ndiagnosis. Our method efficiently segments different types of tissues in breast\nbiopsy images while simultaneously predicting a discriminative map for\nidentifying important areas in an image. Our network, Y-Net, extends and\ngeneralizes U-Net by adding a parallel branch for discriminative map generation\nand by supporting convolutional block modularity, which allows the user to\nadjust network efficiency without altering the network topology. Y-Net delivers\nstate-of-the-art segmentation accuracy while learning 6.6x fewer parameters\nthan its closest competitors. The addition of descriptive power from Y-Net's\ndiscriminative segmentation masks improve diagnostic classification accuracy by\n7% over state-of-the-art methods for diagnostic classification. Source code is\navailable at: https://sacmehta.github.io/YNet.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 18:28:37 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Mehta", "Sachin", ""], ["Mercan", "Ezgi", ""], ["Bartlett", "Jamen", ""], ["Weave", "Donald", ""], ["Elmore", "Joann G.", ""], ["Shapiro", "Linda", ""]]}, {"id": "1806.01320", "submitter": "Hsien-Tzu Cheng", "authors": "Hsien-Tzu Cheng, Chun-Hung Chao, Jin-Dong Dong, Hao-Kai Wen, Tyng-Luh\n  Liu, Min Sun", "title": "Cube Padding for Weakly-Supervised Saliency Prediction in 360{\\deg}\n  Videos", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic saliency prediction in 360{\\deg} videos is critical for viewpoint\nguidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal\nnetwork which is (1) weakly-supervised trained and (2) tailor-made for\n360{\\deg} viewing sphere. Note that most existing methods are less scalable\nsince they rely on annotated saliency map for training. Most importantly, they\nconvert 360{\\deg} sphere to 2D images (e.g., a single equirectangular image or\nmultiple separate Normal Field-of-View (NFoV) images) which introduces\ndistortion and image boundaries. In contrast, we propose a simple and effective\nCube Padding (CP) technique as follows. Firstly, we render the 360{\\deg} view\non six faces of a cube using perspective projection. Thus, it introduces very\nlittle distortion. Then, we concatenate all six faces while utilizing the\nconnectivity between faces on the cube for image padding (i.e., Cube Padding)\nin convolution, pooling, convolutional LSTM layers. In this way, CP introduces\nno image boundary while being applicable to almost all Convolutional Neural\nNetwork (CNN) structures. To evaluate our method, we propose Wild-360, a new\n360{\\deg} video saliency dataset, containing challenging videos with saliency\nheatmap annotations. In experiments, our method outperforms baseline methods in\nboth speed and quality.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 18:42:37 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Cheng", "Hsien-Tzu", ""], ["Chao", "Chun-Hung", ""], ["Dong", "Jin-Dong", ""], ["Wen", "Hao-Kai", ""], ["Liu", "Tyng-Luh", ""], ["Sun", "Min", ""]]}, {"id": "1806.01339", "submitter": "Dominique Beaini", "authors": "Dominique Beaini, Sofiane Achiche, Fabrice Nonez, Maxime Raison", "title": "Computing the Spatial Probability of Inclusion inside Partial Contours\n  for Computer Vision Applications", "comments": "Keywords: Computer vision; Stroke analysis; Partial contour;\n  Probability of inclusion; Edge interaction; Image convolution;\n  Electromagnetic potential field", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In Computer Vision, edge detection is one of the favored approaches for\nfeature and object detection in images since it provides information about\ntheir objects boundaries. Other region-based approaches use probabilistic\nanalysis such as clustering and Markov random fields, but those methods cannot\nbe used to analyze edges and their interaction. In fact, only image\nsegmentation can produce regions based on edges, but it requires thresholding\nby simply separating the regions into binary in-out information. Hence, there\nis currently a gap between edge-based and region-based algorithms, since edges\ncannot be used to study the properties of a region and vice versa. The\nobjective of this paper is to present a novel spatial probability analysis that\nallows determining the probability of inclusion inside a set of partial\ncontours (strokes). To answer this objective, we developed a new approach that\nuses electromagnetic convolutions and repulsion optimization to compute the\nrequired probabilities. Hence, it becomes possible to generate a continuous\nspace of probability based only on the edge information, thus bridging the gap\nbetween the edge-based methods and the region-based methods. The developed\nmethod is consistent with the fundamental properties of inclusion probabilities\nand its results are validated by comparing an image with the probability-based\nestimation given by our algorithm. The method can also be generalized to take\ninto consideration the intensity of the edges or to be used for 3D shapes. This\nis the first documented method that allows computing a space of probability\nbased on interacting edges, which opens the path to broader applications such\nas image segmentation and contour completion.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 19:26:51 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 14:49:02 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Beaini", "Dominique", ""], ["Achiche", "Sofiane", ""], ["Nonez", "Fabrice", ""], ["Raison", "Maxime", ""]]}, {"id": "1806.01340", "submitter": "Shuming Jiao", "authors": "Jun Feng, Shuming Jiao, Yang Gao, Ting Lei, Xiaocong Yuan", "title": "Design of optimal illumination patterns in single-pixel imaging using\n  image dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-pixel imaging (SPI) has a major drawback that many sequential\nilluminations are required for capturing one single image with long acquisition\ntime. Basis illumination patterns such as Fourier patterns and Hadamard\npatterns can achieve much better imaging efficiency than random patterns. But\nthe performance is still sub-optimal since the basis patterns are fixed and\nnon-adaptive for varying object images. This Letter proposes a novel scheme for\ndesigning and optimizing the illumination patterns adaptively from an image\ndictionary by extracting the common image features using principal component\nanalysis (PCA). Simulation and experimental results reveal that our proposed\nscheme outperforms conventional Fourier SPI in terms of imaging efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 19:28:12 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 10:32:22 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Feng", "Jun", ""], ["Jiao", "Shuming", ""], ["Gao", "Yang", ""], ["Lei", "Ting", ""], ["Yuan", "Xiaocong", ""]]}, {"id": "1806.01349", "submitter": "Dani\\\"el Reichman", "authors": "Daniel Reichman, Leslie M. Collins, and Jordan M. Malof", "title": "gprHOG and the popularity of Histogram of Oriented Gradients (HOG) for\n  Buried Threat Detection in Ground-Penetrating Radar", "comments": "5 pages, 6 figures, letter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial research has been devoted to the development of algorithms that\nautomate buried threat detection (BTD) with ground penetrating radar (GPR)\ndata, resulting in a large number of proposed algorithms. One popular algorithm\nGPR-based BTD, originally applied by Torrione et al., 2012, is the Histogram of\nOriented Gradients (HOG) feature. In a recent large-scale comparison among five\nveteran institutions, a modified version of HOG referred to here as \"gprHOG\",\nperformed poorly compared to other modern algorithms. In this paper, we provide\nexperimental evidence demonstrating that the modifications to HOG that comprise\ngprHOG result in a substantially better-performing algorithm. The results here,\nin conjunction with the large-scale algorithm comparison, suggest that HOG is\nnot competitive with modern GPR-based BTD algorithms. Given HOG's popularity,\nthese results raise some questions about many existing studies, and suggest\ngprHOG (and especially HOG) should be employed with caution in future studies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 19:51:17 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 15:02:08 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Reichman", "Daniel", ""], ["Collins", "Leslie M.", ""], ["Malof", "Jordan M.", ""]]}, {"id": "1806.01357", "submitter": "Jian Ren", "authors": "Jian Ren, Ilker Hacihaliloglu, Eric A. Singer, David J. Foran, Xin Qi", "title": "Adversarial Domain Adaptation for Classification of Prostate\n  Histopathology Whole-Slide Images", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic and accurate Gleason grading of histopathology tissue slides is\ncrucial for prostate cancer diagnosis, treatment, and prognosis. Usually,\nhistopathology tissue slides from different institutions show heterogeneous\nappearances because of different tissue preparation and staining procedures,\nthus the predictable model learned from one domain may not be applicable to a\nnew domain directly. Here we propose to adopt unsupervised domain adaptation to\ntransfer the discriminative knowledge obtained from the source domain to the\ntarget domain without requiring labeling of images at the target domain. The\nadaptation is achieved through adversarial training to find an invariant\nfeature space along with the proposed Siamese architecture on the target domain\nto add a regularization that is appropriate for the whole-slide images. We\nvalidate the method on two prostate cancer datasets and obtain significant\nclassification improvement of Gleason scores as compared with the baseline\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 20:01:09 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 23:49:17 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Ren", "Jian", ""], ["Hacihaliloglu", "Ilker", ""], ["Singer", "Eric A.", ""], ["Foran", "David J.", ""], ["Qi", "Xin", ""]]}, {"id": "1806.01376", "submitter": "Jian Ren", "authors": "Jian Ren, Jianchao Yang, Ning Xu, David J. Foran", "title": "Factorized Adversarial Networks for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Factorized Adversarial Networks (FAN) to solve\nunsupervised domain adaptation problems for image classification tasks. Our\nnetworks map the data distribution into a latent feature space, which is\nfactorized into a domain-specific subspace that contains domain-specific\ncharacteristics and a task-specific subspace that retains category information,\nfor both source and target domains, respectively. Unsupervised domain\nadaptation is achieved by adversarial training to minimize the discrepancy\nbetween the distributions of two task-specific subspaces from source and target\ndomains. We demonstrate that the proposed approach outperforms state-of-the-art\nmethods on multiple benchmark datasets used in the literature for unsupervised\ndomain adaptation. Furthermore, we collect two real-world tagging datasets that\nare much larger than existing benchmark datasets, and get significant\nimprovement upon baselines, proving the practical value of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 20:39:13 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Ren", "Jian", ""], ["Yang", "Jianchao", ""], ["Xu", "Ning", ""], ["Foran", "David J.", ""]]}, {"id": "1806.01411", "submitter": "Xingyu Liu", "authors": "Xingyu Liu and Charles R. Qi and Leonidas J. Guibas", "title": "FlowNet3D: Learning Scene Flow in 3D Point Clouds", "comments": "CVPR 2019. Source code available at\n  http://github.com/xingyul/flownet3d", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in robotics and human-computer interaction can benefit from\nunderstanding 3D motion of points in a dynamic environment, widely noted as\nscene flow. While most previous methods focus on stereo and RGB-D images as\ninput, few try to estimate scene flow directly from point clouds. In this work,\nwe propose a novel deep neural network named $FlowNet3D$ that learns scene flow\nfrom point clouds in an end-to-end fashion. Our network simultaneously learns\ndeep hierarchical features of point clouds and flow embeddings that represent\npoint motions, supported by two newly proposed learning layers for point sets.\nWe evaluate the network on both challenging synthetic data from FlyingThings3D\nand real Lidar scans from KITTI. Trained on synthetic data only, our network\nsuccessfully generalizes to real scans, outperforming various baselines and\nshowing competitive results to the prior art. We also demonstrate two\napplications of our scene flow output (scan registration and motion\nsegmentation) to show its potential wide use cases.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 22:07:36 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 19:40:43 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 21:33:18 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Liu", "Xingyu", ""], ["Qi", "Charles R.", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1806.01413", "submitter": "Fausto Milletari", "authors": "Fausto Milletari, Nicola Rieke, Maximilian Baust, Marco Esposito,\n  Nassir Navab", "title": "CFCM: Segmentation via Coarse to Fine Context Memory", "comments": "Accepted for presentation at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural-network-based architectures for image segmentation make\nextensive usage of feature forwarding mechanisms to integrate information from\nmultiple scales. Although yielding good results, even deeper architectures and\nalternative methods for feature fusion at different resolutions have been\nscarcely investigated for medical applications. In this work we propose to\nimplement segmentation via an encoder-decoder architecture which differs from\nany other previously published method since (i) it employs a very deep\narchitecture based on residual learning and (ii) combines features via a\nconvolutional Long Short Term Memory (LSTM), instead of concatenation or\nsummation. The intuition is that the memory mechanism implemented by LSTMs can\nbetter integrate features from different scales through a coarse-to-fine\nstrategy; hence the name Coarse-to-Fine Context Memory (CFCM). We demonstrate\nthe remarkable advantages of this approach on two datasets: the Montgomery\ncounty lung segmentation dataset, and the EndoVis 2015 challenge dataset for\nsurgical instrument segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 22:12:41 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Milletari", "Fausto", ""], ["Rieke", "Nicola", ""], ["Baust", "Maximilian", ""], ["Esposito", "Marco", ""], ["Navab", "Nassir", ""]]}, {"id": "1806.01482", "submitter": "Amir Sadeghian", "authors": "Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, S.\n  Hamid Rezatofighi, Silvio Savarese", "title": "SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and\n  Physical Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper addresses the problem of path prediction for multiple interacting\nagents in a scene, which is a crucial step for many autonomous platforms such\nas self-driving cars and social robots. We present \\textit{SoPhie}; an\ninterpretable framework based on Generative Adversarial Network (GAN), which\nleverages two sources of information, the path history of all the agents in a\nscene, and the scene context information, using images of the scene. To predict\na future path for an agent, both physical and social information must be\nleveraged. Previous work has not been successful to jointly model physical and\nsocial interactions. Our approach blends a social attention mechanism with a\nphysical attention that helps the model to learn where to look in a large scene\nand extract the most salient parts of the image relevant to the path. Whereas,\nthe social attention component aggregates information across the different\nagent interactions and extracts the most important trajectory information from\nthe surrounding neighbors. SoPhie also takes advantage of GAN to generates more\nrealistic samples and to capture the uncertain nature of the future paths by\nmodeling its distribution. All these mechanisms enable our approach to predict\nsocially and physically plausible paths for the agents and to achieve\nstate-of-the-art performance on several different trajectory forecasting\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 03:49:46 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 17:42:42 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Sadeghian", "Amir", ""], ["Kosaraju", "Vineet", ""], ["Sadeghian", "Ali", ""], ["Hirose", "Noriaki", ""], ["Rezatofighi", "S. Hamid", ""], ["Savarese", "Silvio", ""]]}, {"id": "1806.01484", "submitter": "Aiden Nibali", "authors": "Aiden Nibali, Zhen He, Stuart Morgan, Luke Prendergast", "title": "3D Human Pose Estimation with 2D Marginal Heatmaps", "comments": "Accepted in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically determining three-dimensional human pose from monocular RGB\nimage data is a challenging problem. The two-dimensional nature of the input\nresults in intrinsic ambiguities which make inferring depth particularly\ndifficult. Recently, researchers have demonstrated that the flexible\nstatistical modelling capabilities of deep neural networks are sufficient to\nmake such inferences with reasonable accuracy. However, many of these models\nuse coordinate output techniques which are memory-intensive, not\ndifferentiable, and/or do not spatially generalise well. We propose\nimprovements to 3D coordinate prediction which avoid the aforementioned\nundesirable traits by predicting 2D marginal heatmaps under an augmented\nsoft-argmax scheme. Our resulting model, MargiPose, produces visually coherent\nheatmaps whilst maintaining differentiability. We are also able to achieve\nstate-of-the-art accuracy on publicly available 3D human pose estimation data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 03:51:14 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 02:47:32 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Nibali", "Aiden", ""], ["He", "Zhen", ""], ["Morgan", "Stuart", ""], ["Prendergast", "Luke", ""]]}, {"id": "1806.01496", "submitter": "Tong Chen", "authors": "Haojie Liu, Tong Chen, Qiu Shen, Tao Yue, Zhan Ma", "title": "Deep Image Compression via End-to-End Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a lossy image compression method based on deep convolutional\nneural networks (CNNs), which outperforms the existing BPG, WebP, JPEG2000 and\nJPEG as measured via multi-scale structural similarity (MS-SSIM), at the same\nbit rate. Currently, most of the CNNs based approaches train the network using\na L2 loss between the reconstructions and the ground-truths in the pixel\ndomain, which leads to over-smoothing results and visual quality degradation\nespecially at a very low bit rate. Therefore, we improve the subjective quality\nwith the combination of a perception loss and an adversarial loss additionally.\nTo achieve better rate-distortion optimization (RDO), we also introduce an\neasy-to-hard transfer learning when adding quantization error and rate\nconstraint. Finally, we evaluate our method on public Kodak and the Test\nDataset P/M released by the Computer Vision Lab of ETH Zurich, resulting in\naveraged 7.81% and 19.1% BD-rate reduction over BPG, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 05:07:51 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Liu", "Haojie", ""], ["Chen", "Tong", ""], ["Shen", "Qiu", ""], ["Yue", "Tao", ""], ["Ma", "Zhan", ""]]}, {"id": "1806.01524", "submitter": "Hengyu Li", "authors": "Hang Liu, Hengyu Li, Jun Luo, Shaorong Xie, Yu Sun", "title": "Construction of all-in-focus images assisted by depth sensing", "comments": "18 pages. This paper has been submitted to Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-focus image fusion is a technique for obtaining an all-in-focus image\nin which all objects are in focus to extend the limited depth of field (DoF) of\nan imaging system. Different from traditional RGB-based methods, this paper\npresents a new multi-focus image fusion method assisted by depth sensing. In\nthis work, a depth sensor is used together with a color camera to capture\nimages of a scene. A graph-based segmentation algorithm is used to segment the\ndepth map from the depth sensor, and the segmented regions are used to guide a\nfocus algorithm to locate in-focus image blocks from among multi-focus source\nimages to construct the reference all-in-focus image. Five test scenes and six\nevaluation metrics were used to compare the proposed method and representative\nstate-of-the-art algorithms. Experimental results quantitatively demonstrate\nthat this method outperforms existing methods in both speed and quality (in\nterms of comprehensive fusion metrics). The generated images can potentially be\nused as reference all-in-focus images.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 07:34:38 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Liu", "Hang", ""], ["Li", "Hengyu", ""], ["Luo", "Jun", ""], ["Xie", "Shaorong", ""], ["Sun", "Yu", ""]]}, {"id": "1806.01531", "submitter": "Xin Wang", "authors": "Xin Wang, Fisher Yu, Lisa Dunlap, Yi-An Ma, Ruth Wang, Azalia\n  Mirhoseini, Trevor Darrell, Joseph E. Gonzalez", "title": "Deep Mixture of Experts via Shallow Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Larger networks generally have greater representational power at the cost of\nincreased computational complexity. Sparsifying such networks has been an\nactive area of research but has been generally limited to static regularization\nor dynamic approaches using reinforcement learning. We explore a mixture of\nexperts (MoE) approach to deep dynamic routing, which activates certain experts\nin the network on a per-example basis. Our novel DeepMoE architecture increases\nthe representational power of standard convolutional networks by adaptively\nsparsifying and recalibrating channel-wise features in each convolutional\nlayer. We employ a multi-headed sparse gating network to determine the\nselection and scaling of channels for each input, leveraging exponential\ncombinations of experts within a single convolutional network. Our proposed\narchitecture is evaluated on four benchmark datasets and tasks, and we show\nthat Deep-MoEs are able to achieve higher accuracy with lower computation than\nstandard convolutional networks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 07:41:04 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 07:48:46 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 20:55:58 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Wang", "Xin", ""], ["Yu", "Fisher", ""], ["Dunlap", "Lisa", ""], ["Ma", "Yi-An", ""], ["Wang", "Ruth", ""], ["Mirhoseini", "Azalia", ""], ["Darrell", "Trevor", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1806.01547", "submitter": "Gullal Singh Cheema", "authors": "Ankita Shukla, Gullal Singh Cheema, Saket Anand", "title": "Semi-Supervised Clustering with Neural Networks", "comments": "9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering using neural networks has recently demonstrated promising\nperformance in machine learning and computer vision applications. However, the\nperformance of current approaches is limited either by unsupervised learning or\ntheir dependence on large set of labeled data samples. In this paper, we\npropose ClusterNet that uses pairwise semantic constraints from very few\nlabeled data samples (<5% of total data) and exploits the abundant unlabeled\ndata to drive the clustering approach. We define a new loss function that uses\npairwise semantic similarity between objects combined with constrained k-means\nclustering to efficiently utilize both labeled and unlabeled data in the same\nframework. The proposed network uses convolution autoencoder to learn a latent\nrepresentation that groups data into k specified clusters, while also learning\nthe cluster centers simultaneously. We evaluate and compare the performance of\nClusterNet on several datasets and state of the art deep clustering approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 08:23:42 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 09:10:35 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Shukla", "Ankita", ""], ["Cheema", "Gullal Singh", ""], ["Anand", "Saket", ""]]}, {"id": "1806.01550", "submitter": "Frederic Jurie", "authors": "Sovann En, Alexis Lechervy, Fr\\'ed\\'eric Jurie", "title": "TS-Net: Combining modality specific and common features for multimodal\n  patch matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal patch matching addresses the problem of finding the\ncorrespondences between image patches from two different modalities, e.g. RGB\nvs sketch or RGB vs near-infrared. The comparison of patches of different\nmodalities can be done by discovering the information common to both modalities\n(Siamese like approaches) or the modality-specific information (Pseudo-Siamese\nlike approaches). We observed that none of these two scenarios is optimal. This\nmotivates us to propose a three-stream architecture, dubbed as TS-Net,\ncombining the benefits of the two. In addition, we show that adding extra\nconstraints in the intermediate layers of such networks further boosts the\nperformance. Experimentations on three multimodal datasets show significant\nperformance gains in comparison with Siamese and Pseudo-Siamese networks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 08:25:46 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["En", "Sovann", ""], ["Lechervy", "Alexis", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1806.01576", "submitter": "Chunhua Shen", "authors": "Lei Zhang, Peng Wang, Chunhua Shen, Lingqiao Liu, Wei Wei, Yanning\n  Zhang, Anton van den Hengel", "title": "Adaptive Importance Learning for Improving Lightweight Image\n  Super-resolution Network", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved remarkable success in single image\nsuper-resolution (SISR). The computing and memory requirements of these methods\nhave hindered their application to broad classes of real devices with limited\ncomputing power, however. One approach to this problem has been lightweight\nnetwork architectures that bal- ance the super-resolution performance and the\ncomputation burden. In this study, we revisit this problem from an orthog- onal\nview, and propose a novel learning strategy to maxi- mize the pixel-wise\nfitting capacity of a given lightweight network architecture. Considering that\nthe initial capacity of the lightweight network is very limited, we present an\nadaptive importance learning scheme for SISR that trains the network with an\neasy-to-complex paradigm by dynam- ically updating the importance of image\npixels on the basis of the training loss. Specifically, we formulate the\nnetwork training and the importance learning into a joint optimization problem.\nWith a carefully designed importance penalty function, the importance of\nindividual pixels can be gradu- ally increased through solving a convex\noptimization problem. The training process thus begins with pixels that are\neasy to reconstruct, and gradually proceeds to more complex pixels as fitting\nimproves.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 09:31:19 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Zhang", "Lei", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Liu", "Lingqiao", ""], ["Wei", "Wei", ""], ["Zhang", "Yanning", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1806.01593", "submitter": "Wei Li", "authors": "Bo Yang Hsueh, Wei Li, I-Chen Wu", "title": "Stochastic Gradient Descent with Hyperbolic-Tangent Decay on\n  Classification", "comments": "WACV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning rate scheduler has been a critical issue in the deep neural network\ntraining. Several schedulers and methods have been proposed, including step\ndecay scheduler, adaptive method, cosine scheduler and cyclical scheduler. This\npaper proposes a new scheduling method, named hyperbolic-tangent decay (HTD).\nWe run experiments on several benchmarks such as: ResNet, Wide ResNet and\nDenseNet for CIFAR-10 and CIFAR-100 datasets, LSTM for PAMAP2 dataset, ResNet\non ImageNet and Fashion-MNIST datasets. In our experiments, HTD outperforms\nstep decay and cosine scheduler in nearly all cases, while requiring less\nhyperparameters than step decay, and more flexible than cosine scheduler. Code\nis available at https://github.com/BIGBALLON/HTD.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 10:14:01 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 22:50:09 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Hsueh", "Bo Yang", ""], ["Li", "Wei", ""], ["Wu", "I-Chen", ""]]}, {"id": "1806.01603", "submitter": "Simon Carbonnelle", "authors": "Simon Carbonnelle and Christophe De Vleeschouwer", "title": "Layer rotation: a surprisingly powerful indicator of generalization in\n  deep networks?", "comments": "Extended version of paper presented at ICML workshop \"Identifying and\n  Understanding Deep Learning Phenomena\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work presents extensive empirical evidence that layer rotation, i.e. the\nevolution across training of the cosine distance between each layer's weight\nvector and its initialization, constitutes an impressively consistent indicator\nof generalization performance. In particular, larger cosine distances between\nfinal and initial weights of each layer consistently translate into better\ngeneralization performance of the final model. Interestingly, this relation\nadmits a network independent optimum: training procedures during which all\nlayers' weights reach a cosine distance of 1 from their initialization\nconsistently outperform other configurations -by up to 30% test accuracy.\nMoreover, we show that layer rotations are easily monitored and controlled\n(helpful for hyperparameter tuning) and potentially provide a unified framework\nto explain the impact of learning rate tuning, weight decay, learning rate\nwarmups and adaptive gradient methods on generalization and training speed. In\nan attempt to explain the surprising properties of layer rotation, we show on a\n1-layer MLP trained on MNIST that layer rotation correlates with the degree to\nwhich features of intermediate layers have been trained.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 10:39:21 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 16:01:43 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Carbonnelle", "Simon", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "1806.01621", "submitter": "Manh Duong Phung", "authors": "Cong Hoang Quach, Van Lien Tran, Duy Hung Nguyen, Viet Thang Nguyen,\n  Minh Trien Pham and Manh Duong Phung", "title": "Real-time Lane Marker Detection Using Template Matching with RGB-D\n  Camera", "comments": "2018 2nd International Conference on Recent Advances in Signal\n  Processing, Telecommunications & Computing (SigTelCom)", "journal-ref": null, "doi": "10.1109/SIGTELCOM.2018.8325781", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of lane detection which is fundamental for\nself-driving vehicles. Our approach exploits both colour and depth information\nrecorded by a single RGB-D camera to better deal with negative factors such as\nlighting conditions and lane-like objects. In the approach, colour and depth\nimages are first converted to a half-binary format and a 2D matrix of 3D\npoints. They are then used as the inputs of template matching and geometric\nfeature extraction processes to form a response map so that its values\nrepresent the probability of pixels being lane markers. To further improve the\nresults, the template and lane surfaces are finally refined by principal\ncomponent analysis and lane model fitting techniques. A number of experiments\nhave been conducted on both synthetic and real datasets. The result shows that\nthe proposed approach can effectively eliminate unwanted noise to accurately\ndetect lane markers in various scenarios. Moreover, the processing speed of 20\nframes per second under hardware configuration of a popular laptop computer\nallows the proposed algorithm to be implemented for real-time autonomous\ndriving applications.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 12:03:18 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Quach", "Cong Hoang", ""], ["Tran", "Van Lien", ""], ["Nguyen", "Duy Hung", ""], ["Nguyen", "Viet Thang", ""], ["Pham", "Minh Trien", ""], ["Phung", "Manh Duong", ""]]}, {"id": "1806.01673", "submitter": "Mohammad Reza Loghmani", "authors": "Mohammad Reza Loghmani, Mirco Planamente, Barbara Caputo, Markus\n  Vincze", "title": "Recurrent Convolutional Fusion for RGB-D Object Recognition", "comments": "Under review at RA-L", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing machines with the ability to recognize objects like humans has\nalways been one of the primary goals of machine vision. The introduction of\nRGB-D cameras has paved the way for a significant leap forward in this\ndirection thanks to the rich information provided by these sensors. However,\nthe machine vision community still lacks an effective method to synergically\nuse the RGB and depth data to improve object recognition. In order to take a\nstep in this direction, we introduce a novel end-to-end architecture for RGB-D\nobject recognition called recurrent convolutional fusion (RCFusion). Our method\ngenerates compact and highly discriminative multi-modal features by combining\ncomplementary RGB and depth information representing different levels of\nabstraction. Extensive experiments on two popular datasets, RGB-D Object\nDataset and JHUIT-50, show that RCFusion significantly outperforms\nstate-of-the-art approaches in both the object categorization and instance\nrecognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 13:14:17 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 14:40:34 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 14:32:35 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Loghmani", "Mohammad Reza", ""], ["Planamente", "Mirco", ""], ["Caputo", "Barbara", ""], ["Vincze", "Markus", ""]]}, {"id": "1806.01677", "submitter": "Stepan Tulyakov", "authors": "Stepan Tulyakov and Anton Ivanov and Francois Fleuret", "title": "Practical Deep Stereo (PDS): Toward applications-friendly deep stereo\n  matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end deep-learning networks recently demonstrated extremely good\nperfor- mance for stereo matching. However, existing networks are difficult to\nuse for practical applications since (1) they are memory-hungry and unable to\nprocess even modest-size images, (2) they have to be trained for a given\ndisparity range. The Practical Deep Stereo (PDS) network that we propose\naddresses both issues: First, its architecture relies on novel bottleneck\nmodules that drastically reduce the memory footprint in inference, and\nadditional design choices allow to handle greater image size during training.\nThis results in a model that leverages large image context to resolve matching\nambiguities. Second, a novel sub-pixel cross- entropy loss combined with a MAP\nestimator make this network less sensitive to ambiguous matches, and applicable\nto any disparity range without re-training. We compare PDS to state-of-the-art\nmethods published over the recent months, and demonstrate its superior\nperformance on FlyingThings3D and KITTI sets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 13:24:40 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Tulyakov", "Stepan", ""], ["Ivanov", "Anton", ""], ["Fleuret", "Francois", ""]]}, {"id": "1806.01683", "submitter": "Kamel Abdelouahab", "authors": "Kamel Abdelouahab and Maxime Pelcat and Jocelyn Serot and Fran\\c{c}ois\n  Berry", "title": "Accelerating CNN inference on FPGAs: A Survey", "comments": "Cloning our HAL submission in ArXiv, Technical Report - Universite\n  Clermont Auvergne, January 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are currently adopted to solve an ever\ngreater number of problems, ranging from speech recognition to image\nclassification and segmentation. The large amount of processing required by\nCNNs calls for dedicated and tailored hardware support methods. Moreover, CNN\nworkloads have a streaming nature, well suited to reconfigurable hardware\narchitectures such as FPGAs. The amount and diversity of research on the\nsubject of CNN FPGA acceleration within the last 3 years demonstrates the\ntremendous industrial and academic interest. This paper presents a\nstate-of-the-art of CNN inference accelerators over FPGAs. The computational\nworkloads, their parallelism and the involved memory accesses are analyzed. At\nthe level of neurons, optimizations of the convolutional and fully connected\nlayers are explained and the performances of the different methods compared. At\nthe network level, approximate computing and datapath optimization methods are\ncovered and state-of-the-art approaches compared. The methods and tools\ninvestigated in this survey represent the recent trends in FPGA CNN inference\naccelerators and will fuel the future advances on efficient hardware deep\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 12:24:49 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Abdelouahab", "Kamel", ""], ["Pelcat", "Maxime", ""], ["Serot", "Jocelyn", ""], ["Berry", "Fran\u00e7ois", ""]]}, {"id": "1806.01729", "submitter": "Jianzhong Sheng", "authors": "Jianzhong Sheng, Chuanbo Chen, Chenchen Fu and Chun Jason Xue", "title": "EasyConvPooling: Random Pooling with Easy Convolution for Accelerating\n  Training and Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution operations dominate the overall execution time of Convolutional\nNeural Networks (CNNs). This paper proposes an easy yet efficient technique for\nboth Convolutional Neural Network training and testing. The conventional\nconvolution and pooling operations are replaced by Easy Convolution and Random\nPooling (ECP). In ECP, we randomly select one pixel out of four and only\nconduct convolution operations of the selected pixel. As a result, only a\nquarter of the conventional convolution computations are needed. Experiments\ndemonstrate that the proposed EasyConvPooling can achieve 1.45x speedup on\ntraining time and 1.64x on testing time. What's more, a speedup of 5.09x on\npure Easy Convolution operations is obtained compared to conventional\nconvolution operations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 14:56:10 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Sheng", "Jianzhong", ""], ["Chen", "Chuanbo", ""], ["Fu", "Chenchen", ""], ["Xue", "Chun Jason", ""]]}, {"id": "1806.01759", "submitter": "Pedro Hermosilla Casajus", "authors": "Pedro Hermosilla, Tobias Ritschel, Pere-Pau V\\'azquez, \\`Alvar\n  Vinacua, Timo Ropinski", "title": "Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point\n  Clouds", "comments": "ACM Transactions on Graphics (Proocedings of SIGGRAPH Asia 2018)", "journal-ref": null, "doi": "10.1145/3272127.3275110", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning systems extensively use convolution operations to process input\ndata. Though convolution is clearly defined for structured data such as 2D\nimages or 3D volumes, this is not true for other data types such as sparse\npoint clouds. Previous techniques have developed approximations to convolutions\nfor restricted conditions. Unfortunately, their applicability is limited and\ncannot be used for general point clouds. We propose an efficient and effective\nmethod to learn convolutions for non-uniformly sampled point clouds, as they\nare obtained with modern acquisition techniques. Learning is enabled by four\nkey novelties: first, representing the convolution kernel itself as a\nmultilayer perceptron; second, phrasing convolution as a Monte Carlo\nintegration problem, third, using this notion to combine information from\nmultiple samplings at different levels; and fourth using Poisson disk sampling\nas a scalable means of hierarchical point cloud learning. The key idea across\nall these contributions is to guarantee adequate consideration of the\nunderlying non-uniform sample distribution function from a Monte Carlo\nperspective. To make the proposed concepts applicable to real-world tasks, we\nfurthermore propose an efficient implementation which significantly reduces the\nGPU memory required during the training process. By employing our method in\nhierarchical network architectures we can outperform most of the\nstate-of-the-art networks on established point cloud segmentation,\nclassification and normal estimation benchmarks. Furthermore, in contrast to\nmost existing approaches, we also demonstrate the robustness of our method with\nrespect to sampling variations, even when training with uniformly sampled data\nonly. To support the direct application of these concepts, we provide a\nready-to-use TensorFlow implementation of these layers at\nhttps://github.com/viscom-ulm/MCCNN\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 15:56:24 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 08:29:12 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Hermosilla", "Pedro", ""], ["Ritschel", "Tobias", ""], ["V\u00e1zquez", "Pere-Pau", ""], ["Vinacua", "\u00c0lvar", ""], ["Ropinski", "Timo", ""]]}, {"id": "1806.01764", "submitter": "Salim Arslan", "authors": "Salim Arslan, Sofia Ira Ktena, Ben Glocker, Daniel Rueckert", "title": "Graph Saliency Maps through Spectral Convolutional Networks: Application\n  to Sex Classification with Brain Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) allow to apply traditional convolution\noperations in non-Euclidean domains, where data are commonly modelled as\nirregular graphs. Medical imaging and, in particular, neuroscience studies\noften rely on such graph representations, with brain connectivity networks\nbeing a characteristic example, while ultimately seeking the locus of\nphenotypic or disease-related differences in the brain. These regions of\ninterest (ROIs) are, then, considered to be closely associated with function\nand/or behaviour. Driven by this, we explore GCNs for the task of ROI\nidentification and propose a visual attribution method based on class\nactivation mapping. By undertaking a sex classification task as proof of\nconcept, we show that this method can be used to identify salient nodes (brain\nregions) without prior node labels. Based on experiments conducted on\nneuroimaging data of more than 5000 participants from UK Biobank, we\ndemonstrate the robustness of the proposed method in highlighting reproducible\nregions across individuals. We further evaluate the neurobiological relevance\nof the identified regions based on evidence from large-scale UK Biobank\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:01:36 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Arslan", "Salim", ""], ["Ktena", "Sofia Ira", ""], ["Glocker", "Ben", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1806.01775", "submitter": "Fuqiang Liu", "authors": "F. Liu, C. Liu and F.Bi", "title": "A Memristor based Unsupervised Neuromorphic System Towards Fast and\n  Energy-Efficient GAN", "comments": "8 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has gained immense success in pushing today's artificial\nintelligence forward. To solve the challenge of limited labeled data in the\nsupervised learning world, unsupervised learning has been proposed years ago\nwhile low accuracy hinters its realistic applications. Generative adversarial\nnetwork (GAN) emerges as an unsupervised learning approach with promising\naccuracy and are under extensively study. However, the execution of GAN is\nextremely memory and computation intensive and results in ultra-low speed and\nhigh-power consumption. In this work, we proposed a holistic solution for fast\nand energy-efficient GAN computation through a memristor-based neuromorphic\nsystem. First, we exploited a hardware and software co-design approach to map\nthe computation blocks in GAN efficiently. We also proposed an efficient data\nflow for optimal parallelism training and testing, depending on the computation\ncorrelations between different computing blocks. To compute the unique and\ncomplex loss of GAN, we developed a diff-block with optimized accuracy and\nperformance. The experiment results on big data show that our design achieves\n2.8x speedup and 6.1x energy-saving compared with the traditional GPU\naccelerator, as well as 5.5x speedup and 1.4x energy-saving compared with the\nprevious FPGA-based accelerator.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 02:45:38 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 06:00:55 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 02:32:16 GMT"}, {"version": "v4", "created": "Sun, 8 Sep 2019 08:46:12 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "F.", ""], ["Liu", "C.", ""], ["Bi", "F.", ""]]}, {"id": "1806.01794", "submitter": "Adam Kosiorek", "authors": "Adam R. Kosiorek, Hyunjik Kim, Ingmar Posner, Yee Whye Teh", "title": "Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects", "comments": "25 pages, 19 figures, NeurIPS 2018, code:\n  https://github.com/akosiorek/sqair, video: https://youtu.be/-IUNQgSLE0c", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep\ngenerative model for videos of moving objects. It can reliably discover and\ntrack objects throughout the sequence of frames, and can also generate future\nframes conditioning on the current frame, thereby simulating expected motion of\nobjects. This is achieved by explicitly encoding object presence, locations and\nappearances in the latent variables of the model. SQAIR retains all strengths\nof its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al., 2016),\nincluding learning in an unsupervised manner, and addresses its shortcomings.\nWe use a moving multi-MNIST dataset to show limitations of AIR in detecting\noverlapping or partially occluded objects, and show how SQAIR overcomes them by\nleveraging temporal consistency of objects. Finally, we also apply SQAIR to\nreal-world pedestrian CCTV data, where it learns to reliably detect, track and\ngenerate walking pedestrians with no supervision.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:29:44 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 16:08:23 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Kosiorek", "Adam R.", ""], ["Kim", "Hyunjik", ""], ["Posner", "Ingmar", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1806.01810", "submitter": "Xiaolong Wang", "authors": "Xiaolong Wang, Abhinav Gupta", "title": "Videos as Space-Time Region Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do humans recognize the action \"opening a book\" ? We argue that there are\ntwo important cues: modeling temporal shape dynamics and modeling functional\nrelationships between humans and objects. In this paper, we propose to\nrepresent videos as space-time region graphs which capture these two important\ncues. Our graph nodes are defined by the object region proposals from different\nframes in a long range video. These nodes are connected by two types of\nrelations: (i) similarity relations capturing the long range dependencies\nbetween correlated objects and (ii) spatial-temporal relations capturing the\ninteractions between nearby objects. We perform reasoning on this graph\nrepresentation via Graph Convolutional Networks. We achieve state-of-the-art\nresults on both Charades and Something-Something datasets. Especially for\nCharades, we obtain a huge 4.4% gain when our model is applied in complex\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:58:59 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 23:56:25 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Wang", "Xiaolong", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1806.01817", "submitter": "Felix Juefei-Xu", "authors": "Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides", "title": "Perturbative Neural Networks", "comments": "To appear in CVPR 2018. http://xujuefei.com/pnn.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are witnessing wide adoption in computer vision\nsystems with numerous applications across a range of visual recognition tasks.\nMuch of this progress is fueled through advances in convolutional neural\nnetwork architectures and learning algorithms even as the basic premise of a\nconvolutional layer has remained unchanged. In this paper, we seek to revisit\nthe convolutional layer that has been the workhorse of state-of-the-art visual\nrecognition models. We introduce a very simple, yet effective, module called a\nperturbation layer as an alternative to a convolutional layer. The perturbation\nlayer does away with convolution in the traditional sense and instead computes\nits response as a weighted linear combination of non-linearly activated\nadditive noise perturbed inputs. We demonstrate both analytically and\nempirically that this perturbation layer can be an effective replacement for a\nstandard convolutional layer. Empirically, deep neural networks with\nperturbation layers, called Perturbative Neural Networks (PNNs), in lieu of\nconvolutional layers perform comparably with standard CNNs on a range of visual\ndatasets (MNIST, CIFAR-10, PASCAL VOC, and ImageNet) with fewer parameters.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 17:15:08 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Juefei-Xu", "Felix", ""], ["Boddeti", "Vishnu Naresh", ""], ["Savvides", "Marios", ""]]}, {"id": "1806.01823", "submitter": "Luis Sanchez Giraldo", "authors": "Luis Gonzalo Sanchez Giraldo and Odelia Schwartz", "title": "Integrating Flexible Normalization into Mid-Level Representations of\n  Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are becoming increasingly popular\nmodels to predict neural responses in visual cortex. However, contextual\neffects, which are prevalent in neural processing and in perception, are not\nexplicitly handled by current CNNs, including those used for neural prediction.\nIn primary visual cortex, neural responses are modulated by stimuli spatially\nsurrounding the classical receptive field in rich ways. These effects have been\nmodeled with divisive normalization approaches, including flexible models,\nwhere spatial normalization is recruited only to the degree responses from\ncenter and surround locations are deemed statistically dependent. We propose a\nflexible normalization model applied to mid-level representations of deep CNNs\nas a tractable way to study contextual normalization mechanisms in mid-level\ncortical areas. This approach captures non-trivial spatial dependencies among\nmid-level features in CNNs, such as those present in textures and other visual\nstimuli, that arise from tiling high order features, geometrically. We expect\nthat the proposed approach can make predictions about when spatial\nnormalization might be recruited in mid-level cortical areas. We also expect\nthis approach to be useful as part of the CNN toolkit, therefore going beyond\nmore restrictive fixed forms of normalization.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 17:26:07 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 14:10:35 GMT"}, {"version": "v3", "created": "Mon, 24 Dec 2018 05:29:25 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Giraldo", "Luis Gonzalo Sanchez", ""], ["Schwartz", "Odelia", ""]]}, {"id": "1806.01873", "submitter": "Junwei Liang", "authors": "Junwei Liang, Lu Jiang, Liangliang Cao, Li-Jia Li, Alexander Hauptmann", "title": "Focal Visual-Text Attention for Visual Question Answering", "comments": "In CVPR 2018. Code, models and dataset are available here:\n  https://memexqa.cs.cmu.edu/", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2890628", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent insights on language and vision with neural networks have been\nsuccessfully applied to simple single-image visual question answering. However,\nto tackle real-life question answering problems on multimedia collections such\nas personal photos, we have to look at whole collections with sequences of\nphotos or videos. When answering questions from a large collection, a natural\nproblem is to identify snippets to support the answer. In this paper, we\ndescribe a novel neural network called Focal Visual-Text Attention network\n(FVTA) for collective reasoning in visual question answering, where both visual\nand text sequence information such as images and text metadata are presented.\nFVTA introduces an end-to-end approach that makes use of a hierarchical process\nto dynamically determine what media and what time to focus on in the sequential\ndata to answer the question. FVTA can not only answer the questions well but\nalso provides the justifications which the system results are based upon to get\nthe answers. FVTA achieves state-of-the-art performance on the MemexQA dataset\nand competitive results on the MovieQA dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 18:08:29 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 22:55:43 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Liang", "Junwei", ""], ["Jiang", "Lu", ""], ["Cao", "Liangliang", ""], ["Li", "Li-Jia", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1806.01896", "submitter": "Yuri G. Gordienko", "authors": "Vlad Taran, Nikita Gordienko, Yuriy Kochura, Yuri Gordienko, Alexandr\n  Rokovyi, Oleg Alienin, Sergii Stirenko", "title": "Performance Evaluation of Deep Learning Networks for Semantic\n  Segmentation of Traffic Stereo-Pair Images", "comments": "8 pages, 10 figures; accepted for presentation at 19-th International\n  Conference on Computer Systems and Technologies (CompSysTech'18) 13-14\n  September 2018, University of Ruse, Bulgaria", "journal-ref": "Proceedings of the 19th International Conference on Computer\n  Systems and Technologies (CompSysTech'18), Boris Rachev and Angel Smrikarov\n  (Eds.). ACM, New York, NY, USA, 73-80 (2018)", "doi": "10.1145/3274005.3274032", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation is one the most demanding task, especially for\nanalysis of traffic conditions for self-driving cars. Here the results of\napplication of several deep learning architectures (PSPNet and ICNet) for\nsemantic image segmentation of traffic stereo-pair images are presented. The\nimages from Cityscapes dataset and custom urban images were analyzed as to the\nsegmentation accuracy and image inference time. For the models pre-trained on\nCityscapes dataset, the inference time was equal in the limits of standard\ndeviation, but the segmentation accuracy was different for various cities and\nstereo channels even. The distributions of accuracy (mean intersection over\nunion - mIoU) values for each city and channel are asymmetric, long-tailed, and\nhave many extreme outliers, especially for PSPNet network in comparison to\nICNet network. Some statistical properties of these distributions (skewness,\nkurtosis) allow us to distinguish these two networks and open the question\nabout relations between architecture of deep learning networks and statistical\ndistribution of the predicted results (mIoU here). The results obtained\ndemonstrated the different sensitivity of these networks to: (1) the local\nstreet view peculiarities in different cities that should be taken into account\nduring the targeted fine tuning the models before their practical applications,\n(2) the right and left data channels in stereo-pairs. For both networks, the\ndifference in the predicted results (mIoU here) for the right and left data\nchannels in stereo-pairs is out of the limits of statistical error in relation\nto mIoU values. It means that the traffic stereo pairs can be effectively used\nnot only for depth calculations (as it is usually used), but also as an\nadditional data channel that can provide much more information about scene\nobjects than simple duplication of the same street view images.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 19:00:35 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Taran", "Vlad", ""], ["Gordienko", "Nikita", ""], ["Kochura", "Yuriy", ""], ["Gordienko", "Yuri", ""], ["Rokovyi", "Alexandr", ""], ["Alienin", "Oleg", ""], ["Stirenko", "Sergii", ""]]}, {"id": "1806.01907", "submitter": "Ahmed Mohammed", "authors": "Ahmed Mohammed, Sule Yildirim, Ivar Farup, Marius Pedersen and\n  {\\O}istein Hovde", "title": "Y-Net: A deep Convolutional Neural Network for Polyp Detection", "comments": "11 Pages, 3 figures", "journal-ref": "British Machine Vision Conference (BMVC), 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal polyps are important precursors to colon cancer, the third most\ncommon cause of cancer mortality for both men and women. It is a disease where\nearly detection is of crucial importance. Colonoscopy is commonly used for\nearly detection of cancer and precancerous pathology. It is a demanding\nprocedure requiring significant amount of time from specialized physicians and\nnurses, in addition to a significant miss-rates of polyps by specialists.\nAutomated polyp detection in colonoscopy videos has been demonstrated to be a\npromising way to handle this problem. {However, polyps detection is a\nchallenging problem due to the availability of limited amount of training data\nand large appearance variations of polyps. To handle this problem, we propose a\nnovel deep learning method Y-Net that consists of two encoder networks with a\ndecoder network. Our proposed Y-Net method} relies on efficient use of\npre-trained and un-trained models with novel sum-skip-concatenation operations.\nEach of the encoders are trained with encoder specific learning rate along the\ndecoder. Compared with the previous methods employing hand-crafted features or\n2-D/3-D convolutional neural network, our approach outperforms state-of-the-art\nmethods for polyp detection with 7.3% F1-score and 13% recall improvement.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 19:33:18 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Mohammed", "Ahmed", ""], ["Yildirim", "Sule", ""], ["Farup", "Ivar", ""], ["Pedersen", "Marius", ""], ["Hovde", "\u00d8istein", ""]]}, {"id": "1806.01911", "submitter": "Rakshith Shetty", "authors": "Rakshith Shetty, Mario Fritz, Bernt Schiele", "title": "Adversarial Scene Editing: Automatic Object Removal from Weak\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While great progress has been made recently in automatic image manipulation,\nit has been limited to object centric images like faces or structured scene\ndatasets. In this work, we take a step towards general scene-level image\nediting by developing an automatic interaction-free object removal model. Our\nmodel learns to find and remove objects from general scene images using\nimage-level labels and unpaired data in a generative adversarial network (GAN)\nframework. We achieve this with two key contributions: a two-stage editor\narchitecture consisting of a mask generator and image in-painter that\nco-operate to remove objects, and a novel GAN based prior for the mask\ngenerator that allows us to flexibly incorporate knowledge about object shapes.\nWe experimentally show on two datasets that our method effectively removes a\nwide variety of objects using weak supervision only\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 19:45:20 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Shetty", "Rakshith", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1806.01935", "submitter": "Andy Hess", "authors": "Andy Hess", "title": "Exploring Feature Reuse in DenseNet Architectures", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Densely Connected Convolutional Networks (DenseNets) have been shown to\nachieve state-of-the-art results on image classification tasks while using\nfewer parameters and computation than competing methods. Since each layer in\nthis architecture has full access to the feature maps of all previous layers,\nthe network is freed from the burden of having to relearn previously useful\nfeatures, thus alleviating issues with vanishing gradients. In this work we\nexplore the question: To what extent is it necessary to connect to all previous\nlayers in order to reap the benefits of feature reuse? To this end, we\nintroduce the notion of local dense connectivity and present evidence that less\nconnectivity, allowing for increased growth rate at a fixed network capacity,\ncan achieve a more efficient reuse of features and lead to higher accuracy in\ndense architectures.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 21:11:23 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Hess", "Andy", ""]]}, {"id": "1806.01954", "submitter": "Iulia Duta", "authors": "Iulia Duta, Andrei Liviu Nicolicioiu, Simion-Vlad Bogolin, Marius\n  Leordeanu", "title": "Mining for meaning: from vision to language through multiple networks\n  consensus", "comments": "Accepted at BMVC 2018", "journal-ref": "British Machine Vision Conference 2018, {BMVC} 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing visual data into natural language is a very challenging task, at\nthe intersection of computer vision, natural language processing and machine\nlearning. Language goes well beyond the description of physical objects and\ntheir interactions and can convey the same abstract idea in many ways. It is\nboth about content at the highest semantic level as well as about fluent form.\nHere we propose an approach to describe videos in natural language by reaching\na consensus among multiple encoder-decoder networks. Finding such a consensual\nlinguistic description, which shares common properties with a larger group, has\na better chance to convey the correct meaning. We propose and train several\nnetwork architectures and use different types of image, audio and video\nfeatures. Each model produces its own description of the input video and the\nbest one is chosen through an efficient, two-phase consensus process. We\ndemonstrate the strength of our approach by obtaining state of the art results\non the challenging MSR-VTT dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 22:50:09 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 08:53:12 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Duta", "Iulia", ""], ["Nicolicioiu", "Andrei Liviu", ""], ["Bogolin", "Simion-Vlad", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1806.01963", "submitter": "Simon Graham Mr", "authors": "Simon Graham, Hao Chen, Jevgenij Gamper, Qi Dou, Pheng-Ann Heng, David\n  Snead, Yee Wah Tsang, Nasir Rajpoot", "title": "MILD-Net: Minimal Information Loss Dilated Network for Gland Instance\n  Segmentation in Colon Histology Images", "comments": "Initial version published at Medical Imaging with Deep Learning\n  (MIDL) 2018", "journal-ref": "Medical Image Analysis vol. 52, pp. 199-211, Feb. 2019", "doi": "10.1016/j.media.2018.12.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of glandular morphology within colon histopathology images is an\nimportant step in determining the grade of colon cancer. Despite the importance\nof this task, manual segmentation is laborious, time-consuming and can suffer\nfrom subjectivity among pathologists. The rise of computational pathology has\nled to the development of automated methods for gland segmentation that aim to\novercome the challenges of manual segmentation. However, this task is\nnon-trivial due to the large variability in glandular appearance and the\ndifficulty in differentiating between certain glandular and non-glandular\nhistological structures. Furthermore, a measure of uncertainty is essential for\ndiagnostic decision making. To address these challenges, we propose a fully\nconvolutional neural network that counters the loss of information caused by\nmax-pooling by re-introducing the original image at multiple points within the\nnetwork. We also use atrous spatial pyramid pooling with varying dilation rates\nfor preserving the resolution and multi-level aggregation. To incorporate\nuncertainty, we introduce random transformations during test time for an\nenhanced segmentation result that simultaneously generates an uncertainty map,\nhighlighting areas of ambiguity. We show that this map can be used to define a\nmetric for disregarding predictions with high uncertainty. The proposed network\nachieves state-of-the-art performance on the GlaS challenge dataset and on a\nsecond independent colorectal adenocarcinoma dataset. In addition, we perform\ngland instance segmentation on whole-slide images from two further datasets to\nhighlight the generalisability of our method. As an extension, we introduce\nMILD-Net+ for simultaneous gland and lumen segmentation, to increase the\ndiagnostic power of the network.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 23:38:01 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 17:11:26 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2019 13:06:07 GMT"}, {"version": "v4", "created": "Mon, 18 Feb 2019 11:30:49 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Graham", "Simon", ""], ["Chen", "Hao", ""], ["Gamper", "Jevgenij", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""], ["Snead", "David", ""], ["Tsang", "Yee Wah", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1806.01977", "submitter": "Faqiang Wang", "authors": "Faqiang Wang, Cuicui Zhao, Jun Liu, Haiyang Huang", "title": "A Variational Image Segmentation Model based on Normalized Cut with\n  Adaptive Similarity and Spatial Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is a fundamental research topic in image processing and\ncomputer vision. In the last decades, researchers developed a large number of\nsegmentation algorithms for various applications. Amongst these algorithms, the\nNormalized cut (Ncut) segmentation method is widely applied due to its good\nperformance. The Ncut segmentation model is an optimization problem whose\nenergy is defined on a specifically designed graph. Thus, the segmentation\nresults of the existing Ncut method are largely dependent on a pre-constructed\nsimilarity measure on the graph since this measure is usually given empirically\nby users. This flaw will lead to some undesirable segmentation results. In this\npaper, we propose a Ncut-based segmentation algorithm by integrating an\nadaptive similarity measure and spatial regularization. The proposed model\ncombines the Parzen-Rosenblatt window method, non-local weights entropy, Ncut\nenergy, and regularizer of phase field in a variational framework. Our method\ncan adaptively update the similarity measure function by estimating some\nparameters. This adaptive procedure enables the proposed algorithm finding a\nbetter similarity measure for classification than the Ncut method. We provide\nsome mathematical interpretation of the proposed adaptive similarity from\nmulti-viewpoints such as statistics and convex optimization. In addition, the\nregularizer of phase field can guarantee that the proposed algorithm has a\nrobust performance in the presence of noise, and it can also rectify the\nsimilarity measure with a spatial priori. The well-posed theory such as the\nexistence of the minimizer for the proposed model is given in the paper.\nCompared with some existing segmentation methods such as the traditional\nNcut-based model and the classical Chan-Vese model, the numerical experiments\nshow that our method can provide promising segmentation results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 02:10:08 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 21:02:26 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 17:32:17 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Wang", "Faqiang", ""], ["Zhao", "Cuicui", ""], ["Liu", "Jun", ""], ["Huang", "Haiyang", ""]]}, {"id": "1806.01985", "submitter": "Mohammadreza Javanmardi", "authors": "Mohammadreza Javanmardi, Xiaojun Qi", "title": "Robust Structured Multi-task Multi-view Sparse Tracking", "comments": "IEEE International Conference on Multimedia and Expo (ICME), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation is a viable solution to visual tracking. In this paper,\nwe propose a structured multi-task multi-view tracking (SMTMVT) method, which\nexploits the sparse appearance model in the particle filter framework to track\ntargets under different challenges. Specifically, we extract features of the\ntarget candidates from different views and sparsely represent them by a linear\ncombination of templates of different views. Unlike the conventional sparse\ntrackers, SMTMVT not only jointly considers the relationship between different\ntasks and different views but also retains the structures among different views\nin a robust multi-task multi-view formulation. We introduce a numerical\nalgorithm based on the proximal gradient method to quickly and effectively find\nthe sparsity by dividing the optimization problem into two subproblems with the\nclosed-form solutions. Both qualitative and quantitative evaluations on the\nbenchmark of challenging image sequences demonstrate the superior performance\nof the proposed tracker against various state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 02:31:47 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Javanmardi", "Mohammadreza", ""], ["Qi", "Xiaojun", ""]]}, {"id": "1806.02003", "submitter": "Abhejit Rajagopal", "authors": "Abhejit Rajagopal, Shivkumar Chandrasekaran, Hrushikesh N. Mhaskar", "title": "Deep Algorithms: designs for networks", "comments": "submitted to Thirty-second Annual Conference on Neural Information\n  Processing Systems (NIPS), May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new design methodology for neural networks that is guided by traditional\nalgorithm design is presented. To prove our point, we present two heuristics\nand demonstrate an algorithmic technique for incorporating additional weights\nin their signal-flow graphs. We show that with training the performance of\nthese networks can not only exceed the performance of the initial network, but\ncan match the performance of more-traditional neural network architectures. A\nkey feature of our approach is that these networks are initialized with\nparameters that provide a known performance threshold for the architecture on a\ngiven task.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 04:39:37 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Rajagopal", "Abhejit", ""], ["Chandrasekaran", "Shivkumar", ""], ["Mhaskar", "Hrushikesh N.", ""]]}, {"id": "1806.02012", "submitter": "Uday Singh Saini", "authors": "Uday Singh Saini, Evangelos E. Papalexakis", "title": "A Peek Into the Hidden Layers of a Convolutional Neural Network Through\n  a Factorization Lens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their increasing popularity and success in a variety of supervised\nlearning problems, deep neural networks are extremely hard to interpret and\ndebug: Given and already trained Deep Neural Net, and a set of test inputs, how\ncan we gain insight into how those inputs interact with different layers of the\nneural network? Furthermore, can we characterize a given deep neural network\nbased on it's observed behavior on different inputs? In this paper we propose a\nnovel factorization based approach on understanding how different deep neural\nnetworks operate. In our preliminary results, we identify fascinating patterns\nthat link the factorization rank (typically used as a measure of\ninterestingness in unsupervised data analysis) with how well or poorly the deep\nnetwork has been trained. Finally, our proposed approach can help provide\nvisual insights on how high-level. interpretable patterns of the network's\ninput behave inside the hidden layers of the deep network.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 05:27:38 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Saini", "Uday Singh", ""], ["Papalexakis", "Evangelos E.", ""]]}, {"id": "1806.02023", "submitter": "Jia-Hong Lee", "authors": "Jia-Hong Lee, Yi-Ming Chan, Ting-Yen Chen, and Chu-Song Chen", "title": "Joint Estimation of Age and Gender from Unconstrained Face Images using\n  Lightweight Multi-task CNN for Mobile Applications", "comments": "To publish in the IEEE first International Conference on Multimedia\n  Information Processing and Retrieval, 2018. (IEEE MIPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic age and gender classification based on unconstrained images has\nbecome essential techniques on mobile devices. With limited computing power,\nhow to develop a robust system becomes a challenging task. In this paper, we\npresent an efficient convolutional neural network (CNN) called lightweight\nmulti-task CNN for simultaneous age and gender classification. Lightweight\nmulti-task CNN uses depthwise separable convolution to reduce the model size\nand save the inference time. On the public challenging Adience dataset, the\naccuracy of age and gender classification is better than baseline multi-task\nCNN methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 06:22:16 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Lee", "Jia-Hong", ""], ["Chan", "Yi-Ming", ""], ["Chen", "Ting-Yen", ""], ["Chen", "Chu-Song", ""]]}, {"id": "1806.02031", "submitter": "Moazzem Hossain", "authors": "Moazzem Hossain, Soichi Nishio, Takafumi Hiranaka and Syoji Kobashi", "title": "Real-time Surgical Tools Recognition in Total Knee Arthroplasty Using\n  Deep Neural Networks", "comments": "Accepted in ICIEV 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Total knee arthroplasty (TKA) is a commonly performed surgical procedure to\nmitigate knee pain and improve functions for people with knee arthritis. The\nprocedure is complicated due to the different surgical tools used in the stages\nof surgery. The recognition of surgical tools in real-time can be a solution to\nsimplify surgical procedures for the surgeon. Also, the presence and movement\nof tools in surgery are crucial information for the recognition of the\noperational phase and to identify the surgical workflow. Therefore, this\nresearch proposes the development of a real-time system for the recognition of\nsurgical tools during surgery using a convolutional neural network (CNN).\nSurgeons wearing smart glasses can see essential information about tools during\nsurgery that may reduce the complication of the procedures. To evaluate the\nperformance of the proposed method, we calculated and compared the Mean Average\nPrecision (MAP) with state-of-the-art methods which are fast R-CNN and\ndeformable part models (DPM). We achieved 87.6% mAP which is better in\ncomparison to the existing methods. With the additional improvements of our\nproposed method, it can be a future point of reference, also the baseline for\noperational phase recognition.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 06:52:48 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Hossain", "Moazzem", ""], ["Nishio", "Soichi", ""], ["Hiranaka", "Takafumi", ""], ["Kobashi", "Syoji", ""]]}, {"id": "1806.02051", "submitter": "Lena Maier-Hein", "authors": "Lena Maier-Hein, Matthias Eisenmann, Annika Reinke, Sinan Onogur,\n  Marko Stankovic, Patrick Scholz, Tal Arbel, Hrvoje Bogunovic, Andrew P.\n  Bradley, Aaron Carass, Carolin Feldmann, Alejandro F. Frangi, Peter M. Full,\n  Bram van Ginneken, Allan Hanbury, Katrin Honauer, Michal Kozubek, Bennett A.\n  Landman, Keno M\\\"arz, Oskar Maier, Klaus Maier-Hein, Bjoern H. Menze, Henning\n  M\\\"uller, Peter F. Neher, Wiro Niessen, Nasir Rajpoot, Gregory C. Sharp,\n  Korsuk Sirinukunwattana, Stefanie Speidel, Christian Stock, Danail Stoyanov,\n  Abdel Aziz Taha, Fons van der Sommen, Ching-Wei Wang, Marc-Andr\\'e Weber,\n  Guoyan Zheng, Pierre Jannin, Annette Kopp-Schneider (shared first/last\n  authors)", "title": "Why rankings of biomedical image analysis competitions should be\n  interpreted with care", "comments": "Article published in Nature Communications: https://rdcu.be/bRmNr", "journal-ref": "Nature communications 9.1 (2018): 5217", "doi": "10.1038/s41467-018-07619-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  International challenges have become the standard for validation of\nbiomedical image analysis methods. Given their scientific impact, it is\nsurprising that a critical analysis of common practices related to the\norganization of challenges has not yet been performed. In this paper, we\npresent a comprehensive analysis of biomedical image analysis challenges\nconducted up to now. We demonstrate the importance of challenges and show that\nthe lack of quality control has critical consequences. First, reproducibility\nand interpretation of the results is often hampered as only a fraction of\nrelevant information is typically provided. Second, the rank of an algorithm is\ngenerally not robust to a number of variables such as the test data used for\nvalidation, the ranking scheme applied and the observers that make the\nreference annotations. To overcome these problems, we recommend best practice\nguidelines and define open research questions to be addressed in the future.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:13:27 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 11:32:07 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Maier-Hein", "Lena", "", "shared first/last\n  authors"], ["Eisenmann", "Matthias", "", "shared first/last\n  authors"], ["Reinke", "Annika", "", "shared first/last\n  authors"], ["Onogur", "Sinan", "", "shared first/last\n  authors"], ["Stankovic", "Marko", "", "shared first/last\n  authors"], ["Scholz", "Patrick", "", "shared first/last\n  authors"], ["Arbel", "Tal", "", "shared first/last\n  authors"], ["Bogunovic", "Hrvoje", "", "shared first/last\n  authors"], ["Bradley", "Andrew P.", "", "shared first/last\n  authors"], ["Carass", "Aaron", "", "shared first/last\n  authors"], ["Feldmann", "Carolin", "", "shared first/last\n  authors"], ["Frangi", "Alejandro F.", "", "shared first/last\n  authors"], ["Full", "Peter M.", "", "shared first/last\n  authors"], ["van Ginneken", "Bram", "", "shared first/last\n  authors"], ["Hanbury", "Allan", "", "shared first/last\n  authors"], ["Honauer", "Katrin", "", "shared first/last\n  authors"], ["Kozubek", "Michal", "", "shared first/last\n  authors"], ["Landman", "Bennett A.", "", "shared first/last\n  authors"], ["M\u00e4rz", "Keno", "", "shared first/last\n  authors"], ["Maier", "Oskar", "", "shared first/last\n  authors"], ["Maier-Hein", "Klaus", "", "shared first/last\n  authors"], ["Menze", "Bjoern H.", "", "shared first/last\n  authors"], ["M\u00fcller", "Henning", "", "shared first/last\n  authors"], ["Neher", "Peter F.", "", "shared first/last\n  authors"], ["Niessen", "Wiro", "", "shared first/last\n  authors"], ["Rajpoot", "Nasir", "", "shared first/last\n  authors"], ["Sharp", "Gregory C.", "", "shared first/last\n  authors"], ["Sirinukunwattana", "Korsuk", "", "shared first/last\n  authors"], ["Speidel", "Stefanie", "", "shared first/last\n  authors"], ["Stock", "Christian", "", "shared first/last\n  authors"], ["Stoyanov", "Danail", "", "shared first/last\n  authors"], ["Taha", "Abdel Aziz", "", "shared first/last\n  authors"], ["van der Sommen", "Fons", "", "shared first/last\n  authors"], ["Wang", "Ching-Wei", "", "shared first/last\n  authors"], ["Weber", "Marc-Andr\u00e9", "", "shared first/last\n  authors"], ["Zheng", "Guoyan", "", "shared first/last\n  authors"], ["Jannin", "Pierre", "", "shared first/last\n  authors"], ["Kopp-Schneider", "Annette", "", "shared first/last\n  authors"]]}, {"id": "1806.02067", "submitter": "Ekta Prashnani", "authors": "Ekta Prashnani, Hong Cai, Yasamin Mostofi, Pradeep Sen", "title": "PieAPP: Perceptual Image-Error Assessment through Pairwise Preference", "comments": "8 pages; 5 figures; proceedings of CVPR 2018", "journal-ref": "E. Prashnani, H. Cai, Y. Mostofi and P. Sen. PieAPP: Perceptual\n  Image-Error Assessment through Pairwise Preference. In Proceedings of the\n  IEEE Conference on Computer Vision and Pattern Recognition, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to estimate the perceptual error between images is an important\nproblem in computer vision with many applications. Although it has been studied\nextensively, however, no method currently exists that can robustly predict\nvisual differences like humans. Some previous approaches used hand-coded\nmodels, but they fail to model the complexity of the human visual system.\nOthers used machine learning to train models on human-labeled datasets, but\ncreating large, high-quality datasets is difficult because people are unable to\nassign consistent error labels to distorted images. In this paper, we present a\nnew learning-based method that is the first to predict perceptual image error\nlike human observers. Since it is much easier for people to compare two given\nimages and identify the one more similar to a reference than to assign quality\nscores to each, we propose a new, large-scale dataset labeled with the\nprobability that humans will prefer one image over another. We then train a\ndeep-learning model using a novel, pairwise-learning framework to predict the\npreference of one distorted image over the other. Our key observation is that\nour trained network can then be used separately with only one distorted image\nand a reference to predict its perceptual error, without ever being trained on\nexplicit human perceptual-error labels. The perceptual error estimated by our\nnew metric, PieAPP, is well-correlated with human opinion. Furthermore, it\nsignificantly outperforms existing algorithms, beating the state-of-the-art by\nalmost 3x on our test set in terms of binary error rate, while also\ngeneralizing to new kinds of distortions, unlike previous learning-based\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:50:53 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Prashnani", "Ekta", ""], ["Cai", "Hong", ""], ["Mostofi", "Yasamin", ""], ["Sen", "Pradeep", ""]]}, {"id": "1806.02070", "submitter": "Christian Payer", "authors": "Christian Payer, Darko \\v{S}tern, Thomas Neff, Horst Bischof, Martin\n  Urschler", "title": "Instance Segmentation and Tracking with Cosine Embeddings and Recurrent\n  Hourglass Networks", "comments": "Accepted for ORAL presentation at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different to semantic segmentation, instance segmentation assigns unique\nlabels to each individual instance of the same class. In this work, we propose\na novel recurrent fully convolutional network architecture for tracking such\ninstance segmentations over time. The network architecture incorporates\nconvolutional gated recurrent units (ConvGRU) into a stacked hourglass network\nto utilize temporal video information. Furthermore, we train the network with a\nnovel embedding loss based on cosine similarities, such that the network\npredicts unique embeddings for every instance throughout videos. Afterwards,\nthese embeddings are clustered among subsequent video frames to create the\nfinal tracked instance segmentations. We evaluate the recurrent hourglass\nnetwork by segmenting left ventricles in MR videos of the heart, where it\noutperforms a network that does not incorporate video information. Furthermore,\nwe show applicability of the cosine embedding loss for segmenting leaf\ninstances on still images of plants. Finally, we evaluate the framework for\ninstance segmentation and tracking on six datasets of the ISBI celltracking\nchallenge, where it shows state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:57:15 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 07:11:38 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2018 09:32:56 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Payer", "Christian", ""], ["\u0160tern", "Darko", ""], ["Neff", "Thomas", ""], ["Bischof", "Horst", ""], ["Urschler", "Martin", ""]]}, {"id": "1806.02121", "submitter": "Jonathan Laserson", "authors": "Jonathan Laserson, Christine Dan Lantsman, Michal Cohen-Sfady, Itamar\n  Tamir, Eli Goz, Chen Brestel, Shir Bar, Maya Atar, Eldad Elnekave", "title": "TextRay: Mining Clinical Reports to Gain a Broad Understanding of Chest\n  X-rays", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chest X-ray (CXR) is by far the most commonly performed radiological\nexamination for screening and diagnosis of many cardiac and pulmonary diseases.\nThere is an immense world-wide shortage of physicians capable of providing\nrapid and accurate interpretation of this study. A radiologist-driven analysis\nof over two million CXR reports generated an ontology including the 40 most\nprevalent pathologies on CXR. By manually tagging a relatively small set of\nsentences, we were able to construct a training set of 959k studies. A deep\nlearning model was trained to predict the findings given the patient frontal\nand lateral scans. For 12 of the findings we compare the model performance\nagainst a team of radiologists and show that in most cases the radiologists\nagree on average more with the algorithm than with each other.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 11:17:59 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Laserson", "Jonathan", ""], ["Lantsman", "Christine Dan", ""], ["Cohen-Sfady", "Michal", ""], ["Tamir", "Itamar", ""], ["Goz", "Eli", ""], ["Brestel", "Chen", ""], ["Bar", "Shir", ""], ["Atar", "Maya", ""], ["Elnekave", "Eldad", ""]]}, {"id": "1806.02132", "submitter": "Yishuo Zhang", "authors": "Yishuo Zhang and Albert C.S. Chung", "title": "Deep supervision with additional labels for retinal vessel segmentation\n  task", "comments": "Accepted be MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00934-2_10", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic analysis of retinal blood images is of vital importance in\ndiagnosis tasks of retinopathy. Segmenting vessels accurately is a fundamental\nstep in analysing retinal images. However, it is usually difficult due to\nvarious imaging conditions, low image contrast and the appearance of\npathologies such as micro-aneurysms. In this paper, we propose a novel method\nwith deep neural networks to solve this problem. We utilize U-net with residual\nconnection to detect vessels. To achieve better accuracy, we introduce an\nedge-aware mechanism, in which we convert the original task into a multi-class\ntask by adding additional labels on boundary areas. In this way, the network\nwill pay more attention to the boundary areas of vessels and achieve a better\nperformance, especially in tiny vessels detecting. Besides, side output layers\nare applied in order to give deep supervision and therefore help convergence.\nWe train and evaluate our model on three databases: DRIVE, STARE, and CHASEDB1.\nExperimental results show that our method has a comparable performance with AUC\nof 97.99% on DRIVE and an efficient running time compared to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 11:41:01 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 07:17:49 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 07:11:08 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Zhang", "Yishuo", ""], ["Chung", "Albert C. S.", ""]]}, {"id": "1806.02143", "submitter": "Heli Ben-Hamu", "authors": "Heli Ben-Hamu, Haggai Maron, Itay Kezurer, Gal Avineri and Yaron\n  Lipman", "title": "Multi-chart Generative Surface Modeling", "comments": null, "journal-ref": "ACM Trans. Graph. 37, 6, Article 215 (December 2018)", "doi": "10.1145/3272127.3275052", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a 3D shape generative model based on deep neural\nnetworks. A new image-like (i.e., tensor) data representation for genus-zero 3D\nshapes is devised. It is based on the observation that complicated shapes can\nbe well represented by multiple parameterizations (charts), each focusing on a\ndifferent part of the shape. The new tensor data representation is used as\ninput to Generative Adversarial Networks for the task of 3D shape generation.\nThe 3D shape tensor representation is based on a multi-chart structure that\nenjoys a shape covering property and scale-translation rigidity.\nScale-translation rigidity facilitates high quality 3D shape learning and\nguarantees unique reconstruction. The multi-chart structure uses as input a\ndataset of 3D shapes (with arbitrary connectivity) and a sparse correspondence\nbetween them. The output of our algorithm is a generative model that learns the\nshape distribution and is able to generate novel shapes, interpolate shapes,\nand explore the generated shape space. The effectiveness of the method is\ndemonstrated for the task of anatomic shape generation including human body and\nbone (teeth) shape generation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 12:25:10 GMT"}, {"version": "v2", "created": "Sun, 27 Jan 2019 13:56:16 GMT"}, {"version": "v3", "created": "Sun, 3 Mar 2019 14:12:52 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Ben-Hamu", "Heli", ""], ["Maron", "Haggai", ""], ["Kezurer", "Itay", ""], ["Avineri", "Gal", ""], ["Lipman", "Yaron", ""]]}, {"id": "1806.02170", "submitter": "Aseem Behl", "authors": "Aseem Behl, Despoina Paschalidou, Simon Donn\\'e, Andreas Geiger", "title": "PointFlowNet: Learning Representations for Rigid Motion Estimation from\n  Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in image-based 3D scene flow estimation, the\nperformance of such approaches has not yet reached the fidelity required by\nmany applications. Simultaneously, these applications are often not restricted\nto image-based estimation: laser scanners provide a popular alternative to\ntraditional cameras, for example in the context of self-driving cars, as they\ndirectly yield a 3D point cloud. In this paper, we propose to estimate 3D\nmotion from such unstructured point clouds using a deep neural network. In a\nsingle forward pass, our model jointly predicts 3D scene flow as well as the 3D\nbounding box and rigid body motion of objects in the scene. While the prospect\nof estimating 3D scene flow from unstructured point clouds is promising, it is\nalso a challenging task. We show that the traditional global representation of\nrigid body motion prohibits inference by CNNs, and propose a translation\nequivariant representation to circumvent this problem. For training our deep\nnetwork, a large dataset is required. Because of this, we augment real scans\nfrom KITTI with virtual objects, realistically modeling occlusions and\nsimulating sensor noise. A thorough comparison with classic and learning-based\ntechniques highlights the robustness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 13:24:30 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 14:20:26 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2019 13:01:03 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Behl", "Aseem", ""], ["Paschalidou", "Despoina", ""], ["Donn\u00e9", "Simon", ""], ["Geiger", "Andreas", ""]]}, {"id": "1806.02237", "submitter": "Holger Roth", "authors": "Holger R. Roth, Chen Shen, Hirohisa Oda, Takaaki Sugino, Masahiro Oda,\n  Yuichiro Hayashi, Kazunari Misawa, Kensaku Mori", "title": "A multi-scale pyramid of 3D fully convolutional networks for abdominal\n  multi-organ segmentation", "comments": "Accepted for presentation at the 21st International Conference on\n  Medical Image Computing and Computer Assisted Intervention - MICCAI 2018,\n  September 16-20, Granada, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning, like 3D fully convolutional networks\n(FCNs), have improved the state-of-the-art in dense semantic segmentation of\nmedical images. However, most network architectures require severely\ndownsampling or cropping the images to meet the memory limitations of today's\nGPU cards while still considering enough context in the images for accurate\nsegmentation. In this work, we propose a novel approach that utilizes\nauto-context to perform semantic segmentation at higher resolutions in a\nmulti-scale pyramid of stacked 3D FCNs. We train and validate our models on a\ndataset of manually annotated abdominal organs and vessels from 377 clinical CT\nimages used in gastric surgery, and achieve promising results with close to 90%\nDice score on average. For additional evaluation, we perform separate testing\non datasets from different sources and achieve competitive results,\nillustrating the robustness of the model and approach.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 15:11:00 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Roth", "Holger R.", ""], ["Shen", "Chen", ""], ["Oda", "Hirohisa", ""], ["Sugino", "Takaaki", ""], ["Oda", "Masahiro", ""], ["Hayashi", "Yuichiro", ""], ["Misawa", "Kazunari", ""], ["Mori", "Kensaku", ""]]}, {"id": "1806.02279", "submitter": "Seung Yeon Shin", "authors": "Seung Yeon Shin, Soochahn Lee, Il Dong Yun, Kyoung Mu Lee", "title": "Deep Vessel Segmentation By Learning Graphical Connectivity", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2019.101556", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep-learning-based system for vessel segmentation.\nExisting methods using CNNs have mostly relied on local appearances learned on\nthe regular image grid, without considering the graphical structure of vessel\nshape. To address this, we incorporate a graph convolutional network into a\nunified CNN architecture, where the final segmentation is inferred by combining\nthe different types of features. The proposed method can be applied to expand\nany type of CNN-based vessel segmentation method to enhance the performance.\nExperiments show that the proposed method outperforms the current\nstate-of-the-art methods on two retinal image datasets as well as a coronary\nartery X-ray angiography dataset.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 16:16:52 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Shin", "Seung Yeon", ""], ["Lee", "Soochahn", ""], ["Yun", "Il Dong", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1806.02284", "submitter": "Michele Dolfi", "authors": "Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas", "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest\n  Documents at Scale", "comments": "Accepted paper at KDD 2018 conference", "journal-ref": null, "doi": "10.1145/3219819.3219834", "report-no": null, "categories": "cs.DL cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make the\ncontained knowledge discoverable. Unfortunately, both the format of these\ndocuments (e.g. the PDF format or bitmap images) as well as the presentation of\nthe data (e.g. complex tables) make the extraction of qualitative and\nquantitive data extremely challenging. In this paper, we present a modular,\ncloud-based platform to ingest documents at scale. This platform, called the\nCorpus Conversion Service (CCS), implements a pipeline which allows users to\nparse and annotate documents (i.e. collect ground-truth), train\nmachine-learning classification algorithms and ultimately convert any type of\nPDF or bitmap-documents to a structured content representation format. We will\nshow that each of the modules is scalable due to an asynchronous microservice\narchitecture and can therefore handle massive amounts of documents.\nFurthermore, we will show that our capability to gather ground-truth is\naccelerated by machine-learning algorithms by at least one order of magnitude.\nThis allows us to both gather large amounts of ground-truth in very little time\nand obtain very good precision/recall metrics in the range of 99\\% with regard\nto content conversion to structured output. The CCS platform is currently\ndeployed on IBM internal infrastructure and serving more than 250 active users\nfor knowledge-engineering project engagements.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 09:44:07 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Staar", "Peter W J", ""], ["Dolfi", "Michele", ""], ["Auer", "Christoph", ""], ["Bekas", "Costas", ""]]}, {"id": "1806.02285", "submitter": "Samuel Kadoury", "authors": "William Mandel, Olivier Turcot, Dejan Knez, Stefan Parent, Samuel\n  Kadoury", "title": "Spatiotemporal Manifold Prediction Model for Anterior Vertebral Body\n  Growth Modulation Surgery in Idiopathic Scoliosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anterior Vertebral Body Growth Modulation (AVBGM) is a minimally invasive\nsurgical technique that gradually corrects spine deformities while preserving\nlumbar motion. However the selection of potential surgical patients is\ncurrently based on clinical judgment and would be facilitated by the\nidentification of patients responding to AVBGM prior to surgery. We introduce a\nstatistical framework for predicting the surgical outcomes following AVBGM in\nadolescents with idiopathic scoliosis. A discriminant manifold is first\nconstructed to maximize the separation between responsive and non-responsive\ngroups of patients treated with AVBGM for scoliosis. The model then uses\nsubject-specific correction trajectories based on articulated transformations\nin order to map spine correction profiles to a group-average piecewise-geodesic\npath. Spine correction trajectories are described in a piecewise-geodesic\nfashion to account for varying times at follow-up exams, regressing the curve\nvia a quadratic optimization process. To predict the evolution of correction, a\nbaseline reconstruction is projected onto the manifold, from which a\nspatiotemporal regression model is built from parallel transport curves\ninferred from neighboring exemplars. The model was trained on 438\nreconstructions and tested on 56 subjects using 3D spine reconstructions from\nfollow-up exams, with the probabilistic framework yielding accurate results\nwith differences of 2.1 +/- 0.6deg in main curve angulation, and generating\nmodels similar to biomechanical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 16:24:28 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Mandel", "William", ""], ["Turcot", "Olivier", ""], ["Knez", "Dejan", ""], ["Parent", "Stefan", ""], ["Kadoury", "Samuel", ""]]}, {"id": "1806.02296", "submitter": "Philip Schniter", "authors": "Edward T. Reehorst and Philip Schniter", "title": "Regularization by Denoising: Clarifications and New Interpretations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization by Denoising (RED), as recently proposed by Romano, Elad, and\nMilanfar, is powerful image-recovery framework that aims to minimize an\nexplicit regularization objective constructed from a plug-in image-denoising\nfunction. Experimental evidence suggests that the RED algorithms are\nstate-of-the-art. We claim, however, that explicit regularization does not\nexplain the RED algorithms. In particular, we show that many of the expressions\nin the paper by Romano et al. hold only when the denoiser has a symmetric\nJacobian, and we demonstrate that such symmetry does not occur with practical\ndenoisers such as non-local means, BM3D, TNRD, and DnCNN. To explain the RED\nalgorithms, we propose a new framework called Score-Matching by Denoising\n(SMD), which aims to match a \"score\" (i.e., the gradient of a log-prior). We\nthen show tight connections between SMD, kernel density estimation, and\nconstrained minimum mean-squared error denoising. Furthermore, we interpret the\nRED algorithms from Romano et al. and propose new algorithms with acceleration\nand convergence guarantees. Finally, we show that the RED algorithms seek a\nconsensus equilibrium solution, which facilitates a comparison to plug-and-play\nADMM.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 16:49:59 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 06:14:05 GMT"}, {"version": "v3", "created": "Tue, 25 Sep 2018 15:03:44 GMT"}, {"version": "v4", "created": "Thu, 1 Nov 2018 04:09:07 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Reehorst", "Edward T.", ""], ["Schniter", "Philip", ""]]}, {"id": "1806.02299", "submitter": "Ziwei Liu", "authors": "Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Hai Li, Yiran Chen", "title": "DPatch: An Adversarial Patch Attack on Object Detectors", "comments": "AAAI Workshop on Artificial Intelligence Safety (SafeAI 2019) Oral\n  Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detectors have emerged as an indispensable module in modern computer\nvision systems. In this work, we propose DPatch -- a black-box\nadversarial-patch-based attack towards mainstream object detectors (i.e. Faster\nR-CNN and YOLO). Unlike the original adversarial patch that only manipulates\nimage-level classifier, our DPatch simultaneously attacks the bounding box\nregression and object classification so as to disable their predictions.\nCompared to prior works, DPatch has several appealing properties: (1) DPatch\ncan perform both untargeted and targeted effective attacks, degrading the mAP\nof Faster R-CNN and YOLO from 75.10% and 65.7% down to below 1%, respectively.\n(2) DPatch is small in size and its attacking effect is location-independent,\nmaking it very practical to implement real-world attacks. (3) DPatch\ndemonstrates great transferability among different detectors as well as\ntraining datasets. For example, DPatch that is trained on Faster R-CNN can\neffectively attack YOLO, and vice versa. Extensive evaluations imply that\nDPatch can perform effective attacks under black-box setup, i.e., even without\nthe knowledge of the attacked network's architectures and parameters.\nSuccessful realization of DPatch also illustrates the intrinsic vulnerability\nof the modern detector architectures to such patch-based adversarial attacks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 17:04:37 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 01:25:05 GMT"}, {"version": "v3", "created": "Sat, 15 Sep 2018 03:08:34 GMT"}, {"version": "v4", "created": "Tue, 23 Apr 2019 16:44:32 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Liu", "Xin", ""], ["Yang", "Huanrui", ""], ["Liu", "Ziwei", ""], ["Song", "Linghao", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1806.02305", "submitter": "Samuel Kadoury", "authors": "Marc-Antoine Boucher, Sarah Lippe, Amelie Damphousse, Ramy El-Jalbout,\n  Samuel Kadoury", "title": "Dilatation of Lateral Ventricles with Brain Volumes in Infants with 3D\n  Transfontanelle US", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) can be used to assess brain development in newborns, as MRI\nis challenging due to immobilization issues, and may require sedation.\nDilatation of the lateral ventricles in the brain is a risk factor for poorer\nneurodevelopment outcomes in infants. Hence, 3D US has the ability to assess\nthe volume of the lateral ventricles similar to clinically standard MRI, but\nmanual segmentation is time consuming. The objective of this study is to\ndevelop an approach quantifying the ratio of lateral ventricular dilatation\nwith respect to total brain volume using 3D US, which can assess the severity\nof macrocephaly. Automatic segmentation of the lateral ventricles is achieved\nwith a multi-atlas deformable registration approach using locally linear\ncorrelation metrics for US-MRI fusion, followed by a refinement step using\ndeformable mesh models. Total brain volume is estimated using a 3D ellipsoid\nmodeling approach. Validation was performed on a cohort of 12 infants, ranging\nfrom 2 to 8.5 months old, where 3D US and MRI were used to compare brain\nvolumes and segmented lateral ventricles. Automatically extracted volumes from\n3D US show a high correlation and no statistically significant difference when\ncompared to ground truth measurements. Differences in volume ratios was 6.0 +/-\n4.8% compared to MRI, while lateral ventricular segmentation yielded a mean\nDice coefficient of 70.8 +/- 3.6% and a mean absolute distance (MAD) of 0.88\n+/- 0.2mm, demonstrating the clinical benefit of this tool in paediatric\nultrasound.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:10:06 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Boucher", "Marc-Antoine", ""], ["Lippe", "Sarah", ""], ["Damphousse", "Amelie", ""], ["El-Jalbout", "Ramy", ""], ["Kadoury", "Samuel", ""]]}, {"id": "1806.02311", "submitter": "Youssef Alami Mejjati", "authors": "Youssef A. Mejjati and Christian Richardt and James Tompkin and Darren\n  Cosker and Kwang In Kim", "title": "Unsupervised Attention-guided Image to Image Translation", "comments": null, "journal-ref": "NIPS 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current unsupervised image-to-image translation techniques struggle to focus\ntheir attention on individual objects without altering the background or the\nway multiple objects interact within a scene. Motivated by the important role\nof attention in human perception, we tackle this limitation by introducing\nunsupervised attention mechanisms that are jointly adversarialy trained with\nthe generators and discriminators. We demonstrate qualitatively and\nquantitatively that our approach is able to attend to relevant regions in the\nimage without requiring supervision, and that by doing so it achieves more\nrealistic mappings compared to recent approaches.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:21:38 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 21:13:13 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 14:43:37 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Mejjati", "Youssef A.", ""], ["Richardt", "Christian", ""], ["Tompkin", "James", ""], ["Cosker", "Darren", ""], ["Kim", "Kwang In", ""]]}, {"id": "1806.02318", "submitter": "S\\'ergio Pereira", "authors": "S\\'ergio Pereira, Victor Alves and Carlos A. Silva", "title": "Adaptive feature recombination and recalibration for semantic\n  segmentation: application to brain tumor segmentation in MRI", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00931-1_81", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been successfully used for brain\ntumor segmentation, specifically, fully convolutional networks (FCNs). FCNs can\nsegment a set of voxels at once, having a direct spatial correspondence between\nunits in feature maps (FMs) at a given location and the corresponding\nclassified voxels. In convolutional layers, FMs are merged to create new FMs,\nso, channel combination is crucial. However, not all FMs have the same\nrelevance for a given class. Recently, in classification problems,\nSqueeze-and-Excitation (SE) blocks have been proposed to re-calibrate FMs as a\nwhole, and suppress the less informative ones. However, this is not optimal in\nFCN due to the spatial correspondence between units and voxels. In this\narticle, we propose feature recombination through linear expansion and\ncompression to create more complex features for semantic segmentation.\nAdditionally, we propose a segmentation SE (SegSE) block for feature\nrecalibration that collects contextual information, while maintaining the\nspatial meaning. Finally, we evaluate the proposed methods in brain tumor\nsegmentation, using publicly available data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:37:15 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Pereira", "S\u00e9rgio", ""], ["Alves", "Victor", ""], ["Silva", "Carlos A.", ""]]}, {"id": "1806.02323", "submitter": "Yi-Hsuan Tsai", "authors": "Jingchun Cheng, Yi-Hsuan Tsai, Wei-Chih Hung, Shengjin Wang,\n  Ming-Hsuan Yang", "title": "Fast and Accurate Online Video Object Segmentation via Tracking Parts", "comments": "Accepted in CVPR'18 as Spotlight. Code and model are available at\n  https://github.com/JingchunCheng/FAVOS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online video object segmentation is a challenging task as it entails to\nprocess the image sequence timely and accurately. To segment a target object\nthrough the video, numerous CNN-based methods have been developed by heavily\nfinetuning on the object mask in the first frame, which is time-consuming for\nonline applications. In this paper, we propose a fast and accurate video object\nsegmentation algorithm that can immediately start the segmentation process once\nreceiving the images. We first utilize a part-based tracking method to deal\nwith challenging factors such as large deformation, occlusion, and cluttered\nbackground. Based on the tracked bounding boxes of parts, we construct a\nregion-of-interest segmentation network to generate part masks. Finally, a\nsimilarity-based scoring function is adopted to refine these object parts by\ncomparing them to the visual information in the first frame. Our method\nperforms favorably against state-of-the-art algorithms in accuracy on the DAVIS\nbenchmark dataset, while achieving much faster runtime performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:43:13 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Cheng", "Jingchun", ""], ["Tsai", "Yi-Hsuan", ""], ["Hung", "Wei-Chih", ""], ["Wang", "Shengjin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1806.02336", "submitter": "Naoyuki Ichimura Dr.", "authors": "Naoyuki Ichimura", "title": "Spatial Frequency Loss for Learning Convolutional Autoencoders", "comments": "9 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a learning method for convolutional autoencoders (CAEs)\nfor extracting features from images. CAEs can be obtained by utilizing\nconvolutional neural networks to learn an approximation to the identity\nfunction in an unsupervised manner. The loss function based on the pixel loss\n(PL) that is the mean squared error between the pixel values of original and\nreconstructed images is the common choice for learning. However, using the loss\nfunction leads to blurred reconstructed images. A method for learning CAEs\nusing a loss function computed from features reflecting spatial frequencies is\nproposed to mitigate the problem. The blurs in reconstructed images show lack\nof high spatial frequency components mainly constituting edges and detailed\ntextures that are important features for tasks such as object detection and\nspatial matching. In order to evaluate the lack of components, a convolutional\nlayer with a Laplacian filter bank as weights is added to CAEs and the mean\nsquared error of features in a subband, called the spatial frequency loss\n(SFL), is computed from the outputs of each filter. The learning is performed\nusing a loss function based on the SFL. Empirical evaluation demonstrates that\nusing the SFL reduces the blurs in reconstructed images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:34:12 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Ichimura", "Naoyuki", ""]]}, {"id": "1806.02400", "submitter": "Edemir Ferreira De Andrade Junior", "authors": "Edemir Ferreira, M\\'ario S. Alvim, and Jefersson A. dos Santos", "title": "A Comparative Study on Unsupervised Domain Adaptation Approaches for\n  Coffee Crop Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the application of existing unsupervised domain\nadaptation (UDA) approaches to the task of transferring knowledge between crop\nregions having different coffee patterns. Given a geographical region with\nfully mapped coffee plantations, we observe that this knowledge can be used to\ntrain a classifier and to map a new county with no need of samples indicated in\nthe target region. Experimental results show that transferring knowledge via\nsome UDA strategies performs better than just applying a classifier trained in\na region to predict coffee crops in a new one. However, UDA methods may lead to\nnegative transfer, which may indicate that domains are too different that\ntransferring knowledge is not appropriate. We also verify that normalization\naffect significantly some UDA methods; we observe a meaningful complementary\ncontribution between coffee crops data; and a visual behavior suggests an\nexistent of a cluster of samples that are more likely to be drawn from a\nspecific data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 19:36:55 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Ferreira", "Edemir", ""], ["Alvim", "M\u00e1rio S.", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "1806.02424", "submitter": "Hao Jiang", "authors": "Quanzeng You, Hao Jiang", "title": "Action4D: Real-time Action Recognition in the Crowd and Clutter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing every person's action in a crowded and cluttered environment is a\nchallenging task. In this paper, we propose a real-time action recognition\nmethod, Action4D, which gives reliable and accurate results in the real-world\nsettings. We propose to tackle the action recognition problem using a holistic\n4D \"scan\" of a cluttered scene to include every detail about the people and\nenvironment. Recognizing multiple people's actions in the cluttered 4D\nrepresentation is a new problem. In this paper, we propose novel methods to\nsolve this problem. We propose a new method to track people in 4D, which can\nreliably detect and follow each person in real time. We propose a new deep\nneural network, the Action4D-Net, to recognize the action of each tracked\nperson. The Action4D-Net's novel structure uses both the global feature and the\nfocused attention to achieve state-of-the-art result. Our real-time method is\ninvariant to camera view angles, resistant to clutter and able to handle crowd.\nThe experimental results show that the proposed method is fast, reliable and\naccurate. Our method paves the way to action recognition in the real-world\napplications and is ready to be deployed to enable smart homes, smart factories\nand smart stores.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 20:59:40 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["You", "Quanzeng", ""], ["Jiang", "Hao", ""]]}, {"id": "1806.02446", "submitter": "Dacheng Tao", "authors": "Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and\n  Dacheng Tao", "title": "Deep Ordinal Regression Network for Monocular Depth Estimation", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation, which plays a crucial role in understanding 3D\nscene geometry, is an ill-posed problem. Recent methods have gained significant\nimprovement by exploring image-level information and hierarchical features from\ndeep convolutional neural networks (DCNNs). These methods model depth\nestimation as a regression problem and train the regression networks by\nminimizing mean squared error, which suffers from slow convergence and\nunsatisfactory local solutions. Besides, existing depth estimation networks\nemploy repeated spatial pooling operations, resulting in undesirable\nlow-resolution feature maps. To obtain high-resolution depth maps,\nskip-connections or multi-layer deconvolution networks are required, which\ncomplicates network training and consumes much more computations. To eliminate\nor at least largely reduce these problems, we introduce a spacing-increasing\ndiscretization (SID) strategy to discretize depth and recast depth network\nlearning as an ordinal regression problem. By training the network using an\nordinary regression loss, our method achieves much higher accuracy and\n\\dd{faster convergence in synch}. Furthermore, we adopt a multi-scale network\nstructure which avoids unnecessary spatial pooling and captures multi-scale\ninformation in parallel.\n  The method described in this paper achieves state-of-the-art results on four\nchallenging benchmarks, i.e., KITTI [17], ScanNet [9], Make3D [50], and NYU\nDepth v2 [42], and win the 1st prize in Robust Vision Challenge 2018. Code has\nbeen made available at: https://github.com/hufu6371/DORN.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 22:36:23 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Fu", "Huan", ""], ["Gong", "Mingming", ""], ["Wang", "Chaohui", ""], ["Batmanghelich", "Kayhan", ""], ["Tao", "Dacheng", ""]]}, {"id": "1806.02452", "submitter": "Tahsin Reasat", "authors": "Samiul Alam, Tahsin Reasat, Rashed Mohammad Doha, Ahmed Imtiaz Humayun", "title": "NumtaDB - Assembled Bengali Handwritten Digits", "comments": "6 page, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To benchmark Bengali digit recognition algorithms, a large publicly available\ndataset is required which is free from biases originating from geographical\nlocation, gender, and age. With this aim in mind, NumtaDB, a dataset consisting\nof more than 85,000 images of hand-written Bengali digits, has been assembled.\nThis paper documents the collection and curation process of numerals along with\nthe salient statistics of the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 23:02:06 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Alam", "Samiul", ""], ["Reasat", "Tahsin", ""], ["Doha", "Rashed Mohammad", ""], ["Humayun", "Ahmed Imtiaz", ""]]}, {"id": "1806.02453", "submitter": "Seung Wook Kim", "authors": "Seung Wook Kim, Makarand Tapaswi, Sanja Fidler", "title": "Visual Reasoning by Progressive Module Networks", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans learn to solve tasks of increasing complexity by building on top of\npreviously acquired knowledge. Typically, there exists a natural progression in\nthe tasks that we learn - most do not require completely independent solutions,\nbut can be broken down into simpler subtasks. We propose to represent a solver\nfor each task as a neural module that calls existing modules (solvers for\nsimpler tasks) in a functional program-like manner. Lower modules are a black\nbox to the calling module, and communicate only via a query and an output.\nThus, a module for a new task learns to query existing modules and composes\ntheir outputs in order to produce its own output. Our model effectively\ncombines previous skill-sets, does not suffer from forgetting, and is fully\ndifferentiable. We test our model in learning a set of visual reasoning tasks,\nand demonstrate improved performances in all tasks by learning progressively.\nBy evaluating the reasoning process using human judges, we show that our model\nis more interpretable than an attention-based baseline.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 23:02:35 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 18:09:38 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Kim", "Seung Wook", ""], ["Tapaswi", "Makarand", ""], ["Fidler", "Sanja", ""]]}, {"id": "1806.02479", "submitter": "Yisu Zhou", "authors": "Yisu Zhou, Xiaolin Hu, Bo Zhang", "title": "Interlinked Convolutional Neural Networks for Face Parsing", "comments": "11 pages, 4 figures, ISNN2015 Conference", "journal-ref": "International Symposium on Neural Networks. Springer, Cham, 2015", "doi": "10.1007/978-3-319-25393-0_25", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face parsing is a basic task in face image analysis. It amounts to labeling\neach pixel with appropriate facial parts such as eyes and nose. In the paper,\nwe present a interlinked convolutional neural network (iCNN) for solving this\nproblem in an end-to-end fashion. It consists of multiple convolutional neural\nnetworks (CNNs) taking input in different scales. A special interlinking layer\nis designed to allow the CNNs to exchange information, enabling them to\nintegrate local and contextual information efficiently. The hallmark of iCNN is\nthe extensive use of downsampling and upsampling in the interlinking layers,\nwhile traditional CNNs usually uses downsampling only. A two-stage pipeline is\nproposed for face parsing and both stages use iCNN. The first stage localizes\nfacial parts in the size-reduced image and the second stage labels the pixels\nin the identified facial parts in the original image. On a benchmark dataset we\nhave obtained better results than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 01:10:08 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Zhou", "Yisu", ""], ["Hu", "Xiaolin", ""], ["Zhang", "Bo", ""]]}, {"id": "1806.02523", "submitter": "Kourosh Meshgi", "authors": "Kourosh Meshgi, Maryam Sadat Mirzaei, Shigeyuki Oba", "title": "Information-Maximizing Sampling to Promote Tracking-by-Detection", "comments": "visual tracking, information-maximizing sampling, active learning,\n  structured sample learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of an adaptive tracking-by-detection algorithm not only\ndepends on the classification and updating processes but also on the sampling.\nTypically, such trackers select their samples from the vicinity of the last\npredicted object location, or from its expected location using a pre-defined\nmotion model, which does not exploit the contents of the samples nor the\ninformation provided by the classifier. We introduced the idea of most\ninformative sampling, in which the sampler attempts to select samples that\ntrouble the classifier of a discriminative tracker. We then proposed an active\ndiscriminative co-tracker that embed an adversarial sampler to increase its\nrobustness against various tracking challenges. Experiments show that our\nproposed tracker outperforms state-of-the-art trackers on various benchmark\nvideos.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 05:59:02 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Meshgi", "Kourosh", ""], ["Mirzaei", "Maryam Sadat", ""], ["Oba", "Shigeyuki", ""]]}, {"id": "1806.02559", "submitter": "Xiang Li", "authors": "Xiang Li, Wenhai Wang, Wenbo Hou, Ruo-Ze Liu, Tong Lu, Jian Yang", "title": "Shape Robust Text Detection with Progressive Scale Expansion Network", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenges of shape robust text detection lie in two aspects: 1) most\nexisting quadrangular bounding box based detectors are difficult to locate\ntexts with arbitrary shapes, which are hard to be enclosed perfectly in a\nrectangle; 2) most pixel-wise segmentation-based detectors may not separate the\ntext instances that are very close to each other. To address these problems, we\npropose a novel Progressive Scale Expansion Network (PSENet), designed as a\nsegmentation-based detector with multiple predictions for each text instance.\nThese predictions correspond to different `kernels' produced by shrinking the\noriginal text instance into various scales. Consequently, the final detection\ncan be conducted through our progressive scale expansion algorithm which\ngradually expands the kernels with minimal scales to the text instances with\nmaximal and complete shapes. Due to the fact that there are large geometrical\nmargins among these minimal kernels, our method is effective to distinguish the\nadjacent text instances and is robust to arbitrary shapes. The state-of-the-art\nresults on ICDAR 2015 and ICDAR 2017 MLT benchmarks further confirm the great\neffectiveness of PSENet. Notably, PSENet outperforms the previous best record\nby absolute 6.37\\% on the curve text dataset SCUT-CTW1500. Code will be\navailable in https://github.com/whai362/PSENet.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 08:28:54 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Li", "Xiang", ""], ["Wang", "Wenhai", ""], ["Hou", "Wenbo", ""], ["Liu", "Ruo-Ze", ""], ["Lu", "Tong", ""], ["Yang", "Jian", ""]]}, {"id": "1806.02562", "submitter": "Alain Jungo", "authors": "Alain Jungo, Raphael Meier, Ekin Ermis, Marcela Blatti-Moreno, Evelyn\n  Herrmann, Roland Wiest, Mauricio Reyes", "title": "On the Effect of Inter-observer Variability for a Reliable Estimation of\n  Uncertainty of Medical Image Segmentation", "comments": "Appears in Medical Image Computing and Computer Assisted\n  Interventions (MICCAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty estimation methods are expected to improve the understanding and\nquality of computer-assisted methods used in medical applications (e.g.,\nneurosurgical interventions, radiotherapy planning), where automated medical\nimage segmentation is crucial. In supervised machine learning, a common\npractice to generate ground truth label data is to merge observer annotations.\nHowever, as many medical image tasks show a high inter-observer variability\nresulting from factors such as image quality, different levels of user\nexpertise and domain knowledge, little is known as to how inter-observer\nvariability and commonly used fusion methods affect the estimation of\nuncertainty of automated image segmentation. In this paper we analyze the\neffect of common image label fusion techniques on uncertainty estimation, and\npropose to learn the uncertainty among observers. The results highlight the\nnegative effect of fusion methods applied in deep learning, to obtain reliable\nestimates of segmentation uncertainty. Additionally, we show that the learned\nobservers' uncertainty can be combined with current standard Monte Carlo\ndropout Bayesian neural networks to characterize uncertainty of model's\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 08:44:43 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Jungo", "Alain", ""], ["Meier", "Raphael", ""], ["Ermis", "Ekin", ""], ["Blatti-Moreno", "Marcela", ""], ["Herrmann", "Evelyn", ""], ["Wiest", "Roland", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1806.02583", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (OBELIX, DTIS, ONERA, Universit\\'e Paris Saclay),\n  Bertrand Le Saux (DTIS, ONERA, Universit\\'e Paris Saclay), S\\'ebastien\n  Lef\\`evre (OBELIX)", "title": "Generative Adversarial Networks for Realistic Synthesis of Hyperspectral\n  Samples", "comments": null, "journal-ref": "International Geoscience and Remote Sensing Symposium (IGARSS\n  2018), Jul 2018, Valencia, Spain", "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the scarcity of annotated hyperspectral data required to\ntrain deep neural networks. Especially, we investigate generative adversarial\nnetworks and their application to the synthesis of consistent labeled spectra.\nBy training such networks on public datasets, we show that these models are not\nonly able to capture the underlying distribution, but also to generate\ngenuine-looking and physically plausible spectra. Moreover, we experimentally\nvalidate that the synthetic samples can be used as an effective data\naugmentation strategy. We validate our approach on several public\nhyper-spectral datasets using a variety of deep classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 09:36:12 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX, DTIS, ONERA, Universit\u00e9 Paris Saclay"], ["Saux", "Bertrand Le", "", "DTIS, ONERA, Universit\u00e9 Paris Saclay"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1806.02609", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mahdyar Ravanbakhsh, Mohamad Baydoun, Damian Campo, Pablo Marin, David\n  Martin, Lucio Marcenaro, Carlo S. Regazzoni", "title": "Learning Multi-Modal Self-Awareness Models for Autonomous Vehicles from\n  Human Driving", "comments": "FUSION 2018 - 21st International Conference on Information Fusion,\n  Cambridge, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for learning self-awareness models for\nautonomous vehicles. The proposed technique is based on the availability of\nsynchronized multi-sensor dynamic data related to different maneuvering tasks\nperformed by a human operator. It is shown that different machine learning\napproaches can be used to first learn single modality models using coupled\nDynamic Bayesian Networks; such models are then correlated at event level to\ndiscover contextual multi-modal concepts. In the presented case, visual\nperception and localization are used as modalities. Cross-correlations among\nmodalities in time is discovered from data and are described as probabilistic\nlinks connecting shared and private multi-modal DBNs at the event (discrete)\nlevel. Results are presented on experiments performed on an autonomous vehicle,\nhighlighting potentiality of the proposed approach to allow anomaly detection\nand autonomous decision making based on learned self-awareness models.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 11:08:53 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""], ["Baydoun", "Mohamad", ""], ["Campo", "Damian", ""], ["Marin", "Pablo", ""], ["Martin", "David", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo S.", ""]]}, {"id": "1806.02612", "submitter": "Xingjun Ma", "authors": "Xingjun Ma, Yisen Wang, Michael E. Houle, Shuo Zhou, Sarah M. Erfani,\n  Shu-Tao Xia, Sudanthi Wijewickrema, James Bailey", "title": "Dimensionality-Driven Learning with Noisy Labels", "comments": "In Proceedings of the International Conference on Machine Learning\n  (ICML), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets with significant proportions of noisy (incorrect) class labels\npresent challenges for training accurate Deep Neural Networks (DNNs). We\npropose a new perspective for understanding DNN generalization for such\ndatasets, by investigating the dimensionality of the deep representation\nsubspace of training samples. We show that from a dimensionality perspective,\nDNNs exhibit quite distinctive learning styles when trained with clean labels\nversus when trained with a proportion of noisy labels. Based on this finding,\nwe develop a new dimensionality-driven learning strategy, which monitors the\ndimensionality of subspaces during training and adapts the loss function\naccordingly. We empirically demonstrate that our approach is highly tolerant to\nsignificant proportions of noisy labels, and can effectively learn\nlow-dimensional local subspaces that capture the data distribution.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 11:11:13 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 14:54:24 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Ma", "Xingjun", ""], ["Wang", "Yisen", ""], ["Houle", "Michael E.", ""], ["Zhou", "Shuo", ""], ["Erfani", "Sarah M.", ""], ["Xia", "Shu-Tao", ""], ["Wijewickrema", "Sudanthi", ""], ["Bailey", "James", ""]]}, {"id": "1806.02613", "submitter": "Daniel Coelho De Castro", "authors": "Daniel C. Castro, Ben Glocker", "title": "Nonparametric Density Flows for MRI Intensity Normalisation", "comments": "Accepted for publication at MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00928-1_24", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the adoption of powerful machine learning methods in medical image\nanalysis, it is becoming increasingly desirable to aggregate data that is\nacquired across multiple sites. However, the underlying assumption of many\nanalysis techniques that corresponding tissues have consistent intensities in\nall images is often violated in multi-centre databases. We introduce a novel\nintensity normalisation scheme based on density matching, wherein the\nhistograms are modelled as Dirichlet process Gaussian mixtures. The source\nmixture model is transformed to minimise its $L^2$ divergence towards a target\nmodel, then the voxel intensities are transported through a mass-conserving\nflow to maintain agreement with the moving density. In a multi-centre study\nwith brain MRI data, we show that the proposed technique produces excellent\ncorrespondence between the matched densities and histograms. We further\ndemonstrate that our method makes tissue intensity statistics substantially\nmore compatible between images than a baseline affine transformation and is\ncomparable to state-of-the-art while providing considerably smoother\ntransformations. Finally, we validate that nonlinear intensity normalisation is\na step toward effective imaging data harmonisation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 11:13:35 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Castro", "Daniel C.", ""], ["Glocker", "Ben", ""]]}, {"id": "1806.02658", "submitter": "Yusuke Sugawara", "authors": "Yusuke Sugawara, Sayaka Shiota, Hitoshi Kiya", "title": "Super-Resolution using Convolutional Neural Networks without Any\n  Checkerboard Artifacts", "comments": "To appear in Proc. ICIP2018 October 07-10, 2018, Athens, Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that a number of excellent super-resolution (SR) methods\nusing convolutional neural networks (CNNs) generate checkerboard artifacts. A\ncondition to avoid the checkerboard artifacts is proposed in this paper. So\nfar, checkerboard artifacts have been mainly studied for linear multirate\nsystems, but the condition to avoid checkerboard artifacts can not be applied\nto CNNs due to the non-linearity of CNNs. We extend the avoiding condition for\nCNNs, and apply the proposed structure to some typical SR methods to confirm\nthe effectiveness of the new scheme. Experiment results demonstrate that the\nproposed structure can perfectly avoid to generate checkerboard artifacts under\ntwo loss conditions: mean square error and perceptual loss, while keeping\nexcellent properties that the SR methods have.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 13:15:50 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Sugawara", "Yusuke", ""], ["Shiota", "Sayaka", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1806.02664", "submitter": "Yuval Atzmon", "authors": "Yuval Atzmon and Gal Chechik", "title": "Probabilistic AND-OR Attribute Grouping for Zero-Shot Learning", "comments": "Accepted to the Conference on Uncertainty in Artificial Intelligence\n  (UAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In zero-shot learning (ZSL), a classifier is trained to recognize visual\nclasses without any image samples. Instead, it is given semantic information\nabout the class, like a textual description or a set of attributes. Learning\nfrom attributes could benefit from explicitly modeling structure of the\nattribute space. Unfortunately, learning of general structure from empirical\nsamples is hard with typical dataset sizes.\n  Here we describe LAGO, a probabilistic model designed to capture natural soft\nand-or relations across groups of attributes. We show how this model can be\nlearned end-to-end with a deep attribute-detection model. The soft group\nstructure can be learned from data jointly as part of the model, and can also\nreadily incorporate prior knowledge about groups if available. The soft and-or\nstructure succeeds to capture meaningful and predictive structures, improving\nthe accuracy of zero-shot learning on two of three benchmarks.\n  Finally, LAGO reveals a unified formulation over two ZSL approaches: DAP\n(Lampert et al., 2009) and ESZSL (Romera-Paredes & Torr, 2015). Interestingly,\ntaking only one singleton group for each attribute, introduces a new\nsoft-relaxation of DAP, that outperforms DAP by ~40.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 13:25:01 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 12:32:48 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Atzmon", "Yuval", ""], ["Chechik", "Gal", ""]]}, {"id": "1806.02679", "submitter": "Konstantinos Kamnitsas", "authors": "Konstantinos Kamnitsas, Daniel C. Castro, Loic Le Folgoc, Ian Walker,\n  Ryutaro Tanno, Daniel Rueckert, Ben Glocker, Antonio Criminisi, Aditya Nori", "title": "Semi-Supervised Learning via Compact Latent Space Clustering", "comments": "Presented as a long oral in ICML 2018. Post-conference camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel cost function for semi-supervised learning of neural\nnetworks that encourages compact clustering of the latent space to facilitate\nseparation. The key idea is to dynamically create a graph over embeddings of\nlabeled and unlabeled samples of a training batch to capture underlying\nstructure in feature space, and use label propagation to estimate its high and\nlow density regions. We then devise a cost function based on Markov chains on\nthe graph that regularizes the latent space to form a single compact cluster\nper class, while avoiding to disturb existing clusters during optimization. We\nevaluate our approach on three benchmarks and compare to state-of-the art with\npromising results. Our approach combines the benefits of graph-based\nregularization with efficient, inductive inference, does not require\nmodifications to a network architecture, and can thus be easily applied to\nexisting networks to enable an effective use of unlabeled data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 13:41:56 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 19:20:19 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Kamnitsas", "Konstantinos", ""], ["Castro", "Daniel C.", ""], ["Folgoc", "Loic Le", ""], ["Walker", "Ian", ""], ["Tanno", "Ryutaro", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""], ["Criminisi", "Antonio", ""], ["Nori", "Aditya", ""]]}, {"id": "1806.02682", "submitter": "Manuel Lagunas", "authors": "Manuel Lagunas and Elena Garces", "title": "Transfer Learning for Illustration Classification", "comments": "9 pages, 8 figures, 4 tables", "journal-ref": "2017 Spanish Computer Graphics Conference (CEIG)", "doi": "10.2312/ceig.20171213", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of image classification has shown an outstanding success thanks to\nthe development of deep learning techniques. Despite the great performance\nobtained, most of the work has focused on natural images ignoring other domains\nlike artistic depictions. In this paper, we use transfer learning techniques to\npropose a new classification network with better performance in illustration\nimages. Starting from the deep convolutional network VGG19, pre-trained with\nnatural images, we propose two novel models which learn object representations\nin the new domain. Our optimized network will learn new low-level features of\nthe images (colours, edges, textures) while keeping the knowledge of the\nobjects and shapes that it already learned from the ImageNet dataset. Thus,\nrequiring much less data for the training. We propose a novel dataset of\nillustration images labelled by content where our optimized architecture\nachieves $\\textbf{86.61\\%}$ of top-1 and $\\textbf{97.21\\%}$ of top-5 precision.\nWe additionally demonstrate that our model is still able to recognize objects\nin photographs.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 09:06:16 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Lagunas", "Manuel", ""], ["Garces", "Elena", ""]]}, {"id": "1806.02705", "submitter": "Matthew Blaschko", "authors": "Mathijs Schuurmans, Maxim Berman, Matthew B. Blaschko", "title": "Efficient semantic image segmentation with superpixel pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we evaluate the use of superpixel pooling layers in deep\nnetwork architectures for semantic segmentation. Superpixel pooling is a\nflexible and efficient replacement for other pooling strategies that\nincorporates spatial prior information. We propose a simple and efficient\nGPU-implementation of the layer and explore several designs for the integration\nof the layer into existing network architectures. We provide experimental\nresults on the IBSR and Cityscapes dataset, demonstrating that superpixel\npooling can be leveraged to consistently increase network accuracy with minimal\ncomputational overhead. Source code is available at\nhttps://github.com/bermanmaxim/superpixPool\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 14:38:08 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Schuurmans", "Mathijs", ""], ["Berman", "Maxim", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "1806.02724", "submitter": "Ronghang Hu", "authors": "Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas,\n  Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,\n  Trevor Darrell", "title": "Speaker-Follower Models for Vision-and-Language Navigation", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation guided by natural language instructions presents a challenging\nreasoning problem for instruction followers. Natural language instructions\ntypically identify only a few high-level decisions and landmarks rather than\ncomplete low-level motor behaviors; much of the missing information must be\ninferred based on perceptual context. In machine learning settings, this is\ndoubly challenging: it is difficult to collect enough annotated data to enable\nlearning of this reasoning process from scratch, and also difficult to\nimplement the reasoning process using generic sequence models. Here we describe\nan approach to vision-and-language navigation that addresses both these issues\nwith an embedded speaker model. We use this speaker model to (1) synthesize new\ninstructions for data augmentation and to (2) implement pragmatic reasoning,\nwhich evaluates how well candidate action sequences explain an instruction.\nBoth steps are supported by a panoramic action space that reflects the\ngranularity of human-generated instructions. Experiments show that all three\ncomponents of this approach---speaker-driven data augmentation, pragmatic\nreasoning and panoramic action space---dramatically improve the performance of\na baseline instruction follower, more than doubling the success rate over the\nbest existing approach on a standard benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 15:15:35 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 01:38:56 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Fried", "Daniel", ""], ["Hu", "Ronghang", ""], ["Cirik", "Volkan", ""], ["Rohrbach", "Anna", ""], ["Andreas", "Jacob", ""], ["Morency", "Louis-Philippe", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["Saenko", "Kate", ""], ["Klein", "Dan", ""], ["Darrell", "Trevor", ""]]}, {"id": "1806.02750", "submitter": "Hassan Ismail Fawaz", "authors": "Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane\n  Idoumghar, Pierre-Alain Muller", "title": "Evaluating surgical skills from kinematic data using convolutional\n  neural networks", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00937-3_25", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for automatic surgical skills assessment is increasing, especially\nbecause manual feedback from senior surgeons observing junior surgeons is prone\nto subjectivity and time consuming. Thus, automating surgical skills evaluation\nis a very important step towards improving surgical practice. In this paper, we\ndesigned a Convolutional Neural Network (CNN) to evaluate surgeon skills by\nextracting patterns in the surgeon motions performed in robotic surgery. The\nproposed method is validated on the JIGSAWS dataset and achieved very\ncompetitive results with 100% accuracy on the suturing and needle passing\ntasks. While we leveraged from the CNNs efficiency, we also managed to mitigate\nits black-box effect using class activation map. This feature allows our method\nto automatically highlight which parts of the surgical task influenced the\nskill prediction and can be used to explain the classification and to provide\npersonalized feedback to the trainee.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 16:06:10 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Fawaz", "Hassan Ismail", ""], ["Forestier", "Germain", ""], ["Weber", "Jonathan", ""], ["Idoumghar", "Lhassane", ""], ["Muller", "Pierre-Alain", ""]]}, {"id": "1806.02850", "submitter": "Shrinivasan Sankar", "authors": "Shrinivasan Sankar and Adrien Bartoli", "title": "Model-based active learning to detect isometric deformable objects in\n  the wild with deep architectures", "comments": "Accepted in Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent past, algorithms based on Convolutional Neural Networks (CNNs)\nhave achieved significant milestones in object recognition. With large examples\nof each object class, standard datasets train well for inter-class variability.\nHowever, gathering sufficient data to train for a particular instance of an\nobject within a class is impractical. Furthermore, quantitatively assessing the\nimaging conditions for each image in a given dataset is not feasible. By\ngenerating sufficient images with known imaging conditions, we study to what\nextent CNNs can cope with hard imaging conditions for instance-level\nrecognition in an active learning regime.\n  Leveraging powerful rendering techniques to achieve instance-level detection,\nwe present results of training three state-of-the-art object detection\nalgorithms namely, Fast R-CNN, Faster R-CNN and YOLO9000, for hard imaging\nconditions imposed into the scene by rendering. Our extensive experiments\nproduce a mean Average Precision score of 0.92 on synthetic images and 0.83 on\nreal images using the best performing Faster R-CNN. We show for the first time\nhow well detection algorithms based on deep architectures fare for each hard\nimaging condition studied.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 18:18:14 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Sankar", "Shrinivasan", ""], ["Bartoli", "Adrien", ""]]}, {"id": "1806.02877", "submitter": "Siwei Lyu", "authors": "Yuezun Li, Ming-Ching Chang, Siwei Lyu", "title": "In Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye\n  Blinking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new developments in deep generative networks have significantly improve\nthe quality and efficiency in generating realistically-looking fake face\nvideos. In this work, we describe a new method to expose fake face videos\ngenerated with neural networks. Our method is based on detection of eye\nblinking in the videos, which is a physiological signal that is not well\npresented in the synthesized fake videos. Our method is tested over benchmarks\nof eye-blinking detection datasets and also show promising performance on\ndetecting videos generated with DeepFake.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 19:36:09 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 19:28:49 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Li", "Yuezun", ""], ["Chang", "Ming-Ching", ""], ["Lyu", "Siwei", ""]]}, {"id": "1806.02888", "submitter": "Md Nasir Uddin Laskar", "authors": "Md Nasir Uddin Laskar, Luis G Sanchez Giraldo, and Odelia Schwartz", "title": "Correspondence of Deep Neural Networks and the Brain for Visual Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) trained on objects and scenes have\nshown intriguing ability to predict some response properties of visual cortical\nneurons. However, the factors and computations that give rise to such ability,\nand the role of intermediate processing stages in explaining changes that\ndevelop across areas of the cortical hierarchy, are poorly understood. We\nfocused on the sensitivity to textures as a paradigmatic example, since recent\nneurophysiology experiments provide rich data pointing to texture sensitivity\nin secondary but not primary visual cortex. We developed a quantitative\napproach for selecting a subset of the neural unit population from the CNN that\nbest describes the brain neural recordings. We found that the first two layers\nof the CNN showed qualitative and quantitative correspondence to the cortical\ndata across a number of metrics. This compatibility was reduced for the\narchitecture alone rather than the learned weights, for some other related\nhierarchical models, and only mildly in the absence of a nonlinear computation\nakin to local divisive normalization. Our results show that the CNN class of\nmodel is effective for capturing changes that develop across early areas of\ncortex, and has the potential to facilitate understanding of the computations\nthat give rise to hierarchical processing in the brain.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:20:07 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Laskar", "Md Nasir Uddin", ""], ["Giraldo", "Luis G Sanchez", ""], ["Schwartz", "Odelia", ""]]}, {"id": "1806.02891", "submitter": "Bolei Zhou", "authors": "Bolei Zhou, Yiyou Sun, David Bau, Antonio Torralba", "title": "Revisiting the Importance of Individual Units in CNNs via Ablation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the importance of the individual units in Convolutional Neural\nNetworks (CNNs) for visual recognition. By conducting unit ablation experiments\non CNNs trained on large scale image datasets, we demonstrate that, though\nablating any individual unit does not hurt overall classification accuracy, it\ndoes lead to significant damage on the accuracy of specific classes. This\nresult shows that an individual unit is specialized to encode information\nrelevant to a subset of classes. We compute the correlation between the\naccuracy drop under unit ablation and various attributes of an individual unit\nsuch as class selectivity and weight L1 norm. We confirm that unit attributes\nsuch as class selectivity are a poor predictor for impact on overall accuracy\nas found previously in recent work \\cite{morcos2018importance}. However, our\nresults show that class selectivity along with other attributes are good\npredictors of the importance of one unit to individual classes. We evaluate the\nimpact of random rotation, batch normalization, and dropout to the importance\nof units to specific classes. Our results show that units with high selectivity\nplay an important role in network classification power at the individual class\nlevel. Understanding and interpreting the behavior of these units is necessary\nand meaningful.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:40:56 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zhou", "Bolei", ""], ["Sun", "Yiyou", ""], ["Bau", "David", ""], ["Torralba", "Antonio", ""]]}, {"id": "1806.02892", "submitter": "Mahdi Kalayeh", "authors": "Mahdi M. Kalayeh, Mubarak Shah", "title": "Training Faster by Separating Modes of Variation in Batch-normalized\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) is essential to effectively train state-of-the-art\ndeep Convolutional Neural Networks (CNN). It normalizes inputs to the layers\nduring training using the statistics of each mini-batch. In this work, we study\nBN from the viewpoint of Fisher kernels. We show that assuming samples within a\nmini-batch are from the same probability density function, then BN is identical\nto the Fisher vector of a Gaussian distribution. That means BN can be explained\nin terms of kernels that naturally emerge from the probability density function\nof the underlying data distribution. However, given the rectifying\nnon-linearities employed in CNN architectures, distribution of inputs to the\nlayers show heavy tail and asymmetric characteristics. Therefore, we propose\napproximating underlying data distribution not with one, but a mixture of\nGaussian densities. Deriving Fisher vector for a Gaussian Mixture Model (GMM),\nreveals that BN can be improved by independently normalizing with respect to\nthe statistics of disentangled sub-populations. We refer to our proposed soft\npiecewise version of BN as Mixture Normalization (MN). Through extensive set of\nexperiments on CIFAR-10 and CIFAR-100, we show that MN not only effectively\naccelerates training image classification and Generative Adversarial networks,\nbut also reaches higher quality models.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:41:09 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 16:09:03 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Kalayeh", "Mahdi M.", ""], ["Shah", "Mubarak", ""]]}, {"id": "1806.02907", "submitter": "Tejas Krishna Reddy", "authors": "Tejas K, Swathi C, Rajesh Kumar M", "title": "Copy Move Forgery using Hus Invariant Moments and Log Polar\n  Transformations", "comments": "This paper was submitted, accepted and presented in the 3rd\n  International Conference on RTEICT, IEEE Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in interchange of data, there is a growing necessity of\nsecurity. Considering the volumes of digital data that is transmitted, they are\nin need to be secure. Among the many forms of tampering possible, one\nwidespread technique is Copy Move Forgery CMF. This forgery occurs when parts\nof the image are copied and duplicated elsewhere in the same image. There exist\na number of algorithms to detect such a forgery in which the primary step\ninvolved is feature extraction. The feature extraction techniques employed must\nhave lesser time and space complexity involved for an efficient and faster\nprocessing of media. Also, majority of the existing state of art techniques\noften tend to falsely match similar genuine objects as copy move forged during\nthe detection process. To tackle these problems, the paper proposes a novel\nalgorithm that recognizes a unique approach of using Hus Invariant Moments and\nLog polar Transformations to reduce feature vector dimension to one feature per\nblock simultaneously detecting CMF among genuine similar objects in an image.\nThe qualitative and quantitative results obtained demonstrate the effectiveness\nof this algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 21:22:36 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["K", "Tejas", ""], ["C", "Swathi", ""], ["M", "Rajesh Kumar", ""]]}, {"id": "1806.02918", "submitter": "Maria Shugrina", "authors": "Maria Shugrina, Amlan Kar, Karan Singh, Sanja Fidler", "title": "Color Sails: Discrete-Continuous Palettes for Deep Color Exploration", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present color sails, a discrete-continuous color gamut representation that\nextends the color gradient analogy to three dimensions and allows interactive\ncontrol of the color blending behavior. Our representation models a wide\nvariety of color distributions in a compact manner, and lends itself to\napplications such as color exploration for graphic design, illustration and\nsimilar fields. We propose a Neural Network that can fit a color sail to any\nimage. Then, the user can adjust color sail parameters to change the base\ncolors, their blending behavior and the number of colors, exploring a wide\nrange of options for the original design. In addition, we propose a Deep\nLearning model that learns to automatically segment an image into\ncolor-compatible alpha masks, each equipped with its own color sail. This\nallows targeted color exploration by either editing their corresponding color\nsails or using standard software packages. Our model is trained on a custom\ndiverse dataset of art and design. We provide both quantitative evaluations,\nand a user study, demonstrating the effectiveness of color sail interaction.\nInteractive demos are available at www.colorsails.com.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 22:42:00 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Shugrina", "Maria", ""], ["Kar", "Amlan", ""], ["Singh", "Karan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1806.02919", "submitter": "Ding Liu", "authors": "Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S. Huang", "title": "Non-Local Recurrent Network for Image Restoration", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classic methods have shown non-local self-similarity in natural images\nto be an effective prior for image restoration. However, it remains unclear and\nchallenging to make use of this intrinsic property via deep networks. In this\npaper, we propose a non-local recurrent network (NLRN) as the first attempt to\nincorporate non-local operations into a recurrent neural network (RNN) for\nimage restoration. The main contributions of this work are: (1) Unlike existing\nmethods that measure self-similarity in an isolated manner, the proposed\nnon-local module can be flexibly integrated into existing deep networks for\nend-to-end training to capture deep feature correlation between each location\nand its neighborhood. (2) We fully employ the RNN structure for its parameter\nefficiency and allow deep feature correlation to be propagated along adjacent\nrecurrent states. This new design boosts robustness against inaccurate\ncorrelation estimation due to severely degraded images. (3) We show that it is\nessential to maintain a confined neighborhood for computing deep feature\ncorrelation given degraded images. This is in contrast to existing practice\nthat deploys the whole image. Extensive experiments on both image denoising and\nsuper-resolution tasks are conducted. Thanks to the recurrent non-local\noperations and correlation propagation, the proposed NLRN achieves superior\nresults to state-of-the-art methods with much fewer parameters.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 22:50:49 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 05:44:37 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Liu", "Ding", ""], ["Wen", "Bihan", ""], ["Fan", "Yuchen", ""], ["Loy", "Chen Change", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1806.02934", "submitter": "Ashwin Vijayakumar", "authors": "Ashwin Kalyan, Stefan Lee, Anitha Kannan, Dhruv Batra", "title": "Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse\n  Annotations", "comments": "To be presented at ICML 2018; 10 pages 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many structured prediction problems (particularly in vision and language\ndomains) are ambiguous, with multiple outputs being correct for an input - e.g.\nthere are many ways of describing an image, multiple ways of translating a\nsentence; however, exhaustively annotating the applicability of all possible\noutputs is intractable due to exponentially large output spaces (e.g. all\nEnglish sentences). In practice, these problems are cast as multi-class\nprediction, with the likelihood of only a sparse set of annotations being\nmaximized - unfortunately penalizing for placing beliefs on plausible but\nunannotated outputs. We make and test the following hypothesis - for a given\ninput, the annotations of its neighbors may serve as an additional supervisory\nsignal. Specifically, we propose an objective that transfers supervision from\nneighboring examples. We first study the properties of our developed method in\na controlled toy setup before reporting results on multi-label classification\nand two image-grounded sequence modeling tasks - captioning and question\ngeneration. We evaluate using standard task-specific metrics and measures of\noutput diversity, finding consistent improvements over standard maximum\nlikelihood training and other baselines.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 01:18:10 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Kalyan", "Ashwin", ""], ["Lee", "Stefan", ""], ["Kannan", "Anitha", ""], ["Batra", "Dhruv", ""]]}, {"id": "1806.02952", "submitter": "Gusi Te", "authors": "Gusi Te, Wei Hu, Zongming Guo, Amin Zheng", "title": "RGCNN: Regularized Graph CNN for Point Cloud Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud, an efficient 3D object representation, has become popular with\nthe development of depth sensing and 3D laser scanning techniques. It has\nattracted attention in various applications such as 3D tele-presence,\nnavigation for unmanned vehicles and heritage reconstruction. The understanding\nof point clouds, such as point cloud segmentation, is crucial in exploiting the\ninformative value of point clouds for such applications. Due to the\nirregularity of the data format, previous deep learning works often convert\npoint clouds to regular 3D voxel grids or collections of images before feeding\nthem into neural networks, which leads to voluminous data and quantization\nartifacts. In this paper, we instead propose a regularized graph convolutional\nneural network (RGCNN) that directly consumes point clouds. Leveraging on\nspectral graph theory, we treat features of points in a point cloud as signals\non graph, and define the convolution over graph by Chebyshev polynomial\napproximation. In particular, we update the graph Laplacian matrix that\ndescribes the connectivity of features in each layer according to the\ncorresponding learned features, which adaptively captures the structure of\ndynamic graphs. Further, we deploy a graph-signal smoothness prior in the loss\nfunction, thus regularizing the learning process. Experimental results on the\nShapeNet part dataset show that the proposed approach significantly reduces the\ncomputational complexity while achieving competitive performance with the state\nof the art. Also, experiments show RGCNN is much more robust to both noise and\npoint cloud density in comparison with other methods. We further apply RGCNN to\npoint cloud classification and achieve competitive results on ModelNet40\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 02:50:19 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Te", "Gusi", ""], ["Hu", "Wei", ""], ["Guo", "Zongming", ""], ["Zheng", "Amin", ""]]}, {"id": "1806.02964", "submitter": "Tianwei Lin", "authors": "Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, Ming Yang", "title": "BSN: Boundary Sensitive Network for Temporal Action Proposal Generation", "comments": "Accepted at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action proposal generation is an important yet challenging problem,\nsince temporal proposals with rich action content are indispensable for\nanalysing real-world videos with long duration and high proportion irrelevant\ncontent. This problem requires methods not only generating proposals with\nprecise temporal boundaries, but also retrieving proposals to cover truth\naction instances with high recall and high overlap using relatively fewer\nproposals. To address these difficulties, we introduce an effective proposal\ngeneration method, named Boundary-Sensitive Network (BSN), which adopts \"local\nto global\" fashion. Locally, BSN first locates temporal boundaries with high\nprobabilities, then directly combines these boundaries as proposals. Globally,\nwith Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating\nthe confidence of whether a proposal contains an action within its region. We\nconduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14,\nwhere BSN outperforms other state-of-the-art temporal action proposal\ngeneration methods with high recall and high temporal precision. Finally,\nfurther experiments demonstrate that by combining existing action classifiers,\nour method significantly improves the state-of-the-art temporal action\ndetection performance.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 04:22:54 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 06:35:47 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 10:48:22 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Lin", "Tianwei", ""], ["Zhao", "Xu", ""], ["Su", "Haisheng", ""], ["Wang", "Chongjing", ""], ["Yang", "Ming", ""]]}, {"id": "1806.02974", "submitter": "Ram Prakash Sharma Mr.", "authors": "Ram Prakash Sharma and Somnath Dey", "title": "Fingerprint liveness detection using local quality features", "comments": "21 pages, 11 figures, 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint-based recognition has been widely deployed in various\napplications. However, current recognition systems are vulnerable to spoofing\nattacks which make use of an artificial replica of a fingerprint to deceive the\nsensors. In such scenarios, fingerprint liveness detection ensures the actual\npresence of a real legitimate fingerprint in contrast to a fake\nself-manufactured synthetic sample. In this paper, we propose a static\nsoftware-based approach using quality features to detect the liveness in a\nfingerprint. We have extracted features from a single fingerprint image to\novercome the issues faced in dynamic software-based approaches which require\nlonger computational time and user cooperation. The proposed system extracts 8\nsensor independent quality features on a local level containing minute details\nof the ridge-valley structure of real and fake fingerprints. These local\nquality features constitutes a 13-dimensional feature vector. The system is\ntested on a publically available dataset of LivDet 2009 competition. The\nexperimental results exhibit supremacy of the proposed method over current\nstate-of-the-art approaches providing least average classification error of\n5.3% for LivDet 2009. Additionally, effectiveness of the best performing\nfeatures over LivDet 2009 is evaluated on the latest LivDet 2015 dataset which\ncontain fingerprints fabricated using unknown spoof materials. An average\nclassification error rate of 4.22% is achieved in comparison with 4.49%\nobtained by the LivDet 2015 winner. Further, the proposed system utilizes a\nsingle fingerprint image, which results in faster implications and makes it\nmore user-friendly.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 05:48:10 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Sharma", "Ram Prakash", ""], ["Dey", "Somnath", ""]]}, {"id": "1806.02984", "submitter": "Jiedong Hao", "authors": "Jiedong Hao and Jing Dong and Wei Wang and Tieniu Tan", "title": "DeepFirearm: Learning Discriminative Feature Representation for\n  Fine-grained Firearm Retrieval", "comments": "6 pages, 5 figures, accepted by ICPR 2018. Code are available at\n  https://github.com/jdhao/deep_firearm. Dataset is available at\n  http://forensics.idealtest.org/Firearm14k/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There are great demands for automatically regulating inappropriate appearance\nof shocking firearm images in social media or identifying firearm types in\nforensics. Image retrieval techniques have great potential to solve these\nproblems. To facilitate research in this area, we introduce Firearm 14k, a\nlarge dataset consisting of over 14,000 images in 167 categories. It can be\nused for both fine-grained recognition and retrieval of firearm images. Recent\nadvances in image retrieval are mainly driven by fine-tuning state-of-the-art\nconvolutional neural networks for retrieval task. The conventional single\nmargin contrastive loss, known for its simplicity and good performance, has\nbeen widely used. We find that it performs poorly on the Firearm 14k dataset\ndue to: (1) Loss contributed by positive and negative image pairs is unbalanced\nduring training process. (2) A huge domain gap exists between this dataset and\nImageNet. We propose to deal with the unbalanced loss by employing a double\nmargin contrastive loss. We tackle the domain gap issue with a two-stage\ntraining strategy, where we first fine-tune the network for classification, and\nthen fine-tune it for retrieval. Experimental results show that our approach\noutperforms the conventional single margin approach by a large margin (up to\n88.5% relative improvement) and even surpasses the strong triplet-loss-based\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 06:45:32 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 08:37:16 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Hao", "Jiedong", ""], ["Dong", "Jing", ""], ["Wang", "Wei", ""], ["Tan", "Tieniu", ""]]}, {"id": "1806.02987", "submitter": "Arne Schumann", "authors": "Krassimir Valev, Arne Schumann, Lars Sommer, J\\\"urgen Beyerer", "title": "A Systematic Evaluation of Recent Deep Learning Architectures for\n  Fine-Grained Vehicle Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained vehicle classification is the task of classifying make, model,\nand year of a vehicle. This is a very challenging task, because vehicles of\ndifferent types but similar color and viewpoint can often look much more\nsimilar than vehicles of same type but differing color and viewpoint. Vehicle\nmake, model, and year in com- bination with vehicle color - are of importance\nin several applications such as vehicle search, re-identification, tracking,\nand traffic analysis. In this work we investigate the suitability of several\nrecent landmark convolutional neural network (CNN) architectures, which have\nshown top results on large scale image classification tasks, for the task of\nfine-grained classification of vehicles. We compare the performance of the\nnetworks VGG16, several ResNets, Inception architectures, the recent DenseNets,\nand MobileNet. For classification we use the Stanford Cars-196 dataset which\nfeatures 196 different types of vehicles. We investigate several aspects of CNN\ntraining, such as data augmentation and training from scratch vs. fine-tuning.\nImportantly, we introduce no aspects in the architectures or training process\nwhich are specific to vehicle classification. Our final model achieves a\nstate-of-the-art classification accuracy of 94.6% outperforming all related\nworks, even approaches which are specifically tailored for the task, e.g. by\nincluding vehicle part detections.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 06:55:16 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Valev", "Krassimir", ""], ["Schumann", "Arne", ""], ["Sommer", "Lars", ""], ["Beyerer", "J\u00fcrgen", ""]]}, {"id": "1806.02997", "submitter": "Aleksei Vasilev", "authors": "Aleksei Vasilev, Vladimir Golkov, Marc Meissner, Ilona Lipp, Eleonora\n  Sgarlata, Valentina Tomassini, Derek K. Jones, Daniel Cremers", "title": "q-Space Novelty Detection with Variational Autoencoders", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, novelty detection is the task of identifying novel\nunseen data. During training, only samples from the normal class are available.\nTest samples are classified as normal or abnormal by assignment of a novelty\nscore. Here we propose novelty detection methods based on training variational\nautoencoders (VAEs) on normal data. Since abnormal samples are not used during\ntraining, we define novelty metrics based on the (partially complementary)\nassumptions that the VAE is less capable of reconstructing abnormal samples\nwell; that abnormal samples more strongly violate the VAE regularizer; and that\nabnormal samples differ from normal samples not only in input-feature space,\nbut also in the VAE latent space and VAE output. These approaches, combined\nwith various possibilities of using (e.g. sampling) the probabilistic VAE to\nobtain scalar novelty scores, yield a large family of methods. We apply these\nmethods to magnetic resonance imaging, namely to the detection of\ndiffusion-space (q-space) abnormalities in diffusion MRI scans of multiple\nsclerosis patients, i.e. to detect multiple sclerosis lesions without using any\nlesion labels for training. Many of our methods outperform previously proposed\nq-space novelty detection methods. We also evaluate the proposed methods on the\nMNIST handwritten digits dataset and show that many of them are able to\noutperform the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 07:28:36 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 17:34:51 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Vasilev", "Aleksei", ""], ["Golkov", "Vladimir", ""], ["Meissner", "Marc", ""], ["Lipp", "Ilona", ""], ["Sgarlata", "Eleonora", ""], ["Tomassini", "Valentina", ""], ["Jones", "Derek K.", ""], ["Cremers", "Daniel", ""]]}, {"id": "1806.02998", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (CMM)", "title": "Logarithmic mathematical morphology: a new framework adaptive to\n  illumination changes", "comments": null, "journal-ref": "23rd Iberoamerican Congress on Pattern Recognition (CIARP 2018),\n  Nov 2018, Madrid, Spain. Springer International Publishing, Lecture Notes in\n  Computer Science, 11401, pp.453-461, 2019, Progress in Pattern Recognition,\n  Image Analysis, Computer Vision, and Applications.\n  https://atvs.ii.uam.es/ciarp2018/", "doi": "10.1007/978-3-030-13469-3_53", "report-no": null, "categories": "cs.CV math.GN math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new set of mathematical morphology (MM) operators adaptive to illumination\nchanges caused by variation of exposure time or light intensity is defined\nthanks to the Logarithmic Image Processing (LIP) model. This model based on the\nphysics of acquisition is consistent with human vision. The fundamental\noperators, the logarithmic-dilation and the logarithmic-erosion, are defined\nwith the LIP-addition of a structuring function. The combination of these two\nadjunct operators gives morphological filters, namely the logarithmic-opening\nand closing, useful for pattern recognition. The mathematical relation existing\nbetween ``classical'' dilation and erosion and their logarithmic-versions is\nestablished facilitating their implementation. Results on simulated and real\nimages show that logarithmic-MM is more efficient on low-contrasted information\nthan ``classical'' MM.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 07:28:42 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 09:51:12 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 15:51:21 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Noyel", "Guillaume", "", "CMM"]]}, {"id": "1806.03002", "submitter": "Taegyun Jeon", "authors": "Junghoon Seo, Seunghyun Jeon, Taegyun Jeon", "title": "Domain Adaptive Generation of Aircraft on Satellite Imagery via\n  Simulated and Unsupervised Learning", "comments": "presented at the International Workshop on Machine Learning for\n  Artificial Intelligence Platforms held in 2017 Asian Conference on Machine\n  Learning (MLAIP@ACML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and classification for aircraft are the most important tasks\nin the satellite image analysis. The success of modern detection and\nclassification methods has been based on machine learning and deep learning.\nOne of the key requirements for those learning processes is huge data to train.\nHowever, there is an insufficient portion of aircraft since the targets are on\nmilitary action and oper- ation. Considering the characteristics of satellite\nimagery, this paper attempts to provide a framework of the simulated and\nunsupervised methodology without any additional su- pervision or physical\nassumptions. Finally, the qualitative and quantitative analysis revealed a\npotential to replenish insufficient data for machine learning platform for\nsatellite image analysis.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 07:46:34 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Seo", "Junghoon", ""], ["Jeon", "Seunghyun", ""], ["Jeon", "Taegyun", ""]]}, {"id": "1806.03014", "submitter": "Masahiro Oda", "authors": "Masahiro Oda, Takayuki Kitasaka, Kazuhiro Furukawa, Ryoji Miyahara,\n  Yoshiki Hirooka, Hidemi Goto, Nassir Navab, Kensaku Mori", "title": "Machine learning-based colon deformation estimation method for\n  colonoscope tracking", "comments": "Accepted paper for oral presentation at SPIE Medical Imaging 2018,\n  Houston, TX, USA", "journal-ref": "SPIE Medical Imaging 2018: Image-Guided Procedures, Robotic\n  Interventions, and Modeling", "doi": "10.1117/12.2293936", "report-no": "Published in Proceedings of SPIE 10576, Medical Imaging 2018:\n  Image-Guided Procedures, Robotic Interventions, and Modeling, 1057619", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a colon deformation estimation method, which can be used\nto estimate colon deformations during colonoscope insertions. Colonoscope\ntracking or navigation system that navigates a physician to polyp positions\nduring a colonoscope insertion is required to reduce complications such as\ncolon perforation. A previous colonoscope tracking method obtains a colonoscope\nposition in the colon by registering a colonoscope shape and a colon shape. The\ncolonoscope shape is obtained using an electromagnetic sensor, and the colon\nshape is obtained from a CT volume. However, large tracking errors were\nobserved due to colon deformations occurred during colonoscope insertions. Such\ndeformations make the registration difficult. Because the colon deformation is\ncaused by a colonoscope, there is a strong relationship between the colon\ndeformation and the colonoscope shape. An estimation method of colon\ndeformations occur during colonoscope insertions is necessary to reduce\ntracking errors. We propose a colon deformation estimation method. This method\nis used to estimate a deformed colon shape from a colonoscope shape. We use the\nregression forests algorithm to estimate a deformed colon shape. The regression\nforests algorithm is trained using pairs of colon and colonoscope shapes, which\ncontains deformations occur during colonoscope insertions. As a preliminary\nstudy, we utilized the method to estimate deformations of a colon phantom. In\nour experiments, the proposed method correctly estimated deformed colon phantom\nshapes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 08:15:29 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Oda", "Masahiro", ""], ["Kitasaka", "Takayuki", ""], ["Furukawa", "Kazuhiro", ""], ["Miyahara", "Ryoji", ""], ["Hirooka", "Yoshiki", ""], ["Goto", "Hidemi", ""], ["Navab", "Nassir", ""], ["Mori", "Kensaku", ""]]}, {"id": "1806.03018", "submitter": "Xiangyu Zhu", "authors": "Xiangyu Zhu, Hao Liu, Zhen Lei, Hailin Shi, Fan Yang, Dong Yi, Guojun\n  Qi, Stan Z. Li", "title": "Large-scale Bisample Learning on ID Versus Spot Face Recognition", "comments": "Accepted by special issue on Deep Learning for Face Analysis.\n  International Journal of Computer Vision (IJCV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world face recognition applications, there is a tremendous amount of\ndata with two images for each person. One is an ID photo for face enrollment,\nand the other is a probe photo captured on spot. Most existing methods are\ndesigned for training data with limited breadth (a relatively small number of\nclasses) and sufficient depth (many samples for each class). They would meet\ngreat challenges on ID versus Spot (IvS) data, including the under-represented\nintra-class variations and an excessive demand on computing devices. In this\npaper, we propose a deep learning based large-scale bisample learning (LBL)\nmethod for IvS face recognition. To tackle the bisample problem with only two\nsamples for each class, a classification-verification-classification (CVC)\ntraining strategy is proposed to progressively enhance the IvS performance.\nBesides, a dominant prototype softmax (DP-softmax) is incorporated to make the\ndeep learning scalable on large-scale classes. We conduct LBL on a IvS face\ndataset with more than two million identities. Experimental results show the\nproposed method achieves superior performance to previous ones, validating the\neffectiveness of LBL on IvS face recognition.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 08:27:55 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 10:18:24 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 02:43:50 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Zhu", "Xiangyu", ""], ["Liu", "Hao", ""], ["Lei", "Zhen", ""], ["Shi", "Hailin", ""], ["Yang", "Fan", ""], ["Yi", "Dong", ""], ["Qi", "Guojun", ""], ["Li", "Stan Z.", ""]]}, {"id": "1806.03019", "submitter": "Masahiro Oda Dr.", "authors": "Masahiro Oda, Natsuki Shimizu, Holger R. Roth, Ken'ichi Karasawa,\n  Takayuki Kitasaka, Kazunari Misawa, Michitaka Fujiwara, Daniel Rueckert,\n  Kensaku Mori", "title": "3D FCN Feature Driven Regression Forest-Based Pancreas Localization and\n  Segmentation", "comments": "Presented in MICCAI 2017 workshop, DLMIA 2017 (Deep Learning in\n  Medical Image Analysis and Multimodal Learning for Clinical Decision Support)", "journal-ref": "DLMIA 2017, ML-CDS 2017: Deep Learning in Medical Image Analysis\n  and Multimodal Learning for Clinical Decision Support, pp.222-230", "doi": "10.1007/978-3-319-67558-9_26", "report-no": "Published in LNCS Vol.10553", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fully automated atlas-based pancreas segmentation\nmethod from CT volumes utilizing 3D fully convolutional network (FCN)\nfeature-based pancreas localization. Segmentation of the pancreas is difficult\nbecause it has larger inter-patient spatial variations than other organs.\nPrevious pancreas segmentation methods failed to deal with such variations. We\npropose a fully automated pancreas segmentation method that contains novel\nlocalization and segmentation. Since the pancreas neighbors many other organs,\nits position and size are strongly related to the positions of the surrounding\norgans. We estimate the position and the size of the pancreas (localized) from\nglobal features by regression forests. As global features, we use intensity\ndifferences and 3D FCN deep learned features, which include automatically\nextracted essential features for segmentation. We chose 3D FCN features from a\ntrained 3D U-Net, which is trained to perform multi-organ segmentation. The\nglobal features include both the pancreas and surrounding organ information.\nAfter localization, a patient-specific probabilistic atlas-based pancreas\nsegmentation is performed. In evaluation results with 146 CT volumes, we\nachieved 60.6% of the Jaccard index and 73.9% of the Dice overlap.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 08:34:30 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Oda", "Masahiro", ""], ["Shimizu", "Natsuki", ""], ["Roth", "Holger R.", ""], ["Karasawa", "Ken'ichi", ""], ["Kitasaka", "Takayuki", ""], ["Misawa", "Kazunari", ""], ["Fujiwara", "Michitaka", ""], ["Rueckert", "Daniel", ""], ["Mori", "Kensaku", ""]]}, {"id": "1806.03027", "submitter": "Xi Zhang", "authors": "Xu Ouyang, Xi Zhang, Di Ma, Gady Agam", "title": "Generating Image Sequence from Description with LSTM Conditional GAN", "comments": "Accepted by ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating images from word descriptions is a challenging task. Generative\nadversarial networks(GANs) are shown to be able to generate realistic images of\nreal-life objects. In this paper, we propose a new neural network architecture\nof LSTM Conditional Generative Adversarial Networks to generate images of\nreal-life objects. Our proposed model is trained on the Oxford-102 Flowers and\nCaltech-UCSD Birds-200-2011 datasets. We demonstrate that our proposed model\nproduces the better results surpassing other state-of-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 08:47:18 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Ouyang", "Xu", ""], ["Zhang", "Xi", ""], ["Ma", "Di", ""], ["Agam", "Gady", ""]]}, {"id": "1806.03028", "submitter": "Amir Nazemi", "authors": "Amir Nazemi, Mohammad Javad Shafiee, Zohreh Azimifar and Alexander\n  Wong", "title": "Unsupervised Feature Learning Toward a Real-time Vehicle Make and Model\n  Recognition", "comments": "15 pages include 14 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vehicle Make and Model Recognition (MMR) systems provide a fully automatic\nframework to recognize and classify different vehicle models. Several\napproaches have been proposed to address this challenge, however they can\nperform in restricted conditions. Here, we formulate the vehicle make and model\nrecognition as a fine-grained classification problem and propose a new\nconfigurable on-road vehicle make and model recognition framework. We benefit\nfrom the unsupervised feature learning methods and in more details we employ\nLocality constraint Linear Coding (LLC) method as a fast feature encoder for\nencoding the input SIFT features. The proposed method can perform in real\nenvironments of different conditions. This framework can recognize fifty models\nof vehicles and has an advantage to classify every other vehicle not belonging\nto one of the specified fifty classes as an unknown vehicle. The proposed MMR\nframework can be configured to become faster or more accurate based on the\napplication domain. The proposed approach is examined on two datasets including\nIranian on-road vehicle dataset and CompuCar dataset. The Iranian on-road\nvehicle dataset contains images of 50 models of vehicles captured in real\nsituations by traffic cameras in different weather and lighting conditions.\nExperimental results show superiority of the proposed framework over the\nstate-of-the-art methods on Iranian on-road vehicle datatset and comparable\nresults on CompuCar dataset with 97.5% and 98.4% accuracies, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 08:53:50 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Nazemi", "Amir", ""], ["Shafiee", "Mohammad Javad", ""], ["Azimifar", "Zohreh", ""], ["Wong", "Alexander", ""]]}, {"id": "1806.03051", "submitter": "Frederic Jurie", "authors": "Michel Moukari, Sylvaine Picard, Loic Simon, Fr\\'ed\\'eric Jurie", "title": "Deep multi-scale architectures for monocular depth estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at understanding the role of multi-scale information in the\nestimation of depth from monocular images. More precisely, the paper\ninvestigates four different deep CNN architectures, designed to explicitly make\nuse of multi-scale features along the network, and compare them to a\nstate-of-the-art single-scale approach. The paper also shows that involving\nmulti-scale features in depth estimation not only improves the performance in\nterms of accuracy, but also gives qualitatively better depth maps. Experiments\nare done on the widely used NYU Depth dataset, on which the proposed method\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 09:49:10 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Moukari", "Michel", ""], ["Picard", "Sylvaine", ""], ["Simon", "Loic", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1806.03084", "submitter": "Qingqiu Huang", "authors": "Qingqiu Huang, Yu Xiong, Dahua Lin", "title": "Unifying Identification and Context Learning for Person Recognition", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of face recognition techniques, recognizing persons\nunder unconstrained settings remains challenging. Issues like profile views,\nunfavorable lighting, and occlusions can cause substantial difficulties.\nPrevious works have attempted to tackle this problem by exploiting the context,\ne.g. clothes and social relations. While showing promising improvement, they\nare usually limited in two important aspects, relying on simple heuristics to\ncombine different cues and separating the construction of context from people\nidentities. In this work, we aim to move beyond such limitations and propose a\nnew framework to leverage context for person recognition. In particular, we\npropose a Region Attention Network, which is learned to adaptively combine\nvisual cues with instance-dependent weights. We also develop a unified\nformulation, where the social contexts are learned along with the reasoning of\npeople identities. These models substantially improve the robustness when\nworking with the complex contextual relations in unconstrained environments. On\ntwo large datasets, PIPA and Cast In Movies (CIM), a new dataset proposed in\nthis work, our method consistently achieves state-of-the-art performance under\nmultiple evaluation policies.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 11:05:05 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Huang", "Qingqiu", ""], ["Xiong", "Yu", ""], ["Lin", "Dahua", ""]]}, {"id": "1806.03106", "submitter": "Alain Jungo", "authors": "Alain Jungo, Raphael Meier, Ekin Ermis, Evelyn Herrmann, Mauricio\n  Reyes", "title": "Uncertainty-driven Sanity Check: Application to Postoperative Brain\n  Tumor Cavity Segmentation", "comments": "Appears in Medical Imaging with Deep Learning (MIDL), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty estimates of modern neuronal networks provide additional\ninformation next to the computed predictions and are thus expected to improve\nthe understanding of the underlying model. Reliable uncertainties are\nparticularly interesting for safety-critical computer-assisted applications in\nmedicine, e.g., neurosurgical interventions and radiotherapy planning. We\npropose an uncertainty-driven sanity check for the identification of\nsegmentation results that need particular expert review. Our method uses a\nfully-convolutional neural network and computes uncertainty estimates by the\nprinciple of Monte Carlo dropout. We evaluate the performance of the proposed\nmethod on a clinical dataset with 30 postoperative brain tumor images. The\nmethod can segment the highly inhomogeneous resection cavities accurately (Dice\ncoefficients 0.792 $\\pm$ 0.154). Furthermore, the proposed sanity check is able\nto detect the worst segmentation and three out of the four outliers. The\nresults highlight the potential of using the additional information from the\nmodel's parameter uncertainty to validate the segmentation performance of a\ndeep learning model.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:09:39 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Jungo", "Alain", ""], ["Meier", "Raphael", ""], ["Ermis", "Ekin", ""], ["Herrmann", "Evelyn", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1806.03111", "submitter": "Stefano Moriconi", "authors": "Stefano Moriconi, Maria A. Zuluaga, H. Rolf J\\\"ager, Parashkev Nachev,\n  S\\'ebastien Ourselin, M. Jorge Cardoso", "title": "VTrails: Inferring Vessels with Geodesic Connectivity Trees", "comments": null, "journal-ref": "IPMI 2017: Information Processing in Medical Imaging pp 672-684", "doi": "10.1007/978-3-319-59050-9_53", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of vessel morphology and connectivity has an impact on a number\nof cardiovascular and neurovascular applications by providing patient-specific\nhigh-level quantitative features such as spatial location, direction and scale.\nIn this paper we present an end-to-end approach to extract an acyclic vascular\ntree from angiographic data by solving a connectivity-enforcing anisotropic\nfast marching over a voxel-wise tensor field representing the orientation of\nthe underlying vascular tree. The method is validated using synthetic and real\nvascular images. We compare VTrails against classical and state-of-the-art\nridge detectors for tubular structures by assessing the connectedness of the\nvesselness map and inspecting the synthesized tensor field as proof of concept.\nVTrails performance is evaluated on images with different levels of\ndegradation: we verify that the extracted vascular network is an acyclic graph\n(i.e. a tree), and we report the extraction accuracy, precision and recall.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:16:30 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Moriconi", "Stefano", ""], ["Zuluaga", "Maria A.", ""], ["J\u00e4ger", "H. Rolf", ""], ["Nachev", "Parashkev", ""], ["Ourselin", "S\u00e9bastien", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1806.03228", "submitter": "Amir Alansary", "authors": "Amir Alansary, Loic Le Folgoc, Ghislain Vaillant, Ozan Oktay, Yuanwei\n  Li, Wenjia Bai, Jonathan Passerat-Palmbach, Ricardo Guerrero, Konstantinos\n  Kamnitsas, Benjamin Hou, Steven McDonagh, Ben Glocker, Bernhard Kainz, Daniel\n  Rueckert", "title": "Automatic View Planning with Multi-scale Deep Reinforcement Learning\n  Agents", "comments": "Accepted for MICCAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully automatic method to find standardized view planes in 3D\nimage acquisitions. Standard view images are important in clinical practice as\nthey provide a means to perform biometric measurements from similar anatomical\nregions. These views are often constrained to the native orientation of a 3D\nimage acquisition. Navigating through target anatomy to find the required view\nplane is tedious and operator-dependent. For this task, we employ a multi-scale\nreinforcement learning (RL) agent framework and extensively evaluate several\nDeep Q-Network (DQN) based strategies. RL enables a natural learning paradigm\nby interaction with the environment, which can be used to mimic experienced\noperators. We evaluate our results using the distance between the anatomical\nlandmarks and detected planes, and the angles between their normal vector and\ntarget. The proposed algorithm is assessed on the mid-sagittal and\nanterior-posterior commissure planes of brain MRI, and the 4-chamber long-axis\nplane commonly used in cardiac MRI, achieving accuracy of 1.53mm, 1.98mm and\n4.84mm, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 15:49:45 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Alansary", "Amir", ""], ["Folgoc", "Loic Le", ""], ["Vaillant", "Ghislain", ""], ["Oktay", "Ozan", ""], ["Li", "Yuanwei", ""], ["Bai", "Wenjia", ""], ["Passerat-Palmbach", "Jonathan", ""], ["Guerrero", "Ricardo", ""], ["Kamnitsas", "Konstantinos", ""], ["Hou", "Benjamin", ""], ["McDonagh", "Steven", ""], ["Glocker", "Ben", ""], ["Kainz", "Bernhard", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1806.03265", "submitter": "Weicheng Kuo", "authors": "Weicheng Kuo, Christian H\\\"ane, Esther Yuh, Pratik Mukherjee, Jitendra\n  Malik", "title": "PatchFCN for Intracranial Hemorrhage Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of detecting and segmenting acute intracranial\nhemorrhage on head computed tomography (CT) scans. We propose to solve both\ntasks as a semantic segmentation problem using a patch-based fully\nconvolutional network (PatchFCN). This formulation allows us to accurately\nlocalize hemorrhages while bypassing the complexity of object detection. Our\nsystem demonstrates competitive performance with a human expert and the\nstate-of-the-art on classification tasks (0.976, 0.966 AUC of ROC on\nretrospective and prospective test sets) and on segmentation tasks (0.785 pixel\nAP, 0.766 Dice score), while using much less data and a simpler system. In\naddition, we conduct a series of controlled experiments to understand \"why\"\nPatchFCN outperforms standard FCN. Our studies show that PatchFCN finds a good\ntrade-off between batch diversity and the amount of context during training.\nThese findings may also apply to other medical segmentation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 16:28:05 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 23:09:29 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kuo", "Weicheng", ""], ["H\u00e4ne", "Christian", ""], ["Yuh", "Esther", ""], ["Mukherjee", "Pratik", ""], ["Malik", "Jitendra", ""]]}, {"id": "1806.03275", "submitter": "Xiaoshuai Zhang", "authors": "Xiaoshuai Zhang, Wenhan Yang, Yueyu Hu, Jiaying Liu", "title": "DMCNN: Dual-Domain Multi-Scale Convolutional Neural Network for\n  Compression Artifacts Removal", "comments": "To appear in IEEE ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JPEG is one of the most commonly used standards among lossy image compression\nmethods. However, JPEG compression inevitably introduces various kinds of\nartifacts, especially at high compression rates, which could greatly affect the\nQuality of Experience (QoE). Recently, convolutional neural network (CNN) based\nmethods have shown excellent performance for removing the JPEG artifacts. Lots\nof efforts have been made to deepen the CNNs and extract deeper features, while\nrelatively few works pay attention to the receptive field of the network. In\nthis paper, we illustrate that the quality of output images can be\nsignificantly improved by enlarging the receptive fields in many cases. One\nstep further, we propose a Dual-domain Multi-scale CNN (DMCNN) to take full\nadvantage of redundancies on both the pixel and DCT domains. Experiments show\nthat DMCNN sets a new state-of-the-art for the task of JPEG artifact removal.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 17:01:52 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 07:22:49 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zhang", "Xiaoshuai", ""], ["Yang", "Wenhan", ""], ["Hu", "Yueyu", ""], ["Liu", "Jiaying", ""]]}, {"id": "1806.03318", "submitter": "Robert DiPietro", "authors": "Robert DiPietro and Gregory D. Hager", "title": "Unsupervised Learning for Surgical Motion by Learning to Predict the\n  Future", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that it is possible to learn meaningful representations of surgical\nmotion, without supervision, by learning to predict the future. An architecture\nthat combines an RNN encoder-decoder and mixture density networks (MDNs) is\ndeveloped to model the conditional distribution over future motion given past\nmotion. We show that the learned encodings naturally cluster according to\nhigh-level activities, and we demonstrate the usefulness of these learned\nencodings in the context of information retrieval, where a database of surgical\nmotion is searched for suturing activity using a motion-based query. Future\nprediction with MDNs is found to significantly outperform simpler baselines as\nwell as the best previously-published result for this task, advancing\nstate-of-the-art performance from an F1 score of 0.60 +- 0.14 to 0.77 +- 0.05.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 18:47:06 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["DiPietro", "Robert", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1806.03348", "submitter": "Mohammad Akbari", "authors": "Mohammad Akbari, Jie Liang, Jingning Han", "title": "DSSLIC: Deep Semantic Segmentation-based Layered Image Compression", "comments": "- More Experimental results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has revolutionized many computer vision fields in the last few\nyears, including learning-based image compression. In this paper, we propose a\ndeep semantic segmentation-based layered image compression (DSSLIC) framework\nin which the semantic segmentation map of the input image is obtained and\nencoded as the base layer of the bit-stream. A compact representation of the\ninput image is also generated and encoded as the first enhancement layer. The\nsegmentation map and the compact version of the image are then employed to\nobtain a coarse reconstruction of the image. The residual between the input and\nthe coarse reconstruction is additionally encoded as another enhancement layer.\nExperimental results show that the proposed framework outperforms the\nH.265/HEVC-based BPG and other codecs in both PSNR and MS-SSIM metrics across a\nwide range of bit rates in RGB domain. Besides, since semantic segmentation map\nis included in the bit-stream, the proposed scheme can facilitate many other\ntasks such as image search and object-based adaptive image compression.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 20:38:34 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 09:03:51 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 19:01:30 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Akbari", "Mohammad", ""], ["Liang", "Jie", ""], ["Han", "Jingning", ""]]}, {"id": "1806.03361", "submitter": "Jessica Souza Sena", "authors": "Jessica Sena and Artur Jordao and William Robson Schwartz", "title": "A Content-Based Late Fusion Approach Applied to Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variety of pedestrians detectors proposed in recent years has encouraged\nsome works to fuse pedestrian detectors to achieve a more accurate detection.\nThe intuition behind is to combine the detectors based on its spatial\nconsensus. We propose a novel method called Content-Based Spatial Consensus\n(CSBC), which, in addition to relying on spatial consensus, considers the\ncontent of the detection windows to learn a weighted-fusion of pedestrian\ndetectors. The result is a reduction in false alarms and an enhancement in the\ndetection. In this work, we also demonstrate that there is small influence of\nthe feature used to learn the contents of the windows of each detector, which\nenables our method to be efficient even employing simple features. The CSBC\novercomes state-of-the-art fusion methods in the ETH dataset and in the Caltech\ndataset. Particularly, our method is more efficient since fewer detectors are\nnecessary to achieve expressive results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 21:35:09 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Sena", "Jessica", ""], ["Jordao", "Artur", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1806.03370", "submitter": "Etienne Pot", "authors": "Etienne Pot, Alexander Toshev, Jana Kosecka", "title": "Self-supervisory Signals for Object Discovery and Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In robotic applications, we often face the challenge of discovering new\nobjects while having very little or no labelled training data. In this paper we\nexplore the use of self-supervision provided by a robot traversing an\nenvironment to learn representations of encountered objects. Knowledge of\nego-motion and depth perception enables the agent to effectively associate\nmultiple object proposals, which serve as training data for learning object\nrepresentations from unlabelled images. We demonstrate the utility of this\nrepresentation in two ways. First, we can automatically discover objects by\nperforming clustering in the learned embedding space. Each resulting cluster\ncontains examples of one instance seen from various viewpoints and scales.\nSecond, given a small number of labeled images, we can efficiently learn\ndetectors for these labels. In the few-shot regime, these detectors have a\nsubstantially higher mAP of 0.22 compared to 0.12 of off-the-shelf standard\ndetectors trained on this limited data. Thus, the proposed self-supervision\nresults in effective environment specific object discovery and detection at no\nor very small human labeling cost.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 22:50:28 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Pot", "Etienne", ""], ["Toshev", "Alexander", ""], ["Kosecka", "Jana", ""]]}, {"id": "1806.03379", "submitter": "Suren Jayasuriya", "authors": "Li-Chi Huang, Kuldeep Kulkarni, Anik Jha, Suhas Lohit, Suren\n  Jayasuriya, Pavan Turaga", "title": "CS-VQA: Visual Question Answering with Compressively Sensed Images", "comments": "5 pages, 2 figures, accepted to ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is a complex semantic task requiring both\nnatural language processing and visual recognition. In this paper, we explore\nwhether VQA is solvable when images are captured in a sub-Nyquist compressive\nparadigm. We develop a series of deep-network architectures that exploit\navailable compressive data to increasing degrees of accuracy, and show that VQA\nis indeed solvable in the compressed domain. Our results show that there is\nnominal degradation in VQA performance when using compressive measurements, but\nthat accuracy can be recovered when VQA pipelines are used in conjunction with\nstate-of-the-art deep neural networks for CS reconstruction. The results\npresented yield important implications for resource-constrained VQA\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 23:26:22 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Huang", "Li-Chi", ""], ["Kulkarni", "Kuldeep", ""], ["Jha", "Anik", ""], ["Lohit", "Suhas", ""], ["Jayasuriya", "Suren", ""], ["Turaga", "Pavan", ""]]}, {"id": "1806.03412", "submitter": "Philipp Lottes", "authors": "Philipp Lottes, Jens Behley, Andres Milioto, and Cyrill Stachniss", "title": "Fully Convolutional Networks with Sequential Information for Robust Crop\n  and Weed Detection in Precision Farming", "comments": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L), 2018", "journal-ref": null, "doi": "10.1109/LRA.2018.2846289", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the use of agrochemicals is an important component towards\nsustainable agriculture. Robots that can perform targeted weed control offer\nthe potential to contribute to this goal, for example, through specialized\nweeding actions such as selective spraying or mechanical weed removal. A\nprerequisite of such systems is a reliable and robust plant classification\nsystem that is able to distinguish crop and weed in the field. A major\nchallenge in this context is the fact that different fields show a large\nvariability. Thus, classification systems have to robustly cope with\nsubstantial environmental changes with respect to weed pressure and weed types,\ngrowth stages of the crop, visual appearance, and soil conditions. In this\npaper, we propose a novel crop-weed classification system that relies on a\nfully convolutional network with an encoder-decoder structure and incorporates\nspatial information by considering image sequences. Exploiting the crop\narrangement information that is observable from the image sequences enables our\nsystem to robustly estimate a pixel-wise labeling of the images into crop and\nweed, i.e., a semantic segmentation. We provide a thorough experimental\nevaluation, which shows that our system generalizes well to previously unseen\nfields under varying environmental conditions --- a key capability to actually\nuse such systems in precision framing. We provide comparisons to other\nstate-of-the-art approaches and show that our system substantially improves the\naccuracy of crop-weed classification without requiring a retraining of the\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 04:56:01 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Lottes", "Philipp", ""], ["Behley", "Jens", ""], ["Milioto", "Andres", ""], ["Stachniss", "Cyrill", ""]]}, {"id": "1806.03413", "submitter": "Philipp Lottes", "authors": "Philipp Lottes, Jens Behley, Nived Chebrolu, Andres Milioto, and\n  Cyrill Stachniss", "title": "Joint Stem Detection and Crop-Weed Classification for Plant-specific\n  Treatment in Precision Farming", "comments": "Submitted to the International Conference on Intelligent Robots and\n  Systems (IROS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying agrochemicals is the default procedure for conventional weed control\nin crop production, but has negative impacts on the environment. Robots have\nthe potential to treat every plant in the field individually and thus can\nreduce the required use of such chemicals. To achieve that, robots need the\nability to identify crops and weeds in the field and must additionally select\neffective treatments. While certain types of weed can be treated mechanically,\nother types need to be treated by (selective) spraying. In this paper, we\npresent an approach that provides the necessary information for effective\nplant-specific treatment. It outputs the stem location for weeds, which allows\nfor mechanical treatments, and the covered area of the weed for selective\nspraying. Our approach uses an end-to-end trainable fully convolutional network\nthat simultaneously estimates stem positions as well as the covered area of\ncrops and weeds. It jointly learns the class-wise stem detection and the\npixel-wise semantic segmentation. Experimental evaluations on different\nreal-world datasets show that our approach is able to reliably solve this\nproblem. Compared to state-of-the-art approaches, our approach not only\nsubstantially improves the stem detection accuracy, i.e., distinguishing crop\nand weed stems, but also provides an improvement in the semantic segmentation\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 04:56:07 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Lottes", "Philipp", ""], ["Behley", "Jens", ""], ["Chebrolu", "Nived", ""], ["Milioto", "Andres", ""], ["Stachniss", "Cyrill", ""]]}, {"id": "1806.03430", "submitter": "Shiqian Ma", "authors": "Shiqian Ma, Necdet Serhat Aybat", "title": "Efficient Optimization Algorithms for Robust Principal Component\n  Analysis and Its Variants", "comments": "to appear in Proceedings of the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust PCA has drawn significant attention in the last decade due to its\nsuccess in numerous application domains, ranging from bio-informatics,\nstatistics, and machine learning to image and video processing in computer\nvision. Robust PCA and its variants such as sparse PCA and stable PCA can be\nformulated as optimization problems with exploitable special structures. Many\nspecialized efficient optimization methods have been proposed to solve robust\nPCA and related problems. In this paper we review existing optimization methods\nfor solving convex and nonconvex relaxations/variants of robust PCA, discuss\ntheir advantages and disadvantages, and elaborate on their convergence\nbehaviors. We also provide some insights for possible future research\ndirections including new algorithmic frameworks that might be suitable for\nimplementing on multi-processor setting to handle large-scale problems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 07:27:01 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Ma", "Shiqian", ""], ["Aybat", "Necdet Serhat", ""]]}, {"id": "1806.03445", "submitter": "Hongjiao Guan", "authors": "Hongjiao Guan, Yingtao Zhang, H. D. Cheng and Xianglong Tang", "title": "Abstaining Classification When Error Costs are Unequal and Unknown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstaining classificaiton aims to reject to classify the easily misclassified\nexamples, so it is an effective approach to increase the clasificaiton\nreliability and reduce the misclassification risk in the cost-sensitive\napplications. In such applications, different types of errors (false positive\nor false negative) usaully have unequal costs. And the error costs, which\ndepend on specific applications, are usually unknown. However, current\nabstaining classification methods either do not distinguish the error types, or\nthey need the cost information of misclassification and rejection, which are\nrealized in the framework of cost-sensitive learning. In this paper, we propose\na bounded-abstention method with two constraints of reject rates (BA2), which\nperforms abstaining classification when error costs are unequal and unknown.\nBA2 aims to obtain the optimal area under the ROC curve (AUC) by constraining\nthe reject rates of the positive and negative classes respectively.\nSpecifically, we construct the receiver operating characteristic (ROC) curve,\nand stepwise search the optimal reject thresholds from both ends of the curve,\nuntill the two constraints are satisfied. Experimental results show that BA2\nobtains higher AUC and lower total cost than the state-of-the-art abstaining\nclassification methods. Meanwhile, BA2 achieves controllable reject rates of\nthe positive and negative classes.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 09:00:08 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 03:37:34 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Guan", "Hongjiao", ""], ["Zhang", "Yingtao", ""], ["Cheng", "H. D.", ""], ["Tang", "Xianglong", ""]]}, {"id": "1806.03465", "submitter": "Ivan Kre\\v{s}o", "authors": "Ivan Kre\\v{s}o, Marin Or\\v{s}i\\'c, Petra Bevandi\\'c, Sini\\v{s}a\n  \\v{S}egvi\\'c", "title": "Robust Semantic Segmentation with Ladder-DenseNet Models", "comments": "4 pages, 4 figures, CVPR 2018 Robust Vision Challenge Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present semantic segmentation experiments with a model capable to perform\npredictions on four benchmark datasets: Cityscapes, ScanNet, WildDash and\nKITTI. We employ a ladder-style convolutional architecture featuring a modified\nDenseNet-169 model in the downsampling datapath, and only one convolution in\neach stage of the upsampling datapath. Due to limited computing resources, we\nperform the training only on Cityscapes Fine train+val, ScanNet train, WildDash\nval and KITTI train. We evaluate the trained model on the test subsets of the\nfour benchmarks in concordance with the guidelines of the Robust Vision\nChallenge ROB 2018. The performed experiments reveal several interesting\nfindings which we describe and discuss.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 11:48:23 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Kre\u0161o", "Ivan", ""], ["Or\u0161i\u0107", "Marin", ""], ["Bevandi\u0107", "Petra", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "1806.03486", "submitter": "Pieter Van Molle", "authors": "Pieter Van Molle, Tim Verbelen, Elias De Coninck, Cedric De Boom,\n  Pieter Simoens, Bart Dhoedt", "title": "Learning to Grasp from a Single Demonstration", "comments": "10 pages, 5 figures, IAS-15 2018 workshop on Learning Applications\n  for Intelligent Autonomous Robots", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based approaches for robotic grasping using visual sensors typically\nrequire collecting a large size dataset, either manually labeled or by many\ntrial and errors of a robotic manipulator in the real or simulated world. We\npropose a simpler learning-from-demonstration approach that is able to detect\nthe object to grasp from merely a single demonstration using a convolutional\nneural network we call GraspNet. In order to increase robustness and decrease\nthe training time even further, we leverage data from previous demonstrations\nto quickly fine-tune a GrapNet for each new demonstration. We present some\npreliminary results on a grasping experiment with the Franka Panda cobot for\nwhich we can train a GraspNet with only hundreds of train iterations.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 15:30:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Van Molle", "Pieter", ""], ["Verbelen", "Tim", ""], ["De Coninck", "Elias", ""], ["De Boom", "Cedric", ""], ["Simoens", "Pieter", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1806.03497", "submitter": "Siyuan Qi", "authors": "Siyuan Qi, Baoxiong Jia, Song-Chun Zhu", "title": "Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data\n  for Future Prediction", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future predictions on sequence data (e.g., videos or audios) require the\nalgorithms to capture non-Markovian and compositional properties of high-level\nsemantics. Context-free grammars are natural choices to capture such\nproperties, but traditional grammar parsers (e.g., Earley parser) only take\nsymbolic sentences as inputs. In this paper, we generalize the Earley parser to\nparse sequence data which is neither segmented nor labeled. This generalized\nEarley parser integrates a grammar parser with a classifier to find the optimal\nsegmentation and labels, and makes top-down future predictions. Experiments\nshow that our method significantly outperforms other approaches for future\nhuman activity prediction.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 16:07:02 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Qi", "Siyuan", ""], ["Jia", "Baoxiong", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1806.03510", "submitter": "Alexey Shvets", "authors": "Selim S. Seferbekov, Vladimir I. Iglovikov, Alexander V. Buslaev and\n  Alexey A. Shvets", "title": "Feature Pyramid Network for Multi-Class Land Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation is in-demand in satellite imagery processing. Because\nof the complex environment, automatic categorization and segmentation of land\ncover is a challenging problem. Solving it can help to overcome many obstacles\nin urban planning, environmental engineering or natural landscape monitoring.\nIn this paper, we propose an approach for automatic multi-class land\nsegmentation based on a fully convolutional neural network of feature pyramid\nnetwork (FPN) family. This network is consisted of pre-trained on ImageNet\nResnet50 encoder and neatly developed decoder. Based on validation results,\nleaderboard score and our own experience this network shows reliable results\nfor the DEEPGLOBE - CVPR 2018 land cover classification sub-challenge.\nMoreover, this network moderately uses memory that allows using GTX 1080 or\n1080 TI video cards to perform whole training and makes pretty fast\npredictions.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 17:30:30 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 19:17:09 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Seferbekov", "Selim S.", ""], ["Iglovikov", "Vladimir I.", ""], ["Buslaev", "Alexander V.", ""], ["Shvets", "Alexey A.", ""]]}, {"id": "1806.03535", "submitter": "Uwe Schmidt", "authors": "Uwe Schmidt, Martin Weigert, Coleman Broaddus, Gene Myers", "title": "Cell Detection with Star-convex Polygons", "comments": "Conference paper at MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00934-2_30", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection and segmentation of cells and nuclei in microscopy images\nis important for many biological applications. Recent successful learning-based\napproaches include per-pixel cell segmentation with subsequent pixel grouping,\nor localization of bounding boxes with subsequent shape refinement. In\nsituations of crowded cells, these can be prone to segmentation errors, such as\nfalsely merging bordering cells or suppressing valid cell instances due to the\npoor approximation with bounding boxes. To overcome these issues, we propose to\nlocalize cell nuclei via star-convex polygons, which are a much better shape\nrepresentation as compared to bounding boxes and thus do not need shape\nrefinement. To that end, we train a convolutional neural network that predicts\nfor every pixel a polygon for the cell instance at that position. We\ndemonstrate the merits of our approach on two synthetic datasets and one\nchallenging dataset of diverse fluorescence microscopy images.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 19:38:24 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 15:25:25 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Schmidt", "Uwe", ""], ["Weigert", "Martin", ""], ["Broaddus", "Coleman", ""], ["Myers", "Gene", ""]]}, {"id": "1806.03536", "submitter": "Keyulu Xu", "authors": "Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi\n  Kawarabayashi, Stefanie Jegelka", "title": "Representation Learning on Graphs with Jumping Knowledge Networks", "comments": "ICML 2018, accepted as a long oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning approaches for representation learning on graphs follow\na neighborhood aggregation procedure. We analyze some important properties of\nthese models, and propose a strategy to overcome those. In particular, the\nrange of \"neighboring\" nodes that a node's representation draws from strongly\ndepends on the graph structure, analogous to the spread of a random walk. To\nadapt to local neighborhood properties and tasks, we explore an architecture --\njumping knowledge (JK) networks -- that flexibly leverages, for each node,\ndifferent neighborhood ranges to enable better structure-aware representation.\nIn a number of experiments on social, bioinformatics and citation networks, we\ndemonstrate that our model achieves state-of-the-art performance. Furthermore,\ncombining the JK framework with models like Graph Convolutional Networks,\nGraphSAGE and Graph Attention Networks consistently improves those models'\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 19:49:57 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 19:52:28 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Xu", "Keyulu", ""], ["Li", "Chengtao", ""], ["Tian", "Yonglong", ""], ["Sonobe", "Tomohiro", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1806.03556", "submitter": "Akila Pemasiri", "authors": "Akila Pemasiri and Kien Nguyen and Sridha Sridharan and Clinton Fookes", "title": "Sparse Over-complete Patch Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image patch matching, which is the process of identifying corresponding\npatches across images, has been used as a subroutine for many computer vision\nand image processing tasks. State -of-the-art patch matching techniques take\nimage patches as input to a convolutional neural network to extract the patch\nfeatures and evaluate their similarity. Our aim in this paper is to improve on\nthe state of the art patch matching techniques by observing the fact that a\nsparse-overcomplete representation of an image posses statistical properties of\nnatural visual scenes which can be exploited for patch matching. We propose a\nnew paradigm which encodes image patch details by encoding the patch and\nsubsequently using this sparse representation as input to a neural network to\ncompare the patches. As sparse coding is based on a generative model of natural\nimage patches, it can represent the patch in terms of the fundamental visual\ncomponents from which it has been composed of, leading to similar sparse codes\nfor patches which are built from similar components. Once the sparse coded\nfeatures are extracted, we employ a fully-connected neural network, which\ncaptures the non-linear relationships between features, for comparison. We have\nevaluated our approach using the Liberty and Notredame subsets of the popular\nUBC patch dataset and set a new benchmark outperforming all state-of-the-art\npatch matching techniques for these datasets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 23:18:11 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 07:21:38 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Pemasiri", "Akila", ""], ["Nguyen", "Kien", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1806.03560", "submitter": "Akila Pemasiri", "authors": "Akila Pemasiri and Kien Nguyen and Sridha Sridhara and and Clinton\n  Fookes", "title": "Semantic Correspondence: A Hierarchical Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing semantic correspondence across images when the objects in the\nimages have undergone complex deformations remains a challenging task in the\nfield of computer vision. In this paper, we propose a hierarchical method to\ntackle this problem by first semantically targeting the foreground objects to\nlocalize the search space and then looking deeply into multiple levels of the\nfeature representation to search for point-level correspondence. In contrast to\nexisting approaches, which typically penalize large discrepancies, our approach\nallows for significant displacements, with the aim to accommodate large\ndeformations of the objects in scene. Localizing the search space by\nsemantically matching object-level correspondence, our method robustly handles\nlarge deformations of objects. Representing the target region by concatenated\nhypercolumn features which take into account the hierarchical levels of the\nsurrounding context, helps to clear the ambiguity to further improve the\naccuracy. By conducting multiple experiments across scenes with non-rigid\nobjects, we validate the proposed approach, and show that it outperforms the\nstate of the art methods for semantic correspondence establishment.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 00:30:37 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Pemasiri", "Akila", ""], ["Nguyen", "Kien", ""], ["Sridhara", "Sridha", ""], ["Fookes", "and Clinton", ""]]}, {"id": "1806.03574", "submitter": "Duo Lu", "authors": "Duo Lu, Dijiang Huang, Anshul Rai", "title": "FMHash: Deep Hashing of In-Air-Handwriting for User Identification", "comments": "6 pages, 10 figures, deep hashing, in-air-handwriting, user\n  identification, biometrics, accepted by ICC 2019 security track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many mobile systems and wearable devices, such as Virtual Reality (VR) or\nAugmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID\nand password for signing into a virtual website. However, they are usually\nequipped with gesture capture interfaces to allow the user to interact with the\nsystem directly with hand gestures. Although gesture-based authentication has\nbeen well-studied, less attention is paid to the gesture-based user\nidentification problem, which is essentially an input method of account ID and\nan efficient searching and indexing method of a database of gesture signals. In\nthis paper, we propose FMHash (i.e., Finger Motion Hash), a user identification\nframework that can generate a compact binary hash code from a piece of\nin-air-handwriting of an ID string. This hash code enables indexing and fast\nsearch of a large account database using the in-air-handwriting by a hash\ntable. To demonstrate the effectiveness of the framework, we implemented a\nprototype and achieved >99.5% precision and >92.6% recall with exact hash code\nmatch on a dataset of 200 accounts collected by us. The ability of hashing\nin-air-handwriting pattern to binary code can be used to achieve convenient\nsign-in and sign-up with in-air-handwriting gesture ID on future mobile and\nwearable systems connected to the Internet.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 02:15:29 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 20:49:27 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Lu", "Duo", ""], ["Huang", "Dijiang", ""], ["Rai", "Anshul", ""]]}, {"id": "1806.03575", "submitter": "Yiqi Yan", "authors": "Yiqi Yan and Lei Zhang and Jun Li and Wei Wei and Yanning Zhang", "title": "Accurate Spectral Super-resolution from Single RGB Image Using\n  Multi-scale CNN", "comments": "PRCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from traditional hyperspectral super-resolution approaches that\nfocus on improving the spatial resolution, spectral super-resolution aims at\nproducing a high-resolution hyperspectral image from the RGB observation with\nsuper-resolution in spectral domain. However, it is challenging to accurately\nreconstruct a high-dimensional continuous spectrum from three discrete\nintensity values at each pixel, since too much information is lost during the\nprocedure where the latent hyperspectral image is downsampled (e.g., with x10\nscaling factor) in spectral domain to produce an RGB observation. To address\nthis problem, we present a multi-scale deep convolutional neural network (CNN)\nto explicitly map the input RGB image into a hyperspectral image. Through\nsymmetrically downsampling and upsampling the intermediate feature maps in a\ncascading paradigm, the local and non-local image information can be jointly\nencoded for spectral representation, ultimately improving the spectral\nreconstruction accuracy. Extensive experiments on a large hyperspectral dataset\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 02:32:02 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 05:29:18 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 08:59:39 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Yan", "Yiqi", ""], ["Zhang", "Lei", ""], ["Li", "Jun", ""], ["Wei", "Wei", ""], ["Zhang", "Yanning", ""]]}, {"id": "1806.03576", "submitter": "Yu Zhan", "authors": "Yu Zhan, Wan-Lei Zhao", "title": "Instance Search via Instance Level Segmentation and Feature\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance search is an interesting task as well as a challenging issue due to\nthe lack of effective feature representation. In this paper, an instance level\nfeature representation built upon fully convolutional instance-aware\nsegmentation is proposed. The feature is ROI-pooled from the segmented instance\nregion. So that instances in various sizes and layouts are represented by deep\nfeatures in uniform length. This representation is further enhanced by the use\nof deformable ResNeXt blocks. Superior performance is observed in terms of its\ndistinctiveness and scalability on a challenging evaluation dataset built by\nourselves. In addition, the proposed enhancement on the network structure also\nshows superior performance on the instance segmentation task.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 02:39:52 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 03:14:51 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Zhan", "Yu", ""], ["Zhao", "Wan-Lei", ""]]}, {"id": "1806.03580", "submitter": "Mehdi Faraji", "authors": "Yuying Li and Mehdi Faraji", "title": "EREL Selection using Morphological Relation", "comments": "6 pages, 8 figures, accepted to be published in International\n  Conference on SMART MULTIMEDIA, 2018. The final authenticated publication is\n  available online at https://doi.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work concentrates on Extremal Regions of Extremum Level (EREL)\nselection. EREL is a recently proposed feature detector aiming at detecting\nregions from a set of extremal regions. This is a branching problem derived\nfrom segmentation of arterial wall boundaries from Intravascular Ultrasound\n(IVUS) images. For each IVUS frame, a set of EREL regions is generated to\ndescribe the luminal area of human coronary. Each EREL is then fitted by an\nellipse to represent the luminal border. The goal is to assign the most\nappropriate EREL as the lumen. In this work, EREL selection carries out in two\nrounds. In the first round, the pattern in a set of EREL regions is analyzed\nand used to generate an approximate luminal region. Then, the two-dimensional\n(2D) correlation coefficients are computed between this approximate region and\neach EREL to keep the ones with tightest relevance. In the second round, a\ncompactness measure is calculated for each EREL and its fitted ellipse to\nguarantee that the resulting EREL has not affected by the common artifacts such\nas bifurcations, shadows, and side branches. We evaluated the selected ERELs in\nterms of Hausdorff Distance (HD) and Jaccard Measure (JM) on the train and test\nset of a publicly available dataset. The results show that our selection\nstrategy outperforms the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 04:03:46 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Li", "Yuying", ""], ["Faraji", "Mehdi", ""]]}, {"id": "1806.03584", "submitter": "Mehdi Faraji", "authors": "Mehdi Faraji, Anup Basu", "title": "A Simplified Active Calibration algorithm for Focal Length Estimation", "comments": "5 page, 4 figures, accepted to be published in International\n  Conference of Smart Multimedia, 2018. The final authenticated publication is\n  available online at https://doi.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new linear mathematical formulations to calculate the focal\nlength of a camera in an active platform. Through mathematical derivations, we\nshow that the focal lengths in each direction can be estimated using only one\npoint correspondence that relates images taken before and after a degenerate\nrotation of the camera. The new formulations will be beneficial in robotic and\ndynamic surveillance environments when the camera needs to be calibrated while\nit freely moves and zooms. By establishing a correspondence between only two\nimages taken after slightly panning and tilting the camera and a reference\nimage, our proposed Simplified Calibration Method is able to calculate the\nfocal length of the camera. We extensively evaluate the derived formulations on\na simulated camera, 3D scenes and real-world images. Our error analysis over\nsimulated and real images indicates that the proposed Simplified Active\nCalibration formulation estimates the parameters of a camera with low error\nrates.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 04:29:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Faraji", "Mehdi", ""], ["Basu", "Anup", ""]]}, {"id": "1806.03589", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas Huang", "title": "Free-Form Image Inpainting with Gated Convolution", "comments": "Accepted in ICCV 2019 Oral; open sourced; interactive demo available:\n  http://jiahuiyu.com/deepfill/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative image inpainting system to complete images with\nfree-form mask and guidance. The system is based on gated convolutions learned\nfrom millions of images without additional labelling efforts. The proposed\ngated convolution solves the issue of vanilla convolution that treats all input\npixels as valid ones, generalizes partial convolution by providing a learnable\ndynamic feature selection mechanism for each channel at each spatial location\nacross all layers. Moreover, as free-form masks may appear anywhere in images\nwith any shape, global and local GANs designed for a single rectangular mask\nare not applicable. Thus, we also present a patch-based GAN loss, named\nSN-PatchGAN, by applying spectral-normalized discriminator on dense image\npatches. SN-PatchGAN is simple in formulation, fast and stable in training.\nResults on automatic image inpainting and user-guided extension demonstrate\nthat our system generates higher-quality and more flexible results than\nprevious methods. Our system helps user quickly remove distracting objects,\nmodify image layouts, clear watermarks and edit faces. Code, demo and models\nare available at: https://github.com/JiahuiYu/generative_inpainting\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 05:51:32 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 03:06:37 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Yu", "Jiahui", ""], ["Lin", "Zhe", ""], ["Yang", "Jimei", ""], ["Shen", "Xiaohui", ""], ["Lu", "Xin", ""], ["Huang", "Thomas", ""]]}, {"id": "1806.03619", "submitter": "Gongning Luo", "authors": "Suyu Dong, Gongning Luo, Kuanquan Wang, Shaodong Cao, Ashley Mercado,\n  Olga Shmuilovich, Henggui Zhang, Shuo Li", "title": "VoxelAtlasGAN: 3D Left Ventricle Segmentation on Echocardiography with\n  Atlas Guided Generation and Voxel-to-voxel Discrimination", "comments": "Accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D left ventricle (LV) segmentation on echocardiography is very important for\ndiagnosis and treatment of cardiac disease. It is not only because of that\nechocardiography is a real-time imaging technology and widespread in clinical\napplication, but also because of that LV segmentation on 3D echocardiography\ncan provide more full volume information of heart than LV segmentation on 2D\nechocardiography. However, 3D LV segmentation on echocardiography is still an\nopen and challenging task owing to the lower contrast, higher noise and data\ndimensionality, limited annotation of 3D echocardiography. In this paper, we\nproposed a novel real-time framework, i.e., VoxelAtlasGAN, for 3D LV\nsegmentation on 3D echocardiography. This framework has three contributions: 1)\nIt is based on voxel-to-voxel conditional generative adversarial nets (cGAN).\nFor the first time, cGAN is used for 3D LV segmentation on echocardiography.\nAnd cGAN advantageously fuses substantial 3D spatial context information from\n3D echocardiography by self-learning structured loss; 2) For the first time, it\nembeds the atlas into an end-to-end optimization framework, which uses 3D LV\natlas as a powerful prior knowledge to improve the inference speed, address the\nlower contrast and the limited annotation problems of 3D echocardiography; 3)\nIt combines traditional discrimination loss and the new proposed consistent\nconstraint, which further improves the generalization of the proposed\nframework. VoxelAtlasGAN was validated on 60 subjects on 3D echocardiography\nand it achieved satisfactory segmentation results and high inference speed. The\nmean surface distance is 1.85 mm, the mean hausdorff surface distance is 7.26\nmm, mean dice is 0.953, the correlation of EF is 0.918, and the mean inference\nspeed is 0.1s. These results have demonstrated that our proposed method has\ngreat potential for clinical application\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 09:25:17 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Dong", "Suyu", ""], ["Luo", "Gongning", ""], ["Wang", "Kuanquan", ""], ["Cao", "Shaodong", ""], ["Mercado", "Ashley", ""], ["Shmuilovich", "Olga", ""], ["Zhang", "Henggui", ""], ["Li", "Shuo", ""]]}, {"id": "1806.03636", "submitter": "ShihChung Lo Ph.D.", "authors": "Shih Chung B. Lo, Matthew T. Freedman, Seong K. Mun, and Shuo Gu", "title": "Transformationally Identical and Invariant Convolutional Neural Networks\n  through Symmetric Element Operators", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematically speaking, a transformationally invariant operator, such as a\ntransformationally identical (TI) matrix kernel (i.e., K= T{K}), commutes with\nthe transformation (T{.}) itself when they operate on the first operand matrix.\nWe found that by consistently applying the same type of TI kernels in a\nconvolutional neural networks (CNN) system, the commutative property holds\nthroughout all layers of convolution processes with and without involving an\nactivation function and/or a 1D convolution across channels within a layer. We\nfurther found that any CNN possessing the same TI kernel property for all\nconvolution layers followed by a flatten layer with weight sharing among their\ntransformation corresponding elements would output the same result for all\ntransformation versions of the original input vector. In short, CNN[ Vi ] =\nCNN[ T{Vi} ] providing every K = T{K} in CNN, where Vi denotes input vector and\nCNN[.] represents the whole CNN process as a function of input vector that\nproduces an output vector. With such a transformationally identical CNN\n(TI-CNN) system, each transformation, that is not associated with a predefined\nTI used in data augmentation, would inherently include all of its corresponding\ntransformation versions of the input vector for the training. Hence the use of\nsame TI property for every kernel in the CNN would serve as an orientation or a\ntranslation independent training guide in conjunction with the\nerror-backpropagation during the training. This TI kernel property is desirable\nfor applications requiring a highly consistent output result from corresponding\ntransformation versions of an input. Several C programming routines are\nprovided to facilitate interested parties of using the TI-CNN technique which\nis expected to produce a better generalization performance than its ordinary\nCNN counterpart.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 11:16:36 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 04:58:17 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 22:42:54 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Lo", "Shih Chung B.", ""], ["Freedman", "Matthew T.", ""], ["Mun", "Seong K.", ""], ["Gu", "Shuo", ""]]}, {"id": "1806.03695", "submitter": "Mehdi Faraji", "authors": "Mehdi Faraji, Irene Cheng, Iris Naudin, Anup Basu", "title": "Segmentation of Arterial Walls in Intravascular Ultrasound\n  Cross-Sectional Images Using Extremal Region Selection", "comments": "15 pages, 5 figures, published in Elsevier Ultrasonics", "journal-ref": null, "doi": "10.1016/j.ultras.2017.11.020", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intravascular Ultrasound (IVUS) is an intra-operative imaging modality that\nfacilitates observing and appraising the vessel wall structure of the human\ncoronary arteries. Segmentation of arterial wall boundaries from the IVUS\nimages is not only crucial for quantitative analysis of the vessel walls and\nplaque characteristics, but is also necessary for generating 3D reconstructed\nmodels of the artery. The aim of this study is twofold. Firstly, we investigate\nthe feasibility of using a recently proposed region detector, namely Extremal\nRegion of Extremum Level (EREL) to delineate the luminal and media-adventitia\nborders in IVUS frames acquired by 20 MHz probes. Secondly, we propose a region\nselection strategy to label two ERELs as lumen and media based on the stability\nof their textural information. We extensively evaluated our selection strategy\non the test set of a standard publicly available dataset containing 326 IVUS\nB-mode images. We showed that in the best case, the average Hausdorff Distances\n(HD) between the extracted ERELs and the actual lumen and media were $0.22$ mm\nand $0.45$ mm, respectively. The results of our experiments revealed that our\nselection strategy was able to segment the lumen with $\\le 0.3$ mm HD to the\ngold standard even though the images contained major artifacts such as\nbifurcations, shadows, and side branches. Moreover, when there was no artifact,\nour proposed method was able to delineate media-adventitia boundaries with\n$0.31$ mm HD to the gold standard. Furthermore, our proposed segmentation\nmethod runs in time that is linear in the number of pixels in each frame. Based\non the results of this work, by using a 20 MHz IVUS probe with controlled\npullback, not only can we now analyze the internal structure of human arteries\nmore accurately, but also segment each frame during the pullback procedure\nbecause of the low run time of our proposed segmentation method.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 17:37:11 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Faraji", "Mehdi", ""], ["Cheng", "Irene", ""], ["Naudin", "Iris", ""], ["Basu", "Anup", ""]]}, {"id": "1806.03698", "submitter": "Dina Bashkirova", "authors": "Dina Bashkirova, Ben Usman and Kate Saenko", "title": "Unsupervised Video-to-Video Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation is a recently proposed task of\ntranslating an image to a different style or domain given only unpaired image\nexamples at training time. In this paper, we formulate a new task of\nunsupervised video-to-video translation, which poses its own unique challenges.\nTranslating video implies learning not only the appearance of objects and\nscenes but also realistic motion and transitions between consecutive frames.We\ninvestigate the performance of per-frame video-to-video translation using\nexisting image-to-image translation networks, and propose a spatio-temporal 3D\ntranslator as an alternative solution to this problem. We evaluate our 3D\nmethod on multiple synthetic datasets, such as moving colorized digits, as well\nas the realistic segmentation-to-video GTA dataset and a new CT-to-MRI\nvolumetric images translation dataset. Our results show that frame-wise\ntranslation produces realistic results on a single frame level but\nunderperforms significantly on the scale of the whole video compared to our\nthree-dimensional translation approach, which is better able to learn the\ncomplex structure of video and motion and continuity of object appearance.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 18:18:26 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Bashkirova", "Dina", ""], ["Usman", "Ben", ""], ["Saenko", "Kate", ""]]}, {"id": "1806.03720", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "title": "Stochastic seismic waveform inversion using generative adversarial\n  networks as a geological prior", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an application of deep generative models in the context of\npartial-differential equation (PDE) constrained inverse problems. We combine a\ngenerative adversarial network (GAN) representing an a priori model that\ncreates subsurface geological structures and their petrophysical properties,\nwith the numerical solution of the PDE governing the propagation of acoustic\nwaves within the earth's interior. We perform Bayesian inversion using an\napproximate Metropolis-adjusted Langevin algorithm (MALA) to sample from the\nposterior given seismic observations. Gradients with respect to the model\nparameters governing the forward problem are obtained by solving the adjoint of\nthe acoustic wave equation. Gradients of the mismatch with respect to the\nlatent variables are obtained by leveraging the differentiable nature of the\ndeep neural network used to represent the generative model. We show that\napproximate MALA sampling allows efficient Bayesian inversion of model\nparameters obtained from a prior represented by a deep generative model,\nobtaining a diverse set of realizations that reflect the observed seismic\nresponse.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 20:38:29 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Mosser", "Lukas", ""], ["Dubrule", "Olivier", ""], ["Blunt", "Martin J.", ""]]}, {"id": "1806.03724", "submitter": "Hexiang Hu", "authors": "Hexiang Hu, Wei-Lun Chao, Fei Sha", "title": "Learning Answer Embeddings for Visual Question Answering", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel probabilistic model for visual question answering (Visual\nQA). The key idea is to infer two sets of embeddings: one for the image and the\nquestion jointly and the other for the answers. The learning objective is to\nlearn the best parameterization of those embeddings such that the correct\nanswer has higher likelihood among all possible answers. In contrast to several\nexisting approaches of treating Visual QA as multi-way classification, the\nproposed approach takes the semantic relationships (as characterized by the\nembeddings) among answers into consideration, instead of viewing them as\nindependent ordinal numbers. Thus, the learned embedded function can be used to\nembed unseen answers (in the training dataset). These properties make the\napproach particularly appealing for transfer learning for open-ended Visual QA,\nwhere the source dataset on which the model is learned has limited overlapping\nwith the target dataset in the space of answers. We have also developed\nlarge-scale optimization techniques for applying the model to datasets with a\nlarge number of answers, where the challenge is to properly normalize the\nproposed probabilistic models. We validate our approach on several Visual QA\ndatasets and investigate its utility for transferring models across datasets.\nThe empirical results have shown that the approach performs well not only on\nin-domain learning but also on transfer learning.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 21:01:24 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Hu", "Hexiang", ""], ["Chao", "Wei-Lun", ""], ["Sha", "Fei", ""]]}, {"id": "1806.03726", "submitter": "Hexiang Hu", "authors": "Wei-Lun Chao, Hexiang Hu, Fei Sha", "title": "Cross-Dataset Adaptation for Visual Question Answering", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of cross-dataset adaptation for visual question\nanswering (Visual QA). Our goal is to train a Visual QA model on a source\ndataset but apply it to another target one. Analogous to domain adaptation for\nvisual recognition, this setting is appealing when the target dataset does not\nhave a sufficient amount of labeled data to learn an \"in-domain\" model. The key\nchallenge is that the two datasets are constructed differently, resulting in\nthe cross-dataset mismatch on images, questions, or answers.\n  We overcome this difficulty by proposing a novel domain adaptation algorithm.\nOur method reduces the difference in statistical distributions by transforming\nthe feature representation of the data in the target dataset. Moreover, it\nmaximizes the likelihood of answering questions (in the target dataset)\ncorrectly using the Visual QA model trained on the source dataset. We\nempirically studied the effectiveness of the proposed approach on adapting\namong several popular Visual QA datasets. We show that the proposed method\nimproves over baselines where there is no adaptation and several other\nadaptation methods. We both quantitatively and qualitatively analyze when the\nadaptation can be mostly effective.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 21:06:28 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Chao", "Wei-Lun", ""], ["Hu", "Hexiang", ""], ["Sha", "Fei", ""]]}, {"id": "1806.03753", "submitter": "Kapil Sharma Prof.", "authors": "Kapil Sharma, Gurjit Singh Walia, Ashish Kumar, Astitwa Saxena,\n  Kuldeep Singh", "title": "Robust Object Tracking with Crow Search Optimized Multi-cue Particle\n  Filter", "comments": "25 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Filter(PF) is used extensively for estimation of target Non-linear\nand Non-gaussian state. However, its performance suffers due to inherent\nproblem of sample degeneracy and impoverishment. In order to address this, we\npropose a novel resampling method based upon Crow Search Optimization to\novercome low performing particles detected as outlier. Proposed outlier\ndetection mechanism with transductive reliability achieve faster convergence of\nproposed PF tracking framework. In addition, we present an adaptive fuzzy\nfusion model to integrate multi-cue extracted for each evaluated particle.\nAutomatic boosting and suppression of particles using proposed fusion model not\nonly enhances performance of resampling method but also achieve optimal state\nestimation. Performance of the proposed tracker is evaluated over 12 benchmark\nvideo sequences and compared with state-of-the-art solutions. Qualitative and\nquantitative results reveals that the proposed tracker not only outperforms\nexisting solutions but also efficiently handle various tracking challenges. On\naverage of outcome, we achieve CLE of 7.98 and F-measure of 0.734.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 00:37:55 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Sharma", "Kapil", ""], ["Walia", "Gurjit Singh", ""], ["Kumar", "Ashish", ""], ["Saxena", "Astitwa", ""], ["Singh", "Kuldeep", ""]]}, {"id": "1806.03772", "submitter": "Guoxia Wang", "authors": "Guoxia Wang, Xiaohui Liang, Frederick W. B. Li", "title": "DOOBNet: Deep Object Occlusion Boundary Detection from an Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object occlusion boundary detection is a fundamental and crucial research\nproblem in computer vision. This is challenging to solve as encountering the\nextreme boundary/non-boundary class imbalance during training an object\nocclusion boundary detector. In this paper, we propose to address this class\nimbalance by up-weighting the loss contribution of false negative and false\npositive examples with our novel Attention Loss function. We also propose a\nunified end-to-end multi-task deep object occlusion boundary detection network\n(DOOBNet) by sharing convolutional features to simultaneously predict object\nboundary and occlusion orientation. DOOBNet adopts an encoder-decoder structure\nwith skip connection in order to automatically learn multi-scale and\nmulti-level features. We significantly surpass the state-of-the-art on the PIOD\ndataset (ODS F-score of .702) and the BSDS ownership dataset (ODS F-score of\n.555), as well as improving the detecting speed to as 0.037s per image on the\nPIOD dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 02:24:31 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 13:36:14 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 14:18:34 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Wang", "Guoxia", ""], ["Liang", "Xiaohui", ""], ["Li", "Frederick W. B.", ""]]}, {"id": "1806.03796", "submitter": "Yash Upadhyay", "authors": "Yash Upadhyay, Paul Schrater", "title": "Generative Adversarial Network Architectures For Image Synthesis Using\n  Capsule Networks", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Generative Adversarial Network (GAN) architectures\nthat use Capsule Networks for image-synthesis. Based on the principal of\npositional-equivariance of features, Capsule Network's ability to encode\nspatial relationships between the features of the image helps it become a more\npowerful critic in comparison to Convolutional Neural Networks (CNNs) used in\ncurrent architectures for image synthesis. Our proposed GAN architectures learn\nthe data manifold much faster and therefore, synthesize visually accurate\nimages in significantly lesser number of training samples and training epochs\nin comparison to GANs and its variants that use CNNs. Apart from analyzing the\nquantitative results corresponding the images generated by different\narchitectures, we also explore the reasons for the lower coverage and diversity\nexplored by the GAN architectures that use CNN critics.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 03:54:24 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 07:55:20 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 15:45:28 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2018 18:33:11 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Upadhyay", "Yash", ""], ["Schrater", "Paul", ""]]}, {"id": "1806.03811", "submitter": "Shuming Jiao", "authors": "Shuming Jiao, Zhi Jin, Chenliang Chang, Changyuan Zhou, Wenbin Zou,\n  Xia Li", "title": "Compression of phase-only holograms with JPEG standard and deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a critical issue to reduce the enormous amount of data in the\nprocessing, storage and transmission of a hologram in digital format. In\nphotograph compression, the JPEG standard is commonly supported by almost every\nsystem and device. It will be favorable if JPEG standard is applicable to\nhologram compression, with advantages of universal compatibility. However, the\nreconstructed image from a JPEG compressed hologram suffers from severe quality\ndegradation since some high frequency features in the hologram will be lost\nduring the compression process. In this work, we employ a deep convolutional\nneural network to reduce the artifacts in a JPEG compressed hologram.\nSimulation and experimental results reveal that our proposed \"JPEG + deep\nlearning\" hologram compression scheme can achieve satisfactory reconstruction\nresults for a computer-generated phase-only hologram after compression.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 05:11:58 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Jiao", "Shuming", ""], ["Jin", "Zhi", ""], ["Chang", "Chenliang", ""], ["Zhou", "Changyuan", ""], ["Zou", "Wenbin", ""], ["Li", "Xia", ""]]}, {"id": "1806.03831", "submitter": "Mohit Shridhar", "authors": "Mohit Shridhar, David Hsu", "title": "Interactive Visual Grounding of Referring Expressions for Human-Robot\n  Interaction", "comments": "In Robotics: Science & Systems (RSS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents INGRESS, a robot system that follows human natural\nlanguage instructions to pick and place everyday objects. The core issue here\nis the grounding of referring expressions: infer objects and their\nrelationships from input images and language expressions. INGRESS allows for\nunconstrained object categories and unconstrained language expressions.\nFurther, it asks questions to disambiguate referring expressions interactively.\nTo achieve these, we take the approach of grounding by generation and propose a\ntwo-stage neural network model for grounding. The first stage uses a neural\nnetwork to generate visual descriptions of objects, compares them with the\ninput language expression, and identifies a set of candidate objects. The\nsecond stage uses another neural network to examine all pairwise relations\nbetween the candidates and infers the most likely referred object. The same\nneural networks are used for both grounding and question generation for\ndisambiguation. Experiments show that INGRESS outperformed a state-of-the-art\nmethod on the RefCOCO dataset and in robot experiments with humans.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 06:58:19 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Shridhar", "Mohit", ""], ["Hsu", "David", ""]]}, {"id": "1806.03848", "submitter": "Richard McKinley", "authors": "Andreas Hess, Raphael Meier, Johannes Kaesmacher, Simon Jung, Fabien\n  Scalzo, David Liebeskind, Roland Wiest, and Richard McKinley", "title": "Synthetic Perfusion Maps: Imaging Perfusion Deficits in DSC-MRI with\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel convolutional neural net- work based method\nfor perfusion map generation in dynamic suscepti- bility contrast-enhanced\nperfusion imaging. The proposed architecture is trained end-to-end and solely\nrelies on raw perfusion data for inference. We used a dataset of 151 acute\nischemic stroke cases for evaluation. Our method generates perfusion maps that\nare comparable to the target maps used for clinical routine, while being\nmodel-free, fast, and less noisy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 07:52:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Hess", "Andreas", ""], ["Meier", "Raphael", ""], ["Kaesmacher", "Johannes", ""], ["Jung", "Simon", ""], ["Scalzo", "Fabien", ""], ["Liebeskind", "David", ""], ["Wiest", "Roland", ""], ["McKinley", "Richard", ""]]}, {"id": "1806.03852", "submitter": "Alex Hern\\'andez Garc\\'ia", "authors": "Alex Hern\\'andez-Garc\\'ia, Peter K\\\"onig", "title": "Data augmentation instead of explicit regularization", "comments": "Major changes: 1. updated figures; 2. statistical significance\n  analysis of results through bootstrap; 3. information about carbon emissions\n  produced with the experiments; 4. extended discussion about the need for\n  weight decay and dropout; 5. literature review updated; 6. slight update of\n  the definitions of explicit and implicit regularization, with several\n  examples for illustration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to most machine learning models, modern deep artificial neural\nnetworks typically include multiple components that contribute to\nregularization. Despite the fact that some (explicit) regularization\ntechniques, such as weight decay and dropout, require costly fine-tuning of\nsensitive hyperparameters, the interplay between them and other elements that\nprovide implicit regularization is not well understood yet. Shedding light upon\nthese interactions is key to efficiently using computational resources and may\ncontribute to solving the puzzle of generalization in deep learning. Here, we\nfirst provide formal definitions of explicit and implicit regularization that\nhelp understand essential differences between techniques. Second, we contrast\ndata augmentation with weight decay and dropout. Our results show that visual\nobject categorization models trained with data augmentation alone achieve the\nsame performance or higher than models trained also with weight decay and\ndropout, as is common practice. We conclude that the contribution on\ngeneralization of weight decay and dropout is not only superfluous when\nsufficient implicit regularization is provided, but also such techniques can\ndramatically deteriorate the performance if the hyperparameters are not\ncarefully tuned for the architecture and data set. In contrast, data\naugmentation systematically provides large generalization gains and does not\nrequire hyperparameter re-tuning. In view of our results, we suggest to\noptimize neural networks without weight decay and dropout to save computational\nresources, hence carbon emissions, and focus more on data augmentation and\nother inductive biases to improve performance and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 08:10:18 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 07:55:57 GMT"}, {"version": "v3", "created": "Sun, 9 Dec 2018 16:22:29 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 13:11:51 GMT"}, {"version": "v5", "created": "Thu, 12 Nov 2020 10:35:59 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Hern\u00e1ndez-Garc\u00eda", "Alex", ""], ["K\u00f6nig", "Peter", ""]]}, {"id": "1806.03853", "submitter": "Shangzhen Luan", "authors": "Shangzhen Luan, Yan Li, Xiaodi Wang, Baochang Zhang", "title": "Object detection and tracking benchmark in industry based on improved\n  correlation filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time object detection and tracking have shown to be the basis of\nintelligent production for industrial 4.0 applications. It is a challenging\ntask because of various distorted data in complex industrial setting. The\ncorrelation filter (CF) has been used to trade off the low-cost computation and\nhigh performance. However, traditional CF training strategy can not get\nsatisfied performance for the various industrial data; because the simple\nsampling(bagging) during training process will not find the exact solutions in\na data space with a large diversity. In this paper, we propose\nDijkstra-distance based correlation filters (DBCF), which establishes a new\nlearning framework that embeds distribution-related constraints into the\nmulti-channel correlation filters (MCCF). DBCF is able to handle the huge\nvariations existing in the industrial data by improving those constraints based\non the shortest path among all solutions. To evaluate DBCF, we build a new\ndataset as the benchmark for industrial 4.0 application. Extensive experiments\ndemonstrate that DBCF produces high performance and exceeds the\nstate-of-the-art methods. The dataset and source code can be found at\nhttps://github.com/bczhangbczhang\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 08:15:30 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 01:29:05 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Luan", "Shangzhen", ""], ["Li", "Yan", ""], ["Wang", "Xiaodi", ""], ["Zhang", "Baochang", ""]]}, {"id": "1806.03863", "submitter": "Joao Carreira", "authors": "Joao Carreira, Viorica Patraucean, Laurent Mazare, Andrew Zisserman,\n  Simon Osindero", "title": "Massively Parallel Video Networks", "comments": "Fixed typos in densenet model definition in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of causal video understanding models that aims to\nimprove efficiency of video processing by maximising throughput, minimising\nlatency, and reducing the number of clock cycles. Leveraging operation\npipelining and multi-rate clocks, these models perform a minimal amount of\ncomputation (e.g. as few as four convolutional layers) for each frame per\ntimestep to produce an output. The models are still very deep, with dozens of\nsuch operations being performed but in a pipelined fashion that enables\ndepth-parallel computation. We illustrate the proposed principles by applying\nthem to existing image architectures and analyse their behaviour on two video\ntasks: action recognition and human keypoint localisation. The results show\nthat a significant degree of parallelism, and implicitly speedup, can be\nachieved with little loss in performance.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 08:46:51 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 17:52:49 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Carreira", "Joao", ""], ["Patraucean", "Viorica", ""], ["Mazare", "Laurent", ""], ["Zisserman", "Andrew", ""], ["Osindero", "Simon", ""]]}, {"id": "1806.03891", "submitter": "Juil Sock", "authors": "Juil Sock, Kwang In Kim, Caner Sahin and Tae-Kyun Kim", "title": "Multi-Task Deep Networks for Depth-Based 6D Object Pose and Joint\n  Registration in Crowd Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In bin-picking scenarios, multiple instances of an object of interest are\nstacked in a pile randomly, and hence, the instances are inherently subjected\nto the challenges: severe occlusion, clutter, and similar-looking distractors.\nMost existing methods are, however, for single isolated object instances, while\nsome recent methods tackle crowd scenarios as post-refinement which accounts\nmultiple object relations. In this paper, we address recovering 6D poses of\nmultiple instances in bin-picking scenarios in depth modality by multi-task\nlearning in deep neural networks. Our architecture jointly learns multiple\nsub-tasks: 2D detection, depth, and 3D pose estimation of individual objects;\nand joint registration of multiple objects. For training data generation, depth\nimages of physically plausible object pose configurations are generated by a 3D\nobject model in a physics simulation, which yields diverse occlusion patterns\nto learn. We adopt a state-of-the-art object detector, and 2D offsets are\nfurther estimated via a network to refine misaligned 2D detections. The depth\nand 3D pose estimator is designed to generate multiple hypotheses per\ndetection. This allows the joint registration network to learn occlusion\npatterns and remove physically implausible pose hypotheses. We apply our\narchitecture on both synthetic (our own and Sileane dataset) and real (a public\nBin-Picking dataset) data, showing that it significantly outperforms\nstate-of-the-art methods by 15-31% in average precision.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 10:05:42 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Sock", "Juil", ""], ["Kim", "Kwang In", ""], ["Sahin", "Caner", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1806.03902", "submitter": "Haimin Zhang", "authors": "Haimin Zhang and Min Xu", "title": "Dual Pattern Learning Networks by Empirical Dual Prediction Risk\n  Minimization", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the observation that humans can learn patterns from two given\nimages at one time, we propose a dual pattern learning network architecture in\nthis paper. Unlike conventional networks, the proposed architecture has two\ninput branches and two loss functions. Instead of minimizing the empirical risk\nof a given dataset, dual pattern learning networks is trained by minimizing the\nempirical dual prediction loss. We show that this can improve the performance\nfor single image classification. This architecture forces the network to learn\ndiscriminative class-specific features by analyzing and comparing two input\nimages. In addition, the dual input structure allows the network to have a\nconsiderably large number of image pairs, which can help address the\noverfitting issue due to limited training data. Moreover, we propose to\nassociate each input branch with a random interest value for learning\ncorresponding image during training. This method can be seen as a stochastic\nregularization technique, and can further lead to generalization performance\nimprovement. State-of-the-art deep networks can be adapted to dual pattern\nlearning networks without increasing the same number of parameters. Extensive\nexperiments on CIFAR-10, CIFAR- 100, FI-8, Google commands dataset, and MNIST\ndemonstrate that our DPLNets exhibit better performance than original networks.\nThe experimental results on subsets of CIFAR- 10, CIFAR-100, and MNIST\ndemonstrate that dual pattern learning networks have good generalization\nperformance on small datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 11:00:58 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zhang", "Haimin", ""], ["Xu", "Min", ""]]}, {"id": "1806.03905", "submitter": "Vivek Kumar Singh", "authors": "Vivek Kumar Singh, Hatem Rashwan, Farhan Akram, Nidhi Pandey, Md.\n  Mostaf Kamal Sarker, Adel Saleh, Saddam Abdulwahab, Najlaa Maaroof, Santiago\n  Romani, Domenec Puig", "title": "Retinal Optic Disc Segmentation using Conditional Generative Adversarial\n  Network", "comments": "8 pages, Submitted to 21st International Conference of the Catalan\n  Association for Artificial Intelligence (CCIA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a retinal image segmentation method based on conditional\nGenerative Adversarial Network (cGAN) to segment optic disc. The proposed model\nconsists of two successive networks: generator and discriminator. The generator\nlearns to map information from the observing input (i.e., retinal fundus color\nimage), to the output (i.e., binary mask). Then, the discriminator learns as a\nloss function to train this mapping by comparing the ground-truth and the\npredicted output with observing the input image as a condition.Experiments were\nperformed on two publicly available dataset; DRISHTI GS1 and RIM-ONE. The\nproposed model outperformed state-of-the-art-methods by achieving around 0.96%\nand 0.98% of Jaccard and Dice coefficients, respectively. Moreover, an image\nsegmentation is performed in less than a second on recent GPU.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 11:04:05 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Singh", "Vivek Kumar", ""], ["Rashwan", "Hatem", ""], ["Akram", "Farhan", ""], ["Pandey", "Nidhi", ""], ["Sarker", "Md. Mostaf Kamal", ""], ["Saleh", "Adel", ""], ["Abdulwahab", "Saddam", ""], ["Maaroof", "Najlaa", ""], ["Romani", "Santiago", ""], ["Puig", "Domenec", ""]]}, {"id": "1806.03960", "submitter": "Ruohan ZHang", "authors": "Ruohan Zhang, Zhuode Liu, Luxin Zhang, Jake A. Whritner, Karl S.\n  Muller, Mary M. Hayhoe, Dana H. Ballard", "title": "AGIL: Learning Attention from Human for Visuomotor Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When intelligent agents learn visuomotor behaviors from human demonstrations,\nthey may benefit from knowing where the human is allocating visual attention,\nwhich can be inferred from their gaze. A wealth of information regarding\nintelligent decision making is conveyed by human gaze allocation; hence,\nexploiting such information has the potential to improve the agents'\nperformance. With this motivation, we propose the AGIL (Attention Guided\nImitation Learning) framework. We collect high-quality human action and gaze\ndata while playing Atari games in a carefully controlled experimental setting.\nUsing these data, we first train a deep neural network that can predict human\ngaze positions and visual attention with high accuracy (the gaze network) and\nthen train another network to predict human actions (the policy network).\nIncorporating the learned attention model from the gaze network into the policy\nnetwork significantly improves the action prediction accuracy and task\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 18:36:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zhang", "Ruohan", ""], ["Liu", "Zhuode", ""], ["Zhang", "Luxin", ""], ["Whritner", "Jake A.", ""], ["Muller", "Karl S.", ""], ["Hayhoe", "Mary M.", ""], ["Ballard", "Dana H.", ""]]}, {"id": "1806.03961", "submitter": "Liangbo He", "authors": "Liangbo He, Hao Sun", "title": "Attention Incorporate Network: A network can adapt various data size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional neural networks for image processing, the inputs of the neural\nnetworks should be the same size such as 224*224*3. But how can we train the\nneural net model with different input size? A common way to do is image\ndeformation which accompany a problem of information loss (e.g. image crop or\nwrap). Sequence model(RNN, LSTM, etc.) can accept different size of input like\ntext and audio. But one disadvantage for sequence model is that the previous\ninformation will become more fragmentary during the transfer in time step, it\nwill make the network hard to train especially for long sequential data. In\nthis paper we propose a new network structure called Attention Incorporate\nNetwork(AIN). It solve the problem of different size of inputs including:\nimages, text, audio, and extract the key features of the inputs by attention\nmechanism, pay different attention depends on the importance of the features\nnot rely on the data size. Experimentally, AIN achieve a higher accuracy,\nbetter convergence comparing to the same size of other network structure\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 11:09:35 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["He", "Liangbo", ""], ["Sun", "Hao", ""]]}, {"id": "1806.03962", "submitter": "Bastiaan Veeling", "authors": "Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen and Max\n  Welling", "title": "Rotation Equivariant CNNs for Digital Pathology", "comments": "To be presented at MICCAI 2018. Implementations of equivariant layers\n  available at https://github.com/basveeling/keras_gcnn . PCam details and data\n  at https://github.com/basveeling/pcam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for digital pathology segmentation, based on the\nobservation that histopathology images are inherently symmetric under rotation\nand reflection. Utilizing recent findings on rotation equivariant CNNs, the\nproposed model leverages these symmetries in a principled manner. We present a\nvisual analysis showing improved stability on predictions, and demonstrate that\nexploiting rotation equivariance significantly improves tumor detection\nperformance on a challenging lymph node metastases dataset. We further present\na novel derived dataset to enable principled comparison of machine learning\nmodels, in combination with an initial benchmark. Through this dataset, the\ntask of histopathology diagnosis becomes accessible as a challenging benchmark\nfor fundamental machine learning research.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:13:37 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Veeling", "Bastiaan S.", ""], ["Linmans", "Jasper", ""], ["Winkens", "Jim", ""], ["Cohen", "Taco", ""], ["Welling", "Max", ""]]}, {"id": "1806.03963", "submitter": "Morteza Mardani", "authors": "Morteza Mardani, Qingyun Sun, Shreyas Vasawanala, Vardan Papyan, Hatef\n  Monajemi, John Pauly, and David Donoho", "title": "Neural Proximal Gradient Descent for Compressive Imaging", "comments": "arXiv admin note: text overlap with arXiv:1711.10046", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering high-resolution images from limited sensory data typically leads\nto a serious ill-posed inverse problem, demanding inversion algorithms that\neffectively capture the prior information. Learning a good inverse mapping from\ntraining data faces severe challenges, including: (i) scarcity of training\ndata; (ii) need for plausible reconstructions that are physically feasible;\n(iii) need for fast reconstruction, especially in real-time applications. We\ndevelop a successful system solving all these challenges, using as basic\narchitecture the recurrent application of proximal gradient algorithm. We learn\na proximal map that works well with real images based on residual networks.\nContraction of the resulting map is analyzed, and incoherence conditions are\ninvestigated that drive the convergence of the iterates. Extensive experiments\nare carried out under different settings: (a) reconstructing abdominal MRI of\npediatric patients from highly undersampled Fourier-space data and (b)\nsuperresolving natural face images. Our key findings include: 1. a recurrent\nResNet with a single residual block unrolled from an iterative algorithm yields\nan effective proximal which accurately reveals MR image details. 2. Our\narchitecture significantly outperforms conventional non-recurrent deep ResNets\nby 2dB SNR; it is also trained much more rapidly. 3. It outperforms\nstate-of-the-art compressed-sensing Wavelet-based methods by 4dB SNR, with 100x\nspeedups in reconstruction time.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 21:48:39 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Mardani", "Morteza", ""], ["Sun", "Qingyun", ""], ["Vasawanala", "Shreyas", ""], ["Papyan", "Vardan", ""], ["Monajemi", "Hatef", ""], ["Pauly", "John", ""], ["Donoho", "David", ""]]}, {"id": "1806.03968", "submitter": "Raeid Saqur", "authors": "Raeid Saqur, Sal Vivona", "title": "CapsGAN: Using Dynamic Routing for Generative Adversarial Networks", "comments": "Draft Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel technique for generating images in the 3D\ndomain from images with high degree of geometrical transformations. By\ncoalescing two popular concurrent methods that have seen rapid ascension to the\nmachine learning zeitgeist in recent years: GANs (Goodfellow et. al.) and\nCapsule networks (Sabour, Hinton et. al.) - we present: \\textbf{CapsGAN}. We\nshow that CapsGAN performs better than or equal to traditional CNN based GANs\nin generating images with high geometric transformations using rotated MNIST.\nIn the process, we also show the efficacy of using capsules architecture in the\nGANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the\nperformance control and training stability by experimenting with using\nWasserstein distance (gradient clipping, penalty) and Spectral Normalization.\nThe experimental findings of this paper should propel the application of\ncapsules and GANs in the still exciting and nascent domain of 3D image\ngeneration, and plausibly video (frame) generation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 21:33:46 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Saqur", "Raeid", ""], ["Vivona", "Sal", ""]]}, {"id": "1806.03969", "submitter": "Nandakishore Puttashamachar", "authors": "Nandakishore Puttashamachar and Ulas Bagci", "title": "End to End Brain Fiber Orientation Estimation using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the various Brain Neuron tracking techniques, which\nis one of the most significant applications of Diffusion Tensor Imaging.\nTractography provides us with a non-invasive method to analyze underlying\ntissue micro-structure. Understanding the structure and organization of the\ntissues facilitates us with a diagnosis method to identify any aberrations and\nprovide acute information on the occurrences of brain ischemia or stroke, the\nmutation of neurological diseases such as Alzheimer, multiple sclerosis and so\non. Time if of essence and accurate localization of the aberrations can help\nsave or change a diseased life. Following up with the limitations introduced by\nthe current Tractography techniques such as computational complexity,\nreconstruction errors during tensor estimation and standardization, we aim to\nelucidate these limitations through our research findings. We introduce an end\nto end Deep Learning framework which can accurately estimate the most probable\nlikelihood orientation at each voxel along a neuronal pathway. We use\nProbabilistic Tractography as our baseline model to obtain the training data\nand which also serve as a Tractography Gold Standard for our evaluations.\nThrough experiments we show that our Deep Network can do a significant\nimprovement over current Tractography implementations by reducing the run-time\ncomplexity to a significant new level. Our architecture also allows for\nvariable sized input DWI signals eliminating the need to worry about memory\nissues as seen with the traditional techniques. The advantage of this\narchitecture is that it is perfectly desirable to be processed on a cloud setup\nand utilize the existing multi GPU frameworks to perform whole brain\nTractography in minutes rather than hours. We evaluate our network with Gold\nStandard and benchmark its performance across several parameters.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 18:07:25 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Puttashamachar", "Nandakishore", ""], ["Bagci", "Ulas", ""]]}, {"id": "1806.03972", "submitter": "Duong Nguyen", "authors": "Duong Nguyen, Rodolphe Vadaine, Guillaume Hajduch, Ren\\'e Garello and\n  Ronan Fablet", "title": "A Multi-task Deep Learning Architecture for Maritime Surveillance using\n  AIS Data Streams", "comments": "Accepted to IEEE DSAA 2018", "journal-ref": null, "doi": "10.1109/DSAA.2018.00044", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a world of global trading, maritime safety, security and efficiency are\ncrucial issues. We propose a multi-task deep learning framework for vessel\nmonitoring using Automatic Identification System (AIS) data streams. We combine\nrecurrent neural networks with latent variable modeling and an embedding of AIS\nmessages to a new representation space to jointly address key issues to be\ndealt with when considering AIS data streams: massive amount of streaming data,\nnoisy data and irregular timesampling. We demonstrate the relevance of the\nproposed deep learning framework on real AIS datasets for a three-task setting,\nnamely trajectory reconstruction, anomaly detection and vessel type\nidentification.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 19:21:09 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 08:25:33 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 21:12:20 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Nguyen", "Duong", ""], ["Vadaine", "Rodolphe", ""], ["Hajduch", "Guillaume", ""], ["Garello", "Ren\u00e9", ""], ["Fablet", "Ronan", ""]]}, {"id": "1806.03973", "submitter": "Astha Sharma", "authors": "Astha Sharma", "title": "State Classification with CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a plenty of research going on in field of object recognition, but\nobject state recognition has not been addressed as much. There are many\nimportant applications which can utilize object state recognition, such as, in\nrobotics, to decide for how to grab an object. A convolution neural network was\ndesigned to classify an image to one of its states. The approach used for\ntraining is transfer learning with Inception v3 module of GoogLeNet used as the\npre-trained model. The model was trained on images of 18 cooking objects and\ntested on another set of cooking objects. The model was able to classify those\nimages with 76% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 18:51:52 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 19:48:17 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Sharma", "Astha", ""]]}, {"id": "1806.03981", "submitter": "Jasdeep Singh", "authors": "William Bakst, Linus Meyer-Teruel, Jasdeep Singh", "title": "Rethinking Radiology: An Analysis of Different Approaches to BraTS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the deep learning architectures currently used for\npixel-wise segmentation of primary and secondary glioblastomas and low-grade\ngliomas. We implement various models such as the popular UNet architecture and\ncompare the performance of these implementations on the BRATS dataset. This\npaper will explore the different approaches and combinations, offering an in\ndepth discussion of how they perform and how we may improve upon them using\nmore recent advancements in deep learning architectures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 21:58:54 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Bakst", "William", ""], ["Meyer-Teruel", "Linus", ""], ["Singh", "Jasdeep", ""]]}, {"id": "1806.03982", "submitter": "YongKeun Park", "authors": "YoungJu Jo, Hyungjoo Cho, Sang Yun Lee, Gunho Choi, Geon Kim,\n  Hyun-seok Min, YongKeun Park", "title": "Quantitative Phase Imaging and Artificial Intelligence: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in quantitative phase imaging (QPI) and artificial\nintelligence (AI) have opened up the possibility of an exciting frontier. The\nfast and label-free nature of QPI enables the rapid generation of large-scale\nand uniform-quality imaging data in two, three, and four dimensions.\nSubsequently, the AI-assisted interrogation of QPI data using data-driven\nmachine learning techniques results in a variety of biomedical applications.\nAlso, machine learning enhances QPI itself. Herein, we review the synergy\nbetween QPI and machine learning with a particular focus on deep learning.\nFurther, we provide practical guidelines and perspectives for further\ndevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 23:52:20 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 08:37:40 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Jo", "YoungJu", ""], ["Cho", "Hyungjoo", ""], ["Lee", "Sang Yun", ""], ["Choi", "Gunho", ""], ["Kim", "Geon", ""], ["Min", "Hyun-seok", ""], ["Park", "YongKeun", ""]]}, {"id": "1806.03987", "submitter": "Majeed Kassis", "authors": "Majeed Kassis, Jumana Nassour, Jihad El-Sana", "title": "Writing Style Invariant Deep Learning Model for Historical Manuscripts\n  Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical manuscript alignment is a widely known problem in document\nanalysis. Finding the differences between manuscript editions is mostly done\nmanually. In this paper, we present a writer independent deep learning model\nwhich is trained on several writing styles, and able to achieve high detection\naccuracy when tested on writing styles not present in training data. We test\nour model using cross validation, each time we train the model on five\nmanuscripts, and test it on the other two manuscripts, never seen in the\ntraining data. We've applied cross validation on seven manuscripts, netting 21\ndifferent tests, achieving average accuracy of $\\%92.17$. We also present a new\nalignment algorithm based on dynamic sized sliding window, which is able to\nsuccessfully handle complex cases.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 11:13:39 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Kassis", "Majeed", ""], ["Nassour", "Jumana", ""], ["El-Sana", "Jihad", ""]]}, {"id": "1806.03992", "submitter": "Mathew Cherukara", "authors": "Mathew J. Cherukara, Youssef S.G. Nashed, Ross J. Harder", "title": "Real-time coherent diffraction inversion using deep generative networks", "comments": null, "journal-ref": "Scientific Reports (2018) 8:16520", "doi": "10.1038/s41598-018-34525-1", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase retrieval, or the process of recovering phase information in reciprocal\nspace to reconstruct images from measured intensity alone, is the underlying\nbasis to a variety of imaging applications including coherent diffraction\nimaging (CDI). Typical phase retrieval algorithms are iterative in nature, and\nhence, are time-consuming and computationally expensive, precluding real-time\nimaging. Furthermore, iterative phase retrieval algorithms struggle to converge\nto the correct solution especially in the presence of strong phase structures.\nIn this work, we demonstrate the training and testing of CDI NN, a pair of deep\ndeconvolutional networks trained to predict structure and phase in real space\nof a 2D object from its corresponding far-field diffraction intensities alone.\nOnce trained, CDI NN can invert a diffraction pattern to an image within a few\nmilliseconds of compute time on a standard desktop machine, opening the door to\nreal-time imaging.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:33:27 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Cherukara", "Mathew J.", ""], ["Nashed", "Youssef S. G.", ""], ["Harder", "Ross J.", ""]]}, {"id": "1806.03994", "submitter": "Henrique Weber Mr.", "authors": "Henrique Weber, Donald Pr\\'evost, and Jean-Fran\\c{c}ois Lalonde", "title": "Learning to Estimate Indoor Lighting from 3D Objects", "comments": "3DV 2018 - International Conference on 3D Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a step towards a more accurate prediction of the\nenvironment light given a single picture of a known object. To achieve this, we\ndeveloped a deep learning method that is able to encode the latent space of\nindoor lighting using few parameters and that is trained on a database of\nenvironment maps. This latent space is then used to generate predictions of the\nlight that are both more realistic and accurate than previous methods. To\nachieve this, our first contribution is a deep autoencoder which is capable of\nlearning the feature space that compactly models lighting. Our second\ncontribution is a convolutional neural network that predicts the light from a\nsingle image of a known object. To train these networks, our third contribution\nis a novel dataset that contains 21,000 HDR indoor environment maps. The\nresults indicate that the predictor can generate plausible lighting estimations\neven from diffuse objects.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 14:01:31 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 17:06:19 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2018 15:02:31 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Weber", "Henrique", ""], ["Pr\u00e9vost", "Donald", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1806.03997", "submitter": "Ayushi Sinha", "authors": "Ayushi Sinha, Xingtong Liu, Austin Reiter, Masaru Ishii, Gregory D.\n  Hager and Russell H. Taylor", "title": "Endoscopic navigation in the absence of CT imaging", "comments": "8 pages, 3 figures, MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00937-3_8", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical examinations that involve endoscopic exploration of the nasal cavity\nand sinuses often do not have a reference image to provide structural context\nto the clinician. In this paper, we present a system for navigation during\nclinical endoscopic exploration in the absence of computed tomography (CT)\nscans by making use of shape statistics from past CT scans. Using a deformable\nregistration algorithm along with dense reconstructions from video, we show\nthat we are able to achieve submillimeter registrations in in-vivo clinical\ndata and are able to assign confidence to these registrations using confidence\ncriteria established using simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 03:21:13 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Sinha", "Ayushi", ""], ["Liu", "Xingtong", ""], ["Reiter", "Austin", ""], ["Ishii", "Masaru", ""], ["Hager", "Gregory D.", ""], ["Taylor", "Russell H.", ""]]}, {"id": "1806.04009", "submitter": "Daniel O\\~noro-Rubio", "authors": "Daniel O\\~noro-Rubio and Mathias Niepert", "title": "Contextual Hourglass Networks for Segmentation and Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hourglass networks such as the U-Net and V-Net are popular neural\narchitectures for medical image segmentation and counting problems. Typical\ninstances of hourglass networks contain shortcut connections between mirroring\nlayers. These shortcut connections improve the performance and it is\nhypothesized that this is due to mitigating effects on the vanishing gradient\nproblem and the ability of the model to combine feature maps from earlier and\nlater layers. We propose a method for not only combining feature maps of\nmirroring layers but also feature maps of layers with different spatial\ndimensions. For instance, the method enables the integration of the bottleneck\nfeature map with those of the reconstruction layers. The proposed approach is\napplicable to any hourglass architecture. We evaluated the contextual hourglass\nnetworks on image segmentation and object counting problems in the medical\ndomain. We achieve competitive results outperforming popular hourglass networks\nby up to 17 percentage points.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 11:55:56 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["O\u00f1oro-Rubio", "Daniel", ""], ["Niepert", "Mathias", ""]]}, {"id": "1806.04010", "submitter": "Max Frei", "authors": "Max Frei and Frank Einar Kruis", "title": "Fully automated primary particle size analysis of agglomerates on\n  transmission electron microscopy images via artificial neural networks", "comments": null, "journal-ref": "Powder Technology, Volume 332, 2018, Pages 120-130", "doi": "10.1016/j.powtec.2018.03.032", "report-no": null, "categories": "cs.CV cond-mat.mtrl-sci physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a high demand for fully automated methods for the analysis of\nprimary particle size distributions of agglomerates on transmission electron\nmicroscopy images. Therefore, a novel method, based on the utilization of\nartificial neural networks, was proposed, implemented and validated. The\ntraining of the artificial neural networks requires large quantities (up to\nseveral hundreds of thousands) of transmission electron microscopy images of\nagglomerates consisting of primary particles with known sizes. Since the manual\nevaluation of such large amounts of transmission electron microscopy images is\nnot feasible, a synthesis of lifelike transmission electron microscopy images\nas training data was implemented. The proposed method can compete with\nstate-of-the-art automated imaging particle size methods like the Hough\ntransformation, ultimate erosion and watershed transformation and is in some\ncases even able to outperform these methods. It is however still outperformed\nby the manual analysis.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 13:11:09 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Frei", "Max", ""], ["Kruis", "Frank Einar", ""]]}, {"id": "1806.04012", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mahdyar Ravanbakhsh, Mohamad Baydoun, Damian Campo, Pablo Marin, David\n  Martin, Lucio Marcenaro, Carlo S. Regazzoni", "title": "Hierarchy of GANs for learning embodied self-awareness model", "comments": "2018 IEEE International Conference on Image Processing - ICIP'18.\n  arXiv admin note: text overlap with arXiv:1806.02609", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years several architectures have been proposed to learn embodied\nagents complex self-awareness models. In this paper, dynamic incremental\nself-awareness (SA) models are proposed that allow experiences done by an agent\nto be modeled in a hierarchical fashion, starting from more simple situations\nto more structured ones. Each situation is learned from subsets of private\nagent perception data as a model capable to predict normal behaviors and detect\nabnormalities. Hierarchical SA models have been already proposed using low\ndimensional sensorial inputs. In this work, a hierarchical model is introduced\nby means of a cross-modal Generative Adversarial Networks (GANs) processing\nhigh dimensional visual data. Different levels of the GANs are detected in a\nself-supervised manner using GANs discriminators decision boundaries. Real\nexperiments on semi-autonomous ground vehicles are presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 13:24:57 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""], ["Baydoun", "Mohamad", ""], ["Campo", "Damian", ""], ["Marin", "Pablo", ""], ["Martin", "David", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo S.", ""]]}, {"id": "1806.04017", "submitter": "Aboul Ella Hassanien Abo", "authors": "Aya Salama Abdelhady, Aboul Ella Hassanenin, and Aly Fahmy", "title": "Sheep identity recognition, age and weight estimation datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased interest of scientists, producers and consumers in sheep\nidentification has been stimulated by the dramatic increase in population and\nthe urge to increase productivity. The world population is expected to exceed\n9.6 million in 2050. For this reason, awareness is raised towards the necessity\nof effective livestock production. Sheep is considered as one of the main of\nfood resources. Most of the research now is directed towards developing real\ntime applications that facilitate sheep identification for breed management and\ngathering related information like weight and age. Weight and age are key\nmatrices in assessing the effectiveness of production. For this reason, visual\nanalysis proved recently its significant success over other approaches. Visual\nanalysis techniques need enough images for testing and study completion. For\nthis reason, collecting sheep images database is a vital step to fulfill such\nobjective. We provide here datasets for testing and comparing such algorithms\nwhich are under development. Our collected dataset consists of 416 color images\nfor different features of sheep in different postures. Images were collected\nfifty two sheep at a range of year from three months to six years. For each\nsheep, two images were captured for both sides of the body, two images for both\nsides of the face, one image from the top view, one image for the hip and one\nimage for the teeth. The collected images cover different illumination, quality\nlevels and angle of rotation. The allocated data set can be used to test sheep\nidentification, weigh estimation, and age detection algorithms. Such algorithms\nare crucial for disease management, animal assessment and ownership.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 16:31:19 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Abdelhady", "Aya Salama", ""], ["Hassanenin", "Aboul Ella", ""], ["Fahmy", "Aly", ""]]}, {"id": "1806.04051", "submitter": "Dakai Jin", "authors": "Dakai Jin and Ziyue Xu and Youbao Tang and Adam P. Harrison and Daniel\n  J. Mollura", "title": "CT-Realistic Lung Nodule Simulation from 3D Conditional Generative\n  Adversarial Networks for Robust Lung Segmentation", "comments": "MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data availability plays a critical role for the performance of deep learning\nsystems. This challenge is especially acute within the medical image domain,\nparticularly when pathologies are involved, due to two factors: 1) limited\nnumber of cases, and 2) large variations in location, scale, and appearance. In\nthis work, we investigate whether augmenting a dataset with artificially\ngenerated lung nodules can improve the robustness of the progressive\nholistically nested network (P-HNN) model for pathological lung segmentation of\nCT scans. To achieve this goal, we develop a 3D generative adversarial network\n(GAN) that effectively learns lung nodule property distributions in 3D space.\nIn order to embed the nodules within their background context, we condition the\nGAN based on a volume of interest whose central part containing the nodule has\nbeen erased. To further improve realism and blending with the background, we\npropose a novel multi-mask reconstruction loss. We train our method on over\n1000 nodules from the LIDC dataset. Qualitative results demonstrate the\neffectiveness of our method compared to the state-of-art. We then use our GAN\nto generate simulated training images where nodules lie on the lung border,\nwhich are cases where the published P-HNN model struggles. Qualitative and\nquantitative results demonstrate that armed with these simulated images, the\nP-HNN model learns to better segment lung regions under these challenging\nsituations. As a result, our system provides a promising means to help overcome\nthe data paucity that commonly afflicts medical imaging.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 15:19:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Jin", "Dakai", ""], ["Xu", "Ziyue", ""], ["Tang", "Youbao", ""], ["Harrison", "Adam P.", ""], ["Mollura", "Daniel J.", ""]]}, {"id": "1806.04066", "submitter": "Chen Qin", "authors": "Chen Qin, Wenjia Bai, Jo Schlemper, Steffen E. Petersen, Stefan K.\n  Piechnik, Stefan Neubauer, and Daniel Rueckert", "title": "Joint Learning of Motion Estimation and Segmentation for Cardiac MR\n  Image Sequences", "comments": "accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac motion estimation and segmentation play important roles in\nquantitatively assessing cardiac function and diagnosing cardiovascular\ndiseases. In this paper, we propose a novel deep learning method for joint\nestimation of motion and segmentation from cardiac MR image sequences. The\nproposed network consists of two branches: a cardiac motion estimation branch\nwhich is built on a novel unsupervised Siamese style recurrent spatial\ntransformer network, and a cardiac segmentation branch that is based on a fully\nconvolutional network. In particular, a joint multi-scale feature encoder is\nlearned by optimizing the segmentation branch and the motion estimation branch\nsimultaneously. This enables the weakly-supervised segmentation by taking\nadvantage of features that are unsupervisedly learned in the motion estimation\nbranch from a large amount of unannotated data. Experimental results using\ncardiac MRI images from 220 subjects show that the joint learning of both tasks\nis complementary and the proposed models outperform the competing methods\nsignificantly in terms of accuracy and speed.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 15:45:47 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Qin", "Chen", ""], ["Bai", "Wenjia", ""], ["Schlemper", "Jo", ""], ["Petersen", "Steffen E.", ""], ["Piechnik", "Stefan K.", ""], ["Neubauer", "Stefan", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1806.04070", "submitter": "Jianmin Liu", "authors": "Liu Jian-min", "title": "The Research of the Real-time Detection and Recognition of Targets in\n  Streetscape Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study proposes a method for the real-time detection and recognition of\ntargets in streetscape videos. The proposed method is based on separation\nconfidence computation and scale synthesis optimization. We use the proposed\nmethod to detect and recognize targets in streetscape videos with high frame\nrates and high definition. Furthermore, we experimentally demonstrate that the\naccuracy and robustness of our proposed method are superior to those of\nconventional methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 15:50:41 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Jian-min", "Liu", ""]]}, {"id": "1806.04074", "submitter": "V\\'ictor Ponce-L\\'opez", "authors": "V\\'ictor Ponce-L\\'opez, Tilo Burghardt, Sion Hannunna, Dima Damen,\n  Alessandro Masullo, Majid Mirmehdi", "title": "Semantically Selective Augmentation for Deep Compact Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep person re-identification approach that combines\nsemantically selective, deep data augmentation with clustering-based network\ncompression to generate high performance, light and fast inference networks. In\nparticular, we propose to augment limited training data via sampling from a\ndeep convolutional generative adversarial network (DCGAN), whose discriminator\nis constrained by a semantic classifier to explicitly control the domain\nspecificity of the generation process. Thereby, we encode information in the\nclassifier network which can be utilized to steer adversarial synthesis, and\nwhich fuels our CondenseNet ID-network training. We provide a quantitative and\nqualitative analysis of the approach and its variants on a number of datasets,\nobtaining results that outperform the state-of-the-art on the LIMA dataset for\nlong-term monitoring in indoor living spaces.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 15:58:10 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 15:26:15 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 14:27:43 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Ponce-L\u00f3pez", "V\u00edctor", ""], ["Burghardt", "Tilo", ""], ["Hannunna", "Sion", ""], ["Damen", "Dima", ""], ["Masullo", "Alessandro", ""], ["Mirmehdi", "Majid", ""]]}, {"id": "1806.04166", "submitter": "Jun-Ting Hsieh", "authors": "Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li Fei-Fei, Juan Carlos\n  Niebles", "title": "Learning to Decompose and Disentangle Representations for Video\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to predict future video frames given a sequence of input frames.\nDespite large amounts of video data, this remains a challenging task because of\nthe high-dimensionality of video frames. We address this challenge by proposing\nthe Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework\nthat combines structured probabilistic models and deep networks to\nautomatically (i) decompose the high-dimensional video that we aim to predict\ninto components, and (ii) disentangle each component to have low-dimensional\ntemporal dynamics that are easier to predict. Crucially, with an appropriately\nspecified generative model of video frames, our DDPAE is able to learn both the\nlatent decomposition and disentanglement without explicit supervision. For the\nMoving MNIST dataset, we show that DDPAE is able to recover the underlying\ncomponents (individual digits) and disentanglement (appearance and location) as\nwe would intuitively do. We further demonstrate that DDPAE can be applied to\nthe Bouncing Balls dataset involving complex interactions between multiple\nobjects to predict the video frame directly from the pixels and recover\nphysical states without explicit supervision.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 18:12:59 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 18:44:19 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Hsieh", "Jun-Ting", ""], ["Liu", "Bingbin", ""], ["Huang", "De-An", ""], ["Fei-Fei", "Li", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1806.04171", "submitter": "Neal Wadhwa", "authors": "Neal Wadhwa, Rahul Garg, David E. Jacobs, Bryan E. Feldman, Nori\n  Kanazawa, Robert Carroll, Yair Movshovitz-Attias, Jonathan T. Barron, Yael\n  Pritch, and Marc Levoy", "title": "Synthetic Depth-of-Field with a Single-Camera Mobile Phone", "comments": "Accepted to SIGGRAPH 2018. Basis for Portrait Mode on Google Pixel 2\n  and Pixel 2 XL", "journal-ref": null, "doi": "10.1145/3197517.3201329", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shallow depth-of-field is commonly used by photographers to isolate a subject\nfrom a distracting background. However, standard cell phone cameras cannot\nproduce such images optically, as their short focal lengths and small apertures\ncapture nearly all-in-focus images. We present a system to computationally\nsynthesize shallow depth-of-field images with a single mobile camera and a\nsingle button press. If the image is of a person, we use a person segmentation\nnetwork to separate the person and their accessories from the background. If\navailable, we also use dense dual-pixel auto-focus hardware, effectively a\n2-sample light field with an approximately 1 millimeter baseline, to compute a\ndense depth map. These two signals are combined and used to render a defocused\nimage. Our system can process a 5.4 megapixel image in 4 seconds on a mobile\nphone, is fully automatic, and is robust enough to be used by non-experts. The\nmodular nature of our system allows it to degrade naturally in the absence of a\ndual-pixel sensor or a human subject.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 18:29:12 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Wadhwa", "Neal", ""], ["Garg", "Rahul", ""], ["Jacobs", "David E.", ""], ["Feldman", "Bryan E.", ""], ["Kanazawa", "Nori", ""], ["Carroll", "Robert", ""], ["Movshovitz-Attias", "Yair", ""], ["Barron", "Jonathan T.", ""], ["Pritch", "Yael", ""], ["Levoy", "Marc", ""]]}, {"id": "1806.04209", "submitter": "Meenakshi Khosla", "authors": "Meenakshi Khosla, Keith Jamison, Amy Kuceyeski and Mert Sabuncu", "title": "3D Convolutional Neural Networks for Classification of Functional\n  Connectomes", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resting-state functional MRI (rs-fMRI) scans hold the potential to serve as a\ndiagnostic or prognostic tool for a wide variety of conditions, such as autism,\nAlzheimer's disease, and stroke. While a growing number of studies have\ndemonstrated the promise of machine learning algorithms for rs-fMRI based\nclinical or behavioral prediction, most prior models have been limited in their\ncapacity to exploit the richness of the data. For example, classification\ntechniques applied to rs-fMRI often rely on region-based summary statistics\nand/or linear models. In this work, we propose a novel volumetric Convolutional\nNeural Network (CNN) framework that takes advantage of the full-resolution 3D\nspatial structure of rs-fMRI data and fits non-linear predictive models. We\nshowcase our approach on a challenging large-scale dataset (ABIDE, with N >\n2,000) and report state-of-the-art accuracy results on rs-fMRI-based\ndiscrimination of autism patients and healthy controls.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:30:20 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 17:15:27 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Khosla", "Meenakshi", ""], ["Jamison", "Keith", ""], ["Kuceyeski", "Amy", ""], ["Sabuncu", "Mert", ""]]}, {"id": "1806.04224", "submitter": "Martin Rajchl PhD", "authors": "Martin Rajchl and Nick Pawlowski and Daniel Rueckert and Paul M.\n  Matthews and Ben Glocker", "title": "NeuroNet: Fast and Robust Reproduction of Multiple Brain Image\n  Segmentation Pipelines", "comments": "International conference on Medical Imaging with Deep Learning (MIDL)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NeuroNet is a deep convolutional neural network mimicking multiple popular\nand state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM.\nThe network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank\nImaging Study that have been automatically segmented into brain tissue and\ncortical and sub-cortical structures using the standard neuroimaging pipelines.\nTraining a single model from these complementary and partially overlapping\nlabel maps yields a new powerful \"all-in-one\", multi-output segmentation tool.\nThe processing time for a single subject is reduced by an order of magnitude\ncompared to running each individual software package. We demonstrate very good\nreproducibility of the original outputs while increasing robustness to\nvariations in the input data. We believe NeuroNet could be an important tool in\nlarge-scale population imaging studies and serve as a new standard in\nneuroscience by reducing the risk of introducing bias when choosing a specific\nsoftware package.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 20:21:23 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Rajchl", "Martin", ""], ["Pawlowski", "Nick", ""], ["Rueckert", "Daniel", ""], ["Matthews", "Paul M.", ""], ["Glocker", "Ben", ""]]}, {"id": "1806.04226", "submitter": "Michael Anderson", "authors": "Michael R. Anderson, Michael Cafarella, German Ros, Thomas F. Wenisch", "title": "Physical Representation-based Predicate Optimization for a Visual\n  Analytics Database", "comments": "Camera-ready version of the paper submitted to ICDE 2019, In\n  Proceedings of the 35th IEEE International Conference on Data Engineering\n  (ICDE 2019)", "journal-ref": "Proceedings of the 35th IEEE International Conference on Data\n  Engineering (ICDE 2019), 1466-1477", "doi": "10.1109/ICDE.2019.00132", "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying the content of images, video, and other non-textual data sources\nrequires expensive content extraction methods. Modern extraction techniques are\nbased on deep convolutional neural networks (CNNs) and can classify objects\nwithin images with astounding accuracy. Unfortunately, these methods are slow:\nprocessing a single image can take about 10 milliseconds on modern GPU-based\nhardware. As massive video libraries become ubiquitous, running a content-based\nquery over millions of video frames is prohibitive.\n  One promising approach to reduce the runtime cost of queries of visual\ncontent is to use a hierarchical model, such as a cascade, where simple cases\nare handled by an inexpensive classifier. Prior work has sought to design\ncascades that optimize the computational cost of inference by, for example,\nusing smaller CNNs. However, we observe that there are critical factors besides\nthe inference time that dramatically impact the overall query time. Notably, by\ntreating the physical representation of the input image as part of our query\noptimization---that is, by including image transforms, such as resolution\nscaling or color-depth reduction, within the cascade---we can optimize data\nhandling costs and enable drastically more efficient classifier cascades.\n  In this paper, we propose Tahoma, which generates and evaluates many\npotential classifier cascades that jointly optimize the CNN architecture and\ninput data representation. Our experiments on a subset of ImageNet show that\nTahoma's input transformations speed up cascades by up to 35 times. We also\nfind up to a 98x speedup over the ResNet50 classifier with no loss in accuracy,\nand a 280x speedup if some accuracy is sacrificed.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 20:28:12 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 21:36:12 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 18:08:09 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Anderson", "Michael R.", ""], ["Cafarella", "Michael", ""], ["Ros", "German", ""], ["Wenisch", "Thomas F.", ""]]}, {"id": "1806.04259", "submitter": "Korsuk Sirinukunwattana", "authors": "Korsuk Sirinukunwattana, Nasullah Khalid Alham, Clare Verrill, Jens\n  Rittscher", "title": "Improving Whole Slide Segmentation Through Visual Context - A Systematic\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While challenging, the dense segmentation of histology images is a necessary\nfirst step to assess changes in tissue architecture and cellular morphology.\nAlthough specific convolutional neural network architectures have been applied\nwith great success to the problem, few effectively incorporate visual context\ninformation from multiple scales. With this paper, we present a systematic\ncomparison of different architectures to assess how including multi-scale\ninformation affects segmentation performance. A publicly available breast\ncancer and a locally collected prostate cancer datasets are being utilised for\nthis study. The results support our hypothesis that visual context and scale\nplay a crucial role in histology image classification problems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 22:35:51 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Sirinukunwattana", "Korsuk", ""], ["Alham", "Nasullah Khalid", ""], ["Verrill", "Clare", ""], ["Rittscher", "Jens", ""]]}, {"id": "1806.04265", "submitter": "Wojciech Samek", "authors": "Clemens Seibold, Wojciech Samek, Anna Hilsmann, Peter Eisert", "title": "Accurate and Robust Neural Networks for Security Related Applications\n  Exampled by Face Morphing Attacks", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks tend to learn only what they need for a task. A\nmanipulation of the training data can counter this phenomenon. In this paper,\nwe study the effect of different alterations of the training data, which limit\nthe amount and position of information that is available for the decision\nmaking. We analyze the accuracy and robustness against semantic and black box\nattacks on the networks that were trained on different training data\nmodifications for the particular example of morphing attacks. A morphing attack\nis an attack on a biometric facial recognition system where the system is\nfooled to match two different individuals with the same synthetic face image.\nSuch a synthetic image can be created by aligning and blending images of the\ntwo individuals that should be matched with this image.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 23:24:11 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Seibold", "Clemens", ""], ["Samek", "Wojciech", ""], ["Hilsmann", "Anna", ""], ["Eisert", "Peter", ""]]}, {"id": "1806.04284", "submitter": "Chenhui Chu", "authors": "Chenhui Chu, Mayu Otani and Yuta Nakashima", "title": "iParaphrasing: Extracting Visually Grounded Paraphrases via an Image", "comments": "COLING 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A paraphrase is a restatement of the meaning of a text in other words.\nParaphrases have been studied to enhance the performance of many natural\nlanguage processing tasks. In this paper, we propose a novel task iParaphrasing\nto extract visually grounded paraphrases (VGPs), which are different phrasal\nexpressions describing the same visual concept in an image. These extracted\nVGPs have the potential to improve language and image multimodal tasks such as\nvisual question answering and image captioning. How to model the similarity\nbetween VGPs is the key of iParaphrasing. We apply various existing methods as\nwell as propose a novel neural network-based method with image attention, and\nreport the results of the first attempt toward iParaphrasing.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 00:58:59 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Chu", "Chenhui", ""], ["Otani", "Mayu", ""], ["Nakashima", "Yuta", ""]]}, {"id": "1806.04314", "submitter": "Yaming Wang", "authors": "Yaming Wang, Xiao Tan, Yi Yang, Xiao Liu, Errui Ding, Feng Zhou, Larry\n  S. Davis", "title": "3D Pose Estimation for Fine-Grained Object Categories", "comments": "4th International Workshop on Recovering 6D Object Pose (ECCVW 2018).\n  arXiv admin note: text overlap with arXiv:1810.09263", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing object pose estimation datasets are related to generic object types\nand there is so far no dataset for fine-grained object categories. In this\nwork, we introduce a new large dataset to benchmark pose estimation for\nfine-grained objects, thanks to the availability of both 2D and 3D fine-grained\ndata recently. Specifically, we augment two popular fine-grained recognition\ndatasets (StanfordCars and CompCars) by finding a fine-grained 3D CAD model for\neach sub-category and manually annotating each object in images with 3D pose.\nWe show that, with enough training data, a full perspective model with\ncontinuous parameters can be estimated using 2D appearance information alone.\nWe achieve this via a framework based on Faster/Mask R-CNN. This goes beyond\nprevious works on category-level pose estimation, which only estimate\ndiscrete/continuous viewpoint angles or recover rotation matrices often with\nthe help of key points. Furthermore, with fine-grained 3D models available, we\nincorporate a dense 3D representation named as location field into the\nCNN-based pose estimation framework to further improve the performance. The new\ndataset is available at www.umiacs.umd.edu/~wym/3dpose.html\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 03:44:54 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 03:22:11 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 06:27:34 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Wang", "Yaming", ""], ["Tan", "Xiao", ""], ["Yang", "Yi", ""], ["Liu", "Xiao", ""], ["Ding", "Errui", ""], ["Zhou", "Feng", ""], ["Davis", "Larry S.", ""]]}, {"id": "1806.04329", "submitter": "Jun Xu", "authors": "Jun Xu, Wangpeng An, Lei Zhang, David Zhang", "title": "Sparse, Collaborative, or Nonnegative Representation: Which Helps\n  Pattern Classification?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of sparse representation (SR) and collaborative representation (CR)\nfor pattern classification has been widely studied in tasks such as face\nrecognition and object categorization. Despite the success of SR/CR based\nclassifiers, it is still arguable whether it is the $\\ell_{1}$-norm sparsity or\nthe $\\ell_{2}$-norm collaborative property that brings the success of SR/CR\nbased classification. In this paper, we investigate the use of nonnegative\nrepresentation (NR) for pattern classification, which is largely ignored by\nprevious work. Our analyses reveal that NR can boost the representation power\nof homogeneous samples while limiting the representation power of heterogeneous\nsamples, making the representation sparse and discriminative simultaneously and\nthus providing a more effective solution to representation based classification\nthan SR/CR. Our experiments demonstrate that the proposed NR based classifier\n(NRC) outperforms previous representation based classifiers. With deep features\nas inputs, it also achieves state-of-the-art performance on various visual\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 04:38:40 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 06:16:48 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Xu", "Jun", ""], ["An", "Wangpeng", ""], ["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1806.04331", "submitter": "Xue Yang", "authors": "Xue Yang, Hao Sun, Kun Fu, Jirui Yang, Xian Sun, Menglong Yan, Zhi Guo", "title": "Automatic Ship Detection of Remote Sensing Images from Google Earth in\n  Complex Scenes Based on Multi-Scale Rotation Dense Feature Pyramid Networks", "comments": "14 pages, 11 figures", "journal-ref": "Remote Sens. 2018, 10, 132", "doi": "10.3390/rs10010132", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ship detection has been playing a significant role in the field of remote\nsensing for a long time but it is still full of challenges. The main\nlimitations of traditional ship detection methods usually lie in the complexity\nof application scenarios, the difficulty of intensive object detection and the\nredundancy of detection region. In order to solve such problems above, we\npropose a framework called Rotation Dense Feature Pyramid Networks (R-DFPN)\nwhich can effectively detect ship in different scenes including ocean and port.\nSpecifically, we put forward the Dense Feature Pyramid Network (DFPN), which is\naimed at solving the problem resulted from the narrow width of the ship.\nCompared with previous multi-scale detectors such as Feature Pyramid Network\n(FPN), DFPN builds the high-level semantic feature-maps for all scales by means\nof dense connections, through which enhances the feature propagation and\nencourages the feature reuse. Additionally, in the case of ship rotation and\ndense arrangement, we design a rotation anchor strategy to predict the minimum\ncircumscribed rectangle of the object so as to reduce the redundant detection\nregion and improve the recall. Furthermore, we also propose multi-scale ROI\nAlign for the purpose of maintaining the completeness of semantic and spatial\ninformation. Experiments based on remote sensing images from Google Earth for\nship detection show that our detection method based on R-DFPN representation\nhas a state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 04:51:36 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Yang", "Xue", ""], ["Sun", "Hao", ""], ["Fu", "Kun", ""], ["Yang", "Jirui", ""], ["Sun", "Xian", ""], ["Yan", "Menglong", ""], ["Guo", "Zhi", ""]]}, {"id": "1806.04360", "submitter": "Bo Zhao", "authors": "Bo Zhao, Xinwei Sun, Yanwei Fu, Yuan Yao, Yizhou Wang", "title": "MSplit LBI: Realizing Feature Selection and Dense Estimation\n  Simultaneously in Few-shot and Zero-shot Learning", "comments": "Accepted by the 35th International Conference on Machine Learning", "journal-ref": "International Conference on Machine Learning 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is one typical and general topic of learning a good embedding model to\nefficiently learn the representation coefficients between two spaces/subspaces.\nTo solve this task, $L_{1}$ regularization is widely used for the pursuit of\nfeature selection and avoiding overfitting, and yet the sparse estimation of\nfeatures in $L_{1}$ regularization may cause the underfitting of training data.\n$L_{2}$ regularization is also frequently used, but it is a biased estimator.\nIn this paper, we propose the idea that the features consist of three\northogonal parts, \\emph{namely} sparse strong signals, dense weak signals and\nrandom noise, in which both strong and weak signals contribute to the fitting\nof data. To facilitate such novel decomposition, \\emph{MSplit} LBI is for the\nfirst time proposed to realize feature selection and dense estimation\nsimultaneously. We provide theoretical and simulational verification that our\nmethod exceeds $L_{1}$ and $L_{2}$ regularization, and extensive experimental\nresults show that our method achieves state-of-the-art performance in the\nfew-shot and zero-shot learning.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 07:07:44 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhao", "Bo", ""], ["Sun", "Xinwei", ""], ["Fu", "Yanwei", ""], ["Yao", "Yuan", ""], ["Wang", "Yizhou", ""]]}, {"id": "1806.04368", "submitter": "Julia Rackerseder", "authors": "Julia Rackerseder, Maximilian Baust, R\\\"udiger G\\\"obl, Nassir Navab,\n  and Christoph Hennersperger", "title": "Initialize globally before acting locally: Enabling Landmark-free 3D US\n  to MRI Registration", "comments": "This is a pre-print of an article published in the Proceedings of the\n  21st International Conference on Medical Image Computing and Computer\n  Assisted Interventions (MICCAI), Granada, Spain, September 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration of partial-view 3D US volumes with MRI data is influenced by\ninitialization. The standard of practice is using extrinsic or intrinsic\nlandmarks, which can be very tedious to obtain. To overcome the limitations of\nregistration initialization, we present a novel approach that is based on\nEuclidean distance maps derived from easily obtainable coarse segmentations. We\nevaluate our approach quantitatively on the publicly available RESECT dataset\nand show that it is robust regarding overlap of target area and initial\nposition. Furthermore, our method provides initializations that are suitable\nfor state-of-the-art nonlinear, deformable image registration algorithm's\ncapture ranges.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 07:27:53 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Rackerseder", "Julia", ""], ["Baust", "Maximilian", ""], ["G\u00f6bl", "R\u00fcdiger", ""], ["Navab", "Nassir", ""], ["Hennersperger", "Christoph", ""]]}, {"id": "1806.04374", "submitter": "Michael McCann", "authors": "Michael T. McCann and Vincent Andrearczyk and Michael Unser and Adrien\n  Depeursinge", "title": "Fast Rotational Sparse Coding", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for rotational sparse coding along with an efficient\nimplementation using steerability. Sparse coding (also called dictionary\nlearning) is an important technique in image processing, useful in inverse\nproblems, compression, and analysis; however, the usual formulation fails to\ncapture an important aspect of the structure of images: images are formed from\nbuilding blocks, e.g., edges, lines, or points, that appear at different\nlocations, orientations, and scales. The sparse coding problem can be\nreformulated to explicitly account for these transforms, at the cost of\nincreased computation. In this work, we propose an algorithm for a rotational\nversion of sparse coding that is based on K-SVD with additional rotation\noperations. We then propose a method to accelerate these rotations by learning\nthe dictionary in a steerable basis. Our experiments on patch coding and\ntexture classification demonstrate that the proposed algorithm is fast enough\nfor practical use and compares favorably to standard sparse coding.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 07:49:42 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 19:00:46 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["McCann", "Michael T.", ""], ["Andrearczyk", "Vincent", ""], ["Unser", "Michael", ""], ["Depeursinge", "Adrien", ""]]}, {"id": "1806.04391", "submitter": "Kai Hu", "authors": "Xiaoteng Zhang, Yixin Bao, Feiyun Zhang, Kai Hu, Yicheng Wang, Liang\n  Zhu, Qinzhu He, Yining Lin, Jie Shao and Yao Peng", "title": "Qiniu Submission to ActivityNet Challenge 2018", "comments": "4 pages, 3 figures, CVPR workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce our submissions for the tasks of trimmed activity\nrecognition (Kinetics) and trimmed event recognition (Moments in Time) for\nActivitynet Challenge 2018. In the two tasks, non-local neural networks and\ntemporal segment networks are implemented as our base models. Multi-modal cues\nsuch as RGB image, optical flow and acoustic signal have also been used in our\nmethod. We also propose new non-local-based models for further improvement on\nthe recognition accuracy. The final submissions after ensembling the models\nachieve 83.5% top-1 accuracy and 96.8% top-5 accuracy on the Kinetics\nvalidation set, 35.81% top-1 accuracy and 62.59% top-5 accuracy on the MIT\nvalidation set.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 08:42:55 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Zhang", "Xiaoteng", ""], ["Bao", "Yixin", ""], ["Zhang", "Feiyun", ""], ["Hu", "Kai", ""], ["Wang", "Yicheng", ""], ["Zhu", "Liang", ""], ["He", "Qinzhu", ""], ["Lin", "Yining", ""], ["Shao", "Jie", ""], ["Peng", "Yao", ""]]}, {"id": "1806.04413", "submitter": "Adriano Pinto", "authors": "Adriano Pinto, Sergio Pereira, Raphael Meier, Victor Alves, Roland\n  Wiest, Carlos A. Silva, and Mauricio Reyes", "title": "Enhancing clinical MRI Perfusion maps with data-driven maps of\n  complementary nature for lesion outcome prediction", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00931-1_13", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stroke is the second most common cause of death in developed countries, where\nrapid clinical intervention can have a major impact on a patient's life. To\nperform the revascularization procedure, the decision making of physicians\nconsiders its risks and benefits based on multi-modal MRI and clinical\nexperience. Therefore, automatic prediction of the ischemic stroke lesion\noutcome has the potential to assist the physician towards a better stroke\nassessment and information about tissue outcome. Typically, automatic methods\nconsider the information of the standard kinetic models of diffusion and\nperfusion MRI (e.g. Tmax, TTP, MTT, rCBF, rCBV) to perform lesion outcome\nprediction. In this work, we propose a deep learning method to fuse this\ninformation with an automated data selection of the raw 4D PWI image\ninformation, followed by a data-driven deep-learning modeling of the underlying\nblood flow hemodynamics. We demonstrate the ability of the proposed approach to\nimprove prediction of tissue at risk before therapy, as compared to only using\nthe standard clinical perfusion maps, hence suggesting on the potential\nbenefits of the proposed data-driven raw perfusion data modelling approach.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 09:34:20 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Pinto", "Adriano", ""], ["Pereira", "Sergio", ""], ["Meier", "Raphael", ""], ["Alves", "Victor", ""], ["Wiest", "Roland", ""], ["Silva", "Carlos A.", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1806.04422", "submitter": "Kele Xu", "authors": "Dawei Feng, Kele Xu, Haibo Mi, Feifan Liao and Yan Zhou", "title": "Sample Dropout for Audio Scene Classification Using Multi-Scale Dense\n  Connected Convolutional Neural Network", "comments": "Accepted to 2018 Pacific Rim Knowledge Acquisition Workshop (PKAW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic scene classification is an intricate problem for a machine. As an\nemerging field of research, deep Convolutional Neural Networks (CNN) achieve\nconvincing results. In this paper, we explore the use of multi-scale Dense\nconnected convolutional neural network (DenseNet) for the classification task,\nwith the goal to improve the classification performance as multi-scale features\ncan be extracted from the time-frequency representation of the audio signal. On\nthe other hand, most of previous CNN-based audio scene classification\napproaches aim to improve the classification accuracy, by employing different\nregularization techniques, such as the dropout of hidden units and data\naugmentation, to reduce overfitting. It is widely known that outliers in the\ntraining set have a high negative influence on the trained model, and culling\nthe outliers may improve the classification performance, while it is often\nunder-explored in previous studies. In this paper, inspired by the silence\nremoval in the speech signal processing, a novel sample dropout approach is\nproposed, which aims to remove outliers in the training dataset. Using the\nDCASE 2017 audio scene classification datasets, the experimental results\ndemonstrates the proposed multi-scale DenseNet providing a superior performance\nthan the traditional single-scale DenseNet, while the sample dropout method can\nfurther improve the classification robustness of multi-scale DenseNet.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 09:59:11 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Feng", "Dawei", ""], ["Xu", "Kele", ""], ["Mi", "Haibo", ""], ["Liao", "Feifan", ""], ["Zhou", "Yan", ""]]}, {"id": "1806.04429", "submitter": "Pulkit Kumar", "authors": "Pulkit Kumar, Pravin Nagar, Chetan Arora and Anubha Gupta", "title": "U-SegNet: Fully Convolutional Neural Network based Automated Brain\n  tissue segmentation Tool", "comments": "Accepted in ICIP, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated brain tissue segmentation into white matter (WM), gray matter (GM),\nand cerebro-spinal fluid (CSF) from magnetic resonance images (MRI) is helpful\nin the diagnosis of neuro-disorders such as epilepsy, Alzheimer's, multiple\nsclerosis, etc. However, thin GM structures at the periphery of cortex and\nsmooth transitions on tissue boundaries such as between GM and WM, or WM and\nCSF pose difficulty in building a reliable segmentation tool. This paper\nproposes a Fully Convolutional Neural Network (FCN) tool, that is a hybrid of\ntwo widely used deep learning segmentation architectures SegNet and U-Net, for\nimproved brain tissue segmentation. We propose a skip connection inspired from\nU-Net, in the SegNet architetcure, to incorporate fine multiscale information\nfor better tissue boundary identification. We show that the proposed U-SegNet\narchitecture, improves segmentation performance, as measured by average dice\nratio, to 89.74% on the widely used IBSR dataset consisting of T-1 weighted MRI\nvolumes of 18 subjects.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 10:21:32 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Kumar", "Pulkit", ""], ["Nagar", "Pravin", ""], ["Arora", "Chetan", ""], ["Gupta", "Anubha", ""]]}, {"id": "1806.04498", "submitter": "Yasin Yaz{\\i}c{\\i}", "authors": "Yasin Yaz{\\i}c{\\i}, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap,\n  Georgios Piliouras, Vijay Chandrasekhar", "title": "The Unusual Effectiveness of Averaging in GAN Training", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine two different techniques for parameter averaging in GAN training.\nMoving Average (MA) computes the time-average of parameters, whereas\nExponential Moving Average (EMA) computes an exponentially discounted sum.\nWhilst MA is known to lead to convergence in bilinear settings, we provide the\n-- to our knowledge -- first theoretical arguments in support of EMA. We show\nthat EMA converges to limit cycles around the equilibrium with vanishing\namplitude as the discount parameter approaches one for simple bilinear games\nand also enhances the stability of general GAN training. We establish\nexperimentally that both techniques are strikingly effective in the\nnon-convex-concave GAN setting as well. Both improve inception and FID scores\non different architectures and for different GAN objectives. We provide\ncomprehensive experimental results across a range of datasets -- mixture of\nGaussians, CIFAR-10, STL-10, CelebA and ImageNet -- to demonstrate its\neffectiveness. We achieve state-of-the-art results on CIFAR-10 and produce\nclean CelebA face images.\\footnote{~The code is available at\n\\url{https://github.com/yasinyazici/EMA_GAN}}\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:27:23 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 12:17:11 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Yaz\u0131c\u0131", "Yasin", ""], ["Foo", "Chuan-Sheng", ""], ["Winkler", "Stefan", ""], ["Yap", "Kim-Hui", ""], ["Piliouras", "Georgios", ""], ["Chandrasekhar", "Vijay", ""]]}, {"id": "1806.04533", "submitter": "Xintong Wang", "authors": "Jianming Lv and Xintong Wang", "title": "Cross-dataset Person Re-Identification Using Similarity Preserved\n  Generative Adversarial Networks", "comments": "Accepted by KSEM 2018. arXiv admin note: text overlap with\n  arXiv:1803.07293", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) aims to match the image frames which contain\nthe same person in the surveillance videos. Most of the Re-ID algorithms\nconduct supervised training in some small labeled datasets, so directly\ndeploying these trained models to the real-world large camera networks may lead\nto a poor performance due to underfitting. The significant difference between\nthe source training dataset and the target testing dataset makes it challenging\nto incrementally optimize the model. To address this challenge, we propose a\nnovel solution by transforming the unlabeled images in the target domain to fit\nthe original classifier by using our proposed similarity preserved generative\nadversarial networks model, SimPGAN. Specifically, SimPGAN adopts the\ngenerative adversarial networks with the cycle consistency constraint to\ntransform the unlabeled images in the target domain to the style of the source\ndomain. Meanwhile, SimPGAN uses the similarity consistency loss, which is\nmeasured by a siamese deep convolutional neural network, to preserve the\nsimilarity of the transformed images of the same person. Comprehensive\nexperiments based on multiple real surveillance datasets are conducted, and the\nresults show that our algorithm is better than the state-of-the-art\ncross-dataset unsupervised person Re-ID algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 03:40:06 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 02:20:59 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Lv", "Jianming", ""], ["Wang", "Xintong", ""]]}, {"id": "1806.04548", "submitter": "Pingkun Yan", "authors": "Grant Haskins, Jochen Kruecker, Uwe Kruger, Sheng Xu, Peter A. Pinto,\n  Brad J. Wood, Pingkun Yan", "title": "Learning Deep Similarity Metric for 3D MR-TRUS Registration", "comments": "To appear on IJCARS", "journal-ref": null, "doi": "10.1007/s11548-018-1875-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The fusion of transrectal ultrasound (TRUS) and magnetic resonance\n(MR) images for guiding targeted prostate biopsy has significantly improved the\nbiopsy yield of aggressive cancers. A key component of MR-TRUS fusion is image\nregistration. However, it is very challenging to obtain a robust automatic\nMR-TRUS registration due to the large appearance difference between the two\nimaging modalities. The work presented in this paper aims to tackle this\nproblem by addressing two challenges: (i) the definition of a suitable\nsimilarity metric and (ii) the determination of a suitable optimization\nstrategy.\n  Methods: This work proposes the use of a deep convolutional neural network to\nlearn a similarity metric for MR-TRUS registration. We also use a composite\noptimization strategy that explores the solution space in order to search for a\nsuitable initialization for the second-order optimization of the learned\nmetric. Further, a multi-pass approach is used in order to smooth the metric\nfor optimization.\n  Results: The learned similarity metric outperforms the classical mutual\ninformation and also the state-of-the-art MIND feature based methods. The\nresults indicate that the overall registration framework has a large capture\nrange. The proposed deep similarity metric based approach obtained a mean TRE\nof 3.86mm (with an initial TRE of 16mm) for this challenging problem.\n  Conclusion: A similarity metric that is learned using a deep neural network\ncan be used to assess the quality of any given image registration and can be\nused in conjunction with the aforementioned optimization framework to perform\nautomatic registration that is robust to poor initialization.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:13:00 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 13:50:43 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Haskins", "Grant", ""], ["Kruecker", "Jochen", ""], ["Kruger", "Uwe", ""], ["Xu", "Sheng", ""], ["Pinto", "Peter A.", ""], ["Wood", "Brad J.", ""], ["Yan", "Pingkun", ""]]}, {"id": "1806.04552", "submitter": "Sreecharan Sankaranarayanan", "authors": "Sreecharan Sankaranarayanan, Raghuram Mandyam Annasamy, Katia Sycara,\n  Carolyn Penstein Ros\\'e", "title": "Combining Model-Free Q-Ensembles and Model-Based Approaches for Informed\n  Exploration", "comments": "Submitted to the Thirty-Second Annual Conference on Neural\n  Information Processing Systems (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-Ensembles are a model-free approach where input images are fed into\ndifferent Q-networks and exploration is driven by the assumption that\nuncertainty is proportional to the variance of the output Q-values obtained.\nThey have been shown to perform relatively well compared to other exploration\nstrategies. Further, model-based approaches, such as encoder-decoder models\nhave been used successfully for next frame prediction given previous frames.\nThis paper proposes to integrate the model-free Q-ensembles and model-based\napproaches with the hope of compounding the benefits of both and achieving\nsuperior exploration as a result. Results show that a model-based trajectory\nmemory approach when combined with Q-ensembles produces superior performance\nwhen compared to only using Q-ensembles.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:24:02 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Sankaranarayanan", "Sreecharan", ""], ["Annasamy", "Raghuram Mandyam", ""], ["Sycara", "Katia", ""], ["Ros\u00e9", "Carolyn Penstein", ""]]}, {"id": "1806.04561", "submitter": "Miguel Sim\\~oes", "authors": "Miguel Sim\\~oes, Jos\\'e Bioucas-Dias, Luis B. Almeida", "title": "An Extension of Averaged-Operator-Based Algorithms", "comments": "26th Eur. Signal Process. Conf. (EUSIPCO 2018), accepted. 5 pages, 1\n  figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the algorithms used to solve minimization problems with\nsparsity-inducing regularizers are generic in the sense that they do not take\ninto account the sparsity of the solution in any particular way. However,\nalgorithms known as semismooth Newton are able to take advantage of this\nsparsity to accelerate their convergence. We show how to extend these\nalgorithms in different directions, and study the convergence of the resulting\nalgorithms by showing that they are a particular case of an extension of the\nwell-known Krasnosel'ski\\u{\\i}--Mann scheme.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:35:40 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Sim\u00f5es", "Miguel", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Almeida", "Luis B.", ""]]}, {"id": "1806.04564", "submitter": "Jianning Li", "authors": "Jianning Li", "title": "Detection of Premature Ventricular Contractions Using Densely Connected\n  Deep Convolutional Neural Network with Spatial Pyramid Pooling Layer", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Premature ventricular contraction(PVC) is a type of premature ectopic beat\noriginating from the ventricles. Automatic method for accurate and robust\ndetection of PVC is highly clinically desired.Currently, most of these methods\nare developed and tested using the same database divided into training and\ntesting set and their generalization performance across databases has not been\nfully validated. In this paper, a method based on densely connected\nconvolutional neural network and spatial pyramid pooling is proposed for PVC\ndetection which can take arbitrarily-sized QRS complexes as input both in\ntraining and testing. With a much less complicated and more straightforward\narchitecture,the proposed network achieves comparable results to current\nstate-of-the-art deep learning based method with regard to accuracy,sensitivity\nand specificity by training and testing using the MIT-BIH arrhythmia database\nas benchmark.Besides the benchmark database,QRS complexes are extracted from\nfour more open databases namely the St-Petersburg Institute of Cardiological\nTechnics 12-lead Arrhythmia Database,The MIT-BIH Normal Sinus Rhythm\nDatabase,The MIT-BIH Long Term Database and European ST-T Database. The\nextracted QRS complexes are different in length and sampling rate among the\nfive databases.Cross-database training and testing is also experimented.The\nperformance of the network shows an improvement on the benchmark database\naccording to the result demonstrating the advantage of using multiple databases\nfor training over using only a single database.The network also achieves\nsatisfactory scores on the other four databases showing good generalization\ncapability.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:42:35 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 02:40:26 GMT"}, {"version": "v3", "created": "Sun, 24 Jun 2018 08:11:51 GMT"}, {"version": "v4", "created": "Tue, 26 Jun 2018 13:14:31 GMT"}, {"version": "v5", "created": "Wed, 27 Jun 2018 02:55:06 GMT"}, {"version": "v6", "created": "Fri, 23 Nov 2018 02:29:20 GMT"}, {"version": "v7", "created": "Thu, 10 Oct 2019 16:35:24 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Li", "Jianning", ""]]}, {"id": "1806.04576", "submitter": "Rozita Teymourzadeh", "authors": "Rozita Teymourzadeh, Amirrize Alpha, VH Mok", "title": "Smart Novel Computer-based Analytical Tool for Image Forgery\n  Authentication", "comments": "Circuit and Systems (CAS) Conference. Pp.120-125. ISBN:\n  978-1-4673-3117", "journal-ref": null, "doi": "10.1109/ICCircuitsAndSystems.2012.6408276", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an integration of image forgery detection with image\nfacial recognition using black propagation neural network (BPNN). We observed\nthat facial image recognition by itself will always give a matching output or\nclosest possible output image for every input image irrespective of the\nauthenticity or otherwise not of the testing input image. Based on this, we are\nproposing the combination of the blind but powerful automation image forgery\ndetection for entire input images for the BPNN recognition program. Hence, an\ninput image must first be authenticated before being fed into the recognition\nprogram. Thus, an image security identification and authentication requirement,\nany image that fails the authentication/verification stage are not to be used\nas an input/test image. In addition, the universal smart GUI tool is proposed\nand designed to perform image forgery detection with the high accuracy of 2%\nerror rate.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 14:07:58 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Teymourzadeh", "Rozita", ""], ["Alpha", "Amirrize", ""], ["Mok", "VH", ""]]}, {"id": "1806.04597", "submitter": "Guang Yang A", "authors": "Jun Chen, Guang Yang, Zhifan Gao, Hao Ni, Elsa Angelini, Raad\n  Mohiaddin, Tom Wong, Yanping Zhang, Xiuquan Du, Heye Zhang, Jennifer Keegan,\n  David Firmin", "title": "Multiview Two-Task Recursive Attention Model for Left Atrium and Atrial\n  Scars Segmentation", "comments": "8 pages, 4 figures, accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Late Gadolinium Enhanced Cardiac MRI (LGE-CMRI) for detecting atrial scars in\natrial fibrillation (AF) patients has recently emerged as a promising technique\nto stratify patients, guide ablation therapy and predict treatment success.\nVisualisation and quantification of scar tissues require a segmentation of both\nthe left atrium (LA) and the high intensity scar regions from LGE-CMRI images.\nThese two segmentation tasks are challenging due to the cancelling of healthy\ntissue signal, low signal-to-noise ratio and often limited image quality in\nthese patients. Most approaches require manual supervision and/or a second\nbright-blood MRI acquisition for anatomical segmentation. Segmenting both the\nLA anatomy and the scar tissues automatically from a single LGE-CMRI\nacquisition is highly in demand. In this study, we proposed a novel fully\nautomated multiview two-task (MVTT) recursive attention model working directly\non LGE-CMRI images that combines a sequential learning and a dilated residual\nlearning to segment the LA (including attached pulmonary veins) and delineate\nthe atrial scars simultaneously via an innovative attention model. Compared to\nother state-of-the-art methods, the proposed MVTT achieves compelling\nimprovement, enabling to generate a patient-specific anatomical and atrial scar\nassessment model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 15:16:32 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Chen", "Jun", ""], ["Yang", "Guang", ""], ["Gao", "Zhifan", ""], ["Ni", "Hao", ""], ["Angelini", "Elsa", ""], ["Mohiaddin", "Raad", ""], ["Wong", "Tom", ""], ["Zhang", "Yanping", ""], ["Du", "Xiuquan", ""], ["Zhang", "Heye", ""], ["Keegan", "Jennifer", ""], ["Firmin", "David", ""]]}, {"id": "1806.04599", "submitter": "Kumar Vijay Mishra", "authors": "Fabio Giovanneschi, Kumar Vijay Mishra, Maria Antonia Gonzalez-Huici,\n  Yonina C. Eldar and Joachim H. G. Ender", "title": "Dictionary Learning for Adaptive GPR Landmine Classification", "comments": "16 pages, 11 figures, 10 tables", "journal-ref": null, "doi": "10.1109/TGRS.2019.2931134", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground penetrating radar (GPR) target detection and classification is a\nchallenging task. Here, we consider online dictionary learning (DL) methods to\nobtain sparse representations (SR) of the GPR data to enhance feature\nextraction for target classification via support vector machines. Online\nmethods are preferred because traditional batch DL like K-SVD is not scalable\nto high-dimensional training sets and infeasible for real-time operation. We\nalso develop Drop-Off MINi-batch Online Dictionary Learning (DOMINODL) which\nexploits the fact that a lot of the training data may be correlated. The\nDOMINODL algorithm iteratively considers elements of the training set in small\nbatches and drops off samples which become less relevant. For the case of\nabandoned anti-personnel landmines classification, we compare the performance\nof K-SVD with three online algorithms: classical Online Dictionary Learning,\nits correlation-based variant, and DOMINODL. Our experiments with real data\nfrom L-band GPR show that online DL methods reduce learning time by 36-93% and\nincrease mine detection by 4-28% over K-SVD. Our DOMINODL is the fastest and\nretains similar classification performance as the other two online DL\napproaches. We use a Kolmogorov-Smirnoff test distance and the\nDvoretzky-Kiefer-Wolfowitz inequality for the selection of DL input parameters\nleading to enhanced classification results. To further compare with\nstate-of-the-art classification approaches, we evaluate a convolutional neural\nnetwork (CNN) classifier which performs worse than the proposed approach.\nMoreover, when the acquired samples are randomly reduced by 25%, 50% and 75%,\nsparse decomposition based classification with DL remains robust while the CNN\naccuracy is drastically compromised.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 14:42:42 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 10:08:05 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Giovanneschi", "Fabio", ""], ["Mishra", "Kumar Vijay", ""], ["Gonzalez-Huici", "Maria Antonia", ""], ["Eldar", "Yonina C.", ""], ["Ender", "Joachim H. G.", ""]]}, {"id": "1806.04606", "submitter": "Xu Lan", "authors": "Xu Lan, Xiatian Zhu and Shaogang Gong", "title": "Knowledge Distillation by On-the-Fly Native Ensemble", "comments": "To appear in NIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation is effective to train small and generalisable network\nmodels for meeting the low-memory and fast running requirements. Existing\noffline distillation methods rely on a strong pre-trained teacher, which\nenables favourable knowledge discovery and transfer but requires a complex\ntwo-phase training procedure. Online counterparts address this limitation at\nthe price of lacking a highcapacity teacher. In this work, we present an\nOn-the-fly Native Ensemble (ONE) strategy for one-stage online distillation.\nSpecifically, ONE trains only a single multi-branch network while\nsimultaneously establishing a strong teacher on-the- fly to enhance the\nlearning of target network. Extensive evaluations show that ONE improves the\ngeneralisation performance a variety of deep neural networks more significantly\nthan alternative methods on four image classification dataset: CIFAR10,\nCIFAR100, SVHN, and ImageNet, whilst having the computational efficiency\nadvantages.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 15:28:53 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 22:27:55 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Lan", "Xu", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1806.04611", "submitter": "Mejdi Ben Dkhil", "authors": "Mejdi Ben Dkhil, Ali Wali, and Adel M. Alimi", "title": "A Hierarchical Fuzzy System for an Advanced Driving Assistance System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a hierarchical fuzzy system by evaluating the risk\nstate for a Driver Assistance System in order to contribute in reducing the\nroad accident's number. A key component of this system is its ability to\ncontinually detect and test the inside and outside risks in real time: The\noutside car risks by detecting various road moving objects; this proposed\nsystem stands on computer vision approaches. The inside risks by presenting an\nautomatic system for drowsy driving identification or detection by evaluating\nEEG signals of the driver; this developed system is based on computer vision\ntechniques and biometrics factors (electroencephalogram EEG). This proposed\nsystem is then composed of three main modules. The first module is responsible\nfor identifying the driver drowsiness state through his eye movements (physical\ndrowsiness). The second one is responsible for detecting and analysing his\nphysiological signals to also identify his drowsiness state (moral drowsiness).\nThe third module is responsible to evaluate the road driving risks by detecting\nof the road different moving objects in a real time. The final decision will be\nobtained by merging of the three detection systems through the use of fuzzy\ndecision rules. Finally, the proposed approach has been improved on ten samples\nfrom a proposed dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 18:33:42 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Dkhil", "Mejdi Ben", ""], ["Wali", "Ali", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1806.04618", "submitter": "Nicholas Heller", "authors": "Nicholas Heller, Joshua Dean, Nikolaos Papanikolopoulos", "title": "Imperfect Segmentation Labels: How Much Do They Matter?", "comments": "9 pages, 3 figures, Accepted at MICCAI LABELS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Labeled datasets for semantic segmentation are imperfect, especially in\nmedical imaging where borders are often subtle or ill-defined. Little work has\nbeen done to analyze the effect that label errors have on the performance of\nsegmentation methodologies. Here we present a large-scale study of model\nperformance in the presence of varying types and degrees of error in training\ndata. We trained U-Net, SegNet, and FCN32 several times for liver segmentation\nwith 10 different modes of ground-truth perturbation. Our results show that for\neach architecture, performance steadily declines with boundary-localized\nerrors, however, U-Net was significantly more robust to jagged boundary errors\nthan the other architectures. We also found that each architecture was very\nrobust to non-boundary-localized errors, suggesting that boundary-localized\nerrors are fundamentally different and more challenging problem than random\nlabel errors in a classification setting.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 15:54:42 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 20:27:00 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 03:10:53 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Heller", "Nicholas", ""], ["Dean", "Joshua", ""], ["Papanikolopoulos", "Nikolaos", ""]]}, {"id": "1806.04620", "submitter": "Vinicius Furlan", "authors": "Vinicius S. Furlan, Ruzena Bajcsy, Erickson R. Nascimento", "title": "Fast forwarding Egocentric Videos by Listening and Watching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable technological advance in well-equipped wearable devices is\npushing an increasing production of long first-person videos. However, since\nmost of these videos have long and tedious parts, they are forgotten or never\nseen. Despite a large number of techniques proposed to fast-forward these\nvideos by highlighting relevant moments, most of them are image based only.\nMost of these techniques disregard other relevant sensors present in the\ncurrent devices such as high-definition microphones. In this work, we propose a\nnew approach to fast-forward videos using psychoacoustic metrics extracted from\nthe soundtrack. These metrics can be used to estimate the annoyance of a\nsegment allowing our method to emphasize moments of sound pleasantness. The\nefficiency of our method is demonstrated through qualitative results and\nquantitative results as far as of speed-up and instability are concerned.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 15:58:53 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Furlan", "Vinicius S.", ""], ["Bajcsy", "Ruzena", ""], ["Nascimento", "Erickson R.", ""]]}, {"id": "1806.04627", "submitter": "Soheil Esmaeilzadeh", "authors": "Soheil Esmaeilzadeh, Ouassim Khebzegga, Mehrad Moradshahi", "title": "Clinical Parameters Prediction for Gait Disorder Recognition", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict clinical parameters in order to diagnose gait disorders\nin a patient is of great value in planning treatments. It is known that\n\\textit{decision parameters} such as cadence, step length, and walking speed\nare critical in the diagnosis of gait disorders in patients. This project aims\nto predict the decision parameters using two ways and afterwards giving advice\non whether a patient needs treatment or not. In one way, we use clinically\nmeasured parameters such as Ankle Dorsiflexion, age, walking speed, step\nlength, stride length, weight over height squared (BMI) and etc. to predict the\ndecision parameters. In a second way, we use videos recorded from patient's\nwalking tests in a clinic in order to extract the coordinates of the joints of\nthe patient over time and predict the decision parameters. Finally, having the\ndecision parameters we pre-classify gait disorder intensity of a patient and as\nthe result make decisions on whether a patient needs treatment or not.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 17:29:27 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Esmaeilzadeh", "Soheil", ""], ["Khebzegga", "Ouassim", ""], ["Moradshahi", "Mehrad", ""]]}, {"id": "1806.04646", "submitter": "George Gondim-Ribeiro", "authors": "George Gondim-Ribeiro, Pedro Tabacof, Eduardo Valle", "title": "Adversarial Attacks on Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks are malicious inputs that derail machine-learning models.\nWe propose a scheme to attack autoencoders, as well as a quantitative\nevaluation framework that correlates well with the qualitative assessment of\nthe attacks. We assess --- with statistically validated experiments --- the\nresistance to attacks of three variational autoencoders (simple, convolutional,\nand DRAW) in three datasets (MNIST, SVHN, CelebA), showing that both DRAW's\nrecurrence and attention mechanism lead to better resistance. As autoencoders\nare proposed for compressing data --- a scenario in which their safety is\nparamount --- we expect more attention will be given to adversarial attacks on\nthem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 16:59:14 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Gondim-Ribeiro", "George", ""], ["Tabacof", "Pedro", ""], ["Valle", "Eduardo", ""]]}, {"id": "1806.04659", "submitter": "Xiang Wang", "authors": "Xiang Wang, Shaodi You, Xi Li, Huimin Ma", "title": "Weakly-Supervised Semantic Segmentation by Iteratively Mining Common\n  Object Features", "comments": "accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised semantic segmentation under image tags supervision is a\nchallenging task as it directly associates high-level semantic to low-level\nappearance. To bridge this gap, in this paper, we propose an iterative\nbottom-up and top-down framework which alternatively expands object regions and\noptimizes segmentation network. We start from initial localization produced by\nclassification networks. While classification networks are only responsive to\nsmall and coarse discriminative object regions, we argue that, these regions\ncontain significant common features about objects. So in the bottom-up step, we\nmine common object features from the initial localization and expand object\nregions with the mined features. To supplement non-discriminative regions,\nsaliency maps are then considered under Bayesian framework to refine the object\nregions. Then in the top-down step, the refined object regions are used as\nsupervision to train the segmentation network and to predict object masks.\nThese object masks provide more accurate localization and contain more regions\nof object. Further, we take these object masks as initial localization and mine\ncommon object features from them. These processes are conducted iteratively to\nprogressively produce fine object masks and optimize segmentation networks.\nExperimental results on Pascal VOC 2012 dataset demonstrate that the proposed\nmethod outperforms previous state-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 17:42:10 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Wang", "Xiang", ""], ["You", "Shaodi", ""], ["Li", "Xi", ""], ["Ma", "Huimin", ""]]}, {"id": "1806.04725", "submitter": "Dongqing Zhang", "authors": "Dongqing Zhang, Jianing Wang, Jack H. Noble, Benoit M. Dawant", "title": "Accurate Detection of Inner Ears in Head CTs Using a Deep\n  Volume-to-Volume Regression Network with False Positive Suppression and a\n  Shape-Based Constraint", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cochlear implants (CIs) are neural prosthetics which are used to treat\npatients with hearing loss. CIs use an array of electrodes which are surgically\ninserted into the cochlea to stimulate the auditory nerve endings. After\nsurgery, CIs need to be programmed. Studies have shown that the spatial\nrelationship between the intra-cochlear anatomy and electrodes derived from\nmedical images can guide CI programming and lead to significant improvement in\nhearing outcomes. However, clinical head CT images are usually obtained from\nscanners of different brands with different protocols. The field of view thus\nvaries greatly and visual inspection is needed to document their content prior\nto applying algorithms for electrode localization and intra-cochlear anatomy\nsegmentation. In this work, to determine the presence/absence of inner ears and\nto accurately localize them in head CTs, we use a volume-to-volume\nconvolutional neural network which can be trained end-to-end to map a raw CT\nvolume to probability maps which indicate inner ear positions. We incorporate a\nfalse positive suppression strategy in training and apply a shape-based\nconstraint. We achieve a labeling accuracy of 98.59% and a localization error\nof 2.45mm. The localization error is significantly smaller than a random\nforest-based approach that has been proposed recently to perform the same task.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 19:19:00 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Zhang", "Dongqing", ""], ["Wang", "Jianing", ""], ["Noble", "Jack H.", ""], ["Dawant", "Benoit M.", ""]]}, {"id": "1806.04728", "submitter": "Leonid Karlinsky", "authors": "Leonid Karlinsky, Joseph Shtok, Sivan Harary, Eli Schwartz, Amit\n  Aides, Rogerio Feris, Raja Giryes, Alex M. Bronstein", "title": "RepMet: Representative-based metric learning for classification and\n  one-shot object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning (DML) has been successfully applied to object\nclassification, both in the standard regime of rich training data and in the\nfew-shot scenario, where each category is represented by only a few examples.\nIn this work, we propose a new method for DML that simultaneously learns the\nbackbone network parameters, the embedding space, and the multi-modal\ndistribution of each of the training categories in that space, in a single\nend-to-end training process. Our approach outperforms state-of-the-art methods\nfor DML-based object classification on a variety of standard fine-grained\ndatasets. Furthermore, we demonstrate the effectiveness of our approach on the\nproblem of few-shot object detection, by incorporating the proposed DML\narchitecture as a classification head into a standard object detection model.\nWe achieve the best results on the ImageNet-LOC dataset compared to strong\nbaselines, when only a few training examples are available. We also offer the\ncommunity a new episodic benchmark based on the ImageNet dataset for the\nfew-shot object detection task.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 19:25:38 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 13:13:24 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 13:33:52 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Karlinsky", "Leonid", ""], ["Shtok", "Joseph", ""], ["Harary", "Sivan", ""], ["Schwartz", "Eli", ""], ["Aides", "Amit", ""], ["Feris", "Rogerio", ""], ["Giryes", "Raja", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1806.04734", "submitter": "Leonid Karlinsky", "authors": "Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias\n  Marder, Rogerio Feris, Abhishek Kumar, Raja Giryes, Alex M. Bronstein", "title": "Delta-encoder: an effective sample synthesis method for few-shot object\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to classify new categories based on just one or a few examples is a\nlong-standing challenge in modern computer vision. In this work, we proposes a\nsimple yet effective method for few-shot (and one-shot) object recognition. Our\napproach is based on a modified auto-encoder, denoted Delta-encoder, that\nlearns to synthesize new samples for an unseen category just by seeing few\nexamples from it. The synthesized samples are then used to train a classifier.\nThe proposed approach learns to both extract transferable intra-class\ndeformations, or \"deltas\", between same-class pairs of training examples, and\nto apply those deltas to the few provided examples of a novel class (unseen\nduring training) in order to efficiently synthesize samples from that new\nclass. The proposed method improves over the state-of-the-art in one-shot\nobject-recognition and compares favorably in the few-shot case. Upon acceptance\ncode will be made available.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 19:31:11 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 11:41:46 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 14:38:20 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Schwartz", "Eli", ""], ["Karlinsky", "Leonid", ""], ["Shtok", "Joseph", ""], ["Harary", "Sivan", ""], ["Marder", "Mattias", ""], ["Feris", "Rogerio", ""], ["Kumar", "Abhishek", ""], ["Giryes", "Raja", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1806.04765", "submitter": "Adon Phillips", "authors": "Adon Phillips, Iris Teo, Jochen Lang", "title": "Fully Convolutional Network for Melanoma Diagnostics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work seeks to determine how modern machine learning techniques may be\napplied to the previously unexplored topic of melanoma diagnostics using\ndigital pathology. We curated a new dataset of 50 patient cases of cutaneous\nmelanoma using digital pathology. We provide gold standard annotations for\nthree tissue types (tumour, epidermis, and dermis) which are important for the\nprognostic measurements known as Breslow thickness and Clark level. Then, we\ndevised a novel multi-stride fully convolutional network (FCN) architecture\nthat outperformed other networks trained and evaluated using the same data\naccording to standard metrics. Finally, we trained a model to detect and\nlocalize the target tissue types. When processing previously unseen cases, our\nmodel's output is qualitatively very similar to the gold standard. In addition\nto the standard metrics computed as a baseline for our approach, we asked three\nadditional pathologists to measure the Breslow thickness on the network's\noutput. Their responses were diagnostically equivalent to the ground truth\nmeasurements, and when removing cases where a measurement was not appropriate,\ninter-rater reliability (IRR) between the four pathologists was 75.0%. Given\nthe qualitative and quantitative results, it is possible to overcome the\ndiscriminative challenges of the skin and tumour anatomy for segmentation using\nmodern machine learning techniques, though more work is required to improve the\nnetwork's performance on dermis segmentation. Further, we show that it is\npossible to achieve a level of accuracy required to manually perform the\nBreslow thickness measurement.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 20:53:55 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Phillips", "Adon", ""], ["Teo", "Iris", ""], ["Lang", "Jochen", ""]]}, {"id": "1806.04768", "submitter": "Nevan Wichers", "authors": "Nevan Wichers, Ruben Villegas, Dumitru Erhan, Honglak Lee", "title": "Hierarchical Long-term Video Prediction without Supervision", "comments": "International Conference on Machine Learning (ICML) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of recent research has been devoted to video prediction and generation,\nyet most of the previous works have demonstrated only limited success in\ngenerating videos on short-term horizons. The hierarchical video prediction\nmethod by Villegas et al. (2017) is an example of a state-of-the-art method for\nlong-term video prediction, but their method is limited because it requires\nground truth annotation of high-level structures (e.g., human joint landmarks)\nat training time. Our network encodes the input frame, predicts a high-level\nencoding into the future, and then a decoder with access to the first frame\nproduces the predicted image from the predicted encoding. The decoder also\nproduces a mask that outlines the predicted foreground object (e.g., person) as\na by-product. Unlike Villegas et al. (2017), we develop a novel training method\nthat jointly trains the encoder, the predictor, and the decoder together\nwithout highlevel supervision; we further improve upon this by using an\nadversarial loss in the feature space to train the predictor. Our method can\npredict about 20 seconds into the future and provides better results compared\nto Denton and Fergus (2018) and Finn et al. (2016) on the Human 3.6M dataset.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 21:11:07 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Wichers", "Nevan", ""], ["Villegas", "Ruben", ""], ["Erhan", "Dumitru", ""], ["Lee", "Honglak", ""]]}, {"id": "1806.04779", "submitter": "Nicholas Heller", "authors": "Nicholas Heller, Derek Anderson, Matt Baker, Brad Juffer, and Nikolaos\n  Papanikolopoulos", "title": "Convolutional Neural Networks for Aircraft Noise Monitoring", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Air travel is one of the fastest growing modes of transportation, however,\nthe effects of aircraft noise on populations surrounding airports is hindering\nits growth. In an effort to study and ultimately mitigate the impact that this\nnoise has, many airports continuously monitor the aircraft noise in their\nsurrounding communities. Noise monitoring and analysis is complicated by the\nfact that aircraft are not the only source of noise. In this work, we show that\na Convolutional Neural Network is well-suited for the task of identifying noise\nevents which are not caused by aircraft. Our system achieves an accuracy of\n0.970 when trained on 900 manually labeled noise events. Our training data and\na TensorFlow implementation of our model are available at\nhttps://github.com/neheller/aircraftnoise.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 21:51:50 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Heller", "Nicholas", ""], ["Anderson", "Derek", ""], ["Baker", "Matt", ""], ["Juffer", "Brad", ""], ["Papanikolopoulos", "Nikolaos", ""]]}, {"id": "1806.04793", "submitter": "Fabian Tschopp", "authors": "Fabian David Tschopp, Michael B. Reiser, Srinivas C. Turaga", "title": "A Connectome Based Hexagonal Lattice Convolutional Network Model of the\n  Drosophila Visual System", "comments": "Work in progress. Final paper with results from an updated model with\n  new connectome data will be coming soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What can we learn from a connectome? We constructed a simplified model of the\nfirst two stages of the fly visual system, the lamina and medulla. The\nresulting hexagonal lattice convolutional network was trained using\nbackpropagation through time to perform object tracking in natural scene\nvideos. Networks initialized with weights from connectome reconstructions\nautomatically discovered well-known orientation and direction selectivity\nproperties in T4 neurons and their inputs, while networks initialized at random\ndid not. Our work is the first demonstration, that knowledge of the connectome\ncan enable in silico predictions of the functional properties of individual\nneurons in a circuit, leading to an understanding of circuit function from\nstructure alone.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 22:57:14 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 10:36:40 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Tschopp", "Fabian David", ""], ["Reiser", "Michael B.", ""], ["Turaga", "Srinivas C.", ""]]}, {"id": "1806.04807", "submitter": "Chengzhou Tang", "authors": "Chengzhou Tang, Ping Tan", "title": "BA-Net: Dense Bundle Adjustment Network", "comments": "Revised for rebuttal Code available at\n  https://github.com/frobelbest/BANet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a network architecture to solve the\nstructure-from-motion (SfM) problem via feature-metric bundle adjustment (BA),\nwhich explicitly enforces multi-view geometry constraints in the form of\nfeature-metric error. The whole pipeline is differentiable so that the network\ncan learn suitable features that make the BA problem more tractable.\nFurthermore, this work introduces a novel depth parameterization to recover\ndense per-pixel depth. The network first generates several basis depth maps\naccording to the input image and optimizes the final depth as a linear\ncombination of these basis depth maps via feature-metric BA. The basis depth\nmaps generator is also learned via end-to-end training. The whole system nicely\ncombines domain knowledge (i.e. hard-coded multi-view geometry constraints) and\ndeep learning (i.e. feature learning and basis depth maps learning) to address\nthe challenging dense SfM problem. Experiments on large scale real data prove\nthe success of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 00:51:48 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 11:08:26 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2019 19:20:00 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Tang", "Chengzhou", ""], ["Tan", "Ping", ""]]}, {"id": "1806.04828", "submitter": "Xue Yang", "authors": "Xue Yang, Hao Sun, Xian Sun, Menglong Yan, Zhi Guo, and Kun Fu", "title": "Position Detection and Direction Prediction for Arbitrary-Oriented Ships\n  via Multitask Rotation Region Convolutional Neural Network", "comments": null, "journal-ref": "IEEE ACCESS. 2018, 6, 50839 - 50849", "doi": "10.1109/ACCESS.2018.2869884", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ship detection is of great importance and full of challenges in the field of\nremote sensing. The complexity of application scenarios, the redundancy of\ndetection region, and the difficulty of dense ship detection are all the main\nobstacles that limit the successful operation of traditional methods in ship\ndetection. In this paper, we propose a brand new detection model based on\nmultitask rotational region convolutional neural network to solve the problems\nabove. This model is mainly consist of five consecutive parts: Dense Feature\nPyramid Network (DFPN), adaptive region of interest (ROI) Align, rotational\nbounding box regression, prow direction prediction and rotational nonmaximum\nsuppression (R-NMS). First of all, the low-level location information and\nhigh-level semantic information are fully utilized through multiscale feature\nnetworks. Then, we design Adaptive ROI Align to obtain high quality proposals\nwhich remain complete spatial and semantic information. Unlike most previous\napproaches, the prediction obtained by our method is the minimum bounding\nrectangle of the object with less redundant regions. Therefore, rotational\nregion detection framework is more suitable to detect the dense object than\ntraditional detection model. Additionally, we can find the berthing and sailing\ndirection of ship through prediction. A detailed evaluation based on SRSS for\nrotation detection shows that our detection method has a competitive\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 02:48:44 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 10:34:21 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Yang", "Xue", ""], ["Sun", "Hao", ""], ["Sun", "Xian", ""], ["Yan", "Menglong", ""], ["Guo", "Zhi", ""], ["Fu", "Kun", ""]]}, {"id": "1806.04845", "submitter": "Mingli Song", "authors": "Zunlei Feng, Zhenyun Yu, Yezhou Yang, Yongcheng Jing, Junxiao Jiang,\n  Mingli Song", "title": "Interpretable Partitioned Embedding for Customized Fashion Outfit\n  Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent fashion outfit composition becomes more and more popular in these\nyears. Some deep learning based approaches reveal competitive composition\nrecently. However, the unexplainable characteristic makes such deep learning\nbased approach cannot meet the the designer, businesses and consumers' urge to\ncomprehend the importance of different attributes in an outfit composition. To\nrealize interpretable and customized fashion outfit compositions, we propose a\npartitioned embedding network to learn interpretable representations from\nclothing items. The overall network architecture consists of three components:\nan auto-encoder module, a supervised attributes module and a multi-independent\nmodule. The auto-encoder module serves to encode all useful information into\nthe embedding. In the supervised attributes module, multiple attributes labels\nare adopted to ensure that different parts of the overall embedding correspond\nto different attributes. In the multi-independent module, adversarial operation\nare adopted to fulfill the mutually independent constraint. With the\ninterpretable and partitioned embedding, we then construct an outfit\ncomposition graph and an attribute matching map. Given specified attributes\ndescription, our model can recommend a ranked list of outfit composition with\ninterpretable matching scores. Extensive experiments demonstrate that 1) the\npartitioned embedding have unmingled parts which corresponding to different\nattributes and 2) outfits recommended by our model are more desirable in\ncomparison with the existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 04:57:06 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 01:21:10 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 12:57:01 GMT"}, {"version": "v4", "created": "Thu, 21 Jun 2018 04:21:30 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Feng", "Zunlei", ""], ["Yu", "Zhenyun", ""], ["Yang", "Yezhou", ""], ["Jing", "Yongcheng", ""], ["Jiang", "Junxiao", ""], ["Song", "Mingli", ""]]}, {"id": "1806.04860", "submitter": "Chen Zhu", "authors": "Zhou Su, Chen Zhu, Yinpeng Dong, Dongqi Cai, Yurong Chen, Jianguo Li", "title": "Learning Visual Knowledge Memory Networks for Visual Question Answering", "comments": "Supplementary to CVPR 2018 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) requires joint comprehension of images and\nnatural language questions, where many questions can't be directly or clearly\nanswered from visual content but require reasoning from structured human\nknowledge with confirmation from visual content. This paper proposes visual\nknowledge memory network (VKMN) to address this issue, which seamlessly\nincorporates structured human knowledge and deep visual features into memory\nnetworks in an end-to-end learning framework. Comparing to existing methods for\nleveraging external knowledge for supporting VQA, this paper stresses more on\ntwo missing mechanisms. First is the mechanism for integrating visual contents\nwith knowledge facts. VKMN handles this issue by embedding knowledge triples\n(subject, relation, target) and deep visual features jointly into the visual\nknowledge features. Second is the mechanism for handling multiple knowledge\nfacts expanding from question and answer pairs. VKMN stores joint embedding\nusing key-value pair structure in the memory networks so that it is easy to\nhandle multiple facts. Experiments show that the proposed method achieves\npromising results on both VQA v1.0 and v2.0 benchmarks, while outperforms\nstate-of-the-art methods on the knowledge-reasoning related questions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 06:37:42 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Su", "Zhou", ""], ["Zhu", "Chen", ""], ["Dong", "Yinpeng", ""], ["Cai", "Dongqi", ""], ["Chen", "Yurong", ""], ["Li", "Jianguo", ""]]}, {"id": "1806.04895", "submitter": "Mingkui Tan", "authors": "Jiezhang Cao, Yong Guo, Qingyao Wu, Chunhua Shen, Junzhou Huang,\n  Mingkui Tan", "title": "Adversarial Learning with Local Coordinate Coding", "comments": "14 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) aim to generate realistic data from\nsome prior distribution (e.g., Gaussian noises). However, such prior\ndistribution is often independent of real data and thus may lose semantic\ninformation (e.g., geometric structure or content in images) of data. In\npractice, the semantic information might be represented by some latent\ndistribution learned from data, which, however, is hard to be used for sampling\nin GANs. In this paper, rather than sampling from the pre-defined prior\ndistribution, we propose a Local Coordinate Coding (LCC) based sampling method\nto improve GANs. We derive a generalization bound for LCC based GANs and prove\nthat a small dimensional input is sufficient to achieve good generalization.\nExtensive experiments on various real-world datasets demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 08:49:30 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 02:05:09 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Cao", "Jiezhang", ""], ["Guo", "Yong", ""], ["Wu", "Qingyao", ""], ["Shen", "Chunhua", ""], ["Huang", "Junzhou", ""], ["Tan", "Mingkui", ""]]}, {"id": "1806.04932", "submitter": "Josep L. Rossello", "authors": "Alejandro Mor\\'an, Christiam F. Frasser and Josep L. Rossell\\'o", "title": "Reservoir Computing Hardware with Cellular Automata", "comments": "20 pages, 11 figures, draft of an article currently submitted to IEEE\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV nlin.CG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Elementary cellular automata (ECA) is a widely studied one-dimensional\nprocessing methodology where the successive iteration of the automaton may lead\nto the recreation of a rich pattern dynamic. Recently, cellular automata have\nbeen proposed as a feasible way to implement Reservoir Computing (RC) systems\nin which the automata rule is fixed and the training is performed using a\nlinear regression. In this work we perform an exhaustive study of the\nperformance of the different ECA rules when applied to pattern recognition of\ntime-independent input signals using a RC scheme. Once the different ECA rules\nhave been tested, the most accurate one (rule 90) is selected to implement a\ndigital circuit. Rule 90 is easily reproduced using a reduced set of XOR gates\nand shift-registers, thus representing a high-performance alternative for RC\nhardware implementation in terms of processing time, circuit area, power\ndissipation and system accuracy. The model (both in software and its hardware\nimplementation) has been tested using a pattern recognition task of handwritten\nnumbers (the MNIST database) for which we obtained competitive results in terms\nof accuracy, speed and power dissipation. The proposed model can be considered\nto be a low-cost method to implement fast pattern recognition digital circuits.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:28:44 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 09:23:43 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Mor\u00e1n", "Alejandro", ""], ["Frasser", "Christiam F.", ""], ["Rossell\u00f3", "Josep L.", ""]]}, {"id": "1806.04935", "submitter": "Ana Serrano", "authors": "Ana Serrano, Elena Garces, Diego Gutierrez, Belen Masia", "title": "Convolutional sparse coding for capturing high speed video content", "comments": null, "journal-ref": "Computer Graphics Forum 36, 8, Pages 380-389 (February 2017)", "doi": "10.1111/cgf.13086", "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video capture is limited by the trade-off between spatial and temporal\nresolution: when capturing videos of high temporal resolution, the spatial\nresolution decreases due to bandwidth limitations in the capture system.\nAchieving both high spatial and temporal resolution is only possible with\nhighly specialized and very expensive hardware, and even then the same basic\ntrade-off remains. The recent introduction of compressive sensing and sparse\nreconstruction techniques allows for the capture of single-shot high-speed\nvideo, by coding the temporal information in a single frame, and then\nreconstructing the full video sequence from this single coded image and a\ntrained dictionary of image patches. In this paper, we first analyze this\napproach, and find insights that help improve the quality of the reconstructed\nvideos. We then introduce a novel technique, based on convolutional sparse\ncoding (CSC), and show how it outperforms the state-of-the-art, patch-based\napproach in terms of flexibility and efficiency, due to the convolutional\nnature of its filter banks. The key idea for CSC high-speed video acquisition\nis extending the basic formulation by imposing an additional constraint in the\ntemporal dimension, which enforces sparsity of the first-order derivatives over\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:31:07 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Serrano", "Ana", ""], ["Garces", "Elena", ""], ["Gutierrez", "Diego", ""], ["Masia", "Belen", ""]]}, {"id": "1806.04942", "submitter": "Ana Serrano", "authors": "Ana Serrano, Felix Heide, Diego Gutierrez, Gordon Wetzstein, Belen\n  Masia", "title": "Convolutional Sparse Coding for High Dynamic Range Imaging", "comments": null, "journal-ref": "Computer Graphics Forum 35, 2, Pages 153-163 (May 2016)", "doi": "10.1111/cgf.12819", "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current HDR acquisition techniques are based on either (i) fusing\nmultibracketed, low dynamic range (LDR) images, (ii) modifying existing\nhardware and capturing different exposures simultaneously with multiple\nsensors, or (iii) reconstructing a single image with spatially-varying pixel\nexposures. In this paper, we propose a novel algorithm to recover high-quality\nHDRI images from a single, coded exposure. The proposed reconstruction method\nbuilds on recently-introduced ideas of convolutional sparse coding (CSC); this\npaper demonstrates how to make CSC practical for HDR imaging. We demonstrate\nthat the proposed algorithm achieves higher-quality reconstructions than\nalternative methods, we evaluate optical coding schemes, analyze algorithmic\nparameters, and build a prototype coded HDR camera that demonstrates the\nutility of convolutional sparse HDRI coding with a custom hardware platform.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:48:33 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Serrano", "Ana", ""], ["Heide", "Felix", ""], ["Gutierrez", "Diego", ""], ["Wetzstein", "Gordon", ""], ["Masia", "Belen", ""]]}, {"id": "1806.04955", "submitter": "Mirek Janatka Mr", "authors": "Mirek Janatka, Ashwin Sridhar, John Kelly, Danail Stoyanov", "title": "Higher Order of Motion Magnification for Vessel Localisation in Surgical\n  Video", "comments": "Accepted to the International Conference On Medical Image Computing &\n  Computer Assisted Intervention, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locating vessels during surgery is critical for avoiding inadvertent damage,\nyet vasculature can be difficult to identify. Video motion magnification can\npotentially highlight vessels by exaggerating subtle motion embedded within the\nvideo to become perceivable to the surgeon. In this paper, we explore a\nphysiological model of artery distension to extend motion magnification to\nincorporate higher orders of motion, leveraging the difference in acceleration\nover time (jerk) in pulsatile motion to highlight the vascular pulse wave. Our\nmethod is compared to first and second order motion based Eulerian video\nmagnification algorithms. Using data from a surgical video retrieved during a\nrobotic prostatectomy, we show that our method can accentuate\ncardio-physiological features and produce a more succinct and clearer video for\nmotion magnification, with more similarities in areas without motion to the\nsource video at large magnifications. We validate the approach with a Structure\nSimilarity (SSIM) and Peak Signal to Noise Ratio (PSNR) assessment of three\nvideos at an increasing working distance, using three different levels of\noptical magnification. Spatio-temporal cross sections are presented to show the\neffectiveness of our proposal and video samples are provided to demonstrates\nqualitatively our results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 11:22:54 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Janatka", "Mirek", ""], ["Sridhar", "Ashwin", ""], ["Kelly", "John", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1806.04957", "submitter": "Shreyank Jyoti", "authors": "Shreyank Jyoti, Abhinav Dhall", "title": "Expression Empowered ResiDen Network for Facial Action Unit Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper explores the topic of Facial Action Unit (FAU) detection in the\nwild. In particular, we are interested in answering the following questions:\n(1) how useful are residual connections across dense blocks for face analysis?\n(2) how useful is the information from a network trained for categorical Facial\nExpression Recognition (FER) for the task of FAU detection? The proposed\nnetwork (ResiDen) exploits dense blocks along with residual connections and\nuses auxiliary information from a FER network. The experiments are performed on\nthe EmotionNet and DISFA datasets. The experiments show the usefulness of\nfacial expression information for AU detection. The proposed network achieves\nstate-of-art results on the two databases. Analysis of the results for cross\ndatabase protocol shows the effectiveness of the network.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 11:28:26 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Jyoti", "Shreyank", ""], ["Dhall", "Abhinav", ""]]}, {"id": "1806.04972", "submitter": "Xiaoran Chen", "authors": "Xiaoran Chen, Ender Konukoglu", "title": "Unsupervised Detection of Lesions in Brain MRI using constrained\n  adversarial auto-encoders", "comments": "9 pages, 5 figures, accepted at MIDL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion detection in brain Magnetic Resonance Images (MRI) remains a\nchallenging task. State-of-the-art approaches are mostly based on supervised\nlearning making use of large annotated datasets. Human beings, on the other\nhand, even non-experts, can detect most abnormal lesions after seeing a handful\nof healthy brain images. Replicating this capability of using prior information\non the appearance of healthy brain structure to detect lesions can help\ncomputers achieve human level abnormality detection, specifically reducing the\nneed for numerous labeled examples and bettering generalization of previously\nunseen lesions. To this end, we study detection of lesion regions in an\nunsupervised manner by learning data distribution of brain MRI of healthy\nsubjects using auto-encoder based methods. We hypothesize that one of the main\nlimitations of the current models is the lack of consistency in latent\nrepresentation. We propose a simple yet effective constraint that helps mapping\nof an image bearing lesion close to its corresponding healthy image in the\nlatent space. We use the Human Connectome Project dataset to learn distribution\nof healthy-appearing brain MRI and report improved detection, in terms of AUC,\nof the lesions in the BRATS challenge dataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 12:15:54 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Chen", "Xiaoran", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1806.05024", "submitter": "Simon Jenni", "authors": "Simon Jenni and Paolo Favaro", "title": "Self-Supervised Feature Learning by Learning to Spot Artifacts", "comments": "CVPR 2018 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel self-supervised learning method based on adversarial\ntraining. Our objective is to train a discriminator network to distinguish real\nimages from images with synthetic artifacts, and then to extract features from\nits intermediate layers that can be transferred to other data domains and\ntasks. To generate images with artifacts, we pre-train a high-capacity\nautoencoder and then we use a damage and repair strategy: First, we freeze the\nautoencoder and damage the output of the encoder by randomly dropping its\nentries. Second, we augment the decoder with a repair network, and train it in\nan adversarial manner against the discriminator. The repair network helps\ngenerate more realistic images by inpainting the dropped feature entries. To\nmake the discriminator focus on the artifacts, we also make it predict what\nentries in the feature were dropped. We demonstrate experimentally that\nfeatures learned by creating and spotting artifacts achieve state of the art\nperformance in several benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:24:42 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Jenni", "Simon", ""], ["Favaro", "Paolo", ""]]}, {"id": "1806.05030", "submitter": "Herman Kamper", "authors": "Herman Kamper and Michael Roth", "title": "Visually grounded cross-lingual keyword spotting in speech", "comments": "5 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work considered how images paired with speech can be used as\nsupervision for building speech systems when transcriptions are not available.\nWe ask whether visual grounding can be used for cross-lingual keyword spotting:\ngiven a text keyword in one language, the task is to retrieve spoken utterances\ncontaining that keyword in another language. This could enable searching\nthrough speech in a low-resource language using text queries in a high-resource\nlanguage. As a proof-of-concept, we use English speech with German queries: we\nuse a German visual tagger to add keyword labels to each training image, and\nthen train a neural network to map English speech to German keywords. Without\nseeing parallel speech-transcriptions or translations, the model achieves a\nprecision at ten of 58%. We show that most erroneous retrievals contain\nequivalent or semantically relevant keywords; excluding these would improve\nP@10 to 91%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:37:34 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Kamper", "Herman", ""], ["Roth", "Michael", ""]]}, {"id": "1806.05034", "submitter": "Bernardino Romera-Paredes", "authors": "Simon A. A. Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De\n  Fauw, Joseph R. Ledsam, Klaus H. Maier-Hein, S. M. Ali Eslami, Danilo Jimenez\n  Rezende, Olaf Ronneberger", "title": "A Probabilistic U-Net for Segmentation of Ambiguous Images", "comments": "Last update: added further details about the LIDC experiment. 11\n  pages for the main paper, 28 pages including appendix. 5 figures in the main\n  paper, 18 figures in total, Advances in Neural Information Processing Systems\n  (NeurIPS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world vision problems suffer from inherent ambiguities. In clinical\napplications for example, it might not be clear from a CT scan alone which\nparticular region is cancer tissue. Therefore a group of graders typically\nproduces a set of diverse but plausible segmentations. We consider the task of\nlearning a distribution over segmentations given an input. To this end we\npropose a generative segmentation model based on a combination of a U-Net with\na conditional variational autoencoder that is capable of efficiently producing\nan unlimited number of plausible hypotheses. We show on a lung abnormalities\nsegmentation task and on a Cityscapes segmentation task that our model\nreproduces the possible segmentation variants as well as the frequencies with\nwhich they occur, doing so significantly better than published approaches.\nThese models could have a high impact in real-world applications, such as being\nused as clinical decision-making algorithms accounting for multiple plausible\nsemantic segmentation hypotheses to provide possible diagnoses and recommend\nfurther actions to resolve the present ambiguities.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:47:04 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 15:34:57 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2018 09:50:55 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 18:26:47 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Kohl", "Simon A. A.", ""], ["Romera-Paredes", "Bernardino", ""], ["Meyer", "Clemens", ""], ["De Fauw", "Jeffrey", ""], ["Ledsam", "Joseph R.", ""], ["Maier-Hein", "Klaus H.", ""], ["Eslami", "S. M. Ali", ""], ["Rezende", "Danilo Jimenez", ""], ["Ronneberger", "Olaf", ""]]}, {"id": "1806.05083", "submitter": "Heather Couture", "authors": "Heather D. Couture, J.S. Marron, Charles M. Perou, Melissa A.\n  Troester, Marc Niethammer", "title": "Multiple Instance Learning for Heterogeneous Images: Training a CNN for\n  Histopathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance (MI) learning with a convolutional neural network enables\nend-to-end training in the presence of weak image-level labels. We propose a\nnew method for aggregating predictions from smaller regions of the image into\nan image-level classification by using the quantile function. The quantile\nfunction provides a more complete description of the heterogeneity within each\nimage, improving image-level classification. We also adapt image augmentation\nto the MI framework by randomly selecting cropped regions on which to apply MI\naggregation during each epoch of training. This provides a mechanism to study\nthe importance of MI learning. We validate our method on five different\nclassification tasks for breast tumor histology and provide a visualization\nmethod for interpreting local image classifications that could lead to future\ninsights into tumor heterogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 14:24:44 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Couture", "Heather D.", ""], ["Marron", "J. S.", ""], ["Perou", "Charles M.", ""], ["Troester", "Melissa A.", ""], ["Niethammer", "Marc", ""]]}, {"id": "1806.05086", "submitter": "Jan Eric Lenssen", "authors": "Jan Eric Lenssen, Matthias Fey, Pascal Libuschewski", "title": "Group Equivariant Capsule Networks", "comments": "Presented at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present group equivariant capsule networks, a framework to introduce\nguaranteed equivariance and invariance properties to the capsule network idea.\nOur work can be divided into two contributions. First, we present a generic\nrouting by agreement algorithm defined on elements of a group and prove that\nequivariance of output pose vectors, as well as invariance of output\nactivations, hold under certain conditions. Second, we connect the resulting\nequivariant capsule networks with work from the field of group convolutional\nnetworks. Through this connection, we provide intuitions of how both methods\nrelate and are able to combine the strengths of both approaches in one deep\nneural network architecture. The resulting framework allows sparse evaluation\nof the group convolution operator, provides control over specific equivariance\nand invariance properties, and can use routing by agreement instead of pooling\noperations. In addition, it is able to provide interpretable and equivariant\nrepresentation vectors as output capsules, which disentangle evidence of object\nexistence from its pose.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 14:30:27 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 17:21:35 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Lenssen", "Jan Eric", ""], ["Fey", "Matthias", ""], ["Libuschewski", "Pascal", ""]]}, {"id": "1806.05091", "submitter": "Tomasz Trzcinski", "authors": "Norbert Kapinski, Jakub Zielinski, Bartosz A. Borucki, Tomasz\n  Trzcinski, Beata Ciszkowska-Lyson, Krzysztof S. Nowinski", "title": "Estimating Achilles tendon healing progress with convolutional neural\n  networks", "comments": "Paper accepted to MICCAI'18", "journal-ref": null, "doi": "10.1007/978-3-030-00934-2_105", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative assessment of a treatment progress in the Achilles tendon\nhealing process - one of the most common musculoskeletal disorder in modern\nmedical practice - is typically a long and complex process: multiple MRI\nprotocols need to be acquired and analysed by radiology experts. In this paper,\nwe propose to significantly reduce the complexity of this assessment using a\nnovel method based on a pre-trained convolutional neural network. We first\ntrain our neural network on over 500,000 2D axial cross-sections from over 3000\n3D MRI studies to classify MRI images as belonging to a healthy or injured\nclass, depending on the patient's condition. We then take the outputs of\nmodified pre-trained network and apply linear regression on the PCA-reduced\nspace of the features to assess treatment progress. Our method allows to reduce\nup to 5-fold the amount of data needed to be registered during the MRI scan\nwithout any information loss. Furthermore, we are able to predict the healing\nprocess phase with equal accuracy to human experts in 3 out of 6 main criteria.\nFinally, contrary to the current approaches to regeneration assessment that\nrely on radiologist subjective opinion, our method allows to objectively\ncompare different treatments methods which can lead to improved diagnostics and\npatient's recovery.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 14:43:21 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 02:21:55 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Kapinski", "Norbert", ""], ["Zielinski", "Jakub", ""], ["Borucki", "Bartosz A.", ""], ["Trzcinski", "Tomasz", ""], ["Ciszkowska-Lyson", "Beata", ""], ["Nowinski", "Krzysztof S.", ""]]}, {"id": "1806.05104", "submitter": "Hannah Spitzer", "authors": "Hannah Spitzer, Kai Kiwitz, Katrin Amunts, Stefan Harmeling, Timo\n  Dickscheid", "title": "Improving Cytoarchitectonic Segmentation of Human Brain Areas with\n  Self-supervised Siamese Networks", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cytoarchitectonic parcellations of the human brain serve as anatomical\nreferences in multimodal atlas frameworks. They are based on analysis of\ncell-body stained histological sections and the identification of borders\nbetween brain areas. The de-facto standard involves a semi-automatic,\nreproducible border detection, but does not scale with high-throughput imaging\nin large series of sections at microscopical resolution. Automatic\nparcellation, however, is extremely challenging due to high variation in the\ndata, and the need for a large field of view at microscopic resolution. The\nperformance of a recently proposed Convolutional Neural Network model that\naddresses this problem especially suffers from the naturally limited amount of\nexpert annotations for training. To circumvent this limitation, we propose to\npre-train neural networks on a self-supervised auxiliary task, predicting the\n3D distance between two patches sampled from the same brain. Compared to a\nrandom initialization, fine-tuning from these networks results in significantly\nbetter segmentations. We show that the self-supervised model has implicitly\nlearned to distinguish several cortical brain areas -- a strong indicator that\nthe proposed auxiliary task is appropriate for cytoarchitectonic mapping.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 15:17:27 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Spitzer", "Hannah", ""], ["Kiwitz", "Kai", ""], ["Amunts", "Katrin", ""], ["Harmeling", "Stefan", ""], ["Dickscheid", "Timo", ""]]}, {"id": "1806.05129", "submitter": "Xueqing Deng", "authors": "Xueqing Deng, Yi Zhu, Shawn Newsam", "title": "What Is It Like Down There? Generating Dense Ground-Level Views and\n  Image Features From Overhead Imagery Using Conditional Generative Adversarial\n  Networks", "comments": "10 pages, 5 figures, camera-ready version of ACM SIGSPATIAL 2018\n  (ORAL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates conditional generative adversarial networks (cGANs)\nto overcome a fundamental limitation of using geotagged media for geographic\ndiscovery, namely its sparse and uneven spatial distribution. We train a cGAN\nto generate ground-level views of a location given overhead imagery. We show\nthe \"fake\" ground-level images are natural looking and are structurally similar\nto the real images. More significantly, we show the generated images are\nrepresentative of the locations and that the representations learned by the\ncGANs are informative. In particular, we show that dense feature maps generated\nusing our framework are more effective for land-cover classification than\napproaches which spatially interpolate features extracted from sparse\nground-level images. To our knowledge, ours is the first work to use cGANs to\ngenerate ground-level views given overhead imagery and to explore the benefits\nof the learned representations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:17:30 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 18:26:56 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Deng", "Xueqing", ""], ["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1806.05147", "submitter": "Frederik Pahde", "authors": "Frederik Pahde, Patrick J\\\"ahnichen, Tassilo Klein, Moin Nabi", "title": "Cross-modal Hallucination for Few-shot Fine-grained Recognition", "comments": "CVPR 2018 Workshop on Fine-Grained Visual Categorization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep learning algorithms generally require large amounts of\ndata for model training. Lack thereof can severely deteriorate the performance,\nparticularly in scenarios with fine-grained boundaries between categories. To\nthis end, we propose a multimodal approach that facilitates bridging the\ninformation gap by means of meaningful joint embeddings. Specifically, we\npresent a benchmark that is multimodal during training (i.e. images and texts)\nand single-modal in testing time (i.e. images), with the associated task to\nutilize multimodal data in base classes (with many samples), to learn explicit\nvisual classifiers for novel classes (with few samples). Next, we propose a\nframework built upon the idea of cross-modal data hallucination. In this\nregard, we introduce a discriminative text-conditional GAN for sample\ngeneration with a simple self-paced strategy for sample selection. We show the\nresults of our proposed discriminative hallucinated method for 1-, 2-, and 5-\nshot learning on the CUB dataset, where the accuracy is improved by employing\nmultimodal data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 17:06:10 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 09:22:20 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Pahde", "Frederik", ""], ["J\u00e4hnichen", "Patrick", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "1806.05154", "submitter": "Evangelos Mazomenos", "authors": "Evangelos B. Mazomenos, Kamakshi Bansal, Bruce Martin, Andrew Smith,\n  Susan Wright, and Danail Stoyanov", "title": "Automated Performance Assessment in Transoesophageal Echocardiography\n  with Convolutional Neural Networks", "comments": "to be presented in MICCAI 2018, Granada, Spain, 16-20 Sep 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transoesophageal echocardiography (TEE) is a valuable diagnostic and\nmonitoring imaging modality. Proper image acquisition is essential for\ndiagnosis, yet current assessment techniques are solely based on manual expert\nreview. This paper presents a supervised deep learn ing framework for\nautomatically evaluating and grading the quality of TEE images. To obtain the\nnecessary dataset, 38 participants of varied experience performed TEE exams\nwith a high-fidelity virtual reality (VR) platform. Two Convolutional Neural\nNetwork (CNN) architectures, AlexNet and VGG, structured to perform regression,\nwere finetuned and validated on manually graded images from three evaluators.\nTwo different scoring strategies, a criteria-based percentage and an overall\ngeneral impression, were used. The developed CNN models estimate the average\nscore with a root mean square accuracy ranging between 84%-93%, indicating the\nability to replicate expert valuation. Proposed strategies for automated TEE\nassessment can have a significant impact on the training process of new TEE\noperators, providing direct feedback and facilitating the development of the\nnecessary dexterous skills.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 17:29:29 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Mazomenos", "Evangelos B.", ""], ["Bansal", "Kamakshi", ""], ["Martin", "Bruce", ""], ["Smith", "Andrew", ""], ["Wright", "Susan", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1806.05173", "submitter": "Yexun Zhang", "authors": "Yexun Zhang, Ya Zhang, Wenbin Cai", "title": "A Unified Framework for Generalizable Style Transfer: Style and Content\n  Separation", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.06454", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image style transfer has drawn broad attention in recent years. However, most\nexisting methods aim to explicitly model the transformation between different\nstyles, and the learned model is thus not generalizable to new styles. We here\npropose a unified style transfer framework for both character typeface transfer\nand neural style transfer tasks leveraging style and content separation. A key\nmerit of such framework is its generalizability to new styles and contents. The\noverall framework consists of style encoder, content encoder, mixer and\ndecoder. The style encoder and content encoder are used to extract the style\nand content representations from the corresponding reference images. The mixer\nintegrates the above two representations and feeds it into the decoder to\ngenerate images with the target style and content. During training, the encoder\nnetworks learn to extract styles and contents from limited size of\nstyle/content reference images. This learning framework allows simultaneous\nstyle transfer among multiple styles and can be deemed as a special\n`multi-task' learning scenario. The encoders are expected to capture the\nunderlying features for different styles and contents which is generalizable to\nnew styles and contents. Under this framework, we design two individual\nnetworks for character typeface transfer and neural style transfer,\nrespectively. For character typeface transfer, to separate the style features\nand content features, we leverage the conditional dependence of styles and\ncontents given an image. For neural style transfer, we leverage the statistical\ninformation of feature maps in certain layers to represent style. Extensive\nexperimental results have demonstrated the effectiveness and robustness of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 01:39:02 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Zhang", "Yexun", ""], ["Zhang", "Ya", ""], ["Cai", "Wenbin", ""]]}, {"id": "1806.05177", "submitter": "Subba Reddy Oota", "authors": "Subba Reddy Oota, Naresh Manwani, and Bapi Raju S", "title": "fMRI Semantic Category Decoding using Linguistic Encoding of Word\n  Embeddings", "comments": "12 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The dispute of how the human brain represents conceptual knowledge has been\nargued in many scientific fields. Brain imaging studies have shown that the\nspatial patterns of neural activation in the brain are correlated with thinking\nabout different semantic categories of words (for example, tools, animals, and\nbuildings) or when viewing the related pictures. In this paper, we present a\ncomputational model that learns to predict the neural activation captured in\nfunctional magnetic resonance imaging (fMRI) data of test words. Unlike the\nmodels with hand-crafted features that have been used in the literature, in\nthis paper we propose a novel approach wherein decoding models are built with\nfeatures extracted from popular linguistic encodings of Word2Vec, GloVe,\nMeta-Embeddings in conjunction with the empirical fMRI data associated with\nviewing several dozen concrete nouns. We compared these models with several\nother models that use word features extracted from FastText, Randomly-generated\nfeatures, Mitchell's 25 features [1]. The experimental results show that the\npredicted fMRI images using Meta-Embeddings meet the state-of-the-art\nperformance. Although models with features from GloVe and Word2Vec predict fMRI\nimages similar to the state-of-the-art model, model with features from\nMeta-Embeddings predicts significantly better. The proposed scheme that uses\npopular linguistic encoding offers a simple and easy approach for semantic\ndecoding from fMRI experiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:59:33 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Oota", "Subba Reddy", ""], ["Manwani", "Naresh", ""], ["S", "Bapi Raju", ""]]}, {"id": "1806.05182", "submitter": "Alexey Shvets", "authors": "Alexander V. Buslaev, Selim S. Seferbekov, Vladimir I. Iglovikov and\n  Alexey A. Shvets", "title": "Fully Convolutional Network for Automatic Road Extraction from Satellite\n  Imagery", "comments": "arXiv admin note: substantial text overlap with arXiv:1806.03510,\n  arXiv:1804.08024, arXiv:1801.05746, arXiv:1803.01207", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysis of high-resolution satellite images has been an important research\ntopic for traffic management, city planning, and road monitoring. One of the\nproblems here is automatic and precise road extraction. From an original image,\nit is difficult and computationally expensive to extract roads due to presences\nof other road-like features with straight edges. In this paper, we propose an\napproach for automatic road extraction based on a fully convolutional neural\nnetwork of U-net family. This network consists of ResNet-34 pre-trained on\nImageNet and decoder adapted from vanilla U-Net. Based on validation results,\nleaderboard and our own experience this network shows superior results for the\nDEEPGLOBE - CVPR 2018 road extraction sub-challenge. Moreover, this network\nuses moderate memory that allows using just one GTX 1080 or 1080ti video cards\nto perform whole training and makes pretty fast predictions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:36:55 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 19:20:13 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Buslaev", "Alexander V.", ""], ["Seferbekov", "Selim S.", ""], ["Iglovikov", "Vladimir I.", ""], ["Shvets", "Alexey A.", ""]]}, {"id": "1806.05199", "submitter": "Alexandre de Siqueira", "authors": "Alexandre Fioravante de Siqueira and Wagner Massayuki Nakasuga and\n  Sandro Guedes", "title": "Skeletracks: automatic separation of overlapping fission tracks in\n  apatite and muscovite using image processing", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major difficulties of automatic track counting using\nphotomicrographs is separating overlapped tracks. We address this issue\ncombining image processing algorithms such as skeletonization, and we test our\nalgorithm with several binarization techniques. The counting algorithm was\nsuccessfully applied to determine the efficiency factor GQR, necessary for\nstandardless fission-track dating, involving counting induced tracks in apatite\nand muscovite with superficial densities of about $6 \\times 10^5$\ntracks/$cm^2$.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 18:03:47 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 20:59:40 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["de Siqueira", "Alexandre Fioravante", ""], ["Nakasuga", "Wagner Massayuki", ""], ["Guedes", "Sandro", ""]]}, {"id": "1806.05217", "submitter": "Vadim Lebedev", "authors": "Vadim Lebedev, Artem Babenko, Victor Lempitsky", "title": "Impostor Networks for Fast Fine-Grained Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce impostor networks, an architecture that allows to\nperform fine-grained recognition with high accuracy and using a light-weight\nconvolutional network, making it particularly suitable for fine-grained\napplications on low-power and non-GPU enabled platforms. Impostor networks\ncompensate for the lightness of its `backend' network by combining it with a\nlightweight non-parametric classifier. The combination of a convolutional\nnetwork and such non-parametric classifier is trained in an end-to-end fashion.\nSimilarly to convolutional neural networks, impostor networks can fit\nlarge-scale training datasets very well, while also being able to generalize to\nnew data points. At the same time, the bulk of computations within impostor\nnetworks happen through nearest neighbor search in high-dimensions. Such search\ncan be performed efficiently on a variety of architectures including standard\nCPUs, where deep convolutional networks are inefficient. In a series of\nexperiments with three fine-grained datasets, we show that impostor networks\nare able to boost the classification accuracy of a moderate-sized convolutional\nnetwork considerably at a very small computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 18:44:10 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Lebedev", "Vadim", ""], ["Babenko", "Artem", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1806.05226", "submitter": "Jessica Sena", "authors": "Artur Jordao and Antonio C. Nazare Jr. and Jessica Sena and William\n  Robson Schwartz", "title": "Human Activity Recognition Based on Wearable Sensor Data: A\n  Standardization of the State-of-the-Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition based on wearable sensor data has been an\nattractive research topic due to its application in areas such as healthcare\nand smart environments. In this context, many works have presented remarkable\nresults using accelerometer, gyroscope and magnetometer data to represent the\nactivities categories. However, current studies do not consider important\nissues that lead to skewed results, making it hard to assess the quality of\nsensor-based human activity recognition and preventing a direct comparison of\nprevious works. These issues include the samples generation processes and the\nvalidation protocols used. We emphasize that in other research areas, such as\nimage classification and object detection, these issues are already\nwell-defined, which brings more efforts towards the application. Inspired by\nthis, we conduct an extensive set of experiments that analyze different sample\ngeneration processes and validation protocols to indicate the vulnerable points\nin human activity recognition based on wearable sensor data. For this purpose,\nwe implement and evaluate several top-performance methods, ranging from\nhandcrafted-based approaches to convolutional neural networks. According to our\nstudy, most of the experimental evaluations that are currently employed are not\nadequate to perform the activity recognition in the context of wearable sensor\ndata, in which the recognition accuracy drops considerably when compared to an\nappropriate evaluation approach. To the best of our knowledge, this is the\nfirst study that tackles essential issues that compromise the understanding of\nthe performance in human activity recognition based on wearable sensor data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:07:29 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 21:09:00 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 17:59:54 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Jordao", "Artur", ""], ["Nazare", "Antonio C.", "Jr."], ["Sena", "Jessica", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1806.05228", "submitter": "Thibault Groueix M.", "authors": "Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell,\n  Mathieu Aubry", "title": "3D-CODED : 3D Correspondences by Deep Deformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deep learning approach for matching deformable shapes by\nintroducing {\\it Shape Deformation Networks} which jointly encode 3D shapes and\ncorrespondences. This is achieved by factoring the surface representation into\n(i) a template, that parameterizes the surface, and (ii) a learnt global\nfeature vector that parameterizes the transformation of the template into the\ninput surface. By predicting this feature for a new shape, we implicitly\npredict correspondences between this shape and the template. We show that these\ncorrespondences can be improved by an additional step which improves the shape\nfeature by minimizing the Chamfer distance between the input and transformed\ntemplate. We demonstrate that our simple approach improves on state-of-the-art\nresults on the difficult FAUST-inter challenge, with an average correspondence\nerror of 2.88cm. We show, on the TOSCA dataset, that our method is robust to\nmany types of perturbations, and generalizes to non-human shapes. This\nrobustness allows it to perform well on real unclean, meshes from the the SCAPE\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:07:37 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 09:06:24 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Groueix", "Thibault", ""], ["Fisher", "Matthew", ""], ["Kim", "Vladimir G.", ""], ["Russell", "Bryan C.", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1806.05229", "submitter": "Ayan Chakrabarti", "authors": "Zhihao Xia, Ayan Chakrabarti", "title": "Identifying Recurring Patterns with Deep Neural Networks for Natural\n  Image Denoising", "comments": "Project page at https://projects.ayanc.org/rpcnn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising methods must effectively model, implicitly or explicitly, the\nvast diversity of patterns and textures that occur in natural images. This is\nchallenging, even for modern methods that leverage deep neural networks trained\nto regress to clean images from noisy inputs. One recourse is to rely on\n\"internal\" image statistics, by searching for similar patterns within the input\nimage itself. In this work, we propose a new method for natural image denoising\nthat trains a deep neural network to determine whether patches in a noisy image\ninput share common underlying patterns. Given a pair of noisy patches, our\nnetwork predicts whether different sub-band coefficients of the original\nnoise-free patches are similar. The denoising algorithm then aggregates matched\ncoefficients to obtain an initial estimate of the clean image. Finally, this\nestimate is provided as input, along with the original noisy image, to a\nstandard regression-based denoising network. Experiments show that our method\nachieves state-of-the-art color image denoising performance, including with a\nblind version that trains a common model for a range of noise levels, and does\nnot require knowledge of level of noise in an input image. Our approach also\nhas a distinct advantage when training with limited amounts of training data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:11:19 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 22:59:27 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 17:36:26 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Xia", "Zhihao", ""], ["Chakrabarti", "Ayan", ""]]}, {"id": "1806.05233", "submitter": "Soheil Esmaeilzadeh", "authors": "Soheil Esmaeilzadeh, Yao Yang, Ehsan Adeli", "title": "End-to-End Parkinson Disease Diagnosis using Brain MR-Images by 3D-CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we use a deep learning framework for simultaneous\nclassification and regression of Parkinson disease diagnosis based on MR-Images\nand personal information (i.e. age, gender). We intend to facilitate and\nincrease the confidence in Parkinson disease diagnosis through our deep\nlearning framework.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:23:51 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Esmaeilzadeh", "Soheil", ""], ["Yang", "Yao", ""], ["Adeli", "Ehsan", ""]]}, {"id": "1806.05237", "submitter": "Satrya Fajri Pratama", "authors": "Siti Asmah Bero, Azah Kamilah Muda, Yun-Huoy Choo, Noor Azilah Muda\n  and Satrya Fajri Pratama", "title": "Weighted Tanimoto Coefficient for 3D Molecule Structure Similarity\n  Measurement", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity searching of molecular structure has been an important application\nin the Chemoinformatics, especially in drug discovery. Similarity searching is\na common method used for identification of molecular structure. It involve\nthree main principal component of similarity searching: structure\nrepresentation; weighting scheme; and similarity coefficient. In this paper, we\nintroduces Weighted Tanimoto Coefficient based on weighted Euclidean distance\nin order to investigate the effect of weight function on the result for\nsimilarity searching. The Tanimoto coefficient is one of the popular similarity\ncoefficients used to measure the similarity between pairs of the molecule. The\nmost of research area found that the similarity searching is based on binary or\nfingerprint data. Meanwhile, we used non-binary data and was set amphetamine\nstructure as a reference or targeted structure and the rest of the dataset\nbecomes a database structure. Throughout this study, it showed that there is\ndefinitely gives a different result between a similarity searching with and\nwithout weight.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 06:55:06 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Bero", "Siti Asmah", ""], ["Muda", "Azah Kamilah", ""], ["Choo", "Yun-Huoy", ""], ["Muda", "Noor Azilah", ""], ["Pratama", "Satrya Fajri", ""]]}, {"id": "1806.05252", "submitter": "Amir Sadovnik", "authors": "Amir Sadovnik, Wassim Gharbi, Thanh Vu, Andrew Gallagher", "title": "Finding your Lookalike: Measuring Face Similarity Rather than Face\n  Identity", "comments": "Accepted to the 1st CVPR Workshop on Visual Understanding of\n  Subjective Attributes of Data 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face images are one of the main areas of focus for computer vision, receiving\non a wide variety of tasks. Although face recognition is probably the most\nwidely researched, many other tasks such as kinship detection, facial\nexpression classification and facial aging have been examined. In this work we\npropose the new, subjective task of quantifying perceived face similarity\nbetween a pair of faces. That is, we predict the perceived similarity between\nfacial images, given that they are not of the same person. Although this task\nis clearly correlated with face recognition, it is different and therefore\njustifies a separate investigation. Humans often remark that two persons look\nalike, even in cases where the persons are not actually confused with one\nanother. In addition, because face similarity is different than traditional\nimage similarity, there are challenges in data collection and labeling, and\ndealing with diverging subjective opinions between human labelers. We present\nevidence that finding facial look-alikes and recognizing faces are two distinct\ntasks. We propose a new dataset for facial similarity and introduce the\nLookalike network, directed towards similar face classification, which\noutperforms the ad hoc usage of a face recognition network directed at the same\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 20:17:53 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Sadovnik", "Amir", ""], ["Gharbi", "Wassim", ""], ["Vu", "Thanh", ""], ["Gallagher", "Andrew", ""]]}, {"id": "1806.05269", "submitter": "Shreyansh Daftry", "authors": "Shreyansh Daftry, Yashasvi Agrawal, Larry Matthies", "title": "Online Self-supervised Scene Segmentation for Micro Aerial Vehicles", "comments": null, "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA)\n  2018 Workshop on Representing a Complex World", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been numerous advances in the development of payload and\npower constrained lightweight Micro Aerial Vehicles (MAVs). As these robots\naspire for high-speed autonomous flights in complex dynamic environments,\nrobust scene understanding at long-range becomes critical. The problem is\nheavily characterized by either the limitations imposed by sensor capabilities\nfor geometry-based methods, or the need for large-amounts of manually annotated\ntraining data required by data-driven methods. This motivates the need to build\nsystems that have the capability to alleviate these problems by exploiting the\ncomplimentary strengths of both geometry and data-driven methods. In this\npaper, we take a step in this direction and propose a generic framework for\nadaptive scene segmentation using self-supervised online learning. We present\nthis in the context of vision-based autonomous MAV flight, and demonstrate the\nefficacy of our proposed system through extensive experiments on benchmark\ndatasets and real-world field tests.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 21:18:37 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Daftry", "Shreyansh", ""], ["Agrawal", "Yashasvi", ""], ["Matthies", "Larry", ""]]}, {"id": "1806.05272", "submitter": "Mireille Boutin", "authors": "Tarun Yellamraju and Jonas Hepp and Mireille Boutin", "title": "Benchmarks for Image Classification and Other High-dimensional Pattern\n  Recognition Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good classification method should yield more accurate results than simple\nheuristics. But there are classification problems, especially high-dimensional\nones like the ones based on image/video data, for which simple heuristics can\nwork quite accurately; the structure of the data in such problems is easy to\nuncover without any sophisticated or computationally expensive method. On the\nother hand, some problems have a structure that can only be found with\nsophisticated pattern recognition methods. We are interested in quantifying the\ndifficulty of a given high-dimensional pattern recognition problem. We consider\nthe case where the patterns come from two pre-determined classes and where the\nobjects are represented by points in a high-dimensional vector space. However,\nthe framework we propose is extendable to an arbitrarily large number of\nclasses. We propose classification benchmarks based on simple random projection\nheuristics. Our benchmarks are 2D curves parameterized by the classification\nerror and computational cost of these simple heuristics. Each curve divides the\nplane into a \"positive- gain\" and a \"negative-gain\" region. The latter contains\nmethods that are ill-suited for the given classification problem. The former is\ndivided into two by the curve asymptote; methods that lie in the small region\nunder the curve but right of the asymptote merely provide a computational gain\nbut no structural advantage over the random heuristics. We prove that the curve\nasymptotes are optimal (i.e. at Bayes error) in some cases, and thus no\nsophisticated method can provide a structural advantage over the random\nheuristics. Such classification problems, an example of which we present in our\nnumerical experiments, provide poor ground for testing new pattern\nclassification methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 21:22:30 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Yellamraju", "Tarun", ""], ["Hepp", "Jonas", ""], ["Boutin", "Mireille", ""]]}, {"id": "1806.05285", "submitter": "Gilles Puy", "authors": "Gilles Puy and Patrick P\\'erez", "title": "A Flexible Convolutional Solver with Application to Photorealistic Style\n  Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new flexible deep convolutional neural network (convnet) to\nperform fast visual style transfer. In contrast to existing convnets that\naddress the same task, our architecture derives directly from the structure of\nthe gradient descent originally used to solve the style transfer problem [Gatys\net al., 2016]. Like existing convnets, ours approximately solves the original\nproblem much faster than the gradient descent. However, our network is uniquely\nflexible by design: it can be manipulated at runtime to enforce new constraints\non the final solution. In particular, we show how to modify it to obtain a\nphotorealistic result with no retraining. We study the modifications made by\n[Luan et al., 2017] to the original cost function of [Gatys et al., 2016] to\nachieve photorealistic style transfer. These modifications affect directly the\ngradient descent and can be reported on-the-fly in our network. These\nmodifications are possible as the proposed architecture stems from unrolling\nthe gradient descent.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 22:05:35 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Puy", "Gilles", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1806.05299", "submitter": "Takayuki Yamada", "authors": "Takayuki Yamada", "title": "Geometric Shape Features Extraction Using a Steady State Partial\n  Differential Equation System", "comments": "31 pages, 10 figures", "journal-ref": "Journal of Computational Design and Engineering, 2019", "doi": "10.1016/j.jcde.2019.03.006", "report-no": null, "categories": "cs.CV cs.AI cs.GR math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unified method for extracting geometric shape features from binary image\ndata using a steady state partial differential equation (PDE) system as a\nboundary value problem is presented in this paper. The PDE and functions are\nformulated to extract the thickness, orientation, and skeleton simultaneously.\nThe main advantages of the proposed method is that the orientation is defined\nwithout derivatives and thickness computation is not imposed a topological\nconstraint on the target shape. A one-dimensional analytical solution is\nprovided to validate the proposed method. In addition, two-dimensional\nnumerical examples are presented to confirm the usefulness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 23:33:08 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 02:57:20 GMT"}, {"version": "v3", "created": "Sat, 13 Apr 2019 22:57:30 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yamada", "Takayuki", ""]]}, {"id": "1806.05320", "submitter": "Huiyuan Zhuo", "authors": "Huiyuan Zhuo, Xuelin Qian, Yanwei Fu, Heng Yang, Xiangyang Xue", "title": "SCSP: Spectral Clustering Filter Pruning with Soft Self-adaption Manners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) has achieved significant success in\ncomputer vision field. However, the high computational cost of the deep complex\nmodels prevents the deployment on edge devices with limited memory and\ncomputational resource. In this paper, we proposed a novel filter pruning for\nconvolutional neural networks compression, namely spectral clustering filter\npruning with soft self-adaption manners (SCSP). We first apply spectral\nclustering on filters layer by layer to explore their intrinsic connections and\nonly count on efficient groups. By self-adaption manners, the pruning\noperations can be done in few epochs to let the network gradually choose\nmeaningful groups. According to this strategy, we not only achieve model\ncompression while keeping considerable performance, but also find a novel angle\nto interpret the model compression process.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 01:24:17 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Zhuo", "Huiyuan", ""], ["Qian", "Xuelin", ""], ["Fu", "Yanwei", ""], ["Yang", "Heng", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1806.05337", "submitter": "Chandan Singh", "authors": "Chandan Singh, W. James Murdoch, Bin Yu", "title": "Hierarchical interpretations for neural network predictions", "comments": "Published in ICLR 2019", "journal-ref": "ICLR 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 02:41:03 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 07:15:40 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Singh", "Chandan", ""], ["Murdoch", "W. James", ""], ["Yu", "Bin", ""]]}, {"id": "1806.05341", "submitter": "Qingqiu Huang", "authors": "Qingqiu Huang, Yuanjun Xiong, Yu Xiong, Yuqi Zhang, Dahua Lin", "title": "From Trailers to Storylines: An Efficient Way to Learn from Movies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The millions of movies produced in the human history are valuable resources\nfor computer vision research. However, learning a vision model from movie data\nwould meet with serious difficulties. A major obstacle is the computational\ncost -- the length of a movie is often over one hour, which is substantially\nlonger than the short video clips that previous study mostly focuses on. In\nthis paper, we explore an alternative approach to learning vision models from\nmovies. Specifically, we consider a framework comprised of a visual module and\na temporal analysis module. Unlike conventional learning methods, the proposed\napproach learns these modules from different sets of data -- the former from\ntrailers while the latter from movies. This allows distinctive visual features\nto be learned within a reasonable budget while still preserving long-term\ntemporal structures across an entire movie. We construct a large-scale dataset\nfor this study and define a series of tasks on top. Experiments on this dataset\nshowed that the proposed method can substantially reduce the training time\nwhile obtaining highly effective features and coherent temporal structures.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 02:52:09 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Huang", "Qingqiu", ""], ["Xiong", "Yuanjun", ""], ["Xiong", "Yu", ""], ["Zhang", "Yuqi", ""], ["Lin", "Dahua", ""]]}, {"id": "1806.05343", "submitter": "Kun Zhao", "authors": "Kun Zhao, Arnold Wiliem, Shaokang Chen, Brian C. Lovell", "title": "Convex Class Model on Symmetric Positive Definite Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of Symmetric Positive Definite (SPD) manifold features has\nbeen proven in various computer vision tasks. However, due to the non-Euclidean\ngeometry of these features, existing Euclidean machineries cannot be directly\nused. In this paper, we tackle the classification tasks with limited training\ndata on SPD manifolds. Our proposed framework, named Manifold Convex Class\nModel, represents each class on SPD manifolds using a convex model, and\nclassification can be performed by computing distances to the convex models. We\nprovide three methods based on different metrics to address the optimization\nproblem of the smallest distance of a point to the convex model on SPD\nmanifold. The efficacy of our proposed framework is demonstrated both on\nsynthetic data and several computer vision tasks including object recognition,\ntexture classification, person re-identification and traffic scene\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 02:57:35 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 08:17:10 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Zhao", "Kun", ""], ["Wiliem", "Arnold", ""], ["Chen", "Shaokang", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1806.05361", "submitter": "Yu-Xiao Guo", "authors": "Yu-Xiao Guo and Xin Tong", "title": "View-volume Network for Semantic Scene Completion from a Single Depth\n  Image", "comments": "To appear in IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a View-Volume convolutional neural network (VVNet) for inferring\nthe occupancy and semantic labels of a volumetric 3D scene from a single depth\nimage. The VVNet concatenates a 2D view CNN and a 3D volume CNN with a\ndifferentiable projection layer. Given a single RGBD image, our method extracts\nthe detailed geometric features from the input depth image with a 2D view CNN\nand then projects the features into a 3D volume according to the input depth\nmap via a projection layer. After that, we learn the 3D context information of\nthe scene with a 3D volume CNN for computing the result volumetric occupancy\nand semantic labels. With combined 2D and 3D representations, the VVNet\nefficiently reduces the computational cost, enables feature extraction from\nmulti-channel high resolution inputs, and thus significantly improves the\nresult accuracy. We validate our method and demonstrate its efficiency and\neffectiveness on both synthetic SUNCG and real NYU dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 04:42:05 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Guo", "Yu-Xiao", ""], ["Tong", "Xin", ""]]}, {"id": "1806.05363", "submitter": "Yeng Liong Wong", "authors": "Hengfui Liau, Nimmagadda Yamini and YengLiong Wong", "title": "Fire SSD: Wide Fire Modules based Single Shot Detector on Edge Device", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of edge computing, there is an increasing need for running\nconvolutional neural network based object detection on small form factor edge\ncomputing devices with limited compute and thermal budget for applications such\nas video surveillance. To address this problem, efficient object detection\nframeworks such as YOLO and SSD were proposed. However, SSD based object\ndetection that uses VGG16 as backend network is insufficient to achieve real\ntime speed on edge devices. To further improve the detection speed, the backend\nnetwork is replaced by more efficient networks such as SqueezeNet and\nMobileNet. Although the speed is greatly improved, it comes with a price of\nlower accuracy. In this paper, we propose an efficient SSD named Fire SSD. Fire\nSSD achieves 70.7mAP on Pascal VOC 2007 test set. Fire SSD achieves the speed\nof 30.6FPS on low power mainstream CPU and is about 6 times faster than SSD300\nand has about 4 times smaller model size. Fire SSD also achieves 22.2FPS on\nintegrated GPU.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 04:56:41 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 06:22:59 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 09:13:37 GMT"}, {"version": "v4", "created": "Wed, 17 Oct 2018 06:26:57 GMT"}, {"version": "v5", "created": "Tue, 11 Dec 2018 03:14:12 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Liau", "Hengfui", ""], ["Yamini", "Nimmagadda", ""], ["Wong", "YengLiong", ""]]}, {"id": "1806.05372", "submitter": "Ming Sun", "authors": "Ming Sun, Yuchen Yuan, Feng Zhou, Errui Ding", "title": "Multi-Attention Multi-Class Constraint for Fine-grained Image\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based learning for fine-grained image recognition remains a\nchallenging task, where most of the existing methods treat each object part in\nisolation, while neglecting the correlations among them. In addition, the\nmulti-stage or multi-scale mechanisms involved make the existing methods less\nefficient and hard to be trained end-to-end. In this paper, we propose a novel\nattention-based convolutional neural network (CNN) which regulates multiple\nobject parts among different input images. Our method first learns multiple\nattention region features of each input image through the one-squeeze\nmulti-excitation (OSME) module, and then apply the multi-attention multi-class\nconstraint (MAMC) in a metric learning framework. For each anchor feature, the\nMAMC functions by pulling same-attention same-class features closer, while\npushing different-attention or different-class features away. Our method can be\neasily trained end-to-end, and is highly efficient which requires only one\ntraining stage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog\nspecies dataset that surpasses similar existing datasets by category coverage,\ndata volume and annotation quality. This dataset will be released upon\nacceptance to facilitate the research of fine-grained image recognition.\nExtensive experiments are conducted to show the substantial improvements of our\nmethod on four benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 05:45:22 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Sun", "Ming", ""], ["Yuan", "Yuchen", ""], ["Zhou", "Feng", ""], ["Ding", "Errui", ""]]}, {"id": "1806.05376", "submitter": "Xuaner (Cecilia) Zhang", "authors": "Xuaner Zhang, Ren Ng, Qifeng Chen", "title": "Single Image Reflection Separation with Perceptual Losses", "comments": "9 pages, 8 figures, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to separating reflection from a single image. The\napproach uses a fully convolutional network trained end-to-end with losses that\nexploit low-level and high-level image information. Our loss function includes\ntwo perceptual losses: a feature loss from a visual perception network, and an\nadversarial loss that encodes characteristics of images in the transmission\nlayers. We also propose a novel exclusion loss that enforces pixel-level layer\nseparation. We create a dataset of real-world images with reflection and\ncorresponding ground-truth transmission layers for quantitative evaluation and\nmodel training. We validate our method through comprehensive quantitative\nexperiments and show that our approach outperforms state-of-the-art reflection\nremoval methods in PSNR, SSIM, and perceptual user study. We also extend our\nmethod to two other image enhancement tasks to demonstrate the generality of\nour approach.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 06:03:03 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Zhang", "Xuaner", ""], ["Ng", "Ren", ""], ["Chen", "Qifeng", ""]]}, {"id": "1806.05382", "submitter": "Kohei Yamamoto", "authors": "Kohei Yamamoto, Kurato Maeno", "title": "PCAS: Pruning Channels with Attention Statistics for Deep Network\n  Compression", "comments": "Accepted at BMVC 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression techniques for deep neural networks are important for\nimplementing them on small embedded devices. In particular, channel-pruning is\na useful technique for realizing compact networks. However, many conventional\nmethods require manual setting of compression ratios in each layer. It is\ndifficult to analyze the relationships between all layers, especially for\ndeeper models. To address these issues, we propose a simple channel-pruning\ntechnique based on attention statistics that enables to evaluate the importance\nof channels. We improved the method by means of a criterion for automatic\nchannel selection, using a single compression ratio for the entire model in\nplace of per-layer model analysis. The proposed approach achieved superior\nperformance over conventional methods with respect to accuracy and the\ncomputational costs for various models and datasets. We provide analysis\nresults for behavior of the proposed criterion on different datasets to\ndemonstrate its favorable properties for channel pruning.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 06:28:59 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 03:22:02 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 06:58:29 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Yamamoto", "Kohei", ""], ["Maeno", "Kurato", ""]]}, {"id": "1806.05421", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Marcus Rohrbach and Tinne Tuytelaars", "title": "Selfless Sequential Learning", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential learning, also called lifelong learning, studies the problem of\nlearning tasks in a sequence with access restricted to only the data of the\ncurrent task. In this paper we look at a scenario with fixed model capacity,\nand postulate that the learning process should not be selfish, i.e. it should\naccount for future tasks to be added and thus leave enough capacity for them.\nTo achieve Selfless Sequential Learning we study different regularization\nstrategies and activation functions. We find that imposing sparsity at the\nlevel of the representation (i.e.~neuron activations) is more beneficial for\nsequential learning than encouraging parameter sparsity. In particular, we\npropose a novel regularizer, that encourages representation sparsity by means\nof neural inhibition. It results in few active neurons which in turn leaves\nmore free neurons to be utilized by upcoming tasks. As neural inhibition over\nan entire layer can be too drastic, especially for complex tasks requiring\nstrong representations, our regularizer only inhibits other neurons in a local\nneighbourhood, inspired by lateral inhibition processes in the brain. We\ncombine our novel regularizer, with state-of-the-art lifelong learning methods\nthat penalize changes to important previously learned parts of the network. We\nshow that our new regularizer leads to increased sparsity which translates in\nconsistent performance improvement %over alternative regularizers we studied on\ndiverse datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 09:06:10 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 07:23:59 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 13:58:26 GMT"}, {"version": "v4", "created": "Sun, 16 Dec 2018 02:13:23 GMT"}, {"version": "v5", "created": "Fri, 12 Apr 2019 14:47:02 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Rohrbach", "Marcus", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1806.05452", "submitter": "Xiaoran Chen", "authors": "Xiaoran Chen, Nick Pawlowski, Martin Rajchl, Ben Glocker, Ender\n  Konukoglu", "title": "Deep Generative Models in the Real-World: An Open Challenge from Medical\n  Imaging", "comments": "10 pages. 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning led to novel generative modeling techniques\nthat achieve unprecedented quality in generated samples and performance in\nlearning complex distributions in imaging data. These new models in medical\nimage computing have important applications that form clinically relevant and\nvery challenging unsupervised learning problems. In this paper, we explore the\nfeasibility of using state-of-the-art auto-encoder-based deep generative\nmodels, such as variational and adversarial auto-encoders, for one such task:\nabnormality detection in medical imaging. We utilize typical, publicly\navailable datasets with brain scans from healthy subjects and patients with\nstroke lesions and brain tumors. We use the data from healthy subjects to train\ndifferent auto-encoder based models to learn the distribution of healthy images\nand detect pathologies as outliers. Models that can better learn the data\ndistribution should be able to detect outliers more accurately. We evaluate the\ndetection performance of deep generative models and compare them with non-deep\nlearning based approaches to provide a benchmark of the current state of\nresearch. We conclude that abnormality detection is a challenging task for deep\ngenerative models and large room exists for improvement. In order to facilitate\nfurther research, we aim to provide carefully pre-processed imaging data\navailable to the research community.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 10:23:54 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Chen", "Xiaoran", ""], ["Pawlowski", "Nick", ""], ["Rajchl", "Martin", ""], ["Glocker", "Ben", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1806.05455", "submitter": "Frank Glavin", "authors": "Frank G. Glavin and Michael G. Madden", "title": "Analysis of the Effect of Unexpected Outliers in the Classification of\n  Spectroscopy Data", "comments": "Irish Conference on Artificial Intelligence and Cognitive Science\n  (2009)", "journal-ref": "Glavin, Frank G., and Michael G. Madden. \"Analysis of the effect\n  of unexpected outliers in the classification of spectroscopy data.\"\n  Artificial Intelligence and Cognitive Science (AICS), pp. 124-133. Springer,\n  Berlin, Heidelberg, 2009", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-class classification algorithms are very widely used, but we argue that\nthey are not always ideal from a theoretical perspective, because they assume\nall classes are characterized by the data, whereas in many applications,\ntraining data for some classes may be entirely absent, rare, or statistically\nunrepresentative. We evaluate one-sided classifiers as an alternative, since\nthey assume that only one class (the target) is well characterized. We consider\na task of identifying whether a substance contains a chlorinated solvent, based\non its chemical spectrum. For this application, it is not really feasible to\ncollect a statistically representative set of outliers, since that group may\ncontain \\emph{anything} apart from the target chlorinated solvents. Using a new\none-sided classification toolkit, we compare a One-Sided k-NN algorithm with\ntwo well-known binary classification algorithms, and conclude that the\none-sided classifier is more robust to unexpected outliers.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 10:44:03 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Glavin", "Frank G.", ""], ["Madden", "Michael G.", ""]]}, {"id": "1806.05473", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra, Behzad Bozorgtabar, Jean-Philippe Thiran,\n  Mauricio Reyes", "title": "Efficient Active Learning for Image Classification and Segmentation\n  using a Sample Selection and Conditional Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training robust deep learning (DL) systems for medical image classification\nor segmentation is challenging due to limited images covering different disease\ntypes and severity. We propose an active learning (AL) framework to select most\ninformative samples and add to the training data. We use conditional generative\nadversarial networks (cGANs) to generate realistic chest xray images with\ndifferent disease characteristics by conditioning its generation on a real\nimage sample. Informative samples to add to the training set are identified\nusing a Bayesian neural network. Experiments show our proposed AL framework is\nable to achieve state of the art performance by using about 35% of the full\ndataset, thus saving significant time and effort over conventional methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 11:29:10 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 11:36:58 GMT"}, {"version": "v3", "created": "Fri, 22 Jun 2018 08:32:12 GMT"}, {"version": "v4", "created": "Tue, 22 Oct 2019 15:17:27 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Mahapatra", "Dwarikanath", ""], ["Bozorgtabar", "Behzad", ""], ["Thiran", "Jean-Philippe", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1806.05476", "submitter": "Rodrigo Berriel", "authors": "Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue,\n  Alberto F. de Souza, Thiago Oliveira-Santos", "title": "Copycat CNN: Stealing Knowledge by Persuading Confession with Random\n  Non-Labeled Data", "comments": "8 pages, 3 figures, accepted by IJCNN 2018", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489592", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, Convolutional Neural Networks (CNNs) have been\nachieving state-of-the-art performance on a variety of problems. Many companies\nemploy resources and money to generate these models and provide them as an API,\ntherefore it is in their best interest to protect them, i.e., to avoid that\nsomeone else copies them. Recent studies revealed that state-of-the-art CNNs\nare vulnerable to adversarial examples attacks, and this weakness indicates\nthat CNNs do not need to operate in the problem domain (PD). Therefore, we\nhypothesize that they also do not need to be trained with examples of the PD in\norder to operate in it.\n  Given these facts, in this paper, we investigate if a target black-box CNN\ncan be copied by persuading it to confess its knowledge through random\nnon-labeled data. The copy is two-fold: i) the target network is queried with\nrandom data and its predictions are used to create a fake dataset with the\nknowledge of the network; and ii) a copycat network is trained with the fake\ndataset and should be able to achieve similar performance as the target\nnetwork.\n  This hypothesis was evaluated locally in three problems (facial expression,\nobject, and crosswalk classification) and against a cloud-based API. In the\ncopy attacks, images from both non-problem domain and PD were used. All copycat\nnetworks achieved at least 93.7% of the performance of the original models with\nnon-problem domain data, and at least 98.6% using additional data from the PD.\nAdditionally, the copycat CNN successfully copied at least 97.3% of the\nperformance of the Microsoft Azure Emotion API. Our results show that it is\npossible to create a copycat CNN by simply querying a target network as\nblack-box with random non-labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 11:32:27 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Correia-Silva", "Jacson Rodrigues", ""], ["Berriel", "Rodrigo F.", ""], ["Badue", "Claudine", ""], ["de Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "1806.05502", "submitter": "Fabian B. Fuchs Mr", "authors": "Fabian B. Fuchs, Oliver Groth, Adam R. Kosiorek, Alex Bewley, Markus\n  Wulfmeier, Andrea Vedaldi, Ingmar Posner", "title": "Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually predicting the stability of block towers is a popular task in the\ndomain of intuitive physics. While previous work focusses on prediction\naccuracy, a one-dimensional performance measure, we provide a broader analysis\nof the learned physical understanding of the final model and how the learning\nprocess can be guided. To this end, we introduce neural stethoscopes as a\ngeneral purpose framework for quantifying the degree of importance of specific\nfactors of influence in deep neural networks as well as for actively promoting\nand suppressing information as appropriate. In doing so, we unify concepts from\nmultitask learning as well as training with auxiliary and adversarial losses.\nWe apply neural stethoscopes to analyse the state-of-the-art neural network for\nstability prediction. We show that the baseline model is susceptible to being\nmisled by incorrect visual cues. This leads to a performance breakdown to the\nlevel of random guessing when training on scenarios where visual cues are\ninversely correlated with stability. Using stethoscopes to promote meaningful\nfeature extraction increases performance from 51% to 90% prediction accuracy.\nConversely, training on an easy dataset where visual cues are positively\ncorrelated with stability, the baseline model learns a bias leading to poor\nperformance on a harder dataset. Using an adversarial stethoscope, the network\nis successfully de-biased, leading to a performance increase from 66% to 88%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:35:50 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 09:51:18 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 19:31:50 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 16:32:21 GMT"}, {"version": "v5", "created": "Fri, 6 Sep 2019 13:49:37 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Fuchs", "Fabian B.", ""], ["Groth", "Oliver", ""], ["Kosiorek", "Adam R.", ""], ["Bewley", "Alex", ""], ["Wulfmeier", "Markus", ""], ["Vedaldi", "Andrea", ""], ["Posner", "Ingmar", ""]]}, {"id": "1806.05506", "submitter": "Mantang Guo", "authors": "Mantang Guo, Hao Zhu, Guoqing Zhou, Qing Wang", "title": "Dense Light Field Reconstruction From Sparse Sampling Using Residual\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A light field records numerous light rays from a real-world scene. However,\ncapturing a dense light field by existing devices is a time-consuming process.\nBesides, reconstructing a large amount of light rays equivalent to multiple\nlight fields using sparse sampling arises a severe challenge for existing\nmethods. In this paper, we present a learning based method to reconstruct\nmultiple novel light fields between two mutually independent light fields. We\nindicate that light rays distributed in different light fields have the same\nconsistent constraints under a certain condition. The most significant\nconstraint is a depth related correlation between angular and spatial\ndimensions. Our method avoids working out the error-sensitive constraint by\nemploying a deep neural network. We solve residual values of pixels on epipolar\nplane image (EPI) to reconstruct novel light fields. Our method is able to\nreconstruct 2 to 4 novel light fields between two mutually independent input\nlight fields. We also compare our results with those yielded by a number of\nalternatives elsewhere in the literature, which shows our reconstructed light\nfields have better structure similarity and occlusion relationship.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:45:55 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 11:34:57 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Guo", "Mantang", ""], ["Zhu", "Hao", ""], ["Zhou", "Guoqing", ""], ["Wang", "Qing", ""]]}, {"id": "1806.05510", "submitter": "Marco Ciccone", "authors": "Francesco Lattari, Marco Ciccone, Matteo Matteucci, Jonathan Masci,\n  Francesco Visin", "title": "ReConvNet: Video Object Segmentation with Spatio-Temporal Features\n  Modulation", "comments": "CVPR Workshop - DAVIS Challenge 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ReConvNet, a recurrent convolutional architecture for\nsemi-supervised video object segmentation that is able to fast adapt its\nfeatures to focus on any specific object of interest at inference time.\nGeneralization to new objects never observed during training is known to be a\nhard task for supervised approaches that would need to be retrained. To tackle\nthis problem, we propose a more efficient solution that learns spatio-temporal\nfeatures self-adapting to the object of interest via conditional affine\ntransformations. This approach is simple, can be trained end-to-end and does\nnot necessarily require extra training steps at inference time. Our method\nshows competitive results on DAVIS2016 with respect to state-of-the art\napproaches that use online fine-tuning, and outperforms them on DAVIS2017.\nReConvNet shows also promising results on the DAVIS-Challenge 2018 winning the\n$10$-th position.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:52:28 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 13:57:58 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Lattari", "Francesco", ""], ["Ciccone", "Marco", ""], ["Matteucci", "Matteo", ""], ["Masci", "Jonathan", ""], ["Visin", "Francesco", ""]]}, {"id": "1806.05512", "submitter": "Alexander Wong", "authors": "Alexander Wong", "title": "NetScore: Towards Universal Metrics for Large-scale Performance Analysis\n  of Deep Neural Networks for Practical On-Device Edge Usage", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the focus in the design of deep neural networks has been on improving\naccuracy, leading to more powerful yet highly complex network architectures\nthat are difficult to deploy in practical scenarios, particularly on edge\ndevices such as mobile and other consumer devices given their high\ncomputational and memory requirements. As a result, there has been a recent\ninterest in the design of quantitative metrics for evaluating deep neural\nnetworks that accounts for more than just model accuracy as the sole indicator\nof network performance. In this study, we continue the conversation towards\nuniversal metrics for evaluating the performance of deep neural networks for\npractical on-device edge usage. In particular, we propose a new balanced metric\ncalled NetScore, which is designed specifically to provide a quantitative\nassessment of the balance between accuracy, computational complexity, and\nnetwork architecture complexity of a deep neural network, which is important\nfor on-device edge operation. In what is one of the largest comparative\nanalysis between deep neural networks in literature, the NetScore metric, the\ntop-1 accuracy metric, and the popular information density metric were compared\nacross a diverse set of 60 different deep convolutional neural networks for\nimage classification on the ImageNet Large Scale Visual Recognition Challenge\n(ILSVRC 2012) dataset. The evaluation results across these three metrics for\nthis diverse set of networks are presented in this study to act as a reference\nguide for practitioners in the field. The proposed NetScore metric, along with\nthe other tested metrics, are by no means perfect, but the hope is to push the\nconversation towards better universal metrics for evaluating deep neural\nnetworks for use in practical on-device edge scenarios to help guide\npractitioners in model design for such scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:55:35 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 01:33:02 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Wong", "Alexander", ""]]}, {"id": "1806.05525", "submitter": "Mohsen Ghafoorian", "authors": "Mohsen Ghafoorian, Cedric Nugteren, N\\'ora Baka, Olaf Booij, Michael\n  Hofmann", "title": "EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane\n  Detection", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have been successfully applied to semantic\nsegmentation problems. However, there are many problems that are inherently not\npixel-wise classification problems but are nevertheless frequently formulated\nas semantic segmentation. This ill-posed formulation consequently necessitates\nhand-crafted scenario-specific and computationally expensive post-processing\nmethods to convert the per pixel probability maps to final desired outputs.\nGenerative adversarial networks (GANs) can be used to make the semantic\nsegmentation network output to be more realistic or better\nstructure-preserving, decreasing the dependency on potentially complex\npost-processing. In this work, we propose EL-GAN: a GAN framework to mitigate\nthe discussed problem using an embedding loss. With EL-GAN, we discriminate\nbased on learned embeddings of both the labels and the prediction at the same\ntime. This results in more stable training due to having better discriminative\ninformation, benefiting from seeing both `fake' and `real' predictions at the\nsame time. This substantially stabilizes the adversarial training process. We\nuse the TuSimple lane marking challenge to demonstrate that with our proposed\nframework it is viable to overcome the inherent anomalies of posing it as a\nsemantic segmentation problem. Not only is the output considerably more similar\nto the labels when compared to conventional methods, the subsequent\npost-processing is also simpler and crosses the competitive 96% accuracy\nthreshold.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 13:20:20 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 16:54:27 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Ghafoorian", "Mohsen", ""], ["Nugteren", "Cedric", ""], ["Baka", "N\u00f3ra", ""], ["Booij", "Olaf", ""], ["Hofmann", "Michael", ""]]}, {"id": "1806.05530", "submitter": "Yuqi Han", "authors": "Yuqi Han, Jinghong Nan, Zengshuo Zhang, Jingjing Wang and Baojun Zhao", "title": "Correlation Tracking via Robust Region Proposals", "comments": "4 pages, 3 figures, IET2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, correlation filter-based trackers have received extensive attention\ndue to their simplicity and superior speed. However, such trackers perform\npoorly when the target undergoes occlusion, viewpoint change or other\nchallenging attributes due to pre-defined sampling strategy. To tackle these\nissues, in this paper, we propose an adaptive region proposal scheme to\nfacilitate visual tracking. To be more specific, a novel tracking monitoring\nindicator is advocated to forecast tracking failure. Afterwards, we incorporate\ndetection and scale proposals respectively, to recover from model drift as well\nas handle aspect ratio variation. We test the proposed algorithm on several\nchallenging sequences, which have demonstrated that the proposed tracker\nperforms favourably against state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 13:30:30 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Han", "Yuqi", ""], ["Nan", "Jinghong", ""], ["Zhang", "Zengshuo", ""], ["Wang", "Jingjing", ""], ["Zhao", "Baojun", ""]]}, {"id": "1806.05569", "submitter": "Wufeng Xue", "authors": "Wufeng Xue, Gary Brahm, Stephanie Leung, Ogla Shmuilovich and Shuo Li", "title": "Cardiac Motion Scoring with Segment- and Subject-level Non-Local\n  Modeling", "comments": "MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion scoring of cardiac myocardium is of paramount importance for early\ndetection and diagnosis of various cardiac disease. It aims at identifying\nregional wall motions into one of the four types including normal, hypokinetic,\nakinetic, and dyskinetic, and is extremely challenging due to the complex\nmyocardium deformation and subtle inter-class difference of motion patterns.\nAll existing work on automated motion analysis are focused on binary\nabnormality detection to avoid the much more demanding motion scoring, which is\nurgently required in real clinical practice yet has never been investigated\nbefore. In this work, we propose Cardiac-MOS, the first powerful method for\ncardiac motion scoring from cardiac MR sequences based on deep convolution\nneural network. Due to the locality of convolution, the relationship between\ndistant non-local responses of the feature map cannot be explored, which is\nclosely related to motion difference between segments. In Cardiac-MOS, such\nnon-local relationship is modeled with non-local neural network within each\nsegment and across all segments of one subject, i.e., segment- and\nsubject-level non-local modeling, and lead to obvious performance improvement.\nBesides, Cardiac-MOS can effectively extract motion information from MR\nsequences of various lengths by interpolating the convolution kernel along the\ntemporal dimension, therefore can be applied to MR sequences of multiple\nsources. Experiments on 1440 myocardium segments of 90 subjects from short axis\nMR sequences of multiple lengths prove that Cardiac-MOS achieves reliable\nperformance, with correlation of 0.926 for motion score index estimation and\naccuracy of 77.4\\% for motion scoring. Cardiac-MOS also outperforms all\nexisting work for binary abnormality detection. As the first automatic motion\nscoring solution, Cardiac-MOS demonstrates great potential in future clinical\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:20:59 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Xue", "Wufeng", ""], ["Brahm", "Gary", ""], ["Leung", "Stephanie", ""], ["Shmuilovich", "Ogla", ""], ["Li", "Shuo", ""]]}, {"id": "1806.05570", "submitter": "Shumao Pang", "authors": "Shumao Pang, Stephanie Leung, Ilanit Ben Nachum, Qianjin Feng, Shuo Li", "title": "Direct Automated Quantitative Measurement of Spine via Cascade Amplifier\n  Regression Network", "comments": "Accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated quantitative measurement of the spine (i.e., multiple indices\nestimation of heights, widths, areas, and so on for the vertebral body and\ndisc) is of the utmost importance in clinical spinal disease diagnoses, such as\nosteoporosis, intervertebral disc degeneration, and lumbar disc herniation, yet\nstill an unprecedented challenge due to the variety of spine structure and the\nhigh dimensionality of indices to be estimated. In this paper, we propose a\nnovel cascade amplifier regression network (CARN), which includes the CARN\narchitecture and local shape-constrained manifold regularization (LSCMR) loss\nfunction, to achieve accurate direct automated multiple indices estimation. The\nCARN architecture is composed of a cascade amplifier network (CAN) for\nexpressive feature embedding and a linear regression model for multiple indices\nestimation. The CAN consists of cascade amplifier units (AUs), which are used\nfor selective feature reuse by stimulating effective feature and suppressing\nredundant feature during propagating feature map between adjacent layers, thus\nan expressive feature embedding is obtained. During training, the LSCMR is\nutilized to alleviate overfitting and generate realistic estimation by learning\nthe multiple indices distribution. Experiments on MR images of 195 subjects\nshow that the proposed CARN achieves impressive performance with mean absolute\nerrors of 1.2496 mm, 1.2887 mm, and 1.2692 mm for estimation of 15 heights of\ndiscs, 15 heights of vertebral bodies, and total indices respectively. The\nproposed method has great potential in clinical spinal disease diagnoses.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:21:01 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Pang", "Shumao", ""], ["Leung", "Stephanie", ""], ["Nachum", "Ilanit Ben", ""], ["Feng", "Qianjin", ""], ["Li", "Shuo", ""]]}, {"id": "1806.05573", "submitter": "Armine Vardazaryan", "authors": "Armine Vardazaryan, Didier Mutter, Jacques Marescaux, Nicolas Padoy", "title": "Weakly-Supervised Learning for Tool Localization in Laparoscopic Videos", "comments": "MICCAI LABELS 2018. Supplementary video: https://youtu.be/7VWVY04Z0MA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical tool localization is an essential task for the automatic analysis of\nendoscopic videos. In the literature, existing methods for tool localization,\ntracking and segmentation require training data that is fully annotated,\nthereby limiting the size of the datasets that can be used and the\ngeneralization of the approaches. In this work, we propose to circumvent the\nlack of annotated data with weak supervision. We propose a deep architecture,\ntrained solely on image level annotations, that can be used for both tool\npresence detection and localization in surgical videos. Our architecture relies\non a fully convolutional neural network, trained end-to-end, enabling us to\nlocalize surgical tools without explicit spatial annotations. We demonstrate\nthe benefits of our approach on a large public dataset, Cholec80, which is\nfully annotated with binary tool presence information and of which 5 videos\nhave been fully annotated with bounding boxes and tool centers for the\nevaluation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:27:12 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 08:21:13 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Vardazaryan", "Armine", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1806.05580", "submitter": "Jakob Wasserthal", "authors": "Jakob Wasserthal, Peter F. Neher, Klaus H. Maier-Hein", "title": "Tract orientation mapping for bundle-specific tractography", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the major white matter tracts are of great interest to numerous studies\nin neuroscience and medicine, their manual dissection in larger cohorts from\ndiffusion MRI tractograms is time-consuming, requires expert knowledge and is\nhard to reproduce. Tract orientation mapping (TOM) is a novel concept that\nfacilitates bundle-specific tractography based on a learned mapping from the\noriginal fiber orientation distribution function (fODF) peaks to a list of\ntract orientation maps (also abbr. TOM). Each TOM represents one of the known\ntracts with each voxel containing no more than one orientation vector. TOMs can\nact as a prior or even as direct input for tractography. We use an\nencoder-decoder fully-convolutional neural network architecture to learn the\nrequired mapping. In comparison to previous concepts for the reconstruction of\nspecific bundles, the presented one avoids various cumbersome processing steps\nlike whole brain tractography, atlas registration or clustering. We compare it\nto four state of the art bundle recognition methods on 20 different bundles in\na total of 105 subjects from the Human Connectome Project. Results are\nanatomically convincing even for difficult tracts, while reaching low angular\nerrors, unprecedented runtimes and top accuracy values (Dice). Our code and our\ndata are openly available.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:38:06 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Wasserthal", "Jakob", ""], ["Neher", "Peter F.", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1806.05594", "submitter": "Ben Athiwaratkun", "authors": "Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson", "title": "There Are Many Consistent Explanations of Unlabeled Data: Why You Should\n  Average", "comments": "Appears at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presently the most successful approaches to semi-supervised learning are\nbased on consistency regularization, whereby a model is trained to be robust to\nsmall perturbations of its inputs and parameters. To understand consistency\nregularization, we conceptually explore how loss geometry interacts with\ntraining procedures. The consistency loss dramatically improves generalization\nperformance over supervised-only training; however, we show that SGD struggles\nto converge on the consistency loss and continues to make large steps that lead\nto changes in predictions on the test data. Motivated by these observations, we\npropose to train consistency-based methods with Stochastic Weight Averaging\n(SWA), a recent approach which averages weights along the trajectory of SGD\nwith a modified learning rate schedule. We also propose fast-SWA, which further\naccelerates convergence by averaging multiple points within each cycle of a\ncyclical learning rate schedule. With weight averaging, we achieve the best\nknown semi-supervised results on CIFAR-10 and CIFAR-100, over many different\nquantities of labeled training data. For example, we achieve 5.0% error on\nCIFAR-10 with only 4000 labels, compared to the previous best result in the\nliterature of 6.3%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:58:36 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 16:21:21 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 15:26:31 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Athiwaratkun", "Ben", ""], ["Finzi", "Marc", ""], ["Izmailov", "Pavel", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1806.05610", "submitter": "Hamed Shah-Hosseini", "authors": "Hamed Shah-Hosseini", "title": "From Self-ception to Image Self-ception: A method to represent an image\n  with its own approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A concept of defining images based on its own approximate ones is proposed\nhere, which is called 'Self-ception'. In this regard, an algorithm is proposed\nto implement the self-ception for images, which we call it 'Image Self-ception'\nsince we use it for images. We can control the accuracy of this self-ception\nrepresentation by deciding how many segments or regions we want to use for the\nrepresentation. Some self-ception images are included in the paper. The video\nversions of the proposed image self-ception algorithm in action are shown in a\nYouTube channel (find it by Googling image self-ception).\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 15:28:28 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Shah-Hosseini", "Hamed", ""]]}, {"id": "1806.05620", "submitter": "Berta Besc\\'os Torcal", "authors": "Berta Bescos, Jos\\'e M. F\\'acil, Javier Civera and Jos\\'e Neira", "title": "DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes", "comments": "This work has been accepted at IEEE Robotics and Automation Letters,\n  and will be presented at the IEEE Conference on Intelligent Robots and\n  Systems 2018", "journal-ref": "IEEE Robotics and Automation Letters ( Volume: 3, Issue: 4, Oct.\n  2018 )", "doi": "10.1109/LRA.2018.2860039", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of scene rigidity is typical in SLAM algorithms. Such a strong\nassumption limits the use of most visual SLAM systems in populated real-world\nenvironments, which are the target of several relevant applications like\nservice robotics or autonomous vehicles. In this paper we present DynaSLAM, a\nvisual SLAM system that, building over ORB-SLAM2 [1], adds the capabilities of\ndynamic object detection and background inpainting. DynaSLAM is robust in\ndynamic scenarios for monocular, stereo and RGB-D configurations. We are\ncapable of detecting the moving objects either by multi-view geometry, deep\nlearning or both. Having a static map of the scene allows inpainting the frame\nbackground that has been occluded by such dynamic objects. We evaluate our\nsystem in public monocular, stereo and RGB-D datasets. We study the impact of\nseveral accuracy/speed trade-offs to assess the limits of the proposed\nmethodology. DynaSLAM outperforms the accuracy of standard visual SLAM\nbaselines in highly dynamic scenarios. And it also estimates a map of the\nstatic parts of the scene, which is a must for long-term applications in\nreal-world environments.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 15:52:07 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 08:09:22 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Bescos", "Berta", ""], ["F\u00e1cil", "Jos\u00e9 M.", ""], ["Civera", "Javier", ""], ["Neira", "Jos\u00e9", ""]]}, {"id": "1806.05622", "submitter": "Joon Son Chung", "authors": "Joon Son Chung, Arsha Nagrani, Andrew Zisserman", "title": "VoxCeleb2: Deep Speaker Recognition", "comments": "To appear in Interspeech 2018. The audio-visual dataset can be\n  downloaded from http://www.robots.ox.ac.uk/~vgg/data/voxceleb2 .\n  1806.05622v2: minor fixes; 5 pages", "journal-ref": null, "doi": "10.21437/Interspeech.2018-1929", "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is speaker recognition under noisy and\nunconstrained conditions.\n  We make two key contributions. First, we introduce a very large-scale\naudio-visual speaker recognition dataset collected from open-source media.\nUsing a fully automated pipeline, we curate VoxCeleb2 which contains over a\nmillion utterances from over 6,000 speakers. This is several times larger than\nany publicly available speaker recognition dataset.\n  Second, we develop and compare Convolutional Neural Network (CNN) models and\ntraining strategies that can effectively recognise identities from voice under\nvarious conditions. The models trained on the VoxCeleb2 dataset surpass the\nperformance of previous works on a benchmark dataset by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 15:59:12 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 01:49:17 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chung", "Joon Son", ""], ["Nagrani", "Arsha", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1806.05645", "submitter": "Marc Tanti", "authors": "Hoa Trong Vu, Claudio Greco, Aliia Erofeeva, Somayeh Jafaritazehjan,\n  Guido Linders, Marc Tanti, Alberto Testoni, Raffaella Bernardi, Albert Gatt", "title": "Grounded Textual Entailment", "comments": "15 pages, 2 figures, 14 tables, 2 appendices. Accepted in COLING 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing semantic relations between sentences, such as entailment, is a\nlong-standing challenge for computational semantics. Logic-based models analyse\nentailment in terms of possible worlds (interpretations, or situations) where a\npremise P entails a hypothesis H iff in all worlds where P is true, H is also\ntrue. Statistical models view this relationship probabilistically, addressing\nit in terms of whether a human would likely infer H from P. In this paper, we\nwish to bridge these two perspectives, by arguing for a visually-grounded\nversion of the Textual Entailment task. Specifically, we ask whether models can\nperform better if, in addition to P and H, there is also an image\n(corresponding to the relevant \"world\" or \"situation\"). We use a multimodal\nversion of the SNLI dataset (Bowman et al., 2015) and we compare \"blind\" and\nvisually-augmented models of textual entailment. We show that visual\ninformation is beneficial, but we also conduct an in-depth error analysis that\nreveals that current multimodal models are not performing \"grounding\" in an\noptimal fashion.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 16:56:44 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Vu", "Hoa Trong", ""], ["Greco", "Claudio", ""], ["Erofeeva", "Aliia", ""], ["Jafaritazehjan", "Somayeh", ""], ["Linders", "Guido", ""], ["Tanti", "Marc", ""], ["Testoni", "Alberto", ""], ["Bernardi", "Raffaella", ""], ["Gatt", "Albert", ""]]}, {"id": "1806.05653", "submitter": "Amirhossein Dadashzadeh", "authors": "Amirhossein Dadashzadeh, Alireza Tavakoli Targhi, Maryam Tahmasbi,\n  Majid Mirmehdi", "title": "HGR-Net: A Fusion Network for Hand Gesture Segmentation and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage convolutional neural network (CNN) architecture for\nrobust recognition of hand gestures, called HGR-Net, where the first stage\nperforms accurate semantic segmentation to determine hand regions, and the\nsecond stage identifies the gesture. The segmentation stage architecture is\nbased on the combination of fully convolutional residual network and atrous\nspatial pyramid pooling. Although the segmentation sub-network is trained\nwithout depth information, it is particularly robust against challenges such as\nillumination variations and complex backgrounds. The recognition stage deploys\na two-stream CNN, which fuses the information from the red-green-blue and\nsegmented images by combining their deep representations in a fully connected\nlayer before classification. Extensive experiments on public datasets show that\nour architecture achieves almost as good as state-of-the-art performance in\nsegmentation and recognition of static hand gestures, at a fraction of training\ntime, run time, and model size. Our method can operate at an average of 23 ms\nper frame.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 17:15:16 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 20:29:36 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2019 17:43:46 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Dadashzadeh", "Amirhossein", ""], ["Targhi", "Alireza Tavakoli", ""], ["Tahmasbi", "Maryam", ""], ["Mirmehdi", "Majid", ""]]}, {"id": "1806.05660", "submitter": "\\'Angel Alexander Cabrera", "authors": "\\'Angel Alexander Cabrera, Fred Hohman, Jason Lin, Duen Horng Chau", "title": "Interactive Classification for Deep Learning Interpretation", "comments": "Presented as a demo at CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive system enabling users to manipulate images to\nexplore the robustness and sensitivity of deep learning image classifiers.\nUsing modern web technologies to run in-browser inference, users can remove\nimage features using inpainting algorithms and obtain new classifications in\nreal time, which allows them to ask a variety of \"what if\" questions by\nexperimentally modifying images and seeing how the model reacts. Our system\nallows users to compare and contrast what image regions humans and machine\nlearning models use for classification, revealing a wide range of surprising\nresults ranging from spectacular failures (e.g., a \"water bottle\" image becomes\na \"concert\" when removing a person) to impressive resilience (e.g., a \"baseball\nplayer\" image remains correctly classified even without a glove or base). We\ndemonstrate our system at The 2018 Conference on Computer Vision and Pattern\nRecognition (CVPR) for the audience to try it live. Our system is open-sourced\nat https://github.com/poloclub/interactive-classification. A video demo is\navailable at https://youtu.be/llub5GcOF6w.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 17:36:02 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 20:57:55 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Cabrera", "\u00c1ngel Alexander", ""], ["Hohman", "Fred", ""], ["Lin", "Jason", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1806.05662", "submitter": "Zhilin Yang", "authors": "Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen,\n  Ruslan Salakhutdinov, Yann LeCun", "title": "GLoMo: Unsupervisedly Learned Relational Graphs as Transferable\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep transfer learning approaches have mainly focused on learning\ngeneric feature vectors from one task that are transferable to other tasks,\nsuch as word embeddings in language and pretrained convolutional features in\nvision. However, these approaches usually transfer unary features and largely\nignore more structured graphical representations. This work explores the\npossibility of learning generic latent relational graphs that capture\ndependencies between pairs of data units (e.g., words or pixels) from\nlarge-scale unlabeled data and transferring the graphs to downstream tasks. Our\nproposed transfer learning framework improves performance on various tasks\nincluding question answering, natural language inference, sentiment analysis,\nand image classification. We also show that the learned graphs are generic\nenough to be transferred to different embeddings on which the graphs have not\nbeen trained (including GloVe embeddings, ELMo embeddings, and task-specific\nRNN hidden unit), or embedding-free units such as image pixels.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 17:41:19 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 04:36:08 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 20:24:33 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Yang", "Zhilin", ""], ["Zhao", "Jake", ""], ["Dhingra", "Bhuwan", ""], ["He", "Kaiming", ""], ["Cohen", "William W.", ""], ["Salakhutdinov", "Ruslan", ""], ["LeCun", "Yann", ""]]}, {"id": "1806.05666", "submitter": "Anurag Ranjan", "authors": "Anurag Ranjan and Javier Romero and Michael J. Black", "title": "Learning Human Optical Flow", "comments": "British Machine Vision Conference 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optical flow of humans is well known to be useful for the analysis of\nhuman action. Given this, we devise an optical flow algorithm specifically for\nhuman motion and show that it is superior to generic flow methods. Designing a\nmethod by hand is impractical, so we develop a new training database of image\nsequences with ground truth optical flow. For this we use a 3D model of the\nhuman body and motion capture data to synthesize realistic flow fields. We then\ntrain a convolutional neural network to estimate human flow fields from pairs\nof images. Since many applications in human motion analysis depend on speed,\nand we anticipate mobile applications, we base our method on SpyNet with\nseveral modifications. We demonstrate that our trained network is more accurate\nthan a wide range of top methods on held-out test data and that it generalizes\nwell to real image sequences. When combined with a person detector/tracker, the\napproach provides a full solution to the problem of 2D human flow estimation.\nBoth the code and the dataset are available for research.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 17:50:36 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 12:21:40 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ranjan", "Anurag", ""], ["Romero", "Javier", ""], ["Black", "Michael J.", ""]]}, {"id": "1806.05724", "submitter": "Xia Zhong", "authors": "Xia Zhong, Mario Amrehn, Nishant Ravikumar, Shuqing Chen, Norbert\n  Strobel, Annette Birkhold, Markus Kowarschik, Rebecca Fahrig, Andreas Maier", "title": "Action Learning for 3D Point Cloud Based Organ Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel point cloud based 3D organ segmentation pipeline utilizing\ndeep Q-learning. In order to preserve shape properties, the learning process is\nguided using a statistical shape model. The trained agent directly predicts\npiece-wise linear transformations for all vertices in each iteration. This\nmapping between the ideal transformation for an object outline estimation is\nlearned based on image features. To this end, we introduce aperture features\nthat extract gray values by sampling the 3D volume within the cone centered\naround the associated vertex and its normal vector. Our approach is also\ncapable of estimating a hierarchical pyramid of non rigid deformations for\nmulti-resolution meshes. In the application phase, we use a marginal approach\nto gradually estimate affine as well as non-rigid transformations. We performed\nextensive evaluations to highlight the robust performance of our approach on a\nvariety of challenge data as well as clinical data. Additionally, our method\nhas a run time ranging from 0.3 to 2.7 seconds to segment each organ. In\naddition, we show that the proposed method can be applied to different organs,\nX-ray based modalities, and scanning protocols without the need of transfer\nlearning. As we learn actions, even unseen reference meshes can be processed as\ndemonstrated in an example with the Visible Human. From this we conclude that\nour method is robust, and we believe that our method can be successfully\napplied to many more applications, in particular, in the interventional imaging\nspace.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 20:14:25 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Zhong", "Xia", ""], ["Amrehn", "Mario", ""], ["Ravikumar", "Nishant", ""], ["Chen", "Shuqing", ""], ["Strobel", "Norbert", ""], ["Birkhold", "Annette", ""], ["Kowarschik", "Markus", ""], ["Fahrig", "Rebecca", ""], ["Maier", "Andreas", ""]]}, {"id": "1806.05742", "submitter": "Dogucan Yaman", "authors": "Dogucan Yaman, Fevziye Irem Eyiokur, Nurdan Sezgin, Haz{\\i}m Kemal\n  Ekenel", "title": "Age and Gender Classification From Ear Images", "comments": "7 pages, 3 figures, accepted for IAPR/IEEE International Workshop on\n  Biometrics and Forensics (IWBF) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a detailed analysis on extracting soft biometric\ntraits, age and gender, from ear images. Although there have been a few\nprevious work on gender classification using ear images, to the best of our\nknowledge, this study is the first work on age classification from ear images.\nIn the study, we have utilized both geometric features and appearance-based\nfeatures for ear representation. The utilized geometric features are based on\neight anthropometric landmarks and consist of 14 distance measurements and two\narea calculations. The appearance-based methods employ deep convolutional\nneural networks for representation and classification. The well-known\nconvolutional neural network models, namely, AlexNet, VGG-16, GoogLeNet, and\nSqueezeNet have been adopted for the study. They have been fine-tuned on a\nlarge-scale ear dataset that has been built from the profile and\nclose-to-profile face images in the Multi-PIE face dataset. This way, we have\nperformed a domain adaptation. The updated models have been fine-tuned once\nmore time on the small-scale target ear dataset, which contains only around 270\near images for training. According to the experimental results,\nappearance-based methods have been found to be superior to the methods based on\ngeometric features. We have achieved 94\\% accuracy for gender classification,\nwhereas 52\\% accuracy has been obtained for age classification. These results\nindicate that ear images provide useful cues for age and gender classification,\nhowever, further work is required for age estimation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 21:00:09 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Yaman", "Dogucan", ""], ["Eyiokur", "Fevziye Irem", ""], ["Sezgin", "Nurdan", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "1806.05759", "submitter": "Ari Morcos", "authors": "Ari S. Morcos, Maithra Raghu, and Samy Bengio", "title": "Insights on representational similarity in neural networks with\n  canonical correlation", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing different neural network representations and determining how\nrepresentations evolve over time remain challenging open questions in our\nunderstanding of the function of neural networks. Comparing representations in\nneural networks is fundamentally difficult as the structure of representations\nvaries greatly, even across groups of networks trained on identical tasks, and\nover the course of training. Here, we develop projection weighted CCA\n(Canonical Correlation Analysis) as a tool for understanding neural networks,\nbuilding off of SVCCA, a recently proposed method (Raghu et al., 2017). We\nfirst improve the core method, showing how to differentiate between signal and\nnoise, and then apply this technique to compare across a group of CNNs,\ndemonstrating that networks which generalize converge to more similar\nrepresentations than networks which memorize, that wider networks converge to\nmore similar solutions than narrow networks, and that trained networks with\nidentical topology but different learning rates converge to distinct clusters\nwith diverse representations. We also investigate the representational dynamics\nof RNNs, across both training and sequential timesteps, finding that RNNs\nconverge in a bottom-up pattern over the course of training and that the hidden\nstate is highly variable over the course of a sequence, even when accounting\nfor linear transforms. Together, these results provide new insights into the\nfunction of CNNs and RNNs, and demonstrate the utility of using CCA to\nunderstand representations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 22:34:11 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 23:09:23 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 18:59:02 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Morcos", "Ari S.", ""], ["Raghu", "Maithra", ""], ["Bengio", "Samy", ""]]}, {"id": "1806.05764", "submitter": "Alice Lucas", "authors": "Alice Lucas, Santiago Lopez Tapia, Rafael Molina, Aggelos K.\n  Katsaggelos", "title": "Generative Adversarial Networks and Perceptual Losses for Video\n  Super-Resolution", "comments": "In the the review process IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2019.2895768", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR) has become one of the most critical problems in\nvideo processing. In the deep learning literature, recent works have shown the\nbenefits of using adversarial-based and perceptual losses to improve the\nperformance on various image restoration tasks; however, these have yet to be\napplied for video super-resolution. In this work, we propose a Generative\nAdversarial Network(GAN)-based formulation for VSR. We introduce a new\ngenerator network optimized for the VSR problem, named VSRResNet, along with a\nnew discriminator architecture to properly guide VSRResNet during the GAN\ntraining. We further enhance our VSR GAN formulation with two regularizers, a\ndistance loss in feature-space and pixel-space, to obtain our final\nVSRResFeatGAN model. We show that pre-training our generator with the\nMean-Squared-Error loss only quantitatively surpasses the current\nstate-of-the-art VSR models. Finally, we employ the PercepDist metric (Zhang et\nal., 2018) to compare state-of-the-art VSR models. We show that this metric\nmore accurately evaluates the perceptual quality of SR solutions obtained from\nneural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we\nshow that our proposed model, the VSRResFeatGAN model, outperforms current\nstate-of-the-art SR models, both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 23:14:14 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 15:11:45 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Lucas", "Alice", ""], ["Tapia", "Santiago Lopez", ""], ["Molina", "Rafael", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1806.05779", "submitter": "Michele Pratusevich", "authors": "Michele Pratusevich", "title": "Deep Learning Approximation: Zero-Shot Neural Network Speedup", "comments": "Submitted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks offer high-accuracy solutions to a range of problems, but are\ncostly to run in production systems because of computational and memory\nrequirements during a forward pass. Given a trained network, we propose a\ntechique called Deep Learning Approximation to build a faster network in a tiny\nfraction of the time required for training by only manipulating the network\nstructure and coefficients without requiring re-training or access to the\ntraining data. Speedup is achieved by by applying a sequential series of\nindependent optimizations that reduce the floating-point operations (FLOPs)\nrequired to perform a forward pass. First, lossless optimizations are applied,\nfollowed by lossy approximations using singular value decomposition (SVD) and\nlow-rank matrix decomposition. The optimal approximation is chosen by weighing\nthe relative accuracy loss and FLOP reduction according to a single parameter\nspecified by the user. On PASCAL VOC 2007 with the YOLO network, we show an\nend-to-end 2x speedup in a network forward pass with a 5% drop in mAP that can\nbe re-gained by finetuning.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 01:25:47 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Pratusevich", "Michele", ""]]}, {"id": "1806.05781", "submitter": "John See", "authors": "Yee-Hui Oh, John See, Anh Cat Le Ngo, Raphael Chung-Wei Phan, Vishnu\n  Monn Baskaran", "title": "A Survey of Automatic Facial Micro-expression Analysis: Databases,\n  Methods and Challenges", "comments": "45 pages, single column preprint version. Submitted: 2 December 2017,\n  Accepted: 12 June 2018 to Frontiers in Psychology", "journal-ref": null, "doi": "10.3389/fpsyg.2018.01128", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, automatic facial micro-expression analysis has\ngarnered increasing attention from experts across different disciplines because\nof its potential applications in various fields such as clinical diagnosis,\nforensic investigation and security systems. Advances in computer algorithms\nand video acquisition technology have rendered machine analysis of facial\nmicro-expressions possible today, in contrast to decades ago when it was\nprimarily the domain of psychiatrists where analysis was largely manual.\nIndeed, although the study of facial micro-expressions is a well-established\nfield in psychology, it is still relatively new from the computational\nperspective with many interesting problems. In this survey, we present a\ncomprehensive review of state-of-the-art databases and methods for\nmicro-expressions spotting and recognition. Individual stages involved in the\nautomation of these tasks are also described and reviewed at length. In\naddition, we also deliberate on the challenges and future directions in this\ngrowing field of automatic facial micro-expression analysis.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 01:42:33 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Oh", "Yee-Hui", ""], ["See", "John", ""], ["Ngo", "Anh Cat Le", ""], ["Phan", "Raphael Chung-Wei", ""], ["Baskaran", "Vishnu Monn", ""]]}, {"id": "1806.05789", "submitter": "Usman Roshan", "authors": "Yunzhe Xue and Usman Roshan", "title": "Image classification and retrieval with random depthwise signed\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a random convolutional neural network to generate a feature space\nin which we study image classification and retrieval performance. Put briefly\nwe apply random convolutional blocks followed by global average pooling to\ngenerate a new feature, and we repeat this k times to produce a k-dimensional\nfeature space. This can be interpreted as partitioning the space of image\npatches with random hyperplanes which we formalize as a random depthwise\nconvolutional neural network. In the network's final layer we perform image\nclassification and retrieval with the linear support vector machine and\nk-nearest neighbor classifiers and study other empirical properties. We show\nthat the ratio of image pixel distribution similarity across classes to within\nclasses is higher in our network's final layer compared to the input space.\nWhen we apply the linear support vector machine for image classification we see\nthat the accuracy is higher than if we were to train just the final layer of\nVGG16, ResNet18, and DenseNet40 with random weights. In the same setting we\ncompare it to an unsupervised feature learning method and find our accuracy to\nbe comparable on CIFAR10 but higher on CIFAR100 and STL10. We see that the\naccuracy is not far behind that of trained networks, particularly in the top-k\nsetting. For example the top-2 accuracy of our network is near 90% on both\nCIFAR10 and a 10-class mini ImageNet, and 85% on STL10. We find that k-nearest\nneighbor gives a comparable precision on the Corel Princeton Image Similarity\nBenchmark than if we were to use the final layer of trained networks. As with\nother networks we find that our network fails to a black box attack even though\nwe lack a gradient and use the sign activation. We highlight sensitivity of our\nnetwork to background as a potential pitfall and an advantage. Overall our work\npushes the boundary of what can be achieved with random weights.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 02:26:11 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 22:12:31 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 21:20:48 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Xue", "Yunzhe", ""], ["Roshan", "Usman", ""]]}, {"id": "1806.05793", "submitter": "John Ray Bergado", "authors": "John Ray Bergado and Claudio Persello and Alfred Stein", "title": "Recurrent Multiresolution Convolutional Networks for VHR Image\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2018.2837357", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Classification of very high resolution (VHR) satellite images has three major\nchallenges: 1) inherent low intra-class and high inter-class spectral\nsimilarities, 2) mismatching resolution of available bands, and 3) the need to\nregularize noisy classification maps. Conventional methods have addressed these\nchallenges by adopting separate stages of image fusion, feature extraction, and\npost-classification map regularization. These processing stages, however, are\nnot jointly optimizing the classification task at hand. In this study, we\npropose a single-stage framework embedding the processing stages in a recurrent\nmultiresolution convolutional network trained in an end-to-end manner. The\nfeedforward version of the network, called FuseNet, aims to match the\nresolution of the panchromatic and multispectral bands in a VHR image using\nconvolutional layers with corresponding downsampling and upsampling operations.\nContextual label information is incorporated into FuseNet by means of a\nrecurrent version called ReuseNet. We compared FuseNet and ReuseNet against the\nuse of separate processing steps for both image fusion, e.g. pansharpening and\nresampling through interpolation, and map regularization such as conditional\nrandom fields. We carried out our experiments on a land cover classification\ntask using a Worldview-03 image of Quezon City, Philippines and the ISPRS 2D\nsemantic labeling benchmark dataset of Vaihingen, Germany. FuseNet and ReuseNet\nsurpass the baseline approaches in both quantitative and qualitative results.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 03:01:43 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Bergado", "John Ray", ""], ["Persello", "Claudio", ""], ["Stein", "Alfred", ""]]}, {"id": "1806.05796", "submitter": "Ziheng Wang", "authors": "Ziheng Wang, Ann Majewicz Fey", "title": "Deep Learning with Convolutional Neural Network for Objective Skill\n  Evaluation in Robot-assisted Surgery", "comments": "Manuscript published. For reference, see\n  https://link.springer.com/article/10.1007/s11548-018-1860-1", "journal-ref": "2018 International Journal of Computer Assisted Radiology and\n  Surgery", "doi": "10.1007/s11548-018-1860-1", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of robot-assisted surgery, the role of data-driven approaches\nto integrate statistics and machine learning is growing rapidly with prominent\ninterests in objective surgical skill assessment. However, most existing work\nrequires translating robot motion kinematics into intermediate features or\ngesture segments that are expensive to extract, lack efficiency, and require\nsignificant domain-specific knowledge. We propose an analytical deep learning\nframework for skill assessment in surgical training. A deep convolutional\nneural network is implemented to map multivariate time series data of the\nmotion kinematics to individual skill levels. We perform experiments on the\npublic minimally invasive surgical robotic dataset, JHU-ISI Gesture and Skill\nAssessment Working Set (JIGSAWS). Our proposed learning model achieved a\ncompetitive accuracy of 92.5%, 95.4%, and 91.3%, in the standard training\ntasks: Suturing, Needle-passing, and Knot-tying, respectively. Without the need\nof engineered features or carefully-tuned gesture segmentation, our model can\nsuccessfully decode skill information from raw motion profiles via end-to-end\nlearning. Meanwhile, the proposed model is able to reliably interpret skills\nwithin 1-3 second window, without needing an observation of entire training\ntrial. This study highlights the potentials of deep architectures for an\nproficient online skill assessment in modern surgical training.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 03:22:06 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 06:25:35 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Wang", "Ziheng", ""], ["Fey", "Ann Majewicz", ""]]}, {"id": "1806.05798", "submitter": "Ziheng Wang", "authors": "Ziheng Wang, Ann Majewicz Fey", "title": "SATR-DL: Improving Surgical Skill Assessment and Task Recognition in\n  Robot-assisted Surgery with Deep Neural Networks", "comments": null, "journal-ref": "2018 40th Annual International Conference of the IEEE Engineering\n  in Medicine and Biology Society (EMBC)", "doi": "10.1109/EMBC.2018.8512575", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: This paper focuses on an automated analysis of surgical motion\nprofiles for objective skill assessment and task recognition in robot-assisted\nsurgery. Existing techniques heavily rely on conventional statistic measures or\nshallow modelings based on hand-engineered features and gesture segmentation.\nSuch developments require significant expert knowledge, are prone to errors,\nand are less efficient in online adaptive training systems. Methods: In this\nwork, we present an efficient analytic framework with a parallel deep learning\narchitecture, SATR-DL, to assess trainee expertise and recognize surgical\ntraining activity. Through an end-to-end learning technique, abstract\ninformation of spatial representations and temporal dynamics is jointly\nobtained directly from raw motion sequences. Results: By leveraging a shared\nhigh-level representation learning, the resulting model is successful in the\nrecognition of trainee skills and surgical tasks, suturing, needle-passing, and\nknot-tying. Meanwhile, we explore the use of ensemble in classification at the\ntrial level, where the SATR-DL outperforms state-of-the-art performance by\nachieving accuracies of 0.960 and 1.000 in skill assessment and task\nrecognition, respectively. Conclusion: This study highlights the potential of\nSATR-DL to provide improvements for an efficient data-driven assessment in\nintelligent robotic surgery.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 03:31:23 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Wang", "Ziheng", ""], ["Fey", "Ann Majewicz", ""]]}, {"id": "1806.05804", "submitter": "Vijetha Gattupalli", "authors": "Vijetha Gattupalli, Yaoxin Zhuo, Baoxin Li", "title": "Weakly Supervised Deep Image Hashing through Tag Embeddings", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches to semantic image hashing have been formulated as supervised\nlearning problems that utilize images and label information to learn the binary\nhash codes. However, large-scale labeled image data is expensive to obtain,\nthus imposing a restriction on the usage of such algorithms. On the other hand,\nunlabelled image data is abundant due to the existence of many Web image\nrepositories. Such Web images may often come with images tags that contain\nuseful information, although raw tags, in general, do not readily lead to\nsemantic labels. Motivated by this scenario, we formulate the problem of\nsemantic image hashing as a weakly-supervised learning problem. We utilize the\ninformation contained in the user-generated tags associated with the images to\nlearn the hash codes. More specifically, we extract the word2vec semantic\nembeddings of the tags and use the information contained in them for\nconstraining the learning. Accordingly, we name our model Weakly Supervised\nDeep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of\nsemantic image retrieval and is compared against several state-of-art models.\nResults show that our approach sets a new state-of-art in the area of weekly\nsupervised image hashing.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 05:24:30 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 05:20:17 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 02:53:56 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Gattupalli", "Vijetha", ""], ["Zhuo", "Yaoxin", ""], ["Li", "Baoxin", ""]]}, {"id": "1806.05810", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Samuel Rota Bul\\`o, Barbara Caputo, Elisa Ricci", "title": "Best sources forward: domain generalization through source-specific nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long standing problem in visual object categorization is the ability of\nalgorithms to generalize across different testing conditions. The problem has\nbeen formalized as a covariate shift among the probability distributions\ngenerating the training data (source) and the test data (target) and several\ndomain adaptation methods have been proposed to address this issue. While these\napproaches have considered the single source-single target scenario, it is\nplausible to have multiple sources and require adaptation to any possible\ntarget domain. This last scenario, named Domain Generalization (DG), is the\nfocus of our work. Differently from previous DG methods which learn domain\ninvariant representations from source data, we design a deep network with\nmultiple domain-specific classifiers, each associated to a source domain. At\ntest time we estimate the probabilities that a target sample belongs to each\nsource domain and exploit them to optimally fuse the classifiers predictions.\nTo further improve the generalization ability of our model, we also introduced\na domain agnostic component supporting the final classifier. Experiments on two\npublic benchmarks demonstrate the power of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 05:42:20 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Caputo", "Barbara", ""], ["Ricci", "Elisa", ""]]}, {"id": "1806.05824", "submitter": "Alexandre Benoit", "authors": "Amina Ben Hamida (LISTIC), A Benoit (LISTIC), Patrick Lambert\n  (LISTIC), Chokri Ben Amar (REGIM)", "title": "Three dimensional Deep Learning approach for remote sensing image\n  classification", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, Institute of\n  Electrical and Electronics Engineers, 2018, pp.1 - 15", "doi": "10.1109/TGRS.2018.2818945", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a variety of approaches has been enriching the field of Remote\nSensing (RS) image processing and analysis. Unfortunately, existing methods\nremain limited faced to the rich spatio-spectral content of today's large\ndatasets. It would seem intriguing to resort to Deep Learning (DL) based\napproaches at this stage with regards to their ability to offer accurate\nsemantic interpretation of the data. However, the specificity introduced by the\ncoexistence of spectral and spatial content in the RS datasets widens the scope\nof the challenges presented to adapt DL methods to these contexts. Therefore,\nthe aim of this paper is firstly to explore the performance of DL architectures\nfor the RS hyperspectral dataset classification and secondly to introduce a new\nthree-dimensional DL approach that enables a joint spectral and spatial\ninformation process. A set of three-dimensional schemes is proposed and\nevaluated. Experimental results based on well knownhyperspectral datasets\ndemonstrate that the proposed method is able to achieve a better classification\nrate than state of the art methods with lower computational costs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 06:35:47 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Hamida", "Amina Ben", "", "LISTIC"], ["Benoit", "A", "", "LISTIC"], ["Lambert", "Patrick", "", "LISTIC"], ["Amar", "Chokri Ben", "", "REGIM"]]}, {"id": "1806.05842", "submitter": "Maxime Ferrera", "authors": "Maxime Ferrera (LIRMM), Julien Moras, Pauline Trouv\\'e-Peloux, Vincent\n  Creuze (ICAR)", "title": "Real-time Monocular Visual Odometry for Turbid and Dynamic Underwater\n  Environments", "comments": null, "journal-ref": "Sensors, MDPI, 2019", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of robotic underwater operations, the visual degradations\ninduced by the medium properties make difficult the exclusive use of cameras\nfor localization purpose. Hence, most localization methods are based on\nexpensive navigational sensors associated with acoustic positioning. On the\nother hand, visual odometry and visual SLAM have been exhaustively studied for\naerial or terrestrial applications, but state-of-the-art algorithms fail\nunderwater. In this paper we tackle the problem of using a simple low-cost\ncamera for underwater localization and propose a new monocular visual odometry\nmethod dedicated to the underwater environment. We evaluate different tracking\nmethods and show that optical flow based tracking is more suited to underwater\nimages than classical approaches based on descriptors. We also propose a\nkeyframe-based visual odometry approach highly relying on nonlinear\noptimization. The proposed algorithm has been assessed on both simulated and\nreal underwater datasets and outperforms state-of-the-art visual SLAM methods\nunder many of the most challenging conditions. The main application of this\nwork is the localization of Remotely Operated Vehicles (ROVs) used for\nunderwater archaeological missions but the developed system can be used in any\nother applications as long as visual information is available.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 07:47:00 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 08:46:56 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 09:15:04 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Ferrera", "Maxime", "", "LIRMM"], ["Moras", "Julien", "", "ICAR"], ["Trouv\u00e9-Peloux", "Pauline", "", "ICAR"], ["Creuze", "Vincent", "", "ICAR"]]}, {"id": "1806.05882", "submitter": "Yang Yue", "authors": "Yang Yue, Liuyuan He, Gan He, Jian.K.Liu, Kai Du, Yonghong Tian,\n  Tiejun Huang", "title": "A simple blind-denoising filter inspired by electrically coupled\n  photoreceptors in the retina", "comments": "16 pages, 8 figures, 9 tables, Submitted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoreceptors in the retina are coupled by electrical synapses called \"gap\njunctions\". It has long been established that gap junctions increase the\nsignal-to-noise ratio of photoreceptors. Inspired by electrically coupled\nphotoreceptors, we introduced a simple filter, the PR-filter, with only one\nvariable. On BSD68 dataset, PR-filter showed outstanding performance in SSIM\nduring blind denoising tasks. It also significantly improved the performance of\nstate-of-the-art convolutional neural network blind denosing on non-Gaussian\nnoise. The performance of keeping more details might be attributed to small\nreceptive field of the photoreceptors.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 10:08:59 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 05:53:00 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 03:01:55 GMT"}, {"version": "v4", "created": "Mon, 27 Aug 2018 10:04:13 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Yue", "Yang", ""], ["He", "Liuyuan", ""], ["He", "Gan", ""], ["Liu", "Jian. K.", ""], ["Du", "Kai", ""], ["Tian", "Yonghong", ""], ["Huang", "Tiejun", ""]]}, {"id": "1806.05886", "submitter": "Ngoc Minh Tran", "authors": "Tran Ngoc Minh, Mathieu Sinn, Hoang Thanh Lam, Martin Wistuba", "title": "Automated Image Data Preprocessing with Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data preparation, i.e. the process of transforming raw data into a format\nthat can be used for training effective machine learning models, is a tedious\nand time-consuming task. For image data, preprocessing typically involves a\nsequence of basic transformations such as cropping, filtering, rotating or\nflipping images. Currently, data scientists decide manually based on their\nexperience which transformations to apply in which particular order to a given\nimage data set. Besides constituting a bottleneck in real-world data science\nprojects, manual image data preprocessing may yield suboptimal results as data\nscientists need to rely on intuition or trial-and-error approaches when\nexploring the space of possible image transformations and thus might not be\nable to discover the most effective ones. To mitigate the inefficiency and\npotential ineffectiveness of manual data preprocessing, this paper proposes a\ndeep reinforcement learning framework to automatically discover the optimal\ndata preprocessing steps for training an image classifier. The framework takes\nas input sets of labeled images and predefined preprocessing transformations.\nIt jointly learns the classifier and the optimal preprocessing transformations\nfor individual images. Experimental results show that the proposed approach not\nonly improves the accuracy of image classifiers, but also makes them\nsubstantially more robust to noisy inputs at test time.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 10:15:10 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 17:42:02 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Minh", "Tran Ngoc", ""], ["Sinn", "Mathieu", ""], ["Lam", "Hoang Thanh", ""], ["Wistuba", "Martin", ""]]}, {"id": "1806.05892", "submitter": "Ahmed Imtiaz Humayun", "authors": "Ahmed Imtiaz Humayun, Shabnam Ghaffarzadegan, Zhe Feng and Taufiq\n  Hasan", "title": "Learning Front-end Filter-bank Parameters using Convolutional Neural\n  Networks for Abnormal Heart Sound Detection", "comments": "4 pages, 6 figures, IEEE International Engineering in Medicine and\n  Biology Conference (EMBC)", "journal-ref": null, "doi": "10.1109/EMBC.2018.8512578", "report-no": null, "categories": "cs.CV cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic heart sound abnormality detection can play a vital role in the\nearly diagnosis of heart diseases, particularly in low-resource settings. The\nstate-of-the-art algorithms for this task utilize a set of Finite Impulse\nResponse (FIR) band-pass filters as a front-end followed by a Convolutional\nNeural Network (CNN) model. In this work, we propound a novel CNN architecture\nthat integrates the front-end bandpass filters within the network using\ntime-convolution (tConv) layers, which enables the FIR filter-bank parameters\nto become learnable. Different initialization strategies for the learnable\nfilters, including random parameters and a set of predefined FIR filter-bank\ncoefficients, are examined. Using the proposed tConv layers, we add constraints\nto the learnable FIR filters to ensure linear and zero phase responses.\nExperimental evaluations are performed on a balanced 4-fold cross-validation\ntask prepared using the PhysioNet/CinC 2016 dataset. Results demonstrate that\nthe proposed models yield superior performance compared to the state-of-the-art\nsystem, while the linear phase FIR filterbank method provides an absolute\nimprovement of 9.54% over the baseline in terms of an overall accuracy metric.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 10:33:31 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Humayun", "Ahmed Imtiaz", ""], ["Ghaffarzadegan", "Shabnam", ""], ["Feng", "Zhe", ""], ["Hasan", "Taufiq", ""]]}, {"id": "1806.05946", "submitter": "Federico Magliani", "authors": "Federico Magliani and Tomaso Fontanini and Andrea Prati", "title": "Efficient Nearest Neighbors Search for Large-Scale Landmark Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of landmark recognition has achieved excellent results in\nsmall-scale datasets. When dealing with large-scale retrieval, issues that were\nirrelevant with small amount of data, quickly become fundamental for an\nefficient retrieval phase. In particular, computational time needs to be kept\nas low as possible, whilst the retrieval accuracy has to be preserved as much\nas possible. In this paper we propose a novel multi-index hashing method called\nBag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows\nto drastically reduce the query time and outperforms the accuracy results\ncompared to the state-of-the-art methods for large-scale landmark recognition.\nIt has been demonstrated that this family of algorithms can be applied on\ndifferent embedding techniques like VLAD and R-MAC obtaining excellent results\nin very short times on different public datasets: Holidays+Flickr1M, Oxford105k\nand Paris106k.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:18:59 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Magliani", "Federico", ""], ["Fontanini", "Tomaso", ""], ["Prati", "Andrea", ""]]}, {"id": "1806.05974", "submitter": "Lorenz Berger", "authors": "Lorenz Berger, Eoin Hyde, Matt Gibb, Nevil Pavithran, Garin Kelly,\n  Faiz Mumtaz, S\\'ebastien Ourselin", "title": "Boosted Training of Convolutional Neural Networks for Multi-Class\n  Segmentation", "comments": "arXiv admin note: substantial text overlap with arXiv:1709.02764", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks on large and sparse datasets is still\nchallenging and can require large amounts of computation and memory. In this\nwork, we address the task of performing semantic segmentation on large\nvolumetric data sets, such as CT scans. Our contribution is threefold: 1) We\npropose a boosted sampling scheme that uses a-posterior error maps, generated\nthroughout training, to focus sampling on difficult regions, resulting in a\nmore informative loss. This results in a significant training speed up and\nimproves learning performance for image segmentation. 2) We propose a novel\nalgorithm for boosting the SGD learning rate schedule by adaptively increasing\nand lowering the learning rate, avoiding the need for extensive hyperparameter\ntuning. 3) We show that our method is able to attain new state-of-the-art\nresults on the VISCERAL Anatomy benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:42:20 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 16:23:15 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Berger", "Lorenz", ""], ["Hyde", "Eoin", ""], ["Gibb", "Matt", ""], ["Pavithran", "Nevil", ""], ["Kelly", "Garin", ""], ["Mumtaz", "Faiz", ""], ["Ourselin", "S\u00e9bastien", ""]]}, {"id": "1806.05978", "submitter": "Felix Laumann", "authors": "Kumar Shridhar, Felix Laumann, Marcus Liwicki", "title": "Uncertainty Estimations by Softplus normalization in Bayesian\n  Convolutional Neural Networks with Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel uncertainty estimation for classification tasks for\nBayesian convolutional neural networks with variational inference. By\nnormalizing the output of a Softplus function in the final layer, we estimate\naleatoric and epistemic uncertainty in a coherent manner. The intractable\nposterior probability distributions over weights are inferred by Bayes by\nBackprop. Firstly, we demonstrate how this reliable variational inference\nmethod can serve as a fundamental construct for various network architectures.\nOn multiple datasets in supervised learning settings (MNIST, CIFAR-10,\nCIFAR-100), this variational inference method achieves performances equivalent\nto frequentist inference in identical architectures, while the two desiderata,\na measure for uncertainty and regularization are incorporated naturally.\nSecondly, we examine how our proposed measure for aleatoric and epistemic\nuncertainties is derived and validate it on the aforementioned datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:55:18 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 11:35:35 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 05:37:03 GMT"}, {"version": "v4", "created": "Mon, 10 Sep 2018 08:16:32 GMT"}, {"version": "v5", "created": "Wed, 14 Nov 2018 13:48:37 GMT"}, {"version": "v6", "created": "Tue, 14 May 2019 09:04:11 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Shridhar", "Kumar", ""], ["Laumann", "Felix", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1806.05984", "submitter": "Rodrigo Berriel", "authors": "Rodrigo F. Berriel, Edilson de Aguiar, Alberto F. de Souza, Thiago\n  Oliveira-Santos", "title": "Ego-Lane Analysis System (ELAS): Dataset and Algorithms", "comments": "13 pages, 17 figures,\n  github.com/rodrigoberriel/ego-lane-analysis-system, and published by Image\n  and Vision Computing (IMAVIS)", "journal-ref": "Image and Vision Computing 68 (2017) 64-75", "doi": "10.1016/J.IMAVIS.2017.07.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decreasing costs of vision sensors and advances in embedded hardware boosted\nlane related research detection, estimation, and tracking in the past two\ndecades. The interest in this topic has increased even more with the demand for\nadvanced driver assistance systems (ADAS) and self-driving cars. Although\nextensively studied independently, there is still need for studies that propose\na combined solution for the multiple problems related to the ego-lane, such as\nlane departure warning (LDW), lane change detection, lane marking type (LMT)\nclassification, road markings detection and classification, and detection of\nadjacent lanes (i.e., immediate left and right lanes) presence. In this paper,\nwe propose a real-time Ego-Lane Analysis System (ELAS) capable of estimating\nego-lane position, classifying LMTs and road markings, performing LDW and\ndetecting lane change events. The proposed vision-based system works on a\ntemporal sequence of images. Lane marking features are extracted in perspective\nand Inverse Perspective Mapping (IPM) images that are combined to increase\nrobustness. The final estimated lane is modeled as a spline using a combination\nof methods (Hough lines with Kalman filter and spline with particle filter).\nBased on the estimated lane, all other events are detected. To validate ELAS\nand cover the lack of lane datasets in the literature, a new dataset with more\nthan 20 different scenes (in more than 15,000 frames) and considering a variety\nof scenarios (urban road, highways, traffic, shadows, etc.) was created. The\ndataset was manually annotated and made publicly available to enable evaluation\nof several events that are of interest for the research community (i.e., lane\nestimation, change, and centering; road markings; intersections; LMTs;\ncrosswalks and adjacent lanes). ELAS achieved high detection rates in all\nreal-world events and proved to be ready for real-time applications.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 14:02:44 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Berriel", "Rodrigo F.", ""], ["de Aguiar", "Edilson", ""], ["de Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "1806.06004", "submitter": "Peter Anderson", "authors": "Peter Anderson, Stephen Gould, Mark Johnson", "title": "Partially-Supervised Image Captioning", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning models are becoming increasingly successful at describing\nthe content of images in restricted domains. However, if these models are to\nfunction in the wild - for example, as assistants for people with impaired\nvision - a much larger number and variety of visual concepts must be\nunderstood. To address this problem, we teach image captioning models new\nvisual concepts from labeled images and object detection datasets. Since image\nlabels and object classes can be interpreted as partial captions, we formulate\nthis problem as learning from partially-specified sequence data. We then\npropose a novel algorithm for training sequence models, such as recurrent\nneural networks, on partially-specified sequences which we represent using\nfinite state automata. In the context of image captioning, our method lifts the\nrestriction that previously required image captioning models to be trained on\npaired image-sentence corpora only, or otherwise required specialized model\narchitectures to take advantage of alternative data modalities. Applying our\napproach to an existing neural captioning model, we achieve state of the art\nresults on the novel object captioning task using the COCO dataset. We further\nshow that we can train a captioning model to describe new visual concepts from\nthe Open Images dataset while maintaining competitive COCO evaluation scores.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 14:52:40 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 15:29:42 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Anderson", "Peter", ""], ["Gould", "Stephen", ""], ["Johnson", "Mark", ""]]}, {"id": "1806.06029", "submitter": "Sagie Benaim", "authors": "Sagie Benaim, Lior Wolf", "title": "One-Shot Unsupervised Cross Domain Translation", "comments": "Published at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a single image x from domain A and a set of images from domain B, our\ntask is to generate the analogous of x in B. We argue that this task could be a\nkey AI capability that underlines the ability of cognitive agents to act in the\nworld and present empirical evidence that the existing unsupervised domain\ntranslation methods fail on this task. Our method follows a two step process.\nFirst, a variational autoencoder for domain B is trained. Then, given the new\nsample x, we create a variational autoencoder for domain A by adapting the\nlayers that are close to the image in order to directly fit x, and only\nindirectly adapt the other layers. Our experiments indicate that the new method\ndoes as well, when trained on one sample x, as the existing domain transfer\nmethods, when these enjoy a multitude of training samples from domain A. Our\ncode is made publicly available at\nhttps://github.com/sagiebenaim/OneShotTranslation\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 16:03:36 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 09:25:09 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Benaim", "Sagie", ""], ["Wolf", "Lior", ""]]}, {"id": "1806.06034", "submitter": "Tengyu Ma", "authors": "Xiaohan Wang, Tengyu Ma, James Ainooson, Seunghwan Cha, Xiaotian Wang,\n  Azhar Molla, and Maithilee Kunda", "title": "The Toybox Dataset of Egocentric Visual Object Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In object recognition research, many commonly used datasets (e.g., ImageNet\nand similar) contain relatively sparse distributions of object instances and\nviews, e.g., one might see a thousand different pictures of a thousand\ndifferent giraffes, mostly taken from a few conventionally photographed angles.\nThese distributional properties constrain the types of computational\nexperiments that are able to be conducted with such datasets, and also do not\nreflect naturalistic patterns of embodied visual experience. As a contribution\nto the small (but growing) number of multi-view object datasets that have been\ncreated to bridge this gap, we introduce a new video dataset called Toybox that\ncontains egocentric (i.e., first-person perspective) videos of common household\nobjects and toys being manually manipulated to undergo structured\ntransformations, such as rotation, translation, and zooming. To illustrate\npotential uses of Toybox, we also present initial neural network experiments\nthat examine 1) how training on different distributions of object instances and\nviews affects recognition performance, and 2) how viewpoint-dependent object\nconcepts are represented within the hidden layers of a trained network.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 16:17:02 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 17:00:14 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 21:37:42 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Wang", "Xiaohan", ""], ["Ma", "Tengyu", ""], ["Ainooson", "James", ""], ["Cha", "Seunghwan", ""], ["Wang", "Xiaotian", ""], ["Molla", "Azhar", ""], ["Kunda", "Maithilee", ""]]}, {"id": "1806.06053", "submitter": "Joon Son Chung", "authors": "Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman", "title": "Deep Lip Reading: a comparison of models and an online application", "comments": "To appear in Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to develop state-of-the-art models for lip reading\n-- visual speech recognition. We develop three architectures and compare their\naccuracy and training times: (i) a recurrent model using LSTMs; (ii) a fully\nconvolutional model; and (iii) the recently proposed transformer model. The\nrecurrent and fully convolutional models are trained with a Connectionist\nTemporal Classification loss and use an explicit language model for decoding,\nthe transformer is a sequence-to-sequence model. Our best performing model\nimproves the state-of-the-art word error rate on the challenging BBC-Oxford Lip\nReading Sentences 2 (LRS2) benchmark dataset by over 20 percent.\n  As a further contribution we investigate the fully convolutional model when\nused for online (real time) lip reading of continuous speech, and show that it\nachieves high performance with low latency.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 17:37:01 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Afouras", "Triantafyllos", ""], ["Chung", "Joon Son", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1806.06098", "submitter": "Kyle Genova", "authors": "Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel\n  Vlasic, William T. Freeman", "title": "Unsupervised Training for 3D Morphable Model Regression", "comments": "CVPR 2018 version with supplemental material\n  (http://openaccess.thecvf.com/content_cvpr_2018/html/Genova_Unsupervised_Training_for_CVPR_2018_paper.html)", "journal-ref": "Conference on Computer Vision and Pattern Recognition (CVPR),\n  2018, pp. 8377-8386", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for training a regression network from image pixels to 3D\nmorphable model coordinates using only unlabeled photographs. The training loss\nis based on features from a facial recognition network, computed on-the-fly by\nrendering the predicted faces with a differentiable renderer. To make training\nfrom features feasible and avoid network fooling effects, we introduce three\nobjectives: a batch distribution loss that encourages the output distribution\nto match the distribution of the morphable model, a loopback loss that ensures\nthe network can correctly reinterpret its own output, and a multi-view identity\nloss that compares the features of the predicted 3D face and the input\nphotograph from multiple viewing angles. We train a regression network using\nthese objectives, a set of unlabeled photographs, and the morphable model\nitself, and demonstrate state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 19:31:20 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Genova", "Kyle", ""], ["Cole", "Forrester", ""], ["Maschinot", "Aaron", ""], ["Sarna", "Aaron", ""], ["Vlasic", "Daniel", ""], ["Freeman", "William T.", ""]]}, {"id": "1806.06157", "submitter": "Fabien Baradel", "authors": "Fabien Baradel, Natalia Neverova, Christian Wolf, Julien Mille, Greg\n  Mori", "title": "Object Level Visual Reasoning in Videos", "comments": "Accepted at ECCV 2018 - long version (16 pages + ref)", "journal-ref": "ECCV 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition is typically addressed by detecting key concepts\nlike global and local motion, features related to object classes present in the\nscene, as well as features related to the global context. The next open\nchallenges in activity recognition require a level of understanding that pushes\nbeyond this and call for models with capabilities for fine distinction and\ndetailed comprehension of interactions between actors and objects in a scene.\nWe propose a model capable of learning to reason about semantically meaningful\nspatiotemporal interactions in videos. The key to our approach is a choice of\nperforming this reasoning at the object level through the integration of state\nof the art object detection networks. This allows the model to learn detailed\nspatial interactions that exist at a semantic, object-interaction relevant\nlevel. We evaluate our method on three standard datasets (Twenty-BN\nSomething-Something, VLOG and EPIC Kitchens) and achieve state of the art\nresults on all of them. Finally, we show visualizations of the interactions\nlearned by the model, which illustrate object classes and their interactions\ncorresponding to different activity classes.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 00:33:50 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 08:20:02 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 08:59:32 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Baradel", "Fabien", ""], ["Neverova", "Natalia", ""], ["Wolf", "Christian", ""], ["Mille", "Julien", ""], ["Mori", "Greg", ""]]}, {"id": "1806.06172", "submitter": "Mohammad Hajizadeh Saffar", "authors": "Mohammad Hajizadeh Saffar, Mohsen Fayyaz, Mohammad Sabokrou, Mahmood\n  Fathy", "title": "Semantic Video Segmentation: A Review on Recent Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives an overview on semantic segmentation consists of an\nexplanation of this field, it's status and relation with other vision\nfundamental tasks, different datasets and common evaluation parameters that\nhave been used by researchers. This survey also includes an overall review on a\nvariety of recent approaches (RDF, MRF, CRF, etc.) and their advantages and\nchallenges and shows the superiority of CNN-based semantic segmentation systems\non CamVid and NYUDv2 datasets. In addition, some areas that is ideal for future\nwork have mentioned.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 02:31:00 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Saffar", "Mohammad Hajizadeh", ""], ["Fayyaz", "Mohsen", ""], ["Sabokrou", "Mohammad", ""], ["Fathy", "Mahmood", ""]]}, {"id": "1806.06176", "submitter": "Paul Pu Liang", "authors": "Yao-Hung Hubert Tsai and Paul Pu Liang and Amir Zadeh and\n  Louis-Philippe Morency and Ruslan Salakhutdinov", "title": "Learning Factorized Multimodal Representations", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning multimodal representations is a fundamentally complex research\nproblem due to the presence of multiple heterogeneous sources of information.\nAlthough the presence of multiple modalities provides additional valuable\ninformation, there are two key challenges to address when learning from\nmultimodal data: 1) models must learn the complex intra-modal and cross-modal\ninteractions for prediction and 2) models must be robust to unexpected missing\nor noisy modalities during testing. In this paper, we propose to optimize for a\njoint generative-discriminative objective across multimodal data and labels. We\nintroduce a model that factorizes representations into two sets of independent\nfactors: multimodal discriminative and modality-specific generative factors.\nMultimodal discriminative factors are shared across all modalities and contain\njoint multimodal features required for discriminative tasks such as sentiment\nprediction. Modality-specific generative factors are unique for each modality\nand contain the information required for generating data. Experimental results\nshow that our model is able to learn meaningful multimodal representations that\nachieve state-of-the-art or competitive performance on six multimodal datasets.\nOur model demonstrates flexible generative capabilities by conditioning on\nindependent factors and can reconstruct missing modalities without\nsignificantly impacting performance. Lastly, we interpret our factorized\nrepresentations to understand the interactions that influence multimodal\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 03:48:50 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 00:17:03 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 14:16:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Tsai", "Yao-Hung Hubert", ""], ["Liang", "Paul Pu", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1806.06177", "submitter": "Kai-Xuan Chen", "authors": "Kai-Xuan Chen, Xiao-Jun Wu, Rui Wang, Josef Kittler", "title": "Riemannian kernel based Nystr\\\"om method for approximate\n  infinite-dimensional covariance descriptors with application to image set\n  classification", "comments": "6 pages, 3 figures, International Conference on Pattern Recognition\n  2018", "journal-ref": null, "doi": "10.1109/ICPR.2018.8545822", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of pattern recognition, using the CovDs (Covariance\nDescriptors) to represent data and taking the metrics of the resulting\nRiemannian manifold into account have been widely adopted for the task of image\nset classification. Recently, it has been proven that infinite-dimensional\nCovDs are more discriminative than their low-dimensional counterparts. However,\nthe form of infinite-dimensional CovDs is implicit and the computational load\nis high. We propose a novel framework for representing image sets by\napproximating infinite-dimensional CovDs in the paradigm of the Nystr\\\"om\nmethod based on a Riemannian kernel. We start by modeling the images via CovDs,\nwhich lie on the Riemannian manifold spanned by SPD (Symmetric Positive\nDefinite) matrices. We then extend the Nystr\\\"om method to the SPD manifold and\nobtain the approximations of CovDs in RKHS (Reproducing Kernel Hilbert Space).\nFinally, we approximate infinite-dimensional CovDs via these approximations.\nEmpirically, we apply our framework to the task of image set classification.\nThe experimental results obtained on three benchmark datasets show that our\nproposed approximate infinite-dimensional CovDs outperform the original CovDs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 04:28:20 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 07:53:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Chen", "Kai-Xuan", ""], ["Wu", "Xiao-Jun", ""], ["Wang", "Rui", ""], ["Kittler", "Josef", ""]]}, {"id": "1806.06178", "submitter": "Kai-Xuan Chen", "authors": "Kai-Xuan Chen, Xiao-Jun Wu", "title": "Component SPD Matrices: A lower-dimensional discriminative data\n  descriptor for image set classification", "comments": "8 pages,5 figures, Computational Visual Media, 2018", "journal-ref": null, "doi": "10.1007/s41095-018-0119-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of pattern recognition, using the SPD (Symmetric Positive\nDefinite) matrices to represent data and taking the metrics of resulting\nRiemannian manifold into account have been widely used for the task of image\nset classification. In this paper, we propose a new data representation\nframework for image sets named CSPD (Component Symmetric Positive Definite).\nFirstly, we obtain sub-image sets by dividing the image set into square blocks\nwith the same size, and use traditional SPD model to describe them. Then, we\nuse the results of the Riemannian kernel on SPD matrices as similarities of\ncorresponding sub-image sets. Finally, the CSPD matrix appears in the form of\nthe kernel matrix for all the sub-image sets, and CSPDi,j denotes the\nsimilarity between i-th sub-image set and j-th sub-image set. Here, the\nRiemannian kernel is shown to satisfy the Mercer's theorem, so our proposed\nCSPD matrix is symmetric and positive definite and also lies on a Riemannian\nmanifold. On three benchmark datasets, experimental results show that CSPD is a\nlower-dimensional and more discriminative data descriptor for the task of image\nset classification.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 04:31:59 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Chen", "Kai-Xuan", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1806.06183", "submitter": "Ryan Benmalek", "authors": "Ryan Y. Benmalek, Claire Cardie, Serge Belongie, Xiadong He, Jianfeng\n  Gao", "title": "The Neural Painter: Multi-Turn Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we combine two research threads from Vision/ Graphics and\nNatural Language Processing to formulate an image generation task conditioned\non attributes in a multi-turn setting. By multiturn, we mean the image is\ngenerated in a series of steps of user-specified conditioning information. Our\nproposed approach is practically useful and offers insights into neural\ninterpretability. We introduce a framework that includes a novel training\nalgorithm as well as model improvements built for the multi-turn setting. We\ndemonstrate that this framework generates a sequence of images that match the\ngiven conditioning information and that this task is useful for more detailed\nbenchmarking and analysis of conditional image generation methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 04:52:03 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Benmalek", "Ryan Y.", ""], ["Cardie", "Claire", ""], ["Belongie", "Serge", ""], ["He", "Xiadong", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1806.06193", "submitter": "Yin Cui", "authors": "Yin Cui, Yang Song, Chen Sun, Andrew Howard, Serge Belongie", "title": "Large Scale Fine-Grained Categorization and Domain-Specific Transfer\n  Learning", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring the knowledge learned from large scale datasets (e.g., ImageNet)\nvia fine-tuning offers an effective solution for domain-specific fine-grained\nvisual categorization (FGVC) tasks (e.g., recognizing bird species or car make\nand model). In such scenarios, data annotation often calls for specialized\ndomain knowledge and thus is difficult to scale. In this work, we first tackle\na problem in large scale FGVC. Our method won first place in iNaturalist 2017\nlarge scale species classification challenge. Central to the success of our\napproach is a training scheme that uses higher image resolution and deals with\nthe long-tailed distribution of training data. Next, we study transfer learning\nvia fine-tuning from large scale datasets to small scale, domain-specific FGVC\ndatasets. We propose a measure to estimate domain similarity via Earth Mover's\nDistance and demonstrate that transfer learning benefits from pre-training on a\nsource domain that is similar to the target domain by this measure. Our\nproposed transfer learning outperforms ImageNet pre-training and obtains\nstate-of-the-art results on multiple commonly used FGVC datasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 06:11:06 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Cui", "Yin", ""], ["Song", "Yang", ""], ["Sun", "Chen", ""], ["Howard", "Andrew", ""], ["Belongie", "Serge", ""]]}, {"id": "1806.06195", "submitter": "Chao Yang Mr.", "authors": "Chao Yang, Taehwan Kim, Ruizhe Wang, Hao Peng and C.-C. Jay Kuo", "title": "Show, Attend and Translate: Unsupervised Image Translation with\n  Self-Regularization and Attention", "comments": "Accepted at IEEE Transactions on Image Processing (TIP)", "journal-ref": null, "doi": "10.1109/TIP.2019.2914583", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image translation between two domains is a class of problems aiming to learn\nmapping from an input image in the source domain to an output image in the\ntarget domain. It has been applied to numerous domains, such as data\naugmentation, domain adaptation, and unsupervised training. When paired\ntraining data is not accessible, image translation becomes an ill-posed\nproblem. We constrain the problem with the assumption that the translated image\nneeds to be perceptually similar to the original image and also appears to be\ndrawn from the new domain, and propose a simple yet effective image translation\nmodel consisting of a single generator trained with a self-regularization term\nand an adversarial term. We further notice that existing image translation\ntechniques are agnostic to the subjects of interest and often introduce\nunwanted changes or artifacts to the input. Thus we propose to add an attention\nmodule to predict an attention map to guide the image translation process. The\nmodule learns to attend to key parts of the image while keeping everything else\nunaltered, essentially avoiding undesired artifacts or changes. The predicted\nattention map also opens door to applications such as unsupervised segmentation\nand saliency detection. Extensive experiments and evaluations show that our\nmodel while being simpler, achieves significantly better performance than\nexisting image translation methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 07:02:47 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 19:10:49 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 20:59:05 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Yang", "Chao", ""], ["Kim", "Taehwan", ""], ["Wang", "Ruizhe", ""], ["Peng", "Hao", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1806.06198", "submitter": "Yabin Zhang", "authors": "Yabin Zhang, Kui Jia, Zhixin Wang", "title": "Part-Aware Fine-grained Object Categorization using Weakly Supervised\n  Part Detection Network", "comments": "TMM paper version. Codes are available at:\n  https://github.com/YBZh/PartNet", "journal-ref": "IEEE Transactions on Multimedia, 2019", "doi": "10.1109/TMM.2019.2939747", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained object categorization aims for distinguishing objects of\nsubordinate categories that belong to the same entry-level object category. The\ntask is challenging due to the facts that (1) training images with ground-truth\nlabels are difficult to obtain, and (2) variations among different subordinate\ncategories are subtle. It is well established that characterizing features of\ndifferent subordinate categories are located on local parts of object\ninstances. In fact, careful part annotations are available in many fine-grained\ncategorization datasets. However, manually annotating object parts requires\nexpertise, which is also difficult to generalize to new fine-grained\ncategorization tasks. In this work, we propose a Weakly Supervised Part\nDetection Network (PartNet) that is able to detect discriminative local parts\nfor use of fine-grained categorization. A vanilla PartNet builds on top of a\nbase subnetwork two parallel streams of upper network layers, which\nrespectively compute scores of classification probabilities (over subordinate\ncategories) and detection probabilities (over a specified number of\ndiscriminative part detectors) for local regions of interest (RoIs). The\nimage-level prediction is obtained by aggregating element-wise products of\nthese region-level probabilities. To generate a diverse set of RoIs as inputs\nof PartNet, we propose a simple Discretized Part Proposals module (DPP) that\ndirectly targets for proposing candidates of discriminative local parts, with\nno bridging via object-level proposals. Experiments on the benchmark\nCUB-200-2011 and Oxford Flower 102 datasets show the efficacy of our proposed\nmethod for both discriminative part detection and fine-grained categorization.\nIn particular, we achieve the new state-of-the-art performance on CUB-200-2011\ndataset when ground-truth part annotations are not available.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 07:08:59 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 14:32:09 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Zhang", "Yabin", ""], ["Jia", "Kui", ""], ["Wang", "Zhixin", ""]]}, {"id": "1806.06208", "submitter": "Sauradip Nag", "authors": "Sauradip Nag, Pallab Kumar Ganguly, Sumit Roy, Sourab Jha, Krishna\n  Bose, Abhishek Jha, Kousik Dasgupta", "title": "Offline Extraction of Indic Regional Language from Natural Scene Image\n  using Text Segmentation and Deep Convolutional Sequence", "comments": "Accepted in Second International Conference on Computational\n  Intelligence, Communications, and Business Analytics (CICBA-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional language extraction from a natural scene image is always a\nchallenging proposition due to its dependence on the text information extracted\nfrom Image. Text Extraction on the other hand varies on different lighting\ncondition, arbitrary orientation, inadequate text information, heavy background\ninfluence over text and change of text appearance. This paper presents a novel\nunified method for tackling the above challenges. The proposed work uses an\nimage correction and segmentation technique on the existing Text Detection\nPipeline an Efficient and Accurate Scene Text Detector (EAST). EAST uses\nstandard PVAnet architecture to select features and non maximal suppression to\ndetect text from image. Text recognition is done using combined architecture of\nMaxOut convolution neural network (CNN) and Bidirectional long short term\nmemory (LSTM) network. After recognizing text using the Deep Learning based\napproach, the native Languages are translated to English and tokenized using\nstandard Text Tokenizers. The tokens that very likely represent a location is\nused to find the Global Positioning System (GPS) coordinates of the location\nand subsequently the regional languages spoken in that location is extracted.\nThe proposed method is tested on a self generated dataset collected from\nGovernment of India dataset and experimented on Standard Dataset to evaluate\nthe performance of the proposed technique. Comparative study with a few\nstate-of-the-art methods on text detection, recognition and extraction of\nregional language from images shows that the proposed method outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:31:06 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 20:10:03 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Nag", "Sauradip", ""], ["Ganguly", "Pallab Kumar", ""], ["Roy", "Sumit", ""], ["Jha", "Sourab", ""], ["Bose", "Krishna", ""], ["Jha", "Abhishek", ""], ["Dasgupta", "Kousik", ""]]}, {"id": "1806.06228", "submitter": "Soujanya Poria", "authors": "N. Majumder, D. Hazarika, A. Gelbukh, E. Cambria, S. Poria", "title": "Multimodal Sentiment Analysis using Hierarchical Fusion with Context\n  Modeling", "comments": "Accepted for publication at Knowledge Based Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Multimodal sentiment analysis is a very actively growing field of research. A\npromising area of opportunity in this field is to improve the multimodal fusion\nmechanism. We present a novel feature fusion strategy that proceeds in a\nhierarchical fashion, first fusing the modalities two in two and only then\nfusing all three modalities. On multimodal sentiment analysis of individual\nutterances, our strategy outperforms conventional concatenation of features by\n1%, which amounts to 5% reduction in error rate. On utterance-level multimodal\nsentiment analysis of multi-utterance video clips, for which current\nstate-of-the-art techniques incorporate contextual information from other\nutterances of the same clip, our hierarchical fusion gives up to 2.4% (almost\n10% error rate reduction) over currently used concatenation. The implementation\nof our method is publicly available in the form of open-source code.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 12:05:24 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Majumder", "N.", ""], ["Hazarika", "D.", ""], ["Gelbukh", "A.", ""], ["Cambria", "E.", ""], ["Poria", "S.", ""]]}, {"id": "1806.06244", "submitter": "Ben Glocker", "authors": "Robert Robinson and Ozan Oktay and Wenjia Bai and Vanya Valindria and\n  Mihir Sanghvi and Nay Aung and Jos\\'e Paiva and Filip Zemrak and Kenneth Fung\n  and Elena Lukaschuk and Aaron Lee and Valentina Carapella and Young Jin Kim\n  and Bernhard Kainz and Stefan Piechnik and Stefan Neubauer and Steffen\n  Petersen and Chris Page and Daniel Rueckert and Ben Glocker", "title": "Real-time Prediction of Segmentation Quality", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning based image segmentation methods have\nenabled real-time performance with human-level accuracy. However, occasionally\neven the best method fails due to low image quality, artifacts or unexpected\nbehaviour of black box algorithms. Being able to predict segmentation quality\nin the absence of ground truth is of paramount importance in clinical practice,\nbut also in large-scale studies to avoid the inclusion of invalid data in\nsubsequent analysis.\n  In this work, we propose two approaches of real-time automated quality\ncontrol for cardiovascular MR segmentations using deep learning. First, we\ntrain a neural network on 12,880 samples to predict Dice Similarity\nCoefficients (DSC) on a per-case basis. We report a mean average error (MAE) of\n0.03 on 1,610 test samples and 97% binary classification accuracy for\nseparating low and high quality segmentations. Secondly, in the scenario where\nno manually annotated data is available, we train a network to predict DSC\nscores from estimated quality obtained via a reverse testing strategy. We\nreport an MAE=0.14 and 91% binary classification accuracy for this case.\nPredictions are obtained in real-time which, when combined with real-time\nsegmentation methods, enables instant feedback on whether an acquired scan is\nanalysable while the patient is still in the scanner. This further enables new\napplications of optimising image acquisition towards best possible analysis\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 13:53:31 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Robinson", "Robert", ""], ["Oktay", "Ozan", ""], ["Bai", "Wenjia", ""], ["Valindria", "Vanya", ""], ["Sanghvi", "Mihir", ""], ["Aung", "Nay", ""], ["Paiva", "Jos\u00e9", ""], ["Zemrak", "Filip", ""], ["Fung", "Kenneth", ""], ["Lukaschuk", "Elena", ""], ["Lee", "Aaron", ""], ["Carapella", "Valentina", ""], ["Kim", "Young Jin", ""], ["Kainz", "Bernhard", ""], ["Piechnik", "Stefan", ""], ["Neubauer", "Stefan", ""], ["Petersen", "Steffen", ""], ["Page", "Chris", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""]]}, {"id": "1806.06284", "submitter": "ShahRukh Athar", "authors": "ShahRukh Athar, Evgeny Burnaev, Victor Lempitsky", "title": "Latent Convolutional Models", "comments": "Updated with more recent experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new latent model of natural images that can be learned on\nlarge-scale datasets. The learning process provides a latent embedding for\nevery image in the training dataset, as well as a deep convolutional network\nthat maps the latent space to the image space. After training, the new model\nprovides a strong and universal image prior for a variety of image restoration\ntasks such as large-hole inpainting, superresolution, and colorization. To\nmodel high-resolution natural images, our approach uses latent spaces of very\nhigh dimensionality (one to two orders of magnitude higher than previous latent\nimage models). To tackle this high dimensionality, we use latent spaces with a\nspecial manifold structure (convolutional manifolds) parameterized by a ConvNet\nof a certain architecture. In the experiments, we compare the learned latent\nmodels with latent models learned by autoencoders, advanced variants of\ngenerative adversarial networks, and a strong baseline system using simpler\nparameterization of the latent space. Our model outperforms the competing\napproaches over a range of restoration tasks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 19:31:32 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 04:35:49 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Athar", "ShahRukh", ""], ["Burnaev", "Evgeny", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1806.06296", "submitter": "Thomas Lansdall-Welfare", "authors": "Sen Jia, Thomas Lansdall-Welfare, Nello Cristianini", "title": "Right for the Right Reason: Training Agnostic Networks", "comments": "Author's original version", "journal-ref": null, "doi": "10.1007/978-3-030-01768-2_14", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of a neural network being requested to classify\nimages (or other inputs) without making implicit use of a \"protected concept\",\nthat is a concept that should not play any role in the decision of the network.\nTypically these concepts include information such as gender or race, or other\ncontextual information such as image backgrounds that might be implicitly\nreflected in unknown correlations with other variables, making it insufficient\nto simply remove them from the input features. In other words, making accurate\npredictions is not good enough if those predictions rely on information that\nshould not be used: predictive performance is not the only important metric for\nlearning systems. We apply a method developed in the context of domain\nadaptation to address this problem of \"being right for the right reason\", where\nwe request a classifier to make a decision in a way that is entirely 'agnostic'\nto a given protected concept (e.g. gender, race, background etc.), even if this\ncould be implicitly reflected in other attributes via unknown correlations.\nAfter defining the concept of an 'agnostic model', we demonstrate how the\nDomain-Adversarial Neural Network can remove unwanted information from a model\nusing a gradient reversal layer.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:09:40 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Jia", "Sen", ""], ["Lansdall-Welfare", "Thomas", ""], ["Cristianini", "Nello", ""]]}, {"id": "1806.06298", "submitter": "Xianglei Xing", "authors": "Xianglei Xing, Ruiqi Gao, Tian Han, Song-Chun Zhu, Ying Nian Wu", "title": "Deformable Generator Network: Unsupervised Disentanglement of Appearance\n  and Geometry", "comments": "version 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deformable generator model to disentangle the appearance and\ngeometric information for both image and video data in a purely unsupervised\nmanner. The appearance generator network models the information related to\nappearance, including color, illumination, identity or category, while the\ngeometric generator performs geometric warping, such as rotation and\nstretching, through generating deformation field which is used to warp the\ngenerated appearance to obtain the final image or video sequences. Two\ngenerators take independent latent vectors as input to disentangle the\nappearance and geometric information from image or video sequences. For video\ndata, a nonlinear transition model is introduced to both the appearance and\ngeometric generators to capture the dynamics over time. The proposed scheme is\ngeneral and can be easily integrated into different generative models. An\nextensive set of qualitative and quantitative experiments shows that the\nappearance and geometric information can be well disentangled, and the learned\ngeometric generator can be conveniently transferred to other image datasets to\nfacilitate knowledge transfer tasks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:17:02 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 05:20:31 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 01:26:23 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Xing", "Xianglei", ""], ["Gao", "Ruiqi", ""], ["Han", "Tian", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1806.06321", "submitter": "Hiliwi Leake Kidane", "authors": "Hiliwi Leake Kidane", "title": "Comparative survey of visual object classifiers", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of Visual Object Classes represents one of the most elaborated\nareas of interest in Computer Vision. It is always challenging to get one\nspecific detector, descriptor or classifier that provides the expected object\nclassification result. Consequently, it critical to compare the different\ndetection, descriptor and classifier methods available and chose a single or\ncombination of two or three to get an optimal result. In this paper, we have\npresented a comparative survey of different feature descriptors and\nclassifiers. From feature descriptors, SIFT (Sparse & Dense) and HeuSIFT\ncombination colour descriptors; From classification techniques, Support Vector\nClassifier, K-Nearest Neighbor, ADABOOST, and fisher are covered in comparative\npractical implementation survey.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 01:20:32 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Kidane", "Hiliwi Leake", ""]]}, {"id": "1806.06397", "submitter": "Karim Armanious", "authors": "Karim Armanious, Chenming Jiang, Marc Fischer, Thomas K\\\"ustner,\n  Konstantin Nikolaou, Sergios Gatidis, Bin Yang", "title": "MedGAN: Medical Image Translation using GANs", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": "10.1016/j.compmedimag.2019.101684", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation is considered a new frontier in the field of\nmedical image analysis, with numerous potential applications. However, a large\nportion of recent approaches offers individualized solutions based on\nspecialized task-specific architectures or require refinement through\nnon-end-to-end training. In this paper, we propose a new framework, named\nMedGAN, for medical image-to-image translation which operates on the image\nlevel in an end-to-end manner. MedGAN builds upon recent advances in the field\nof generative adversarial networks (GANs) by merging the adversarial framework\nwith a new combination of non-adversarial losses. We utilize a discriminator\nnetwork as a trainable feature extractor which penalizes the discrepancy\nbetween the translated medical images and the desired modalities. Moreover,\nstyle-transfer losses are utilized to match the textures and fine-structures of\nthe desired target images to the translated images. Additionally, we present a\nnew generator architecture, titled CasNet, which enhances the sharpness of the\ntranslated medical outputs through progressive refinement via encoder-decoder\npairs. Without any application-specific modifications, we apply MedGAN on three\ndifferent tasks: PET-CT translation, correction of MR motion artefacts and PET\nimage denoising. Perceptual analysis by radiologists and quantitative\nevaluations illustrate that the MedGAN outperforms other existing translation\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 15:45:10 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 14:34:21 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Armanious", "Karim", ""], ["Jiang", "Chenming", ""], ["Fischer", "Marc", ""], ["K\u00fcstner", "Thomas", ""], ["Nikolaou", "Konstantin", ""], ["Gatidis", "Sergios", ""], ["Yang", "Bin", ""]]}, {"id": "1806.06406", "submitter": "Ming Tang", "authors": "Ming Tang, Linyu Zheng, Bin Yu, Jinqiao Wang", "title": "Fast Kernelized Correlation Filters without Boundary Effect", "comments": "A minor revision of its last version. 11+6 pages, 4+2 figures, 5\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, correlation filter based trackers (CF trackers) have\nattracted much attention from the vision community because of their top\nperformance in both localization accuracy and efficiency. The society of visual\ntracking, however, still needs to deal with the following difficulty on CF\ntrackers: avoiding or eliminating the boundary effect completely, in the\nmeantime, exploiting non-linear kernels and running efficiently. In this paper,\nwe propose a fast kernelized correlation filter without boundary effect\n(nBEKCF) to solve this problem. To avoid the boundary effect thoroughly, a set\nof \\emph{real} and \\emph{dense} patches is sampled through the traditional\nsliding window and used as the training samples to train nBEKCF to fit a\nGaussian response map. Non-linear kernels can be applied naturally in nBEKCF\ndue to its different theoretical foundation from the existing CF trackers'. To\nachieve the fast training and detection, a set of cyclic bases is introduced to\nconstruct the filter. Two algorithms, ACSII and CCIM, are developed to\nsignificantly accelerate the calculation of kernel correlation matrices. ACSII\nand CCIM fully exploit the density of training samples and cyclic structure of\nbases, and totally run in space domain. The efficiency of CCIM exceeds that of\nthe FFT counterpart remarkably in our task. Extensive experiments on six public\ndatasets, OTB-2013, OTB-2015, NfS, VOT2018, GOT10k, and TrackingNet, show that\ncompared to the CF trackers designed to relax the boundary effect, BACF and\nSRDCF, our nBEKCF achieves higher localization accuracy without tricks, in the\nmeanwhile, runs at higher FPS.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 16:25:35 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 07:14:00 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 08:00:32 GMT"}, {"version": "v4", "created": "Fri, 18 Sep 2020 04:09:13 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2020 01:17:57 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Tang", "Ming", ""], ["Zheng", "Linyu", ""], ["Yu", "Bin", ""], ["Wang", "Jinqiao", ""]]}, {"id": "1806.06418", "submitter": "Ming Tang", "authors": "Ming Tang, Bin Yu, Fan Zhang, Jinqiao Wang", "title": "High-speed Tracking with Multi-kernel Correlation Filters", "comments": "10+3 pages, 12 figures, 1 table, accepted by CVPR2018. This version\n  corrects some typos, and supplements a proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter (CF) based trackers are currently ranked top in terms of\ntheir performances. Nevertheless, only some of them, such as\nKCF~\\cite{henriques15} and MKCF~\\cite{tangm15}, are able to exploit the\npowerful discriminability of non-linear kernels. Although MKCF achieves more\npowerful discriminability than KCF through introducing multi-kernel learning\n(MKL) into KCF, its improvement over KCF is quite limited and its computational\nburden increases significantly in comparison with KCF. In this paper, we will\nintroduce the MKL into KCF in a different way than MKCF. We reformulate the MKL\nversion of CF objective function with its upper bound, alleviating the negative\nmutual interference of different kernels significantly. Our novel MKCF tracker,\nMKCFup, outperforms KCF and MKCF with large margins and can still work at very\nhigh fps. Extensive experiments on public datasets show that our method is\nsuperior to state-of-the-art algorithms for target objects of small move at\nvery high speed.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 17:38:15 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Tang", "Ming", ""], ["Yu", "Bin", ""], ["Zhang", "Fan", ""], ["Wang", "Jinqiao", ""]]}, {"id": "1806.06422", "submitter": "Yin Cui", "authors": "Yin Cui, Guandao Yang, Andreas Veit, Xun Huang, Serge Belongie", "title": "Learning to Evaluate Image Captioning", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation metrics for image captioning face two challenges. Firstly,\ncommonly used metrics such as CIDEr, METEOR, ROUGE and BLEU often do not\ncorrelate well with human judgments. Secondly, each metric has well known blind\nspots to pathological caption constructions, and rule-based metrics lack\nprovisions to repair such blind spots once identified. For example, the newly\nproposed SPICE correlates well with human judgments, but fails to capture the\nsyntactic structure of a sentence. To address these two challenges, we propose\na novel learning based discriminative evaluation metric that is directly\ntrained to distinguish between human and machine-generated captions. In\naddition, we further propose a data augmentation scheme to explicitly\nincorporate pathological transformations as negative examples during training.\nThe proposed metric is evaluated with three kinds of robustness tests and its\ncorrelation with human judgments. Extensive experiments show that the proposed\ndata augmentation scheme not only makes our metric more robust toward several\npathological transformations, but also improves its correlation with human\njudgments. Our metric outperforms other metrics on both caption level human\ncorrelation in Flickr 8k and system level human correlation in COCO. The\nproposed approach could be served as a learning based evaluation metric that is\ncomplementary to existing rule-based metrics.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 17:57:32 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Cui", "Yin", ""], ["Yang", "Guandao", ""], ["Veit", "Andreas", ""], ["Huang", "Xun", ""], ["Belongie", "Serge", ""]]}, {"id": "1806.06423", "submitter": "C. H. Huck Yang", "authors": "C.-H. Huck Yang, Jia-Hong Huang, Fangyu Liu, Fang-Yi Chiu, Mengya Gao,\n  Weifeng Lyu, I-Hung Lin M.D., Jesper Tegner", "title": "A Novel Hybrid Machine Learning Model for Auto-Classification of Retinal\n  Diseases", "comments": "Accepted at the Joint ICML and IJCAI Workshop on Computational\n  Biology (ICML-IJCAI WCB) to be held in Stockholm SWEDEN, 2018. Referring to\n  https://sites.google.com/view/wcb2018/accepted-papers?authuser=0", "journal-ref": "ICML-IJCAI Workshop 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic clinical diagnosis of retinal diseases has emerged as a promising\napproach to facilitate discovery in areas with limited access to specialists.\nWe propose a novel visual-assisted diagnosis hybrid model based on the support\nvector machine (SVM) and deep neural networks (DNNs). The model incorporates\ncomplementary strengths of DNNs and SVM. Furthermore, we present a new clinical\nretina label collection for ophthalmology incorporating 32 retina diseases\nclasses. Using EyeNet, our model achieves 89.73% diagnosis accuracy and the\nmodel performance is comparable to the professional ophthalmologists.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 18:22:55 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Yang", "C. -H. Huck", ""], ["Huang", "Jia-Hong", ""], ["Liu", "Fangyu", ""], ["Chiu", "Fang-Yi", ""], ["Gao", "Mengya", ""], ["Lyu", "Weifeng", ""], ["D.", "I-Hung Lin M.", ""], ["Tegner", "Jesper", ""]]}, {"id": "1806.06465", "submitter": "Roberto Mart\\'in-Mart\\'in", "authors": "Roberto Mart\\'in-Mart\\'in and Clemens Eppner and Oliver Brock", "title": "The RBO Dataset of Articulated Objects and Interactions", "comments": "6 pages; Submitted to the International Journal of Robotics Research\n  (Data Paper), Sage; Equal contribution by first two authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dataset with models of 14 articulated objects commonly found in\nhuman environments and with RGB-D video sequences and wrenches recorded of\nhuman interactions with them. The 358 interaction sequences total 67 minutes of\nhuman manipulation under varying experimental conditions (type of interaction,\nlighting, perspective, and background). Each interaction with an object is\nannotated with the ground truth poses of its rigid parts and the kinematic\nstate obtained by a motion capture system. For a subset of 78 sequences (25\nminutes), we also measured the interaction wrenches. The object models contain\ntextured three-dimensional triangle meshes of each link and their motion\nconstraints. We provide Python scripts to download and visualize the data. The\ndata is available at https://tu-rbo.github.io/articulated-objects/ and hosted\nat https://zenodo.org/record/1036660/.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 23:51:02 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Eppner", "Clemens", ""], ["Brock", "Oliver", ""]]}, {"id": "1806.06503", "submitter": "Zhixin Shu", "authors": "Zhixin Shu, Mihir Sahasrabudhe, Alp Guler, Dimitris Samaras, Nikos\n  Paragios, Iasonas Kokkinos", "title": "Deforming Autoencoders: Unsupervised Disentangling of Shape and\n  Appearance", "comments": "17 pages including references, plus 12 pages appendix. Video\n  available at : https://youtu.be/Oi7pyxKkF1g Code will be made available soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce Deforming Autoencoders, a generative model for\nimages that disentangles shape from appearance in an unsupervised manner. As in\nthe deformable template paradigm, shape is represented as a deformation between\na canonical coordinate system (`template') and an observed image, while\nappearance is modeled in `canonical', template, coordinates, thus discarding\nvariability due to deformations. We introduce novel techniques that allow this\napproach to be deployed in the setting of autoencoders and show that this\nmethod can be used for unsupervised group-wise image alignment. We show\nexperiments with expression morphing in humans, hands, and digits, face\nmanipulation, such as shape and appearance interpolation, as well as\nunsupervised landmark localization. A more powerful form of unsupervised\ndisentangling becomes possible in template coordinates, allowing us to\nsuccessfully decompose face images into shading and albedo, and further\nmanipulate face images.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 05:49:59 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Shu", "Zhixin", ""], ["Sahasrabudhe", "Mihir", ""], ["Guler", "Alp", ""], ["Samaras", "Dimitris", ""], ["Paragios", "Nikos", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1806.06506", "submitter": "Ahmed Imtiaz Humayun", "authors": "Ahmed Imtiaz Humayun, Md. Tauhiduzzaman Khan, Shabnam Ghaffarzadegan,\n  Zhe Feng and Taufiq Hasan", "title": "An Ensemble of Transfer, Semi-supervised and Supervised Learning Methods\n  for Pathological Heart Sound Classification", "comments": "5 pages, 5 figures, Interspeech 2018 accepted manuscript", "journal-ref": null, "doi": "10.21437/Interspeech.2018-2413", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we propose an ensemble of classifiers to distinguish between\nvarious degrees of abnormalities of the heart using Phonocardiogram (PCG)\nsignals acquired using digital stethoscopes in a clinical setting, for the\nINTERSPEECH 2018 Computational Paralinguistics (ComParE) Heart Beats\nSubChallenge. Our primary classification framework constitutes a convolutional\nneural network with 1D-CNN time-convolution (tConv) layers, which uses features\ntransferred from a model trained on the 2016 Physionet Heart Sound Database. We\nalso employ a Representation Learning (RL) approach to generate features in an\nunsupervised manner using Deep Recurrent Autoencoders and use Support Vector\nMachine (SVM) and Linear Discriminant Analysis (LDA) classifiers. Finally, we\nutilize an SVM classifier on a high-dimensional segment-level feature extracted\nusing various functionals on short-term acoustic features, i.e., Low-Level\nDescriptors (LLD). An ensemble of the three different approaches provides a\nrelative improvement of 11.13% compared to our best single sub-system in terms\nof the Unweighted Average Recall (UAR) performance metric on the evaluation\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 06:04:12 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 05:53:06 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Humayun", "Ahmed Imtiaz", ""], ["Khan", "Md. Tauhiduzzaman", ""], ["Ghaffarzadegan", "Shabnam", ""], ["Feng", "Zhe", ""], ["Hasan", "Taufiq", ""]]}, {"id": "1806.06519", "submitter": "Adrien Deli\\`ege Mr", "authors": "Adrien Deli\\`ege, Anthony Cioppa, Marc Van Droogenbroeck", "title": "HitNet: a neural network with capsules embedded in a Hit-or-Miss layer,\n  extended with hybrid data augmentation and ghost capsules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks designed for the task of classification have become a\ncommodity in recent years. Many works target the development of better\nnetworks, which results in a complexification of their architectures with more\nlayers, multiple sub-networks, or even the combination of multiple classifiers.\nIn this paper, we show how to redesign a simple network to reach excellent\nperformances, which are better than the results reproduced with CapsNet on\nseveral datasets, by replacing a layer with a Hit-or-Miss layer. This layer\ncontains activated vectors, called capsules, that we train to hit or miss a\ncentral capsule by tailoring a specific centripetal loss function. We also show\nhow our network, named HitNet, is capable of synthesizing a representative\nsample of the images of a given class by including a reconstruction network.\nThis possibility allows to develop a data augmentation step combining\ninformation from the data space and the feature space, resulting in a hybrid\ndata augmentation process. In addition, we introduce the possibility for\nHitNet, to adopt an alternative to the true target when needed by using the new\nconcept of ghost capsules, which is used here to detect potentially mislabeled\nimages in the training data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 07:08:11 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Deli\u00e8ge", "Adrien", ""], ["Cioppa", "Anthony", ""], ["Van Droogenbroeck", "Marc", ""]]}, {"id": "1806.06530", "submitter": "Sergiu Deitsch", "authors": "Sergiu Deitsch, Claudia Buerhop-Lutz, Evgenii Sovetkin, Ansgar\n  Steland, Andreas Maier, Florian Gallwitz, Christian Riess", "title": "Segmentation of Photovoltaic Module Cells in Uncalibrated\n  Electroluminescence Images", "comments": null, "journal-ref": null, "doi": "10.1007/s00138-021-01191-9", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High resolution electroluminescence (EL) images captured in the infrared\nspectrum allow to visually and non-destructively inspect the quality of\nphotovoltaic (PV) modules. Currently, however, such a visual inspection\nrequires trained experts to discern different kinds of defects, which is\ntime-consuming and expensive. Automated segmentation of cells is therefore a\nkey step in automating the visual inspection workflow. In this work, we propose\na robust automated segmentation method for extraction of individual solar cells\nfrom EL images of PV modules. This enables controlled studies on large amounts\nof data to understanding the effects of module degradation over time-a process\nnot yet fully understood. The proposed method infers in several steps a\nhigh-level solar module representation from low-level edge features. An\nimportant step in the algorithm is to formulate the segmentation problem in\nterms of lens calibration by exploiting the plumbline constraint. We evaluate\nour method on a dataset of various solar modules types containing a total of\n408 solar cells with various defects. Our method robustly solves this task with\na median weighted Jaccard index of 94.47% and an $F_1$ score of 97.62%, both\nindicating a very high similarity between automatically segmented and ground\ntruth solar cell masks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 07:38:55 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 20:07:25 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 17:45:11 GMT"}, {"version": "v4", "created": "Mon, 24 May 2021 20:46:34 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Deitsch", "Sergiu", ""], ["Buerhop-Lutz", "Claudia", ""], ["Sovetkin", "Evgenii", ""], ["Steland", "Ansgar", ""], ["Maier", "Andreas", ""], ["Gallwitz", "Florian", ""], ["Riess", "Christian", ""]]}, {"id": "1806.06575", "submitter": "Thu Nguyen-Phuoc", "authors": "Thu Nguyen-Phuoc, Chuan Li, Stephen Balaban, Yong-Liang Yang", "title": "RenderNet: A deep convolutional network for differentiable rendering\n  from 3D shapes", "comments": "14 pages, 9 figures", "journal-ref": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional computer graphics rendering pipeline is designed for procedurally\ngenerating 2D quality images from 3D shapes with high performance. The\nnon-differentiability due to discrete operations such as visibility computation\nmakes it hard to explicitly correlate rendering parameters and the resulting\nimage, posing a significant challenge for inverse rendering tasks. Recent work\non differentiable rendering achieves differentiability either by designing\nsurrogate gradients for non-differentiable operations or via an approximate but\ndifferentiable renderer. These methods, however, are still limited when it\ncomes to handling occlusion, and restricted to particular rendering effects. We\npresent RenderNet, a differentiable rendering convolutional network with a\nnovel projection unit that can render 2D images from 3D shapes. Spatial\nocclusion and shading calculation are automatically encoded in the network. Our\nexperiments show that RenderNet can successfully learn to implement different\nshaders, and can be used in inverse rendering tasks to estimate shape, pose,\nlighting and texture from a single image.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 09:45:33 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 21:07:31 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 16:24:00 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Nguyen-Phuoc", "Thu", ""], ["Li", "Chuan", ""], ["Balaban", "Stephen", ""], ["Yang", "Yong-Liang", ""]]}, {"id": "1806.06594", "submitter": "Mehryar Emambakhsh", "authors": "Mehryar Emambakhsh, Alessandro Bay, Eduard Vazquez", "title": "Deep Recurrent Neural Network for Multi-target Filtering", "comments": "The 25th International Conference on MultiMedia Modeling (MMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of fixed motion and measurement models for\nmulti-target filtering using an adaptive learning framework. This is performed\nby defining target tuples with random finite set terminology and utilisation of\nrecurrent neural networks with a long short-term memory architecture. A novel\ndata association algorithm compatible with the predicted tracklet tuples is\nproposed, enabling the update of occluded targets, in addition to assigning\nbirth, survival and death of targets. The algorithm is evaluated over a\ncommonly used filtering simulation scenario, with highly promising results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 10:47:14 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 12:45:19 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Emambakhsh", "Mehryar", ""], ["Bay", "Alessandro", ""], ["Vazquez", "Eduard", ""]]}, {"id": "1806.06595", "submitter": "Felix Bragman", "authors": "Felix J.S. Bragman, Ryutaro Tanno, Zach Eaton-Rosen, Wenqi Li, David\n  J. Hawkes, Sebastien Ourselin, Daniel C. Alexander, Jamie R. McClelland and\n  M. Jorge Cardoso", "title": "Uncertainty in multitask learning: joint representations for\n  probabilistic MR-only radiotherapy planning", "comments": "Early-accept at MICCAI 2018, 8 pages, 4 figures", "journal-ref": null, "doi": "10.1007/978-3-030-00937-3_1", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-task neural network architectures provide a mechanism that jointly\nintegrates information from distinct sources. It is ideal in the context of\nMR-only radiotherapy planning as it can jointly regress a synthetic CT (synCT)\nscan and segment organs-at-risk (OAR) from MRI. We propose a probabilistic\nmulti-task network that estimates: 1) intrinsic uncertainty through a\nheteroscedastic noise model for spatially-adaptive task loss weighting and 2)\nparameter uncertainty through approximate Bayesian inference. This allows\nsampling of multiple segmentations and synCTs that share their network\nrepresentation. We test our model on prostate cancer scans and show that it\nproduces more accurate and consistent synCTs with a better estimation in the\nvariance of the errors, state of the art results in OAR segmentation and a\nmethodology for quality assurance in radiotherapy treatment planning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 10:56:12 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Bragman", "Felix J. S.", ""], ["Tanno", "Ryutaro", ""], ["Eaton-Rosen", "Zach", ""], ["Li", "Wenqi", ""], ["Hawkes", "David J.", ""], ["Ourselin", "Sebastien", ""], ["Alexander", "Daniel C.", ""], ["McClelland", "Jamie R.", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1806.06611", "submitter": "Son Tran", "authors": "Son N. Tran, Qing Zhang, Mohan Karunanithi", "title": "On Multi-resident Activity Recognition in Ambient Smart-Homes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Increasing attention to the research on activity monitoring in smart homes\nhas motivated the employment of ambient intelligence to reduce the deployment\ncost and solve the privacy issue. Several approaches have been proposed for\nmulti-resident activity recognition, however, there still lacks a comprehensive\nbenchmark for future research and practical selection of models. In this paper\nwe study different methods for multi-resident activity recognition and evaluate\nthem on same sets of data. The experimental results show that recurrent neural\nnetwork with gated recurrent units is better than other models and also\nconsiderably efficient, and that using combined activities as single labels is\nmore effective than represent them as separate labels.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 12:04:10 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Tran", "Son N.", ""], ["Zhang", "Qing", ""], ["Karunanithi", "Mohan", ""]]}, {"id": "1806.06613", "submitter": "Marie-Lena Eckert", "authors": "Marie-Lena Eckert, Wolfgang Heidrich, Nils Thuerey", "title": "Coupled Fluid Density and Motion from Single Views", "comments": "Computer Graphics Forum (2018), further information:\n  https://ge.in.tum.de/publications/2018-cgf-eckert/, video:\n  https://www.youtube.com/watch?v=J2wkPNBJLaI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to reconstruct a fluid's 3D density and motion\nbased on just a single sequence of images. This is rendered possible by using\npowerful physical priors for this strongly under-determined problem. More\nspecifically, we propose a novel strategy to infer density updates strongly\ncoupled to previous and current estimates of the flow motion. Additionally, we\nemploy an accurate discretization and depth-based regularizers to compute\nstable solutions. Using only one view for the reconstruction reduces the\ncomplexity of the capturing setup drastically and could even allow for online\nvideo databases or smart-phone videos as inputs. The reconstructed 3D velocity\ncan then be flexibly utilized, e.g., for re-simulation, domain modification or\nguiding purposes. We will demonstrate the capacity of our method with a series\nof synthetic test cases and the reconstruction of real smoke plumes captured\nwith a Raspberry Pi camera.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 12:05:20 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Eckert", "Marie-Lena", ""], ["Heidrich", "Wolfgang", ""], ["Thuerey", "Nils", ""]]}, {"id": "1806.06621", "submitter": "Jonas Adler", "authors": "Jonas Adler and Sebastian Lunz", "title": "Banach Wasserstein GAN", "comments": "In NeurIPS2018. 10 pages, 9 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein Generative Adversarial Networks (WGANs) can be used to generate\nrealistic samples from complicated image distributions. The Wasserstein metric\nused in WGANs is based on a notion of distance between individual images, which\ninduces a notion of distance between probability distributions of images. So\nfar the community has considered $\\ell^2$ as the underlying distance. We\ngeneralize the theory of WGAN with gradient penalty to Banach spaces, allowing\npractitioners to select the features to emphasize in the generator. We further\ndiscuss the effect of some particular choices of underlying norms, focusing on\nSobolev norms. Finally, we demonstrate a boost in performance for an\nappropriate choice of norm on CIFAR-10 and CelebA.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 12:15:43 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 12:57:46 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Adler", "Jonas", ""], ["Lunz", "Sebastian", ""]]}, {"id": "1806.06719", "submitter": "Alex Zwanenburg", "authors": "Alex Zwanenburg, Stefan Leger, Linda Agolli, Karoline Pilz, Esther\n  G.C. Troost, Christian Richter, and Steffen L\\\"ock", "title": "Assessing robustness of radiomic features by image perturbation", "comments": "31 pages, 14 figures pre-submission version", "journal-ref": "Scientific Reports (2019) 9:614", "doi": "10.1038/s41598-018-36938-4", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image features need to be robust against differences in positioning,\nacquisition and segmentation to ensure reproducibility. Radiomic models that\nonly include robust features can be used to analyse new images, whereas models\nwith non-robust features may fail to predict the outcome of interest\naccurately. Test-retest imaging is recommended to assess robustness, but may\nnot be available for the phenotype of interest. We therefore investigated 18\nmethods to determine feature robustness based on image perturbations.\nTest-retest and perturbation robustness were compared for 4032 features that\nwere computed from the gross tumour volume in two cohorts with computed\ntomography imaging: I) 31 non-small-cell lung cancer (NSCLC) patients; II): 19\nhead-and-neck squamous cell carcinoma (HNSCC) patients. Robustness was measured\nusing the intraclass correlation coefficient (1,1) (ICC). Features with\nICC$\\geq0.90$ were considered robust. The NSCLC cohort contained more robust\nfeatures for test-retest imaging than the HNSCC cohort ($73.5\\%$ vs. $34.0\\%$).\nA perturbation chain consisting of noise addition, affine translation, volume\ngrowth/shrinkage and supervoxel-based contour randomisation identified the\nfewest false positive robust features (NSCLC: $3.3\\%$; HNSCC: $10.0\\%$). Thus,\nthis perturbation chain may be used to assess feature robustness.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 14:05:47 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Zwanenburg", "Alex", ""], ["Leger", "Stefan", ""], ["Agolli", "Linda", ""], ["Pilz", "Karoline", ""], ["Troost", "Esther G. C.", ""], ["Richter", "Christian", ""], ["L\u00f6ck", "Steffen", ""]]}, {"id": "1806.06769", "submitter": "Ahmed Taha", "authors": "Ahmed Taha, Pechin Lo, Junning Li, Tao Zhao", "title": "Kid-Net: Convolution Networks for Kidney Vessels Segmentation from\n  CT-Volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation plays an important role in modeling\npatient-specific anatomy. We propose a convolution neural network, called\nKid-Net, along with a training schema to segment kidney vessels: artery, vein\nand collecting system. Such segmentation is vital during the surgical planning\nphase in which medical decisions are made before surgical incision. Our main\ncontribution is developing a training schema that handles unbalanced data,\nreduces false positives and enables high-resolution segmentation with a limited\nmemory budget. These objectives are attained using dynamic weighting, random\nsampling and 3D patch segmentation. Manual medical image annotation is both\ntime-consuming and expensive. Kid-Net reduces kidney vessels segmentation time\nfrom matter of hours to minutes. It is trained end-to-end using 3D patches from\nvolumetric CT-images. A complete segmentation for a 512x512x512 CT-volume is\nobtained within a few minutes (1-2 mins) by stitching the output 3D patches\ntogether. Feature down-sampling and up-sampling are utilized to achieve higher\nclassification and localization accuracies. Quantitative and qualitative\nevaluation results on a challenging testing dataset show Kid-Net competence.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:25:07 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Taha", "Ahmed", ""], ["Lo", "Pechin", ""], ["Li", "Junning", ""], ["Zhao", "Tao", ""]]}, {"id": "1806.06778", "submitter": "Maciej Zieba", "authors": "Maciej Zieba, Piotr Semberecki, Tarek El-Gaaly, Tomasz Trzcinski", "title": "BinGAN: Learning Compact Binary Descriptors with a Regularized GAN", "comments": "Paper accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel regularization method for Generative\nAdversarial Networks, which allows the model to learn discriminative yet\ncompact binary representations of image patches (image descriptors). We employ\nthe dimensionality reduction that takes place in the intermediate layers of the\ndiscriminator network and train binarized low-dimensional representation of the\npenultimate layer to mimic the distribution of the higher-dimensional preceding\nlayers. To achieve this, we introduce two loss terms that aim at: (i) reducing\nthe correlation between the dimensions of the binarized low-dimensional\nrepresentation of the penultimate layer i. e. maximizing joint entropy) and\n(ii) propagating the relations between the dimensions in the high-dimensional\nspace to the low-dimensional space. We evaluate the resulting binary image\ndescriptors on two challenging applications, image matching and retrieval, and\nachieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:39:09 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 06:23:55 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 21:27:58 GMT"}, {"version": "v4", "created": "Tue, 6 Nov 2018 17:06:57 GMT"}, {"version": "v5", "created": "Wed, 7 Nov 2018 06:55:58 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Zieba", "Maciej", ""], ["Semberecki", "Piotr", ""], ["El-Gaaly", "Tarek", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1806.06793", "submitter": "Mohammad Tavakolian", "authors": "Mohammad Tavakolian and Abdenour Hadid", "title": "Deep Spatiotemporal Representation of the Face for Automatic Pain\n  Intensity Estimation", "comments": "5 pages, 4 figures, Accepted in ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic pain intensity assessment has a high value in disease diagnosis\napplications. Inspired by the fact that many diseases and brain disorders can\ninterrupt normal facial expression formation, we aim to develop a computational\nmodel for automatic pain intensity assessment from spontaneous and micro facial\nvariations. For this purpose, we propose a 3D deep architecture for dynamic\nfacial video representation. The proposed model is built by stacking several\nconvolutional modules where each module encompasses a 3D convolution kernel\nwith a fixed temporal depth, several parallel 3D convolutional kernels with\ndifferent temporal depths, and an average pooling layer. Deploying variable\ntemporal depths in the proposed architecture allows the model to effectively\ncapture a wide range of spatiotemporal variations on the faces. Extensive\nexperiments on the UNBC-McMaster Shoulder Pain Expression Archive database show\nthat our proposed model yields in a promising performance compared to the\nstate-of-the-art in automatic pain intensity estimation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 16:02:40 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Tavakolian", "Mohammad", ""], ["Hadid", "Abdenour", ""]]}, {"id": "1806.06811", "submitter": "Isabel Funke", "authors": "Isabel Funke and Alexander Jenke and S\\\"oren Torge Mees and J\\\"urgen\n  Weitz and Stefanie Speidel and Sebastian Bodenstedt", "title": "Temporal coherence-based self-supervised learning for laparoscopic\n  workflow analysis", "comments": "Accepted at the Workshop on Context-Aware Operating Theaters (OR\n  2.0), a MICCAI satellite event", "journal-ref": "CARE 2018, CLIP 2018, OR 2.0 2018, ISIC 2018. Lecture Notes in\n  Computer Science, vol 11041 (2018) 85-93", "doi": "10.1007/978-3-030-01201-4_11", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to provide the right type of assistance at the right time,\ncomputer-assisted surgery systems need context awareness. To achieve this,\nmethods for surgical workflow analysis are crucial. Currently, convolutional\nneural networks provide the best performance for video-based workflow analysis\ntasks. For training such networks, large amounts of annotated data are\nnecessary. However, collecting a sufficient amount of data is often costly,\ntime-consuming, and not always feasible. In this paper, we address this problem\nby presenting and comparing different approaches for self-supervised\npretraining of neural networks on unlabeled laparoscopic videos using temporal\ncoherence. We evaluate our pretrained networks on Cholec80, a publicly\navailable dataset for surgical phase segmentation, on which a maximum F1 score\nof 84.6 was reached. Furthermore, we were able to achieve an increase of the F1\nscore of up to 10 points when compared to a non-pretrained neural network.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 16:31:25 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 13:32:09 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Funke", "Isabel", ""], ["Jenke", "Alexander", ""], ["Mees", "S\u00f6ren Torge", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""], ["Bodenstedt", "Sebastian", ""]]}, {"id": "1806.06820", "submitter": "Yasutaka Narazaki", "authors": "Yasutaka Narazaki, Vedhus Hoskere, Tu A. Hoang, Billie F. Spencer Jr", "title": "Automated Bridge Component Recognition using Video Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the automated recognition of structural bridge\ncomponents using video data. Although understanding video data for structural\ninspections is straightforward for human inspectors, the implementation of the\nsame task using machine learning methods has not been fully realized. In\nparticular, single-frame image processing techniques, such as convolutional\nneural networks (CNNs), are not expected to identify structural components\naccurately when the image is a close-up view, lacking contextual information\nregarding where on the structure the image originates. Inspired by the\nsignificant progress in video processing techniques, this study investigates\nautomated bridge component recognition using video data, where the information\nfrom the past frames is used to augment the understanding of the current frame.\nA new simulated video dataset is created to train the machine learning\nalgorithms. Then, convolutional Neural Networks (CNNs) with recurrent\narchitectures are designed and applied to implement the automated bridge\ncomponent recognition task. Results are presented for simulated video data, as\nwell as video collected in the field.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 16:59:05 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 00:21:01 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Narazaki", "Yasutaka", ""], ["Hoskere", "Vedhus", ""], ["Hoang", "Tu A.", ""], ["Spencer", "Billie F.", "Jr"]]}, {"id": "1806.06876", "submitter": "Subhankar Chattoraj", "authors": "Sawon Pratiher and Subhankar Chattoraj", "title": "Diving Deep onto Discriminative Ensemble of Histological Hashing &\n  Class-Specific Manifold Learning for Multi-class Breast Carcinoma Taxonomy", "comments": "This paper is accepted for presentation at 44th International\n  Conference on Acoustics, Speech, and Signal Processing (IEEE ICASSP), UK,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological images (HI) encrypt resolution dependent heterogeneous\ntextures & diverse color distribution variability, manifesting in\nmicro-structural surface tissue convolutions. Also, inherently high coherency\nof cancerous cells poses significant challenges to breast cancer (BC)\nmulti-classification. As such, multi-class stratification is sparsely explored\n& prior work mainly focus on benign & malignant tissue characterization only,\nwhich forestalls further quantitative analysis of subordinate classes like\nadenosis, mucinous carcinoma & fibroadenoma etc, for diagnostic competence. In\nthis work, a fully-automated, near-real-time & computationally inexpensive\nrobust multi-classification deep framework from HI is presented.\n  The proposed scheme employs deep neural network (DNN) aided discriminative\nensemble of holistic class-specific manifold learning (CSML) for underlying HI\nsub-space embedding & HI hashing based local shallow signatures. The model\nachieves 95.8% accuracy pertinent to multi-classification & 2.8% overall\nperformance improvement & 38.2% enhancement for Lobular carcinoma (LC)\nsub-class recognition rate as compared to the existing state-of-the-art on well\nknown BreakHis dataset is achieved. Also, 99.3% recognition rate at 200X & a\nsensitivity of 100% for binary grading at all magnification validates its\nsuitability for clinical deployment in hand-held smart devices.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 18:24:16 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 02:30:18 GMT"}, {"version": "v3", "created": "Sun, 24 Mar 2019 05:20:22 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Pratiher", "Sawon", ""], ["Chattoraj", "Subhankar", ""]]}, {"id": "1806.06886", "submitter": "Aditya Sharma", "authors": "Aditya Sharma, Prabhjot Kaur, Aditya Nigam, Arnav Bhavsar", "title": "Learning to Decode 7T-like MR Image Reconstruction from 3T MR Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing demand for high field magnetic resonance (MR) scanner indicates\nthe need for high-quality MR images for accurate medical diagnosis. However,\ncost constraints, instead, motivate a need for algorithms to enhance images\nfrom low field scanners. We propose an approach to process the given low field\n(3T) MR image slices to reconstruct the corresponding high field (7T-like)\nslices. Our framework involves a novel architecture of a merged convolutional\nautoencoder with a single encoder and multiple decoders. Specifically, we\nemploy three decoders with random initializations, and the proposed training\napproach involves selection of a particular decoder in each weight-update\niteration for back propagation. We demonstrate that the proposed algorithm\noutperforms some related contemporary methods in terms of performance and\nreconstruction time.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 18:46:15 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Sharma", "Aditya", ""], ["Kaur", "Prabhjot", ""], ["Nigam", "Aditya", ""], ["Bhavsar", "Arnav", ""]]}, {"id": "1806.06888", "submitter": "Jean-Philippe Mercier", "authors": "Jean-Philippe Mercier, Chaitanya Mitash, Philippe Gigu\\`ere and\n  Abdeslam Boularias", "title": "Learning Object Localization and 6D Pose Estimation from Simulation and\n  Weakly Labeled Real Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a process for efficiently training a point-wise object\ndetector that enables localizing objects and computing their 6D poses in\ncluttered and occluded scenes. Accurate pose estimation is typically a\nrequirement for robust robotic grasping and manipulation of objects placed in\ncluttered, tight environments, such as a shelf with multiple objects. To\nminimize the human labor required for annotation, the proposed object detector\nis first trained in simulation by using automatically annotated synthetic\nimages. We then show that the performance of the detector can be substantially\nimproved by using a small set of weakly annotated real images, where a human\nprovides only a list of objects present in each image without indicating the\nlocation of the objects. To close the gap between real and synthetic images, we\nadopt a domain adaptation approach through adversarial training. The detector\nresulting from this training process can be used to localize objects by using\nits per-object activation maps. In this work, we use the activation maps to\nguide the search of 6D poses of objects. Our proposed approach is evaluated on\nseveral publicly available datasets for pose estimation. We also evaluated our\nmodel on classification and localization in unsupervised and semi-supervised\nsettings. The results clearly indicate that this approach could provide an\nefficient way toward fully automating the training process of computer vision\nmodels used in robotics.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 18:48:12 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 02:41:03 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Mercier", "Jean-Philippe", ""], ["Mitash", "Chaitanya", ""], ["Gigu\u00e8re", "Philippe", ""], ["Boularias", "Abdeslam", ""]]}, {"id": "1806.06926", "submitter": "Wojciech Samek", "authors": "Christopher Anders, Gr\\'egoire Montavon, Wojciech Samek, Klaus-Robert\n  M\\\"uller", "title": "Understanding Patch-Based Learning by Explaining Predictions", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are able to learn highly predictive models of video data. Due\nto video length, a common strategy is to train them on small video snippets. We\napply the deep Taylor / LRP technique to understand the deep network's\nclassification decisions, and identify a \"border effect\": a tendency of the\nclassifier to look mainly at the bordering frames of the input. This effect\nrelates to the step size used to build the video snippet, which we can then\ntune in order to improve the classifier's accuracy without retraining the\nmodel. To our knowledge, this is the the first work to apply the deep Taylor /\nLRP technique on any video analyzing neural network.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 23:44:45 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Anders", "Christopher", ""], ["Montavon", "Gr\u00e9goire", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1806.06927", "submitter": "Jaehong Kim", "authors": "Jaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee,\n  Youngduck Choi, Yongseok Choi, Dong-Yeon Cho, Jiwon Kim", "title": "Auto-Meta: Automated Gradient Based Meta Learner Search", "comments": "Presented at NIPS 2018 Workshop on Meta-Learning (MetaLearn 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully automating machine learning pipelines is one of the key challenges of\ncurrent artificial intelligence research, since practical machine learning\noften requires costly and time-consuming human-powered processes such as model\ndesign, algorithm development, and hyperparameter tuning. In this paper, we\nverify that automated architecture search synergizes with the effect of\ngradient-based meta learning. We adopt the progressive neural architecture\nsearch \\cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal\narchitectures for meta-learners. The gradient based meta-learner whose\narchitecture was automatically found achieved state-of-the-art results on the\n5-shot 5-way Mini-ImageNet classification problem with $74.65\\%$ accuracy,\nwhich is $11.54\\%$ improvement over the result obtained by the first\ngradient-based meta-learner called MAML\n\\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is\nthe first successful neural architecture search implementation in the context\nof meta learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 04:28:02 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 19:02:53 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Kim", "Jaehong", ""], ["Lee", "Sangyeul", ""], ["Kim", "Sungwan", ""], ["Cha", "Moonsu", ""], ["Lee", "Jung Kwon", ""], ["Choi", "Youngduck", ""], ["Choi", "Yongseok", ""], ["Cho", "Dong-Yeon", ""], ["Kim", "Jiwon", ""]]}, {"id": "1806.06939", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Mario Fritz, Bernt Schiele", "title": "Bayesian Prediction of Future Street Scenes through Importance Sampling\n  based Optimization", "comments": "The objective in (8) allows for trivial solutions e.g. the prior", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For autonomous agents to successfully operate in the real world, anticipation\nof future events and states of their environment is a key competence. This\nproblem can be formalized as a sequence prediction problem, where a number of\nobservations are used to predict the sequence into the future. However,\nreal-world scenarios demand a model of uncertainty of such predictions, as\nfuture states become increasingly uncertain and multi-modal -- in particular on\nlong time horizons. This makes modelling and learning challenging. We cast\nstate of the art semantic segmentation and future prediction models based on\ndeep learning into a Bayesian formulation that in turn allows for a full\nBayesian treatment of the prediction problem. We present a new sampling scheme\nfor this model that draws from the success of variational autoencoders by\nincorporating a recognition network. In the experiments we show that our model\noutperforms prior work in accuracy of the predicted segmentation and provides\ncalibrated probabilities that also better capture the multi-modal aspects of\npossible future states of street scenes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 20:51:36 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 09:30:26 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1806.06946", "submitter": "Alexey Potapov", "authors": "Alexey Potapov, Innokentii Zhdanov, Oleg Scherbakov, Nikolai\n  Skorobogatko, Hugo Latapie, Enzo Fenoglio", "title": "Semantic Image Retrieval by Uniting Deep Neural Networks and Cognitive\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and video retrieval by their semantic content has been an important and\nchallenging task for years, because it ultimately requires bridging the\nsymbolic/subsymbolic gap. Recent successes in deep learning enabled detection\nof objects belonging to many classes greatly outperforming traditional computer\nvision techniques. However, deep learning solutions capable of executing\nretrieval queries are still not available. We propose a hybrid solution\nconsisting of a deep neural network for object detection and a cognitive\narchitecture for query execution. Specifically, we use YOLOv2 and OpenCog.\nQueries allowing the retrieval of video frames containing objects of specified\nclasses and specified spatial arrangement are implemented.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:53:53 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Potapov", "Alexey", ""], ["Zhdanov", "Innokentii", ""], ["Scherbakov", "Oleg", ""], ["Skorobogatko", "Nikolai", ""], ["Latapie", "Hugo", ""], ["Fenoglio", "Enzo", ""]]}, {"id": "1806.06970", "submitter": "Shan E Ahmed Raza", "authors": "Shan E Ahmed Raza, Khalid AbdulJabbar, Mariam Jamal-Hanjani, Selvaraju\n  Veeriah, John Le Quesne, Charles Swanton, Yinyin Yuan", "title": "Deconvolving convolution neural network for cell detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic cell detection in histology images is a challenging task due to\nvarying size, shape and features of cells and stain variations across a large\ncohort. Conventional deep learning methods regress the probability of each\npixel belonging to the centre of a cell followed by detection of local maxima.\nWe present deconvolution as an alternate approach to local maxima detection.\nThe ground truth points are convolved with a mapping filter to generate\nartifical labels. A convolutional neural network (CNN) is modified to convolve\nit's output with the same mapping filter and is trained for the mapped labels.\nOutput of the trained CNN is then deconvolved to generate points as cell\ndetection. We compare our method with state-of-the-art deep learning approaches\nwhere the results show that the proposed approach detects cells with\ncomparatively high precision and F1-score.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 22:26:49 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Raza", "Shan E Ahmed", ""], ["AbdulJabbar", "Khalid", ""], ["Jamal-Hanjani", "Mariam", ""], ["Veeriah", "Selvaraju", ""], ["Quesne", "John Le", ""], ["Swanton", "Charles", ""], ["Yuan", "Yinyin", ""]]}, {"id": "1806.06984", "submitter": "Tom Runia", "authors": "Tom F.H. Runia, Cees G.M. Snoek, Arnold W.M. Smeulders", "title": "Repetition Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual repetition is ubiquitous in our world. It appears in human activity\n(sports, cooking), animal behavior (a bee's waggle dance), natural phenomena\n(leaves in the wind) and in urban environments (flashing lights). Estimating\nvisual repetition from realistic video is challenging as periodic motion is\nrarely perfectly static and stationary. To better deal with realistic video, we\nelevate the static and stationary assumptions often made by existing work. Our\nspatiotemporal filtering approach, established on the theory of periodic\nmotion, effectively handles a wide variety of appearances and requires no\nlearning. Starting from motion in 3D we derive three periodic motion types by\ndecomposition of the motion field into its fundamental components. In addition,\nthree temporal motion continuities emerge from the field's temporal dynamics.\nFor the 2D perception of 3D motion we consider the viewpoint relative to the\nmotion; what follows are 18 cases of recurrent motion perception. To estimate\nrepetition under all circumstances, our theory implies constructing a mixture\nof differential motion maps: gradient, divergence and curl. We temporally\nconvolve the motion maps with wavelet filters to estimate repetitive dynamics.\nOur method is able to spatially segment repetitive motion directly from the\ntemporal filter responses densely computed over the motion maps. For\nexperimental verification of our claims, we use our novel dataset for\nrepetition estimation, better-reflecting reality with non-static and\nnon-stationary repetitive motion. On the task of repetition counting, we obtain\nfavorable results compared to a deep learning alternative.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 23:30:23 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Runia", "Tom F. H.", ""], ["Snoek", "Cees G. M.", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1806.06985", "submitter": "Minh-Tan Pham", "authors": "Minh-Tan Pham, Erchan Aptoula, S\\'ebastien Lef\\`evre", "title": "Classification of remote sensing images using attribute profiles and\n  feature profiles from different trees: a comparative study", "comments": "4 pages, to appear in IGARSS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of this paper is to conduct a comparative study on remote\nsensing image classification using the morphological attribute profiles (APs)\nand feature profiles (FPs) generated from different types of tree structures.\nOver the past few years, APs have been among the most effective methods to\nmodel the image's spatial and contextual information. Recently, a novel\nextension of APs called FPs has been proposed by replacing pixel gray-levels\nwith some statistical and geometrical features when forming the output\nprofiles. FPs have been proved to be more efficient than the standard APs when\ngenerated from component trees (max-tree and min-tree). In this work, we\ninvestigate their performance on the inclusion tree (tree of shapes) and\npartition trees (alpha tree and omega tree). Experimental results from both\npanchromatic and hyperspectral images again confirm the efficiency of FPs\ncompared to APs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 23:34:55 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Pham", "Minh-Tan", ""], ["Aptoula", "Erchan", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""]]}, {"id": "1806.06986", "submitter": "Zhe Wu", "authors": "Zhe Wu, Navaneeth Bodla, Bharat Singh, Mahyar Najibi, Rama Chellappa,\n  Larry S. Davis", "title": "Soft Sampling for Robust Object Detection", "comments": "Accepted in BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robustness of object detection under the presence of missing\nannotations. In this setting, the unlabeled object instances will be treated as\nbackground, which will generate an incorrect training signal for the detector.\nInterestingly, we observe that after dropping 30% of the annotations (and\nlabeling them as background), the performance of CNN-based object detectors\nlike Faster-RCNN only drops by 5% on the PASCAL VOC dataset. We provide a\ndetailed explanation for this result. To further bridge the performance gap, we\npropose a simple yet effective solution, called Soft Sampling. Soft Sampling\nre-weights the gradients of RoIs as a function of overlap with positive\ninstances. This ensures that the uncertain background regions are given a\nsmaller weight compared to the hardnegatives. Extensive experiments on curated\nPASCAL VOC datasets demonstrate the effectiveness of the proposed Soft Sampling\nmethod at different annotation drop rates. Finally, we show that on\nOpenImagesV3, which is a real-world dataset with missing annotations, Soft\nSampling outperforms standard detection baselines by over 3%.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 23:40:14 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 03:29:55 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Wu", "Zhe", ""], ["Bodla", "Navaneeth", ""], ["Singh", "Bharat", ""], ["Najibi", "Mahyar", ""], ["Chellappa", "Rama", ""], ["Davis", "Larry S.", ""]]}, {"id": "1806.06987", "submitter": "Yuanwei Li", "authors": "Yuanwei Li, Amir Alansary, Juan J. Cerrolaza, Bishesh Khanal, Matthew\n  Sinclair, Jacqueline Matthew, Chandni Gupta, Caroline Knight, Bernhard Kainz,\n  and Daniel Rueckert", "title": "Fast Multiple Landmark Localisation Using a Patch-based Iterative\n  Network", "comments": "8 pages, 4 figures, Accepted for MICCAI 2018", "journal-ref": "LNCS 11070 (2018) 563-571", "doi": "10.1007/978-3-030-00928-1_64", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Patch-based Iterative Network (PIN) for fast and accurate\nlandmark localisation in 3D medical volumes. PIN utilises a Convolutional\nNeural Network (CNN) to learn the spatial relationship between an image patch\nand anatomical landmark positions. During inference, patches are repeatedly\npassed to the CNN until the estimated landmark position converges to the true\nlandmark location. PIN is computationally efficient since the inference stage\nonly selectively samples a small number of patches in an iterative fashion\nrather than a dense sampling at every location in the volume. Our approach\nadopts a multi-task learning framework that combines regression and\nclassification to improve localisation accuracy. We extend PIN to localise\nmultiple landmarks by using principal component analysis, which models the\nglobal anatomical relationships between landmarks. We have evaluated PIN using\n72 3D ultrasound images from fetal screening examinations. PIN achieves\nquantitatively an average landmark localisation error of 5.59mm and a runtime\nof 0.44s to predict 10 landmarks per volume. Qualitatively, anatomical 2D\nstandard scan planes derived from the predicted landmark locations are visually\nsimilar to the clinical ground truth. Source code is publicly available at\nhttps://github.com/yuanwei1989/landmark-detection.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 23:51:02 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 00:27:22 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Li", "Yuanwei", ""], ["Alansary", "Amir", ""], ["Cerrolaza", "Juan J.", ""], ["Khanal", "Bishesh", ""], ["Sinclair", "Matthew", ""], ["Matthew", "Jacqueline", ""], ["Gupta", "Chandni", ""], ["Knight", "Caroline", ""], ["Kainz", "Bernhard", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1806.07011", "submitter": "Xavier Puig", "authors": "Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja\n  Fidler, Antonio Torralba", "title": "VirtualHome: Simulating Household Activities via Programs", "comments": "CVPR 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in modeling complex activities that occur in\na typical household. We propose to use programs, i.e., sequences of atomic\nactions and interactions, as a high level representation of complex tasks.\nPrograms are interesting because they provide a non-ambiguous representation of\na task, and allow agents to execute them. However, nowadays, there is no\ndatabase providing this type of information. Towards this goal, we first\ncrowd-source programs for a variety of activities that happen in people's\nhomes, via a game-like interface used for teaching kids how to code. Using the\ncollected dataset, we show how we can learn to extract programs directly from\nnatural language descriptions or from videos. We then implement the most common\natomic (inter)actions in the Unity3D game engine, and use our programs to\n\"drive\" an artificial agent to execute tasks in a simulated household\nenvironment. Our VirtualHome simulator allows us to create a large activity\nvideo dataset with rich ground-truth, enabling training and testing of video\nunderstanding models. We further showcase examples of our agent performing\ntasks in our VirtualHome based on language descriptions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 02:16:44 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Puig", "Xavier", ""], ["Ra", "Kevin", ""], ["Boben", "Marko", ""], ["Li", "Jiaman", ""], ["Wang", "Tingwu", ""], ["Fidler", "Sanja", ""], ["Torralba", "Antonio", ""]]}, {"id": "1806.07026", "submitter": "Wenxue Cui", "authors": "Wenxue Cui, Feng Jiang, Xinwei Gao, Wen Tao, Debin Zhao", "title": "Deep neural network based sparse measurement matrix for image compressed\n  sensing", "comments": "5 pages, accepted by ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random matrix (GRM) has been widely used to generate linear\nmeasurements in compressed sensing (CS) of natural images. However, there\nactually exist two disadvantages with GRM in practice. One is that GRM has\nlarge memory requirement and high computational complexity, which restrict the\napplications of CS. Another is that the CS measurements randomly obtained by\nGRM cannot provide sufficient reconstruction performances. In this paper, a\nDeep neural network based Sparse Measurement Matrix (DSMM) is learned by the\nproposed convolutional network to reduce the sampling computational complexity\nand improve the CS reconstruction performance. Two sub networks are included in\nthe proposed network, which are the sampling sub-network and the reconstruction\nsub-network. In the sampling sub-network, the sparsity and the normalization\nare both considered by the limitation of the storage and the computational\ncomplexity. In order to improve the CS reconstruction performance, a\nreconstruction sub-network are introduced to help enhance the sampling\nsub-network. So by the offline iterative training of the proposed end-to-end\nnetwork, the DSMM is generated for accurate measurement and excellent\nreconstruction. Experimental results demonstrate that the proposed DSMM\noutperforms GRM greatly on representative CS reconstruction methods\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 02:53:12 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Cui", "Wenxue", ""], ["Jiang", "Feng", ""], ["Gao", "Xinwei", ""], ["Tao", "Wen", ""], ["Zhao", "Debin", ""]]}, {"id": "1806.07049", "submitter": "Huan Fu", "authors": "Huan Fu, Mingming Gong, Chaohui Wang, and Dacheng Tao", "title": "MoE-SPNet: A Mixture-of-Experts Scene Parsing Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene parsing is an indispensable component in understanding the semantics\nwithin a scene. Traditional methods rely on handcrafted local features and\nprobabilistic graphical models to incorporate local and global cues. Recently,\nmethods based on fully convolutional neural networks have achieved new records\non scene parsing. An important strategy common to these methods is the\naggregation of hierarchical features yielded by a deep convolutional neural\nnetwork. However, typical algorithms usually aggregate hierarchical\nconvolutional features via concatenation or linear combination, which cannot\nsufficiently exploit the diversities of contextual information in multi-scale\nfeatures and the spatial inhomogeneity of a scene. In this paper, we propose a\nmixture-of-experts scene parsing network (MoE-SPNet) that incorporates a\nconvolutional mixture-of-experts layer to assess the importance of features\nfrom different levels and at different spatial locations. In addition, we\npropose a variant of mixture-of-experts called the adaptive hierarchical\nfeature aggregation (AHFA) mechanism which can be incorporated into existing\nscene parsing networks that use skip-connections to fuse features layer-wisely.\nIn the proposed networks, different levels of features at each spatial location\nare adaptively re-weighted according to the local structure and surrounding\ncontextual information before aggregation. We demonstrate the effectiveness of\nthe proposed methods on two scene parsing datasets including PASCAL VOC 2012\nand SceneParse150 based on two kinds of baseline models FCN-8s and\nDeepLab-ASPP.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 05:32:49 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Fu", "Huan", ""], ["Gong", "Mingming", ""], ["Wang", "Chaohui", ""], ["Tao", "Dacheng", ""]]}, {"id": "1806.07064", "submitter": "Yi Li Dr.", "authors": "Yi Li, Wei Ping", "title": "Cancer Metastasis Detection With Neural Conditional Random Field", "comments": "9 pages, 5 figures, MIDL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer diagnosis often requires accurate detection of metastasis in\nlymph nodes through Whole-slide Images (WSIs). Recent advances in deep\nconvolutional neural networks (CNNs) have shown significant successes in\nmedical image analysis and particularly in computational histopathology.\nBecause of the outrageous large size of WSIs, most of the methods divide one\nslide into lots of small image patches and perform classification on each patch\nindependently. However, neighboring patches often share spatial correlations,\nand ignoring these spatial correlations may result in inconsistent predictions.\nIn this paper, we propose a neural conditional random field (NCRF) deep\nlearning framework to detect cancer metastasis in WSIs. NCRF considers the\nspatial correlations between neighboring patches through a fully connected CRF\nwhich is directly incorporated on top of a CNN feature extractor. The whole\ndeep network can be trained end-to-end with standard back-propagation algorithm\nwith minor computational overhead from the CRF component. The CNN feature\nextractor can also benefit from considering spatial correlations via the CRF\ncomponent. Compared to the baseline method without considering spatial\ncorrelations, we show that the proposed NCRF framework obtains probability maps\nof patch predictions with better visual quality. We also demonstrate that our\nmethod outperforms the baseline in cancer metastasis detection on the\nCamelyon16 dataset and achieves an average FROC score of 0.8096 on the test\nset. NCRF is open sourced at https://github.com/baidu-research/NCRF.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 06:44:34 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Li", "Yi", ""], ["Ping", "Wei", ""]]}, {"id": "1806.07072", "submitter": "Sauradip Nag", "authors": "Sauradip Nag, Palaiahnakote Shivakumara, Wu Yirui, Umapada Pal, and\n  Tong Lu", "title": "A New COLD Feature based Handwriting Analysis for Ethnicity/Nationality\n  Identification", "comments": "Accepted in ICFHR18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying crime for forensic investigating teams when crimes involve people\nof different nationals is challenging. This paper proposes a new method for\nethnicity (nationality) identification based on Cloud of Line Distribution\n(COLD) features of handwriting components. The proposed method, at first,\nexplores tangent angle for the contour pixels in each row and the mean of\nintensity values of each row in an image for segmenting text lines. For\nsegmented text lines, we use tangent angle and direction of base lines to\nremove rule lines in the image. We use polygonal approximation for finding\ndominant points for contours of edge components. Then the proposed method\nconnects the nearest dominant points of every dominant point, which results in\nline segments of dominant point pairs. For each line segment, the proposed\nmethod estimates angle and length, which gives a point in polar domain. For all\nthe line segments, the proposed method generates dense points in polar domain,\nwhich results in COLD distribution. As character component shapes change,\naccording to nationals, the shape of the distribution changes. This observation\nis extracted based on distance from pixels of distribution to Principal Axis of\nthe distribution. Then the features are subjected to an SVM classifier for\nidentifying nationals. Experiments are conducted on a complex dataset, which\nshow the proposed method is effective and outperforms the existing method\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 07:14:54 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Nag", "Sauradip", ""], ["Shivakumara", "Palaiahnakote", ""], ["Yirui", "Wu", ""], ["Pal", "Umapada", ""], ["Lu", "Tong", ""]]}, {"id": "1806.07078", "submitter": "Bi Li", "authors": "Bi Li, Wenxuan Xie, Wenjun Zeng and Wenyu Liu", "title": "Learning to Update for Object Tracking with Recurrent Meta-learner", "comments": "accepted to TIP 2019", "journal-ref": "IEEE Transactions on Image Processing 2019", "doi": "10.1109/TIP.2019.2900577", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model update lies at the heart of object tracking. Generally, model update is\nformulated as an online learning problem where a target model is learned over\nthe online training set. Our key innovation is to \\emph{formulate the model\nupdate problem in the meta-learning framework and learn the online learning\nalgorithm itself using large numbers of offline videos}, i.e., \\emph{learning\nto update}. The learned updater takes as input the online training set and\noutputs an updated target model. As a first attempt, we design the learned\nupdater based on recurrent neural networks (RNNs) and demonstrate its\napplication in a template-based tracker and a correlation filter-based tracker.\nOur learned updater consistently improves the base trackers and runs faster\nthan realtime on GPU while requiring small memory footprint during testing.\nExperiments on standard benchmarks demonstrate that our learned updater\noutperforms commonly used update baselines including the efficient exponential\nmoving average (EMA)-based update and the well-designed stochastic gradient\ndescent (SGD)-based update. Equipped with our learned updater, the\ntemplate-based tracker achieves state-of-the-art performance among realtime\ntrackers on GPU.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 07:31:37 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 15:02:10 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 03:56:10 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Li", "Bi", ""], ["Xie", "Wenxuan", ""], ["Zeng", "Wenjun", ""], ["Liu", "Wenyu", ""]]}, {"id": "1806.07109", "submitter": "Ya\\\"el Balbastre", "authors": "Ya\\\"el Balbastre, Mikael Brudfors, Kevin Bronik, and John Ashburner", "title": "Diffeomorphic brain shape modelling using Gauss-Newton optimisation", "comments": "8 pages, 4 figures, conference paper, accepted at MICCAI 2018", "journal-ref": "MICCAI 2018. Lecture Notes in Computer Science, vol 11070", "doi": "10.1007/978-3-030-00928-1_97", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape modelling describes methods aimed at capturing the natural variability\nof shapes and commonly relies on probabilistic interpretations of\ndimensionality reduction techniques such as principal component analysis. Due\nto their computational complexity when dealing with dense deformation models\nsuch as diffeomorphisms, previous attempts have focused on explicitly reducing\ntheir dimension, diminishing de facto their flexibility and ability to model\ncomplex shapes such as brains. In this paper, we present a generative model of\nshape that allows the covariance structure of deformations to be captured\nwithout squashing their domain, resulting in better normalisation. An efficient\ninference scheme based on Gauss-Newton optimisation is used, which enables\nprocessing of 3D neuroimaging data. We trained this algorithm on segmented\nbrains from the OASIS database, generating physiologically meaningful\ndeformation trajectories. To prove the model's robustness, we applied it to\nunseen data, which resulted in equivalent fitting scores.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 08:52:09 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Balbastre", "Ya\u00ebl", ""], ["Brudfors", "Mikael", ""], ["Bronik", "Kevin", ""], ["Ashburner", "John", ""]]}, {"id": "1806.07110", "submitter": "Nuno C. Garcia", "authors": "Nuno Garcia, Pietro Morerio, Vittorio Murino", "title": "Modality Distillation with Multiple Stream Networks for Action\n  Recognition", "comments": "Accepted at ECCV 2018; Supp. material at p.16; code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diverse input data modalities can provide complementary cues for several\ntasks, usually leading to more robust algorithms and better performance.\nHowever, while a (training) dataset could be accurately designed to include a\nvariety of sensory inputs, it is often the case that not all modalities could\nbe available in real life (testing) scenarios, where a model has to be\ndeployed. This raises the challenge of how to learn robust representations\nleveraging multimodal data in the training stage, while considering limitations\nat test time, such as noisy or missing modalities.\n  This paper presents a new approach for multimodal video action recognition,\ndeveloped within the unified frameworks of distillation and privileged\ninformation, named generalized distillation. Particularly, we consider the case\nof learning representations from depth and RGB videos, while relying on RGB\ndata only at test time. We propose a new approach to train an hallucination\nnetwork that learns to distill depth features through multiplicative\nconnections of spatiotemporal representations, leveraging soft labels and hard\nlabels, as well as distance between feature maps. We report state-of-the-art\nresults on video action classification on the largest multimodal dataset\navailable for this task, the NTU RGB+D. Code available at\nhttps://github.com/ncgarcia/modality-distillation .\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 08:56:13 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 15:19:56 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Garcia", "Nuno", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""]]}, {"id": "1806.07119", "submitter": "Hui Li", "authors": "Hui Li, Xiao-Jun Wu, Tariq S. Durrani", "title": "Infrared and Visible Image Fusion with ResNet and zero-phase component\n  analysis", "comments": "22pages, 9 figures, 7 tables", "journal-ref": "Journal of Infrared Physics & Technology, available online 12\n  September 2019", "doi": "10.1016/j.infrared.2019.103039", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction and processing tasks play a key role in Image Fusion, and\nthe fusion performance is directly affected by the different features and\nprocessing methods undertaken. By contrast, most of deep learning-based methods\nuse deep features directly without feature extraction or processing. This leads\nto the fusion performance degradation in some cases. To solve these drawbacks,\nwe propose a deep features and zero-phase component analysis (ZCA) based novel\nfusion framework is this paper. Firstly, the residual network (ResNet) is used\nto extract deep features from source images. Then ZCA is utilized to normalize\nthe deep features and obtain initial weight maps. The final weight maps are\nobtained by employing a soft-max operation in association with the initial\nweight maps. Finally, the fused image is reconstructed using a\nweighted-averaging strategy. Compared with the existing fusion methods,\nexperimental results demonstrate that the proposed framework achieves better\nperformance in both objective assessment and visual quality. The code of our\nfusion algorithm is available at\nhttps://github.com/hli1221/imagefusion_resnet50\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 09:15:54 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 04:57:33 GMT"}, {"version": "v3", "created": "Sun, 29 Jul 2018 12:06:40 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 06:41:48 GMT"}, {"version": "v5", "created": "Tue, 18 Dec 2018 07:55:18 GMT"}, {"version": "v6", "created": "Tue, 17 Sep 2019 09:20:32 GMT"}, {"version": "v7", "created": "Fri, 20 Sep 2019 15:10:32 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Li", "Hui", ""], ["Wu", "Xiao-Jun", ""], ["Durrani", "Tariq S.", ""]]}, {"id": "1806.07124", "submitter": "Roshanak Zakizadeh", "authors": "Roshanak Zakizadeh, Michele Sasdelli, Yu Qian, Eduard Vazquez", "title": "FineTag: Multi-attribute Classification at Fine-grained Level in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the extraction of the fine-grained attributes of an\ninstance as a `multi-attribute classification' problem. To this end, we propose\nan end-to-end architecture by adopting the bi-linear Convolutional Neural\nNetwork with the pairwise ranking loss. This is the first time such\narchitecture is applied for the fine-grained attributes classification problem.\nWe compared the proposed method with a competitive deep Convolutional Neural\nNetwork baseline. Extensive experiments show that the proposed method\nattains/outperforms the performance of compared baseline with significantly\nless number of parameters ($40\\times$ less). We demonstrated our approach on\nCUB200 birds dataset whose annotations are adapted in this work for\nmulti-attribute classification at fine-grained level.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 09:27:58 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 11:47:36 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Zakizadeh", "Roshanak", ""], ["Sasdelli", "Michele", ""], ["Qian", "Yu", ""], ["Vazquez", "Eduard", ""]]}, {"id": "1806.07131", "submitter": "Silas {\\O}rting", "authors": "Silas Nyboe {\\O}rting, and Jens Petersen, and Veronika Cheplygina, and\n  Laura H. Thomsen, and Mathilde M W Wille, and Marleen de Bruijne", "title": "Feature learning based on visual similarity triplets in medical image\n  analysis: A case study of emphysema in chest CT scans", "comments": "10 pages. Submitted to LABELS2018 - MICCAI Workshop on Large-scale\n  Annotation of Biomedical data and Expert Label Synthesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised feature learning using convolutional neural networks (CNNs) can\nprovide concise and disease relevant representations of medical images.\nHowever, training CNNs requires annotated image data. Annotating medical images\ncan be a time-consuming task and even expert annotations are subject to\nsubstantial inter- and intra-rater variability. Assessing visual similarity of\nimages instead of indicating specific pathologies or estimating disease\nseverity could allow non-experts to participate, help uncover new patterns, and\npossibly reduce rater variability. We consider the task of assessing emphysema\nextent in chest CT scans. We derive visual similarity triplets from visually\nassessed emphysema extent and learn a low dimensional embedding using CNNs. We\nevaluate the networks on 973 images, and show that the CNNs can learn disease\nrelevant feature representations from derived similarity triplets. To our\nknowledge this is the first medical image application where similarity triplets\nhas been used to learn a feature representation that can be used for embedding\nunseen test images\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 09:43:55 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["\u00d8rting", "Silas Nyboe", ""], ["Petersen", "Jens", ""], ["Cheplygina", "Veronika", ""], ["Thomsen", "Laura H.", ""], ["Wille", "Mathilde M W", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1806.07146", "submitter": "Germonda Mooij", "authors": "Germonda Mooij, Ines Bagulho, Henkjan Huisman", "title": "Automatic segmentation of prostate zones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks have become state-of-the-art techniques for automatic\nmedical image analysis, with the U-net architecture being the most popular at\nthis moment. In this article we report the application of a 3D version of U-net\nto the automatic segmentation of prostate peripheral and transition zones in 3D\nMRI images. Our results are slightly better than recent studies that used 2D\nU-net and handcrafted feature approaches.\n  In addition, we test ideas for improving the 3D U-net setup, by 1) letting\nthe network segment surrounding tissues, making use of the fixed anatomy, and\n2) adjusting the network architecture to reflect the anisotropy in the\ndimensions of the MRI image volumes. While the latter adjustment gave a\nmarginal improvement, the former adjustment showed a significant deterioration\nof the network performance. We were able to explain this deterioration by\ninspecting feature map activations in all layers of the network. We show that\nto segment more tissues the network replaces feature maps that were dedicated\nto detecting prostate peripheral zones, by feature maps detecting the\nsurrounding tissues.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 10:23:15 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Mooij", "Germonda", ""], ["Bagulho", "Ines", ""], ["Huisman", "Henkjan", ""]]}, {"id": "1806.07155", "submitter": "Jun Yu", "authors": "Jun Yu, Xiao-Jun Wu, Josef Kittler", "title": "Semi-supervised Hashing for Semi-Paired Cross-View Retrieval", "comments": "6 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, hashing techniques have gained importance in large-scale retrieval\ntasks because of their retrieval speed. Most of the existing cross-view\nframeworks assume that data are well paired. However, the fully-paired\nmultiview situation is not universal in real applications. The aim of the\nmethod proposed in this paper is to learn the hashing function for semi-paired\ncross-view retrieval tasks. To utilize the label information of partial data,\nwe propose a semi-supervised hashing learning framework which jointly performs\nfeature extraction and classifier learning. The experimental results on two\ndatasets show that our method outperforms several state-of-the-art methods in\nterms of retrieval accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 11:17:37 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Yu", "Jun", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1806.07171", "submitter": "Anguelos Nicolaou", "authors": "Anguelos Nicolaou, Sounak Dey, Vincent Christlein, Andreas Maier,\n  Dimosthenis Karatzas", "title": "Non-deterministic Behavior of Ranking-based Metrics when Evaluating\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding data into vector spaces is a very popular strategy of pattern\nrecognition methods. When distances between embeddings are quantized,\nperformance metrics become ambiguous. In this paper, we present an analysis of\nthe ambiguity quantized distances introduce and provide bounds on the effect.\nWe demonstrate that it can have a measurable effect in empirical data in\nstate-of-the-art systems. We also approach the phenomenon from a computer\nsecurity perspective and demonstrate how someone being evaluated by a third\nparty can exploit this ambiguity and greatly outperform a random predictor\nwithout even access to the input data. We also suggest a simple solution making\nthe performance metrics, which rely on ranking, totally deterministic and\nimpervious to such exploits.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 11:59:10 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 10:43:07 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Nicolaou", "Anguelos", ""], ["Dey", "Sounak", ""], ["Christlein", "Vincent", ""], ["Maier", "Andreas", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1806.07185", "submitter": "Thomas Lucas", "authors": "Thomas Lucas, Corentin Tallec, Jakob Verbeek, Yann Ollivier", "title": "Mixed batches and symmetric discriminators for GAN training", "comments": "Accepted at ICML 2018 (long oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are pow- erful generative models based\non providing feed- back to a generative network via a discriminator network.\nHowever, the discriminator usually as- sesses individual samples. This prevents\nthe dis- criminator from accessing global distributional statistics of\ngenerated samples, and often leads to mode dropping: the generator models only\npart of the target distribution. We propose to feed the discriminator with\nmixed batches of true and fake samples, and train it to predict the ratio of\ntrue samples in the batch. The latter score does not depend on the order of\nsamples in a batch. Rather than learning this invariance, we introduce a\ngeneric permutation-invariant discriminator ar- chitecture. This architecture\nis provably a uni- versal approximator of all symmetric functions.\nExperimentally, our approach reduces mode col- lapse in GANs on two synthetic\ndatasets, and obtains good results on the CIFAR10 and CelebA datasets, both\nqualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 12:39:11 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Lucas", "Thomas", ""], ["Tallec", "Corentin", ""], ["Verbeek", "Jakob", ""], ["Ollivier", "Yann", ""]]}, {"id": "1806.07201", "submitter": "Yue Zhang", "authors": "Yue Zhang, Shun Miao, Tommaso Mansi, Rui Liao", "title": "Task Driven Generative Modeling for Unsupervised Domain Adaptation:\n  Application to X-ray Image Segmentation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-00934-2_67", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic parsing of anatomical objects in X-ray images is critical to many\nclinical applications in particular towards image-guided invention and workflow\nautomation. Existing deep network models require a large amount of labeled\ndata. However, obtaining accurate pixel-wise labeling in X-ray images relies\nheavily on skilled clinicians due to the large overlaps of anatomy and the\ncomplex texture patterns. On the other hand, organs in 3D CT scans preserve\nclearer structures as well as sharper boundaries and thus can be easily\ndelineated. In this paper, we propose a novel model framework for learning\nautomatic X-ray image parsing from labeled CT scans. Specifically, a Dense\nImage-to-Image network (DI2I) for multi-organ segmentation is first trained on\nX-ray like Digitally Reconstructed Radiographs (DRRs) rendered from 3D CT\nvolumes. Then we introduce a Task Driven Generative Adversarial Network\n(TD-GAN) architecture to achieve simultaneous style transfer and parsing for\nunseen real X-ray images. TD-GAN consists of a modified cycle-GAN substructure\nfor pixel-to-pixel translation between DRRs and X-ray images and an added\nmodule leveraging the pre-trained DI2I to enforce segmentation consistency. The\nTD-GAN framework is general and can be easily adapted to other learning tasks.\nIn the numerical experiments, we validate the proposed model on 815 DRRs and\n153 topograms. While the vanilla DI2I without any adaptation fails completely\non segmenting the topograms, the proposed model does not require any topogram\nlabels and is able to provide a promising average dice of 85% which achieves\nthe same level accuracy of supervised training (88%).\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 22:39:35 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Zhang", "Yue", ""], ["Miao", "Shun", ""], ["Mansi", "Tommaso", ""], ["Liao", "Rui", ""]]}, {"id": "1806.07202", "submitter": "Ye Wang", "authors": "Ye Wang and Mi Lu", "title": "An optimized system to solve text-based CAPTCHA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CAPTCHA(Completely Automated Public Turing test to Tell Computers and Humans\nApart) can be used to protect data from auto bots. Countless kinds of CAPTCHAs\nare thus designed, while we most frequently utilize text-based scheme because\nof most convenience and user-friendly way \\cite{bursztein2011text}. Currently,\nvarious types of CAPTCHAs need corresponding segmentation to identify single\ncharacter due to the numerous different segmentation ways. Our goal is to\ndefeat the CAPTCHA, thus firstly the CAPTCHAs need to be split into character\nby character. There isn't a regular segmentation algorithm to obtain the\ndivided characters in all kinds of examples, which means that we have to treat\nthe segmentation individually. In this paper, we build a whole system to defeat\nthe CAPTCHAs as well as achieve state-of-the-art performance. In detail, we\npresent our self-adaptive algorithm to segment different kinds of characters\noptimally, and then utilize both the existing methods and our own constructed\nconvolutional neural network as an extra classifier. Results are provided\nshowing how our system work well towards defeating these CAPTCHAs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 06:52:19 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Wang", "Ye", ""], ["Lu", "Mi", ""]]}, {"id": "1806.07226", "submitter": "Wei Jiang", "authors": "Wei Jiang, Yan Wu", "title": "DFNet: Semantic Segmentation on Panoramic Images with Dynamic Loss\n  Weights and Residual Fusion Block", "comments": "6 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the self-driving and automatic parking, perception is the basic and\ncritical technique, moreover, the detection of lane markings and parking slots\nis an important part of visual perception. In this paper, we use the semantic\nsegmentation method to segment the area and classify the class of lane makings\nand parking slots on panoramic surround view (PSV) dataset. We propose the\nDFNet and make two main contributions, one is dynamic loss weights, and the\nother is residual fusion block (RFB). Dynamic loss weights are varying from\nclasses, calculated according to the pixel number of each class in a batch. RFB\nis composed of two convolutional layers, one pooling layer, and a fusion layer\nto combine the feature maps by pixel multiplication. We evaluate our method on\nPSV dataset, and achieve an advanced result.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 05:09:25 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Jiang", "Wei", ""], ["Wu", "Yan", ""]]}, {"id": "1806.07227", "submitter": "Tewodros Mulugeta Dagnew", "authors": "Tewodros Mulugeta Dagnew, Dalia Coppi, Marcello Pelillo, Rita\n  Cucchiara", "title": "A Graph Transduction Game for Multi-target Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning is a popular class of techniques to learn from\nlabeled and unlabeled data. The paper proposes an application of a recently\nproposed approach of graph transduction that exploits game theoretic notions to\nthe problem of multiple people tracking. Within the proposed framework, targets\nare considered as players of a multi-player non-cooperative game. The\nequilibria of the game is considered as a consistent labeling solution and thus\nan estimation of the target association in the sequence of frames. Patches of\npersons are extracted from the video frames using a HOG based detector and\ntheir similarity is modeled using distances among their covariance matrices.\nThe solution we propose achieves satisfactory results on video surveillance\ndatasets. The experiments show the robustness of the method even with a heavy\nunbalance between the number of labeled and unlabeled input patches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 05:47:35 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 01:54:54 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Dagnew", "Tewodros Mulugeta", ""], ["Coppi", "Dalia", ""], ["Pelillo", "Marcello", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1806.07237", "submitter": "Nima Hatami", "authors": "Nima Hatami, Micha\\\"el Sdika, and H\\'el\\`ene Ratiney", "title": "Magnetic Resonance Spectroscopy Quantification using Deep Learning", "comments": "21st International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance spectroscopy (MRS) is an important technique in biomedical\nresearch and it has the unique capability to give a non-invasive access to the\nbiochemical content (metabolites) of scanned organs. In the literature, the\nquantification (the extraction of the potential biomarkers from the MRS\nsignals) involves the resolution of an inverse problem based on a parametric\nmodel of the metabolite signal. However, poor signal-to-noise ratio (SNR),\npresence of the macromolecule signal or high correlation between metabolite\nspectral patterns can cause high uncertainties for most of the metabolites,\nwhich is one of the main reasons that prevents use of MRS in clinical routine.\nIn this paper, quantification of metabolites in MR Spectroscopic imaging using\ndeep learning is proposed. A regression framework based on the Convolutional\nNeural Networks (CNN) is introduced for an accurate estimation of spectral\nparameters. The proposed model learns the spectral features from a large-scale\nsimulated data set with different variations of human brain spectra and SNRs.\nExperimental results demonstrate the accuracy of the proposed method, compared\nto state of the art standard quantification method (QUEST), on concentration of\n20 metabolites and the macromolecule.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 13:56:56 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Hatami", "Nima", ""], ["Sdika", "Micha\u00ebl", ""], ["Ratiney", "H\u00e9l\u00e8ne", ""]]}, {"id": "1806.07243", "submitter": "William Norcliffe-Brown", "authors": "Will Norcliffe-Brown, Efstathios Vafeias, Sarah Parisot", "title": "Learning Conditioned Graph Structures for Interpretable Visual Question\n  Answering", "comments": "NIPS 2018 (13 pages, 7 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question answering is a challenging problem requiring a combination of\nconcepts from Computer Vision and Natural Language Processing. Most existing\napproaches use a two streams strategy, computing image and question features\nthat are consequently merged using a variety of techniques. Nonetheless, very\nfew rely on higher level image representations, which can capture semantic and\nspatial relationships. In this paper, we propose a novel graph-based approach\nfor Visual Question Answering. Our method combines a graph learner module,\nwhich learns a question specific graph representation of the input image, with\nthe recent concept of graph convolutions, aiming to learn image representations\nthat capture question specific interactions. We test our approach on the VQA v2\ndataset using a simple baseline architecture enhanced by the proposed graph\nlearner module. We obtain promising results with 66.18% accuracy and\ndemonstrate the interpretability of the proposed method. Code can be found at\ngithub.com/aimbrain/vqa-project.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 13:59:05 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 09:32:04 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 10:17:33 GMT"}, {"version": "v4", "created": "Tue, 26 Jun 2018 10:22:43 GMT"}, {"version": "v5", "created": "Thu, 5 Jul 2018 14:24:36 GMT"}, {"version": "v6", "created": "Thu, 1 Nov 2018 14:10:47 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Norcliffe-Brown", "Will", ""], ["Vafeias", "Efstathios", ""], ["Parisot", "Sarah", ""]]}, {"id": "1806.07247", "submitter": "Canyi Lu", "authors": "Canyi Lu", "title": "Tensor-Tensor Product Toolbox", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.03728.\n  Carnegie Mellon University", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tensor-tensor product (t-product) [M. E. Kilmer and C. D. Martin, 2011]\nis a natural generalization of matrix multiplication. Based on t-product, many\noperations on matrix can be extended to tensor cases, including tensor SVD,\ntensor spectral norm, tensor nuclear norm [C. Lu, et al., 2018] and many\nothers. The linear algebraic structure of tensors are similar to the matrix\ncases. We develop a Matlab toolbox to implement several basic operations on\ntensors based on t-product. The toolbox is available at\nhttps://github.com/canyilu/tproduct.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 08:14:42 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 03:23:18 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Lu", "Canyi", ""]]}, {"id": "1806.07272", "submitter": "Syed Zulqarnain Gilani", "authors": "Xiang Yan, Syed Zulqarnain Gilani, Hanlin Qin and Ajmal Mian", "title": "Unsupervised Deep Multi-focus Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have recently been used for multi-focus image\nfusion. However, due to the lack of labeled data for supervised training of\nsuch networks, existing methods have resorted to adding Gaussian blur in\nfocused images to simulate defocus and generate synthetic training data with\nground-truth for supervised learning. Moreover, they classify pixels as focused\nor defocused and leverage the results to construct the fusion weight maps which\nthen necessitates a series of post-processing steps. In this paper, we present\nunsupervised end-to-end learning for directly predicting the fully focused\noutput image from multi-focus input image pairs. The proposed approach uses a\nnovel CNN architecture trained to perform fusion without the need for ground\ntruth fused images and exploits the image structural similarity (SSIM) to\ncalculate the loss; a metric that is widely accepted for fused image quality\nevaluation. Consequently, we are able to utilize {\\em real} benchmark datasets,\ninstead of simulated ones, to train our network. The model is a feed-forward,\nfully convolutional neural network that can process images of variable sizes\nduring test time. Extensive evaluations on benchmark datasets show that our\nmethod outperforms existing state-of-the-art in terms of visual quality and\nobjective evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:15:47 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Yan", "Xiang", ""], ["Gilani", "Syed Zulqarnain", ""], ["Qin", "Hanlin", ""], ["Mian", "Ajmal", ""]]}, {"id": "1806.07286", "submitter": "Mejdi Ben Dkhil", "authors": "Mejdi Ben Dkhil, Ali Wali, and Adel M. Alimi", "title": "Drowsy Driver Detection by EEG Analysis Using Fast Fourier Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we try to analyze drowsiness which is a major factor in many\ntraffic accidents due to the clear decline in the attention and recognition of\ndanger drivers. The object of this work is to develop an automatic method to\nevaluate the drowsiness stage by analysis of EEG signals records. The absolute\nband power of the EEG signal was computed by taking the Fast Fourier Transform\n(FFT) of the time series signal. Finally, the algorithm developed in this work\nhas been improved on eight samples from the Physionet sleep-EDF database.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 14:17:51 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Dkhil", "Mejdi Ben", ""], ["Wali", "Ali", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1806.07370", "submitter": "Yunho Jeon", "authors": "Yunho Jeon, Junmo Kim", "title": "Constructing Fast Network through Deconstruction of Convolution", "comments": "To be appeared in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have achieved great success in various vision\ntasks; however, they incur heavy resource costs. By using deeper and wider\nnetworks, network accuracy can be improved rapidly. However, in an environment\nwith limited resources (e.g., mobile applications), heavy networks may not be\nusable. This study shows that naive convolution can be deconstructed into a\nshift operation and pointwise convolution. To cope with various convolutions,\nwe propose a new shift operation called active shift layer (ASL) that\nformulates the amount of shift as a learnable function with shift parameters.\nThis new layer can be optimized end-to-end through backpropagation and it can\nprovide optimal shift values. Finally, we apply this layer to a light and fast\nnetwork that surpasses existing state-of-the-art networks.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 10:54:27 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 10:58:34 GMT"}, {"version": "v3", "created": "Tue, 11 Sep 2018 12:32:20 GMT"}, {"version": "v4", "created": "Thu, 25 Oct 2018 11:24:20 GMT"}, {"version": "v5", "created": "Wed, 31 Oct 2018 13:28:43 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Jeon", "Yunho", ""], ["Kim", "Junmo", ""]]}, {"id": "1806.07371", "submitter": "Guangxiang Zhu", "authors": "Guangxiang Zhu, Zhiao Huang and Chongjie Zhang", "title": "Object-Oriented Dynamics Predictor", "comments": "Accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization has been one of the major challenges for learning dynamics\nmodels in model-based reinforcement learning. However, previous work on\naction-conditioned dynamics prediction focuses on learning the pixel-level\nmotion and thus does not generalize well to novel environments with different\nobject layouts. In this paper, we present a novel object-oriented framework,\ncalled object-oriented dynamics predictor (OODP), which decomposes the\nenvironment into objects and predicts the dynamics of objects conditioned on\nboth actions and object-to-object relations. It is an end-to-end neural network\nand can be trained in an unsupervised manner. To enable the generalization\nability of dynamics learning, we design a novel CNN-based relation mechanism\nthat is class-specific (rather than object-specific) and exploits the locality\nprinciple. Empirical results show that OODP significantly outperforms previous\nmethods in terms of generalization over novel environments with various object\nlayouts. OODP is able to learn from very few environments and accurately\npredict dynamics in a large number of unseen environments. In addition, OODP\nlearns semantically and visually interpretable dynamics models.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 13:54:36 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 12:01:37 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 05:39:08 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhu", "Guangxiang", ""], ["Huang", "Zhiao", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1806.07372", "submitter": "Feng Tian Ph.D Eng.", "authors": "Feng Tian, Jia Yue, Xing Wan, Kuo-Min Chao, Qinghua Zheng", "title": "Learning Unit State Recognition Based on Multi-channel Data Fusion", "comments": "4 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in MOOC, the current e-learning systems have\nadvantages of alleviating barriers by time differences, and geographically\nspatial separation between teachers and students. However, there has been a\n'lack of supervision' problem that e-learner's learning unit state(LUS) can't\nbe supervised automatically. In this paper, we present a fusion framework\nconsidering three channel data sources: 1) videos/images from a camera, 2) eye\nmovement information tracked by a low solution eye tracker and 3) mouse\nmovement. Based on these data modalities, we propose a novel approach of\nmulti-channel data fusion to explore the learning unit state recognition. We\nalso propose a method to build a learning state recognition model to avoid\nmanually labeling image data. The experiments were carried on our designed\nonline learning prototype system, and we choose CART, Random Forest and GBDT\nregression model to predict e-learner's learning state. The results show that\nmulti-channel data fusion model have a better recognition performance in\ncomparison with single channel model. In addition, a best recognition\nperformance can be reached when image, eye movement and mouse movement features\nare fused.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:59:35 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Tian", "Feng", ""], ["Yue", "Jia", ""], ["Wan", "Xing", ""], ["Chao", "Kuo-Min", ""], ["Zheng", "Qinghua", ""]]}, {"id": "1806.07373", "submitter": "Evan Shelhamer", "authors": "Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alexei A. Efros, Sergey\n  Levine", "title": "Few-Shot Segmentation Propagation with Guided Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based methods for visual segmentation have made progress on\nparticular types of segmentation tasks, but are limited by the necessary\nsupervision, the narrow definitions of fixed tasks, and the lack of control\nduring inference for correcting errors. To remedy the rigidity and annotation\nburden of standard approaches, we address the problem of few-shot segmentation:\ngiven few image and few pixel supervision, segment any images accordingly. We\npropose guided networks, which extract a latent task representation from any\namount of supervision, and optimize our architecture end-to-end for fast,\naccurate few-shot segmentation. Our method can switch tasks without further\noptimization and quickly update when given more guidance. We report the first\nresults for segmentation from one pixel per concept and show real-time\ninteractive video segmentation. Our unified approach propagates pixel\nannotations across space for interactive segmentation, across time for video\nsegmentation, and across scenes for semantic segmentation. Our guided segmentor\nis state-of-the-art in accuracy for the amount of annotation and time. See\nhttp://github.com/shelhamer/revolver for code, models, and more details.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 17:02:03 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Rakelly", "Kate", ""], ["Shelhamer", "Evan", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.07374", "submitter": "Maroua Tounsi", "authors": "Maroua Tounsi, Ikram Moalla, Frank Lebourgeois, Adel M. Alimi", "title": "Multilingual Scene Character Recognition System using Sparse\n  Auto-Encoder for Efficient Local Features Representation in Bag of Features", "comments": "This paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of texts existing in camera-captured images has become an\nimportant issue for a great deal of research during the past few decades. This\ngive birth to Scene Character Recognition (SCR) which is an important step in\nscene text recognition pipeline. In this paper, we extended the Bag of Features\n(BoF)-based model using deep learning for representing features for accurate\nSCR of different languages. In the features coding step, a deep Sparse\nAuto-encoder (SAE)-based strategy was applied to enhance the representative and\ndiscriminative abilities of image features. This deep learning architecture\nprovides more efficient features representation and therefore a better\nrecognition accuracy. Our system was evaluated extensively on all the scene\ncharacter datasets of five different languages. The experimental results proved\nthe efficiency of our system for a multilingual SCR.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 11:21:42 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 11:30:35 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 16:39:20 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 17:24:39 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Tounsi", "Maroua", ""], ["Moalla", "Ikram", ""], ["Lebourgeois", "Frank", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1806.07375", "submitter": "Dorian Tsai", "authors": "Dorian Tsai and Donald G Dansereau and Thierry Peynot and Peter Corke", "title": "Distinguishing Refracted Features using Light Field Cameras with\n  Application to Structure from Motion", "comments": "8 pages, 8 figures, submission to IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots must reliably interact with refractive objects in many applications;\nhowever, refractive objects can cause many robotic vision algorithms to become\nunreliable or even fail, particularly feature-based matching applications, such\nas structure-from-motion. We propose a method to distinguish between refracted\nand Lambertian image features using a light field camera. Specifically, we\npropose to use textural cross-correlation to characterise apparent feature\nmotion in a single light field, and compare this motion to its Lambertian\nequivalent based on 4D light field geometry. Our refracted feature\ndistinguisher has a 34.3% higher rate of detection compared to state-of-the-art\nfor light fields captured with large baselines relative to the refractive\nobject. Our method also applies to light field cameras with much smaller\nbaselines than previously considered, yielding up to 2 times better detection\nfor 2D-refractive objects, such as a sphere, and up to 8 times better for\n1D-refractive objects, such as a cylinder. For structure from motion, we\ndemonstrate that rejecting refracted features using our distinguisher yields up\nto 42.4% lower reprojection error, and lower failure rate when the robot is\napproaching refractive objects. Our method lead to more robust robot vision in\nthe presence of refractive objects.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 06:38:28 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Tsai", "Dorian", ""], ["Dansereau", "Donald G", ""], ["Peynot", "Thierry", ""], ["Corke", "Peter", ""]]}, {"id": "1806.07376", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan, Mehul Bhatt, Srikrishna Vardarajan, Seyed Ali Amirshahi,\n  Stella Yu", "title": "Semantic Analysis of (Reflectional) Visual Symmetry: A Human-Centred\n  Computational Model for Declarative Explainability", "comments": "Preprint of accepted article / Journal: Advances in Cognitive\n  Systems. ( http://www.cogsys.org/journal )", "journal-ref": "Advances in Cognitive Systems. (http://www.cogsys.org/journal),\n  2018", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational model for the semantic interpretation of symmetry\nin naturalistic scenes. Key features include a human-centred representation,\nand a declarative, explainable interpretation model supporting deep semantic\nquestion-answering founded on an integration of methods in knowledge\nrepresentation and deep learning based computer vision. In the backdrop of the\nvisual arts, we showcase the framework's capability to generate human-centred,\nqueryable, relational structures, also evaluating the framework with an\nempirical study on the human perception of visual symmetry. Our framework\nrepresents and is driven by the application of foundational, integrated Vision\nand Knowledge Representation and Reasoning methods for applications in the\narts, and the psychological and social sciences.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 11:47:46 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 17:59:34 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Vardarajan", "Srikrishna", ""], ["Amirshahi", "Seyed Ali", ""], ["Yu", "Stella", ""]]}, {"id": "1806.07377", "submitter": "Shani Gamrian", "authors": "Shani Gamrian, Yoav Goldberg", "title": "Transfer Learning for Related Reinforcement Learning Tasks via\n  Image-to-Image Translation", "comments": "Proceedings of the 36th International Conference on Machine Learning\n  (ICML 2019)", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:2063-2072, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable success of Deep RL in learning control policies from\nraw pixels, the resulting models do not generalize. We demonstrate that a\ntrained agent fails completely when facing small visual changes, and that\nfine-tuning---the common transfer learning paradigm---fails to adapt to these\nchanges, to the extent that it is faster to re-train the model from scratch. We\nshow that by separating the visual transfer task from the control policy we\nachieve substantially better sample efficiency and transfer behavior, allowing\nan agent trained on the source task to transfer well to the target tasks. The\nvisual mapping from the target to the source domain is performed using\nunaligned GANs, resulting in a control policy that can be further improved\nusing imitation learning from imperfect demonstrations. We demonstrate the\napproach on synthetic visual variants of the Breakout game, as well as on\ntransfer between subsequent levels of Road Fighter, a Nintendo car-driving\ngame. A visualization of our approach can be seen in\nhttps://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo .\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 09:25:29 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 15:24:18 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 14:03:11 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 19:43:31 GMT"}, {"version": "v5", "created": "Sat, 2 Feb 2019 16:09:02 GMT"}, {"version": "v6", "created": "Thu, 4 Jul 2019 06:51:34 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Gamrian", "Shani", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1806.07378", "submitter": "Xukun Li", "authors": "Xukun Li and Huaiyu Zhang and Doina Caragea and Muhammad Imran", "title": "Localizing and Quantifying Damage in Social Media Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional post-disaster assessment of damage heavily relies on expensive\nGIS data, especially remote sensing image data. In recent years, social media\nhas become a rich source of disaster information that may be useful in\nassessing damage at a lower cost. Such information includes text (e.g., tweets)\nor images posted by eyewitnesses of a disaster. Most of the existing research\nexplores the use of text in identifying situational awareness information\nuseful for disaster response teams. The use of social media images to assess\ndisaster damage is limited. In this paper, we propose a novel approach, based\non convolutional neural networks and class activation maps, to locate damage in\na disaster image and to quantify the degree of the damage. Our proposed\napproach enables the use of social network images for post-disaster damage\nassessment and provides an inexpensive and feasible alternative to the more\nexpensive GIS approach.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 02:30:23 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Li", "Xukun", ""], ["Zhang", "Huaiyu", ""], ["Caragea", "Doina", ""], ["Imran", "Muhammad", ""]]}, {"id": "1806.07379", "submitter": "Ramon Gonzalez", "authors": "Ramon Gonzalez, Karl Iagnemma", "title": "DeepTerramechanics: Terrain Classification and Slip Estimation for\n  Ground Robots via Deep Learning", "comments": "22 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Terramechanics plays a critical role in the areas of ground vehicles and\nground mobile robots since understanding and estimating the variables\ninfluencing the vehicle-terrain interaction may mean the success or the failure\nof an entire mission. This research applies state-of-the-art algorithms in deep\nlearning to two key problems: estimating wheel slip and classifying the terrain\nbeing traversed by a ground robot. Three data sets collected by ground robotic\nplatforms (MIT single-wheel testbed, MSL Curiosity rover, and tracked robot\nFitorobot) are employed in order to compare the performance of traditional\nmachine learning methods (i.e. Support Vector Machine (SVM) and Multi-layer\nPerceptron (MLP)) against Deep Neural Networks (DNNs) and Convolutional Neural\nNetworks (CNNs). This work also shows the impact that certain tuning parameters\nand the network architecture (MLP, DNN and CNN) play on the performance of\nthose methods. This paper also contributes a deep discussion with the lessons\nlearned in the implementation of DNNs and CNNs and how these methods can be\nextended to solve other problems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 07:29:25 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Gonzalez", "Ramon", ""], ["Iagnemma", "Karl", ""]]}, {"id": "1806.07380", "submitter": "Jingqing Zhang", "authors": "Binbing Liao, Jingqing Zhang, Chao Wu, Douglas McIlwraith, Tong Chen,\n  Shengwen Yang, Yike Guo, Fei Wu", "title": "Deep Sequence Learning with Auxiliary Information for Traffic Prediction", "comments": "KDD 2018. The first two authors share equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting traffic conditions from online route queries is a challenging task\nas there are many complicated interactions over the roads and crowds involved.\nIn this paper, we intend to improve traffic prediction by appropriate\nintegration of three kinds of implicit but essential factors encoded in\nauxiliary information. We do this within an encoder-decoder sequence learning\nframework that integrates the following data: 1) offline geographical and\nsocial attributes. For example, the geographical structure of roads or public\nsocial events such as national celebrations; 2) road intersection information.\nIn general, traffic congestion occurs at major junctions; 3) online crowd\nqueries. For example, when many online queries issued for the same destination\ndue to a public performance, the traffic around the destination will\npotentially become heavier at this location after a while. Qualitative and\nquantitative experiments on a real-world dataset from Baidu have demonstrated\nthe effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:38:22 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Liao", "Binbing", ""], ["Zhang", "Jingqing", ""], ["Wu", "Chao", ""], ["McIlwraith", "Douglas", ""], ["Chen", "Tong", ""], ["Yang", "Shengwen", ""], ["Guo", "Yike", ""], ["Wu", "Fei", ""]]}, {"id": "1806.07381", "submitter": "Dzung Doan Anh", "authors": "Anh-Dzung Doan, Abdul Mohsi Jawaid, Thanh-Toan Do, Tat-Jun Chin", "title": "G2D: from GTA to Data", "comments": "9 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes G2D, a software that enables capturing videos from\nGrand Theft Auto V (GTA V), a popular role playing game set in an expansive\nvirtual city. The target users of our software are computer vision researchers\nwho wish to collect hyper-realistic computer-generated imagery of a city from\nthe street level, under controlled 6DOF camera poses and varying environmental\nconditions (weather, season, time of day, traffic density, etc.).\n  G2D accesses/calls the native functions of the game; hence users can directly\ninteract with G2D while playing the game. Specifically, G2D enables users to\nmanipulate conditions of the virtual environment on the fly, while the gameplay\ncamera is set to automatically retrace a predetermined 6DOF camera pose\ntrajectory within the game coordinate system. Concurrently, automatic screen\ncapture is executed while the virtual environment is being explored. G2D and\nits source code are publicly available at https://goo.gl/SS7fS6\n  In addition, we demonstrate an application of G2D to generate a large-scale\ndataset with groundtruth camera poses for testing structure-from-motion (SfM)\nalgorithms. The dataset and generated 3D point clouds are also made available\nat https://goo.gl/DNzxHx\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:24:03 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Doan", "Anh-Dzung", ""], ["Jawaid", "Abdul Mohsi", ""], ["Do", "Thanh-Toan", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "1806.07382", "submitter": "Xinyu Chen", "authors": "Xinyu Chen, Qiang Guan, Li-Ta Lo, Simon Su, James Ahrens and Trilce\n  Estrada", "title": "In situ TensorView: In situ Visualization of Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks(CNNs) are complex systems. They are trained so\nthey can adapt their internal connections to recognize images, texts and more.\nIt is both interesting and helpful to visualize the dynamics within such deep\nartificial neural networks so that people can understand how these artificial\nnetworks are learning and making predictions. In the field of scientific\nsimulations, visualization tools like Paraview have long been utilized to\nprovide insights and understandings. We present in situ TensorView to visualize\nthe training and functioning of CNNs as if they are systems of scientific\nsimulations. In situ TensorView is a loosely coupled in situ visualization open\nframework that provides multiple viewers to help users to visualize and\nunderstand their networks. It leverages the capability of co-processing from\nParaview to provide real-time visualization during training and predicting\nphases. This avoid heavy I/O overhead for visualizing large dynamic systems.\nOnly a small number of lines of codes are injected in TensorFlow framework. The\nvisualization can provide guidance to adjust the architecture of networks, or\ncompress the pre-trained networks. We showcase visualizing the training of\nLeNet-5 and VGG16 using in situ TensorView.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 22:51:12 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Chen", "Xinyu", ""], ["Guan", "Qiang", ""], ["Lo", "Li-Ta", ""], ["Su", "Simon", ""], ["Ahrens", "James", ""], ["Estrada", "Trilce", ""]]}, {"id": "1806.07383", "submitter": "Ahmed Taha", "authors": "Ahmed Taha, Moustafa Meshry, Xitong Yang, Yi-Ting Chen, Larry Davis", "title": "Two Stream Self-Supervised Learning for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised approach using spatio-temporal signals between\nvideo frames for action recognition. A two-stream architecture is leveraged to\ntangle spatial and temporal representation learning. Our task is formulated as\nboth a sequence verification and spatio-temporal alignment tasks. The former\ntask requires motion temporal structure understanding while the latter couples\nthe learned motion with the spatial representation. The self-supervised\npre-trained weights effectiveness is validated on the action recognition task.\nQuantitative evaluation shows the self-supervised approach competence on three\ndatasets: HMDB51, UCF101, and Honda driving dataset (HDD). Further\ninvestigations to boost performance and generalize validity are still required.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 17:51:35 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Taha", "Ahmed", ""], ["Meshry", "Moustafa", ""], ["Yang", "Xitong", ""], ["Chen", "Yi-Ting", ""], ["Davis", "Larry", ""]]}, {"id": "1806.07409", "submitter": "Thomas Tanay", "authors": "Thomas Tanay, Jerone T. A. Andrews and Lewis D. Griffin", "title": "Built-in Vulnerabilities to Imperceptible Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing models that are robust to small adversarial perturbations of their\ninputs has proven remarkably difficult. In this work we show that the reverse\nproblem---making models more vulnerable---is surprisingly easy. After\npresenting some proofs of concept on MNIST, we introduce a generic tilting\nattack that injects vulnerabilities into the linear layers of pre-trained\nnetworks by increasing their sensitivity to components of low variance in the\ntraining data without affecting their performance on test data. We illustrate\nthis attack on a multilayer perceptron trained on SVHN and use it to design a\nstand-alone adversarial module which we call a steganogram decoder. Finally, we\nshow on CIFAR-10 that a poisoning attack with a poisoning rate as low as 0.1%\ncan induce vulnerabilities to chosen imperceptible backdoor signals in\nstate-of-the-art networks. Beyond their practical implications, these different\nresults shed new light on the nature of the adversarial example phenomenon.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 18:12:36 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 22:20:58 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Tanay", "Thomas", ""], ["Andrews", "Jerone T. A.", ""], ["Griffin", "Lewis D.", ""]]}, {"id": "1806.07416", "submitter": "Aryan Mobiny", "authors": "Aryan Mobiny, Hien Van Nguyen", "title": "Fast CapsNet for Lung Cancer Screening", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-00934-2_82", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is the leading cause of cancer-related deaths in the past several\nyears. A major challenge in lung cancer screening is the detection of lung\nnodules from computed tomography (CT) scans. State-of-the-art approaches in\nautomated lung nodule classification use deep convolutional neural networks\n(CNNs). However, these networks require a large number of training samples to\ngeneralize well. This paper investigates the use of capsule networks (CapsNets)\nas an alternative to CNNs. We show that CapsNets significantly outperforms CNNs\nwhen the number of training samples is small. To increase the computational\nefficiency, our paper proposes a consistent dynamic routing mechanism that\nresults in $3\\times$ speedup of CapsNet. Finally, we show that the original\nimage reconstruction method of CapNets performs poorly on lung nodule data. We\npropose an efficient alternative, called convolutional decoder, that yields\nlower reconstruction error and higher classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 18:26:11 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Mobiny", "Aryan", ""], ["Van Nguyen", "Hien", ""]]}, {"id": "1806.07420", "submitter": "Peter van Beek", "authors": "Peter van Beek", "title": "Improved Image Selection for Stack-Based HDR Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stack-based high dynamic range (HDR) imaging is a technique for achieving a\nlarger dynamic range in an image by combining several low dynamic range images\nacquired at different exposures. Minimizing the set of images to combine, while\nensuring that the resulting HDR image fully captures the scene's irradiance, is\nimportant to avoid long image acquisition and post-processing times. The\nproblem of selecting the set of images has received much attention. However,\nexisting methods either are not fully automatic, can be slow, or can fail to\nfully capture more challenging scenes. In this paper, we propose a fully\nautomatic method for selecting the set of exposures to acquire that is both\nfast and more accurate. We show on an extensive set of benchmark scenes that\nour proposed method leads to improved HDR images as measured against ground\ntruth using the mean squared error, a pixel-based metric, and a visible\ndifference predictor and a quality score, both perception-based metrics.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 18:37:33 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["van Beek", "Peter", ""]]}, {"id": "1806.07421", "submitter": "Vitali Petsiuk", "authors": "Vitali Petsiuk, Abir Das, Kate Saenko", "title": "RISE: Randomized Input Sampling for Explanation of Black-box Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are being used increasingly to automate data analysis\nand decision making, yet their decision-making process is largely unclear and\nis difficult to explain to the end users. In this paper, we address the problem\nof Explainable AI for deep neural networks that take images as input and output\na class probability. We propose an approach called RISE that generates an\nimportance map indicating how salient each pixel is for the model's prediction.\nIn contrast to white-box approaches that estimate pixel importance using\ngradients or other internal network state, RISE works on black-box models. It\nestimates importance empirically by probing the model with randomly masked\nversions of the input image and obtaining the corresponding outputs. We compare\nour approach to state-of-the-art importance extraction methods using both an\nautomatic deletion/insertion metric and a pointing metric based on\nhuman-annotated object segments. Extensive experiments on several benchmark\ndatasets show that our approach matches or exceeds the performance of other\nmethods, including white-box approaches.\n  Project page: http://cs-people.bu.edu/vpetsiuk/rise/\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 18:41:30 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 18:57:58 GMT"}, {"version": "v3", "created": "Tue, 25 Sep 2018 18:16:18 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Petsiuk", "Vitali", ""], ["Das", "Abir", ""], ["Saenko", "Kate", ""]]}, {"id": "1806.07441", "submitter": "Zhiyu Sun", "authors": "Zhiyu Sun, Jia Lu, Stephen Baek", "title": "Wall Stress Estimation of Cerebral Aneurysm based on Zernike\n  Convolutional Neural Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (ConvNets) have demonstrated an exceptional\ncapacity to discern visual patterns from digital images and signals.\nUnfortunately, such powerful ConvNets do not generalize well to\narbitrary-shaped manifolds, where data representation does not fit into a\ntensor-like grid. Hence, many fields of science and engineering, where data\npoints possess some manifold structure, cannot enjoy the full benefits of the\nrecent advances in ConvNets. The aneurysm wall stress estimation problem\nintroduced in this paper is one of many such problems. The problem is\nwell-known to be of a paramount clinical importance, but yet, traditional\nConvNets cannot be applied due to the manifold structure of the data, neither\ndoes the state-of-the-art geometric ConvNets perform well. Motivated by this,\nwe propose a new geometric ConvNet method named ZerNet, which builds upon our\nnovel mathematical generalization of convolution and pooling operations on\nmanifolds. Our study shows that the ZerNet outperforms the other\nstate-of-the-art geometric ConvNets in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 06:51:25 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Sun", "Zhiyu", ""], ["Lu", "Jia", ""], ["Baek", "Stephen", ""]]}, {"id": "1806.07486", "submitter": "Yuanwei Li", "authors": "Yuanwei Li, Bishesh Khanal, Benjamin Hou, Amir Alansary, Juan J.\n  Cerrolaza, Matthew Sinclair, Jacqueline Matthew, Chandni Gupta, Caroline\n  Knight, Bernhard Kainz, Daniel Rueckert", "title": "Standard Plane Detection in 3D Fetal Ultrasound Using an Iterative\n  Transformation Network", "comments": "8 pages, 2 figures, accepted for MICCAI 2018; Added link to source\n  code", "journal-ref": "LNCS 11070 (2018) 392-400", "doi": "10.1007/978-3-030-00928-1_45", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard scan plane detection in fetal brain ultrasound (US) forms a crucial\nstep in the assessment of fetal development. In clinical settings, this is done\nby manually manoeuvring a 2D probe to the desired scan plane. With the advent\nof 3D US, the entire fetal brain volume containing these standard planes can be\neasily acquired. However, manual standard plane identification in 3D volume is\nlabour-intensive and requires expert knowledge of fetal anatomy. We propose a\nnew Iterative Transformation Network (ITN) for the automatic detection of\nstandard planes in 3D volumes. ITN uses a convolutional neural network to learn\nthe relationship between a 2D plane image and the transformation parameters\nrequired to move that plane towards the location/orientation of the standard\nplane in the 3D volume. During inference, the current plane image is passed\niteratively to the network until it converges to the standard plane location.\nWe explore the effect of using different transformation representations as\nregression outputs of ITN. Under a multi-task learning framework, we introduce\nadditional classification probability outputs to the network to act as\nconfidence measures for the regressed transformation parameters in order to\nfurther improve the localisation accuracy. When evaluated on 72 US volumes of\nfetal brain, our method achieves an error of 3.83mm/12.7 degrees and\n3.80mm/12.6 degrees for the transventricular and transcerebellar planes\nrespectively and takes 0.46s per plane. Source code is publicly available at\nhttps://github.com/yuanwei1989/plane-detection.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 22:29:20 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 00:49:04 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Li", "Yuanwei", ""], ["Khanal", "Bishesh", ""], ["Hou", "Benjamin", ""], ["Alansary", "Amir", ""], ["Cerrolaza", "Juan J.", ""], ["Sinclair", "Matthew", ""], ["Matthew", "Jacqueline", ""], ["Gupta", "Chandni", ""], ["Knight", "Caroline", ""], ["Kainz", "Bernhard", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1806.07489", "submitter": "Sara Soltaninejad", "authors": "Sara Soltaninejad, Irene Cheng, and Anup Basu", "title": "Towards the identification of Parkinson's Disease using only T1 MR\n  Images", "comments": "ICSM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's Disease (PD) is one of the most common types of neurological\ndiseases caused by progressive degeneration of dopamin- ergic neurons in the\nbrain. Even though there is no fixed cure for this neurodegenerative disease,\nearlier diagnosis followed by earlier treatment can help patients have a better\nquality of life. Magnetic Resonance Imag- ing (MRI) has been one of the most\npopular diagnostic tool in recent years because it avoids harmful radiations.\nIn this paper, we investi- gate the plausibility of using MRIs for\nautomatically diagnosing PD. Our proposed method has three main steps : 1)\nPreprocessing, 2) Fea- ture Extraction, and 3) Classification. The FreeSurfer\nlibrary is used for the first and the second steps. For classification, three\nmain types of classifiers, including Logistic Regression (LR), Random Forest\n(RF) and Support Vector Machine (SVM), are applied and their classification\nabil- ity is compared. The Parkinsons Progression Markers Initiative (PPMI)\ndata set is used to evaluate the proposed method. The proposed system prove to\nbe promising in assisting the diagnosis of PD.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 22:44:19 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Soltaninejad", "Sara", ""], ["Cheng", "Irene", ""], ["Basu", "Anup", ""]]}, {"id": "1806.07490", "submitter": "Yuanwei Li", "authors": "Yuanwei Li, Chin Pang Ho, Navtej Chahal, Roxy Senior, Meng-Xing Tang", "title": "Myocardial Segmentation of Contrast Echocardiograms Using Random Forests\n  Guided by Shape Model", "comments": "8 pages, 2 figures, accepted for MICCAI 2016", "journal-ref": null, "doi": "10.1007/978-3-319-46726-9_19", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myocardial Contrast Echocardiography (MCE) with micro-bubble contrast agent\nenables myocardial perfusion quantification which is invaluable for the early\ndetection of coronary artery diseases. In this paper, we proposed a new\nsegmentation method called Shape Model guided Random Forests (SMRF) for the\nanalysis of MCE data. The proposed method utilizes a statistical shape model of\nthe myocardium to guide the Random Forest (RF) segmentation in two ways. First,\nwe introduce a novel Shape Model (SM) feature which captures the global\nstructure and shape of the myocardium to produce a more accurate RF probability\nmap. Second, the shape model is fitted to the RF probability map to further\nrefine and constrain the final segmentation to plausible myocardial shapes.\nEvaluated on clinical MCE images from 15 patients, our method obtained\npromising results (Dice=0.81, Jaccard=0.70, MAD=1.68 mm, HD=6.53 mm) and showed\na notable improvement in segmentation accuracy over the classic RF and its\nvariants.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 22:48:50 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Li", "Yuanwei", ""], ["Ho", "Chin Pang", ""], ["Chahal", "Navtej", ""], ["Senior", "Roxy", ""], ["Tang", "Meng-Xing", ""]]}, {"id": "1806.07492", "submitter": "Gustavo Botelho de Souza", "authors": "Gustavo Botelho de Souza, Jo\\~ao Paulo Papa and Aparecido Nilceu\n  Marana", "title": "On the Learning of Deep Local Features for Robust Face Spoofing\n  Detection", "comments": null, "journal-ref": "Proceedings of 31st Conference on Graphics, Patterns and Images\n  (SIBGRAPI) 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometrics emerged as a robust solution for security systems. However, given\nthe dissemination of biometric applications, criminals are developing\ntechniques to circumvent them by simulating physical or behavioral traits of\nlegal users (spoofing attacks). Despite face being a promising characteristic\ndue to its universality, acceptability and presence of cameras almost\neverywhere, face recognition systems are extremely vulnerable to such frauds\nsince they can be easily fooled with common printed facial photographs.\nState-of-the-art approaches, based on Convolutional Neural Networks (CNNs),\npresent good results in face spoofing detection. However, these methods do not\nconsider the importance of learning deep local features from each facial\nregion, even though it is known from face recognition that each facial region\npresents different visual aspects, which can also be exploited for face\nspoofing detection. In this work we propose a novel CNN architecture trained in\ntwo steps for such task. Initially, each part of the neural network learns\nfeatures from a given facial region. Afterwards, the whole model is fine-tuned\non the whole facial images. Results show that such pre-training step allows the\nCNN to learn different local spoofing cues, improving the performance and the\nconvergence speed of the final model, outperforming the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 22:56:17 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 13:00:52 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["de Souza", "Gustavo Botelho", ""], ["Papa", "Jo\u00e3o Paulo", ""], ["Marana", "Aparecido Nilceu", ""]]}, {"id": "1806.07497", "submitter": "Yuanwei Li", "authors": "Yuanwei Li, Chin Pang Ho, Matthieu Toulemonde, Navtej Chahal, Roxy\n  Senior, Meng-Xing Tang", "title": "Fully Automatic Myocardial Segmentation of Contrast Echocardiography\n  Sequence Using Random Forests Guided by Shape Model", "comments": "11 pages, 9 figures, published in TMI", "journal-ref": null, "doi": "10.1109/TMI.2017.2747081", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myocardial contrast echocardiography (MCE) is an imaging technique that\nassesses left ventricle function and myocardial perfusion for the detection of\ncoronary artery diseases. Automatic MCE perfusion quantification is challenging\nand requires accurate segmentation of the myocardium from noisy and\ntime-varying images. Random forests (RF) have been successfully applied to many\nmedical image segmentation tasks. However, the pixel-wise RF classifier ignores\ncontextual relationships between label outputs of individual pixels. RF which\nonly utilizes local appearance features is also susceptible to data suffering\nfrom large intensity variations. In this paper, we demonstrate how to overcome\nthe above limitations of classic RF by presenting a fully automatic\nsegmentation pipeline for myocardial segmentation in full-cycle 2D MCE data.\nSpecifically, a statistical shape model is used to provide shape prior\ninformation that guide the RF segmentation in two ways. First, a novel shape\nmodel (SM) feature is incorporated into the RF framework to generate a more\naccurate RF probability map. Second, the shape model is fitted to the RF\nprobability map to refine and constrain the final segmentation to plausible\nmyocardial shapes. We further improve the performance by introducing a bounding\nbox detection algorithm as a preprocessing step in the segmentation pipeline.\nOur approach on 2D image is further extended to 2D+t sequence which ensures\ntemporal consistency in the resultant sequence segmentations. When evaluated on\nclinical MCE data, our proposed method achieves notable improvement in\nsegmentation accuracy and outperforms other state-of-the-art methods including\nthe classic RF and its variants, active shape model and image registration.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 23:18:07 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Li", "Yuanwei", ""], ["Ho", "Chin Pang", ""], ["Toulemonde", "Matthieu", ""], ["Chahal", "Navtej", ""], ["Senior", "Roxy", ""], ["Tang", "Meng-Xing", ""]]}, {"id": "1806.07527", "submitter": "Mykhaylo Andriluka", "authors": "Mykhaylo Andriluka, Jasper R. R. Uijlings, Vittorio Ferrari", "title": "Fluid Annotation: A Human-Machine Collaboration Interface for Full Image\n  Annotation", "comments": "ACM MultiMedia 2018. Live demo is available at fluidann.appspot.com", "journal-ref": null, "doi": "10.1145/3240508.3241916", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Fluid Annotation, an intuitive human-machine collaboration\ninterface for annotating the class label and outline of every object and\nbackground region in an image. Fluid annotation is based on three principles:\n(I) Strong Machine-Learning aid. We start from the output of a strong neural\nnetwork model, which the annotator can edit by correcting the labels of\nexisting regions, adding new regions to cover missing objects, and removing\nincorrect regions. The edit operations are also assisted by the model. (II)\nFull image annotation in a single pass. As opposed to performing a series of\nsmall annotation tasks in isolation, we propose a unified interface for full\nimage annotation in a single pass. (III) Empower the annotator. We empower the\nannotator to choose what to annotate and in which order. This enables\nconcentrating on what the machine does not already know, i.e. putting human\neffort only on the errors it made. This helps using the annotation budget\neffectively. Through extensive experiments on the COCO+Stuff dataset, we\ndemonstrate that Fluid Annotation leads to accurate annotations very\nefficiently, taking three times less annotation time than the popular LabelMe\ninterface.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 02:32:00 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 06:26:30 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2018 16:54:46 GMT"}, {"version": "v4", "created": "Fri, 21 Sep 2018 09:11:25 GMT"}, {"version": "v5", "created": "Thu, 20 Dec 2018 14:20:56 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Andriluka", "Mykhaylo", ""], ["Uijlings", "Jasper R. R.", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1806.07550", "submitter": "Shilin Zhu", "authors": "Shilin Zhu, Xin Dong, Hao Su", "title": "Binary Ensemble Neural Network: More Bits per Network or More Networks\n  per Bit?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks (BNN) have been studied extensively since they run\ndramatically faster at lower memory and power consumption than floating-point\nnetworks, thanks to the efficiency of bit operations. However, contemporary\nBNNs whose weights and activations are both single bits suffer from severe\naccuracy degradation. To understand why, we investigate the representation\nability, speed and bias/variance of BNNs through extensive experiments. We\nconclude that the error of BNNs is predominantly caused by the intrinsic\ninstability (training time) and non-robustness (train & test time). Inspired by\nthis investigation, we propose the Binary Ensemble Neural Network (BENN) which\nleverages ensemble methods to improve the performance of BNNs with limited\nefficiency cost. While ensemble techniques have been broadly believed to be\nonly marginally helpful for strong classifiers such as deep neural networks,\nour analyses and experiments show that they are naturally a perfect fit to\nboost BNNs. We find that our BENN, which is faster and much more robust than\nstate-of-the-art binary networks, can even surpass the accuracy of the\nfull-precision floating number network with the same architecture.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 04:48:18 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 07:08:37 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhu", "Shilin", ""], ["Dong", "Xin", ""], ["Su", "Hao", ""]]}, {"id": "1806.07554", "submitter": "Sara Soltaninejad", "authors": "Chirag Balakrishna, Sarshar Dadashzadeh, and Sara Soltaninejad", "title": "Automatic detection of lumen and media in the IVUS images using U-Net\n  with VGG16 Encoder", "comments": "10 pages, under review for International Conference of Smart\n  Multimedia (ICSM) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary heart disease is one of the top rank leading cause of mortality in\nthe world which can be because of plaque burden inside the arteries.\nIntravascular Ultrasound (IVUS) has been recognized as power- ful imaging\ntechnology which captures the real time and high resolution images of the\ncoronary arteries and can be used for the analysis of these plaques. The IVUS\nsegmentation involves the extraction of two arterial walls components namely,\nlumen and media. In this paper, we investi- gate the effectiveness of\nConvolutional Neural Networks including U-Net to segment ultrasound scans of\narteries. In particular, the proposed seg- mentation network was built based on\nthe the U-Net with the VGG16 encoder. Experiments were done for evaluating the\nproposed segmen- tation architecture which show promising quantitative and\nqualitative results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 05:01:17 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Balakrishna", "Chirag", ""], ["Dadashzadeh", "Sarshar", ""], ["Soltaninejad", "Sara", ""]]}, {"id": "1806.07564", "submitter": "Javier Ribera", "authors": "Javier Ribera, David G\\\"uera, Yuhao Chen, Edward J. Delp", "title": "Locating Objects Without Bounding Boxes", "comments": "12 pages, double-column, 8 figures, accepted at Computer Vision and\n  Pattern Recognition (CVPR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in convolutional neural networks (CNN) have achieved\nremarkable results in locating objects in images. In these networks, the\ntraining procedure usually requires providing bounding boxes or the maximum\nnumber of expected objects. In this paper, we address the task of estimating\nobject locations without annotated bounding boxes which are typically\nhand-drawn and time consuming to label. We propose a loss function that can be\nused in any fully convolutional network (FCN) to estimate object locations.\nThis loss function is a modification of the average Hausdorff distance between\ntwo unordered sets of points. The proposed method has no notion of bounding\nboxes, region proposals, or sliding windows. We evaluate our method with three\ndatasets designed to locate people's heads, pupil centers and plant centers. We\noutperform state-of-the-art generic object detectors and methods fine-tuned for\npupil tracking.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 05:57:26 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 05:14:07 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ribera", "Javier", ""], ["G\u00fcera", "David", ""], ["Chen", "Yuhao", ""], ["Delp", "Edward J.", ""]]}, {"id": "1806.07568", "submitter": "Jaehong Kim", "authors": "Jaehong Kim, Sungeun Hong, Yongseok Choi, Jiwon Kim", "title": "Doubly Nested Network for Resource-Efficient Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose doubly nested network(DNNet) where all neurons represent their own\nsub-models that solve the same task. Every sub-model is nested both layer-wise\nand channel-wise. While nesting sub-models layer-wise is straight-forward with\ndeep-supervision as proposed in \\cite{xie2015holistically}, channel-wise\nnesting has not been explored in the literature to our best knowledge.\nChannel-wise nesting is non-trivial as neurons between consecutive layers are\nall connected to each other. In this work, we introduce a technique to solve\nthis problem by sorting channels topologically and connecting neurons\naccordingly. For the purpose, channel-causal convolutions are used. Slicing\ndoubly nested network gives a working sub-network. The most notable application\nof our proposed network structure with slicing operation is resource-efficient\ninference. At test time, computing resources such as time and memory available\nfor running the prediction algorithm can significantly vary across devices and\napplications. Given a budget constraint, we can slice the network accordingly\nand use a sub-model for inference within budget, requiring no additional\ncomputation such as training or fine-tuning after deployment. We demonstrate\nthe effectiveness of our approach in several practical scenarios of utilizing\navailable resource efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 06:11:35 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Kim", "Jaehong", ""], ["Hong", "Sungeun", ""], ["Choi", "Yongseok", ""], ["Kim", "Jiwon", ""]]}, {"id": "1806.07574", "submitter": "Kartik Gupta", "authors": "Kartik Gupta, Darius Burschka and Arnav Bhavsar", "title": "Classifying Object Manipulation Actions based on Grasp-types and\n  Motion-Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address a challenging problem of fine-grained and\ncoarse-grained recognition of object manipulation actions. Due to the\nvariations in geometrical and motion constraints, there are different\nmanipulations actions possible to perform different sets of actions with an\nobject. Also, there are subtle movements involved to complete most of object\nmanipulation actions. This makes the task of object manipulation action\nrecognition difficult with only just the motion information. We propose to use\ngrasp and motion-constraints information to recognise and understand action\nintention with different objects. We also provide an extensive experimental\nevaluation on the recent Yale Human Grasping dataset consisting of large set of\n455 manipulation actions. The evaluation involves a) Different contemporary\nmulti-class classifiers, and binary classifiers with one-vs-one multi- class\nvoting scheme, b) Differential comparisons results based on subsets of\nattributes involving information of grasp and motion-constraints, c)\nFine-grained and Coarse-grained object manipulation action recognition based on\nfine-grained as well as coarse-grained grasp type information, and d)\nComparison between Instance level and Sequence level modeling of object\nmanipulation actions. Our results justifies the efficacy of grasp attributes\nfor the task of fine-grained and coarse-grained object manipulation action\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 06:57:34 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Gupta", "Kartik", ""], ["Burschka", "Darius", ""], ["Bhavsar", "Arnav", ""]]}, {"id": "1806.07589", "submitter": "B Uma Shankar", "authors": "Subhasis Banerjee, Sushmita Mitra, Anmol Sharma and B. Uma Shankar", "title": "A CADe System for Gliomas in Brain MRI using Convolutional Neural\n  Networks", "comments": "The paper consists of 11 Pages, 6 Figures, 7 Tables, 56 References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the success of Convolutional Neural Networks (CNN), we develop a\nnovel Computer Aided Detection (CADe) system using CNN for Glioblastoma\nMultiforme (GBM) detection and segmentation from multi channel MRI data. A\ntwo-stage approach first identifies the presence of GBM. This is followed by a\nGBM localization in each \"abnormal\" MR slice. As part of the CADe system, two\nCNN architectures viz. Classification CNN (C-CNN) and Detection CNN (D-CNN) are\nemployed. The CADe system considers MRI data consisting of four sequences\n($T_1$, $T_{1c},$ $T_2$, and $T_{2FLAIR}$) as input, and automatically\ngenerates the bounding boxes encompassing the tumor regions in each slice which\nis deemed abnormal. Experimental results demonstrate that the proposed CADe\nsystem, when used as a preliminary step before segmentation, can allow improved\ndelineation of tumor region while reducing false positives arising in normal\nareas of the brain. The GrowCut method, employed for tumor segmentation,\ntypically requires a foreground and background seed region for initialization.\nHere the algorithm is initialized with seeds automatically generated from the\noutput of the proposed CADe system, thereby resulting in improved performance\nas compared to that using random seeds.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 07:30:38 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Banerjee", "Subhasis", ""], ["Mitra", "Sushmita", ""], ["Sharma", "Anmol", ""], ["Shankar", "B. Uma", ""]]}, {"id": "1806.07592", "submitter": "Michael Thoreau", "authors": "Michael Thoreau, Navinda Kottege", "title": "Deep Similarity Metric Learning for Real-Time Pedestrian Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking by detection is a common approach to solving the Multiple Object\nTracking problem. In this paper we show how learning a deep similarity metric\ncan improve three key aspects of pedestrian tracking on a multiple object\ntracking benchmark. We train a convolutional neural network to learn an\nembedding function in a Siamese configuration on a large person\nre-identification dataset. The offline-trained embedding network is integrated\nin to the tracking formulation to improve performance while retaining real-time\nperformance. The proposed tracker stores appearance metrics while detections\nare strong, using this appearance information to: prevent ID switches,\nassociate tracklets through occlusion, and propose new detections where\ndetector confidence is low. This method achieves competitive results in\nevaluation, especially among online, real-time approaches. We present an\nablative study showing the impact of each of the three uses of our deep\nappearance metric.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 07:45:50 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 14:13:21 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Thoreau", "Michael", ""], ["Kottege", "Navinda", ""]]}, {"id": "1806.07635", "submitter": "Ren\\'e Schuster", "authors": "Patrik Feth, Mohammed Naveed Akram, Ren\\'e Schuster and Oliver\n  Wasenm\\\"uller", "title": "Dynamic Risk Assessment for Vehicles of Higher Automation Levels by Deep\n  Learning", "comments": null, "journal-ref": "International Workshop on Artificial Intelligence Safety\n  Engineering (WAISE) 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicles of higher automation levels require the creation of situation\nawareness. One important aspect of this situation awareness is an understanding\nof the current risk of a driving situation. In this work, we present a novel\napproach for the dynamic risk assessment of driving situations based on images\nof a front stereo camera using deep learning. To this end, we trained a deep\nneural network with recorded monocular images, disparity maps and a risk metric\nfor diverse traffic scenes. Our approach can be used to create the\naforementioned situation awareness of vehicles of higher automation levels and\ncan serve as a heterogeneous channel to systems based on radar or lidar sensors\nthat are used traditionally for the calculation of risk metrics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 09:41:14 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Feth", "Patrik", ""], ["Akram", "Mohammed Naveed", ""], ["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""]]}, {"id": "1806.07644", "submitter": "Gustavo Botelho de Souza", "authors": "Johnatan S. Oliveira, Gustavo B. Souza, Anderson R. Rocha, Fl\\'avio E.\n  Deus and Aparecido N. Marana", "title": "Cross-Domain Deep Face Matching for Real Banking Security Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring the security of transactions is currently one of the major\nchallenges that banking systems deal with. The usage of face for biometric\nauthentication of users is attracting large investments from banks worldwide\ndue to its convenience and acceptability by people, especially in cross-domain\nscenarios, in which facial images from ID documents are compared with digital\nself-portraits (selfies) for the automated opening of new checking accounts,\ne.g, or financial transactions authorization. Actually, the comparison of\nselfies and IDs has also been applied in another wide variety of tasks\nnowadays, such as automated immigration control. The major difficulty in such\nprocess consists in attenuating the differences between the facial images\ncompared given their different domains. In this work, in addition to collecting\na large cross-domain face dataset, with 27,002 real facial images of selfies\nand ID documents (13,501 subjects) captured from the databases of the major\npublic Brazilian bank, we propose a novel architecture for such cross-domain\nmatching problem based on deep features extracted by two well-referenced\nConvolutional Neural Networks (CNN). Results obtained on the dataset collected,\ncalled FaceBank, with accuracy rates higher than 93%, demonstrate the\nrobustness of the proposed approach to the cross-domain face matching problem\nand its feasible application in real banking security systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 10:00:52 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 22:15:39 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 12:56:58 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Oliveira", "Johnatan S.", ""], ["Souza", "Gustavo B.", ""], ["Rocha", "Anderson R.", ""], ["Deus", "Fl\u00e1vio E.", ""], ["Marana", "Aparecido N.", ""]]}, {"id": "1806.07686", "submitter": "Hongliu Cao", "authors": "Hongliu Cao, Simon Bernard, Laurent Heutte and Robert Sabourin", "title": "Dynamic voting in multi-view learning for radiomics applications", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer diagnosis and treatment often require a personalized analysis for each\npatient nowadays, due to the heterogeneity among the different types of tumor\nand among patients. Radiomics is a recent medical imaging field that has shown\nduring the past few years to be promising for achieving this personalization.\nHowever, a recent study shows that most of the state-of-the-art works in\nRadiomics fail to identify this problem as a multi-view learning task and that\nmulti-view learning techniques are generally more efficient. In this work, we\npropose to further investigate the potential of one family of multi-view\nlearning methods based on Multiple Classifiers Systems where one classifier is\nlearnt on each view and all classifiers are combined afterwards. In particular,\nwe propose a random forest based dynamic weighted voting scheme, which\npersonalizes the combination of views for each new patient for classification\ntasks. The proposed method is validated on several real-world Radiomics\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 12:11:48 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 10:18:28 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Cao", "Hongliu", ""], ["Bernard", "Simon", ""], ["Heutte", "Laurent", ""], ["Sabourin", "Robert", ""]]}, {"id": "1806.07688", "submitter": "Breton Minnehan", "authors": "Breton Minnehan and Andreas Savakis", "title": "DEFRAG: Deep Euclidean Feature Representations through Adaptation on the\n  Grassmann Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique for training deep networks with the objective of\nobtaining feature representations that exist in a Euclidean space and exhibit\nstrong clustering behavior. Our desired features representations have three\ntraits: they can be compared using a standard Euclidian distance metric,\nsamples from the same class are tightly clustered, and samples from different\nclasses are well separated. However, most deep networks do not enforce such\nfeature representations. The DEFRAG training technique consists of two steps:\nfirst good feature clustering behavior is encouraged though an auxiliary loss\nfunction based on the Silhouette clustering metric. Then the feature space is\nretracted onto a Grassmann manifold to ensure that the L_2 Norm forms a\nsimilarity metric. The DEFRAG technique achieves state of the art results on\nstandard classification datasets using a relatively small network architecture\nwith significantly fewer parameters than many standard networks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 12:13:53 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Minnehan", "Breton", ""], ["Savakis", "Andreas", ""]]}, {"id": "1806.07697", "submitter": "Zhao Kang", "authors": "Zhao Kang, Xiao Lu, Jinfeng Yi, Zenglin Xu", "title": "Self-weighted Multiple Kernel Learning for Graph-based Clustering and\n  Semi-supervised Classification", "comments": "Accepted by IJCAI 2018, Code is available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple kernel learning (MKL) method is generally believed to perform better\nthan single kernel method. However, some empirical studies show that this is\nnot always true: the combination of multiple kernels may even yield an even\nworse performance than using a single kernel. There are two possible reasons\nfor the failure: (i) most existing MKL methods assume that the optimal kernel\nis a linear combination of base kernels, which may not hold true; and (ii) some\nkernel weights are inappropriately assigned due to noises and carelessly\ndesigned algorithms. In this paper, we propose a novel MKL framework by\nfollowing two intuitive assumptions: (i) each kernel is a perturbation of the\nconsensus kernel; and (ii) the kernel that is close to the consensus kernel\nshould be assigned a large weight. Impressively, the proposed method can\nautomatically assign an appropriate weight to each kernel without introducing\nadditional parameters, as existing methods do. The proposed framework is\nintegrated into a unified framework for graph-based clustering and\nsemi-supervised classification. We have conducted experiments on multiple\nbenchmark datasets and our empirical results verify the superiority of the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 12:46:43 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Kang", "Zhao", ""], ["Lu", "Xiao", ""], ["Yi", "Jinfeng", ""], ["Xu", "Zenglin", ""]]}, {"id": "1806.07715", "submitter": "Weijia Lu", "authors": "Weijia Lu and Jie Shuai and Shuyan Gu and Joel Xue", "title": "Method to Annotate Arrhythmias by Deep Network", "comments": null, "journal-ref": null, "doi": "10.1109/Cybermatics_2018.2018.00307", "report-no": null, "categories": "eess.SP cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study targets to automatically annotate on arrhythmia by deep network.\nThe investigated types include sinus rhythm, asystole (Asys), supraventricular\ntachycardia (Tachy), ventricular flutter or fibrillation (VF/VFL), ventricular\ntachycardia (VT). Methods: 13s limb lead ECG chunks from MIT malignant\nventricular arrhythmia database (VFDB) and MIT normal sinus rhythm database\nwere partitioned into subsets for 5-fold cross validation. These signals were\nresampled to 200Hz, filtered to remove baseline wandering, projected to 2D gray\nspectrum and then fed into a deep network with brand-new structure. In this\nnetwork, a feature vector for a single time point was retrieved by residual\nlayers, from which latent representation was extracted by variational\nautoencoder (VAE). These front portions were trained to meet a certain\nthreshold in loss function, then fixed while training procedure switched to\nremaining bidirectional recurrent neural network (RNN), the very portions to\npredict an arrhythmia category. Attention windows were polynomial lumped on RNN\noutputs for learning from details to outlines. And over sampling was employed\nfor imbalanced data. The trained model was wrapped into docker image for\ndeployment in edge or cloud. Conclusion: Promising sensitivities were achieved\nin four arrhythmias and good precision rates in two ventricular arrhythmias\nwere also observed. Moreover, it was proven that latent representation by VAE,\ncan significantly boost the speed of convergence and accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 00:15:06 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Lu", "Weijia", ""], ["Shuai", "Jie", ""], ["Gu", "Shuyan", ""], ["Xue", "Joel", ""]]}, {"id": "1806.07753", "submitter": "Francisco M. Castro", "authors": "Francisco Manuel Castro, Manuel Jes\\'us Mar\\'in-Jim\\'enez, Nicol\\'as\n  Guil, Nicol\\'as P\\'erez de la Blanca", "title": "Multimodal feature fusion for CNN-based gait recognition: an empirical\n  comparison", "comments": "arXiv admin note: text overlap with arXiv:1603.01006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People identification in video based on the way they walk (i.e. gait) is a\nrelevant task in computer vision using a non-invasive approach. Standard and\ncurrent approaches typically derive gait signatures from sequences of binary\nenergy maps of subjects extracted from images, but this process introduces a\nlarge amount of non-stationary noise, thus, conditioning their efficacy. In\ncontrast, in this paper we focus on the raw pixels, or simple functions derived\nfrom them, letting advanced learning techniques to extract relevant features.\nTherefore, we present a comparative study of different Convolutional Neural\nNetwork (CNN) architectures by using three different modalities (i.e. gray\npixels, optical flow channels and depth maps) on two widely-adopted and\nchallenging datasets: TUM-GAID and CASIA-B. In addition, we perform a\ncomparative study between different early and late fusion methods used to\ncombine the information obtained from each kind of modalities. Our experimental\nresults suggest that (i) the raw pixel values represent a competitive input\nmodality, compared to the traditional state-of-the-art silhouette-based\nfeatures (e.g. GEI), since equivalent or better results are obtained; (ii) the\nfusion of the raw pixel information with information from optical flow and\ndepth maps allows to obtain state-of-the-art results on the gait recognition\ntask with an image resolution several times smaller than the previously\nreported results; and, (iii) the selection and the design of the CNN\narchitecture are critical points that can make a difference between\nstate-of-the-art results or poor ones.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 11:36:22 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 12:27:04 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Castro", "Francisco Manuel", ""], ["Mar\u00edn-Jim\u00e9nez", "Manuel Jes\u00fas", ""], ["Guil", "Nicol\u00e1s", ""], ["de la Blanca", "Nicol\u00e1s P\u00e9rez", ""]]}, {"id": "1806.07754", "submitter": "Ali Diba", "authors": "Ali Diba, Mohsen Fayyaz, Vivek Sharma, M.Mahdi Arzani, Rahman\n  Yousefzadeh, Juergen Gall, Luc Van Gool", "title": "Spatio-Temporal Channel Correlation Networks for Action Classification", "comments": "Accepted in ECCV 2018. arXiv admin note: substantial text overlap\n  with arXiv:1711.08200", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work in this paper is driven by the question if spatio-temporal\ncorrelations are enough for 3D convolutional neural networks (CNN)? Most of the\ntraditional 3D networks use local spatio-temporal features. We introduce a new\nblock that models correlations between channels of a 3D CNN with respect to\ntemporal and spatial features. This new block can be added as a residual unit\nto different parts of 3D CNNs. We name our novel block 'Spatio-Temporal Channel\nCorrelation' (STC). By embedding this block to the current state-of-the-art\narchitectures such as ResNext and ResNet, we improved the performance by 2-3\\%\non Kinetics dataset. Our experiments show that adding STC blocks to current\nstate-of-the-art architectures outperforms the state-of-the-art methods on the\nHMDB51, UCF101 and Kinetics datasets. The other issue in training 3D CNNs is\nabout training them from scratch with a huge labeled dataset to get a\nreasonable performance. So the knowledge learned in 2D CNNs is completely\nignored. Another contribution in this work is a simple and effective technique\nto transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D\nCNN for a stable weight initialization. This allows us to significantly reduce\nthe number of training samples for 3D CNNs. Thus, by fine-tuning this network,\nwe beat the performance of generic and recent methods in 3D CNNs, which were\ntrained on large video datasets, e.g. Sports-1M, and fine-tuned on the target\ndatasets, e.g. HMDB51/UCF101.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 12:43:40 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 06:51:13 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 14:03:04 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Diba", "Ali", ""], ["Fayyaz", "Mohsen", ""], ["Sharma", "Vivek", ""], ["Arzani", "M. Mahdi", ""], ["Yousefzadeh", "Rahman", ""], ["Gall", "Juergen", ""], ["Van Gool", "Luc", ""]]}, {"id": "1806.07755", "submitter": "Gao Huang", "authors": "Qiantong Xu, Gao Huang, Yang Yuan, Chuan Guo, Yu Sun, Felix Wu, Kilian\n  Weinberger", "title": "An empirical study on evaluation metrics of generative adversarial\n  networks", "comments": "arXiv admin note: text overlap with arXiv:1802.03446 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating generative adversarial networks (GANs) is inherently challenging.\nIn this paper, we revisit several representative sample-based evaluation\nmetrics for GANs, and address the problem of how to evaluate the evaluation\nmetrics. We start with a few necessary conditions for metrics to produce\nmeaningful scores, such as distinguishing real from generated samples,\nidentifying mode dropping and mode collapsing, and detecting overfitting. With\na series of carefully designed experiments, we comprehensively investigate\nexisting sample-based metrics and identify their strengths and limitations in\npractical settings. Based on these results, we observe that kernel Maximum Mean\nDiscrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to\nsatisfy most of the desirable properties, provided that the distances between\nsamples are computed in a suitable feature space. Our experiments also unveil\ninteresting properties about the behavior of several popular GAN models, such\nas whether they are memorizing training samples, and how far they are from\nlearning the target distribution.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:01:27 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 00:20:11 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Xu", "Qiantong", ""], ["Huang", "Gao", ""], ["Yuan", "Yang", ""], ["Guo", "Chuan", ""], ["Sun", "Yu", ""], ["Wu", "Felix", ""], ["Weinberger", "Kilian", ""]]}, {"id": "1806.07772", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Bernt Schiele, Mario Fritz", "title": "Accurate and Diverse Sampling of Sequences based on a \"Best of Many\"\n  Sample Objective", "comments": "Added additional references and baselines. (Appeared in CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For autonomous agents to successfully operate in the real world, anticipation\nof future events and states of their environment is a key competence. This\nproblem has been formalized as a sequence extrapolation problem, where a number\nof observations are used to predict the sequence into the future. Real-world\nscenarios demand a model of uncertainty of such predictions, as predictions\nbecome increasingly uncertain -- in particular on long time horizons. While\nimpressive results have been shown on point estimates, scenarios that induce\nmulti-modal distributions over future sequences remain challenging. Our work\naddresses these challenges in a Gaussian Latent Variable model for sequence\nprediction. Our core contribution is a \"Best of Many\" sample objective that\nleads to more accurate and more diverse predictions that better capture the\ntrue variations in real-world sequence data. Beyond our analysis of improved\nmodel fit, our models also empirically outperform prior work on three diverse\ntasks ranging from traffic scenes to weather data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 14:49:45 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 12:56:36 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1806.07777", "submitter": "Anders Eklund", "authors": "Per Welander, Simon Karlsson, Anders Eklund", "title": "Generative Adversarial Networks for Image-to-Image Translation on\n  Multi-Contrast MR Images - A Comparison of CycleGAN and UNIT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In medical imaging, a general problem is that it is costly and time consuming\nto collect high quality data from healthy and diseased subjects. Generative\nadversarial networks (GANs) is a deep learning method that has been developed\nfor synthesizing data. GANs can thereby be used to generate more realistic\ntraining data, to improve classification performance of machine learning\nalgorithms. Another application of GANs is image-to-image translations, e.g.\ngenerating magnetic resonance (MR) images from computed tomography (CT) images,\nwhich can be used to obtain multimodal datasets from a single modality. Here,\nwe evaluate two unsupervised GAN models (CycleGAN and UNIT) for image-to-image\ntranslation of T1- and T2-weighted MR images, by comparing generated synthetic\nMR images to ground truth images. We also evaluate two supervised models; a\nmodification of CycleGAN and a pure generator model. A small perceptual study\nwas also performed to evaluate how visually realistic the synthesized images\nare. It is shown that the implemented GAN models can synthesize visually\nrealistic MR images (incorrectly labeled as real by a human). It is also shown\nthat models producing more visually realistic synthetic images not necessarily\nhave better quantitative error measurements, when compared to ground truth\ndata. Code is available at https://github.com/simontomaskarlsson/GAN-MRI\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 14:56:10 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Welander", "Per", ""], ["Karlsson", "Simon", ""], ["Eklund", "Anders", ""]]}, {"id": "1806.07781", "submitter": "Alexander Khvostikov", "authors": "A. Khvostikov, A. Krylov, O. Kharlova, N. Oleynikova, I. Mikhailov, P.\n  Malkov", "title": "Histological images segmentation of mucous glands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mucous glands lesions analysis and assessing of malignant potential of colon\npolyps are very important tasks of surgical pathology. However, differential\ndiagnosis of colon polyps often seems impossible by classical methods and it is\nnecessary to involve computer methods capable of assessing minimal differences\nto extend the capabilities of the classical pathology examination. Accurate\nsegmentation of mucous glands from histology images is a crucial step to obtain\nreliable morphometric criteria for quantitative diagnostic methods. We review\nmajor trends in histological images segmentation and design a new convolutional\nneural network for mucous gland segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 15:07:31 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Khvostikov", "A.", ""], ["Krylov", "A.", ""], ["Kharlova", "O.", ""], ["Oleynikova", "N.", ""], ["Mikhailov", "I.", ""], ["Malkov", "P.", ""]]}, {"id": "1806.07812", "submitter": "Roman Schaffert", "authors": "Roman Schaffert, Jian Wang, Peter Fischer, Anja Borsdorf, Andreas\n  Maier", "title": "Metric-Driven Learning of Correspondence Weighting for 2-D/3-D Image\n  Registration", "comments": "accepted for publication at the German Conference on Pattern\n  Recognition (GCPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration of pre-operative 3-D volumes to intra-operative 2-D X-ray images\nis important in minimally invasive medical procedures. Rigid registration can\nbe performed by estimating a global rigid motion that optimizes the alignment\nof local correspondences. However, inaccurate correspondences challenge the\nregistration performance. To minimize their influence, we estimate optimal\nweights for correspondences using PointNet. We train the network directly with\nthe criterion to minimize the registration error. We propose an objective\nfunction which includes point-to-plane correspondence-based motion estimation\nand projection error computation, thereby enabling the learning of a weighting\nstrategy that optimally fits the underlying formulation of the registration\ntask in an end-to-end fashion. For single-vertebra registration, we achieve an\naccuracy of 0.74$\\pm$0.26 mm and highly improved robustness. The success rate\nis increased from 79.3 % to 94.3 % and the capture range from 3 mm to 13 mm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:02:27 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 14:44:59 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 12:57:45 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Schaffert", "Roman", ""], ["Wang", "Jian", ""], ["Fischer", "Peter", ""], ["Borsdorf", "Anja", ""], ["Maier", "Andreas", ""]]}, {"id": "1806.07819", "submitter": "G\\\"okhan Yildirim", "authors": "G\\\"okhan Yildirim, Calvin Seward, Urs Bergmann", "title": "Disentangling Multiple Conditional Inputs in GANs", "comments": "5 pages, 9 figures, Paper is accepted to the workshop \"AI for\n  Fashion\" in KDD Conference, 2018, London, United Kingdom", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method that disentangles the effects of multiple\ninput conditions in Generative Adversarial Networks (GANs). In particular, we\ndemonstrate our method in controlling color, texture, and shape of a generated\ngarment image for computer-aided fashion design. To disentangle the effect of\ninput attributes, we customize conditional GANs with consistency loss\nfunctions. In our experiments, we tune one input at a time and show that we can\nguide our network to generate novel and realistic images of clothing articles.\nIn addition, we present a fashion design process that estimates the input\nattributes of an existing garment and modifies them using our generator.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:15:37 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Yildirim", "G\u00f6khan", ""], ["Seward", "Calvin", ""], ["Bergmann", "Urs", ""]]}, {"id": "1806.07822", "submitter": "Tanmay Shankar", "authors": "Tanmay Shankar, Nicholas Rhinehart, Katharina Muelling, Kris M. Kitani", "title": "Learning Neural Parsers with Deterministic Differentiable Imitation\n  Learning", "comments": "Accepted to Conference on Robot Learning, CoRL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the problem of learning to decompose spatial tasks into segments,\nas exemplified by the problem of a painting robot covering a large object.\nInspired by the ability of classical decision tree algorithms to construct\nstructured partitions of their input spaces, we formulate the problem of\ndecomposing objects into segments as a parsing approach. We make the insight\nthat the derivation of a parse-tree that decomposes the object into segments\nclosely resembles a decision tree constructed by ID3, which can be done when\nthe ground-truth available. We learn to imitate an expert parsing oracle, such\nthat our neural parser can generalize to parse natural images without ground\ntruth. We introduce a novel deterministic policy gradient update, DRAG (i.e.,\nDeteRministically AGgrevate) in the form of a deterministic actor-critic\nvariant of AggreVaTeD, to train our neural parser. From another perspective,\nour approach is a variant of the Deterministic Policy Gradient suitable for the\nimitation learning setting. The deterministic policy representation offered by\ntraining our neural parser with DRAG allows it to outperform state of the art\nimitation and reinforcement learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:15:54 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 14:58:04 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Shankar", "Tanmay", ""], ["Rhinehart", "Nicholas", ""], ["Muelling", "Katharina", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1806.07823", "submitter": "Tomas Jakab", "authors": "Tomas Jakab, Ankush Gupta, Hakan Bilen, Andrea Vedaldi", "title": "Unsupervised Learning of Object Landmarks through Conditional Image\n  Generation", "comments": "In NeurIPS 2018. Project page:\n  http://www.robots.ox.ac.uk/~vgg/research/unsupervised_landmarks/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for learning landmark detectors for visual objects (such\nas the eyes and the nose in a face) without any manual supervision. We cast\nthis as the problem of generating images that combine the appearance of the\nobject as seen in a first example image with the geometry of the object as seen\nin a second example image, where the two examples differ by a viewpoint change\nand/or an object deformation. In order to factorize appearance and geometry, we\nintroduce a tight bottleneck in the geometry-extraction process that selects\nand distils geometry-related features. Compared to standard image generation\nproblems, which often use generative adversarial networks, our generation task\nis conditioned on both appearance and geometry and thus is significantly less\nambiguous, to the point that adopting a simple perceptual loss formulation is\nsufficient. We demonstrate that our approach can learn object landmarks from\nsynthetic image deformations or videos, all without manual supervision, while\noutperforming state-of-the-art unsupervised landmark detectors. We further show\nthat our method is applicable to a large variety of datasets - faces, people,\n3D objects, and digits - without any modifications.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:17:00 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 21:56:29 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Jakab", "Tomas", ""], ["Gupta", "Ankush", ""], ["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1806.07836", "submitter": "David K\\\"ugler", "authors": "David K\\\"ugler, Anirban Mukhopadhyay", "title": "How Bad is Good enough: Noisy annotations for instrument pose estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though analysis of Medical Images by Deep Learning achieves unprecedented\nresults across various applications, the effect of \\emph{noisy training\nannotations} is rarely studied in a systematic manner. In Medical Image\nAnalysis, most reports addressing this question concentrate on studying\nsegmentation performance of deep learning classifiers. The absence of\ncontinuous ground truth annotations in these studies limits the value of\nconclusions for applications, where regression is the primary method of choice.\nIn the application of surgical instrument pose estimation, where precision has\na direct clinical impact on patient outcome, studying the effect of \\emph{noisy\nannotations} on deep learning pose estimation techniques is of supreme\nimportance. Real x-ray images are inadequate for this evaluation due to the\nunavailability of ground truth annotations. We circumvent this problem by\ngenerating synthetic radiographs, where the ground truth pose is known and\ntherefore the pose estimation error made by the medical-expert can be estimated\nfrom experiments. Furthermore, this study shows the property of deep neural\nnetworks to learn dominant signals from noisy annotations with sufficient data\nin a regression setting.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:37:35 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["K\u00fcgler", "David", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "1806.07840", "submitter": "Xu Chen", "authors": "En Li and Zhi Zhou and Xu Chen", "title": "Edge Intelligence: On-Demand Deep Learning Model Co-Inference with\n  Device-Edge Synergy", "comments": "ACM SIGCOMM Workshop on Mobile Edge Communications, Budapest,\n  Hungary, August 21-23, 2018. https://dl.acm.org/authorize?N665473", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the backbone technology of machine learning, deep neural networks (DNNs)\nhave have quickly ascended to the spotlight. Running DNNs on\nresource-constrained mobile devices is, however, by no means trivial, since it\nincurs high performance and energy overhead. While offloading DNNs to the cloud\nfor execution suffers unpredictable performance, due to the uncontrolled long\nwide-area network latency. To address these challenges, in this paper, we\npropose Edgent, a collaborative and on-demand DNN co-inference framework with\ndevice-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that\nadaptively partitions DNN computation between device and edge, in order to\nleverage hybrid computation resources in proximity for real-time DNN inference.\n(2) DNN right-sizing that accelerates DNN inference through early-exit at a\nproper intermediate DNN layer to further reduce the computation latency. The\nprototype implementation and extensive evaluations based on Raspberry Pi\ndemonstrate Edgent's effectiveness in enabling on-demand low-latency edge\nintelligence.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:56:54 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 03:36:27 GMT"}, {"version": "v3", "created": "Fri, 14 Sep 2018 02:37:07 GMT"}, {"version": "v4", "created": "Thu, 27 Dec 2018 11:49:55 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "En", ""], ["Zhou", "Zhi", ""], ["Chen", "Xu", ""]]}, {"id": "1806.07844", "submitter": "Alessandro Bay", "authors": "Alessandro Bay, Panagiotis Sidiropoulos, Eduard Vazquez, and Michele\n  Sasdelli", "title": "Hide and Seek tracker: Real-time recovery from target loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the real-time recovery of a video tracker from a\ntarget loss, using information that is already available from the original\ntracker and without a significant computational overhead. More specifically,\nbefore using the tracker output to update the target position we estimate the\ndetection confidence. In the case of a low confidence, the position update is\nrejected and the tracker passes to a single-frame failure mode, during which\nthe patch low-level visual content is used to swiftly update the object\nposition, before recovering from the target loss in the next frame.\nOrthogonally to this improvement, we further enhance the running average method\nused for creating the query model in tracking-through-similarity. The\nexperimental evidence provided by evaluation on standard tracking datasets\n(OTB-50, OTB-100 and OTB-2013) validate that target recovery can be\nsuccessfully achieved without compromising the real-time update of the target\nposition.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:09:02 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Bay", "Alessandro", ""], ["Sidiropoulos", "Panagiotis", ""], ["Vazquez", "Eduard", ""], ["Sasdelli", "Michele", ""]]}, {"id": "1806.07872", "submitter": "Benjamin Burchfiel", "authors": "Benjamin Burchfiel and George Konidaris", "title": "Hybrid Bayesian Eigenobjects: Combining Linear Subspace and Deep Network\n  Methods for 3D Robot Vision", "comments": "To appear in the International Conference on Intelligent Robots\n  (IROS) - Madrid, 2018", "journal-ref": null, "doi": "10.1109/IROS.2018.8593795", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Hybrid Bayesian Eigenobjects (HBEOs), a novel representation for\n3D objects designed to allow a robot to jointly estimate the pose, class, and\nfull 3D geometry of a novel object observed from a single viewpoint in a single\npractical framework. By combining both linear subspace methods and deep\nconvolutional prediction, HBEOs efficiently learn nonlinear object\nrepresentations without directly regressing into high-dimensional space. HBEOs\nalso remove the onerous and generally impractical necessity of input data\nvoxelization prior to inference. We experimentally evaluate the suitability of\nHBEOs to the challenging task of joint pose, class, and shape inference on\nnovel objects and show that, compared to preceding work, HBEOs offer\ndramatically improved performance in all three tasks along with several orders\nof magnitude faster runtime performance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:57:56 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 16:01:19 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Burchfiel", "Benjamin", ""], ["Konidaris", "George", ""]]}, {"id": "1806.07889", "submitter": "Aron Monszpart", "authors": "Aron Monszpart, Paul Guerrero, Duygu Ceylan, Ersin Yumer, Niloy J.\n  Mitra", "title": "iMapper: Interaction-guided Joint Scene and Human Motion Mapping from\n  Monocular Videos", "comments": null, "journal-ref": "Siggraph 2019", "doi": "10.1145/3306346.3322961", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing challenge in scene analysis is the recovery of scene\narrangements under moderate to heavy occlusion, directly from monocular video.\nWhile the problem remains a subject of active research, concurrent advances\nhave been made in the context of human pose reconstruction from monocular\nvideo, including image-space feature point detection and 3D pose recovery.\nThese methods, however, start to fail under moderate to heavy occlusion as the\nproblem becomes severely under-constrained. We approach the problems\ndifferently. We observe that people interact similarly in similar scenes.\nHence, we exploit the correlation between scene object arrangement and motions\nperformed in that scene in both directions: first, typical motions performed\nwhen interacting with objects inform us about possible object arrangements; and\nsecond, object arrangements, in turn, constrain the possible motions.\n  We present iMapper, a data-driven method that focuses on identifying\nhuman-object interactions, and jointly reasons about objects and human movement\nover space-time to recover both a plausible scene arrangement and consistent\nhuman interactions. We first introduce the notion of characteristic\ninteractions as regions in space-time when an informative human-object\ninteraction happens. This is followed by a novel occlusion-aware matching\nprocedure that searches and aligns such characteristic snapshots from an\ninteraction database to best explain the input monocular video. Through\nextensive evaluations, both quantitative and qualitative, we demonstrate that\niMapper significantly improves performance over both dedicated state-of-the-art\nscene analysis and 3D human pose recovery approaches, especially under medium\nto heavy occlusion.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:47:50 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Monszpart", "Aron", ""], ["Guerrero", "Paul", ""], ["Ceylan", "Duygu", ""], ["Yumer", "Ersin", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1806.07908", "submitter": "Moacir Antonelli Ponti", "authors": "Moacir Antonelli Ponti and Gabriel B. Paranhos da Costa", "title": "Como funciona o Deep Learning", "comments": "Book chapter, in Portuguese, 31 pages", "journal-ref": "In: T\\'opicos em Gerenciamento de Dados e Informa\\c{c}\\~oes, SBC,\n  Cap.3, ISBN 978-85-7669-400-7, pp.63-93, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Learning methods are currently the state-of-the-art in many problems\nwhich can be tackled via machine learning, in particular classification\nproblems. However there is still lack of understanding on how those methods\nwork, why they work and what are the limitations involved in using them. In\nthis chapter we will describe in detail the transition from shallow to deep\nnetworks, include examples of code on how to implement them, as well as the\nmain issues one faces when training a deep network. Afterwards, we introduce\nsome theoretical background behind the use of deep models, and discuss their\nlimitations.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 18:04:09 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Ponti", "Moacir Antonelli", ""], ["da Costa", "Gabriel B. Paranhos", ""]]}, {"id": "1806.07987", "submitter": "Alexander Pon", "authors": "Alex D. Pon, Oles Andrienko, Ali Harakeh, Steven L. Waslander", "title": "A Hierarchical Deep Architecture and Mini-Batch Selection Method For\n  Joint Traffic Sign and Light Detection", "comments": "Accepted in the IEEE 15th Conference on Computer and Robot Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traffic light and sign detectors on autonomous cars are integral for road\nscene perception. The literature is abundant with deep learning networks that\ndetect either lights or signs, not both, which makes them unsuitable for\nreal-life deployment due to the limited graphics processing unit (GPU) memory\nand power available on embedded systems. The root cause of this issue is that\nno public dataset contains both traffic light and sign labels, which leads to\ndifficulties in developing a joint detection framework. We present a deep\nhierarchical architecture in conjunction with a mini-batch proposal selection\nmechanism that allows a network to detect both traffic lights and signs from\ntraining on separate traffic light and sign datasets. Our method solves the\noverlapping issue where instances from one dataset are not labelled in the\nother dataset. We are the first to present a network that performs joint\ndetection on traffic lights and signs. We measure our network on the\nTsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small\nTraffic Lights benchmark for traffic light detection and show it outperforms\nthe existing Bosch Small Traffic light state-of-the-art method. We focus on\nautonomous car deployment and show our network is more suitable than others\nbecause of its low memory footprint and real-time image processing time.\nQualitative results can be viewed at https://youtu.be/_YmogPzBXOw\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 21:12:43 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 17:20:44 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Pon", "Alex D.", ""], ["Andrienko", "Oles", ""], ["Harakeh", "Ali", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1806.07996", "submitter": "Dominique Beaini", "authors": "Dominique Beaini, Sofiane Achiche, Yann-Seing Law-Kam Cio, Maxime\n  Raison", "title": "Novel Convolution Kernels for Computer Vision and Shape Analysis based\n  on Electromagnetism", "comments": "Keywords: Shape analysis; Stroke analysis; Computer vision;\n  Electromagnetic potential field; Feature extraction; Image filtering; Image\n  convolution Published in PolyPublie: https://publications.polymtl.ca/3162/", "journal-ref": "Beaini, D., Achiche, S., Law-Kam Cio, Y.-S. & Raison, M. (2018).\n  Novel convolution kernels for computer vision and shape analysis based on\n  electromagnetism (Report). https://publications.polymtl.ca/3162/", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer vision is a growing field with a lot of new applications in\nautomation and robotics, since it allows the analysis of images and shapes for\nthe generation of numerical or analytical information. One of the most used\nmethod of information extraction is image filtering through convolution\nkernels, with each kernel specialized for specific applications. The objective\nof this paper is to present a novel convolution kernels, based on principles of\nelectromagnetic potentials and fields, for a general use in computer vision and\nto demonstrate its usage for shape and stroke analysis. Such filtering\npossesses unique geometrical properties that can be interpreted using well\nunderstood physics theorems. Therefore, this paper focuses on the development\nof the electromagnetic kernels and on their application on images for shape and\nstroke analysis. It also presents several interesting features of\nelectromagnetic kernels, such as resolution, size and orientation independence,\nrobustness to noise and deformation, long distance stroke interaction and\nability to work with 3D images\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 21:31:00 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Beaini", "Dominique", ""], ["Achiche", "Sofiane", ""], ["Cio", "Yann-Seing Law-Kam", ""], ["Raison", "Maxime", ""]]}, {"id": "1806.08015", "submitter": "Ulugbek Kamilov", "authors": "Yu Sun and Ulugbek S. Kamilov", "title": "Stability of Scattering Decoder For Nonlinear Diffractive Imaging", "comments": "in Proceedings of iTWIST'18, Paper-ID: 31, Marseille, France,\n  November, 21-23, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of image reconstruction under multiple light scattering is\nusually formulated as a regularized non-convex optimization. A deep learning\narchitecture, Scattering Decoder (ScaDec), was recently proposed to solve this\nproblem in a purely data-driven fashion. The proposed method was shown to\nsubstantially outperform optimization-based baselines and achieve\nstate-of-the-art results. In this paper, we thoroughly test the robustness of\nScaDec to different permittivity contrasts, number of transmissions, and input\nsignal-to-noise ratios. The results on high-fidelity simulated datasets show\nthat the performance of ScaDec is stable in different settings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 23:00:15 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 13:48:27 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 02:38:12 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2018 17:27:31 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Sun", "Yu", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1806.08028", "submitter": "Ayan Sinha", "authors": "Ayan Sinha, Zhao Chen, Vijay Badrinarayanan and Andrew Rabinovich", "title": "Gradient Adversarial Training of Neural Networks", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose gradient adversarial training, an auxiliary deep learning\nframework applicable to different machine learning problems. In gradient\nadversarial training, we leverage a prior belief that in many contexts,\nsimultaneous gradient updates should be statistically indistinguishable from\neach other. We enforce this consistency using an auxiliary network that\nclassifies the origin of the gradient tensor, and the main network serves as an\nadversary to the auxiliary network in addition to performing standard\ntask-based training. We demonstrate gradient adversarial training for three\ndifferent scenarios: (1) as a defense to adversarial examples we classify\ngradient tensors and tune them to be agnostic to the class of their\ncorresponding example, (2) for knowledge distillation, we do binary\nclassification of gradient tensors derived from the student or teacher network\nand tune the student gradient tensor to mimic the teacher's gradient tensor;\nand (3) for multi-task learning we classify the gradient tensors derived from\ndifferent task loss functions and tune them to be statistically\nindistinguishable. For each of the three scenarios we show the potential of\ngradient adversarial training procedure. Specifically, gradient adversarial\ntraining increases the robustness of a network to adversarial attacks, is able\nto better distill the knowledge from a teacher network to a student network\ncompared to soft targets, and boosts multi-task learning by aligning the\ngradient tensors derived from the task specific loss functions. Overall, our\nexperiments demonstrate that gradient tensors contain latent information about\nwhatever tasks are being trained, and can support diverse machine learning\nproblems when intelligently guided through adversarialization using a auxiliary\nnetwork.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 00:54:07 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Sinha", "Ayan", ""], ["Chen", "Zhao", ""], ["Badrinarayanan", "Vijay", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1806.08037", "submitter": "Qun Liu", "authors": "Manohar Karki, Qun Liu, Robert DiBiano, Saikat Basu, Supratik\n  Mukhopadhyay", "title": "Pixel-level Reconstruction and Classification for Noisy Handwritten\n  Bangla Characters", "comments": "Paper was accepted at the 16th International Conference on Frontiers\n  in Handwriting Recognition (ICFHR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification techniques for images of handwritten characters are\nsusceptible to noise. Quadtrees can be an efficient representation for learning\nfrom sparse features. In this paper, we improve the effectiveness of\nprobabilistic quadtrees by using a pixel level classifier to extract the\ncharacter pixels and remove noise from handwritten character images. The pixel\nlevel denoiser (a deep belief network) uses the map responses obtained from a\npretrained CNN as features for reconstructing the characters eliminating noise.\nWe experimentally demonstrate the effectiveness of our approach by\nreconstructing and classifying a noisy version of handwritten Bangla Numeral\nand Basic Character datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 01:30:30 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Karki", "Manohar", ""], ["Liu", "Qun", ""], ["DiBiano", "Robert", ""], ["Basu", "Saikat", ""], ["Mukhopadhyay", "Supratik", ""]]}, {"id": "1806.08047", "submitter": "Damian Mrowca", "authors": "Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li Fei-Fei,\n  Joshua B. Tenenbaum, Daniel L. K. Yamins", "title": "Flexible Neural Representation for Physics Prediction", "comments": "23 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have a remarkable capacity to understand the physical dynamics of\nobjects in their environment, flexibly capturing complex structures and\ninteractions at multiple levels of detail. Inspired by this ability, we propose\na hierarchical particle-based object representation that covers a wide variety\nof types of three-dimensional objects, including both arbitrary rigid\ngeometrical shapes and deformable materials. We then describe the Hierarchical\nRelation Network (HRN), an end-to-end differentiable neural network based on\nhierarchical graph convolution, that learns to predict physical dynamics in\nthis representation. Compared to other neural network baselines, the HRN\naccurately handles complex collisions and nonrigid deformations, generating\nplausible dynamics predictions at long time scales in novel settings, and\nscaling to large scene configurations. These results demonstrate an\narchitecture with the potential to form the basis of next-generation physics\npredictors for use in computer vision, robotics, and quantitative cognitive\nscience.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 02:19:50 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 05:28:48 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Mrowca", "Damian", ""], ["Zhuang", "Chengxu", ""], ["Wang", "Elias", ""], ["Haber", "Nick", ""], ["Fei-Fei", "Li", ""], ["Tenenbaum", "Joshua B.", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1806.08054", "submitter": "Jiaxiang Wu", "authors": "Jiaxiang Wu, Weidong Huang, Junzhou Huang, Tong Zhang", "title": "Error Compensated Quantized SGD and its Applications to Large-scale\n  Distributed Optimization", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale distributed optimization is of great importance in various\napplications. For data-parallel based distributed learning, the inter-node\ngradient communication often becomes the performance bottleneck. In this paper,\nwe propose the error compensated quantized stochastic gradient descent\nalgorithm to improve the training efficiency. Local gradients are quantized to\nreduce the communication overhead, and accumulated quantization error is\nutilized to speed up the convergence. Furthermore, we present theoretical\nanalysis on the convergence behaviour, and demonstrate its advantage over\ncompetitors. Extensive experiments indicate that our algorithm can compress\ngradients by a factor of up to two magnitudes without performance degradation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 03:21:24 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Wu", "Jiaxiang", ""], ["Huang", "Weidong", ""], ["Huang", "Junzhou", ""], ["Zhang", "Tong", ""]]}, {"id": "1806.08078", "submitter": "Asim Raja", "authors": "Raja Asim", "title": "Finding Original Image Of A Sub Image Using CNNs", "comments": "8 pages, 6 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolututional Neural Networks have achieved state of the art in image\nclassification, object detection and other image related tasks. In this paper I\npresent another use of CNNs i.e. if given a set of images and then giving a\nsingle test image the network identifies that the test image is part of which\nimage from the images given before. This is a task somehow similar to measuring\nimage similarity and can be done using a simple CNN. Doing this task manually\nby looping can be quite a time consuming problem and won't be a generalizable\nsolution. The task is quite similar to doing object detection but for that lots\ntraining data should be given or in the case of sliding window it takes lot of\ntime and my algorithm can work with much fewer examples, is totally\nunsupervised and works much efficiently. Also, I explain that how unsupervised\nalgorithm like K-Means or supervised algorithm like K-NN are not good enough to\nperform this task. The basic idea is that image encodings are collected for\neach image from a CNN, when a test image comes it is replaced by a part of\noriginal image, the encoding is generated using the same network, the frobenius\nnorm is calculated and if it comes under a tolerance level then the test image\nis said to be the part of the original image.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 06:24:08 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Asim", "Raja", ""]]}, {"id": "1806.08089", "submitter": "Daochang Liu", "authors": "Daochang Liu and Tingting Jiang", "title": "Deep Reinforcement Learning for Surgical Gesture Segmentation and\n  Classification", "comments": "8 pages, 2 figures, accepted for MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of surgical gesture is crucial for surgical skill assessment and\nefficient surgery training. Prior works on this task are based on either\nvariant graphical models such as HMMs and CRFs, or deep learning models such as\nRecurrent Neural Networks and Temporal Convolutional Networks. Most of the\ncurrent approaches usually suffer from over-segmentation and therefore low\nsegment-level edit scores. In contrast, we present an essentially different\nmethodology by modeling the task as a sequential decision-making process. An\nintelligent agent is trained using reinforcement learning with hierarchical\nfeatures from a deep model. Temporal consistency is integrated into our action\ndesign and reward mechanism to reduce over-segmentation errors. Experiments on\nJIGSAWS dataset demonstrate that the proposed method performs better than\nstate-of-the-art methods in terms of the edit score and on par in frame-wise\naccuracy. Our code will be released later.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 07:09:43 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Liu", "Daochang", ""], ["Jiang", "Tingting", ""]]}, {"id": "1806.08104", "submitter": "Weifeng Liu", "authors": "Xueqi Ma, Weifeng Liu, Shuying Li, Yicong Zhou", "title": "Hypergraph p-Laplacian Regularization for Remote Sensing Image\n  Recognition", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TGRS.2018.2867570", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is of great importance to preserve locality and similarity information in\nsemi-supervised learning (SSL) based applications. Graph based SSL and manifold\nregularization based SSL including Laplacian regularization (LapR) and\nHypergraph Laplacian regularization (HLapR) are representative SSL methods and\nhave achieved prominent performance by exploiting the relationship of sample\ndistribution. However, it is still a great challenge to exactly explore and\nexploit the local structure of the data distribution. In this paper, we present\nan effect and effective approximation algorithm of Hypergraph p-Laplacian and\nthen propose Hypergraph p-Laplacian regularization (HpLapR) to preserve the\ngeometry of the probability distribution. In particular, p-Laplacian is a\nnonlinear generalization of the standard graph Laplacian and Hypergraph is a\ngeneralization of a standard graph. Therefore, the proposed HpLapR provides\nmore potential to exploiting the local structure preserving. We apply HpLapR to\nlogistic regression and conduct the implementations for remote sensing image\nrecognition. We compare the proposed HpLapR to several popular manifold\nregularization based SSL methods including LapR, HLapR and HpLapR on UC-Merced\ndataset. The experimental results demonstrate the superiority of the proposed\nHpLapR.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 08:28:31 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Ma", "Xueqi", ""], ["Liu", "Weifeng", ""], ["Li", "Shuying", ""], ["Zhou", "Yicong", ""]]}, {"id": "1806.08109", "submitter": "Weifeng Liu", "authors": "Xueqi Ma, Weifeng Liu, Dapeng Tao, Yicong Zhou", "title": "Ensemble p-Laplacian Regularization for Remote Sensing Image Recognition", "comments": "13 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1806.08104", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, manifold regularized semi-supervised learning (MRSSL) received\nconsiderable attention because it successfully exploits the geometry of the\nintrinsic data probability distribution including both labeled and unlabeled\nsamples to leverage the performance of a learning model. As a natural nonlinear\ngeneralization of graph Laplacian, p-Laplacian has been proved having the rich\ntheoretical foundations to better preserve the local structure. However, it is\ndifficult to determine the fitting graph p-Lapalcian i.e. the parameter which\nis a critical factor for the performance of graph p-Laplacian. Therefore, we\ndevelop an ensemble p-Laplacian regularization (EpLapR) to fully approximate\nthe intrinsic manifold of the data distribution. EpLapR incorporates multiple\ngraphs into a regularization term in order to sufficiently explore the\ncomplementation of graph p-Laplacian. Specifically, we construct a fused graph\nby introducing an optimization approach to assign suitable weights on different\np-value graphs. And then, we conduct semi-supervised learning framework on the\nfused graph. Extensive experiments on UC-Merced data set demonstrate the\neffectiveness and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 08:37:48 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Ma", "Xueqi", ""], ["Liu", "Weifeng", ""], ["Tao", "Dapeng", ""], ["Zhou", "Yicong", ""]]}, {"id": "1806.08126", "submitter": "Julien Tierny", "authors": "Guillaume Favelier (PEQUAN), Charles Gueunet (PEQUAN), Attila Gyulassy\n  (SCI Institute), Julien Kitware, Joshua Levine, Jonas Lukasczyk (TU\n  Kaiserslautern), Daisuke Sakurai (ZIB), Maxime Soler (PEQUAN), Julien Tierny\n  (PEQUAN), Will Usher (SCI Institute), Qi Wu (SCI Institute, UC Davis)", "title": "Topological Data Analysis Made Easy with the Topology ToolKit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial presents topological methods for the analysis and visualization\nof scientific data from a user's perspective, with the Topology ToolKit (TTK),\na recently released open-source library for topological data analysis.\nTopological methods have gained considerably in popularity and maturity over\nthe last twenty years and success stories of established methods have been\ndocumented in a wide range of applications (combustion, chemistry,\nastrophysics, material sciences, etc.) with both acquired and simulated data,\nin both post-hoc and in-situ contexts. While reference textbooks have been\npublished on the topic, no tutorial at IEEE VIS has covered this area in recent\nyears, and never at a software level and from a user's point-of-view. This\ntutorial fills this gap by providing a beginner's introduction to topological\nmethods for practitioners, researchers, students, and lecturers. In particular,\ninstead of focusing on theoretical aspects and algorithmic details, this\ntutorial focuses on how topological methods can be useful in practice for\nconcrete data analysis tasks such as segmentation, feature extraction or\ntracking. The tutorial describes in detail how to achieve these tasks with TTK.\nFirst, after an introduction to topological methods and their application in\ndata analysis, a brief overview of TTK's main entry point for end users, namely\nParaView, will be presented. Second, an overview of TTK's main features will be\ngiven. A running example will be described in detail, showcasing how to access\nTTK's features via ParaView, Python, VTK/C++, and C++. Third, hands-on sessions\nwill concretely show how to use TTK in ParaView for multiple, representative\ndata analysis tasks. Fourth, the usage of TTK will be presented for developers,\nin particular by describing several examples of visualization and data analysis\nprojects that were built on top of TTK. Finally, some feedback regarding the\nusage of TTK as a teaching platform for topological analysis will be given.\nPresenters of this tutorial include experts in topological methods, core\nauthors of TTK as well as active users, coming from academia, labs, or\nindustry. A large part of the tutorial will be dedicated to hands-on exercises\nand a rich material package (including TTK pre-installs in virtual machines,\ncode, data, demos, video tutorials, etc.) will be provided to the participants.\nThis tutorial mostly targets students, practitioners and researchers who are\nnot experts in topological methods but who are interested in using them in\ntheir daily tasks. We also target researchers already familiar to topological\nmethods and who are interested in using or contributing to TTK.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:18:58 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Favelier", "Guillaume", "", "PEQUAN"], ["Gueunet", "Charles", "", "PEQUAN"], ["Gyulassy", "Attila", "", "SCI Institute"], ["Kitware", "Julien", "", "TU\n  Kaiserslautern"], ["Levine", "Joshua", "", "TU\n  Kaiserslautern"], ["Lukasczyk", "Jonas", "", "TU\n  Kaiserslautern"], ["Sakurai", "Daisuke", "", "ZIB"], ["Soler", "Maxime", "", "PEQUAN"], ["Tierny", "Julien", "", "PEQUAN"], ["Usher", "Will", "", "SCI Institute"], ["Wu", "Qi", "", "SCI Institute, UC Davis"]]}, {"id": "1806.08129", "submitter": "Romain Bregier", "authors": "Romain Br\\'egier (Inria), Fr\\'ed\\'eric Devernay (PRIMA, IMAGINE),\n  Laetitia Leyrit (LASMEA), James Crowley", "title": "Symmetry Aware Evaluation of 3D Object Detection and Pose Estimation in\n  Scenes of Many Parts in Bulk", "comments": null, "journal-ref": "2017 IEEE International Conference on Computer Vision Workshop\n  (ICCVW), Oct 2017, Venice, France. IEEE", "doi": "10.1109/ICCVW.2017.258", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While 3D object detection and pose estimation has been studied for a long\ntime, its evaluation is not yet completely satisfactory. Indeed, existing\ndatasets typically consist in numerous acquisitions of only a few scenes\nbecause of the tediousness of pose annotation, and existing evaluation\nprotocols cannot handle properly objects with symmetries. This work aims at\naddressing those two points. We first present automatic techniques to produce\nfully annotated RGBD data of many object instances in arbitrary poses, with\nwhich we produce a dataset of thousands of independent scenes of bulk parts\ncomposed of both real and synthetic images. We then propose a consistent\nevaluation methodology suitable for any rigid object, regardless of its\nsymmetries. We illustrate it with two reference object detection and pose\nestimation methods on different objects, and show that incorporating symmetry\nconsiderations into pose estimation methods themselves can lead to significant\nperformance gains. The proposed dataset is available at\nhttp://rbregier.github.io/dataset2017.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:21:35 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Br\u00e9gier", "Romain", "", "Inria"], ["Devernay", "Fr\u00e9d\u00e9ric", "", "PRIMA, IMAGINE"], ["Leyrit", "Laetitia", "", "LASMEA"], ["Crowley", "James", ""]]}, {"id": "1806.08152", "submitter": "Alessandro Masullo", "authors": "Alessandro Masullo, Tilo Burghardt, Dima Damen, Sion Hannuna, Victor\n  Ponce-L\\'opez, Majid Mirmehdi", "title": "CaloriNet: From silhouettes to calorie estimation in private\n  environments", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep fusion architecture, CaloriNet, for the online\nestimation of energy expenditure for free living monitoring in private\nenvironments, where RGB data is discarded and replaced by silhouettes. Our\nfused convolutional neural network architecture is trainable end-to-end, to\nestimate calorie expenditure, using temporal foreground silhouettes alongside\naccelerometer data. The network is trained and cross-validated on a publicly\navailable dataset, SPHERE_RGBD + Inertial_calorie. Results show\nstate-of-the-art minimum error on the estimation of energy expenditure\n(calories per minute), outperforming alternative, standard and single-modal\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 10:09:28 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Masullo", "Alessandro", ""], ["Burghardt", "Tilo", ""], ["Damen", "Dima", ""], ["Hannuna", "Sion", ""], ["Ponce-L\u00f3pez", "Victor", ""], ["Mirmehdi", "Majid", ""]]}, {"id": "1806.08169", "submitter": "Dori Peleg", "authors": "Dori Peleg", "title": "A convex method for classification of groups of examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many applications where it important to perform well on a set of\nexamples as opposed to individual examples. For example in image or video\nclassification the question is does an object appear somewhere in the image or\nvideo while there are several candidates of the object per image or video. In\nthis context, it is not important what is the performance per candidate.\nInstead the performance per group is the ultimate objective.\n  For such problems one popular approach assumes weak supervision where labels\nexist for the entire group and then multiple instance learning is utilized.\nAnother approach is to optimize per candidate, assuming each candidate is\nlabeled, in the belief that this will achieve good performance per group.\n  We will show that better results can be achieved if we offer a new\nmethodology which synthesizes the aforementioned approaches and directly\noptimizes for the final optimization objective while consisting of a convex\noptimization problem which solves the global optimization problem. The benefit\nof grouping examples is demonstrated on an image classification task for\ndetecting polyps in images from capsule endoscopy of the colon. The algorithm\nwas designed to efficiently handle hundreds of millions of examples.\nFurthermore, modifications to the penalty function of the standard SVM\nalgorithm, have proven to significantly improve performance in our test case.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 11:09:46 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Peleg", "Dori", ""]]}, {"id": "1806.08174", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, Josien P. W. Pluim", "title": "Crowd disagreement about medical images is informative", "comments": "Accepted for publication at MICCAI LABELS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers for medical image analysis are often trained with a single\nconsensus label, based on combining labels given by experts or crowds. However,\ndisagreement between annotators may be informative, and thus removing it may\nnot be the best strategy. As a proof of concept, we predict whether a skin\nlesion from the ISIC 2017 dataset is a melanoma or not, based on crowd\nannotations of visual characteristics of that lesion. We compare using the mean\nannotations, illustrating consensus, to standard deviations and other\ndistribution moments, illustrating disagreement. We show that the mean\nannotations perform best, but that the disagreement measures are still\ninformative. We also make the crowd annotations used in this paper available at\n\\url{https://figshare.com/s/5cbbce14647b66286544}.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 11:27:38 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 14:19:41 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "1806.08186", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, David M. J. Tax", "title": "Characterizing multiple instance datasets", "comments": "Published at SIMBAD 2015 workshop", "journal-ref": null, "doi": "10.1007/978-3-319-24261-3_2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many pattern recognition problems, a single feature vector is not\nsufficient to describe an object. In multiple instance learning (MIL), objects\nare represented by sets (\\emph{bags}) of feature vectors (\\emph{instances}).\nThis requires an adaptation of standard supervised classifiers in order to\ntrain and evaluate on these bags of instances. Like for supervised\nclassification, several benchmark datasets and numerous classifiers are\navailable for MIL. When performing a comparison of different MIL classifiers,\nit is important to understand the differences of the datasets, used in the\ncomparison. Seemingly different (based on factors such as dimensionality)\ndatasets may elicit very similar behaviour in classifiers, and vice versa. This\nhas implications for what kind of conclusions may be drawn from the comparison\nresults. We aim to give an overview of the variability of available benchmark\ndatasets and some popular MIL classifiers. We use a dataset dissimilarity\nmeasure, based on the differences between the ROC-curves obtained by different\nclassifiers, and embed this dataset dissimilarity matrix into a low-dimensional\nspace. Our results show that conceptually similar datasets can behave very\ndifferently. We therefore recommend examining such dataset characteristics when\nmaking comparisons between existing and new MIL classifiers.\n  The datasets are available via Figshare at \\url{https://bit.ly/2K9iTja}.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 11:54:49 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Tax", "David M. J.", ""]]}, {"id": "1806.08198", "submitter": "Jin-Dong Dong", "authors": "Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, Min Sun", "title": "DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural\n  Architectures", "comments": "13 pages 9 figures, ECCV 2018 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in Neural Architectural Search (NAS) have achieved\nstate-of-the-art performances in applications such as image classification and\nlanguage modeling. However, these techniques typically ignore device-related\nobjectives such as inference time, memory usage, and power consumption.\nOptimizing neural architecture for device-related objectives is immensely\ncrucial for deploying deep networks on portable devices with limited computing\nresources. We propose DPP-Net: Device-aware Progressive Search for\nPareto-optimal Neural Architectures, optimizing for both device-related (e.g.,\ninference time and memory usage) and device-agnostic (e.g., accuracy and model\nsize) objectives. DPP-Net employs a compact search space inspired by current\nstate-of-the-art mobile CNNs, and further improves search efficiency by\nadopting progressive search (Liu et al. 2017). Experimental results on CIFAR-10\nare poised to demonstrate the effectiveness of Pareto-optimal networks found by\nDPP-Net, for three different devices: (1) a workstation with Titan X GPU, (2)\nNVIDIA Jetson TX1 embedded system, and (3) mobile phone with ARM Cortex-A53.\nCompared to CondenseNet and NASNet (Mobile), DPP-Net achieves better\nperformances: higher accuracy and shorter inference time on various devices.\nAdditional experimental results show that models found by DPP-Net also achieve\nconsiderably-good performance on ImageNet as well.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 12:28:37 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 07:56:33 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Dong", "Jin-Dong", ""], ["Cheng", "An-Chieh", ""], ["Juan", "Da-Cheng", ""], ["Wei", "Wei", ""], ["Sun", "Min", ""]]}, {"id": "1806.08205", "submitter": "Julia Buhmann", "authors": "Julia Buhmann, Renate Krause, Rodrigo Ceballos Lentini, Nils Eckstein,\n  Matthew Cook, Srinivas Turaga, Jan Funke", "title": "Synaptic partner prediction from point annotations in insect brains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput electron microscopy allows recording of lar- ge stacks of\nneural tissue with sufficient resolution to extract the wiring diagram of the\nunderlying neural network. Current efforts to automate this process focus\nmainly on the segmentation of neurons. However, in order to recover a wiring\ndiagram, synaptic partners need to be identi- fied as well. This is especially\nchallenging in insect brains like Drosophila melanogaster, where one\npresynaptic site is associated with multiple post- synaptic elements. Here we\npropose a 3D U-Net architecture to directly identify pairs of voxels that are\npre- and postsynaptic to each other. To that end, we formulate the problem of\nsynaptic partner identification as a classification problem on long-range edges\nbetween voxels to encode both the presence of a synaptic pair and its\ndirection. This formulation allows us to directly learn from synaptic point\nannotations instead of more ex- pensive voxel-based synaptic cleft or vesicle\nannotations. We evaluate our method on the MICCAI 2016 CREMI challenge and\nimprove over the current state of the art, producing 3% fewer errors than the\nnext best method.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 12:42:46 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 09:43:59 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Buhmann", "Julia", ""], ["Krause", "Renate", ""], ["Lentini", "Rodrigo Ceballos", ""], ["Eckstein", "Nils", ""], ["Cook", "Matthew", ""], ["Turaga", "Srinivas", ""], ["Funke", "Jan", ""]]}, {"id": "1806.08235", "submitter": "Omid Kavehei", "authors": "Nhan Duy Truong and Levin Kuhlmann and Mohammad Reza Bonyadi and Omid\n  Kavehei", "title": "Semi-supervised Seizure Prediction with Generative Adversarial Networks", "comments": "6 pages, 5 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1707.01976", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose an approach that can make use of not only labeled\nEEG signals but also the unlabeled ones which is more accessible. We also\nsuggest the use of data fusion to further improve the seizure prediction\naccuracy. Data fusion in our vision includes EEG signals, cardiogram signals,\nbody temperature and time. We use the short-time Fourier transform on 28-s EEG\nwindows as a pre-processing step. A generative adversarial network (GAN) is\ntrained in an unsupervised manner where information of seizure onset is\ndisregarded. The trained Discriminator of the GAN is then used as feature\nextractor. Features generated by the feature extractor are classified by two\nfully-connected layers (can be replaced by any classifier) for the labeled EEG\nsignals. This semi-supervised seizure prediction method achieves area under the\noperating characteristic curve (AUC) of 77.68% and 75.47% for the CHBMIT scalp\nEEG dataset and the Freiburg Hospital intracranial EEG dataset, respectively.\nUnsupervised training without the need of labeling is important because not\nonly it can be performed in real-time during EEG signal recording, but also it\ndoes not require feature engineering effort for each patient.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 07:47:57 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Truong", "Nhan Duy", ""], ["Kuhlmann", "Levin", ""], ["Bonyadi", "Mohammad Reza", ""], ["Kavehei", "Omid", ""]]}, {"id": "1806.08245", "submitter": "Ching Tarn", "authors": "Ching Tarn, Yinan Zhang, Ye Feng", "title": "Reductive Clustering: An Efficient Linear-time Graph-based Divisive\n  Cluster Analysis Approach", "comments": "http://res.ctarn.io/reductive-clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient linear-time graph-based divisive cluster analysis\napproach called Reductive Clustering. The approach tries to reveal the\nhierarchical structural information through reducing the graph into a more\nconcise one repeatedly. With the reductions, the original graph can be divided\ninto subgraphs recursively, and a lite informative dendrogram is constructed\nbased on the divisions. The reduction consists of three steps: selection,\nconnection, and partition. First a subset of vertices of the graph are selected\nas representatives to build a concise graph. The representatives are\nre-connected to maintain a consistent structure with the previous graph. If\npossible, the concise graph is divided into subgraphs, and each subgraph is\nfurther reduced recursively until the termination condition is met. We discuss\nthe approach, along with several selection and connection methods, in detail\nboth theoretically and experimentally in this paper. Our implementations run in\nlinear time and achieve outstanding performance on various types of datasets.\nExperimental results show that they outperform state-of-the-art clustering\nalgorithms with significantly less computing resource requirements.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 13:44:17 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 06:40:56 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2020 12:20:22 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Tarn", "Ching", ""], ["Zhang", "Yinan", ""], ["Feng", "Ye", ""]]}, {"id": "1806.08246", "submitter": "Eric M\\\"uller-Budack", "authors": "Eric M\\\"uller-Budack, Kader Pustu-Iren, Sebastian Diering, Ralph\n  Ewerth", "title": "Finding Person Relations in Image Data of the Internet Archive", "comments": null, "journal-ref": "In: M\\'endez E., Crestani F., Ribeiro C., David G., Lopes J. (eds)\n  Digital Libraries for Open Knowledge. TPDL 2018. Lecture Notes in Computer\n  Science, vol 11057. Springer, Cham", "doi": "10.1007/978-3-030-00066-0_20", "report-no": null, "categories": "cs.DL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multimedia content in the World Wide Web is rapidly growing and contains\nvaluable information for many applications in different domains. For this\nreason, the Internet Archive initiative has been gathering billions of\ntime-versioned web pages since the mid-nineties. However, the huge amount of\ndata is rarely labeled with appropriate metadata and automatic approaches are\nrequired to enable semantic search. Normally, the textual content of the\nInternet Archive is used to extract entities and their possible relations\nacross domains such as politics and entertainment, whereas image and video\ncontent is usually neglected. In this paper, we introduce a system for person\nrecognition in image content of web news stored in the Internet Archive. Thus,\nthe system complements entity recognition in text and allows researchers and\nanalysts to track media coverage and relations of persons more precisely. Based\non a deep learning face recognition approach, we suggest a system that\nautomatically detects persons of interest and gathers sample material, which is\nsubsequently used to identify them in the image data of the Internet Archive.\nWe evaluate the performance of the face recognition system on an appropriate\nstandard benchmark dataset and demonstrate the feasibility of the approach with\ntwo use cases.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 13:48:21 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 13:04:14 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["M\u00fcller-Budack", "Eric", ""], ["Pustu-Iren", "Kader", ""], ["Diering", "Sebastian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1806.08251", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni and Michael S. Ryoo", "title": "Learning Multimodal Representations for Unseen Activities", "comments": null, "journal-ref": "WACV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to learn a joint multimodal representation space that\nenables recognition of unseen activities in videos. We first compare the effect\nof placing various constraints on the embedding space using paired text and\nvideo data. We also propose a method to improve the joint embedding space using\nan adversarial formulation, allowing it to benefit from unpaired text and video\ndata. By using unpaired text data, we show the ability to learn a\nrepresentation that better captures unseen activities.\n  In addition to testing on publicly available datasets, we introduce a new,\nlarge-scale text/video dataset.\n  We experimentally confirm that using paired and unpaired data to learn a\nshared embedding space benefits three difficult tasks (i) zero-shot activity\nclassification, (ii) unsupervised activity discovery, and (iii) unseen activity\ncaptioning, outperforming the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 13:58:49 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 14:37:52 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 17:04:30 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 17:36:54 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1806.08279", "submitter": "Suman Ghosh", "authors": "Arka Ujjal Dey, Suman K. Ghosh, Ernest Valveny", "title": "Don't only Feel Read: Using Scene text to understand advertisements", "comments": "Accepted in CVPR 2018 Workshop: Towards Automatic Understanding of\n  Visual Advertisements (ADS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for automated classification of Advertisement Images,\nusing not just Visual features but also Textual cues extracted from embedded\ntext. Our approach takes inspiration from the assumption that Ad images contain\nmeaningful textual content, that can provide discriminative semantic\ninterpretetion, and can thus aid in classifcation tasks. To this end, we\ndevelop a framework using off-the-shelf components, and demonstrate the\neffectiveness of Textual cues in semantic Classfication tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 14:58:05 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 15:15:33 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 10:04:54 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Dey", "Arka Ujjal", ""], ["Ghosh", "Suman K.", ""], ["Valveny", "Ernest", ""]]}, {"id": "1806.08294", "submitter": "Clara Fernandez Labrador", "authors": "Clara Fernandez-Labrador, Alejandro Perez-Yus, Gonzalo Lopez-Nicolas,\n  Jose J. Guerrero", "title": "Layouts from Panoramic Images with Geometry and Deep Learning", "comments": "8 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel procedure for 3D layout recovery of indoor\nscenes from single 360 degrees panoramic images. With such images, all scene is\nseen at once, allowing to recover closed geometries. Our method combines\nstrategically the accuracy provided by geometric reasoning (lines and vanishing\npoints) with the higher level of data abstraction and pattern recognition\nachieved by deep learning techniques (edge and normal maps). Thus, we extract\nstructural corners from which we generate layout hypotheses of the room\nassuming Manhattan world. The best layout model is selected, achieving good\nperformance on both simple rooms (box-type) and complex shaped rooms (with more\nthan four walls). Experiments of the proposed approach are conducted within two\npublic datasets, SUN360 and Stanford (2D-3D-S) demonstrating the advantages of\nestimating layouts by combining geometry and deep learning and the\neffectiveness of our proposal with respect to the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 15:38:18 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Fernandez-Labrador", "Clara", ""], ["Perez-Yus", "Alejandro", ""], ["Lopez-Nicolas", "Gonzalo", ""], ["Guerrero", "Jose J.", ""]]}, {"id": "1806.08296", "submitter": "Michael Moeller", "authors": "Michael Moeller, Otmar Loffeld, Juergen Gall, Felix Krahmer", "title": "Are good local minima wide in sparse recovery?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of compressed sensing is to exploit representations in suitable\n(overcomplete) dictionaries that allow to recover signals far beyond the\nNyquist rate provided that they admit a sparse representation in the respective\ndictionary. The latter gives rise to the sparse recovery problem of finding the\nbest sparse linear approximation of given data in a given generating system. In\nthis paper we analyze the iterative hard thresholding (IHT) algorithm as one of\nthe most popular greedy methods for solving the sparse recovery problem, and\ndemonstrate that systematically perturbing the IHT algorithm by adding noise to\nintermediate iterates yields improved results. Further improvements can be\nobtained by entirely rephrasing the problem as a parametric deep-learning-type\nof optimization problem. By introducing perturbations via dropout, we\ndemonstrate to significantly outperform the classical IHT algorithm, obtaining\n$3$ to $6$ times lower average objective errors.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 15:39:22 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Moeller", "Michael", ""], ["Loffeld", "Otmar", ""], ["Gall", "Juergen", ""], ["Krahmer", "Felix", ""]]}, {"id": "1806.08338", "submitter": "Saeed Izadi", "authors": "Saeed Izadi, Kathleen P. Moriarty, Ghassan Hamarneh", "title": "Can Deep Learning Relax Endomicroscopy Hardware Miniaturization\n  Requirements?", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confocal laser endomicroscopy (CLE) is a novel imaging modality that provides\nin vivo histological cross-sections of examined tissue. Recently, attempts have\nbeen made to develop miniaturized in vivo imaging devices, specifically\nconfocal laser microscopes, for both clinical and research applications.\nHowever, current implementations of miniature CLE components, such as confocal\nlenses, compromise image resolution, signal-to-noise ratio, or both, which\nnegatively impacts the utility of in vivo imaging. In this work, we demonstrate\nthat software-based techniques can be used to recover lost information due to\nendomicroscopy hardware miniaturization and reconstruct images of higher\nresolution. Particularly, a densely connected convolutional neural network is\nused to reconstruct a high-resolution CLE image from a low-resolution input. In\nthe proposed network, each layer is directly connected to all subsequent\nlayers, which results in an effective combination of low-level and high-level\nfeatures and efficient information flow throughout the network. To train and\nevaluate our network, we use a dataset of 181 high-resolution CLE images. Both\nquantitative and qualitative results indicate superiority of the proposed\nnetwork compared to traditional interpolation techniques and competing\nlearning-based methods. This work demonstrates that software-based\nsuper-resolution is a viable approach to compensate for loss of resolution due\nto endoscopic hardware miniaturization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:28:00 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Izadi", "Saeed", ""], ["Moriarty", "Kathleen P.", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1806.08342", "submitter": "Raghuraman Krishnamoorthi", "authors": "Raghuraman Krishnamoorthi", "title": "Quantizing deep convolutional networks for efficient inference: A\n  whitepaper", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present an overview of techniques for quantizing convolutional neural\nnetworks for inference with integer weights and activations. Per-channel\nquantization of weights and per-layer quantization of activations to 8-bits of\nprecision post-training produces classification accuracies within 2% of\nfloating point networks for a wide variety of CNN architectures. Model sizes\ncan be reduced by a factor of 4 by quantizing weights to 8-bits, even when\n8-bit arithmetic is not supported. This can be achieved with simple, post\ntraining quantization of weights.We benchmark latencies of quantized networks\non CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations\ncompared to floating point on CPUs. Speedups of up to 10x are observed on\nspecialized processors with fixed point SIMD capabilities, like the Qualcomm\nQDSPs with HVX.\n  Quantization-aware training can provide further improvements, reducing the\ngap to floating point to 1% at 8-bit precision. Quantization-aware training\nalso allows for reducing the precision of weights to four bits with accuracy\nlosses ranging from 2% to 10%, with higher accuracy drop for smaller\nnetworks.We introduce tools in TensorFlow and TensorFlowLite for quantizing\nconvolutional networks and review best practices for quantization-aware\ntraining to obtain high accuracy with quantized weights and activations. We\nrecommend that per-channel quantization of weights and per-layer quantization\nof activations be the preferred quantization scheme for hardware acceleration\nand kernel optimization. We also propose that future processors and hardware\naccelerators for optimized inference support precisions of 4, 8 and 16 bits.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:32:46 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Krishnamoorthi", "Raghuraman", ""]]}, {"id": "1806.08354", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Yide Shentu, Dian Chen, Pulkit Agrawal, Trevor Darrell,\n  Sergey Levine, Jitendra Malik", "title": "Learning Instance Segmentation by Interaction", "comments": "Website at https://pathak22.github.io/seg-by-interaction/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for building an active agent that learns to segment\nits visual observations into individual objects by interacting with its\nenvironment in a completely self-supervised manner. The agent uses its current\nsegmentation model to infer pixels that constitute objects and refines the\nsegmentation model by interacting with these pixels. The model learned from\nover 50K interactions generalizes to novel objects and backgrounds. To deal\nwith noisy training signal for segmenting objects obtained by self-supervised\ninteractions, we propose robust set loss. A dataset of robot's interactions\nalong-with a few human labeled examples is provided as a benchmark for future\nresearch. We test the utility of the learned segmentation model by providing\nresults on a downstream vision-based control task of rearranging multiple\nobjects into target configurations from visual inputs alone. Videos, code, and\nrobotic interaction dataset are available at\nhttps://pathak22.github.io/seg-by-interaction/\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:59:09 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Pathak", "Deepak", ""], ["Shentu", "Yide", ""], ["Chen", "Dian", ""], ["Agrawal", "Pulkit", ""], ["Darrell", "Trevor", ""], ["Levine", "Sergey", ""], ["Malik", "Jitendra", ""]]}, {"id": "1806.08409", "submitter": "Chiori Hori Dr.", "authors": "Chiori Hori, Huda Alamri, Jue Wang, Gordon Wichern, Takaaki Hori,\n  Anoop Cherian, Tim K. Marks, Vincent Cartillier, Raphael Gontijo Lopes,\n  Abhishek Das, Irfan Essa, Dhruv Batra, Devi Parikh", "title": "End-to-End Audio Visual Scene-Aware Dialog using Multimodal\n  Attention-Based Video Features", "comments": "A prototype system for the Audio Visual Scene-aware Dialog (AVSD) at\n  DSTC7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog systems need to understand dynamic visual scenes in order to have\nconversations with users about the objects and events around them. Scene-aware\ndialog systems for real-world applications could be developed by integrating\nstate-of-the-art technologies from multiple research areas, including:\nend-to-end dialog technologies, which generate system responses using models\ntrained from dialog data; visual question answering (VQA) technologies, which\nanswer questions about images using learned image features; and video\ndescription technologies, in which descriptions/captions are generated from\nvideos using multimodal information. We introduce a new dataset of dialogs\nabout videos of human behaviors. Each dialog is a typed conversation that\nconsists of a sequence of 10 question-and-answer(QA) pairs between two Amazon\nMechanical Turk (AMT) workers. In total, we collected dialogs on roughly 9,000\nvideos. Using this new dataset for Audio Visual Scene-aware dialog (AVSD), we\ntrained an end-to-end conversation model that generates responses in a dialog\nabout a video. Our experiments demonstrate that using multimodal features that\nwere developed for multimodal attention-based video description enhances the\nquality of generated dialog about dynamic scenes (videos). Our dataset, model\ncode and pretrained models will be publicly available for a new Video\nScene-Aware Dialog challenge.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 19:43:13 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 00:35:25 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Hori", "Chiori", ""], ["Alamri", "Huda", ""], ["Wang", "Jue", ""], ["Wichern", "Gordon", ""], ["Hori", "Takaaki", ""], ["Cherian", "Anoop", ""], ["Marks", "Tim K.", ""], ["Cartillier", "Vincent", ""], ["Lopes", "Raphael Gontijo", ""], ["Das", "Abhishek", ""], ["Essa", "Irfan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1806.08437", "submitter": "Zahra Mirikharaji", "authors": "Zahra Mirikharaji and Ghassan Hamarneh", "title": "Star Shape Prior in Fully Convolutional Networks for Skin Lesion\n  Segmentation", "comments": "Accepted in MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is an important preliminary step towards automatic\nmedical image interpretation. Recently deep convolutional neural networks have\nbecome the first choice for the task of pixel-wise class prediction. While\nincorporating prior knowledge about the structure of target objects has proven\neffective in traditional energy-based segmentation approaches, there has not\nbeen a clear way for encoding prior knowledge into deep learning frameworks. In\nthis work, we propose a new loss term that encodes the star shape prior into\nthe loss function of an end-to-end trainable fully convolutional network (FCN)\nframework. We penalize non-star shape segments in FCN prediction maps to\nguarantee a global structure in segmentation results. Our experiments\ndemonstrate the advantage of regularizing FCN parameters by the star shape\nprior and our results on the ISBI 2017 skin segmentation challenge data set\nachieve the first rank in the segmentation task among $21$ participating teams.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 22:10:19 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Mirikharaji", "Zahra", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1806.08463", "submitter": "Alexander Wong", "authors": "Rene Bidart and Alexander Wong", "title": "TriResNet: A Deep Triple-stream Residual Network for Histopathology\n  Grading", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While microscopic analysis of histopathological slides is generally\nconsidered as the gold standard method for performing cancer diagnosis and\ngrading, the current method for analysis is extremely time consuming and labour\nintensive as it requires pathologists to visually inspect tissue samples in a\ndetailed fashion for the presence of cancer. As such, there has been\nsignificant recent interest in computer aided diagnosis systems for analysing\nhistopathological slides for cancer grading to aid pathologists to perform\ncancer diagnosis and grading in a more efficient, accurate, and consistent\nmanner. In this work, we investigate and explore a deep triple-stream residual\nnetwork (TriResNet) architecture for the purpose of tile-level histopathology\ngrading, which is the critical first step to computer-aided whole-slide\nhistopathology grading. In particular, the design mentality behind the proposed\nTriResNet network architecture is to facilitate for the learning of a more\ndiverse set of quantitative features to better characterize the complex tissue\ncharacteristics found in histopathology samples. Experimental results on two\nwidely-used computer-aided histopathology benchmark datasets (CAMELYON16\ndataset and Invasive Ductal Carcinoma (IDC) dataset) demonstrated that the\nproposed TriResNet network architecture was able to achieve noticeably improved\naccuracies when compared with two other state-of-the-art deep convolutional\nneural network architectures. Based on these promising results, the hope is\nthat the proposed TriResNet network architecture could become a useful tool to\naiding pathologists increase the consistency, speed, and accuracy of the\nhistopathology grading process.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 01:18:14 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Bidart", "Rene", ""], ["Wong", "Alexander", ""]]}, {"id": "1806.08472", "submitter": "Jie Cao", "authors": "Jie Cao, Yibo Hu, Hongwen Zhang, Ran He, Zhenan Sun", "title": "Learning a High Fidelity Pose Invariant Model for High-resolution Face\n  Frontalization", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face frontalization refers to the process of synthesizing the frontal view of\na face from a given profile. Due to self-occlusion and appearance distortion in\nthe wild, it is extremely challenging to recover faithful results and preserve\ntexture details in a high-resolution. This paper proposes a High Fidelity Pose\nInvariant Model (HF-PIM) to produce photographic and identity-preserving\nresults. HF-PIM frontalizes the profiles through a novel texture warping\nprocedure and leverages a dense correspondence field to bind the 2D and 3D\nsurface spaces. We decompose the prerequisite of warping into dense\ncorrespondence field estimation and facial texture map recovering, which are\nboth well addressed by deep networks. Different from those reconstruction\nmethods relying on 3D data, we also propose Adversarial Residual Dictionary\nLearning (ARDL) to supervise facial texture map recovering with only monocular\nimages. Exhaustive experiments on both controlled and uncontrolled environments\ndemonstrate that the proposed method not only boosts the performance of\npose-invariant face recognition but also dramatically improves high-resolution\nfrontalization appearances.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 02:45:22 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 08:41:08 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Cao", "Jie", ""], ["Hu", "Yibo", ""], ["Zhang", "Hongwen", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1806.08482", "submitter": "Xiaoguang Han", "authors": "Chuan Wang and Haibin Huang and Xiaoguang Han and Jue Wang", "title": "Video Inpainting by Jointly Learning Temporal Structure and Spatial\n  Details", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new data-driven video inpainting method for recovering missing\nregions of video frames. A novel deep learning architecture is proposed which\ncontains two sub-networks: a temporal structure inference network and a spatial\ndetail recovering network. The temporal structure inference network is built\nupon a 3D fully convolutional architecture: it only learns to complete a\nlow-resolution video volume given the expensive computational cost of 3D\nconvolution. The low resolution result provides temporal guidance to the\nspatial detail recovering network, which performs image-based inpainting with a\n2D fully convolutional network to produce recovered video frames in their\noriginal resolution. Such two-step network design ensures both the spatial\nquality of each frame and the temporal coherence across frames. Our method\njointly trains both sub-networks in an end-to-end manner. We provide\nqualitative and quantitative evaluation on three datasets, demonstrating that\nour method outperforms previous learning-based video inpainting methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 03:32:57 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 00:34:30 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Wang", "Chuan", ""], ["Huang", "Haibin", ""], ["Han", "Xiaoguang", ""], ["Wang", "Jue", ""]]}, {"id": "1806.08485", "submitter": "Zhongping Ji", "authors": "Zhongping Ji, Xiao Qi, Yigang Wang, Gang Xu, Peng Du, and Qing Wu", "title": "Shape-from-Mask: A Deep Learning Based Human Body Shape Reconstruction\n  from Binary Mask Images", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D content creation is referred to as one of the most fundamental tasks of\ncomputer graphics. And many 3D modeling algorithms from 2D images or curves\nhave been developed over the past several decades. Designers are allowed to\nalign some conceptual images or sketch some suggestive curves, from front,\nside, and top views, and then use them as references in constructing a 3D model\nautomatically or manually. However, to the best of our knowledge, no studies\nhave investigated on 3D human body reconstruction in a similar manner. In this\npaper, we propose a deep learning based reconstruction of 3D human body shape\nfrom 2D orthographic views. A novel CNN-based regression network, with two\nbranches corresponding to frontal and lateral views respectively, is designed\nfor estimating 3D human body shape from 2D mask images. We train our networks\nseparately to decouple the feature descriptors which encode the body parameters\nfrom different views, and fuse them to estimate an accurate human body shape.\nIn addition, to overcome the shortage of training data required for this\npurpose, we propose some significantly data augmentation schemes for 3D human\nbody shapes, which can be used to promote further research on this topic.\nExtensive experimen- tal results demonstrate that visually realistic and\naccurate reconstructions can be achieved effectively using our algorithm.\nRequiring only binary mask images, our method can help users create their own\ndigital avatars quickly, and also make it easy to create digital human body for\n3D game, virtual reality, online fashion shopping.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 04:00:37 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Ji", "Zhongping", ""], ["Qi", "Xiao", ""], ["Wang", "Yigang", ""], ["Xu", "Gang", ""], ["Du", "Peng", ""], ["Wu", "Qing", ""]]}, {"id": "1806.08498", "submitter": "Xiaohan Fei", "authors": "Xiaohan Fei and Stefano Soatto", "title": "Visual-Inertial Object Detection and Mapping", "comments": null, "journal-ref": "ECCV 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to populate an unknown environment with models of\npreviously seen objects, placed in a Euclidean reference frame that is inferred\ncausally and on-line using monocular video along with inertial sensors. The\nsystem we implement returns a sparse point cloud for the regions of the scene\nthat are visible but not recognized as a previously seen object, and a detailed\nobject model and its pose in the Euclidean frame otherwise. The system includes\nbottom-up and top-down components, whereby deep networks trained for detection\nprovide likelihood scores for object hypotheses provided by a nonlinear filter,\nwhose state serves as memory. Additional networks provide likelihood scores for\nedges, which complements detection networks trained to be invariant to small\ndeformations. We test our algorithm on existing datasets, and also introduce\nthe VISMA dataset, that provides ground truth pose, point-cloud map, and object\nmodels, along with time-stamped inertial measurements.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 05:29:31 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 21:43:31 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Fei", "Xiaohan", ""], ["Soatto", "Stefano", ""]]}, {"id": "1806.08503", "submitter": "Shuigeng Zhou", "authors": "Fan Wu, Kai Tian, Jihong Guan, Shuigeng Zhou", "title": "Global Semantic Consistency for Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image recognition, there are many cases where training samples cannot\ncover all target classes. Zero-shot learning (ZSL) utilizes the class semantic\ninformation to classify samples of the unseen categories that have no\ncorresponding samples contained in the training set. In this paper, we propose\nan end-to-end framework, called Global Semantic Consistency Network (GSC-Net\nfor short), which makes complete use of the semantic information of both seen\nand unseen classes, to support effective zero-shot learning. We also adopt a\nsoft label embedding loss to further exploit the semantic relationships among\nclasses. To adapt GSC-Net to a more practical setting, Generalized Zero-shot\nLearning (GZSL), we introduce a parametric novelty detection mechanism. Our\napproach achieves the state-of-the-art performance on both ZSL and GZSL tasks\nover three visual attribute datasets, which validates the effectiveness and\nadvantage of the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 05:55:20 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Wu", "Fan", ""], ["Tian", "Kai", ""], ["Guan", "Jihong", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "1806.08514", "submitter": "Lijun Zhao", "authors": "Lijun Zhao, Huihui Bai, Anhong Wang, Yao Zhao", "title": "Virtual Codec Supervised Re-Sampling Network for Image Compression", "comments": "13 pages, 11 figures Our project can be found in the website:\n  https://github.com/VirtualCodecNetwork", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an image re-sampling compression method by learning\nvirtual codec network (VCN) to resolve the non-differentiable problem of\nquantization function for image compression. Here, the image re-sampling not\nonly refers to image full-resolution re-sampling but also low-resolution\nre-sampling. We generalize this method for standard-compliant image compression\n(SCIC) framework and deep neural networks based compression (DNNC) framework.\nSpecifically, an input image is measured by re-sampling network (RSN) network\nto get re-sampled vectors. Then, these vectors are directly quantized in the\nfeature space in SCIC, or discrete cosine transform coefficients of these\nvectors are quantized to further improve coding efficiency in DNNC. At the\nencoder, the quantized vectors or coefficients are losslessly compressed by\narithmetic coding. At the receiver, the decoded vectors are utilized to restore\ninput image by image decoder network (IDN). In order to train RSN network and\nIDN network together in an end-to-end fashion, our VCN network intimates\nprojection from the re-sampled vectors to the IDN-decoded image. As a result,\ngradients from IDN network to RSN network can be approximated by VCN network's\ngradient. Because dimension reduction can be further achieved by quantization\nin some dimensional space after image re-sampling within auto-encoder\narchitecture, we can well initialize our networks from pre-trained auto-encoder\nnetworks. Through extensive experiments and analysis, it is verified that the\nproposed method has more effectiveness and versatility than many\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 06:48:07 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 02:45:39 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Zhao", "Lijun", ""], ["Bai", "Huihui", ""], ["Wang", "Anhong", ""], ["Zhao", "Yao", ""]]}, {"id": "1806.08522", "submitter": "Nikitha Vallurupalli", "authors": "Nikitha Vallurupalli, Sriharsha Annamaneni, Girish Varma, C V Jawahar,\n  Manu Mathew, Soyeb Nagori", "title": "Efficient Semantic Segmentation using Gradual Grouping", "comments": "CVPR 2018 Workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep CNNs for semantic segmentation have high memory and run time\nrequirements. Various approaches have been proposed to make CNNs efficient like\ngrouped, shuffled, depth-wise separable convolutions. We study the\neffectiveness of these techniques on a real-time semantic segmentation\narchitecture like ERFNet for improving run time by over 5X. We apply these\ntechniques to CNN layers partially or fully and evaluate the testing accuracies\non Cityscapes dataset. We obtain accuracy vs parameters/FLOPs trade offs,\ngiving accuracy scores for models that can run under specified runtime budgets.\nWe further propose a novel training procedure which starts out with a dense\nconvolution but gradually evolves towards a grouped convolution. We show that\nour proposed training method and efficient architecture design can improve\naccuracies by over 8% with depth wise separable convolutions applied on the\nencoder of ERFNet and attaching a light weight decoder. This results in a model\nwhich has a 5X improvement in FLOPs while only suffering a 4% degradation in\naccuracy with respect to ERFNet.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 07:11:41 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Vallurupalli", "Nikitha", ""], ["Annamaneni", "Sriharsha", ""], ["Varma", "Girish", ""], ["Jawahar", "C V", ""], ["Mathew", "Manu", ""], ["Nagori", "Soyeb", ""]]}, {"id": "1806.08523", "submitter": "Subhajit Chaudhury Mr", "authors": "Phongtharin Vinayavekhin, Subhajit Chaudhury, Asim Munawar, Don Joven\n  Agravante, Giovanni De Magistris, Daiki Kimura and Ryuki Tachibana", "title": "Focusing on What is Relevant: Time-Series Learning and Understanding\n  using Attention", "comments": "To appear in ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a contribution towards interpretability of the deep learning\nmodels in different applications of time-series. We propose a temporal\nattention layer that is capable of selecting the relevant information to\nperform various tasks, including data completion, key-frame detection and\nclassification. The method uses the whole input sequence to calculate an\nattention value for each time step. This results in more focused attention\nvalues and more plausible visualisation than previous methods. We apply the\nproposed method to three different tasks. Experimental results show that the\nproposed network produces comparable results to a state of the art. In\naddition, the network provides better interpretability of the decision, that\nis, it generates more significant attention weight to related frames compared\nto similar techniques attempted in the past.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 07:16:08 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Vinayavekhin", "Phongtharin", ""], ["Chaudhury", "Subhajit", ""], ["Munawar", "Asim", ""], ["Agravante", "Don Joven", ""], ["De Magistris", "Giovanni", ""], ["Kimura", "Daiki", ""], ["Tachibana", "Ryuki", ""]]}, {"id": "1806.08562", "submitter": "Savas Ozkan", "authors": "Savas Ozkan, Gozde Bozdagi Akar", "title": "Deep Spectral Convolution Network for HyperSpectral Unmixing", "comments": "Accepted to ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel hyperspectral unmixing technique based on\ndeep spectral convolution networks (DSCN). Particularly, three important\ncontributions are presented throughout this paper. First, fully-connected\nlinear operation is replaced with spectral convolutions to extract local\nspectral characteristics from hyperspectral signatures with a deeper network\narchitecture. Second, instead of batch normalization, we propose a spectral\nnormalization layer which improves the selectivity of filters by normalizing\ntheir spectral responses. Third, we introduce two fusion configurations that\nproduce ideal abundance maps by using the abstract representations computed\nfrom previous layers. In experiments, we use two real datasets to evaluate the\nperformance of our method with other baseline techniques. The experimental\nresults validate that the proposed method outperforms baselines based on Root\nMean Square Error (RMSE).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 09:02:43 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Ozkan", "Savas", ""], ["Akar", "Gozde Bozdagi", ""]]}, {"id": "1806.08565", "submitter": "Federico Magliani", "authors": "Federico Magliani and Andrea Prati", "title": "An accurate retrieval through R-MAC+ descriptors for landmark\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The landmark recognition problem is far from being solved, but with the use\nof features extracted from intermediate layers of Convolutional Neural Networks\n(CNNs), excellent results have been obtained. In this work, we propose some\nimprovements on the creation of R-MAC descriptors in order to make the\nnewly-proposed R-MAC+ descriptors more representative than the previous ones.\nHowever, the main contribution of this paper is a novel retrieval technique,\nthat exploits the fine representativeness of the MAC descriptors of the\ndatabase images. Using this descriptors called \"db regions\" during the\nretrieval stage, the performance is greatly improved. The proposed method is\ntested on different public datasets: Oxford5k, Paris6k and Holidays. It\noutperforms the state-of-the- art results on Holidays and reached excellent\nresults on Oxford5k and Paris6k, overcame only by approaches based on\nfine-tuning strategies.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 09:16:10 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Magliani", "Federico", ""], ["Prati", "Andrea", ""]]}, {"id": "1806.08568", "submitter": "Vincenzo Lomonaco", "authors": "Davide Maltoni and Vincenzo Lomonaco", "title": "Continuous Learning in Single-Incremental-Task Scenarios", "comments": "26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4),\n  several typos and minor mistakes corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown that architectural, regularization and rehearsal\nstrategies can be used to train deep models sequentially on a number of\ndisjoint tasks without forgetting previously acquired knowledge. However, these\nstrategies are still unsatisfactory if the tasks are not disjoint but\nconstitute a single incremental task (e.g., class-incremental learning). In\nthis paper we point out the differences between multi-task and\nsingle-incremental-task scenarios and show that well-known approaches such as\nLWF, EWC and SI are not ideal for incremental task scenarios. A new approach,\ndenoted as AR1, combining architectural and regularization strategies is then\nspecifically proposed. AR1 overhead (in term of memory and computation) is very\nsmall thus making it suitable for online learning. When tested on CORe50 and\niCIFAR-100, AR1 outperformed existing regularization strategies by a good\nmargin.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 09:22:42 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 11:13:40 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 21:49:25 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Maltoni", "Davide", ""], ["Lomonaco", "Vincenzo", ""]]}, {"id": "1806.08572", "submitter": "Omair Hassaan", "authors": "Omair Hassaan, Abeera Shamail, Zain Butt, Murtaza Taj", "title": "Point cloud segmentation using hierarchical tree for architectural\n  models", "comments": "9 pages. 10 figures. Submitted in EuroGraphics 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in the 3D scanning technologies have made the generation\nof highly accurate 3D point clouds relatively easy but the segmentation of\nthese point clouds remains a challenging area. A number of techniques have set\nprecedent of either planar or primitive based segmentation in literature. In\nthis work, we present a novel and an effective primitive based point cloud\nsegmentation algorithm. The primary focus, i.e. the main technical contribution\nof our method is a hierarchical tree which iteratively divides the point cloud\ninto segments. This tree uses an exclusive energy function and a 3D\nconvolutional neural network, HollowNets to classify the segments. We test the\nefficacy of our proposed approach using both real and synthetic data obtaining\nan accuracy greater than 90% for domes and minarets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 09:36:21 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Hassaan", "Omair", ""], ["Shamail", "Abeera", ""], ["Butt", "Zain", ""], ["Taj", "Murtaza", ""]]}, {"id": "1806.08600", "submitter": "Savas Ozkan", "authors": "Savas Ozkan and Akin Ozkan", "title": "KinshipGAN: Synthesizing of Kinship Faces From Family Photos by\n  Regularizing a Deep Face Network", "comments": "Accepted to IEEE ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a kinship generator network that can synthesize a\npossible child face by analyzing his/her parent's photo. For this purpose, we\nfocus on to handle the scarcity of kinship datasets throughout the paper by\nproposing novel solutions in particular. To extract robust features, we\nintegrate a pre-trained face model to the kinship face generator. Moreover, the\ngenerator network is regularized with an additional face dataset and\nadversarial loss to decrease the overfitting of the limited samples. Lastly, we\nadapt cycle-domain transformation to attain a more stable results. Experiments\nare conducted on Families in the Wild (FIW) dataset. The experimental results\nshow that the contributions presented in the paper provide important\nperformance improvements compared to the baseline architecture and our proposed\nmethod yields promising perceptual results.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 11:07:57 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 08:27:48 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Ozkan", "Savas", ""], ["Ozkan", "Akin", ""]]}, {"id": "1806.08612", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Imed Bouazizi, Prakash Kolan, Hossein Najafzadeh", "title": "Ad-Net: Audio-Visual Convolutional Neural Network for Advertisement\n  Detection In Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized advertisement is a crucial task for many of the online\nbusinesses and video broadcasters. Many of today's broadcasters use the same\ncommercial for all customers, but as one can imagine different viewers have\ndifferent interests and it seems reasonable to have customized commercial for\ndifferent group of people, chosen based on their demographic features, and\nhistory. In this project, we propose a framework, which gets the broadcast\nvideos, analyzes them, detects the commercial and replaces it with a more\nsuitable commercial. We propose a two-stream audio-visual convolutional neural\nnetwork, that one branch analyzes the visual information and the other one\nanalyzes the audio information, and then the audio and visual embedding are\nfused together, and are used for commercial detection, and content\ncategorization. We show that using both the visual and audio content of the\nvideos significantly improves the model performance for video analysis. This\nnetwork is trained on a dataset of more than 50k regular video and commercial\nshots, and achieved much better performance compared to the models based on\nhand-crafted features.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 11:52:57 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Minaee", "Shervin", ""], ["Bouazizi", "Imed", ""], ["Kolan", "Prakash", ""], ["Najafzadeh", "Hossein", ""]]}, {"id": "1806.08616", "submitter": "Alexandros Kouris", "authors": "Stylianos I. Venieris, Alexandros Kouris and Christos-Savvas Bouganis", "title": "Deploying Deep Neural Networks in the Embedded Space", "comments": "Accepted at MobiSys18: 2nd International Workshop on Embedded and\n  Mobile Deep Learning (EMDL) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Deep Neural Networks (DNNs) have emerged as the dominant model\nacross various AI applications. In the era of IoT and mobile systems, the\nefficient deployment of DNNs on embedded platforms is vital to enable the\ndevelopment of intelligent applications. This paper summarises our recent work\non the optimised mapping of DNNs on embedded settings. By covering such diverse\ntopics as DNN-to-accelerator toolflows, high-throughput cascaded classifiers\nand domain-specific model design, the presented set of works aim to enable the\ndeployment of sophisticated deep learning models on cutting-edge mobile and\nembedded systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 12:01:11 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Venieris", "Stylianos I.", ""], ["Kouris", "Alexandros", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "1806.08634", "submitter": "Juan Eugenio Iglesias", "authors": "Juan Eugenio Iglesias, Ricardo Insausti, Garikoitz Lerma-Usabiaga,\n  Martina Bocchetta, Koen Van Leemput, Douglas N Greve, Andre van der Kouwe,\n  Bruce Fischl, Cesar Caballero-Gaudes, Pedro M Paz-Alonso", "title": "A probabilistic atlas of the human thalamic nuclei combining ex vivo MRI\n  and histology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human thalamus is a brain structure that comprises numerous, highly\nspecific nuclei. Since these nuclei are known to have different functions and\nto be connected to different areas of the cerebral cortex, it is of great\ninterest for the neuroimaging community to study their volume, shape and\nconnectivity in vivo with MRI. In this study, we present a probabilistic atlas\nof the thalamic nuclei built using ex vivo brain MRI scans and histological\ndata, as well as the application of the atlas to in vivo MRI segmentation. The\natlas was built using manual delineation of 26 thalamic nuclei on the serial\nhistology of 12 whole thalami from six autopsy samples, combined with manual\nsegmentations of the whole thalamus and surrounding structures (caudate,\nputamen, hippocampus, etc.) made on in vivo brain MR data from 39 subjects. The\n3D structure of the histological data and corresponding manual segmentations\nwas recovered using the ex vivo MRI as reference frame, and stacks of blockface\nphotographs acquired during the sectioning as intermediate target. The atlas,\nwhich was encoded as an adaptive tetrahedral mesh, shows a good agreement with\nwith previous histological studies of the thalamus in terms of volumes of\nrepresentative nuclei. When applied to segmentation of in vivo scans using\nBayesian inference, the atlas shows excellent test-retest reliability,\nrobustness to changes in input MRI contrast, and ability to detect differential\nthalamic effects in subjects with Alzheimer's disease. The probabilistic atlas\nand companion segmentation tool are publicly available as part of the\nneuroimaging package FreeSurfer.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 12:42:37 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Iglesias", "Juan Eugenio", ""], ["Insausti", "Ricardo", ""], ["Lerma-Usabiaga", "Garikoitz", ""], ["Bocchetta", "Martina", ""], ["Van Leemput", "Koen", ""], ["Greve", "Douglas N", ""], ["van der Kouwe", "Andre", ""], ["Fischl", "Bruce", ""], ["Caballero-Gaudes", "Cesar", ""], ["Paz-Alonso", "Pedro M", ""]]}, {"id": "1806.08640", "submitter": "Zach Eaton-Rosen", "authors": "Zach Eaton-Rosen, Felix Bragman, Sotirios Bisdas, Sebastien Ourselin,\n  M. Jorge Cardoso", "title": "Towards safe deep learning: accurately quantifying biomarker uncertainty\n  in neural network predictions", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated medical image segmentation, specifically using deep learning, has\nshown outstanding performance in semantic segmentation tasks. However, these\nmethods rarely quantify their uncertainty, which may lead to errors in\ndownstream analysis. In this work we propose to use Bayesian neural networks to\nquantify uncertainty within the domain of semantic segmentation. We also\npropose a method to convert voxel-wise segmentation uncertainty into volumetric\nuncertainty, and calibrate the accuracy and reliability of confidence intervals\nof derived measurements. When applied to a tumour volume estimation\napplication, we demonstrate that by using such modelling of uncertainty, deep\nlearning systems can be made to report volume estimates with well-calibrated\nerror-bars, making them safer for clinical use. We also show that the\nuncertainty estimates extrapolate to unseen data, and that the confidence\nintervals are robust in the presence of artificial noise. This could be used to\nprovide a form of quality control and quality assurance, and may permit further\nadoption of deep learning tools in the clinic.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 12:54:20 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Eaton-Rosen", "Zach", ""], ["Bragman", "Felix", ""], ["Bisdas", "Sotirios", ""], ["Ourselin", "Sebastien", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1806.08641", "submitter": "Adam Hartwell", "authors": "Adam Hartwell, Visakan Kadirkamanathan, Sean R Anderson", "title": "Compact Deep Neural Networks for Computationally Efficient Gesture\n  Classification From Electromyography Signals", "comments": "IEEE BioRob 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning classifiers using surface electromyography are important for\nhuman-machine interfacing and device control. Conventional classifiers such as\nsupport vector machines (SVMs) use manually extracted features based on e.g.\nwavelets. These features tend to be fixed and non-person specific, which is a\nkey limitation due to high person-to-person variability of myography signals.\nDeep neural networks, by contrast, can automatically extract person specific\nfeatures - an important advantage. However, deep neural networks typically have\nthe drawback of large numbers of parameters, requiring large training data sets\nand powerful hardware not suited to embedded systems. This paper solves these\nproblems by introducing a compact deep neural network architecture that is much\nsmaller than existing counterparts. The performance of the compact deep net is\nbenchmarked against an SVM and compared to other contemporary architectures\nacross 10 human subjects, comparing Myo and Delsys Trigno electrode sets. The\naccuracy of the compact deep net was found to be 84.2 +/- 6% versus 70.5 +/- 7%\nfor the SVM on the Myo, and 80.3+/- 7% versus 67.8 +/- 9% for the Delsys\nsystem, demonstrating the superior effectiveness of the proposed compact\nnetwork, which had just 5,889 parameters - orders of magnitude less than some\ncontemporary alternatives in this domain while maintaining better performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 13:01:48 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 17:05:34 GMT"}, {"version": "v3", "created": "Sat, 13 Apr 2019 16:51:45 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Hartwell", "Adam", ""], ["Kadirkamanathan", "Visakan", ""], ["Anderson", "Sean R", ""]]}, {"id": "1806.08672", "submitter": "Rita Kuznetsova", "authors": "Rita Kuznetsova, Oleg Bakhteev, Alexandr Ogaltsov", "title": "Variational learning across domains with triplet information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work investigates deep generative models, which allow us to use training\ndata from one domain to build a model for another domain. We propose the\nVariational Bi-domain Triplet Autoencoder (VBTA) that learns a joint\ndistribution of objects from different domains. We extend the VBTAs objective\nfunction by the relative constraints or triplets that sampled from the shared\nlatent space across domains. In other words, we combine the deep generative\nmodels with a metric learning ideas in order to improve the final objective\nwith the triplets information. The performance of the VBTA model is\ndemonstrated on different tasks: image-to-image translation, bi-directional\nimage generation and cross-lingual document classification.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 13:58:42 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 19:47:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Kuznetsova", "Rita", ""], ["Bakhteev", "Oleg", ""], ["Ogaltsov", "Alexandr", ""]]}, {"id": "1806.08722", "submitter": "Rayson Laroca", "authors": "Diego R. Lucio, Rayson Laroca, Evair Severo, Alceu S. Britto Jr.,\n  David Menotti", "title": "Fully Convolutional Networks and Generative Adversarial Networks Applied\n  to Sclera Segmentation", "comments": "Accepted for presentation at the IEEE International Conference on\n  Biometrics: Theory, Applications, and Systems (BTAS) 2018", "journal-ref": null, "doi": "10.1109/BTAS.2018.8698597", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the world's demand for security systems, biometrics can be seen as an\nimportant topic of research in computer vision. One of the biometric forms that\nhas been gaining attention is the recognition based on sclera. The initial and\nparamount step for performing this type of recognition is the segmentation of\nthe region of interest, i.e. the sclera. In this context, two approaches for\nsuch task based on the Fully Convolutional Network (FCN) and on Generative\nAdversarial Network (GAN) are introduced in this work. FCN is similar to a\ncommon convolution neural network, however the fully connected layers (i.e.,\nthe classification layers) are removed from the end of the network and the\noutput is generated by combining the output of pooling layers from different\nconvolutional ones. The GAN is based on the game theory, where we have two\nnetworks competing with each other to generate the best segmentation. In order\nto perform fair comparison with baselines and quantitative and objective\nevaluations of the proposed approaches, we provide to the scientific community\nnew 1,300 manually segmented images from two databases. The experiments are\nperformed on the UBIRIS.v2 and MICHE databases and the best performing\nconfigurations of our propositions achieved F-score's measures of 87.48% and\n88.32%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 15:16:59 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 13:57:22 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2018 15:14:36 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Lucio", "Diego R.", ""], ["Laroca", "Rayson", ""], ["Severo", "Evair", ""], ["Britto", "Alceu S.", "Jr."], ["Menotti", "David", ""]]}, {"id": "1806.08723", "submitter": "Christian Wachinger", "authors": "Christian Wachinger, Matthew Toews, Georg Langs, William Wells, Polina\n  Golland", "title": "Keypoint Transfer for Fast Whole-Body Segmentation", "comments": "Accepted for publication at IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach for image segmentation based on sparse\ncorrespondences between keypoints in testing and training images. Keypoints\nrepresent automatically identified distinctive image locations, where each\nkeypoint correspondence suggests a transformation between images. We use these\ncorrespondences to transfer label maps of entire organs from the training\nimages to the test image. The keypoint transfer algorithm includes three steps:\n(i) keypoint matching, (ii) voting-based keypoint labeling, and (iii)\nkeypoint-based probabilistic transfer of organ segmentations. We report\nsegmentation results for abdominal organs in whole-body CT and MRI, as well as\nin contrast-enhanced CT and MRI. Our method offers a speed-up of about three\norders of magnitude in comparison to common multi-atlas segmentation, while\nachieving an accuracy that compares favorably. Moreover, keypoint transfer does\nnot require the registration to an atlas or a training phase. Finally, the\nmethod allows for the segmentation of scans with highly variable field-of-view.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 15:18:10 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Wachinger", "Christian", ""], ["Toews", "Matthew", ""], ["Langs", "Georg", ""], ["Wells", "William", ""], ["Golland", "Polina", ""]]}, {"id": "1806.08741", "submitter": "Ziv Yaniv", "authors": "Bradley C. Lowekamp and David T. Chen and Ziv Yaniv and Terry S. Yoo", "title": "Scalable Simple Linear Iterative Clustering (SSLIC) Using a Generic and\n  Parallel Approach", "comments": "manuscript submitted to InsightJournal (ITK)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Superpixel algorithms have proven to be a useful initial step for\nsegmentation and subsequent processing of images, reducing computational\ncomplexity by replacing the use of expensive per-pixel primitives with a\nhigher-level abstraction, superpixels. They have been successfully applied both\nin the context of traditional image analysis and deep learning based\napproaches. In this work, we present a generalized implementation of the simple\nlinear iterative clustering (SLIC) superpixel algorithm that has been\ngeneralized for n-dimensional scalar and multi-channel images. Additionally,\nthe standard iterative implementation is replaced by a parallel, multi-threaded\none. We describe the implementation details and analyze its scalability using a\nstrong scaling formulation. Quantitative evaluation is performed using a 3D\nimage, the Visible Human cryosection dataset, and a 2D image from the same\ndataset. Results show good scalability with runtime gains even when using a\nlarge number of threads that exceeds the physical number of available cores\n(hyperthreading).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 16:02:44 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 15:43:35 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Lowekamp", "Bradley C.", ""], ["Chen", "David T.", ""], ["Yaniv", "Ziv", ""], ["Yoo", "Terry S.", ""]]}, {"id": "1806.08756", "submitter": "Peter Florence", "authors": "Peter R. Florence, Lucas Manuelli, Russ Tedrake", "title": "Dense Object Nets: Learning Dense Visual Object Descriptors By and For\n  Robotic Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the right object representation for manipulation? We would like\nrobots to visually perceive scenes and learn an understanding of the objects in\nthem that (i) is task-agnostic and can be used as a building block for a\nvariety of manipulation tasks, (ii) is generally applicable to both rigid and\nnon-rigid objects, (iii) takes advantage of the strong priors provided by 3D\nvision, and (iv) is entirely learned from self-supervision. This is hard to\nachieve with previous methods: much recent work in grasping does not extend to\ngrasping specific objects or other tasks, whereas task-specific learning may\nrequire many trials to generalize well across object configurations or other\ntasks. In this paper we present Dense Object Nets, which build on recent\ndevelopments in self-supervised dense descriptor learning, as a consistent\nobject representation for visual understanding and manipulation. We demonstrate\nthey can be trained quickly (approximately 20 minutes) for a wide variety of\npreviously unseen and potentially non-rigid objects. We additionally present\nnovel contributions to enable multi-object descriptor learning, and show that\nby modifying our training procedure, we can either acquire descriptors which\ngeneralize across classes of objects, or descriptors that are distinct for each\nobject instance. Finally, we demonstrate the novel application of learned dense\ndescriptors to robotic manipulation. We demonstrate grasping of specific points\non an object across potentially deformed object configurations, and demonstrate\nusing class general descriptors to transfer specific grasps across objects in a\nclass.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 16:38:01 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 17:53:32 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Florence", "Peter R.", ""], ["Manuelli", "Lucas", ""], ["Tedrake", "Russ", ""]]}, {"id": "1806.08814", "submitter": "Mathias Unberath", "authors": "Mathias Unberath, Javad Fotouhi, Jonas Hajek, Andreas Maier, Greg\n  Osgood, Russell Taylor, Mehran Armand, Nassir Navab", "title": "Augmented Reality-based Feedback for Technician-in-the-loop C-arm\n  Repositioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interventional C-arm imaging is crucial to percutaneous orthopedic procedures\nas it enables the surgeon to monitor the progress of surgery on the anatomy\nlevel. Minimally invasive interventions require repeated acquisition of X-ray\nimages from different anatomical views to verify tool placement. Achieving and\nreproducing these views often comes at the cost of increased surgical time and\nradiation dose to both patient and staff. This work proposes a marker-free\n\"technician-in-the-loop\" Augmented Reality (AR) solution for C-arm\nrepositioning. The X-ray technician operating the C-arm interventionally is\nequipped with a head-mounted display capable of recording desired C-arm poses\nin 3D via an integrated infrared sensor. For C-arm repositioning to a\nparticular target view, the recorded C-arm pose is restored as a virtual object\nand visualized in an AR environment, serving as a perceptual reference for the\ntechnician. We conduct experiments in a setting simulating orthopedic trauma\nsurgery. Our proof-of-principle findings indicate that the proposed system can\ndecrease the 2.76 X-ray images required per desired view down to zero,\nsuggesting substantial reductions of radiation dose during C-arm repositioning.\nThe proposed AR solution is a first step towards facilitating communication\nbetween the surgeon and the surgical staff, improving the quality of surgical\nimage acquisition, and enabling context-aware guidance for surgery rooms of the\nfuture. The concept of technician-in-the-loop design will become relevant to\nvarious interventions considering the expected advancements of sensing and\nwearable computing in the near future.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 18:34:48 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Unberath", "Mathias", ""], ["Fotouhi", "Javad", ""], ["Hajek", "Jonas", ""], ["Maier", "Andreas", ""], ["Osgood", "Greg", ""], ["Taylor", "Russell", ""], ["Armand", "Mehran", ""], ["Navab", "Nassir", ""]]}, {"id": "1806.08852", "submitter": "Lorenzo Quir\\'os", "authors": "Lorenzo Quir\\'os", "title": "Multi-Task Handwritten Document Layout Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Document Layout Analysis is a fundamental step in Handwritten Text Processing\nsystems, from the extraction of the text lines to the type of zone it belongs\nto. We present a system based on artificial neural networks which is able to\ndetermine not only the baselines of text lines present in the document, but\nalso performs geometric and logic layout analysis of the document. Experiments\nin three different datasets demonstrate the potential of the method and show\ncompetitive results with respect to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 21:00:07 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 11:30:18 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 15:07:40 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Quir\u00f3s", "Lorenzo", ""]]}, {"id": "1806.08854", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Yuqing Song, Yida Zhao, Jiarong Qiu, Qin Jin and\n  Alexander Hauptmann", "title": "RUC+CMU: System Report for Dense Captioning Events in Videos", "comments": "Winner in ActivityNet 2018 Dense Video Captioning challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This notebook paper presents our system in the ActivityNet Dense Captioning\nin Video task (task 3). Temporal proposal generation and caption generation are\nboth important to the dense captioning task. Therefore, we propose a proposal\nranking model to employ a set of effective feature representations for proposal\ngeneration, and ensemble a series of caption models enhanced with context\ninformation to generate captions robustly on predicted proposals. Our approach\nachieves the state-of-the-art performance on the dense video captioning task\nwith 8.529 METEOR score on the challenge testing set.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 21:03:47 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Chen", "Shizhe", ""], ["Song", "Yuqing", ""], ["Zhao", "Yida", ""], ["Qiu", "Jiarong", ""], ["Jin", "Qin", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1806.08859", "submitter": "Karthik Gopinath", "authors": "Karthik Gopinath, Samrudhdhi B Rangrej and Jayanthi Sivaswamy", "title": "A deep learning framework for segmentation of retinal layers from OCT\n  images", "comments": "Accepted in The 4th Asian Conference on Pattern Recognition (ACPR\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of retinal layers from Optical Coherence Tomography (OCT)\nvolumes is a fundamental problem for any computer aided diagnostic algorithm\ndevelopment. This requires preprocessing steps such as denoising, region of\ninterest extraction, flattening and edge detection all of which involve\nseparate parameter tuning. In this paper, we explore deep learning techniques\nto automate all these steps and handle the presence/absence of pathologies. A\nmodel is proposed consisting of a combination of Convolutional Neural Network\n(CNN) and Long Short Term Memory (LSTM). The CNN is used to extract layers of\ninterest image and extract the edges, while the LSTM is used to trace the layer\nboundary. This model is trained on a mixture of normal and AMD cases using\nminimal data. Validation results on three public datasets show that the\npixel-wise mean absolute error obtained with our system is 1.30 plus or minus\n0.48 which is lower than the inter-marker error of 1.79 plus or minus 0.76. Our\nmodel's performance is also on par with the existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 21:24:58 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Gopinath", "Karthik", ""], ["Rangrej", "Samrudhdhi B", ""], ["Sivaswamy", "Jayanthi", ""]]}, {"id": "1806.08864", "submitter": "Noriaki Hirose", "authors": "Noriaki Hirose, Amir Sadeghian, Fei Xia, Roberto Martin-Martin, and\n  Silvio Savarese", "title": "VUNet: Dynamic Scene View Synthesis for Traversability Estimation using\n  an RGB Camera", "comments": "website: http://svl.stanford.edu/projects/vunet/", "journal-ref": "IEEE Robotics and Automation Letters 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VUNet, a novel view(VU) synthesis method for mobile robots in\ndynamic environments, and its application to the estimation of future\ntraversability. Our method predicts future images for given virtual robot\nvelocity commands using only RGB images at previous and current time steps. The\nfuture images result from applying two types of image changes to the previous\nand current images: 1) changes caused by different camera pose, and 2) changes\ndue to the motion of the dynamic obstacles. We learn to predict these two types\nof changes disjointly using two novel network architectures, SNet and DNet. We\ncombine SNet and DNet to synthesize future images that we pass to our\npreviously presented method GONet to estimate the traversable areas around the\nrobot. Our quantitative and qualitative evaluation indicate that our approach\nfor view synthesis predicts accurate future images in both static and dynamic\nenvironments. We also show that these virtual images can be used to estimate\nfuture traversability correctly. We apply our view synthesis-based\ntraversability estimation method to two applications for assisted\nteleoperation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 21:40:10 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 20:05:27 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Hirose", "Noriaki", ""], ["Sadeghian", "Amir", ""], ["Xia", "Fei", ""], ["Martin-Martin", "Roberto", ""], ["Savarese", "Silvio", ""]]}, {"id": "1806.08896", "submitter": "Cun Mu", "authors": "Cun Mu, Jun Zhao, Guang Yang, Jing Zhang, Zheng Yan", "title": "Towards Practical Visual Search Engine within Elasticsearch", "comments": "Accepted by SIGIR eCom'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our end-to-end content-based image retrieval\nsystem built upon Elasticsearch, a well-known and popular textual search\nengine. As far as we know, this is the first time such a system has been\nimplemented in eCommerce, and our efforts have turned out to be highly\nworthwhile. We end up with a novel and exciting visual search solution that is\nextremely easy to be deployed, distributed, scaled and monitored in a\ncost-friendly manner. Moreover, our platform is intrinsically flexible in\nsupporting multimodal searches, where visual and textual information can be\njointly leveraged in retrieval.\n  The core idea is to encode image feature vectors into a collection of string\ntokens in a way such that closer vectors will share more string tokens in\ncommon. By doing that, we can utilize Elasticsearch to efficiently retrieve\nsimilar images based on similarities within encoded sting tokens. As part of\nthe development, we propose a novel vector to string encoding method, which is\nshown to substantially outperform the previous ones in terms of both precision\nand latency.\n  First-hand experiences in implementing this Elasticsearch-based platform are\nextensively addressed, which should be valuable to practitioners also\ninterested in building visual search engine on top of Elasticsearch.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 02:47:51 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2019 19:45:16 GMT"}, {"version": "v3", "created": "Sun, 17 Mar 2019 00:22:20 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Mu", "Cun", ""], ["Zhao", "Jun", ""], ["Yang", "Guang", ""], ["Zhang", "Jing", ""], ["Yan", "Zheng", ""]]}, {"id": "1806.08906", "submitter": "Yifan Wu", "authors": "Yifan Wu, Fan Yang, Haibin Ling", "title": "Privacy-Protective-GAN for Face De-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face de-identification has become increasingly important as the image sources\nare explosively growing and easily accessible. The advance of new face\nrecognition techniques also arises people's concern regarding the privacy\nleakage. The mainstream pipelines of face de-identification are mostly based on\nthe k-same framework, which bears critiques of low effectiveness and poor\nvisual quality. In this paper, we propose a new framework called\nPrivacy-Protective-GAN (PP-GAN) that adapts GAN with novel verificator and\nregulator modules specially designed for the face de-identification problem to\nensure generating de-identified output with retained structure similarity\naccording to a single input. We evaluate the proposed approach in terms of\nprivacy protection, utility preservation, and structure similarity. Our\napproach not only outperforms existing face de-identification techniques but\nalso provides a practical framework of adapting GAN with priors of domain\nknowledge.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 04:30:57 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Wu", "Yifan", ""], ["Yang", "Fan", ""], ["Ling", "Haibin", ""]]}, {"id": "1806.08970", "submitter": "Md Ashraful Alam Milton", "authors": "Md Ashraful Alam Milton", "title": "Evaluation of Momentum Diverse Input Iterative Fast Gradient Sign Method\n  (M-DI2-FGSM) Based Attack Method on MCS 2018 Adversarial Attacks on Black Box\n  Face Recognition System", "comments": "The Code is available for download in the following github link:\n  https://github.com/miltonbd/mcs_2018_adversarial_attack . arXiv admin note:\n  text overlap with arXiv:1803.06978 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network is the crucial tool for the recent success\nof deep learning based methods on various computer vision tasks like\nclassification, segmentation, and detection. Convolutional neural networks\nachieved state-of-the-art performance in these tasks and every day pushing the\nlimit of computer vision and AI. However, adversarial attack on computer vision\nsystems is threatening their application in the real life and in\nsafety-critical applications. Necessarily, Finding adversarial examples are\nimportant to detect susceptible models to attack and take safeguard measures to\novercome the adversarial attacks. In this regard, MCS 2018 Adversarial Attacks\non Black Box Face Recognition challenge aims to facilitate the research of\nfinding new adversarial attack techniques and their effectiveness in generating\nadversarial examples. In this challenge, the attack\"s nature is targeted-attack\non the black-box neural network where we have no knowledge about black-block\"s\ninner structure. The attacker must modify a set of five images of a single\nperson so that the neural network miss-classify them as target image which is a\nset of five images of another person. In this competition, we applied Momentum\nDiverse Input Iterative Fast Gradient Sign Method (M-DI2-FGSM) to make an\nadversarial attack on black-box face recognition system. We tested our method\non MCS 2018 Adversarial Attacks on Black Box Face Recognition challenge and\nfound competitive result. Our solution got validation score 1.404 which better\nthan baseline score 1.407 and stood 14 place among 132 teams in the\nleader-board. Further improvement can be achieved by finding improved feature\nextraction from source image, carefully chosen hyper-parameters, finding\nimproved substitute model of the black-box and better optimization method.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 14:05:16 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Milton", "Md Ashraful Alam", ""]]}, {"id": "1806.08981", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Jens Petersen, Jesper H Pedersen, Marleen de\n  Bruijne", "title": "Extracting Tree-structures in CT data by Tracking Multiple Statistically\n  Ranked Hypotheses", "comments": "Accepted for publication at the International Journal of Medical\n  Physics and Practice", "journal-ref": null, "doi": "10.1002/mp.13711", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we adapt a method based on multiple hypothesis tracking (MHT)\nthat has been shown to give state-of-the-art vessel segmentation results in\ninteractive settings, for the purpose of extracting trees. Regularly spaced\ntubular templates are fit to image data forming local hypotheses. These local\nhypotheses are used to construct the MHT tree, which is then traversed to make\nsegmentation decisions. However, some critical parameters in this method are\nscale-dependent and have an adverse effect when tracking structures of varying\ndimensions. We propose to use statistical ranking of local hypotheses in\nconstructing the MHT tree, which yields a probabilistic interpretation of\nscores across scales and helps alleviate the scale-dependence of MHT\nparameters. This enables our method to track trees starting from a single seed\npoint. Our method is evaluated on chest CT data to extract airway trees and\ncoronary arteries. In both cases, we show that our method performs\nsignificantly better than the original MHT method.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 15:03:09 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 15:23:20 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Petersen", "Jens", ""], ["Pedersen", "Jesper H", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1806.08990", "submitter": "Zhewei Huang", "authors": "Zhewei Huang, Wen Heng, Yuanzheng Tao, Shuchang Zhou", "title": "Stroke-based Character Reconstruction", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background elimination for noisy character images or character images from\nreal scene is still a challenging problem, due to the bewildering backgrounds,\nuneven illumination, low resolution and different distortions. We propose a\nstroke-based character reconstruction(SCR) method that use a weighted quadratic\nBezier curve(WQBC) to represent strokes of a character. Only training on our\nsynthetic data, our stroke extractor can achieve excellent reconstruction\neffect in real scenes. Meanwhile. It can also help achieve great ability in\ndefending adversarial attacks of character recognizers.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 15:47:34 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 06:38:00 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 13:44:28 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Huang", "Zhewei", ""], ["Heng", "Wen", ""], ["Tao", "Yuanzheng", ""], ["Zhou", "Shuchang", ""]]}, {"id": "1806.08991", "submitter": "Pierre Jacob", "authors": "Pierre Jacob, David Picard, Aymeric Histace, Edouard Klein", "title": "Leveraging Implicit Spatial Information in Global Features for Image\n  Retrieval", "comments": "8 pages, 2 figures and 1 table. Draft paper for conference, IEEE\n  International Conference on Image Processing (ICIP) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most image retrieval methods use global features that aggregate local\ndistinctive patterns into a single representation. However, the aggregation\nprocess destroys the relative spatial information by considering orderless sets\nof local descriptors. We propose to integrate relative spatial information into\nthe aggregation process by taking into account co-occurrences of local patterns\nin a tensor framework. The resulting signature called Improved Spatial Tensor\nAggregation (ISTA) is able to reach state of the art performances on well known\ndatasets such as Holidays, Oxford5k and Paris6k.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 15:47:46 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Jacob", "Pierre", ""], ["Picard", "David", ""], ["Histace", "Aymeric", ""], ["Klein", "Edouard", ""]]}, {"id": "1806.09025", "submitter": "Srishti Gautam", "authors": "Srishti Gautam, Harinarayan K. K., Nirmal Jith, Anil K. Sao, Arnav\n  Bhavsar, Adarsh Natarajan", "title": "Considerations for a PAP Smear Image Analysis System with CNN Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that for automated PAP-smear image classification, nucleus\nfeatures can be very informative. Therefore, the primary step for automated\nscreening can be cell-nuclei detection followed by segmentation of nuclei in\nthe resulting single cell PAP-smear images. We propose a patch based approach\nusing CNN for segmentation of nuclei in single cell images. We then pose the\nquestion of ion of segmentation for classification using representation\nlearning with CNN, and whether low-level CNN features may be useful for\nclassification. We suggest a CNN-based feature level analysis and a transfer\nlearning based approach for classification using both segmented as well full\nsingle cell images. We also propose a decision-tree based approach for\nclassification. Experimental results demonstrate the effectiveness of the\nproposed algorithms individually (with low-level CNN features), and\nsimultaneously proving the sufficiency of cell-nuclei detection (rather than\naccurate segmentation) for classification. Thus, we propose a system for\nanalysis of multi-cell PAP-smear images consisting of a simple nuclei detection\nalgorithm followed by classification using transfer learning.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 19:49:19 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Gautam", "Srishti", ""], ["K.", "Harinarayan K.", ""], ["Jith", "Nirmal", ""], ["Sao", "Anil K.", ""], ["Bhavsar", "Arnav", ""], ["Natarajan", "Adarsh", ""]]}, {"id": "1806.09045", "submitter": "Liang Mi", "authors": "Liang Mi, Wen Zhang, Xianfeng Gu, and Yalin Wang", "title": "Variational Wasserstein Clustering", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new clustering method based on optimal transportation. We solve\noptimal transportation with variational principles, and investigate the use of\npower diagrams as transportation plans for aggregating arbitrary domains into a\nfixed number of clusters. We iteratively drive centroids through target domains\nwhile maintaining the minimum clustering energy by adjusting the power\ndiagrams. Thus, we simultaneously pursue clustering and the Wasserstein\ndistances between the centroids and the target domains, resulting in a\nmeasure-preserving mapping. We demonstrate the use of our method in domain\nadaptation, remeshing, and representation learning on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 22:00:25 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 08:25:12 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 07:55:10 GMT"}, {"version": "v4", "created": "Thu, 26 Jul 2018 06:22:43 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Mi", "Liang", ""], ["Zhang", "Wen", ""], ["Gu", "Xianfeng", ""], ["Wang", "Yalin", ""]]}, {"id": "1806.09046", "submitter": "Thanh Hai Nguyen", "authors": "Thanh Hai Nguyen, Edi Prifti, Yann Chevaleyre, Nataliya Sokolovska and\n  Jean-Daniel Zucker", "title": "Disease Classification in Metagenomics with 2D Embeddings and Deep\n  Learning", "comments": "The annual French Conference in Machine Learning (CAp 2018) - La\n  Conf\\'erence sur l'Apprentissage automatique (CAp), June 2018, Rouen, France.\n  Oral presentation C12. http://cap2018.litislab.fr/Programme-en.html 10 pages,\n  7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) techniques have shown unprecedented success when applied\nto images, waveforms, and text. Generally, when the sample size ($N$) is much\nbigger than the number of features ($d$), DL often outperforms other machine\nlearning (ML) techniques, often through the use of Convolutional Neural\nNetworks (CNNs). However, in many bioinformatics fields (including\nmetagenomics), we encounter the opposite situation where $d$ is significantly\ngreater than $N$. In these situations, applying DL techniques would lead to\nsevere overfitting.\n  Here we aim to improve classification of various diseases with metagenomic\ndata through the use of CNNs. For this we proposed to represent metagenomic\ndata as images. The proposed Met2Img approach relies on taxonomic and t-SNE\nembeddings to transform abundance data into \"synthetic images\".\n  We applied our approach to twelve benchmark data sets including more than\n1400 metagenomic samples. Our results show significant improvements over the\nstate-of-the-art algorithms (Random Forest (RF), Support Vector Machine (SVM)).\nWe observe that the integration of phylogenetic information alongside abundance\ndata improves classification. The proposed approach is not only important in\nclassification setting but also allows to visualize complex metagenomic data.\nThe Met2Img is implemented in Python.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 22:01:27 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Nguyen", "Thanh Hai", ""], ["Prifti", "Edi", ""], ["Chevaleyre", "Yann", ""], ["Sokolovska", "Nataliya", ""], ["Zucker", "Jean-Daniel", ""]]}, {"id": "1806.09055", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Karen Simonyan, Yiming Yang", "title": "DARTS: Differentiable Architecture Search", "comments": "Published at ICLR 2019; Code and pretrained models available at\n  https://github.com/quark0/darts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the scalability challenge of architecture search by\nformulating the task in a differentiable manner. Unlike conventional approaches\nof applying evolution or reinforcement learning over a discrete and\nnon-differentiable search space, our method is based on the continuous\nrelaxation of the architecture representation, allowing efficient search of the\narchitecture using gradient descent. Extensive experiments on CIFAR-10,\nImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in\ndiscovering high-performance convolutional architectures for image\nclassification and recurrent architectures for language modeling, while being\norders of magnitude faster than state-of-the-art non-differentiable techniques.\nOur implementation has been made publicly available to facilitate further\nresearch on efficient architecture search algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 00:06:13 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 06:29:32 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Liu", "Hanxiao", ""], ["Simonyan", "Karen", ""], ["Yang", "Yiming", ""]]}, {"id": "1806.09074", "submitter": "Yukai Wang", "authors": "Yukai Wang, Qizhi Teng, Xiaohai He, Junxi Feng, Tingrong Zhang", "title": "CT-image Super Resolution Using 3D Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": "10.1016/j.cageo.2019.104314", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed Tomography (CT) imaging technique is widely used in geological\nexploration, medical diagnosis and other fields. In practice, however, the\nresolution of CT image is usually limited by scanning devices and great\nexpense. Super resolution (SR) methods based on deep learning have achieved\nsurprising performance in two-dimensional (2D) images. Unfortunately, there are\nfew effective SR algorithms for three-dimensional (3D) images. In this paper,\nwe proposed a novel network named as three-dimensional super resolution\nconvolutional neural network (3DSRCNN) to realize voxel super resolution for CT\nimages. To solve the practical problems in training process such as slow\nconvergence of network training, insufficient memory, etc., we utilized\nadjustable learning rate, residual-learning, gradient clipping, momentum\nstochastic gradient descent (SGD) strategies to optimize training procedure. In\naddition, we have explored the empirical guidelines to set appropriate number\nof layers of network and how to use residual learning strategy. Additionally,\nprevious learning-based algorithms need to separately train for different scale\nfactors for reconstruction, yet our single model can complete the multi-scale\nSR. At last, our method has better performance in terms of PSNR, SSIM and\nefficiency compared with conventional methods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 03:22:33 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Wang", "Yukai", ""], ["Teng", "Qizhi", ""], ["He", "Xiaohai", ""], ["Feng", "Junxi", ""], ["Zhang", "Tingrong", ""]]}, {"id": "1806.09078", "submitter": "Piotr Koniusz", "authors": "Yusuf Tas and Piotr Koniusz", "title": "CNN-based Action Recognition and Supervised Domain Adaptation on 3D Body\n  Skeletons via Kernel Feature Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is ubiquitous across many areas areas of computer vision. It\noften requires large scale datasets for training before being fine-tuned on\nsmall-to-medium scale problems. Activity, or, in other words, action\nrecognition, is one of many application areas of deep learning. While there\nexist many Convolutional Neural Network architectures that work with the RGB\nand optical flow frames, training on the time sequences of 3D body skeleton\njoints is often performed via recurrent networks such as LSTM.\n  In this paper, we propose a new representation which encodes sequences of 3D\nbody skeleton joints in texture-like representations derived from\nmathematically rigorous kernel methods. Such a representation becomes the first\nlayer in a standard CNN network e.g., ResNet-50, which is then used in the\nsupervised domain adaptation pipeline to transfer information from the source\nto target dataset. This lets us leverage the available Kinect-based data beyond\ntraining on a single dataset and outperform simple fine-tuning on any two\ndatasets combined in a naive manner. More specifically, in this paper we\nutilize the overlapping classes between datasets. We associate datapoints of\nthe same class via so-called commonality, known from the supervised domain\nadaptation. We demonstrate state-of-the-art results on three publicly available\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 04:06:18 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Tas", "Yusuf", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1806.09084", "submitter": "Piotr Koniusz", "authors": "Rui Zhang and Yusuf Tas and Piotr Koniusz", "title": "Artwork Identification from Wearable Camera Images for Enhancing\n  Experience of Museum Audiences", "comments": null, "journal-ref": "Museums and the Web, 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems based on image recognition could prove a vital tool in\nenhancing the experience of museum audiences. However, for practical systems\nutilizing wearable cameras, a number of challenges exist which affect the\nquality of image recognition. In this pilot study, we focus on recognition of\nmuseum collections by using a wearable camera in three different museum spaces.\nWe discuss the application of wearable cameras, and the practical and technical\nchallenges in devising a robust system that can recognize artworks viewed by\nthe visitors to create a detailed record of their visit. Specifically, to\nillustrate the impact of different kinds of museum spaces on image recognition,\nwe collect three training datasets of museum exhibits containing variety of\npaintings, clocks, and sculptures. Subsequently, we equip selected visitors\nwith wearable cameras to capture artworks viewed by them as they stroll along\nexhibitions. We use Convolutional Neural Networks (CNN) which are pre-trained\non the ImageNet dataset and fine-tuned on each of the training sets for the\npurpose of artwork identification. In the testing stage, we use CNNs to\nidentify artworks captured by the visitors with a wearable camera. We analyze\nthe accuracy of their recognition and provide an insight into the applicability\nof such a system to further engage audiences with museum exhibitions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 05:01:59 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Zhang", "Rui", ""], ["Tas", "Yusuf", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1806.09090", "submitter": "Mousumi Roy", "authors": "Mousumi Roy, Fusheng Wang, George Teodoro, Miriam B Vos, Alton Brad\n  Farris, and Jun Kong", "title": "Segmentation of Overlapped Steatosis in Whole-Slide Liver Histopathology\n  Microscopy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate steatosis quantification with pathology tissue samples is of high\nclinical importance. However, such pathology measurement is manually made in\nmost clinical practices, subject to severe reader variability due to large\nsampling bias and poor reproducibility. Although some computerized automated\nmethods are developed to quantify the steatosis regions, they present limited\nanalysis capacity for high resolution whole-slide microscopy images and\naccurate overlapped steatosis division. In this paper, we propose a method that\nextracts an individual whole tissue piece at high resolution with minimum\nbackground area by estimating tissue bounding box and rotation angle. This is\nfollowed by the segmentation and segregation of steatosis regions with high\ncurvature point detection and an ellipse fitting quality assessment method. We\nvalidate our method with isolated and overlapped steatosis regions in liver\ntissue images of 11 patients. The experimental results suggest that our method\nis promising for enhanced support of steatosis quantization during the\npathology review for liver disease treatment.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 06:14:06 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Roy", "Mousumi", ""], ["Wang", "Fusheng", ""], ["Teodoro", "George", ""], ["Vos", "Miriam B", ""], ["Farris", "Alton Brad", ""], ["Kong", "Jun", ""]]}, {"id": "1806.09093", "submitter": "Mousumi Roy", "authors": "Mousumi Roy, Fusheng Wang, George Teodoro, Jose Velazqeuz Vega, Daniel\n  Brat and Jun Kong", "title": "Analysis of Cellular Feature Differences of Astrocytomas with Distinct\n  Mutational Profiles Using Digitized Histopathology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular phenotypic features derived from histopathology images are the basis\nof pathologic diagnosis and are thought to be related to underlying molecular\nprofiles. Due to overwhelming cell numbers and population heterogeneity, it\nremains challenging to quantitatively compute and compare features of cells\nwith distinct molecular signatures. In this study, we propose a self-reliant\nand efficient analysis framework that supports quantitative analysis of\ncellular phenotypic difference across distinct molecular groups. To demonstrate\nefficacy, we quantitatively analyze astrocytomas that are molecularly\ncharacterized as either Isocitrate Dehydrogenase (IDH) mutant (MUT) or wildtype\n(WT) using imaging data from The Cancer Genome Atlas database. Representative\ncell instances that are phenotypically different between these two groups are\nretrieved after segmentation, feature computation, data pruning, dimensionality\nreduction, and unsupervised clustering. Our analysis is generic and can be\napplied to a wide set of cell-based biomedical research.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 06:27:27 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Roy", "Mousumi", ""], ["Wang", "Fusheng", ""], ["Teodoro", "George", ""], ["Vega", "Jose Velazqeuz", ""], ["Brat", "Daniel", ""], ["Kong", "Jun", ""]]}, {"id": "1806.09152", "submitter": "Ahmed Abobakr", "authors": "Ahmed Abobakr, Mohammed Hossny and Saeid Nahavandi", "title": "SSIMLayer: Towards Robust Deep Representation Learning via Nonlinear\n  Structural Similarity", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deeper convolutional neural networks provide more capacity to approximate\ncomplex mapping functions. However, increasing network depth imposes\ndifficulties on training and increases model complexity. This paper presents a\nnew nonlinear computational layer of considerably high capacity to the deep\nconvolutional neural network architectures. This layer performs a set of\ncomprehensive convolution operations that mimics the overall function of the\nhuman visual system (HVS) via focusing on learning structural information in\nits input. The core of its computations is evaluating the components of the\nstructural similarity metric (SSIM) in a setting that allows the kernels to\nlearn to match structural information. The proposed SSIMLayer is inherently\nnonlinear and hence, it does not require subsequent nonlinear transformations.\nExperiments conducted on CIFAR-10 benchmark demonstrates that the SSIMLayer\nprovides better convergence than the traditional convolutional layer, bypasses\nthe need for nonlinear transformations and shows more robustness against noise\nperturbations and adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 14:23:49 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 14:08:31 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Abobakr", "Ahmed", ""], ["Hossny", "Mohammed", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "1806.09158", "submitter": "Ribana Roscher", "authors": "J. Oehrlein, A. F\\\"orster, D. Schunck, Y. Dehbi, R. Roscher, J.-H.\n  Haunert", "title": "Inferring Routing Preferences of Bicyclists from Sparse Sets of\n  Trajectories", "comments": "accepted to International Conference on Smart Data and Smart Cities", "journal-ref": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial\n  Information Sciences 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the criteria that bicyclists apply when they choose their\nroutes is crucial for planning new bicycle paths or recommending routes to\nbicyclists. This is becoming more and more important as city councils are\nbecoming increasingly aware of limitations of the transport infrastructure and\nproblems related to automobile traffic. Since different groups of cyclists have\ndifferent preferences, however, searching for a single set of criteria is prone\nto failure. Therefore, in this paper, we present a new approach to classify\ntrajectories recorded and shared by bicyclists into different groups and, for\neach group, to identify favored and unfavored road types. Based on these\nresults we show how to assign weights to the edges of a graph representing the\nroad network such that minimum-weight paths in the graph, which can be computed\nwith standard shortest-path algorithms, correspond to adequate routes. Our\nmethod combines known algorithms for machine learning and the analysis of\ntrajectories in an innovative way and, thereby, constitutes a new comprehensive\nsolution for the problem of deriving routing preferences from initially\nunclassified trajectories. An important property of our method is that it\nyields reasonable results even if the given set of trajectories is sparse in\nthe sense that it does not cover all segments of the cycle network.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 14:42:18 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Oehrlein", "J.", ""], ["F\u00f6rster", "A.", ""], ["Schunck", "D.", ""], ["Dehbi", "Y.", ""], ["Roscher", "R.", ""], ["Haunert", "J. -H.", ""]]}, {"id": "1806.09170", "submitter": "Lucas Ribas", "authors": "Lucas C. Ribas, Jarbas J. M. Sa Junior, Leonardo F. S. Scabini, Odemir\n  M. Bruno", "title": "Fusion of complex networks and randomized neural networks for texture\n  analysis", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a high discriminative texture analysis method based on\nthe fusion of complex networks and randomized neural networks. In this\napproach, the input image is modeled as a complex networks and its topological\nproperties as well as the image pixels are used to train randomized neural\nnetworks in order to create a signature that represents the deep\ncharacteristics of the texture. The results obtained surpassed the accuracies\nof many methods available in the literature. This performance demonstrates that\nour proposed approach opens a promising source of research, which consists of\nexploring the synergy of neural networks and complex networks in the texture\nanalysis field.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 15:57:19 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 18:56:15 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ribas", "Lucas C.", ""], ["Junior", "Jarbas J. M. Sa", ""], ["Scabini", "Leonardo F. S.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1806.09174", "submitter": "Noshaba Cheema", "authors": "Noshaba Cheema, Somayeh Hosseini, Janis Sprenger, Erik Herrmann, Han\n  Du, Klaus Fischer, Philipp Slusallek", "title": "Dilated Temporal Fully-Convolutional Network for Semantic Segmentation\n  of Motion Capture Data", "comments": "Eurographics/ ACM SIGGRAPH Symposium on Computer Animation - Posters\n  2018;\n  $\\href{http://people.mpi-inf.mpg.de/~ncheema/SCA2018_poster.pdf}{\\textit{Poster\n  can be found here.}}$", "journal-ref": null, "doi": "10.2312/sca.20181185", "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of motion capture sequences plays a key part in many\ndata-driven motion synthesis frameworks. It is a preprocessing step in which\nlong recordings of motion capture sequences are partitioned into smaller\nsegments. Afterwards, additional methods like statistical modeling can be\napplied to each group of structurally-similar segments to learn an abstract\nmotion manifold. The segmentation task however often remains a manual task,\nwhich increases the effort and cost of generating large-scale motion databases.\nWe therefore propose an automatic framework for semantic segmentation of motion\ncapture data using a dilated temporal fully-convolutional network. Our model\noutperforms a state-of-the-art model in action segmentation, as well as three\nnetworks for sequence modeling. We further show our model is robust against\nhigh noisy training labels.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 16:40:07 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Cheema", "Noshaba", ""], ["Hosseini", "Somayeh", ""], ["Sprenger", "Janis", ""], ["Herrmann", "Erik", ""], ["Du", "Han", ""], ["Fischer", "Klaus", ""], ["Slusallek", "Philipp", ""]]}, {"id": "1806.09183", "submitter": "Piotr Koniusz", "authors": "Piotr Koniusz and Hongguang Zhang and Fatih Porikli", "title": "A Deeper Look at Power Normalizations", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power Normalizations (PN) are very useful non-linear operators in the context\nof Bag-of-Words data representations as they tackle problems such as feature\nimbalance. In this paper, we reconsider these operators in the deep learning\nsetup by introducing a novel layer that implements PN for non-linear pooling of\nfeature maps. Specifically, by using a kernel formulation, our layer combines\nthe feature vectors and their respective spatial locations in the feature maps\nproduced by the last convolutional layer of CNN. Linearization of such a kernel\nresults in a positive definite matrix capturing the second-order statistics of\nthe feature vectors, to which PN operators are applied. We study two types of\nPN functions, namely (i) MaxExp and (ii) Gamma, addressing their role and\nmeaning in the context of nonlinear pooling. We also provide a probabilistic\ninterpretation of these operators and derive their surrogates with well-behaved\ngradients for end-to-end CNN learning. We apply our theory to practice by\nimplementing the PN layer on a ResNet-50 model and showcase experiments on four\nbenchmarks for fine-grained recognition, scene recognition, and material\nclassification. Our results demonstrate state-of-the-art performance across all\nthese tasks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 17:38:15 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Koniusz", "Piotr", ""], ["Zhang", "Hongguang", ""], ["Porikli", "Fatih", ""]]}, {"id": "1806.09186", "submitter": "Jiayang Liu", "authors": "Jiayang Liu, Weiming Zhang, Yiwei Zhang, Dongdong Hou, Yujia Liu,\n  Hongyue Zha and Nenghai Yu", "title": "Detection based Defense against Adversarial Examples from the\n  Steganalysis Point of View", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have recently led to significant improvements in\nmany fields. However, DNNs are vulnerable to adversarial examples which are\nsamples with imperceptible perturbations while dramatically misleading the\nDNNs. Moreover, adversarial examples can be used to perform an attack on\nvarious kinds of DNN based systems, even if the adversary has no access to the\nunderlying model. Many defense methods have been proposed, such as obfuscating\ngradients of the networks or detecting adversarial examples. However it is\nproved out that these defense methods are not effective or cannot resist\nsecondary adversarial attacks. In this paper, we point out that steganalysis\ncan be applied to adversarial examples detection, and propose a method to\nenhance steganalysis features by estimating the probability of modifications\ncaused by adversarial attacks. Experimental results show that the proposed\nmethod can accurately detect adversarial examples. Moreover, secondary\nadversarial attacks cannot be directly performed to our method because our\nmethod is not based on a neural network but based on high-dimensional\nartificial features and FLD (Fisher Linear Discriminant) ensemble.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 04:57:20 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 01:37:46 GMT"}, {"version": "v3", "created": "Mon, 24 Dec 2018 08:25:50 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Liu", "Jiayang", ""], ["Zhang", "Weiming", ""], ["Zhang", "Yiwei", ""], ["Hou", "Dongdong", ""], ["Liu", "Yujia", ""], ["Zha", "Hongyue", ""], ["Yu", "Nenghai", ""]]}, {"id": "1806.09228", "submitter": "Junru Wu", "authors": "Junru Wu, Yue Wang, Zhenyu Wu, Zhangyang Wang, Ashok Veeraraghavan,\n  Yingyan Lin", "title": "Deep $k$-Means: Re-Training and Parameter Sharing with Harder Cluster\n  Assignments for Compressing Deep Convolutions", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current trend of pushing CNNs deeper with convolutions has created a\npressing demand to achieve higher compression gains on CNNs where convolutions\ndominate the computation and parameter amount (e.g., GoogLeNet, ResNet and Wide\nResNet). Further, the high energy consumption of convolutions limits its\ndeployment on mobile devices. To this end, we proposed a simple yet effective\nscheme for compressing convolutions though applying k-means clustering on the\nweights, compression is achieved through weight-sharing, by only recording $K$\ncluster centers and weight assignment indexes. We then introduced a novel\nspectrally relaxed $k$-means regularization, which tends to make hard\nassignments of convolutional layer weights to $K$ learned cluster centers\nduring re-training. We additionally propose an improved set of metrics to\nestimate energy consumption of CNN hardware implementations, whose estimation\nresults are verified to be consistent with previously proposed energy\nestimation tool extrapolated from actual hardware measurements. We finally\nevaluated Deep $k$-Means across several CNN models in terms of both compression\nratio and energy consumption reduction, observing promising results without\nincurring accuracy loss. The code is available at\nhttps://github.com/Sandbox3aster/Deep-K-Means\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 22:49:24 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Wu", "Junru", ""], ["Wang", "Yue", ""], ["Wu", "Zhenyu", ""], ["Wang", "Zhangyang", ""], ["Veeraraghavan", "Ashok", ""], ["Lin", "Yingyan", ""]]}, {"id": "1806.09230", "submitter": "Soochahn Lee Dr", "authors": "Kyoung Jin Noh, Sang Jun Park, Soochahn Lee", "title": "Scale Space Approximation in Convolutional Neural Networks for Retinal\n  Vessel Segmentation", "comments": "10 pages, 7 figures", "journal-ref": "Computer Methods and Programs in Biomedicine, Volume 178,\n  September 2019, Pages 237-246", "doi": "10.1016/j.cmpb.2019.06.030Get", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal images have the highest resolution and clarity among medical images.\nThus, vessel analysis in retinal images may facilitate early diagnosis and\ntreatment of many chronic diseases. In this paper, we propose a novel\nmulti-scale residual convolutional neural network structure based on a\n\\emph{scale-space approximation (SSA)} block of layers, comprising subsampling\nand subsequent upsampling, for multi-scale representation. Through analysis in\nthe frequency domain, we show that this block structure is a close\napproximation of Gaussian filtering, the operation to achieve scale variations\nin scale-space theory. Experimental evaluations demonstrate that the proposed\nnetwork outperforms current state-of-the-art methods. Ablative analysis shows\nthat the SSA is indeed an important factor in performance improvement.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 23:11:54 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 08:09:52 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Noh", "Kyoung Jin", ""], ["Park", "Sang Jun", ""], ["Lee", "Soochahn", ""]]}, {"id": "1806.09241", "submitter": "Xiaoguang Han", "authors": "Yulong Shi, Xiaoguang Han, Nianjuan Jiang, Kun Zhou, Kui Jia, Jiangbo\n  Lu", "title": "FBI-Pose: Towards Bridging the Gap between 2D Images and 3D Human Poses\n  using Forward-or-Backward Information", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although significant advances have been made in the area of human poses\nestimation from images using deep Convolutional Neural Network (ConvNet), it\nremains a big challenge to perform 3D pose inference in-the-wild. This is due\nto the difficulty to obtain 3D pose groundtruth for outdoor environments. In\nthis paper, we propose a novel framework to tackle this problem by exploiting\nthe information of each bone indicating if it is forward or backward with\nrespect to the view of the camera(we term it Forwardor-Backward Information\nabbreviated as FBI). Our method firstly trains a ConvNet with two branches\nwhich maps an image of a human to both the 2D joint locations and the FBI of\nbones. These information is further fed into a deep regression network to\npredict the 3D positions of joints. To support the training, we also develop an\nannotation user interface and labeled such FBI for around 12K in-the-wild\nimages which are randomly selected from MPII (a public dataset of 2D pose\nannotation). Our experimental results on the standard benchmarks demonstrate\nthat our approach outperforms state-of-the-art methods both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 00:35:10 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Shi", "Yulong", ""], ["Han", "Xiaoguang", ""], ["Jiang", "Nianjuan", ""], ["Zhou", "Kun", ""], ["Jia", "Kui", ""], ["Lu", "Jiangbo", ""]]}, {"id": "1806.09248", "submitter": "Jueqin Qiu", "authors": "Jueqin Qiu, Haisong Xu, Zhengnan Ye", "title": "Color Constancy by Reweighting Image Feature Maps", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 29 (2020) 5711-5721", "doi": "10.1109/TIP.2020.2985296", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a novel illuminant color estimation framework is proposed for\ncomputational color constancy, which incorporates the high representational\ncapacity of deep-learning-based models and the great interpretability of\nassumption-based models. The well-designed building block, feature map reweight\nunit (ReWU), helps to achieve comparative accuracy on benchmark datasets with\nrespect to prior state-of-the-art deep learning based models while requiring\nmore compact model size and cheaper computational cost. In addition to local\ncolor estimation, a confidence estimation branch is also included such that the\nmodel is able to simultaneously produce point estimate and its uncertainty\nestimate, which provides useful clues for local estimates aggregation and\nmultiple illumination estimation. The source code and the dataset have been\nmade available at https://github.com/QiuJueqin/Reweight-CC.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 01:50:26 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 07:36:55 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 13:38:01 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Qiu", "Jueqin", ""], ["Xu", "Haisong", ""], ["Ye", "Zhengnan", ""]]}, {"id": "1806.09256", "submitter": "Cagatay Demiralp", "authors": "Marco Cavallo and \\c{C}a\\u{g}atay Demiralp", "title": "Track Xplorer: A System for Visual Analysis of Sensor-based Motor\n  Activity Predictions", "comments": "EuroVis'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid commoditization of wearable sensors, detecting human movements\nfrom sensor datasets has become increasingly common over a wide range of\napplications. To detect activities, data scientists iteratively experiment with\ndifferent classifiers before deciding which model to deploy. Effective\nreasoning about and comparison of alternative classifiers are crucial in\nsuccessful model development. This is, however, inherently difficult in\ndeveloping classifiers for sensor data, where the intricacy of long temporal\nsequences, high prediction frequency, and imprecise labeling make standard\nevaluation methods relatively ineffective and even misleading. We introduce\nTrack Xplorer, an interactive visualization system to query, analyze, and\ncompare the predictions of sensor-data classifiers. Track Xplorer enables users\nto interactively explore and compare the results of different classifiers, and\nassess their accuracy with respect to the ground-truth labels and video.\nThrough integration with a version control system, Track Xplorer supports\ntracking of models and their parameters without additional workload on model\ndevelopers. Track Xplorer also contributes an extensible algebra over track\nrepresentations to filter, compose, and compare classification outputs,\nenabling users to reason effectively about classifier performance. We apply\nTrack Xplorer in a collaborative project to develop classifiers to detect\nmovements from multisensor data gathered from Parkinson's disease patients. We\ndemonstrate how Track Xplorer helps identify early on possible systemic data\nerrors, effectively track and compare the results of different classifiers, and\nreason about and pinpoint the causes of misclassifications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 02:19:24 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Cavallo", "Marco", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1806.09266", "submitter": "Kuan Fang", "authors": "Kuan Fang, Yuke Zhu, Animesh Garg, Andrey Kurenkov, Viraj Mehta, Li\n  Fei-Fei, Silvio Savarese", "title": "Learning Task-Oriented Grasping for Tool Manipulation from Simulated\n  Self-Supervision", "comments": "RSS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tool manipulation is vital for facilitating robots to complete challenging\ntask goals. It requires reasoning about the desired effect of the task and thus\nproperly grasping and manipulating the tool to achieve the task. Task-agnostic\ngrasping optimizes for grasp robustness while ignoring crucial task-specific\nconstraints. In this paper, we propose the Task-Oriented Grasping Network\n(TOG-Net) to jointly optimize both task-oriented grasping of a tool and the\nmanipulation policy for that tool. The training process of the model is based\non large-scale simulated self-supervision with procedurally generated tool\nobjects. We perform both simulated and real-world experiments on two tool-based\nmanipulation tasks: sweeping and hammering. Our model achieves overall 71.1%\ntask success rate for sweeping and 80.0% task success rate for hammering.\nSupplementary material is available at: bit.ly/task-oriented-grasp\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 03:08:28 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Fang", "Kuan", ""], ["Zhu", "Yuke", ""], ["Garg", "Animesh", ""], ["Kurenkov", "Andrey", ""], ["Mehta", "Viraj", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""]]}, {"id": "1806.09278", "submitter": "Moyini Yao", "authors": "Yuan Liu and Moyini Yao", "title": "Best Vision Technologies Submission to ActivityNet Challenge 2018-Task:\n  Dense-Captioning Events in Videos", "comments": "Rank 2 in ActivityNet Captions Challenge 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note describes the details of our solution to the dense-captioning\nevents in videos task of ActivityNet Challenge 2018. Specifically, we solve\nthis problem with a two-stage way, i.e., first temporal event proposal and then\nsentence generation. For temporal event proposal, we directly leverage the\nthree-stage workflow in [13, 16]. For sentence generation, we capitalize on\nLSTM-based captioning framework with temporal attention mechanism (dubbed as\nLSTM-T). Moreover, the input visual sequence to the LSTM-based video captioning\nmodel is comprised of RGB and optical flow images. At inference, we adopt a\nlate fusion scheme to fuse the two LSTM-based captioning models for sentence\ngeneration.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 04:11:03 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Liu", "Yuan", ""], ["Yao", "Moyini", ""]]}, {"id": "1806.09283", "submitter": "Xiaobin Liu", "authors": "Xiaobin Liu, Shiliang Zhang, Qingming Huang, Wen Gao", "title": "RAM: A Region-Aware Deep Model for Vehicle Re-Identification", "comments": "Accepted by ICME 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works on vehicle Re-ID mainly focus on extracting global features\nand learning distance metrics. Because some vehicles commonly share same model\nand maker, it is hard to distinguish them based on their global appearances.\nCompared with the global appearance, local regions such as decorations and\ninspection stickers attached to the windshield, may be more distinctive for\nvehicle Re-ID. To embed the detailed visual cues in those local regions, we\npropose a Region-Aware deep Model (RAM). Specifically, in addition to\nextracting global features, RAM also extracts features from a series of local\nregions. As each local region conveys more distinctive visual cues, RAM\nencourages the deep model to learn discriminative features. We also introduce a\nnovel learning algorithm to jointly use vehicle IDs, types/models, and colors\nto train the RAM. This strategy fuses more cues for training and results in\nmore discriminative global and regional features. We evaluate our methods on\ntwo large-scale vehicle Re-ID datasets, i.e., VeRi and VehicleID. Experimental\nresults show our methods achieve promising performance in comparison with\nrecent works.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 04:40:49 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Liu", "Xiaobin", ""], ["Zhang", "Shiliang", ""], ["Huang", "Qingming", ""], ["Gao", "Wen", ""]]}, {"id": "1806.09316", "submitter": "Abdelkader Bellarbi", "authors": "Hayet Belghit, Abdelkader Bellarbi, Nadia Zenati, Samir Otmane", "title": "Vision-based Pose Estimation for Augmented Reality : A Comparison Study", "comments": "IEEE International Conference on Pattern Analysis and Intelligent\n  Systems PAIS'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality aims to enrich our real world by inserting 3D virtual\nobjects. In order to accomplish this goal, it is important that virtual\nelements are rendered and aligned in the real scene in an accurate and visually\nacceptable way. The solution of this problem can be related to a pose\nestimation and 3D camera localization. This paper presents a survey on\ndifferent approaches of 3D pose estimation in augmented reality and gives\nclassification of key-points-based techniques. The study given in this paper\nmay help both developers and researchers in the field of augmented reality.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 08:01:45 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Belghit", "Hayet", ""], ["Bellarbi", "Abdelkader", ""], ["Zenati", "Nadia", ""], ["Otmane", "Samir", ""]]}, {"id": "1806.09334", "submitter": "Hamed Heidari-Gorji", "authors": "Hamed Heidari Gorji (1 and 2), Sajjad Zabbah (2), Reza Ebrahimpour (1\n  and 2) ((1) Faculty of Computer Engineering, Shahid Rajaee Teacher Training\n  University, Tehran, Iran (2) School of Cognitive Sciences, Institute for\n  Research in Fundamental Sciences, Tehran, Iran)", "title": "A temporal neural network model for object recognition using a\n  biologically plausible decision making layer", "comments": "Version 2 contains more details about model. Comparisons with some\n  known deep neural networks have been included and are shown in figure 7. text\n  was corrected and edited", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain can recognize different objects as ones that it has experienced before.\nThe recognition accuracy and its processing time depend on task properties such\nas viewing condition, level of noise and etc. Recognition accuracy can be well\nexplained by different models. However, less attention has been paid to the\nprocessing time and the ones that do, are not biologically plausible. By\nextracting features temporally as well as utilizing an accumulation to bound\ndecision making model, an object recognition model accounting for both\nrecognition time and accuracy is proposed. To temporally extract informative\nfeatures in support of possible classes of stimuli, a hierarchical spiking\nneural network, called spiking HMAX is modified. In the decision making part of\nthe model the extracted information accumulates over time using accumulator\nunits. The input category is determined as soon as any of the accumulators\nreaches a threshold, called decision bound. Results show that not only does the\nmodel follow human accuracy in a psychophysics task better than the classic\nspiking HMAX model, but also it predicts human response time in each choice.\nResults provide enough evidence that temporal representation of features are\ninformative since they can improve the accuracy of a biological plausible\ndecision maker over time. This is also in line with the well-known idea of\nspeed accuracy trade-off in decision making studies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 09:04:28 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 18:24:50 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Gorji", "Hamed Heidari", "", "1 and 2"], ["Zabbah", "Sajjad", "", "1\n  and 2"], ["Ebrahimpour", "Reza", "", "1\n  and 2"]]}, {"id": "1806.09346", "submitter": "Andrey Bokovoy", "authors": "Andrey Bokovoy and Konstantin Yakovlev", "title": "Sparse 3D Point-cloud Map Upsampling and Noise Removal as a vSLAM\n  Post-processing Step: Experimental Evaluation", "comments": "10 pages, 4 figures, camera-ready version of paper for \"The 3rd\n  International Conference on Interactive Collaborative Robotics (ICR 2018)\"", "journal-ref": null, "doi": "10.1007/978-3-319-99582-3_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monocular vision-based simultaneous localization and mapping (vSLAM) is\none of the most challenging problem in mobile robotics and computer vision. In\nthis work we study the post-processing techniques applied to sparse 3D\npoint-cloud maps, obtained by feature-based vSLAM algorithms. Map\npost-processing is split into 2 major steps: 1) noise and outlier removal and\n2) upsampling. We evaluate different combinations of known algorithms for\noutlier removing and upsampling on datasets of real indoor and outdoor\nenvironments and identify the most promising combination. We further use it to\nconvert a point-cloud map, obtained by the real UAV performing indoor flight to\n3D voxel grid (octo-map) potentially suitable for path planning.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 09:33:48 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Bokovoy", "Andrey", ""], ["Yakovlev", "Konstantin", ""]]}, {"id": "1806.09410", "submitter": "David K\\\"ugler", "authors": "David K\\\"ugler, Alexander Distergoft, Arjan Kuijper, Anirban\n  Mukhopadhyay", "title": "Exploring Adversarial Examples: Patterns of One-Pixel Attacks", "comments": "Figure 4 corrected from published Version to correct y-axis labels", "journal-ref": "Understanding and Interpreting Machine Learning in Medical Image\n  Computing Applications Lecture Notes in Computer Science vol. 11038. Springer\n  International Publishing, Cham (2018)", "doi": "10.1007/978-3-030-02628-8_8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failure cases of black-box deep learning, e.g. adversarial examples, might\nhave severe consequences in healthcare. Yet such failures are mostly studied in\nthe context of real-world images with calibrated attacks. To demystify the\nadversarial examples, rigorous studies need to be designed. Unfortunately,\ncomplexity of the medical images hinders such study design directly from the\nmedical images. We hypothesize that adversarial examples might result from the\nincorrect mapping of image space to the low dimensional generation manifold by\ndeep networks. To test the hypothesis, we simplify a complex medical problem\nnamely pose estimation of surgical tools into its barest form. An analytical\ndecision boundary and exhaustive search of the one-pixel attack across multiple\nimage dimensions let us localize the regions of frequent successful one-pixel\nattacks at the image space.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 12:20:49 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 16:34:57 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["K\u00fcgler", "David", ""], ["Distergoft", "Alexander", ""], ["Kuijper", "Arjan", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "1806.09445", "submitter": "Beatriz Quintino Ferreira", "authors": "Beatriz Quintino Ferreira, Lu\\'is Ba\\'ia, Jo\\~ao Faria, Ricardo\n  Gamelas Sousa", "title": "A Unified Model with Structured Output for Fashion Images Classification", "comments": "Accepted in KDD 2018's AI for Fashion workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A picture is worth a thousand words. Albeit a clich\\'e, for the fashion\nindustry, an image of a clothing piece allows one to perceive its category\n(e.g., dress), sub-category (e.g., day dress) and properties (e.g., white\ncolour with floral patterns). The seasonal nature of the fashion industry\ncreates a highly dynamic and creative domain with evermore data, making it\nunpractical to manually describe a large set of images (of products). In this\npaper, we explore the concept of visual recognition for fashion images through\nan end-to-end architecture embedding the hierarchical nature of the annotations\ndirectly into the model. Towards that goal, and inspired by the work of [7], we\nhave modified and adapted the original architecture proposal. Namely, we have\nremoved the message passing layer symmetry to cope with Farfetch category tree,\nadded extra layers for hierarchy level specificity, and moved the message\npassing layer into an enriched latent space. We compare the proposed unified\narchitecture against state-of-the-art models and demonstrate the performance\nadvantage of our model for structured multi-level categorization on a dataset\nof about 350k fashion product images.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:19:47 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Ferreira", "Beatriz Quintino", ""], ["Ba\u00eda", "Lu\u00eds", ""], ["Faria", "Jo\u00e3o", ""], ["Sousa", "Ricardo Gamelas", ""]]}, {"id": "1806.09453", "submitter": "Nikita Jaipuria", "authors": "Golnaz Habibi, Nikita Jaipuria, Jonathan P. How", "title": "Context-Aware Pedestrian Motion Prediction In Urban Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel context-based approach for pedestrian motion\nprediction in crowded, urban intersections, with the additional flexibility of\nprediction in similar, but new, environments. Previously, Chen et. al. combined\nMarkovian-based and clustering-based approaches to learn motion primitives in a\ngrid-based world and subsequently predict pedestrian trajectories by modeling\nthe transition between learned primitives as a Gaussian Process (GP). This work\nextends that prior approach by incorporating semantic features from the\nenvironment (relative distance to curbside and status of pedestrian traffic\nlights) in the GP formulation for more accurate predictions of pedestrian\ntrajectories over the same timescale. We evaluate the new approach on\nreal-world data collected using one of the vehicles in the MIT Mobility On\nDemand fleet. The results show 12.5% improvement in prediction accuracy and a\n2.65 times reduction in Area Under the Curve (AUC), which is used as a metric\nto quantify the span of predicted set of trajectories, such that a lower AUC\ncorresponds to a higher level of confidence in the future direction of\npedestrian motion.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:45:57 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Habibi", "Golnaz", ""], ["Jaipuria", "Nikita", ""], ["How", "Jonathan P.", ""]]}, {"id": "1806.09507", "submitter": "Youbao Tang", "authors": "Youbao Tang, Adam P. Harrison, Mohammadhadi Bagheri, Jing Xiao, Ronald\n  M. Summers", "title": "Semi-Automatic RECIST Labeling on CT Scans with Cascaded Convolutional\n  Neural Networks", "comments": "Accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response evaluation criteria in solid tumors (RECIST) is the standard\nmeasurement for tumor extent to evaluate treatment responses in cancer\npatients. As such, RECIST annotations must be accurate. However, RECIST\nannotations manually labeled by radiologists require professional knowledge and\nare time-consuming, subjective, and prone to inconsistency among different\nobservers. To alleviate these problems, we propose a cascaded convolutional\nneural network based method to semi-automatically label RECIST annotations and\ndrastically reduce annotation time. The proposed method consists of two stages:\nlesion region normalization and RECIST estimation. We employ the spatial\ntransformer network (STN) for lesion region normalization, where a localization\nnetwork is designed to predict the lesion region and the transformation\nparameters with a multi-task learning strategy. For RECIST estimation, we adapt\nthe stacked hourglass network (SHN), introducing a relationship constraint loss\nto improve the estimation precision. STN and SHN can both be learned in an\nend-to-end fashion. We train our system on the DeepLesion dataset, obtaining a\nconsensus model trained on RECIST annotations performed by multiple\nradiologists over a multi-year period. Importantly, when judged against the\ninter-reader variability of two additional radiologist raters, our system\nperforms more stably and with less variability, suggesting that RECIST\nannotations can be reliably obtained with reduced labor and time.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 14:52:07 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Tang", "Youbao", ""], ["Harrison", "Adam P.", ""], ["Bagheri", "Mohammadhadi", ""], ["Xiao", "Jing", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1806.09521", "submitter": "Xingtong Liu", "authors": "Xingtong Liu, Ayushi Sinha, Mathias Unberath, Masaru Ishii, Gregory\n  Hager, Russell H. Taylor, Austin Reiter", "title": "Self-supervised Learning for Dense Depth Estimation in Monocular\n  Endoscopy", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-3-030-01201-4_15", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised approach to training convolutional neural\nnetworks for dense depth estimation from monocular endoscopy data without a\npriori modeling of anatomy or shading. Our method only requires sequential data\nfrom monocular endoscopic videos and a multi-view stereo reconstruction method,\ne.g. structure from motion, that supervises learning in a sparse but accurate\nmanner. Consequently, our method requires neither manual interaction, such as\nscaling or labeling, nor patient CT in the training and application phases. We\ndemonstrate the performance of our method on sinus endoscopy data from two\npatients and validate depth prediction quantitatively using corresponding\npatient CT scans where we found submillimeter residual errors.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 15:12:57 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 14:42:23 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Liu", "Xingtong", ""], ["Sinha", "Ayushi", ""], ["Unberath", "Mathias", ""], ["Ishii", "Masaru", ""], ["Hager", "Gregory", ""], ["Taylor", "Russell H.", ""], ["Reiter", "Austin", ""]]}, {"id": "1806.09522", "submitter": "Sulaiman Vesal", "authors": "Sulaiman Vesal, Nishant Ravikumar, Andreas Maier", "title": "SkinNet: A Deep Learning Framework for Skin Lesion Segmentation", "comments": "2 pages, submitted to NSS/MIC 2018", "journal-ref": null, "doi": "10.1109/NSSMIC.2018.8824732", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a steady increase in the incidence of skin cancer worldwide,\nwith a high rate of mortality. Early detection and segmentation of skin lesions\nare crucial for timely diagnosis and treatment, necessary to improve the\nsurvival rate of patients. However, skin lesion segmentation is a challenging\ntask due to the low contrast of lesions and their high similarity in terms of\nappearance, to healthy tissue. This underlines the need for an accurate and\nautomatic approach for skin lesion segmentation. To tackle this issue, we\npropose a convolutional neural network (CNN) called SkinNet. The proposed CNN\nis a modified version of U-Net. We compared the performance of our approach\nwith other state-of-the-art techniques, using the ISBI 2017 challenge dataset.\nOur approach outperformed the others in terms of the Dice coefficient, Jaccard\nindex and sensitivity, evaluated on the held-out challenge test data set,\nacross 5-fold cross validation experiments. SkinNet achieved an average value\nof 85.10, 76.67 and 93.0%, for the DC, JI, and SE, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 15:14:31 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Vesal", "Sulaiman", ""], ["Ravikumar", "Nishant", ""], ["Maier", "Andreas", ""]]}, {"id": "1806.09565", "submitter": "Shuo Liu", "authors": "Shuo Liu, Vijay John, Erik Blasch, Zheng Liu, Ying Huang", "title": "IR2VI: Enhanced Night Environmental Perception by Unsupervised Thermal\n  Image Translation", "comments": "Present at CVPR Workshops 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context enhancement is critical for night vision (NV) applications,\nespecially for the dark night situation without any artificial lights. In this\npaper, we present the infrared-to-visual (IR2VI) algorithm, a novel\nunsupervised thermal-to-visible image translation framework based on generative\nadversarial networks (GANs). IR2VI is able to learn the intrinsic\ncharacteristics from VI images and integrate them into IR images. Since the\nexisting unsupervised GAN-based image translation approaches face several\nchallenges, such as incorrect mapping and lack of fine details, we propose a\nstructure connection module and a region-of-interest (ROI) focal loss method to\naddress the current limitations. Experimental results show the superiority of\nthe IR2VI algorithm over baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 16:57:00 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Liu", "Shuo", ""], ["John", "Vijay", ""], ["Blasch", "Erik", ""], ["Liu", "Zheng", ""], ["Huang", "Ying", ""]]}, {"id": "1806.09573", "submitter": "Weifeng Chen", "authors": "Weifeng Chen, Shengyi Qian, Jia Deng", "title": "Learning Single-Image Depth from Videos using Quality Assessment\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation from a single image in the wild remains a challenging\nproblem. One main obstacle is the lack of high-quality training data for images\nin the wild. In this paper we propose a method to automatically generate such\ndata through Structure-from-Motion (SfM) on Internet videos. The core of this\nmethod is a Quality Assessment Network that identifies high-quality\nreconstructions obtained from SfM. Using this method, we collect single-view\ndepth training data from a large number of YouTube videos and construct a new\ndataset called YouTube3D. Experiments show that YouTube3D is useful in training\ndepth estimation networks and advances the state of the art of single-view\ndepth estimation in the wild.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:17:55 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 21:28:24 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 15:57:28 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Chen", "Weifeng", ""], ["Qian", "Shengyi", ""], ["Deng", "Jia", ""]]}, {"id": "1806.09594", "submitter": "Carl Vondrick", "authors": "Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama,\n  Kevin Murphy", "title": "Tracking Emerges by Colorizing Videos", "comments": "ECCV 2018. Blog post:\n  https://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use large amounts of unlabeled video to learn models for visual tracking\nwithout manual human supervision. We leverage the natural temporal coherency of\ncolor to create a model that learns to colorize gray-scale videos by copying\ncolors from a reference frame. Quantitative and qualitative experiments suggest\nthat this task causes the model to automatically learn to track visual regions.\nAlthough the model is trained without any ground-truth labels, our method\nlearns to track well enough to outperform the latest methods based on optical\nflow. Moreover, our results suggest that failures to track are correlated with\nfailures to colorize, indicating that advancing video colorization may further\nimprove self-supervised visual tracking.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:44:40 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 22:38:39 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Vondrick", "Carl", ""], ["Shrivastava", "Abhinav", ""], ["Fathi", "Alireza", ""], ["Guadarrama", "Sergio", ""], ["Murphy", "Kevin", ""]]}, {"id": "1806.09602", "submitter": "Thomas K\\\"ustner", "authors": "Thomas K\\\"ustner, Sergios Gatidis, Annika Liebgott, Martin Schwartz,\n  Lukas Mauch, Petros Martirosian, Holger Schmidt, Nina F. Schwenzer,\n  Konstantin Nikolaou, Fabian Bamberg, Bin Yang, Fritz Schick", "title": "A Machine-learning framework for automatic reference-free quality\n  assessment in MRI", "comments": null, "journal-ref": null, "doi": "10.1016/j.mri.2018.07.003", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance (MR) imaging offers a wide variety of imaging techniques.\nA large amount of data is created per examination which needs to be checked for\nsufficient quality in order to derive a meaningful diagnosis. This is a manual\nprocess and therefore time- and cost-intensive. Any imaging artifacts\noriginating from scanner hardware, signal processing or induced by the patient\nmay reduce the image quality and complicate the diagnosis or any image\npost-processing. Therefore, the assessment or the ensurance of sufficient image\nquality in an automated manner is of high interest. Usually no reference image\nis available or difficult to define. Therefore, classical reference-based\napproaches are not applicable. Model observers mimicking the human observers\n(HO) can assist in this task. Thus, we propose a new machine-learning-based\nreference-free MR image quality assessment framework which is trained on\nHO-derived labels to assess MR image quality immediately after each\nacquisition. We include the concept of active learning and present an efficient\nblinded reading platform to reduce the effort in the HO labeling procedure.\nDerived image features and the applied classifiers (support-vector-machine,\ndeep neural network) are investigated for a cohort of 250 patients. The MR\nimage quality assessment framework can achieve a high test accuracy of 93.7$\\%$\nfor estimating quality classes on a 5-point Likert-scale. The proposed MR image\nquality assessment framework is able to provide an accurate and efficient\nquality estimation which can be used as a prospective quality assurance\nincluding automatic acquisition adaptation or guided MR scanner operation,\nand/or as a retrospective quality assessment including support of diagnostic\ndecisions or quality control in cohort studies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:56:32 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 09:53:40 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["K\u00fcstner", "Thomas", ""], ["Gatidis", "Sergios", ""], ["Liebgott", "Annika", ""], ["Schwartz", "Martin", ""], ["Mauch", "Lukas", ""], ["Martirosian", "Petros", ""], ["Schmidt", "Holger", ""], ["Schwenzer", "Nina F.", ""], ["Nikolaou", "Konstantin", ""], ["Bamberg", "Fabian", ""], ["Yang", "Bin", ""], ["Schick", "Fritz", ""]]}, {"id": "1806.09607", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita and Taichi Yoshida and Sayaka Shiota and Hitoshi Kiya", "title": "Multi-Exposure Image Fusion Based on Exposure Compensation", "comments": "in Proc. IEEE International Conference on Acoustics, Speech and\n  Signal Processing, pp.1388-1392, Calgary, Alberta, Canada, 19th April, 2018.\n  arXiv admin note: substantial text overlap with arXiv:1805.11211", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel multi-exposure image fusion method based on\nexposure compensation. Multi-exposure image fusion is a method to produce\nimages without color saturation regions, by using photos with different\nexposures. However, in conventional works, it is unclear how to determine\nappropriate exposure values, and moreover, it is difficult to set appropriate\nexposure values at the time of photographing due to time constraints. In the\nproposed method, the luminance of the input multi-exposure images is adjusted\non the basis of the relationship between exposure values and pixel values,\nwhere the relationship is obtained by assuming that a digital camera has a\nlinear response function. The use of a local contrast enhancement method is\nalso considered to improve input multi-exposure images. The compensated images\nare finally combined by one of existing multi-exposure image fusion methods. In\nsome experiments, the effectiveness of the proposed method are evaluated in\nterms of the tone mapped image quality index, statistical naturalness, and\ndiscrete entropy, by comparing the proposed one with conventional ones.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 09:58:35 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Yoshida", "Taichi", ""], ["Shiota", "Sayaka", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1806.09613", "submitter": "Alireza Rahimpour", "authors": "Alireza Rahimpour and Hairong Qi", "title": "Attention-based Few-Shot Person Re-identification Using Meta Learning", "comments": "This is an ongoing project and the method has been completely revised\n  and more details will be available soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the challenging task of person\nre-identification from a new perspective and propose an end-to-end\nattention-based architecture for few-shot re-identification through\nmeta-learning. The motivation for this task lies in the fact that humans, can\nusually identify another person after just seeing that given person a few times\n(or even once) by attending to their memory. On the other hand, the unique\nnature of the person re-identification problem, i.e., only few examples exist\nper identity and new identities always appearing during testing, calls for a\nfew shot learning architecture with the capacity of handling new identities.\nHence, we frame the problem within a meta-learning setting, where a neural\nnetwork based meta-learner is trained to optimize a learner i.e., an\nattention-based matching function. Another challenge of the person\nre-identification problem is the small inter-class difference between different\nidentities and large intra-class difference of the same identity. In order to\nincrease the discriminative power of the model, we propose a new\nattention-based feature encoding scheme that takes into account the critical\nintra-view and cross-view relationship of images. We refer to the proposed\nAttention-based Re-identification Metalearning model as ARM. Extensive\nevaluations demonstrate the advantages of the ARM as compared to the\nstate-of-the-art on the challenging PRID2011, CUHK01, CUHK03 and Market1501\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 22:47:39 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 16:14:24 GMT"}, {"version": "v3", "created": "Sun, 24 Mar 2019 21:41:10 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Rahimpour", "Alireza", ""], ["Qi", "Hairong", ""]]}, {"id": "1806.09648", "submitter": "Ke Yan", "authors": "Ke Yan, Mohammadhadi Bagheri, and Ronald M. Summers", "title": "3D Context Enhanced Region-based Convolutional Neural Network for\n  End-to-End Lesion Detection", "comments": "MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting lesions from computed tomography (CT) scans is an important but\ndifficult problem because non-lesions and true lesions can appear similar. 3D\ncontext is known to be helpful in this differentiation task. However, existing\nend-to-end detection frameworks of convolutional neural networks (CNNs) are\nmostly designed for 2D images. In this paper, we propose 3D context enhanced\nregion-based CNN (3DCE) to incorporate 3D context information efficiently by\naggregating feature maps of 2D images. 3DCE is easy to train and end-to-end in\ntraining and inference. A universal lesion detector is developed to detect all\nkinds of lesions in one algorithm using the DeepLesion dataset. Experimental\nresults on this challenging task prove the effectiveness of 3DCE. We have\nreleased the code of 3DCE in\nhttps://github.com/rsummers11/CADLab/tree/master/lesion_detector_3DCE.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 18:10:16 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 17:56:13 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Yan", "Ke", ""], ["Bagheri", "Mohammadhadi", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1806.09655", "submitter": "Oleh Rybkin", "authors": "Oleh Rybkin, Karl Pertsch, Konstantinos G. Derpanis, Kostas\n  Daniilidis, and Andrew Jaegle", "title": "Learning what you can do before doing anything", "comments": "Published at ICLR 2019. 10 pages + 15 pages of references and\n  appendices", "journal-ref": "International Conference on Learning Representations, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent agents can learn to represent the action spaces of other agents\nsimply by observing them act. Such representations help agents quickly learn to\npredict the effects of their own actions on the environment and to plan complex\naction sequences. In this work, we address the problem of learning an agent's\naction space purely from visual observation. We use stochastic video prediction\nto learn a latent variable that captures the scene's dynamics while being\nminimally sensitive to the scene's static content. We introduce a loss term\nthat encourages the network to capture the composability of visual sequences\nand show that it leads to representations that disentangle the structure of\nactions. We call the full model with composable action representations\nComposable Learned Action Space Predictor (CLASP). We show the applicability of\nour method to synthetic settings and its potential to capture action spaces in\ncomplex, realistic visual settings. When used in a semi-supervised setting, our\nlearned representations perform comparably to existing fully supervised methods\non tasks such as action-conditioned video prediction and planning in the\nlearned action space, while requiring orders of magnitude fewer action labels.\nProject website: https://daniilidis-group.github.io/learned_action_spaces\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 18:33:34 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 18:53:33 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Rybkin", "Oleh", ""], ["Pertsch", "Karl", ""], ["Derpanis", "Konstantinos G.", ""], ["Daniilidis", "Kostas", ""], ["Jaegle", "Andrew", ""]]}, {"id": "1806.09695", "submitter": "Hanxiao Wang", "authors": "Hanxiao Wang, Xiatian Zhu, Shaogang Gong, Tao Xiang", "title": "Person Re-Identification in Identity Regression Space", "comments": "accepted by International Journal of Computer Vision (IJCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing person re-identification (re-id) methods are unsuitable for\nreal-world deployment due to two reasons: Unscalability to large population\nsize, and Inadaptability over time. In this work, we present a unified solution\nto address both problems. Specifically, we propose to construct an Identity\nRegression Space (IRS) based on embedding different training person identities\n(classes) and formulate re-id as a regression problem solved by identity\nregression in the IRS. The IRS approach is characterised by a closed-form\nsolution with high learning efficiency and an inherent incremental learning\ncapability with human-in-the-loop. Extensive experiments on four benchmarking\ndatasets(VIPeR, CUHK01, CUHK03 and Market-1501) show that the IRS model not\nonly outperforms state-of-the-art re-id methods, but also is more scalable to\nlarge re-id population size by rapidly updating model and actively selecting\ninformative samples with reduced human labelling effort.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 20:40:25 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Wang", "Hanxiao", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""], ["Xiang", "Tao", ""]]}, {"id": "1806.09748", "submitter": "Jong Chul Ye", "authors": "Eunhee Kang, Hyun Jung Koo, Dong Hyun Yang, Joon Bum Seo, and Jong\n  Chul Ye", "title": "Cycle Consistent Adversarial Denoising Network for Multiphase Coronary\n  CT Angiography", "comments": "This work is accepted in Medical Physics", "journal-ref": null, "doi": "10.1002/mp.13284", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In coronary CT angiography, a series of CT images are taken at different\nlevels of radiation dose during the examination. Although this reduces the\ntotal radiation dose, the image quality during the low-dose phases is\nsignificantly degraded. To address this problem, here we propose a novel\nsemi-supervised learning technique that can remove the noises of the CT images\nobtained in the low-dose phases by learning from the CT images in the routine\ndose phases. Although a supervised learning approach is not possible due to the\ndifferences in the underlying heart structure in two phases, the images in the\ntwo phases are closely related so that we propose a cycle-consistent\nadversarial denoising network to learn the non-degenerate mapping between the\nlow and high dose cardiac phases. Experimental results showed that the proposed\nmethod effectively reduces the noise in the low-dose CT image while the\npreserving detailed texture and edge information. Moreover, thanks to the\ncyclic consistency and identity loss, the proposed network does not create any\nartificial features that are not present in the input images. Visual grading\nand quality evaluation also confirm that the proposed method provides\nsignificant improvement in diagnostic quality.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 01:17:51 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 12:14:42 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 14:21:14 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Kang", "Eunhee", ""], ["Koo", "Hyun Jung", ""], ["Yang", "Dong Hyun", ""], ["Seo", "Joon Bum", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1806.09755", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Ben Usman, Kuniaki Saito, Neela Kaushik, Judy Hoffman,\n  Kate Saenko", "title": "Syn2Real: A New Benchmark forSynthetic-to-Real Visual Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised transfer of object recognition models from synthetic to real\ndata is an important problem with many potential applications. The challenge is\nhow to \"adapt\" a model trained on simulated images so that it performs well on\nreal-world data without any additional supervision. Unfortunately, current\nbenchmarks for this problem are limited in size and task diversity. In this\npaper, we present a new large-scale benchmark called Syn2Real, which consists\nof a synthetic domain rendered from 3D object models and two real-image domains\ncontaining the same object categories. We define three related tasks on this\nbenchmark: closed-set object classification, open-set object classification,\nand object detection. Our evaluation of multiple state-of-the-art methods\nreveals a large gap in adaptation performance between the easier closed-set\nclassification task and the more difficult open-set and detection tasks. We\nconclude that developing adaptation methods that work well across all three\ntasks presents a significant future challenge for syn2real domain transfer.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 01:53:13 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Peng", "Xingchao", ""], ["Usman", "Ben", ""], ["Saito", "Kuniaki", ""], ["Kaushik", "Neela", ""], ["Hoffman", "Judy", ""], ["Saenko", "Kate", ""]]}, {"id": "1806.09764", "submitter": "Zhiting Hu", "authors": "Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Xiaodan Liang, Lianhui\n  Qin, Haoye Dong, Eric Xing", "title": "Deep Generative Models with Learnable Knowledge Constraints", "comments": "Neural Information Processing Systems (NeurIPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The broad set of deep generative models (DGMs) has achieved remarkable\nadvances. However, it is often difficult to incorporate rich structured domain\nknowledge with the end-to-end DGMs. Posterior regularization (PR) offers a\nprincipled framework to impose structured constraints on probabilistic models,\nbut has limited applicability to the diverse DGMs that can lack a Bayesian\nformulation or even explicit density evaluation. PR also requires constraints\nto be fully specified a priori, which is impractical or suboptimal for complex\nknowledge with learnable uncertain parts. In this paper, we establish\nmathematical correspondence between PR and reinforcement learning (RL), and,\nbased on the connection, expand PR to learn constraints as the extrinsic reward\nin RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is\nflexible to adapt arbitrary constraints with the model jointly. Experiments on\nhuman image generation and templated sentence generation show models with\nlearned knowledge constraints by our algorithm greatly improve over base\ngenerative models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 02:31:35 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 02:10:48 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Hu", "Zhiting", ""], ["Yang", "Zichao", ""], ["Salakhutdinov", "Ruslan", ""], ["Liang", "Xiaodan", ""], ["Qin", "Lianhui", ""], ["Dong", "Haoye", ""], ["Xing", "Eric", ""]]}, {"id": "1806.09766", "submitter": "Puyang Wang", "authors": "Puyang Wang, Vishal M. Patel and Ilker Hacihaliloglu", "title": "Simultaneous Segmentation and Classification of Bone Surfaces from\n  Ultrasound Using a Multi-feature Guided CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various imaging artifacts, low signal-to-noise ratio, and bone surfaces\nappearing several millimeters in thickness have hindered the success of\nultrasound (US) guided computer assisted orthopedic surgery procedures. In this\nwork, a multi-feature guided convolutional neural network (CNN) architecture is\nproposed for simultaneous enhancement, segmentation, and classification of bone\nsurfaces from US data. The proposed CNN consists of two main parts: a\npre-enhancing net, that takes the concatenation of B-mode US scan and three\nfiltered image features for the enhancement of bone surfaces, and a modified\nU-net with a classification layer. The proposed method was validated on 650 in\nvivo US scans collected using two US machines, by scanning knee, femur, distal\nradius and tibia bones. Validation, against expert annotation, achieved\nstatistically significant improvements in segmentation of bone surfaces\ncompared to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 02:35:50 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Wang", "Puyang", ""], ["Patel", "Vishal M.", ""], ["Hacihaliloglu", "Ilker", ""]]}, {"id": "1806.09776", "submitter": "Jindong Wang", "authors": "Yiqiang Chen, Jindong Wang, Meiyu Huang, Han Yu", "title": "Cross-position Activity Recognition with Stratified Transfer Learning", "comments": "Submit to Pervasive and Mobile Computing as an extension to PerCom 18\n  paper; First revision. arXiv admin note: substantial text overlap with\n  arXiv:1801.00820", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition aims to recognize the activities of daily living\nby utilizing the sensors on different body parts. However, when the labeled\ndata from a certain body position (i.e. target domain) is missing, how to\nleverage the data from other positions (i.e. source domain) to help learn the\nactivity labels of this position? When there are several source domains\navailable, it is often difficult to select the most similar source domain to\nthe target domain. With the selected source domain, we need to perform accurate\nknowledge transfer between domains. Existing methods only learn the global\ndistance between domains while ignoring the local property. In this paper, we\npropose a \\textit{Stratified Transfer Learning} (STL) framework to perform both\nsource domain selection and knowledge transfer. STL is based on our proposed\n\\textit{Stratified} distance to capture the local property of domains. STL\nconsists of two components: Stratified Domain Selection (STL-SDS) can select\nthe most similar source domain to the target domain; Stratified Activity\nTransfer (STL-SAT) is able to perform accurate knowledge transfer. Extensive\nexperiments on three public activity recognition datasets demonstrate the\nsuperiority of STL. Furthermore, we extensively investigate the performance of\ntransfer learning across different degrees of similarities and activity levels\nbetween domains. We also discuss the potential applications of STL in other\nfields of pervasive computing for future research.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 03:01:59 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 02:34:07 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Chen", "Yiqiang", ""], ["Wang", "Jindong", ""], ["Huang", "Meiyu", ""], ["Yu", "Han", ""]]}, {"id": "1806.09790", "submitter": "Qijie Zhao", "authors": "Qijie Zhao, Tao Sheng, Yongtao Wang, Feng Ni, Ling Cai", "title": "CFENet: An Accurate and Efficient Single-Shot Object Detector for\n  Autonomous Driving", "comments": "5 pages, 4 figures, CVPR2018, Workshop of Autonomous Driving (WAD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to detect small objects and the speed of the object detector are\nvery important for the application of autonomous driving, and in this paper, we\npropose an effective yet efficient one-stage detector, which gained the second\nplace in the Road Object Detection competition of CVPR2018 workshop - Workshop\nof Autonomous Driving(WAD). The proposed detector inherits the architecture of\nSSD and introduces a novel Comprehensive Feature Enhancement(CFE) module into\nit. Experimental results on this competition dataset as well as the MSCOCO\ndataset demonstrate that the proposed detector (named CFENet) performs much\nbetter than the original SSD and the state-of-the-art method RefineDet\nespecially for small objects, while keeping high efficiency close to the\noriginal SSD. Specifically, the single scale version of the proposed detector\ncan run at the speed of 21 fps, while the multi-scale version with larger input\nsize achieves the mAP 29.69, ranking second on the leaderboard\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 04:48:01 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 07:28:52 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Zhao", "Qijie", ""], ["Sheng", "Tao", ""], ["Wang", "Yongtao", ""], ["Ni", "Feng", ""], ["Cai", "Ling", ""]]}, {"id": "1806.09807", "submitter": "Junjun Jiang", "authors": "Junjun Jiang, Jiayi Ma, Chen Chen, Zhongyuan Wang, Zhihua Cai, and\n  Lizhe Wang", "title": "SuperPCA: A Superpixelwise PCA Approach for Unsupervised Feature\n  Extraction of Hyperspectral Imagery", "comments": "13 pages, 10 figures, Accepted by IEEE TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2018.2828029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an unsupervised dimensionality reduction method, principal component\nanalysis (PCA) has been widely considered as an efficient and effective\npreprocessing step for hyperspectral image (HSI) processing and analysis tasks.\nIt takes each band as a whole and globally extracts the most representative\nbands. However, different homogeneous regions correspond to different objects,\nwhose spectral features are diverse. It is obviously inappropriate to carry out\ndimensionality reduction through a unified projection for an entire HSI. In\nthis paper, a simple but very effective superpixelwise PCA approach, called\nSuperPCA, is proposed to learn the intrinsic low-dimensional features of HSIs.\nIn contrast to classical PCA models, SuperPCA has four main properties. (1)\nUnlike the traditional PCA method based on a whole image, SuperPCA takes into\naccount the diversity in different homogeneous regions, that is, different\nregions should have different projections. (2) Most of the conventional feature\nextraction models cannot directly use the spatial information of HSIs, while\nSuperPCA is able to incorporate the spatial context information into the\nunsupervised dimensionality reduction by superpixel segmentation. (3) Since the\nregions obtained by superpixel segmentation have homogeneity, SuperPCA can\nextract potential low-dimensional features even under noise. (4) Although\nSuperPCA is an unsupervised method, it can achieve competitive performance when\ncompared with supervised approaches. The resulting features are discriminative,\ncompact, and noise resistant, leading to improved HSI classification\nperformance. Experiments on three public datasets demonstrate that the SuperPCA\nmodel significantly outperforms the conventional PCA based dimensionality\nreduction baselines for HSI classification. The Matlab source code is available\nat https://github.com/junjun-jiang/SuperPCA\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 06:38:07 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 03:24:17 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Jiang", "Junjun", ""], ["Ma", "Jiayi", ""], ["Chen", "Chen", ""], ["Wang", "Zhongyuan", ""], ["Cai", "Zhihua", ""], ["Wang", "Lizhe", ""]]}, {"id": "1806.09809", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, Zeynep Akata", "title": "Generating Counterfactual Explanations with Natural Language", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language explanations of deep neural network decisions provide an\nintuitive way for a AI agent to articulate a reasoning process. Current textual\nexplanations learn to discuss class discriminative features in an image.\nHowever, it is also helpful to understand which attributes might change a\nclassification decision if present in an image (e.g., \"This is not a Scarlet\nTanager because it does not have black wings.\") We call such textual\nexplanations counterfactual explanations, and propose an intuitive method to\ngenerate counterfactual explanations by inspecting which evidence in an input\nis missing, but might contribute to a different classification decision if\npresent in the image. To demonstrate our method we consider a fine-grained\nimage classification task in which we take as input an image and a\ncounterfactual class and output text which explains why the image does not\nbelong to a counterfactual class. We then analyze our generated counterfactual\nexplanations both qualitatively and quantitatively using proposed automatic\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 06:43:36 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Hu", "Ronghang", ""], ["Darrell", "Trevor", ""], ["Akata", "Zeynep", ""]]}, {"id": "1806.09820", "submitter": "Arnau Ramisa", "authors": "Charles Packer, Julian McAuley and Arnau Ramisa", "title": "Visually-Aware Personalized Recommendation using Interpretable Image\n  Representations", "comments": "AI for Fashion workshop, held in conjunction with KDD 2018, London. 4\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually-aware recommender systems use visual signals present in the\nunderlying data to model the visual characteristics of items and users'\npreferences towards them. In the domain of clothing recommendation,\nincorporating items' visual information (e.g., product images) is particularly\nimportant since clothing item appearance is often a critical factor in\ninfluencing the user's purchasing decisions. Current state-of-the-art\nvisually-aware recommender systems utilize image features extracted from\npre-trained deep convolutional neural networks, however these extremely\nhigh-dimensional representations are difficult to interpret, especially in\nrelation to the relatively low number of visual properties that may guide\nusers' decisions.\n  In this paper we propose a novel approach to personalized clothing\nrecommendation that models the dynamics of individual users' visual\npreferences. By using interpretable image representations generated with a\nunique feature learning process, our model learns to explain users' prior\nfeedback in terms of their affinity towards specific visual attributes and\nstyles. Our approach achieves state-of-the-art performance on personalized\nranking tasks, and the incorporation of interpretable visual features allows\nfor powerful model introspection, which we demonstrate by using an interactive\nrecommendation algorithm and visualizing the rise and fall of fashion trends\nover time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 07:24:06 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 21:38:37 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Packer", "Charles", ""], ["McAuley", "Julian", ""], ["Ramisa", "Arnau", ""]]}, {"id": "1806.09860", "submitter": "Jianning Li", "authors": "Jianning Li, Long Cao, Yangyang Ge, W. Cheng, M. Bowen, G. Wei", "title": "Multi-Task Deep Convolutional Neural Network for the Segmentation of\n  Type B Aortic Dissection", "comments": "This article has been removed by arXiv administrators because the\n  submitter did not have the rights to agree to the license at the time of\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the entire aorta and true-false lumen is crucial to inform\nplan and follow-up for endovascular repair of the rare yet life threatening\ntype B aortic dissection. Manual segmentation by slice is time-consuming and\nrequires expertise, while current computer-aided methods focus on the\nsegmentation of the entire aorta, are unable to concurrently segment true-false\nlumen, and some require human interaction. We here report a fully automated\napproach based on a 3-D multi-task deep convolutional neural network that\nsegments the entire aorta and true-false lumen from CTA images in a unified\nframework. For training, we built a database containing 254 CTA images (210\npreoperative and 44 postoperative) obtained using various systems from 254\nunique patients with type B aortic dissection. Slice-wise manual segmentation\nof the entire aorta and the true-false lumen for each 3-D CTA image was\nprovided. Upon evaluation of another 16 CTA images (11 preoperative and 5\npostoperative) with ground truth segmentation provided by experienced vascular\nsurgeons, our method achieves a mean dice similarity score(DSC) of 0.910,0.849\nand 0.821 for the entire aorta,true lumen and false lumen respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 09:11:25 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 02:52:43 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 16:52:01 GMT"}, {"version": "v4", "created": "Fri, 9 Nov 2018 10:03:08 GMT"}, {"version": "v5", "created": "Wed, 21 Nov 2018 15:56:31 GMT"}, {"version": "v6", "created": "Thu, 22 Nov 2018 08:46:59 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Li", "Jianning", ""], ["Cao", "Long", ""], ["Ge", "Yangyang", ""], ["Cheng", "W.", ""], ["Bowen", "M.", ""], ["Wei", "G.", ""]]}, {"id": "1806.09882", "submitter": "Pingfan Song", "authors": "Pingfan Song, Miguel R.D. Rodrigues", "title": "Multi-modal Image Processing based on Coupled Dictionary Learning", "comments": "SPAWC 2018, 19th IEEE International Workshop On Signal Processing\n  Advances In Wireless Communications", "journal-ref": null, "doi": "10.1109/SPAWC.2018.8446001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world scenarios, many data processing problems often involve\nheterogeneous images associated with different imaging modalities. Since these\nmultimodal images originate from the same phenomenon, it is realistic to assume\nthat they share common attributes or characteristics. In this paper, we propose\na multi-modal image processing framework based on coupled dictionary learning\nto capture similarities and disparities between different image modalities. In\nparticular, our framework can capture favorable structure similarities across\ndifferent image modalities such as edges, corners, and other elementary\nprimitives in a learned sparse transform domain, instead of the original pixel\ndomain, that can be used to improve a number of image processing tasks such as\ndenoising, inpainting, or super-resolution. Practical experiments demonstrate\nthat incorporating multimodal information using our framework brings notable\nbenefits.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 10:01:10 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Song", "Pingfan", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1806.09907", "submitter": "Christoph Jud", "authors": "Robin Sandk\\\"uhler, Christoph Jud, Simon Andermatt, Philippe C. Cattin", "title": "AirLab: Autograd Image Registration Laboratory", "comments": "Corresponding author: Christoph Jud, e-mail: christoph.jud@unibas.ch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image registration is an active research topic and forms a basis for\nmany medical image analysis tasks. Although image registration is a rather\ngeneral concept specialized methods are usually required to target a specific\nregistration problem. The development and implementation of such methods has\nbeen tough so far as the gradient of the objective has to be computed. Also,\nits evaluation has to be performed preferably on a GPU for larger images and\nfor more complex transformation models and regularization terms. This hinders\nresearchers from rapid prototyping and poses hurdles to reproduce research\nresults. There is a clear need for an environment which hides this complexity\nto put the modeling and the experimental exploration of registration methods\ninto the foreground. With the \"Autograd Image Registration Laboratory\"\n(AIRLab), we introduce an open laboratory for image registration tasks, where\nthe analytic gradients of the objective function are computed automatically and\nthe device where the computations are performed, on a CPU or a GPU, is\ntransparent. It is meant as a laboratory for researchers and developers\nenabling them to rapidly try out new ideas for registering images and to\nreproduce registration results which have already been published. AIRLab is\nimplemented in Python using PyTorch as tensor and optimization library and\nSimpleITK for basic image IO. Therefore, it profits from recent advances made\nby the machine learning community concerning optimization and deep neural\nnetwork models. The presented draft of this paper outlines AIRLab with first\ncode snippets and performance analyses. A more exhaustive introduction will\nfollow as a final version soon.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 11:12:43 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 16:03:27 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Sandk\u00fchler", "Robin", ""], ["Jud", "Christoph", ""], ["Andermatt", "Simon", ""], ["Cattin", "Philippe C.", ""]]}, {"id": "1806.09930", "submitter": "Pingfan Song", "authors": "Pingfan Song, Lior Weizman, Joao F.C. Mota, Yonina C. Eldar, Miguel\n  R.D. Rodrigues", "title": "Coupled Dictionary Learning for Multi-contrast MRI Reconstruction", "comments": "2018 IEEE International Conference on Image Processing (ICIP)", "journal-ref": null, "doi": "10.1109/TMI.2019.2932961", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging tasks often involve multiple contrasts, such as T1- and\nT2-weighted magnetic resonance imaging (MRI) data. These contrasts capture\ninformation associated with the same underlying anatomy and thus exhibit\nsimilarities. In this paper, we propose a Coupled Dictionary Learning based\nmulti-contrast MRI reconstruction (CDLMRI) approach to leverage an available\nguidance contrast to restore the target contrast. Our approach consists of\nthree stages: coupled dictionary learning, coupled sparse denoising, and\n$k$-space consistency enforcing. The first stage learns a group of dictionaries\nthat capture correlations among multiple contrasts. By capitalizing on the\nlearned adaptive dictionaries, the second stage performs joint sparse coding to\ndenoise the corrupted target image with the aid of a guidance contrast. The\nthird stage enforces consistency between the denoised image and the\nmeasurements in the $k$-space domain. Numerical experiments on the\nretrospective under-sampling of clinical MR images demonstrate that\nincorporating additional guidance contrast via our design improves MRI\nreconstruction, compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 11:52:57 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Song", "Pingfan", ""], ["Weizman", "Lior", ""], ["Mota", "Joao F. C.", ""], ["Eldar", "Yonina C.", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1806.09980", "submitter": "Muzammil Behzad", "authors": "Muzammil Behzad, Manal Abdullah, Muhammad Talal Hassan, Yao Ge,\n  Mahmood Ashraf Khan", "title": "Toward Performance Optimization in IoT-based Next-Gen Wireless Sensor\n  Networks", "comments": "45 pages, 22 figures, pending article. arXiv admin note: substantial\n  text overlap with arXiv:1712.04259", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.NI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework for performance optimization in\nInternet of Things (IoT)-based next-generation wireless sensor networks. In\nparticular, a computationally-convenient system is presented to combat two\nmajor research problems in sensor networks. First is the conventionally-tackled\nresource optimization problem which triggers the drainage of battery at a\nfaster rate within a network. Such drainage promotes inefficient resource usage\nthereby causing sudden death of the network. The second main bottleneck for\nsuch networks is that of data degradation. This is because the nodes in such\nnetworks communicate via a wireless channel, where the inevitable presence of\nnoise corrupts the data making it unsuitable for practical applications.\nTherefore, we present a layer-adaptive method via 3-tier communication\nmechanism to ensure the efficient use of resources. This is supported with a\nmathematical coverage model that deals with the formation of coverage holes. We\nalso present a transform-domain based robust algorithm to effectively remove\nthe unwanted components from the data. Our proposed framework offers a handy\nalgorithm that enjoys desirable complexity for real-time applications as shown\nby the extensive simulation results.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 08:42:01 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Behzad", "Muzammil", ""], ["Abdullah", "Manal", ""], ["Hassan", "Muhammad Talal", ""], ["Ge", "Yao", ""], ["Khan", "Mahmood Ashraf", ""]]}, {"id": "1806.09986", "submitter": "Mohammad Hajizadeh Saffar", "authors": "Mohammad Hajizadeh Saffar, Mohsen Fayyaz, Mohammad Sabokrou, Mahmood\n  Fathy", "title": "Online Signature Verification using Deep Representation: A new\n  Descriptor", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.08153", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an accurate method for verifying online signatures. The\nmain difficulty of signature verification come from: (1) Lacking enough\ntraining samples (2) The methods must be spatial change invariant. To deal with\nthese difficulties and modeling the signatures efficiently, we propose a method\nthat a one-class classifier per each user is built on discriminative features.\nFirst, we pre-train a sparse auto-encoder using a large number of unlabeled\nsignatures, then we applied the discriminative features, which are learned by\nauto-encoder to represent the training and testing signatures as a self-thought\nlearning method (i.e. we have introduced a signature descriptor). Finally,\nuser's signatures are modeled and classified using a one-class classifier. The\nproposed method is independent on signature datasets thanks to self-taught\nlearning. The experimental results indicate significant error reduction and\naccuracy enhancement in comparison with state-of-the-art methods on SVC2004 and\nSUSIG datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 03:15:39 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Saffar", "Mohammad Hajizadeh", ""], ["Fayyaz", "Mohsen", ""], ["Sabokrou", "Mohammad", ""], ["Fathy", "Mahmood", ""]]}, {"id": "1806.10040", "submitter": "Yingbin Zheng", "authors": "Li Wang, Weiyuan Shao, Yao Lu, Hao Ye, Jian Pu, Yingbin Zheng", "title": "Crowd Counting with Density Adaption Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is one of the core tasks in various surveillance applications.\nA practical system involves estimating accurate head counts in dynamic\nscenarios under different lightning, camera perspective and occlusion states.\nPrevious approaches estimate head counts despite that they can vary\ndramatically in different density settings; the crowd is often unevenly\ndistributed and the results are therefore unsatisfactory. In this paper, we\npropose a lightweight deep learning framework that can automatically estimate\nthe crowd density level and adaptively choose between different counter\nnetworks that are explicitly trained for different density domains. Experiments\non two recent crowd counting datasets, UCF_CC_50 and ShanghaiTech, show that\nthe proposed mechanism achieves promising improvements over state-of-the-art\nmethods. Moreover, runtime speed is 20 FPS on a single GPU.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 14:57:16 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Wang", "Li", ""], ["Shao", "Weiyuan", ""], ["Lu", "Yao", ""], ["Ye", "Hao", ""], ["Pu", "Jian", ""], ["Zheng", "Yingbin", ""]]}, {"id": "1806.10050", "submitter": "Xiaoming Yu", "authors": "Xiaoming Yu, Zhenqiang Ying, Thomas Li, Shan Liu, Ge Li", "title": "Multi-Mapping Image-to-Image Translation with Central Biasing\n  Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in image-to-image translation have seen a rise in approaches\ngenerating diverse images through a single network. To indicate the target\ndomain for a one-to-many mapping, the latent code is injected into the\ngenerator network. However, we found that the injection method leads to mode\ncollapse because of normalization strategies. Existing normalization strategies\nmight either cause the inconsistency of feature distribution or eliminate the\neffect of the latent code. To solve these problems, we propose the consistency\nwithin diversity criteria for designing the multi-mapping model. Based on the\ncriteria, we propose central biasing normalization to inject the latent code\ninformation. Experiments show that our method can improve the quality and\ndiversity of existing image-to-image translation models, such as StarGAN,\nBicycleGAN, and pix2pix.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:07:42 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 15:29:38 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 16:09:35 GMT"}, {"version": "v4", "created": "Thu, 11 Oct 2018 15:02:03 GMT"}, {"version": "v5", "created": "Fri, 17 Apr 2020 09:17:08 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Yu", "Xiaoming", ""], ["Ying", "Zhenqiang", ""], ["Li", "Thomas", ""], ["Liu", "Shan", ""], ["Li", "Ge", ""]]}, {"id": "1806.10128", "submitter": "Qicheng Lao", "authors": "Qicheng Lao, Thomas Fevens and Boyu Wang", "title": "Leveraging Disease Progression Learning for Medical Image Recognition", "comments": null, "journal-ref": "IEEE International Conference on Bioinformatics and Biomedicine\n  (BIBM) 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike natural images, medical images often have intrinsic characteristics\nthat can be leveraged for neural network learning. For example, images that\nbelong to different stages of a disease may continuously follow a certain\nprogression pattern. In this paper, we propose a novel method that leverages\ndisease progression learning for medical image recognition. In our method,\nsequences of images ordered by disease stages are learned by a neural network\nthat consists of a shared vision model for feature extraction and a long\nshort-term memory network for the learning of stage sequences. Auxiliary vision\noutputs are also included to capture stage features that tend to be discrete\nalong the disease progression. Our proposed method is evaluated on a public\ndiabetic retinopathy dataset, and achieves about 3.3% improvement in disease\nstaging accuracy, compared to the baseline method that does not use disease\nprogression learning.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 17:58:56 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 20:48:18 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Lao", "Qicheng", ""], ["Fevens", "Thomas", ""], ["Wang", "Boyu", ""]]}, {"id": "1806.10170", "submitter": "Gururaj Awate", "authors": "Gururaj Awate, Sunil Bangare, G Pradeepini, S Patil", "title": "Detection of Alzheimers Disease from MRI using Convolutional Neural\n  Network with Tensorflow", "comments": "New version with more accurate information is available at:\n  arXiv:1901.10231", "journal-ref": "IEEE Xplore 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, due to tremendous improvements in high performance computing, it\nhas become easier to train Neural Networks. We intend to take advantage of this\nsituation and apply this technology in solving real world problems. There was a\nneed for automatic diagnosis certain diseases from medical images that could\nhelp a doctor and radiologist for further action towards treating the illness.\nWe chose Alzheimer disease for this purpose. Alzheimer disease is the leading\ncause of dementia and memory loss. Alzheimer disease, it is caused by atrophy\nof the certain brain regions and by brain cell death. MRI scans reveal this\ninformation but atrophy regions are different for different people which makes\nthe diagnosis a little trickier and often gets miss-diagnosed by doctors and\nradiologists. The Dataset used for this project is provided by OASIS, which\ncontains over 400 subjects 100 of which having mild to severe dementia and is\nsupplemented by MMSE and CDR standards of diagnosis in the same context. Enter\nCNN, Convolutional Neural Networks are a hybrid of Kernel Convolutions and\nNeural Networks. Kernel Convolutions is a technique that uses filters to\nrecognize and segment images based on features. Neural Networks consist of\nneurons which are loosely based on human brains neuron which represents a\nsingle classifier and interconnected by weights, have different biases and are\nactivated by some activation functions. By using Convolutional Neural Networks,\nthe problem can be solved with minimal error rate. The technologies we intend\nto use are libraries like CUDA CuDNN for making use of GPU and its multiple\ncores-parallel computing to train models while giving us high performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 18:59:28 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 07:47:22 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Awate", "Gururaj", ""], ["Bangare", "Sunil", ""], ["Pradeepini", "G", ""], ["Patil", "S", ""]]}, {"id": "1806.10171", "submitter": "Dror Simon", "authors": "Dror Simon, Jeremias Sulam, Yaniv Romano, Yue M. Lu and Michael Elad", "title": "MMSE Approximation For Sparse Coding Algorithms Using Stochastic\n  Resonance", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2929464", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding refers to the pursuit of the sparsest representation of a\nsignal in a typically overcomplete dictionary. From a Bayesian perspective,\nsparse coding provides a Maximum a Posteriori (MAP) estimate of the unknown\nvector under a sparse prior. In this work, we suggest enhancing the performance\nof sparse coding algorithms by a deliberate and controlled contamination of the\ninput with random noise, a phenomenon known as stochastic resonance. The\nproposed method adds controlled noise to the input and estimates a sparse\nrepresentation from the perturbed signal. A set of such solutions is then\nobtained by projecting the original input signal onto the recovered set of\nsupports. We present two variants of the described method, which differ in\ntheir final step. The first is a provably convergent approximation to the\nMinimum Mean Square Error (MMSE) estimator, relying on the generative model and\napplying a weighted average over the recovered solutions. The second is a\nrelaxed variant of the former that simply applies an empirical mean. We show\nthat both methods provide a computationally efficient approximation to the MMSE\nestimator, which is typically intractable to compute. We demonstrate our\nfindings empirically and provide a theoretical analysis of our method under\nseveral different cases.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:03:39 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 19:42:54 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 13:03:44 GMT"}, {"version": "v4", "created": "Mon, 22 Oct 2018 13:17:17 GMT"}, {"version": "v5", "created": "Thu, 11 Apr 2019 18:57:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Simon", "Dror", ""], ["Sulam", "Jeremias", ""], ["Romano", "Yaniv", ""], ["Lu", "Yue M.", ""], ["Elad", "Michael", ""]]}, {"id": "1806.10181", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John Hopfield", "title": "Unsupervised Learning by Competing Hidden Units", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences of the USA, 116\n  (16) 7723-7731 (2019)", "doi": "10.1073/pnas.1820458116", "report-no": null, "categories": "cs.LG cs.CV cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely believed that the backpropagation algorithm is essential for\nlearning good feature detectors in early layers of artificial neural networks,\nso that these detectors are useful for the task performed by the higher layers\nof that neural network. At the same time, the traditional form of\nbackpropagation is biologically implausible. In the present paper we propose an\nunusual learning rule, which has a degree of biological plausibility, and which\nis motivated by Hebb's idea that change of the synapse strength should be local\n- i.e. should depend only on the activities of the pre and post synaptic\nneurons. We design a learning algorithm that utilizes global inhibition in the\nhidden layer, and is capable of learning early feature detectors in a\ncompletely unsupervised way. These learned lower layer feature detectors can be\nused to train higher layer weights in a usual supervised way so that the\nperformance of the full network is comparable to the performance of standard\nfeedforward networks trained end-to-end with a backpropagation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:32:58 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 02:36:17 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John", ""]]}, {"id": "1806.10206", "submitter": "Edo Collins", "authors": "Edo Collins, Radhakrishna Achanta, Sabine S\\\"usstrunk", "title": "Deep Feature Factorization For Concept Discovery", "comments": "The European Conference on Computer Vision (ECCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Deep Feature Factorization (DFF), a method capable of localizing\nsimilar semantic concepts within an image or a set of images. We use DFF to\ngain insight into a deep convolutional neural network's learned features, where\nwe detect hierarchical cluster structures in feature space. This is visualized\nas heat maps, which highlight semantically matching regions across a set of\nimages, revealing what the network `perceives' as similar. DFF can also be used\nto perform co-segmentation and co-localization, and we report state-of-the-art\nresults on these tasks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 20:39:13 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 13:05:06 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 11:40:15 GMT"}, {"version": "v4", "created": "Wed, 19 Sep 2018 09:21:36 GMT"}, {"version": "v5", "created": "Mon, 8 Oct 2018 10:49:15 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Collins", "Edo", ""], ["Achanta", "Radhakrishna", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1806.10257", "submitter": "Jia Li", "authors": "Changqun Xia, Jia Li, Jinming Su and Ali Borji", "title": "Learning a Saliency Evaluation Metric Using Crowdsourced Perceptual\n  Judgments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of human fixation prediction, dozens of computational saliency\nmodels are proposed to reveal certain saliency characteristics under different\nassumptions and definitions. As a result, saliency model benchmarking often\nrequires several evaluation metrics to simultaneously assess saliency models\nfrom multiple perspectives. However, most computational metrics are not\ndesigned to directly measure the perceptual similarity of saliency maps so that\nthe evaluation results may be sometimes inconsistent with the subjective\nimpression. To address this problem, this paper first conducts extensive\nsubjective tests to find out how the visual similarities between saliency maps\nare perceived by humans. Based on the crowdsourced data collected in these\ntests, we conclude several key factors in assessing saliency maps and quantize\nthe performance of existing metrics. Inspired by these factors, we propose to\nlearn a saliency evaluation metric based on a two-stream convolutional neural\nnetwork using crowdsourced perceptual judgements. Specifically, the relative\nsaliency score of each pair from the crowdsourced data is utilized to\nregularize the network during the training process. By capturing the key\nfactors shared by various subjects in comparing saliency maps, the learned\nmetric better aligns with human perception of saliency maps, making it a good\ncomplement to the existing metrics. Experimental results validate that the\nlearned metric can be generalized to the comparisons of saliency maps from new\nimages, new datasets, new models and synthetic data. Due to the effectiveness\nof the learned metric, it also can be used to facilitate the development of new\nmodels for fixation prediction.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 00:45:33 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Xia", "Changqun", ""], ["Li", "Jia", ""], ["Su", "Jinming", ""], ["Borji", "Ali", ""]]}, {"id": "1806.10269", "submitter": "Jia Li", "authors": "Lishi Zhang, Chenghan Fu and Jia Li", "title": "Collaborative Annotation of Semantic Objects in Images with\n  Multi-granularity Supervisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Per-pixel masks of semantic objects are very useful in many applications,\nwhich, however, are tedious to be annotated. In this paper, we propose a\nhuman-agent collaborative annotation approach that can efficiently generate\nper-pixel masks of semantic objects in tagged images with multi-granularity\nsupervisions. Given a set of tagged image, a computer agent is first\ndynamically generated to roughly localize the semantic objects described by the\ntag. The agent first extracts massive object proposals from an image and then\ninfer the tag-related ones under the weak and strong supervisions from\nlinguistically and visually similar images and previously annotated object\nmasks. By representing such supervisions by over-complete dictionaries, the\ntag-related object proposals can pop-out according to their sparse coding\nlength, which are then converted to superpixels with binary labels. After that,\nhuman annotators participate in the annotation process by flipping labels and\ndividing superpixels with mouse clicks, which are used as click supervisions\nthat teach the agent to recover false positives/negatives in processing images\nwith the same tags. Experimental results show that our approach can facilitate\nthe annotation process and generate object masks that are highly consistent\nwith those generated by the LabelMe toolbox.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 02:05:25 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Zhang", "Lishi", ""], ["Fu", "Chenghan", ""], ["Li", "Jia", ""]]}, {"id": "1806.10274", "submitter": "Jia Li", "authors": "Jia Li, Pengcheng Yuan, Daxin Gu, Yonghong Tian", "title": "Hierarchical Deep Co-segmentation of Primary Objects in Aerial Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primary object segmentation plays an important role in understanding videos\ngenerated by unmanned aerial vehicles. In this paper, we propose a large-scale\ndataset with 500 aerial videos and manually annotated primary objects. To the\nbest of our knowledge, it is the largest dataset to date for primary object\nsegmentation in aerial videos. From this dataset, we find most aerial videos\ncontain large-scale scenes, small primary objects as well as consistently\nvarying scales and viewpoints. Inspired by that, we propose a hierarchical deep\nco-segmentation approach that repeatedly divides a video into two sub-videos\nformed by the odd and even frames, respectively. In this manner, the primary\nobjects shared by sub-videos can be co-segmented by training two-stream CNNs\nand finally refined within the neighborhood reversible flows. Experimental\nresults show that our approach remarkably outperforms 17 state-of-the-art\nmethods in segmenting primary objects in various types of aerial videos.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 02:34:51 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 07:15:01 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Li", "Jia", ""], ["Yuan", "Pengcheng", ""], ["Gu", "Daxin", ""], ["Tian", "Yonghong", ""]]}, {"id": "1806.10278", "submitter": "Ramanpreet Pahwa Singh", "authors": "Ramanpreet Singh Pahwa, Wei Kiat Leong, Shaohui Foong, Karianto Leman,\n  Minh N. Do", "title": "Feature-less Stitching of Cylindrical Tunnel", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image stitching algorithms use transforms such as homography to\ncombine different views of a scene. They usually work well when the scene is\nplanar or when the camera is only rotated, keeping its position static. This\nseverely limits their use in real world scenarios where an unmanned aerial\nvehicle (UAV) potentially hovers around and flies in an enclosed area while\nrotating to capture a video sequence. We utilize known scene geometry along\nwith recorded camera trajectory to create cylindrical images captured in a\ngiven environment such as a tunnel where the camera rotates around its center.\nThe captured images of the inner surface of the given scene are combined to\ncreate a composite panoramic image that is textured onto a 3D geometrical\nobject in Unity graphical engine to create an immersive environment for end\nusers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 02:56:19 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Pahwa", "Ramanpreet Singh", ""], ["Leong", "Wei Kiat", ""], ["Foong", "Shaohui", ""], ["Leman", "Karianto", ""], ["Do", "Minh N.", ""]]}, {"id": "1806.10287", "submitter": "Youmei Zhang", "authors": "Youmei Zhang, Chunluan Zhou, Faliang Chang, Alex C. Kot", "title": "Attention to Head Locations for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusions, complex backgrounds, scale variations and non-uniform\ndistributions present great challenges for crowd counting in practical\napplications. In this paper, we propose a novel method using an attention model\nto exploit head locations which are the most important cue for crowd counting.\nThe attention model estimates a probability map in which high probabilities\nindicate locations where heads are likely to be present. The estimated\nprobability map is used to suppress non-head regions in feature maps from\nseveral multi-scale feature extraction branches of a convolution neural network\nfor crowd density estimation, which makes our method robust to complex\nbackgrounds, scale variations and non-uniform distributions. In addition, we\nintroduce a relative deviation loss to compensate a commonly used training\nloss, Euclidean distance, to improve the accuracy of sparse crowd density\nestimation. Experiments on Shanghai-Tech, UCF_CC_50 and World-Expo'10 data sets\ndemonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 03:52:19 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Zhang", "Youmei", ""], ["Zhou", "Chunluan", ""], ["Chang", "Faliang", ""], ["Kot", "Alex C.", ""]]}, {"id": "1806.10293", "submitter": "Alexander Irpan", "authors": "Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander\n  Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent\n  Vanhoucke, Sergey Levine", "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic\n  Manipulation", "comments": "CoRL 2018 camera ready. 23 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning vision-based dynamic\nmanipulation skills using a scalable reinforcement learning approach. We study\nthis problem in the context of grasping, a longstanding challenge in robotic\nmanipulation. In contrast to static learning behaviors that choose a grasp\npoint and then execute the desired grasp, our method enables closed-loop\nvision-based control, whereby the robot continuously updates its grasp strategy\nbased on the most recent observations to optimize long-horizon grasp success.\nTo that end, we introduce QT-Opt, a scalable self-supervised vision-based\nreinforcement learning framework that can leverage over 580k real-world grasp\nattempts to train a deep neural network Q-function with over 1.2M parameters to\nperform closed-loop, real-world grasping that generalizes to 96% grasp success\non unseen objects. Aside from attaining a very high success rate, our method\nexhibits behaviors that are quite distinct from more standard grasping systems:\nusing only RGB vision-based perception from an over-the-shoulder camera, our\nmethod automatically learns regrasping strategies, probes objects to find the\nmost effective grasps, learns to reposition objects and perform other\nnon-prehensile pre-grasp manipulations, and responds dynamically to\ndisturbances and perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 04:34:30 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 19:08:00 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 02:40:54 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kalashnikov", "Dmitry", ""], ["Irpan", "Alex", ""], ["Pastor", "Peter", ""], ["Ibarz", "Julian", ""], ["Herzog", "Alexander", ""], ["Jang", "Eric", ""], ["Quillen", "Deirdre", ""], ["Holly", "Ethan", ""], ["Kalakrishnan", "Mrinal", ""], ["Vanhoucke", "Vincent", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.10319", "submitter": "Dongliang He", "authors": "Dongliang He, Fu Li, Qijie Zhao, Xiang Long, Yi Fu, Shilei Wen", "title": "Exploiting Spatial-Temporal Modelling and Multi-Modal Fusion for Human\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, our approach to tackling the task of ActivityNet 2018\nKinetics-600 challenge is described in detail. Though spatial-temporal\nmodelling methods, which adopt either such end-to-end framework as I3D\n\\cite{i3d} or two-stage frameworks (i.e., CNN+RNN), have been proposed in\nexisting state-of-the-arts for this task, video modelling is far from being\nwell solved. In this challenge, we propose spatial-temporal network (StNet) for\nbetter joint spatial-temporal modelling and comprehensively video\nunderstanding. Besides, given that multi-modal information is contained in\nvideo source, we manage to integrate both early-fusion and later-fusion\nstrategy of multi-modal information via our proposed improved temporal Xception\nnetwork (iTXN) for video understanding. Our StNet RGB single model achieves\n78.99\\% top-1 precision in the Kinetics-600 validation set and that of our\nimproved temporal Xception network which integrates RGB, flow and audio\nmodalities is up to 82.35\\%. After model ensemble, we achieve top-1 precision\nas high as 85.0\\% on the validation set and rank No.1 among all submissions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 06:44:02 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["He", "Dongliang", ""], ["Li", "Fu", ""], ["Zhao", "Qijie", ""], ["Long", "Xiang", ""], ["Fu", "Yi", ""], ["Wen", "Shilei", ""]]}, {"id": "1806.10342", "submitter": "Yi-Jie Huang", "authors": "Yi-Jie Huang, Qi Dou, Zi-Xian Wang, Li-Zhi Liu, Ying Jin, Chao-Feng\n  Li, Lisheng Wang, Hao Chen, Rui-Hua Xu", "title": "3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor\n  Segmentation", "comments": "IEEE transactions on cybernetics (2020)", "journal-ref": null, "doi": "10.1109/TCYB.2020.2980145", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of colorectal cancerous regions from 3D Magnetic Resonance (MR)\nimages is a crucial procedure for radiotherapy which conventionally requires\naccurate delineation of tumour boundaries at an expense of labor, time and\nreproducibility. While deep learning based methods serve good baselines in 3D\nimage segmentation tasks, small applicable patch size limits effective\nreceptive field and degrades segmentation performance. In addition, Regions of\ninterest (RoIs) localization from large whole volume 3D images serves as a\npreceding operation that brings about multiple benefits in terms of speed,\ntarget completeness, reduction of false positives. Distinct from sliding window\nor non-joint localization-segmentation based models, we propose a novel\nmultitask framework referred to as 3D RoI-aware U-Net (3D RU-Net), for RoI\nlocalization and in-region segmentation where the two tasks share one backbone\nencoder network. With the region proposals from the encoder, we crop\nmulti-level RoI in-region features from the encoder to form a GPU\nmemory-efficient decoder for detailpreserving segmentation and therefore\nenlarged applicable volume size and effective receptive field. To effectively\ntrain the model, we designed a Dice formulated loss function for the\nglobal-to-local multi-task learning procedure. Based on the efficiency gains,\nwe went on to ensemble models with different receptive fields to achieve even\nhigher performance costing minor extra computational expensiveness. Extensive\nexperiments were conducted on 64 cancerous cases with a four-fold\ncross-validation, and the results showed significant superiority in terms of\naccuracy and efficiency over conventional frameworks. In conclusion, the\nproposed method has a huge potential for extension to other 3D object\nsegmentation tasks from medical images due to its inherent generalizability.\nThe code for the proposed method is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 08:42:58 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 08:04:59 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 04:27:27 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 06:37:11 GMT"}, {"version": "v5", "created": "Fri, 15 Feb 2019 08:46:25 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Huang", "Yi-Jie", ""], ["Dou", "Qi", ""], ["Wang", "Zi-Xian", ""], ["Liu", "Li-Zhi", ""], ["Jin", "Ying", ""], ["Li", "Chao-Feng", ""], ["Wang", "Lisheng", ""], ["Chen", "Hao", ""], ["Xu", "Rui-Hua", ""]]}, {"id": "1806.10343", "submitter": "Ya Lu", "authors": "Ya Lu, Dario Allegra, Marios Anthimopoulos, Filippo Stanco, Giovanni\n  Maria Farinella, Stavroula Mougiakakou", "title": "A Multi-Task Learning Approach for Meal Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key role in the prevention of diet-related chronic diseases plays the\nbalanced nutrition together with a proper diet. The conventional dietary\nassessment methods are time-consuming, expensive and prone to errors. New\ntechnology-based methods that provide reliable and convenient dietary\nassessment, have emerged during the last decade. The advances in the field of\ncomputer vision permitted the use of meal image to assess the nutrient content\nusually through three steps: food segmentation, recognition and volume\nestimation. In this paper, we propose a use one RGB meal image as input to a\nmulti-task learning based Convolutional Neural Network (CNN). The proposed\napproach achieved outstanding performance, while a comparison with\nstate-of-the-art methods indicated that the proposed approach exhibits clear\nadvantage in accuracy, along with a massive reduction of processing time.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 08:44:25 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Lu", "Ya", ""], ["Allegra", "Dario", ""], ["Anthimopoulos", "Marios", ""], ["Stanco", "Filippo", ""], ["Farinella", "Giovanni Maria", ""], ["Mougiakakou", "Stavroula", ""]]}, {"id": "1806.10348", "submitter": "Haoyue Shi", "authors": "Haoyue Shi, Jiayuan Mao, Tete Xiao, Yuning Jiang, Jian Sun", "title": "Learning Visually-Grounded Semantics from Contrastive Adversarial\n  Samples", "comments": "To Appear at COLING 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of grounding distributional representations of texts on\nthe visual domain, namely visual-semantic embeddings (VSE for short). Begin\nwith an insightful adversarial attack on VSE embeddings, we show the limitation\nof current frameworks and image-text datasets (e.g., MS-COCO) both\nquantitatively and qualitatively. The large gap between the number of possible\nconstitutions of real-world semantics and the size of parallel data, to a large\nextent, restricts the model to establish the link between textual semantics and\nvisual concepts. We alleviate this problem by augmenting the MS-COCO image\ncaptioning datasets with textual contrastive adversarial samples. These samples\nare synthesized using linguistic rules and the WordNet knowledge base. The\nconstruction procedure is both syntax- and semantics-aware. The samples enforce\nthe model to ground learned embeddings to concrete concepts within the image.\nThis simple but powerful technique brings a noticeable improvement over the\nbaselines on a diverse set of downstream tasks, in addition to defending\nknown-type adversarial attacks. We release the codes at\nhttps://github.com/ExplorerFreda/VSE-C.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 08:58:57 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Shi", "Haoyue", ""], ["Mao", "Jiayuan", ""], ["Xiao", "Tete", ""], ["Jiang", "Yuning", ""], ["Sun", "Jian", ""]]}, {"id": "1806.10350", "submitter": "Inon Sharony", "authors": "Viktor Mukha and Inon Sharony", "title": "Disparity Image Segmentation For ADAS", "comments": "7 pages, 5 figures, 2 tables, 1 algorithm", "journal-ref": "Elektronik Automotive issue 6 (2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple solution for segmenting grayscale images using existing\nConnected Component Labeling (CCL) algorithms (which are generally applied to\nbinary images), which was efficient enough to be implemented in a constrained\n(embedded automotive) architecture. Our solution customizes the region growing\nand merging approach, and is primarily targeted for stereoscopic disparity\nimages where nearer objects carry more relevance. We provide results from a\nstandard OpenCV implementation for some basic cases and an image from the\nTsukuba stereo-pair dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 09:01:38 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Mukha", "Viktor", ""], ["Sharony", "Inon", ""]]}, {"id": "1806.10354", "submitter": "Benjamin Hepp", "authors": "Benjamin Hepp, Debadeepta Dey, Sudipta N. Sinha, Ashish Kapoor, Neel\n  Joshi, Otmar Hilliges", "title": "Learn-to-Score: Efficient 3D Scene Exploration by Predicting View\n  Utility", "comments": "16 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera equipped drones are nowadays being used to explore large scenes and\nreconstruct detailed 3D maps. When free space in the scene is approximately\nknown, an offline planner can generate optimal plans to efficiently explore the\nscene. However, for exploring unknown scenes, the planner must predict and\nmaximize usefulness of where to go on the fly. Traditionally, this has been\nachieved using handcrafted utility functions. We propose to learn a better\nutility function that predicts the usefulness of future viewpoints. Our learned\nutility function is based on a 3D convolutional neural network. This network\ntakes as input a novel volumetric scene representation that implicitly captures\npreviously visited viewpoints and generalizes to new scenes. We evaluate our\nmethod on several large 3D models of urban scenes using simulated depth\ncameras. We show that our method outperforms existing utility measures in terms\nof reconstruction performance and is robust to sensor noise.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 09:09:50 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 11:50:20 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Hepp", "Benjamin", ""], ["Dey", "Debadeepta", ""], ["Sinha", "Sudipta N.", ""], ["Kapoor", "Ashish", ""], ["Joshi", "Neel", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1806.10359", "submitter": "Aymen Azaza", "authors": "Aymen Azaza and Joost van de Weijer and Ali Douik and Marc Masana", "title": "Context Proposals for Saliency Detection", "comments": "Accepted at Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": "10.1016/j.cviu.2018.06.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental properties of a salient object region is its contrast\nwith the immediate context. The problem is that numerous object regions exist\nwhich potentially can all be salient. One way to prevent an exhaustive search\nover all object regions is by using object proposal algorithms. These return a\nlimited set of regions which are most likely to contain an object. Several\nsaliency estimation methods have used object proposals. However, they focus on\nthe saliency of the proposal only, and the importance of its immediate context\nhas not been evaluated.\n  In this paper, we aim to improve salient object detection. Therefore, we\nextend object proposal methods with context proposals, which allow to\nincorporate the immediate context in the saliency computation. We propose\nseveral saliency features which are computed from the context proposals. In the\nexperiments, we evaluate five object proposal methods for the task of saliency\nsegmentation, and find that Multiscale Combinatorial Grouping outperforms the\nothers. Furthermore, experiments show that the proposed context features\nimprove performance, and that our method matches results on the FT datasets and\nobtains competitive results on three other datasets (PASCAL-S, MSRA-B and\nECSSD).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 09:11:50 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Azaza", "Aymen", ""], ["van de Weijer", "Joost", ""], ["Douik", "Ali", ""], ["Masana", "Marc", ""]]}, {"id": "1806.10417", "submitter": "Marvin Eisenberger", "authors": "Marvin Eisenberger, Zorah L\\\"ahner, Daniel Cremers", "title": "Divergence-Free Shape Interpolation and Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to model and calculate deformation fields between\nshapes embedded in $\\mathbb{R}^D$. Our framework combines naturally\ninterpolating the two input shapes and calculating correspondences at the same\ntime. The key idea is to compute a divergence-free deformation field\nrepresented in a coarse-to-fine basis using the Karhunen-Lo\\`eve expansion. The\nadvantages are that there is no need to discretize the embedding space and the\ndeformation is volume-preserving. Furthermore, the optimization is done on\ndownsampled versions of the shapes but the morphing can be applied to any\nresolution without a heavy increase in complexity. We show results for shape\ncorrespondence, registration, inter- and extrapolation on the TOSCA and FAUST\ndata sets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 11:37:24 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 11:52:34 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Eisenberger", "Marvin", ""], ["L\u00e4hner", "Zorah", ""], ["Cremers", "Daniel", ""]]}, {"id": "1806.10419", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Yao Wang, Alp Aygar, Sohae Chung, Xiuyuan Wang, Yvonne\n  W. Lui, Els Fieremans, Steven Flanagan, Joseph Rath", "title": "MTBI Identification From Diffusion MR Images Using Bag of Adversarial\n  Visual Features", "comments": "IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose bag of adversarial features (BAF) for identifying\nmild traumatic brain injury (MTBI) patients from their diffusion magnetic\nresonance images (MRI) (obtained within one month of injury) by incorporating\nunsupervised feature learning techniques. MTBI is a growing public health\nproblem with an estimated incidence of over 1.7 million people annually in US.\nDiagnosis is based on clinical history and symptoms, and accurate, concrete\nmeasures of injury are lacking. Unlike most of previous works, which use\nhand-crafted features extracted from different parts of brain for MTBI\nclassification, we employ feature learning algorithms to learn more\ndiscriminative representation for this task. A major challenge in this field\nthus far is the relatively small number of subjects available for training.\nThis makes it difficult to use an end-to-end convolutional neural network to\ndirectly classify a subject from MR images. To overcome this challenge, we\nfirst apply an adversarial auto-encoder (with convolutional structure) to learn\npatch-level features, from overlapping image patches extracted from different\nbrain regions. We then aggregate these features through a bag-of-word approach.\nWe perform an extensive experimental study on a dataset of 227 subjects\n(including 109 MTBI patients, and 118 age and sex matched healthy controls),\nand compare the bag-of-deep-features with several previous approaches. Our\nexperimental results show that the BAF significantly outperforms earlier works\nrelying on the mean values of MR metrics in selected brain regions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 11:41:34 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""], ["Aygar", "Alp", ""], ["Chung", "Sohae", ""], ["Wang", "Xiuyuan", ""], ["Lui", "Yvonne W.", ""], ["Fieremans", "Els", ""], ["Flanagan", "Steven", ""], ["Rath", "Joseph", ""]]}, {"id": "1806.10443", "submitter": "Wei Wang", "authors": "Wei Wang, Jing Dong, Yinlong Qian, Tieniu Tan", "title": "Deep Steganalysis: End-to-End Learning with Supervisory Information\n  beyond Class Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has shown its power in steganalysis. However, the\nproposed deep models have been often learned from pre-calculated noise\nresiduals with fixed high-pass filters rather than from raw images. In this\npaper, we propose a new end-to-end learning framework that can learn\nsteganalytic features directly from pixels. In the meantime, the high-pass\nfilters are also automatically learned. Besides class labels, we make use of\nadditional pixel level supervision of cover-stego image pair to jointly and\niteratively train the proposed network which consists of a residual calculation\nnetwork and a steganalysis network. The experimental results prove the\neffectiveness of the proposed architecture.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 12:46:30 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Qian", "Yinlong", ""], ["Tan", "Tieniu", ""]]}, {"id": "1806.10447", "submitter": "Alexey Gruzdev", "authors": "Sergey Zherzdev and Alexey Gruzdev", "title": "LPRNet: License Plate Recognition via Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes LPRNet - end-to-end method for Automatic License Plate\nRecognition without preliminary character segmentation. Our approach is\ninspired by recent breakthroughs in Deep Neural Networks, and works in\nreal-time with recognition accuracy up to 95% for Chinese license plates: 3\nms/plate on nVIDIA GeForce GTX 1080 and 1.3 ms/plate on Intel Core i7-6700K\nCPU. LPRNet consists of the lightweight Convolutional Neural Network, so it can\nbe trained in end-to-end way. To the best of our knowledge, LPRNet is the first\nreal-time License Plate Recognition system that does not use RNNs. As a result,\nthe LPRNet algorithm may be used to create embedded solutions for LPR that\nfeature high level accuracy even on challenging Chinese license plates.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 12:57:17 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Zherzdev", "Sergey", ""], ["Gruzdev", "Alexey", ""]]}, {"id": "1806.10457", "submitter": "Chaitanya Mitash", "authors": "Chaitanya Mitash, Abdeslam Boularias and Kostas Bekris", "title": "Physics-based Scene-level Reasoning for Object Pose Estimation in\n  Clutter", "comments": "18 pages, 13 figures, International Journal of Robotics Research\n  (IJRR) 2019. arXiv admin note: text overlap with arXiv:1710.08577", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on vision-based pose estimation for multiple rigid objects\nplaced in clutter, especially in cases involving occlusions and objects resting\non each other. Progress has been achieved recently in object recognition given\nadvancements in deep learning. Nevertheless, such tools typically require a\nlarge amount of training data and significant manual effort to label objects.\nThis limits their applicability in robotics, where solutions must scale to a\nlarge number of objects and variety of conditions. Moreover, the combinatorial\nnature of the scenes that could arise from the placement of multiple objects is\nhard to capture in the training dataset. Thus, the learned models might not\nproduce the desired level of precision required for tasks, such as robotic\nmanipulation. This work proposes an autonomous process for pose estimation that\nspans from data generation to scene-level reasoning and self-learning. In\nparticular, the proposed framework first generates a labeled dataset for\ntraining a Convolutional Neural Network (CNN) for object detection in clutter.\nThese detections are used to guide a scene-level optimization process, which\nconsiders the interactions between the different objects present in the clutter\nto output pose estimates of high precision. Furthermore, confident estimates\nare used to label online real images from multiple views and re-train the\nprocess in a self-learning pipeline. Experimental results indicate that this\nprocess is quickly able to identify in cluttered scenes physically-consistent\nobject poses that are more precise than the ones found by reasoning over\nindividual instances of objects. Furthermore, the quality of pose estimates\nincreases over time given the self-learning process.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 20:39:11 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 19:47:33 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Mitash", "Chaitanya", ""], ["Boularias", "Abdeslam", ""], ["Bekris", "Kostas", ""]]}, {"id": "1806.10472", "submitter": "Guillaume Noyel", "authors": "Michel Jourlin (IPRI), Guillaume Noyel (IPRI, SIGPH@iPRI)", "title": "Homogeneity of a region in the logarithmic image processing framework:\n  application to region growing algorithms", "comments": null, "journal-ref": "International Workshop on the Physics and Mechanics of Random\n  Structures: from Morphology to Material Properties, Jun 2018, Island of\n  Ol{\\'e}ron, France", "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper deals with the role played by Logarithmic Image Processing\n(LIP) operators for evaluating the homogeneity of a region. Two new criteria of\nheterogeneity are introduced, one based on the LIP addition and the other based\non the LIP scalar multiplication. Such tools are able to manage Region Growing\nalgorithms following the Revol's technique: starting from an initial seed, they\nconsist of applying specific dilations to the growing region while its\ninhomogeneity level does not exceed a certain level. The new approaches we\nintroduce are significantly improving Revol's existing technique by making it\nrobust to contrast variations in images. Such a property strongly reduces the\nchaining effect arising in region growing processes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 13:28:46 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Jourlin", "Michel", "", "IPRI"], ["Noyel", "Guillaume", "", "IPRI, SIGPH@iPRI"]]}, {"id": "1806.10496", "submitter": "Shih-Hong Tsai", "authors": "Shih-hong Tsai", "title": "Customizing an Adversarial Example Generator with Class-Conditional GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are intentionally crafted data with the purpose of\ndeceiving neural networks into misclassification. When we talk about strategies\nto create such examples, we usually refer to perturbation-based methods that\nfabricate adversarial examples by applying invisible perturbations onto normal\ndata. The resulting data reserve their visual appearance to human observers,\nyet can be totally unrecognizable to DNN models, which in turn leads to\ncompletely misleading predictions. In this paper, however, we consider crafting\nadversarial examples from existing data as a limitation to example diversity.\nWe propose a non-perturbation-based framework that generates native adversarial\nexamples from class-conditional generative adversarial networks.As such, the\ngenerated data will not resemble any existing data and thus expand example\ndiversity, raising the difficulty in adversarial defense. We then extend this\nframework to pre-trained conditional GANs, in which we turn an existing\ngenerator into an \"adversarial-example generator\". We conduct experiments on\nour approach for MNIST and CIFAR10 datasets and have satisfactory results,\nshowing that this approach can be a potential alternative to previous attack\nstrategies.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 14:24:27 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Tsai", "Shih-hong", ""]]}, {"id": "1806.10556", "submitter": "Zhenheng Yang", "authors": "Zhenheng Yang and Peng Wang and Yang Wang and Wei Xu and Ram Nevatia", "title": "Every Pixel Counts: Unsupervised Geometry Learning with Holistic 3D\n  Motion Understanding", "comments": "ECCV18' submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to estimate 3D geometry in a single image by watching unlabeled\nvideos via deep convolutional network has made significant process recently.\nCurrent state-of-the-art (SOTA) methods, are based on the learning framework of\nrigid structure-from-motion, where only 3D camera ego motion is modeled for\ngeometry estimation.However, moving objects also exist in many videos, e.g.\nmoving cars in a street scene. In this paper, we tackle such motion by\nadditionally incorporating per-pixel 3D object motion into the learning\nframework, which provides holistic 3D scene flow understanding and helps single\nimage geometry estimation. Specifically, given two consecutive frames from a\nvideo, we adopt a motion network to predict their relative 3D camera pose and a\nsegmentation mask distinguishing moving objects and rigid background. An\noptical flow network is used to estimate dense 2D per-pixel correspondence. A\nsingle image depth network predicts depth maps for both images. The four types\nof information, i.e. 2D flow, camera pose, segment mask and depth maps, are\nintegrated into a differentiable holistic 3D motion parser (HMP), where\nper-pixel 3D motion for rigid background and moving objects are recovered. We\ndesign various losses w.r.t. the two types of 3D motions for training the depth\nand motion networks, yielding further error reduction for estimated geometry.\nFinally, in order to solve the 3D motion confusion from monocular videos, we\ncombine stereo images into joint training. Experiments on KITTI 2015 dataset\nshow that our estimated geometry, 3D motion and moving object masks, not only\nare constrained to be consistent, but also significantly outperforms other SOTA\nalgorithms, demonstrating the benefits of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 16:23:03 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 18:34:48 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Yang", "Zhenheng", ""], ["Wang", "Peng", ""], ["Wang", "Yang", ""], ["Xu", "Wei", ""], ["Nevatia", "Ram", ""]]}, {"id": "1806.10574", "submitter": "Chaofan Chen", "authors": "Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su,\n  Cynthia Rudin", "title": "This Looks Like That: Deep Learning for Interpretable Image Recognition", "comments": "Chaofan Chen and Oscar Li contributed equally to this work. This work\n  has been accepted for spotlight presentation (top 3% of papers) at NeurIPS\n  2019", "journal-ref": "Advances in Neural Information Processing Systems 32 (NeurIPS\n  2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we are faced with challenging image classification tasks, we often\nexplain our reasoning by dissecting the image, and pointing out prototypical\naspects of one class or another. The mounting evidence for each of the classes\nhelps us make our final decision. In this work, we introduce a deep network\narchitecture -- prototypical part network (ProtoPNet), that reasons in a\nsimilar way: the network dissects the image by finding prototypical parts, and\ncombines evidence from the prototypes to make a final classification. The model\nthus reasons in a way that is qualitatively similar to the way ornithologists,\nphysicians, and others would explain to people on how to solve challenging\nimage classification tasks. The network uses only image-level labels for\ntraining without any annotations for parts of images. We demonstrate our method\non the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show\nthat ProtoPNet can achieve comparable accuracy with its analogous\nnon-interpretable counterpart, and when several ProtoPNets are combined into a\nlarger network, it can achieve an accuracy that is on par with some of the\nbest-performing deep models. Moreover, ProtoPNet provides a level of\ninterpretability that is absent in other interpretable deep models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 17:18:03 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 16:44:23 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 17:35:02 GMT"}, {"version": "v4", "created": "Fri, 22 Nov 2019 18:10:36 GMT"}, {"version": "v5", "created": "Sat, 28 Dec 2019 20:12:11 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Chaofan", ""], ["Li", "Oscar", ""], ["Tao", "Chaofan", ""], ["Barnett", "Alina Jade", ""], ["Su", "Jonathan", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1806.10678", "submitter": "Pingfan Song", "authors": "Pingfan Song, Miguel R.D. Rodrigues", "title": "Multimodal Image Denoising based on Coupled Dictionary Learning", "comments": "2018 IEEE International Conference on Image Processing (ICIP). arXiv\n  admin note: text overlap with arXiv:1806.09882", "journal-ref": null, "doi": "10.1109/ICIP.2018.8451697", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new multimodal image denoising approach to\nattenuate white Gaussian additive noise in a given image modality under the aid\nof a guidance image modality. The proposed coupled image denoising approach\nconsists of two stages: coupled sparse coding and reconstruction. The first\nstage performs joint sparse transform for multimodal images with respect to a\ngroup of learned coupled dictionaries, followed by a shrinkage operation on the\nsparse representations. Then, in the second stage, the shrunken\nrepresentations, together with coupled dictionaries, contribute to the\nreconstruction of the denoised image via an inverse transform. The proposed\ndenoising scheme demonstrates the capability to capture both the common and\ndistinct features of different data modalities. This capability makes our\napproach more robust to inconsistencies between the guidance and the target\nimages, thereby overcoming drawbacks such as the texture copying artifacts.\nExperiments on real multimodal images demonstrate that the proposed approach is\nable to better employ guidance information to bring notable benefits in the\nimage denoising task with respect to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 10:56:14 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Song", "Pingfan", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1806.10681", "submitter": "Odemir Bruno PhD", "authors": "Lucas C. Ribas, Wesley N. Goncalves, Odemir M. Bruno", "title": "Dynamic texture analysis with diffusion in networks", "comments": "30 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic texture is a field of research that has gained considerable interest\nfrom computer vision community due to the explosive growth of multimedia\ndatabases. In addition, dynamic texture is present in a wide range of videos,\nwhich makes it very important in expert systems based on videos such as medical\nsystems, traffic monitoring systems, forest fire detection system, among\nothers. In this paper, a new method for dynamic texture characterization based\non diffusion in directed networks is proposed. The dynamic texture is modeled\nas a directed network. The method consists in the analysis of the dynamic of\nthis network after a series of graph cut transformations based on the edge\nweights. For each network transformation, the activity for each vertex is\nestimated. The activity is the relative frequency that one vertex is visited by\nrandom walks in balance. Then, texture descriptor is constructed by\nconcatenating the activity histograms. The main contributions of this paper are\nthe use of directed network modeling and diffusion in network to dynamic\ntexture characterization. These tend to provide better performance in dynamic\ntextures classification. Experiments with rotation and interference of the\nmotion pattern were conducted in order to demonstrate the robustness of the\nmethod. The proposed approach is compared to other dynamic texture methods on\ntwo very well know dynamic texture database and on traffic condition\nclassification, and outperform in most of the cases.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 20:14:08 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Ribas", "Lucas C.", ""], ["Goncalves", "Wesley N.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1806.10696", "submitter": "Fei Xu", "authors": "Fei Xu, Min Xian, Yingtao Zhang, Kuan Huang, H. D. Cheng, Boyu Zhang,\n  Jianrui Ding, Chunping Ning, Ying Wang", "title": "A Hybrid Framework for Tumor Saliency Estimation", "comments": "6 pages and 8 figures, Conference paper and accepted by ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic tumor segmentation of breast ultrasound (BUS) image is quite\nchallenging due to the complicated anatomic structure of breast and poor image\nquality. Most tumor segmentation approaches achieve good performance on BUS\nimages collected in controlled settings; however, the performance degrades\ngreatly with BUS images from different sources. Tumor saliency estimation (TSE)\nhas attracted increasing attention to solving the problem by modeling\nradiologists' attention mechanism. In this paper, we propose a novel hybrid\nframework for TSE, which integrates both high-level domain-knowledge and robust\nlow-level saliency assumptions and can overcome drawbacks caused by direct\nmapping in traditional TSE approaches. The new framework integrated the\nNeutro-Connectedness (NC) map, the adaptive-center, the correlation and the\nlayer structure-based weighted map. The experimental results demonstrate that\nthe proposed approach outperforms state-of-the-art TSE methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 20:51:17 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Xu", "Fei", ""], ["Xian", "Min", ""], ["Zhang", "Yingtao", ""], ["Huang", "Kuan", ""], ["Cheng", "H. D.", ""], ["Zhang", "Boyu", ""], ["Ding", "Jianrui", ""], ["Ning", "Chunping", ""], ["Wang", "Ying", ""]]}, {"id": "1806.10707", "submitter": "Jasjeet Dhaliwal", "authors": "Jasjeet Dhaliwal, Saurabh Shintre", "title": "Gradient Similarity: An Explainable Approach to Detect Adversarial\n  Attacks against Deep Learning", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are susceptible to small-but-specific adversarial\nperturbations capable of deceiving the network. This vulnerability can lead to\npotentially harmful consequences in security-critical applications. To address\nthis vulnerability, we propose a novel metric called \\emph{Gradient Similarity}\nthat allows us to capture the influence of training data on test inputs. We\nshow that \\emph{Gradient Similarity} behaves differently for normal and\nadversarial inputs, and enables us to detect a variety of adversarial attacks\nwith a near perfect ROC-AUC of 95-100\\%. Even white-box adversaries equipped\nwith perfect knowledge of the system cannot bypass our detector easily. On the\nMNIST dataset, white-box attacks are either detected with a high ROC-AUC of\n87-96\\%, or require very high distortion to bypass our detector.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 22:47:37 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Dhaliwal", "Jasjeet", ""], ["Shintre", "Saurabh", ""]]}, {"id": "1806.10726", "submitter": "Junjun Jiang", "authors": "Junjun Jiang, Yi Yu, Jinhui Hu, Suhua Tang, Jiayi Ma", "title": "Deep CNN Denoiser and Multi-layer Neighbor Component Embedding for Face\n  Hallucination", "comments": "Accepted by IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current face hallucination methods, whether they are shallow\nlearning-based or deep learning-based, all try to learn a relationship model\nbetween Low-Resolution (LR) and High-Resolution (HR) spaces with the help of a\ntraining set. They mainly focus on modeling image prior through either\nmodel-based optimization or discriminative inference learning. However, when\nthe input LR face is tiny, the learned prior knowledge is no longer effective\nand their performance will drop sharply. To solve this problem, in this paper\nwe propose a general face hallucination method that can integrate model-based\noptimization and discriminative inference. In particular, to exploit the model\nbased prior, the Deep Convolutional Neural Networks (CNN) denoiser prior is\nplugged into the super-resolution optimization model with the aid of\nimage-adaptive Laplacian regularization. Additionally, we further develop a\nhigh-frequency details compensation method by dividing the face image to facial\ncomponents and performing face hallucination in a multi-layer neighbor\nembedding manner. Experiments demonstrate that the proposed method can achieve\npromising super-resolution results for tiny input LR faces.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 01:00:09 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Jiang", "Junjun", ""], ["Yu", "Yi", ""], ["Hu", "Jinhui", ""], ["Tang", "Suhua", ""], ["Ma", "Jiayi", ""]]}, {"id": "1806.10746", "submitter": "Hiroyuki Kobayashi", "authors": "Osamu Watanabe, Hiroyuki Kobayashi, Hitoshi Kiya", "title": "Two-layer Lossless HDR Coding considering Histogram Sparseness with\n  Backward Compatibility to JPEG", "comments": "to appear in Proc. Picture Coding Symposium, San Francisco, USA, 25th\n  June, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient two-layer coding method using the histogram packing technique\nwith the backward compatibility to the legacy JPEG is proposed in this paper.\nThe JPEG XT, which is the international standard to compress HDR images, adopts\ntwo-layer coding scheme for backward compatibility to the legacy JPEG. However,\nthis two-layer coding structure does not give better lossless performance than\nthe other existing single-layer coding methods for HDR images. Moreover, the\nJPEG XT has problems on determination of the lossless coding parameters;\nFinding appropriate combination of the parameter values is necessary to achieve\ngood lossless performance. The histogram sparseness of HDR images is discussed\nand it is pointed out that the histogram packing technique considering the\nsparseness is able to improve the performance of lossless compression for HDR\nimages and a novel two-layer coding with the histogram packing technique is\nproposed. The experimental results demonstrate that not only the proposed\nmethod has a better lossless compression performance than that of the JPEG XT,\nbut also there is no need to determine image-dependent parameter values for\ngood compression performance in spite of having the backward compatibility to\nthe well known legacy JPEG standard.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 02:53:11 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Watanabe", "Osamu", ""], ["Kobayashi", "Hiroyuki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1806.10748", "submitter": "Ayushi Sinha", "authors": "Ayushi Sinha, Masaru Ishii, Russell H. Taylor, Gregory D. Hager and\n  Austin Reiter", "title": "Towards automatic initialization of registration algorithms using\n  simulated endoscopy images", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registering images from different modalities is an active area of research in\ncomputer aided medical interventions. Several registration algorithms have been\ndeveloped, many of which achieve high accuracy. However, these results are\ndependent on many factors, including the quality of the extracted features or\nsegmentations being registered as well as the initial alignment. Although\nseveral methods have been developed towards improving segmentation algorithms\nand automating the segmentation process, few automatic initialization\nalgorithms have been explored. In many cases, the initial alignment from which\na registration is initiated is performed manually, which interferes with the\nclinical workflow. Our aim is to use scene classification in endoscopic\nprocedures to achieve coarse alignment of the endoscope and a preoperative\nimage of the anatomy. In this paper, we show using simulated scenes that a\nneural network can predict the region of anatomy (with respect to a\npreoperative image) that the endoscope is located in by observing a single\nendoscopic video frame. With limited training and without any hyperparameter\ntuning, our method achieves an accuracy of 76.53 (+/-1.19)%. There are several\navenues for improvement, making this a promising direction of research. Code is\navailable at https://github.com/AyushiSinha/AutoInitialization.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 02:58:25 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Sinha", "Ayushi", ""], ["Ishii", "Masaru", ""], ["Taylor", "Russell H.", ""], ["Hager", "Gregory D.", ""], ["Reiter", "Austin", ""]]}, {"id": "1806.10759", "submitter": "Yuqi Han", "authors": "Yuqi Han, Chenwei Deng, Zengshuo Zhang, Jinghong Nan and Baojun Zhao", "title": "State-aware Anti-drift Robust Correlation Tracking", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TIP.2019.2905984", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter (CF) based trackers have aroused increasing attentions in\nvisual tracking field due to the superior performance on several datasets while\nmaintaining high running speed. For each frame, an ideal filter is trained in\norder to discriminate the target from its surrounding background. Considering\nthat the target always undergoes external and internal interference during\ntracking procedure, the trained filter should take consideration of not only\nthe external distractions but also the target appearance variation\nsynchronously. To this end, we present a State-aware Anti-drift Tracker (SAT)\nin this paper, which jointly model the discrimination and reliability\ninformation in filter learning. Specifically, global context patches are\nincorporated into filter training stage to better distinguish the target from\nbackgrounds. Meanwhile, a color-based reliable mask is learned to encourage the\nfilter to focus on more reliable regions suitable for tracking. We show that\nthe proposed optimization problem could be efficiently solved using Alternative\nDirection Method of Multipliers and fully carried out in Fourier domain.\nExtensive experiments are conducted on OTB-100 datasets to compare the SAT\ntracker (both hand-crafted feature and CNN feature) with other relevant\nstate-of-the-art methods. Both quantitative and qualitative evaluations further\ndemonstrate the effectiveness and robustness of the proposed work.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 03:58:44 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Han", "Yuqi", ""], ["Deng", "Chenwei", ""], ["Zhang", "Zengshuo", ""], ["Nan", "Jinghong", ""], ["Zhao", "Baojun", ""]]}, {"id": "1806.10779", "submitter": "Ping Luo", "authors": "Ping Luo, Jiamin Ren, Zhanglin Peng, Ruimao Zhang, Jingyu Li", "title": "Differentiable Learning-to-Normalize via Switchable Normalization", "comments": "International Conference on Learning Representations (ICLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a learning-to-normalize problem by proposing Switchable\nNormalization (SN), which learns to select different normalizers for different\nnormalization layers of a deep neural network. SN employs three distinct scopes\nto compute statistics (means and variances) including a channel, a layer, and a\nminibatch. SN switches between them by learning their importance weights in an\nend-to-end manner. It has several good properties. First, it adapts to various\nnetwork architectures and tasks (see Fig.1). Second, it is robust to a wide\nrange of batch sizes, maintaining high performance even when small minibatch is\npresented (e.g. 2 images/GPU). Third, SN does not have sensitive\nhyper-parameter, unlike group normalization that searches the number of groups\nas a hyper-parameter. Without bells and whistles, SN outperforms its\ncounterparts on various challenging benchmarks, such as ImageNet, COCO,\nCityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN\nwill help ease the usage and understand the normalization techniques in deep\nlearning. The code of SN has been made available in\nhttps://github.com/switchablenorms/.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 05:55:57 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 11:22:57 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 06:09:51 GMT"}, {"version": "v4", "created": "Sun, 30 Sep 2018 06:10:28 GMT"}, {"version": "v5", "created": "Wed, 24 Apr 2019 05:31:23 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Luo", "Ping", ""], ["Ren", "Jiamin", ""], ["Peng", "Zhanglin", ""], ["Zhang", "Ruimao", ""], ["Li", "Jingyu", ""]]}, {"id": "1806.10781", "submitter": "Chen Du", "authors": "Chen Du, Byeongkeun Kang, Zheng Xu, Ji Dai and Truong Nguyen", "title": "Accurate and efficient video de-fencing using convolutional neural\n  networks and temporal information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De-fencing is to eliminate the captured fence on an image or a video,\nproviding a clear view of the scene. It has been applied for many purposes\nincluding assisting photographers and improving the performance of computer\nvision algorithms such as object detection and recognition. However, the\nstate-of-the-art de-fencing methods have limited performance caused by the\ndifficulty of fence segmentation and also suffer from the motion of the camera\nor objects. To overcome these problems, we propose a novel method consisting of\nsegmentation using convolutional neural networks and a fast/robust recovery\nalgorithm. The segmentation algorithm using convolutional neural network\nachieves significant improvement in the accuracy of fence segmentation. The\nrecovery algorithm using optical flow produces plausible de-fenced images and\nvideos. The proposed method is experimented on both our diverse and complex\ndataset and publicly available datasets. The experimental results demonstrate\nthat the proposed method achieves the state-of-the-art performance for both\nsegmentation and content recovery.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 05:59:56 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Du", "Chen", ""], ["Kang", "Byeongkeun", ""], ["Xu", "Zheng", ""], ["Dai", "Ji", ""], ["Nguyen", "Truong", ""]]}, {"id": "1806.10787", "submitter": "Vijay Gabale Dr", "authors": "Vijay Gabale, Anand Prabhu Subramanian", "title": "How To Extract Fashion Trends From Social Media? A Robust Object\n  Detector With Support For Unsupervised Learning", "comments": "6 pages, 3 figures, AI for Fashion, KDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of social media, fashion inspired from celebrities,\nreputed designers as well as fashion influencers has shortened the cycle of\nfashion design and manufacturing. However, with the explosion of fashion\nrelated content and large number of user generated fashion photos, it is an\narduous task for fashion designers to wade through social media photos and\ncreate a digest of trending fashion. This necessitates deep parsing of fashion\nphotos on social media to localize and classify multiple fashion items from a\ngiven fashion photo. While object detection competitions such as MSCOCO have\nthousands of samples for each of the object categories, it is quite difficult\nto get large labeled datasets for fast fashion items. Moreover,\nstate-of-the-art object detectors do not have any functionality to ingest large\namount of unlabeled data available on social media in order to fine tune object\ndetectors with labeled datasets. In this work, we show application of a generic\nobject detector, that can be pretrained in an unsupervised manner, on 24\ncategories from recently released Open Images V4 dataset. We first train the\nbase architecture of the object detector using unsupervisd learning on 60K\nunlabeled photos from 24 categories gathered from social media, and then\nsubsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset.\nOn 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K\nphotos while performing 11% to 17% better as compared to the state-of-the-art\nobject detectors. We show that this improvement is due to our choice of\narchitecture that lets us do unsupervised learning and that performs\nsignificantly better in identifying small objects.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 06:23:56 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Gabale", "Vijay", ""], ["Subramanian", "Anand Prabhu", ""]]}, {"id": "1806.10805", "submitter": "Pau Rodr\\'iguez L\\'opez", "authors": "Pau Rodr\\'iguez, Miguel A. Bautista, Jordi Gonz\\`alez, Sergio Escalera", "title": "Beyond One-hot Encoding: lower dimensional target embedding", "comments": "Published at Image and Vision Computing", "journal-ref": null, "doi": "10.1016/j.imavis.2018.04.004", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target encoding plays a central role when learning Convolutional Neural\nNetworks. In this realm, One-hot encoding is the most prevalent strategy due to\nits simplicity. However, this so widespread encoding schema assumes a flat\nlabel space, thus ignoring rich relationships existing among labels that can be\nexploited during training. In large-scale datasets, data does not span the full\nlabel space, but instead lies in a low-dimensional output manifold. Following\nthis observation, we embed the targets into a low-dimensional space,\ndrastically improving convergence speed while preserving accuracy. Our\ncontribution is two fold: (i) We show that random projections of the label\nspace are a valid tool to find such lower dimensional embeddings, boosting\ndramatically convergence rates at zero computational cost; and (ii) we propose\na normalized eigenrepresentation of the class manifold that encodes the targets\nwith minimal information loss, improving the accuracy of random projections\nencoding while enjoying the same convergence rates. Experiments on CIFAR-100,\nCUB200-2011, Imagenet, and MIT Places demonstrate that the proposed approach\ndrastically improves convergence speed while reaching very competitive accuracy\nrates.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 07:34:14 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Rodr\u00edguez", "Pau", ""], ["Bautista", "Miguel A.", ""], ["Gonz\u00e0lez", "Jordi", ""], ["Escalera", "Sergio", ""]]}, {"id": "1806.10821", "submitter": "Hyeji Kim", "authors": "Hyeji Kim, Chong-Min Kyung", "title": "Automatic Rank Selection for High-Speed Convolutional Neural Network", "comments": "This idea was submitted to CVPR 2018 (Nov. 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank decomposition plays a central role in accelerating convolutional\nneural network (CNN), and the rank of decomposed kernel-tensor is a key\nparameter that determines the complexity and accuracy of a neural network. In\nthis paper, we define rank selection as a combinatorial optimization problem\nand propose a methodology to minimize network complexity while maintaining the\ndesired accuracy. Combinatorial optimization is not feasible due to search\nspace limitations. To restrict the search space and obtain the optimal rank, we\ndefine the space constraint parameters with a boundary condition. We also\npropose a linearly-approximated accuracy function to predict the fine-tuned\naccuracy of the optimized CNN model during the cost reduction. Experimental\nresults on AlexNet and VGG-16 show that the proposed rank selection algorithm\nsatisfies the accuracy constraint. Our method combined with truncated-SVD\noutperforms state-of-the-art methods in terms of inference and training time at\nalmost the same accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 08:25:40 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 10:03:22 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Kim", "Hyeji", ""], ["Kyung", "Chong-Min", ""]]}, {"id": "1806.10830", "submitter": "Wang Rui", "authors": "Rui Wang, Xiao-Jun Wu, Kai-Xuan Chen, Josef Kittler", "title": "Grassmannian Discriminant Maps (GDM) for Manifold Dimensionality\n  Reduction with Application to Image Set Classification", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image set classification, a considerable progress has been made by\nrepresenting original image sets on Grassmann manifolds. In order to extend the\nadvantages of the Euclidean based dimensionality reduction methods to the\nGrassmann Manifold, several methods have been suggested recently which jointly\nperform dimensionality reduction and metric learning on Grassmann manifold to\nimprove performance. Nevertheless, when applied to complex datasets, the\nlearned features do not exhibit enough discriminatory power. To overcome this\nproblem, we propose a new method named Grassmannian Discriminant Maps (GDM) for\nmanifold dimensionality reduction problems. The core of the method is a new\ndiscriminant function for metric learning and dimensionality reduction. For\ncomparison and better understanding, we also study a simple variations to GDM.\nThe key difference between them is the discriminant function. We experiment on\ndata sets corresponding to three tasks: face recognition, object\ncategorization, and hand gesture recognition to evaluate the proposed method\nand its simple extensions. Compared with the state of the art, the results\nachieved show the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 08:50:24 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Wang", "Rui", ""], ["Wu", "Xiao-Jun", ""], ["Chen", "Kai-Xuan", ""], ["Kittler", "Josef", ""]]}, {"id": "1806.10836", "submitter": "Alessia Amelio Dr.", "authors": "Lucio Amelio and Alessia Amelio", "title": "CT Image Registration in Acute Stroke Monitoring", "comments": "10 pages, 9 figures, Accepted at the 41th Jubilee International\n  Convention on Information and Communication Technology, Electronics and\n  Microelectronics (MIPRO), Opatija, Croatia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new system based on tracking the temporal evolution of stroke\nlesions using an image registration technique on CT exams of the patient's\nbrain. The system is able to compare past CT exams with the most recent one\nrelated to stroke event in order to evaluate past lesions which are not related\nto stroke. Then, it can compare recent CT exams related to the current stroke\nfor assessing the evolution of the lesion over time. A new similarity measure\nis also introduced for the comparison of the source and target images during\nimage registration. It will result in a cheaper, faster and more accessible\nevaluation of the acute phase of the stroke overcoming the current limitations\nof the proposed systems in the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 09:11:57 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Amelio", "Lucio", ""], ["Amelio", "Alessia", ""]]}, {"id": "1806.10850", "submitter": "Priya Narayanan", "authors": "Priya Lakshmi Narayanan, Shan E Ahmed Raza, Andrew Dodson, Barry\n  Gusterson, Mitchell Dowsett and Yinyin Yuan", "title": "DeepSDCS: Dissecting cancer proliferation heterogeneity in Ki67 digital\n  whole slide images", "comments": null, "journal-ref": "MIDL 2018 Conference", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ki67 is an important biomarker for breast cancer. Classification of positive\nand negative Ki67 cells in histology slides is a common approach to determine\ncancer proliferation status. However, there is a lack of generalizable and\naccurate methods to automate Ki67 scoring in large-scale patient cohorts. In\nthis work, we have employed a novel deep learning technique based on\nhypercolumn descriptors for cell classification in Ki67 images. Specifically,\nwe developed the Simultaneous Detection and Cell Segmentation (DeepSDCS)\nnetwork to perform cell segmentation and detection. VGG16 network was used for\nthe training and fine tuning to training data. We extracted the hypercolumn\ndescriptors of each cell to form the vector of activation from specific layers\nto capture features at different granularity. Features from these layers that\ncorrespond to the same pixel were propagated using a stochastic gradient\ndescent optimizer to yield the detection of the nuclei and the final cell\nsegmentations. Subsequently, seeds generated from cell segmentation were\npropagated to a spatially constrained convolutional neural network for the\nclassification of the cells into stromal, lymphocyte, Ki67-positive cancer\ncell, and Ki67-negative cancer cell. We validated its accuracy in the context\nof a large-scale clinical trial of oestrogen-receptor-positive breast cancer.\nWe achieved 99.06% and 89.59% accuracy on two separate test sets of Ki67\nstained breast cancer dataset comprising biopsy and whole-slide images.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 09:43:07 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Narayanan", "Priya Lakshmi", ""], ["Raza", "Shan E Ahmed", ""], ["Dodson", "Andrew", ""], ["Gusterson", "Barry", ""], ["Dowsett", "Mitchell", ""], ["Yuan", "Yinyin", ""]]}, {"id": "1806.10866", "submitter": "Eugen Rusakov", "authors": "Eugen Rusakov, Sebastian Sudholt, Fabian Wolf, Gernot A. Fink", "title": "Expolring Architectures for CNN-Based Word Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal in word spotting is to retrieve parts of document images which are\nrelevant with respect to a certain user-defined query. The recent past has seen\nattribute-based Convolutional Neural Networks take over this field of research.\nAs is common for other fields of computer vision, the CNNs used for this task\nare already considerably deep. The question that arises, however, is: How\ncomplex does a CNN have to be for word spotting? Are increasingly deeper models\ngiving increasingly bet- ter results or does performance behave asymptotically\nfor these architectures? On the other hand, can similar results be obtained\nwith a much smaller CNN? The goal of this paper is to give an answer to these\nquestions. Therefore, the recently successful TPP- PHOCNet will be compared to\na Residual Network, a Densely Connected Convolutional Network and a LeNet\narchitecture empirically. As will be seen in the evaluation, a complex model\ncan be beneficial for word spotting on harder tasks such as the IAM Offline\nDatabase but gives no advantage for easier benchmarks such as the George\nWashington Database.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 10:20:41 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Rusakov", "Eugen", ""], ["Sudholt", "Sebastian", ""], ["Wolf", "Fabian", ""], ["Fink", "Gernot A.", ""]]}, {"id": "1806.10890", "submitter": "Joseph Lemley", "authors": "Joseph Lemley, Anuradha Kar, Alexandru Drimbarean, Peter Corcoran", "title": "Efficient CNN Implementation for Eye-Gaze Estimation on\n  Low-Power/Low-Quality Consumer Imaging Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient eye gaze estimation is important for emerging consumer\nelectronic systems such as driver monitoring systems and novel user interfaces.\nSuch systems are required to operate reliably in difficult, unconstrained\nenvironments with low power consumption and at minimal cost. In this paper a\nnew hardware friendly, convolutional neural network model with minimal\ncomputational requirements is introduced and assessed for efficient\nappearance-based gaze estimation. The model is tested and compared against\nexisting appearance based CNN approaches, achieving better eye gaze accuracy\nwith significantly fewer computational requirements. A brief updated literature\nreview is also provided.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 11:33:03 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Lemley", "Joseph", ""], ["Kar", "Anuradha", ""], ["Drimbarean", "Alexandru", ""], ["Corcoran", "Peter", ""]]}, {"id": "1806.10923", "submitter": "Alexandre Benoit", "authors": "A Benoit (LISTIC), Leonel Cuevas, Jean-Baptiste Thomas (Le2i)", "title": "Deep learning for dehazing: Comparison and analysis", "comments": null, "journal-ref": "Colour and Visual Computing Symposium (CVCS), Sep 2018,\n  Gj{{\\o}}vik, Norway", "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare a recent dehazing method based on deep learning, Dehazenet, with\ntraditional state-of-the-art approaches , on benchmark data with reference.\nDehazenet estimates the depth map from transmission factor on a single color\nimage, which is used to inverse the Koschmieder model of imaging in the\npresence of haze. In this sense, the solution is still attached to the\nKoschmieder model. We demonstrate that the transmission is very well estimated\nby the network, but also that this method exhibits the same limitation than\nothers due to the use of the same imaging model.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 12:37:54 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Benoit", "A", "", "LISTIC"], ["Cuevas", "Leonel", "", "Le2i"], ["Thomas", "Jean-Baptiste", "", "Le2i"]]}, {"id": "1806.10982", "submitter": "Evgeny Izutov", "authors": "Evgeny Izutov", "title": "High Diversity Attribute Guided Face Generation with GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we focused on GAN-based solution for the attribute guided face\nsynthesis. Previous works exploited GANs for generation of photo-realistic face\nimages and did not pay attention to the question of diversity of the resulting\nimages. The proposed solution in its turn introducing novel latent space of\nunit complex numbers is able to provide the diversity on the \"birthday paradox\"\nscore 3 times higher than the size of the training dataset. It is important to\nemphasize that our result is shown on relatively small dataset (20k samples vs\n200k) while preserving photo-realistic properties of generated faces on\nsignificantly higher resolution (128x128 in comparison to 32x32 of previous\nworks).\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 14:02:21 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Izutov", "Evgeny", ""]]}, {"id": "1806.11008", "submitter": "Guilhem Ch\\'eron", "authors": "Guilhem Ch\\'eron, Anton Osokin, Ivan Laptev, Cordelia Schmid", "title": "Modeling Spatio-Temporal Human Track Structure for Action Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses spatio-temporal localization of human actions in video.\nIn order to localize actions in time, we propose a recurrent localization\nnetwork (RecLNet) designed to model the temporal structure of actions on the\nlevel of person tracks. Our model is trained to simultaneously recognize and\nlocalize action classes in time and is based on two layer gated recurrent units\n(GRU) applied separately to two streams, i.e. appearance and optical flow\nstreams. When used together with state-of-the-art person detection and\ntracking, our model is shown to improve substantially spatio-temporal action\nlocalization in videos. The gain is shown to be mainly due to improved temporal\nlocalization. We evaluate our method on two recent datasets for spatio-temporal\naction localization, UCF101-24 and DALY, demonstrating a significant\nimprovement of the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 14:34:00 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Ch\u00e9ron", "Guilhem", ""], ["Osokin", "Anton", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1806.11011", "submitter": "Thomas Serre", "authors": "Yuliang Guo, Lakshmi Narasimhan Govindarajan, Benjamin Kimia, Thomas\n  Serre", "title": "Robust pose tracking with a joint model of appearance and shape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for estimating the 2D pose of an articulated\nobject with an application to automated video analysis of small laboratory\nanimals. We have found that deformable part models developed for humans,\nexemplified by the flexible mixture of parts (FMP) model, typically fail on\nchallenging animal poses. We argue that beyond encoding appearance and spatial\nrelations, shape is needed to overcome the lack of distinctive landmarks on\nlaboratory animal bodies. In our approach, a shape consistent FMP (scFMP) model\ncomputes promising pose candidates after a standard FMP model is used to\nrapidly discard false part detections. This \"cascaded\" approach combines the\nrelative strengths of spatial-relations, appearance and shape representations\nand is shown to yield significant improvements over the original FMP model as\nwell as a representative deep neural network baseline.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 14:37:32 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Guo", "Yuliang", ""], ["Govindarajan", "Lakshmi Narasimhan", ""], ["Kimia", "Benjamin", ""], ["Serre", "Thomas", ""]]}, {"id": "1806.11036", "submitter": "Nicolas Brieu", "authors": "Ansh Kapil, Armin Meier, Aleksandra Zuraw, Keith Steele, Marlon\n  Rebelatto, G\\\"unter Schmidt, Nicolas Brieu", "title": "Deep Semi Supervised Generative Learning for Automated PD-L1 Tumor Cell\n  Scoring on NSCLC Tissue Needle Biopsies", "comments": "10 pages, 7 figures, 3 tables, submited to Scientific Reports on 28\n  June 2018", "journal-ref": "Scientific Reports, volume 8, Article number: 17343 (2018)", "doi": "10.1038/s41598-018-35501-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The level of PD-L1 expression in immunohistochemistry (IHC) assays is a key\nbiomarker for the identification of Non-Small-Cell-Lung-Cancer (NSCLC) patients\nthat may respond to anti PD-1/PD-L1 treatments. The quantification of PD-L1\nexpression currently includes the visual estimation of a Tumor Cell (TC) score\nby a pathologist and consists of evaluating the ratio of PD-L1 positive and\nPD-L1 negative tumor cells. Known challenges like differences in positivity\nestimation around clinically relevant cut-offs and sub-optimal quality of\nsamples makes visual scoring tedious and subjective, yielding a scoring\nvariability between pathologists. In this work, we propose a novel deep\nlearning solution that enables the first automated and objective scoring of\nPD-L1 expression in late stage NSCLC needle biopsies. To account for the low\namount of tissue available in biopsy images and to restrict the amount of\nmanual annotations necessary for training, we explore the use of\nsemi-supervised approaches against standard fully supervised methods. We\nconsolidate the manual annotations used for training as well the visual TC\nscores used for quantitative evaluation with multiple pathologists. Concordance\nmeasures computed on a set of slides unseen during training provide evidence\nthat our automatic scoring method matches visual scoring on the considered\ndataset while ensuring repeatability and objectivity.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 15:30:29 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Kapil", "Ansh", ""], ["Meier", "Armin", ""], ["Zuraw", "Aleksandra", ""], ["Steele", "Keith", ""], ["Rebelatto", "Marlon", ""], ["Schmidt", "G\u00fcnter", ""], ["Brieu", "Nicolas", ""]]}, {"id": "1806.11078", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, Zsolt Kira", "title": "A probabilistic constrained clustering for transfer learning and image\n  category discovery", "comments": "CVPR 2018 Deep-Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based clustering has recently gained popularity, and in\nparticular a constrained clustering formulation has been proposed to perform\ntransfer learning and image category discovery using deep learning. The core\nidea is to formulate a clustering objective with pairwise constraints that can\nbe used to train a deep clustering network; therefore the cluster assignments\nand their underlying feature representations are jointly optimized end-to-end.\nIn this work, we provide a novel clustering formulation to address scalability\nissues of previous work in terms of optimizing deeper networks and larger\namounts of categories. The proposed objective directly minimizes the negative\nlog-likelihood of cluster assignment with respect to the pairwise constraints,\nhas no hyper-parameters, and demonstrates improved scalability and performance\non both supervised learning and unsupervised transfer learning.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 16:49:19 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Lv", "Zhaoyang", ""], ["Schlosser", "Joel", ""], ["Odom", "Phillip", ""], ["Kira", "Zsolt", ""]]}, {"id": "1806.11137", "submitter": "Zhuo Zhao", "authors": "Zhuo Zhao, Lin Yang, Hao Zheng, Ian H. Guldner, Siyuan Zhang, and\n  Danny Z. Chen", "title": "Deep Learning Based Instance Segmentation in 3D Biomedical Images Using\n  Weak Annotation", "comments": "Accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Instance segmentation in 3D images is a fundamental task in biomedical image\nanalysis. While deep learning models often work well for 2D instance\nsegmentation, 3D instance segmentation still faces critical challenges, such as\ninsufficient training data due to various annotation difficulties in 3D\nbiomedical images. Common 3D annotation methods (e.g., full voxel annotation)\nincur high workloads and costs for labeling enough instances for training deep\nlearning 3D instance segmentation models. In this paper, we propose a new weak\nannotation approach for training a fast deep learning 3D instance segmentation\nmodel without using full voxel mask annotation. Our approach needs only 3D\nbounding boxes for all instances and full voxel annotation for a small fraction\nof the instances, and uses a novel two-stage 3D instance segmentation model\nutilizing these two kinds of annotation, respectively. We evaluate our approach\non several biomedical image datasets, and the experimental results show that\n(1) with full annotated boxes and a small amount of masks, our approach can\nachieve similar performance as the best known methods using full annotation,\nand (2) with similar annotation time, our approach outperforms the best known\nmethods that use full annotation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 18:22:52 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Zhao", "Zhuo", ""], ["Yang", "Lin", ""], ["Zheng", "Hao", ""], ["Guldner", "Ian H.", ""], ["Zhang", "Siyuan", ""], ["Chen", "Danny Z.", ""]]}, {"id": "1806.11146", "submitter": "Gamaleldin Elsayed", "authors": "Gamaleldin F. Elsayed, Ian Goodfellow, Jascha Sohl-Dickstein", "title": "Adversarial Reprogramming of Neural Networks", "comments": null, "journal-ref": "International Conference on Learning Representations 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are susceptible to \\emph{adversarial} attacks. In\ncomputer vision, well-crafted perturbations to images can cause neural networks\nto make mistakes such as confusing a cat with a computer. Previous adversarial\nattacks have been designed to degrade performance of models or cause machine\nlearning models to produce specific outputs chosen ahead of time by the\nattacker. We introduce attacks that instead {\\em reprogram} the target model to\nperform a task chosen by the attacker---without the attacker needing to specify\nor compute the desired output for each test-time input. This attack finds a\nsingle adversarial perturbation, that can be added to all test-time inputs to a\nmachine learning model in order to cause the model to perform a task chosen by\nthe adversary---even if the model was not trained to do this task. These\nperturbations can thus be considered a program for the new task. We demonstrate\nadversarial reprogramming on six ImageNet classification models, repurposing\nthese models to perform a counting task, as well as classification tasks:\nclassification of MNIST and CIFAR-10 examples presented as inputs to the\nImageNet model.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 19:06:26 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 22:50:01 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Elsayed", "Gamaleldin F.", ""], ["Goodfellow", "Ian", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1806.11169", "submitter": "Tilak Ratnanather", "authors": "J. Tilak Ratnanather, Sylvain Arguill\\`ere, Kwame S. Kutten, Peter\n  Hubka, Andrej Kral, Laurent Younes", "title": "3D Normal Coordinate Systems for Cortical Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A surface-based diffeomorphic algorithm to generate 3D coordinate grids in\nthe cortical ribbon is described. In the grid, normal coordinate lines are\ngenerated by the diffeomorphic evolution from the grey/white (inner) surface to\nthe grey/csf (outer) surface. Specifically, the cortical ribbon is described by\ntwo triangulated surfaces with open boundaries. Conceptually, the inner surface\nsits on top of the white matter structure and the outer on top of the gray\nmatter. It is assumed that the cortical ribbon consists of cortical columns\nwhich are orthogonal to the white matter surface. This might be viewed as a\nconsequence of the development of the columns in the embryo. It is also assumed\nthat the columns are orthogonal to the outer surface so that the resultant\nvector field is orthogonal to the evolving surface. Then the distance of the\nnormal lines from the vector field such that the inner surface evolves\ndiffeomorphically towards the outer one can be construed as a measure of\nthickness. Applications are described for the auditory cortices in human adults\nand cats with normal hearing or hearing loss. The approach offers great\npotential for cortical morphometry.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 20:16:57 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 15:35:57 GMT"}, {"version": "v3", "created": "Sat, 13 Jul 2019 16:04:29 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Ratnanather", "J. Tilak", ""], ["Arguill\u00e8re", "Sylvain", ""], ["Kutten", "Kwame S.", ""], ["Hubka", "Peter", ""], ["Kral", "Andrej", ""], ["Younes", "Laurent", ""]]}, {"id": "1806.11186", "submitter": "Thomas Tanay", "authors": "Thomas Tanay and Lewis D Griffin", "title": "A New Angle on L2 Regularization", "comments": "An explorable explanation on the phenomenon of adversarial examples\n  in linear classification and its relation to L2 regularization. Interactive\n  version available at: https://thomas-tanay.github.io/post--L2-regularization/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine two high-dimensional clusters and a hyperplane separating them.\nConsider in particular the angle between: the direction joining the two\nclusters' centroids and the normal to the hyperplane. In linear classification,\nthis angle depends on the level of L2 regularization used. Can you explain why?\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 20:57:40 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Tanay", "Thomas", ""], ["Griffin", "Lewis D", ""]]}, {"id": "1806.11191", "submitter": "Yu Tian", "authors": "Yu Tian (1), Xi Peng (1), Long Zhao (1), Shaoting Zhang (2), Dimitris\n  N. Metaxas (1) ((1) Rutgers University, (2) University of North Carolina at\n  Charlotte)", "title": "CR-GAN: Learning Complete Representations for Multi-view Generation", "comments": "7 pages, 9 figures, accepted by IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating multi-view images from a single-view input is an essential yet\nchallenging problem. It has broad applications in vision, graphics, and\nrobotics. Our study indicates that the widely-used generative adversarial\nnetwork (GAN) may learn \"incomplete\" representations due to the single-pathway\nframework: an encoder-decoder network followed by a discriminator network. We\npropose CR-GAN to address this problem. In addition to the single\nreconstruction path, we introduce a generation sideway to maintain the\ncompleteness of the learned embedding space. The two learning pathways\ncollaborate and compete in a parameter-sharing manner, yielding considerably\nimproved generalization ability to \"unseen\" dataset. More importantly, the\ntwo-pathway framework makes it possible to combine both labeled and unlabeled\ndata for self-supervised learning, which further enriches the embedding space\nfor realistic generations. The experimental results prove that CR-GAN\nsignificantly outperforms state-of-the-art methods, especially when generating\nfrom \"unseen\" inputs in wild conditions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 21:04:21 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Tian", "Yu", ""], ["Peng", "Xi", ""], ["Zhao", "Long", ""], ["Zhang", "Shaoting", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1806.11216", "submitter": "Maximilian Seitzer", "authors": "Maximilian Seitzer and Guang Yang and Jo Schlemper and Ozan Oktay and\n  Tobias W\\\"urfl and Vincent Christlein and Tom Wong and Raad Mohiaddin and\n  David Firmin and Jennifer Keegan and Daniel Rueckert and Andreas Maier", "title": "Adversarial and Perceptual Refinement for Compressed Sensing MRI\n  Reconstruction", "comments": "To be published at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches have shown promising performance for compressed\nsensing-based Magnetic Resonance Imaging. While deep neural networks trained\nwith mean squared error (MSE) loss functions can achieve high peak signal to\nnoise ratio, the reconstructed images are often blurry and lack sharp details,\nespecially for higher undersampling rates. Recently, adversarial and perceptual\nloss functions have been shown to achieve more visually appealing results.\nHowever, it remains an open question how to (1) optimally combine these loss\nfunctions with the MSE loss function and (2) evaluate such a perceptual\nenhancement. In this work, we propose a hybrid method, in which a visual\nrefinement component is learnt on top of an MSE loss-based reconstruction\nnetwork. In addition, we introduce a semantic interpretability score, measuring\nthe visibility of the region of interest in both ground truth and reconstructed\nimages, which allows us to objectively quantify the usefulness of the image\nquality for image post-processing and analysis. Applied on a large cardiac MRI\ndataset simulated with 8-fold undersampling, we demonstrate significant\nimprovements ($p<0.01$) over the state-of-the-art in both a human observer\nstudy and the semantic interpretability score.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 22:12:39 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Seitzer", "Maximilian", ""], ["Yang", "Guang", ""], ["Schlemper", "Jo", ""], ["Oktay", "Ozan", ""], ["W\u00fcrfl", "Tobias", ""], ["Christlein", "Vincent", ""], ["Wong", "Tom", ""], ["Mohiaddin", "Raad", ""], ["Firmin", "David", ""], ["Keegan", "Jennifer", ""], ["Rueckert", "Daniel", ""], ["Maier", "Andreas", ""]]}, {"id": "1806.11217", "submitter": "Sumedha Singla", "authors": "Sumedha Singla, Mingming Gong, Siamak Ravanbakhsh, Frank Sciurba,\n  Barnabas Poczos, and Kayhan N. Batmanghelich", "title": "Subject2Vec: Generative-Discriminative Approach from a Set of Image\n  Patches to a Vector", "comments": "MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an attention-based method that aggregates local image features to\na subject-level representation for predicting disease severity. In contrast to\nclassical deep learning that requires a fixed dimensional input, our method\noperates on a set of image patches; hence it can accommodate variable length\ninput image without image resizing. The model learns a clinically interpretable\nsubject-level representation that is reflective of the disease severity. Our\nmodel consists of three mutually dependent modules which regulate each other:\n(1) a discriminative network that learns a fixed-length representation from\nlocal features and maps them to disease severity; (2) an attention mechanism\nthat provides interpretability by focusing on the areas of the anatomy that\ncontribute the most to the prediction task; and (3) a generative network that\nencourages the diversity of the local latent features. The generative term\nensures that the attention weights are non-degenerate while maintaining the\nrelevance of the local regions to the disease severity. We train our model\nend-to-end in the context of a large-scale lung CT study of Chronic Obstructive\nPulmonary Disease (COPD). Our model gives state-of-the art performance in\npredicting clinical measures of severity for COPD. The distribution of the\nattention provides the regional relevance of lung tissue to the clinical\nmeasurements.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 22:12:56 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Singla", "Sumedha", ""], ["Gong", "Mingming", ""], ["Ravanbakhsh", "Siamak", ""], ["Sciurba", "Frank", ""], ["Poczos", "Barnabas", ""], ["Batmanghelich", "Kayhan N.", ""]]}, {"id": "1806.11223", "submitter": "Athanasios Tsiligkaridis", "authors": "Athanasios Tsiligkaridis and Theodoros Tsiligkaridis", "title": "Active query-driven visual search using probabilistic bisection and\n  convolutional neural networks", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel efficient object detection and localization framework\nbased on the probabilistic bisection algorithm. A Convolutional Neural Network\n(CNN) is trained and used as a noisy oracle that provides answers to input\nquery images. The responses along with error probability estimates obtained\nfrom the CNN are used to update beliefs on the object location along each\ndimension. We show that querying along each dimension achieves the same lower\nbound on localization error as the joint query design. Finally, we compare our\napproach to the traditional sliding window technique on a real world face\nlocalization task and show speed improvements by at least an order of magnitude\nwhile maintaining accurate localization.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 23:05:40 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 02:24:08 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 16:16:39 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Tsiligkaridis", "Athanasios", ""], ["Tsiligkaridis", "Theodoros", ""]]}, {"id": "1806.11226", "submitter": "Kamelia Aryafar", "authors": "Murium Iqbal, Adair Kovac, Kamelia Aryafar", "title": "A Multimodal Recommender System for Large-scale Assortment Generation in\n  E-commerce", "comments": "SIGIR eComm Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce platforms surface interesting products largely through product\nrecommendations that capture users' styles and aesthetic preferences. Curating\nrecommendations as a complete complementary set, or assortment, is critical for\na successful e-commerce experience, especially for product categories such as\nfurniture, where items are selected together with the overall theme, style or\nambiance of a space in mind. In this paper, we propose two visually-aware\nrecommender systems that can automatically curate an assortment of living room\nfurniture around a couple of pre-selected seed pieces for the room. The first\nsystem aims to maximize the visual-based style compatibility of the entire\nselection by making use of transfer learning and topic modeling. The second\nsystem extends the first by incorporating text data and applying polylingual\ntopic modeling to infer style over both modalities. We review the production\npipeline for surfacing these visually-aware recommender systems and compare\nthem through offline validations and large-scale online A/B tests on Overstock.\nOur experimental results show that complimentary style is best discovered over\nproduct sets when both visual and textual data are incorporated.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 23:11:54 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Iqbal", "Murium", ""], ["Kovac", "Adair", ""], ["Aryafar", "Kamelia", ""]]}, {"id": "1806.11230", "submitter": "Yu Kong", "authors": "Yu Kong, Yun Fu", "title": "Human Action Recognition and Prediction: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derived from rapid advances in computer vision and machine learning, video\nanalysis tasks have been moving from inferring the present state to predicting\nthe future state. Vision-based action recognition and prediction from videos\nare such tasks, where action recognition is to infer human actions (present\nstate) based upon complete action executions, and action prediction to predict\nhuman actions (future state) based upon incomplete action executions. These two\ntasks have become particularly prevalent topics recently because of their\nexplosively emerging real-world applications, such as visual surveillance,\nautonomous driving vehicle, entertainment, and video retrieval, etc. Many\nattempts have been devoted in the last a few decades in order to build a robust\nand effective framework for action recognition and prediction. In this paper,\nwe survey the complete state-of-the-art techniques in the action recognition\nand prediction. Existing models, popular algorithms, technical difficulties,\npopular action databases, evaluation protocols, and promising future directions\nare also provided with systematic discussions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 23:43:42 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 00:35:27 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kong", "Yu", ""], ["Fu", "Yun", ""]]}, {"id": "1806.11266", "submitter": "Md Amirul Islam", "authors": "Md Amirul Islam, Mrigank Rochan, Shujon Naha, Neil D. B. Bruce, and\n  Yang Wang", "title": "Gated Feedback Refinement Network for Coarse-to-Fine Dense Semantic\n  Image Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective integration of local and global contextual information is crucial\nfor semantic segmentation and dense image labeling. We develop two\nencoder-decoder based deep learning architectures to address this problem. We\nfirst propose a network architecture called Label Refinement Network (LRN) that\npredicts segmentation labels in a coarse-to-fine fashion at several spatial\nresolutions. In this network, we also define loss functions at several stages\nto provide supervision at different stages of training. However, there are\nlimits to the quality of refinement possible if ambiguous information is passed\nforward. In order to address this issue, we also propose Gated Feedback\nRefinement Network (G-FRNet) that addresses this limitation. Initially, G-FRNet\nmakes a coarse-grained prediction which it progressively refines to recover\ndetails by effectively integrating local and global contextual information\nduring the refinement stages. This is achieved by gate units proposed in this\nwork, that control information passed forward in order to resolve the\nambiguity. Experiments were conducted on four challenging dense labeling\ndatasets (CamVid, PASCAL VOC 2012, Horse-Cow Parsing, PASCAL-Person-Part, and\nSUN-RGBD). G-FRNet achieves state-of-the-art semantic segmentation results on\nthe CamVid and Horse-Cow Parsing datasets and produces results competitive with\nthe best performing approaches that appear in the literature for the other\nthree datasets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 04:55:45 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Islam", "Md Amirul", ""], ["Rochan", "Mrigank", ""], ["Naha", "Shujon", ""], ["Bruce", "Neil D. B.", ""], ["Wang", "Yang", ""]]}, {"id": "1806.11269", "submitter": "Jun Chen", "authors": "Yang Xiao, Jun Chen, Yancheng Wang, Zhiguo Cao, Joey Tianyi Zhou,\n  Xiang Bai", "title": "Action Recognition for Depth Video using Multi-view Dynamic Images", "comments": "accepted by Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic imaging is a recently proposed action description paradigm for\nsimultaneously capturing motion and temporal evolution information,\nparticularly in the context of deep convolutional neural networks (CNNs).\nCompared with optical flow for motion characterization, dynamic imaging\nexhibits superior efficiency and compactness. Inspired by the success of\ndynamic imaging in RGB video, this study extends it to the depth domain. To\nbetter exploit three-dimensional (3D) characteristics, multi-view dynamic\nimages are proposed. In particular, the raw depth video is densely projected\nwith respect to different virtual imaging viewpoints by rotating the virtual\ncamera within the 3D space. Subsequently, dynamic images are extracted from the\nobtained multi-view depth videos and multi-view dynamic images are thus\nconstructed from these images. Accordingly, more view-tolerant visual cues can\nbe involved. A novel CNN model is then proposed to perform feature learning on\nmulti-view dynamic images. Particularly, the dynamic images from different\nviews share the same convolutional layers but correspond to different fully\nconnected layers. This is aimed at enhancing the tuning effectiveness on\nshallow convolutional layers by alleviating the gradient vanishing problem.\nMoreover, as the spatial occurrence variation of the actions may impair the\nCNN, an action proposal approach is also put forth. In experiments, the\nproposed approach can achieve state-of-the-art performance on three challenging\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 05:27:26 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 10:40:40 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 16:21:49 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Xiao", "Yang", ""], ["Chen", "Jun", ""], ["Wang", "Yancheng", ""], ["Cao", "Zhiguo", ""], ["Zhou", "Joey Tianyi", ""], ["Bai", "Xiang", ""]]}, {"id": "1806.11306", "submitter": "Jian Xu", "authors": "Jian Xu, Chunheng Wang, Cunzhao Shi, and Baihua Xiao", "title": "Excavate Condition-invariant Space by Intrinsic Encoder", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the human, we can recognize the places across a wide range of changing\nenvironmental conditions such as those caused by weathers, seasons, and\nday-night cycles. We excavate and memorize the stable semantic structure of\ndifferent places and scenes. For example, we can recognize tree whether the\nbare tree in winter or lush tree in summer. Therefore, the intrinsic features\nthat are corresponding to specific semantic contents and condition-invariant of\nappearance changes can be employed to improve the performance of long-term\nplace recognition significantly.\n  In this paper, we propose a novel intrinsic encoder that excavates the\ncondition-invariant latent space of different places under drastic appearance\nchanges. Our method excavates the space of intrinsic structure and semantic\ninformation by proposed self-supervised encoder loss. Different from previous\nlearning based place recognition methods that need paired training data of each\nplace with appearance changes, we employ the weakly-supervised strategy to\nutilize unpaired set-based training data of different environmental conditions.\n  We conduct comprehensive experiments and show that our semi-supervised\nintrinsic encoder achieves remarkable performance for place recognition under\ndrastic appearance changes. The proposed intrinsic encoder outperforms the\nstate-of-the-art image-level place recognition methods on standard benchmark\nNordland.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 08:58:54 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 11:38:58 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 07:11:05 GMT"}, {"version": "v4", "created": "Fri, 29 Mar 2019 04:12:59 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Xu", "Jian", ""], ["Wang", "Chunheng", ""], ["Shi", "Cunzhao", ""], ["Xiao", "Baihua", ""]]}, {"id": "1806.11311", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Ke Sun", "title": "Guaranteed Deterministic Bounds on the Total Variation Distance between\n  Univariate Mixtures", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total variation distance is a core statistical distance between\nprobability measures that satisfies the metric axioms, with value always\nfalling in $[0,1]$. This distance plays a fundamental role in machine learning\nand signal processing: It is a member of the broader class of $f$-divergences,\nand it is related to the probability of error in Bayesian hypothesis testing.\nSince the total variation distance does not admit closed-form expressions for\nstatistical mixtures (like Gaussian mixture models), one often has to rely in\npractice on costly numerical integrations or on fast Monte Carlo approximations\nthat however do not guarantee deterministic lower and upper bounds. In this\nwork, we consider two methods for bounding the total variation of univariate\nmixture models: The first method is based on the information monotonicity\nproperty of the total variation to design guaranteed nested deterministic lower\nbounds. The second method relies on computing the geometric lower and upper\nenvelopes of weighted mixture components to derive deterministic bounds based\non density ratio. We demonstrate the tightness of our bounds in a series of\nexperiments on Gaussian, Gamma and Rayleigh mixture models.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 09:22:41 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Nielsen", "Frank", ""], ["Sun", "Ke", ""]]}, {"id": "1806.11314", "submitter": "Nevrez Imamoglu", "authors": "Nevrez Imamoglu, Yu Oishi, Xiaoqiang Zhang, Guanqun Ding, Yuming Fang,\n  Toru Kouyama, Ryosuke Nakamura", "title": "Hyperspectral Image Dataset for Benchmarking on Salient Object Detection", "comments": "3 pages, 3 figures. 2 tables, appeared in the Proceedings of the 10th\n  International Conference on Quality of Multimedia Experience (QoMEX 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many works have been done on salient object detection using supervised or\nunsupervised approaches on colour images. Recently, a few studies demonstrated\nthat efficient salient object detection can also be implemented by using\nspectral features in visible spectrum of hyperspectral images from natural\nscenes. However, these models on hyperspectral salient object detection were\ntested with a very few number of data selected from various online public\ndataset, which are not specifically created for object detection purposes.\nTherefore, here, we aim to contribute to the field by releasing a hyperspectral\nsalient object detection dataset with a collection of 60 hyperspectral images\nwith their respective ground-truth binary images and representative rendered\ncolour images (sRGB). We took several aspects in consideration during the data\ncollection such as variation in object size, number of objects,\nforeground-background contrast, object position on the image, and etc. Then, we\nprepared ground truth binary images for each hyperspectral data, where salient\nobjects are labelled on the images. Finally, we did performance evaluation\nusing Area Under Curve (AUC) metric on some existing hyperspectral saliency\ndetection models in literature.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 09:31:56 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 01:25:04 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Imamoglu", "Nevrez", ""], ["Oishi", "Yu", ""], ["Zhang", "Xiaoqiang", ""], ["Ding", "Guanqun", ""], ["Fang", "Yuming", ""], ["Kouyama", "Toru", ""], ["Nakamura", "Ryosuke", ""]]}, {"id": "1806.11328", "submitter": "Guilhem Ch\\'eron", "authors": "Guilhem Ch\\'eron, Jean-Baptiste Alayrac, Ivan Laptev, Cordelia Schmid", "title": "A flexible model for training action localization with varying levels of\n  supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal action detection in videos is typically addressed in a\nfully-supervised setup with manual annotation of training videos required at\nevery frame. Since such annotation is extremely tedious and prohibits\nscalability, there is a clear need to minimize the amount of manual\nsupervision. In this work we propose a unifying framework that can handle and\ncombine varying types of less-demanding weak supervision. Our model is based on\ndiscriminative clustering and integrates different types of supervision as\nconstraints on the optimization. We investigate applications of such a model to\ntraining setups with alternative supervisory signals ranging from video-level\nclass labels to the full per-frame annotation of action bounding boxes.\nExperiments on the challenging UCF101-24 and DALY datasets demonstrate\ncompetitive performance of our method at a fraction of supervision used by\nprevious methods. The flexibility of our model enables joint learning from data\nwith different levels of annotation. Experimental results demonstrate a\nsignificant gain by adding a few fully supervised examples to otherwise weakly\nlabeled videos.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 09:56:41 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 22:26:51 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Ch\u00e9ron", "Guilhem", ""], ["Alayrac", "Jean-Baptiste", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1806.11349", "submitter": "Richard Diehl Martinez", "authors": "Rooz Mahdavian, Richard Diehl Martinez", "title": "Ignition: An End-to-End Supervised Model for Training Simulated\n  Self-Driving Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Ignition: an end-to-end neural network architecture for training\nunconstrained self-driving vehicles in simulated environments. The model is a\nResNet-18 variant, which is fed in images from the front of a simulated F1 car,\nand outputs optimal labels for steering, throttle, braking. Importantly, we\nnever explicitly train the model to detect road features like the outline of a\ntrack or distance to other cars; instead, we illustrate that these latent\nfeatures can be automatically encapsulated by the network.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 10:48:33 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Mahdavian", "Rooz", ""], ["Martinez", "Richard Diehl", ""]]}, {"id": "1806.11368", "submitter": "Benjamin Kellenberger", "authors": "Benjamin Kellenberger, Diego Marcos, Devis Tuia", "title": "Detecting Mammals in UAV Images: Best Practices to address a\n  substantially Imbalanced Dataset with Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.rse.2018.06.028", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge over the number of animals in large wildlife reserves is a vital\nnecessity for park rangers in their efforts to protect endangered species.\nManual animal censuses are dangerous and expensive, hence Unmanned Aerial\nVehicles (UAVs) with consumer level digital cameras are becoming a popular\nalternative tool to estimate livestock. Several works have been proposed that\nsemi-automatically process UAV images to detect animals, of which some employ\nConvolutional Neural Networks (CNNs), a recent family of deep learning\nalgorithms that proved very effective in object detection in large datasets\nfrom computer vision. However, the majority of works related to wildlife\nfocuses only on small datasets (typically subsets of UAV campaigns), which\nmight be detrimental when presented with the sheer scale of real study areas\nfor large mammal census. Methods may yield thousands of false alarms in such\ncases. In this paper, we study how to scale CNNs to large wildlife census tasks\nand present a number of recommendations to train a CNN on a large UAV dataset.\nWe further introduce novel evaluation protocols that are tailored to censuses\nand model suitability for subsequent human verification of detections. Using\nour recommendations, we are able to train a CNN reducing the number of false\npositives by an order of magnitude compared to previous state-of-the-art.\nSetting the requirements at 90% recall, our CNN allows to reduce the amount of\ndata required for manual verification by three times, thus making it possible\nfor rangers to screen all the data acquired efficiently and to detect almost\nall animals in the reserve automatically.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 11:59:14 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Kellenberger", "Benjamin", ""], ["Marcos", "Diego", ""], ["Tuia", "Devis", ""]]}, {"id": "1806.11430", "submitter": "Matteo Poggi", "authors": "Matteo Poggi, Filippo Aleotti, Fabio Tosi, Stefano Mattoccia", "title": "Towards real-time unsupervised monocular depth estimation on CPU", "comments": "7 pages, 3 figures. Accepted to IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised depth estimation from a single image is a very attractive\ntechnique with several implications in robotic, autonomous navigation,\naugmented reality and so on. This topic represents a very challenging task and\nthe advent of deep learning enabled to tackle this problem with excellent\nresults. However, these architectures are extremely deep and complex. Thus,\nreal-time performance can be achieved only by leveraging power-hungry GPUs that\ndo not allow to infer depth maps in application fields characterized by\nlow-power constraints. To tackle this issue, in this paper we propose a novel\narchitecture capable to quickly infer an accurate depth map on a CPU, even of\nan embedded system, using a pyramid of features extracted from a single input\nimage. Similarly to state-of-the-art, we train our network in an unsupervised\nmanner casting depth estimation as an image reconstruction problem. Extensive\nexperimental results on the KITTI dataset show that compared to the top\nperforming approach our network has similar accuracy but a much lower\ncomplexity (about 6% of parameters) enabling to infer a depth map for a KITTI\nimage in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard\nCPU. Moreover, by trading accuracy for efficiency, our network allows to infer\nmaps at about 2 Hz and 40 Hz respectively, still being more accurate than most\nstate-of-the-art slower methods. To the best of our knowledge, it is the first\nmethod enabling such performance on CPUs paving the way for effective\ndeployment of unsupervised monocular depth estimation even on embedded systems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 14:18:24 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 14:09:15 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 10:31:36 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Poggi", "Matteo", ""], ["Aleotti", "Filippo", ""], ["Tosi", "Fabio", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "1806.11452", "submitter": "Dino Ienco", "authors": "Raffaele Gaetano, Dino Ienco, Kenji Ose, Remi Cresson", "title": "MRFusion: A Deep Learning architecture to fuse PAN and MS imagery for\n  land cover mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Earth Observation systems provide a multitude of heterogeneous\nremote sensing data. How to manage such richness leveraging its complementarity\nis a crucial chal- lenge in modern remote sensing analysis. Data Fusion\ntechniques deal with this point proposing method to combine and exploit\ncomplementarity among the different data sensors. Considering optical Very High\nSpatial Resolution (VHSR) images, satellites obtain both Multi Spectral (MS)\nand panchro- matic (PAN) images at different spatial resolution. VHSR images\nare extensively exploited to produce land cover maps to deal with agricultural,\necological, and socioeconomic issues as well as assessing ecosystem status,\nmonitoring biodiversity and provid- ing inputs to conceive food risk monitoring\nsystems. Common techniques to produce land cover maps from such VHSR images\ntypically opt for a prior pansharpening of the multi-resolution source for a\nfull resolution processing. Here, we propose a new deep learning architecture\nto jointly use PAN and MS imagery for a direct classification without any prior\nimage fusion or resampling process. By managing the spectral information at its\nnative spatial resolution, our method, named MRFusion, aims at avoiding the\npossible infor- mation loss induced by pansharpening or any other hand-crafted\npreprocessing. Moreover, the proposed architecture is suitably designed to\nlearn non-linear transformations of the sources with the explicit aim of taking\nas much as possible advantage of the complementarity of PAN and MS imagery.\nExperiments are carried out on two-real world scenarios depicting large areas\nwith different land cover characteristics. The characteristics of the proposed\nscenarios underline the applicability and the generality of our method in\noperational settings.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 14:43:48 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Gaetano", "Raffaele", ""], ["Ienco", "Dino", ""], ["Ose", "Kenji", ""], ["Cresson", "Remi", ""]]}, {"id": "1806.11468", "submitter": "Mehdi Faraji", "authors": "Mehdi Faraji, Anup Basu", "title": "Simplified Active Calibration", "comments": "22 pages, 8 figures, and 2 tables. Preprint submitted to Journal of\n  Image and Vision Computing. Journal version of arXiv:1806.03584", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new mathematical formulation to estimate the intrinsic\nparameters of a camera in active or robotic platforms. We show that the focal\nlengths can be estimated using only one point correspondence that relates\nimages taken before and after a degenerate rotation of the camera. The\nestimated focal lengths are then treated as known parameters to obtain a linear\nset of equations to calculate the principal point. Assuming that the principal\npoint is close to the image center, the accuracy of the linear equations are\nincreased by integrating the image center into the formulation. We extensively\nevaluate the formulations on a simulated camera, 3D scenes and real-world\nimages. Our error analysis over simulated and real images indicates that the\nproposed Simplified Active Calibration method estimates the parameters of a\ncamera with low error rates that can be used as an initial guess for further\nnon-linear refinement procedures. Simplified Active Calibration can be employed\nin real-time environments for automatic calibrations given the proposed\nclosed-form solutions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 15:17:32 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 19:23:02 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Faraji", "Mehdi", ""], ["Basu", "Anup", ""]]}, {"id": "1806.11475", "submitter": "Kuangyu Shi PhD", "authors": "Deepa Gunashekar, Sailesh Conjeti, Abhijit Guha Roy, Nassir Navab,\n  Kuangyu Shi", "title": "SynNet: Structure-Preserving Fully Convolutional Networks for Medical\n  Image Synthesis", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross modal image syntheses is gaining significant interests for its ability\nto estimate target images of a different modality from a given set of source\nimages,like estimating MR to MR, MR to CT, CT to PET etc, without the need for\nan actual acquisition.Though they show potential for applications in radiation\ntherapy planning,image super resolution, atlas construction, image segmentation\netc.The synthesis results are not as accurate as the actual acquisition.In this\npaper,we address the problem of multi modal image synthesis by proposing a\nfully convolutional deep learning architecture called the SynNet.We extend the\nproposed architecture for various input output configurations. And finally, we\npropose a structure preserving custom loss function for cross-modal image\nsynthesis.We validate the proposed SynNet and its extended framework on BRATS\ndataset with comparisons against three state-of-the art methods.And the results\nof the proposed custom loss function is validated against the traditional loss\nfunction used by the state-of-the-art methods for cross modal image synthesis.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 15:32:57 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Gunashekar", "Deepa", ""], ["Conjeti", "Sailesh", ""], ["Roy", "Abhijit Guha", ""], ["Navab", "Nassir", ""], ["Shi", "Kuangyu", ""]]}, {"id": "1806.11496", "submitter": "Ardymulya Iswardani Mr", "authors": "Ardymulya Iswardani and Wahyu Hidayat", "title": "Mammographic Image Enhancement using Digital Image Processing Technique", "comments": "3 tables, 5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": "Vol. 16 No. 5", "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Abstract PURPOSES this study aims to perform microcalsification detection by\nperforming image enhancement in mammography image by using transformation of\nnegative image and histogram equalization. image mammography with .pgm format\nchanged to. jpg format then processed into negative image result then processed\nagain using histogram equalization. the results of the image enhancement\nprocess using negative image techniques and equalization histograms are\ncompared and validated with MSE and PSNR on each mammographic image.\nCONCLUSION: Image enhancement process on mammography image can be done, however\nthere are only some image that have improved quality, this affected by\nthreshold usage, which have important role to get better visualization on\nmammographic image. Keywords-component; Image enhancement, image negative,\nhistogram equalization, mammographic, breast cancer\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 15:54:51 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Iswardani", "Ardymulya", ""], ["Hidayat", "Wahyu", ""]]}, {"id": "1806.11517", "submitter": "Pawan Kumar Singh", "authors": "Pawan Kumar Singh, Supratim Das, Ram Sarkar, Mita Nasipuri", "title": "Recognition of Offline Handwritten Devanagari Numerals using Regional\n  Weighted Run Length Features", "comments": "6 pages", "journal-ref": "1st IEEE International Conference on Computer, Electrical and\n  Communication Engineering (ICCECE 2016)", "doi": "10.1109/ICCECE.2016.8009567", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognition of handwritten Roman characters and numerals has been extensively\nstudied in the last few decades and its accuracy reached to a satisfactory\nstate. But the same cannot be said while talking about the Devanagari script\nwhich is one of most popular script in India. This paper proposes an efficient\ndigit recognition system for handwritten Devanagari script. The system uses a\nnovel 196-element Mask Oriented Directional (MOD) features for the recognition\npurpose. The methodology is tested using five conventional classifiers on 6000\nhandwritten digit samples. On applying 3-fold cross-validation scheme, the\nproposed system yields the highest recognition accuracy of 95.02% using Support\nVector Machine (SVM) classifier.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 16:20:31 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Singh", "Pawan Kumar", ""], ["Das", "Supratim", ""], ["Sarkar", "Ram", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1806.11530", "submitter": "Markus D. Solbach", "authors": "John K. Tsotsos, Iuliia Kotseruba, Amir Rasouli, Markus D. Solbach", "title": "Visual Attention and its Intimate Links to Spatial Cognition", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is almost universal to regard attention as the facility that permits an\nagent, human or machine, to give priority processing resources to relevant\nstimuli while ignoring the irrelevant. The reality of how this might manifest\nitself throughout all the forms of perceptual and cognitive processes possessed\nby humans, however, is not as clear. Here we examine this reality with a broad\nperspective in order to highlight the myriad ways that attentional processes\nimpact both perception and cognition. The paper concludes by showing two real\nworld problems that exhibit sufficient complexity to illustrate the ways in\nwhich attention and cognition connect. These then point to new avenues of\nresearch that might illuminate the overall cognitive architecture of spatial\ncognition.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 16:50:47 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Tsotsos", "John K.", ""], ["Kotseruba", "Iuliia", ""], ["Rasouli", "Amir", ""], ["Solbach", "Markus D.", ""]]}, {"id": "1806.11534", "submitter": "Davi Frossard", "authors": "Davi Frossard and Raquel Urtasun", "title": "End-to-end Learning of Multi-sensor 3D Tracking by Detection", "comments": "Presented at IEEE International Conference on Robotics and Automation\n  (ICRA), 2018", "journal-ref": "In 2018 IEEE International Conference on Robotics and Automation\n  (ICRA), pp. 635-642. IEEE, 2018", "doi": "10.1109/ICRA.2018.8462884", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach to tracking by detection that can\nexploit both cameras as well as LIDAR data to produce very accurate 3D\ntrajectories. Towards this goal, we formulate the problem as a linear program\nthat can be solved exactly, and learn convolutional networks for detection as\nwell as matching in an end-to-end manner. We evaluate our model in the\nchallenging KITTI dataset and show very competitive results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 17:02:00 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Frossard", "Davi", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1806.11538", "submitter": "Yikang Li", "authors": "Yikang Li, Wanli Ouyang, Bolei Zhou, Jianping Shi, Chao Zhang,\n  Xiaogang Wang", "title": "Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph\n  Generation", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating scene graph to describe all the relations inside an image gains\nincreasing interests these years. However, most of the previous methods use\ncomplicated structures with slow inference speed or rely on the external data,\nwhich limits the usage of the model in real-life scenarios. To improve the\nefficiency of scene graph generation, we propose a subgraph-based connection\ngraph to concisely represent the scene graph during the inference. A bottom-up\nclustering method is first used to factorize the entire scene graph into\nsubgraphs, where each subgraph contains several objects and a subset of their\nrelationships. By replacing the numerous relationship representations of the\nscene graph with fewer subgraph and object features, the computation in the\nintermediate stage is significantly reduced. In addition, spatial information\nis maintained by the subgraph features, which is leveraged by our proposed\nSpatial-weighted Message Passing~(SMP) structure and Spatial-sensitive Relation\nInference~(SRI) module to facilitate the relationship recognition. On the\nrecent Visual Relationship Detection and Visual Genome datasets, our method\noutperforms the state-of-the-art method in both accuracy and speed.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 17:10:58 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 17:06:21 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Yikang", ""], ["Ouyang", "Wanli", ""], ["Zhou", "Bolei", ""], ["Shi", "Jianping", ""], ["Zhang", "Chao", ""], ["Wang", "Xiaogang", ""]]}]