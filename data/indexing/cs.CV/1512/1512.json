[{"id": "1512.00101", "submitter": "Miao Yu", "authors": "Miao Yu, Shuhan Shen, Zhanyi Hu", "title": "Dynamic Parallel and Distributed Graph Cuts", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2609819", "report-no": null, "categories": "cs.DS cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Graph-cuts are widely used in computer vision. In order to speed up the\noptimization process and improve the scalability for large graphs, Strandmark\nand Kahl introduced a splitting method to split a graph into multiple subgraphs\nfor parallel computation in both shared and distributed memory models. However,\nthis parallel algorithm (parallel BK-algorithm) does not have a polynomial\nbound on the number of iterations and is found non-convergent in some cases due\nto the possible multiple optimal solutions of its sub-problems.\n  To remedy this non-convergence problem, in this work we first introduce a\nmerging method capable of merging any number of those adjacent sub-graphs which\ncould hardly reach an agreement on their overlapped region in the parallel BK\nalgorithm. Based on the pseudo-boolean representations of graph cuts,our\nmerging method is shown able to effectively reuse all the computed flows in\nthese sub-graphs. Through both the splitting and merging, we further propose a\ndynamic parallel and distributed graph-cuts algorithm with guaranteed\nconvergence to the globally optimal solutions within a predefined number of\niterations. In essence, this work provides a general framework to allow more\nsophisticated splitting and merging strategies to be employed to further boost\nperformance. Our dynamic parallel algorithm is validated with extensive\nexperimental results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 00:19:50 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 00:31:20 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Yu", "Miao", ""], ["Shen", "Shuhan", ""], ["Hu", "Zhanyi", ""]]}, {"id": "1512.00130", "submitter": "Tyng-Luh Liu", "authors": "Tsung-Yu Lin, Tsung-Wei Ke, Tyng-Luh Liu", "title": "Implicit Sparse Code Hashing", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of converting large-scale high-dimensional image data\ninto binary codes so that approximate nearest-neighbor search over them can be\nefficiently performed. Different from most of the existing unsupervised\napproaches for yielding binary codes, our method is based on a\ndimensionality-reduction criterion that its resulting mapping is designed to\npreserve the image relationships entailed by the inner products of sparse\ncodes, rather than those implied by the Euclidean distances in the ambient\nspace. While the proposed formulation does not require computing any sparse\ncodes, the underlying computation model still inevitably involves solving an\nunmanageable eigenproblem when extremely high-dimensional descriptors are used.\nTo overcome the difficulty, we consider the column-sampling technique and\npresume a special form of rotation matrix to facilitate subproblem\ndecomposition. We test our method on several challenging image datasets and\ndemonstrate its effectiveness by comparing with state-of-the-art binary coding\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 03:12:09 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Lin", "Tsung-Yu", ""], ["Ke", "Tsung-Wei", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1512.00172", "submitter": "Sebastian Bach", "authors": "Sebastian Bach, Alexander Binder, Gr\\'egoire Montavon, Klaus-Robert\n  M\\\"uller, Wojciech Samek", "title": "Analyzing Classifiers: Fisher Vectors and Deep Neural Networks", "comments": "17 pages (10 main document + references , 7 appendix) 1 Table 7\n  Figures 1 Algorithm submitted to CVPR on 06/11/2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher Vector classifiers and Deep Neural Networks (DNNs) are popular and\nsuccessful algorithms for solving image classification problems. However, both\nare generally considered `black box' predictors as the non-linear\ntransformations involved have so far prevented transparent and interpretable\nreasoning. Recently, a principled technique, Layer-wise Relevance Propagation\n(LRP), has been developed in order to better comprehend the inherent structured\nreasoning of complex nonlinear classification models such as Bag of Feature\nmodels or DNNs. In this paper we (1) extend the LRP framework also for Fisher\nVector classifiers and then use it as analysis tool to (2) quantify the\nimportance of context for classification, (3) qualitatively compare DNNs\nagainst FV classifiers in terms of important image regions and (4) detect\npotential flaws and biases in data. All experiments are performed on the PASCAL\nVOC 2007 data set.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 08:14:11 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Bach", "Sebastian", ""], ["Binder", "Alexander", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1512.00237", "submitter": "Dongsheng An", "authors": "Dongsheng An, Jinli Suo, Xiangyang Ji, Haoqian Wang and Qionghai Dai", "title": "Fast and High Quality Highlight Removal from A Single Image", "comments": "11 pages, 10 figures, submitted to IEEE TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specular reflection exists widely in photography and causes the recorded\ncolor deviating from its true value, so fast and high quality highlight removal\nfrom a single nature image is of great importance. In spite of the progress in\nthe past decades in highlight removal, achieving wide applicability to the\nlarge diversity of nature scenes is quite challenging. To handle this problem,\nwe propose an analytic solution to highlight removal based on an L2\nchromaticity definition and corresponding dichromatic model. Specifically, this\npaper derives a normalized dichromatic model for the pixels with identical\ndiffuse color: a unit circle equation of projection coefficients in two\nsubspaces that are orthogonal to and parallel with the illumination,\nrespectively. In the former illumination orthogonal subspace, which is\nspecular-free, we can conduct robust clustering with an explicit criterion to\ndetermine the cluster number adaptively. In the latter illumination parallel\nsubspace, a property called pure diffuse pixels distribution rule (PDDR) helps\nmap each specular-influenced pixel to its diffuse component. In terms of\nefficiency, the proposed approach involves few complex calculation, and thus\ncan remove highlight from high resolution images fast. Experiments show that\nthis method is of superior performance in various challenging cases.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 12:21:49 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["An", "Dongsheng", ""], ["Suo", "Jinli", ""], ["Ji", "Xiangyang", ""], ["Wang", "Haoqian", ""], ["Dai", "Qionghai", ""]]}, {"id": "1512.00242", "submitter": "Haibing Wu", "authors": "Haibing Wu and Xiaodong Gu", "title": "Towards Dropout Training for Convolutional Neural Networks", "comments": "This paper has been published in Neural Networks,\n  http://www.sciencedirect.com/science/article/pii/S0893608015001446", "journal-ref": "Neural Networks 71: 1-10 (2015)", "doi": "10.1016/j.neunet.2015.07.007", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, dropout has seen increasing use in deep learning. For deep\nconvolutional neural networks, dropout is known to work well in fully-connected\nlayers. However, its effect in convolutional and pooling layers is still not\nclear. This paper demonstrates that max-pooling dropout is equivalent to\nrandomly picking activation based on a multinomial distribution at training\ntime. In light of this insight, we advocate employing our proposed\nprobabilistic weighted pooling, instead of commonly used max-pooling, to act as\nmodel averaging at test time. Empirical evidence validates the superiority of\nprobabilistic weighted pooling. We also empirically show that the effect of\nconvolutional dropout is not trivial, despite the dramatically reduced\npossibility of over-fitting due to the convolutional architecture. Elaborately\ndesigning dropout training simultaneously in max-pooling and fully-connected\nlayers, we achieve state-of-the-art performance on MNIST, and very competitive\nresults on CIFAR-10 and CIFAR-100, relative to other approaches without data\naugmentation. Finally, we compare max-pooling dropout and stochastic pooling,\nboth of which introduce stochasticity based on multinomial distributions at\npooling stage.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 12:46:11 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Wu", "Haibing", ""], ["Gu", "Xiaodong", ""]]}, {"id": "1512.00298", "submitter": "Hendrik Dirks", "authors": "Martin Burger, Hendrik Dirks, Lena Frerking", "title": "On Optical Flow Models for Variational Motion Estimation", "comments": "27 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to discuss and evaluate total variation based\nregularization methods for motion estimation, with particular focus on optical\nflow models. In addition to standard $L^2$ and $L^1$ data fidelities we give an\noverview of different variants of total variation regularization obtained from\ncombination with higher order models and a unified computational optimization\napproach based on primal-dual methods. Moreover, we extend the models by\nBregman iterations and provide an inverse problems perspective to the analysis\nof variational optical flow models. A particular focus of the paper is the\nquantitative evaluation of motion estimation, which is a difficult and often\nunderestimated task. We discuss several approaches for quality measures of\nmotion estimation and apply them to compare the previously discussed\nregularization approaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 15:37:19 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Burger", "Martin", ""], ["Dirks", "Hendrik", ""], ["Frerking", "Lena", ""]]}, {"id": "1512.00389", "submitter": "Alexander Malyshev", "authors": "Andrew Knyazev and Alexander Malyshev", "title": "Accelerated graph-based nonlinear denoising filters", "comments": "10 pages, 6 figures, to appear in Procedia Computer Science, vol.80,\n  2016, International Conference on Computational Science, San Diego, CA, USA,\n  June 6-8, 2016", "journal-ref": "Procedia Computer Science Volume 80, 2016, Pages 607-616,\n  International Conference on Computational Science 2016, ICCS 2016, 6-8 June\n  2016, San Diego, California, USA", "doi": "10.1016/j.procs.2016.05.348", "report-no": "MERL TR2016-068", "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising filters, such as bilateral, guided, and total variation filters,\napplied to images on general graphs may require repeated application if noise\nis not small enough. We formulate two acceleration techniques of the resulted\niterations: conjugate gradient method and Nesterov's acceleration. We\nnumerically show efficiency of the accelerated nonlinear filters for image\ndenoising and demonstrate 2-12 times speed-up, i.e., the acceleration\ntechniques reduce the number of iterations required to reach a given peak\nsignal-to-noise ratio (PSNR) by the above indicated factor of 2-12.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 18:54:19 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 20:00:49 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Knyazev", "Andrew", ""], ["Malyshev", "Alexander", ""]]}, {"id": "1512.00486", "submitter": "Maksim Lapin", "authors": "Maksim Lapin, Matthias Hein, Bernt Schiele", "title": "Loss Functions for Top-k Error: Analysis and Insights", "comments": "In Computer Vision and Pattern Recognition (CVPR), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to push the performance on realistic computer vision tasks, the\nnumber of classes in modern benchmark datasets has significantly increased in\nrecent years. This increase in the number of classes comes along with increased\nambiguity between the class labels, raising the question if top-1 error is the\nright performance measure. In this paper, we provide an extensive comparison\nand evaluation of established multiclass methods comparing their top-k\nperformance both from a practical as well as from a theoretical perspective.\nMoreover, we introduce novel top-k loss functions as modifications of the\nsoftmax and the multiclass SVM losses and provide efficient optimization\nschemes for them. In the experiments, we compare on various datasets all of the\nproposed and established methods for top-k error optimization. An interesting\ninsight of this paper is that the softmax loss yields competitive top-k\nperformance for all k simultaneously. For a specific top-k error, our new top-k\nlosses lead typically to further improvements while being faster to train than\nthe softmax.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 21:22:35 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 15:12:01 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1512.00504", "submitter": "Jamie Schiel", "authors": "Jamie Schiel, Andrew Bainbridge-Smith", "title": "Efficient Edge Detection on Low-Cost FPGAs", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the efficiency of edge detection in embedded applications, such as\nUAV control, is critical for reducing system cost and power dissipation. Field\nprogrammable gate arrays (FPGA) are a good platform for making improvements\nbecause of their specialised internal structure. However, current FPGA edge\ndetectors do not exploit this structure well. A new edge detection architecture\nis proposed that is better optimised for FPGAs. The basis of the architecture\nis the Sobel edge kernels that are shown to be the most suitable because of\ntheir separability and absence of multiplications. Edge intensities are\ncalculated with a new 4:2 compressor that consists of two custom-designed 3:2\ncompressors. Addition speed is increased by breaking carry propagation chains\nwith look-ahead logic. Testing of the design showed it gives a 28% increase in\nspeed and 4.4% reduction in area over previous equivalent designs, which\ndemonstrated that it will lower the cost of edge detection systems, dissipate\nless power and still maintain high-speed control.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 22:32:21 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Schiel", "Jamie", ""], ["Bainbridge-Smith", "Andrew", ""]]}, {"id": "1512.00517", "submitter": "Marius Leordeanu", "authors": "Marius Leordeanu, Alexandra Radu, Shumeet Baluja and Rahul Sukthankar", "title": "Labeling the Features Not the Samples: Efficient Video Classification\n  with Minimal Supervision", "comments": "arXiv admin note: text overlap with arXiv:1411.7714", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is essential for effective visual recognition. We propose\nan efficient joint classifier learning and feature selection method that\ndiscovers sparse, compact representations of input features from a vast sea of\ncandidates, with an almost unsupervised formulation. Our method requires only\nthe following knowledge, which we call the \\emph{feature sign}---whether or not\na particular feature has on average stronger values over positive samples than\nover negatives. We show how this can be estimated using as few as a single\nlabeled training sample per class. Then, using these feature signs, we extend\nan initial supervised learning problem into an (almost) unsupervised clustering\nformulation that can incorporate new data without requiring ground truth\nlabels. Our method works both as a feature selection mechanism and as a fully\ncompetitive classifier. It has important properties, low computational cost and\nexcellent accuracy, especially in difficult cases of very limited training\ndata. We experiment on large-scale recognition in video and show superior speed\nand performance to established feature selection approaches such as AdaBoost,\nLasso, greedy forward-backward selection, and powerful classifiers such as SVM.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 23:24:12 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Leordeanu", "Marius", ""], ["Radu", "Alexandra", ""], ["Baluja", "Shumeet", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1512.00567", "submitter": "Christian Szegedy", "authors": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna", "title": "Rethinking the Inception Architecture for Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are at the core of most state-of-the-art computer\nvision solutions for a wide variety of tasks. Since 2014 very deep\nconvolutional networks started to become mainstream, yielding substantial gains\nin various benchmarks. Although increased model size and computational cost\ntend to translate to immediate quality gains for most tasks (as long as enough\nlabeled data is provided for training), computational efficiency and low\nparameter count are still enabling factors for various use cases such as mobile\nvision and big-data scenarios. Here we explore ways to scale up networks in\nways that aim at utilizing the added computation as efficiently as possible by\nsuitably factorized convolutions and aggressive regularization. We benchmark\nour methods on the ILSVRC 2012 classification challenge validation set\ndemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%\ntop-5 error for single frame evaluation using a network with a computational\ncost of 5 billion multiply-adds per inference and with using less than 25\nmillion parameters. With an ensemble of 4 models and multi-crop evaluation, we\nreport 3.5% top-5 error on the validation set (3.6% error on the test set) and\n17.3% top-1 error on the validation set.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 03:44:38 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 19:34:38 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2015 20:27:50 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Szegedy", "Christian", ""], ["Vanhoucke", "Vincent", ""], ["Ioffe", "Sergey", ""], ["Shlens", "Jonathon", ""], ["Wojna", "Zbigniew", ""]]}, {"id": "1512.00570", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Jimei Yang, Kihyuk Sohn, Honglak Lee", "title": "Attribute2Image: Conditional Image Generation from Visual Attributes", "comments": "19 pages, accepted by ECCV 2016, The 14th European Conference on\n  Computer Vision (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a novel problem of generating images from visual\nattributes. We model the image as a composite of foreground and background and\ndevelop a layered generative model with disentangled latent variables that can\nbe learned end-to-end using a variational auto-encoder. We experiment with\nnatural images of faces and birds and demonstrate that the proposed models are\ncapable of generating realistic and diverse samples with disentangled latent\nrepresentations. We use a general energy minimization algorithm for posterior\ninference of latent variables given novel images. Therefore, the learned\ngenerative models show excellent quantitative and visual results in the tasks\nof attribute-conditioned image reconstruction and completion.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 04:07:28 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 08:55:32 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Yan", "Xinchen", ""], ["Yang", "Jimei", ""], ["Sohn", "Kihyuk", ""], ["Lee", "Honglak", ""]]}, {"id": "1512.00596", "submitter": "Ira Kemelmacher-Shlizerman", "authors": "Ira Kemelmacher-Shlizerman and Steve Seitz and Daniel Miller and Evan\n  Brossard", "title": "The MegaFace Benchmark: 1 Million Faces for Recognition at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent face recognition experiments on a major benchmark LFW show stunning\nperformance--a number of algorithms achieve near to perfect score, surpassing\nhuman recognition rates. In this paper, we advocate evaluations at the million\nscale (LFW includes only 13K photos of 5K people). To this end, we have\nassembled the MegaFace dataset and created the first MegaFace challenge. Our\ndataset includes One Million photos that capture more than 690K different\nindividuals. The challenge evaluates performance of algorithms with increasing\nnumbers of distractors (going from 10 to 1M) in the gallery set. We present\nboth identification and verification performance, evaluate performance with\nrespect to pose and a person's age, and compare as a function of training data\nsize (number of photos and people). We report results of state of the art and\nbaseline algorithms. Our key observations are that testing at the million scale\nreveals big performance differences (of algorithms that perform similarly well\non smaller scale) and that age invariant recognition as well as pose are still\nchallenging for most. The MegaFace dataset, baseline code, and evaluation\nscripts, are all publicly released for further experimentations at:\nmegaface.cs.washington.edu.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 07:17:54 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Kemelmacher-Shlizerman", "Ira", ""], ["Seitz", "Steve", ""], ["Miller", "Daniel", ""], ["Brossard", "Evan", ""]]}, {"id": "1512.00607", "submitter": "Hideitsu Hino", "authors": "Toshiyuki Kato, Hideitsu Hino, and Noboru Murata", "title": "Double Sparse Multi-Frame Image Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of image super resolution algorithms based on the sparse\ncoding are proposed, and some algorithms realize the multi-frame super\nresolution. In multi-frame super resolution based on the sparse coding, both\naccurate image registration and sparse coding are required. Previous study on\nmulti-frame super resolution based on sparse coding firstly apply block\nmatching for image registration, followed by sparse coding to enhance the image\nresolution. In this paper, these two problems are solved by optimizing a single\nobjective function. The results of numerical experiments support the\neffectiveness of the proposed approch.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 08:25:23 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Kato", "Toshiyuki", ""], ["Hino", "Hideitsu", ""], ["Murata", "Noboru", ""]]}, {"id": "1512.00622", "submitter": "Ali Boyali", "authors": "Ali Boyali, Naohisa Hashimoto, Manolya Kavakli", "title": "Continuous and Simultaneous Gesture and Posture Recognition for\n  Commanding a Robotic Wheelchair; Towards Spotting the Signal Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spotting signal patterns with varying lengths has been still an open problem\nin the literature. In this study, we describe a signal pattern recognition\napproach for continuous and simultaneous classification of a tracked hand's\nposture and gestures and map them to steering commands for control of a robotic\nwheelchair. The developed methodology not only affords 100\\% recognition\naccuracy on a streaming signal for continuous recognition, but also brings\nabout a new perspective for building a training dictionary which eliminates\nhuman intervention to spot the gesture or postures on a training signal. In the\ntraining phase we employ a state of art subspace clustering method to find the\nmost representative state samples. The recognition and training framework\nreveal boundaries of the patterns on the streaming signal with a successive\ndecision tree structure intrinsically. We make use of the Collaborative ans\nBlock Sparse Representation based classification methods for continuous gesture\nand posture recognition.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 09:25:52 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Boyali", "Ali", ""], ["Hashimoto", "Naohisa", ""], ["Kavakli", "Manolya", ""]]}, {"id": "1512.00717", "submitter": "Stanislav Pyatykh", "authors": "Stanislav Pyatykh and J\\\"urgen Hesser", "title": "MMSE Estimation for Poisson Noise Removal in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poisson noise suppression is an important preprocessing step in several\napplications, such as medical imaging, microscopy, and astronomical imaging. In\nthis work, we propose a novel patch-wise Poisson noise removal strategy, in\nwhich the MMSE estimator is utilized in order to produce the denoising result\nfor each image patch. Fast and accurate computation of the MMSE estimator is\ncarried out using k-d tree search followed by search in the K-nearest neighbor\ngraph. Our experiments show that the proposed method is the preferable choice\nfor low signal-to-noise ratios.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 14:49:12 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Pyatykh", "Stanislav", ""], ["Hesser", "J\u00fcrgen", ""]]}, {"id": "1512.00743", "submitter": "Amogh Gudi", "authors": "Amogh Gudi", "title": "Recognizing Semantic Features in Faces using Deep Learning", "comments": "Thesis, M.Sc. Artificial Intelligence, University of Amsterdam, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human face constantly conveys information, both consciously and\nsubconsciously. However, as basic as it is for humans to visually interpret\nthis information, it is quite a big challenge for machines. Conventional\nsemantic facial feature recognition and analysis techniques are already in use\nand are based on physiological heuristics, but they suffer from lack of\nrobustness and high computation time. This thesis aims to explore ways for\nmachines to learn to interpret semantic information available in faces in an\nautomated manner without requiring manual design of feature detectors, using\nthe approach of Deep Learning. This thesis provides a study of the effects of\nvarious factors and hyper-parameters of deep neural networks in the process of\ndetermining an optimal network configuration for the task of semantic facial\nfeature recognition. This thesis explores the effectiveness of the system to\nrecognize the various semantic features (like emotions, age, gender, ethnicity\netc.) present in faces. Furthermore, the relation between the effect of\nhigh-level concepts on low level features is explored through an analysis of\nthe similarities in low-level descriptors of different semantic features. This\nthesis also demonstrates a novel idea of using a deep network to generate 3-D\nActive Appearance Models of faces from real-world 2-D images.\n  For a more detailed report on this work, please see [arXiv:1512.00743v1].\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 15:46:26 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 13:33:44 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Gudi", "Amogh", ""]]}, {"id": "1512.00747", "submitter": "Agata Mosinska", "authors": "Agata Mosinska, Raphael Sznitman, Przemys{\\l}aw G{\\l}owacki, Pascal\n  Fua", "title": "Active Learning for Delineation of Curvilinear Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent delineation techniques owe much of their increased effectiveness\nto path classification algorithms that make it possible to distinguish\npromising paths from others. The downside of this development is that they\nrequire annotated training data, which is tedious to produce.\n  In this paper, we propose an Active Learning approach that considerably\nspeeds up the annotation process. Unlike standard ones, it takes advantage of\nthe specificities of the delineation problem. It operates on a graph and can\nreduce the training set size by up to 80% without compromising the\nreconstruction quality.\n  We will show that our approach outperforms conventional ones on various\nbiomedical and natural image datasets, thus showing that it is broadly\napplicable.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 15:57:59 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Mosinska", "Agata", ""], ["Sznitman", "Raphael", ""], ["G\u0142owacki", "Przemys\u0142aw", ""], ["Fua", "Pascal", ""]]}, {"id": "1512.00795", "submitter": "Xiaolong Wang", "authors": "Xiaolong Wang, Ali Farhadi, Abhinav Gupta", "title": "Actions ~ Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What defines an action like \"kicking ball\"? We argue that the true meaning of\nan action lies in the change or transformation an action brings to the\nenvironment. In this paper, we propose a novel representation for actions by\nmodeling an action as a transformation which changes the state of the\nenvironment before the action happens (precondition) to the state after the\naction (effect). Motivated by recent advancements of video representation using\ndeep learning, we design a Siamese network which models the action as a\ntransformation on a high-level feature space. We show that our model gives\nimprovements on standard action recognition datasets including UCF101 and\nHMDB51. More importantly, our approach is able to generalize beyond learned\naction categories and shows significant performance improvement on\ncross-category generalization on our new ACT dataset.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 18:17:32 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 04:51:49 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Wang", "Xiaolong", ""], ["Farhadi", "Ali", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1512.00818", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet Sawhney, Ahmed\n  Elgammal", "title": "Zero-Shot Event Detection by Multimodal Distributional Semantic\n  Embedding of Videos", "comments": "To appear in AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new zero-shot Event Detection method by Multi-modal\nDistributional Semantic embedding of videos. Our model embeds object and action\nconcepts as well as other available modalities from videos into a\ndistributional semantic space. To our knowledge, this is the first Zero-Shot\nevent detection model that is built on top of distributional semantics and\nextends it in the following directions: (a) semantic embedding of multimodal\ninformation in videos (with focus on the visual modalities), (b) automatically\ndetermining relevance of concepts/attributes to a free text query, which could\nbe useful for other applications, and (c) retrieving videos by free text event\nquery (e.g., \"changing a vehicle tire\") based on their content. We embed videos\ninto a distributional semantic space and then measure the similarity between\nvideos and the event query in a free text form. We validated our method on the\nlarge TRECVID MED (Multimedia Event Detection) challenge. Using only the event\ntitle as a query, our method outperformed the state-of-the-art that uses big\ndescriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC\nmetric. It is also an order of magnitude faster.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 19:34:00 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 00:58:49 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Liu", "Jingen", ""], ["Cheng", "Hui", ""], ["Sawhney", "Harpreet", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1512.00901", "submitter": "Mingrui Yang", "authors": "Mingrui Yang, Frank de Hoog, Yuqi Fan, and Wen Hu", "title": "Compressive hyperspectral imaging via adaptive sampling and dictionary\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new sampling strategy for hyperspectral signals\nthat is based on dictionary learning and singular value decomposition (SVD).\nSpecifically, we first learn a sparsifying dictionary from training spectral\ndata using dictionary learning. We then perform an SVD on the dictionary and\nuse the first few left singular vectors as the rows of the measurement matrix\nto obtain the compressive measurements for reconstruction. The proposed method\nprovides significant improvement over the conventional compressive sensing\napproaches. The reconstruction performance is further improved by\nreconditioning the sensing matrix using matrix balancing. We also demonstrate\nthat the combination of dictionary learning and SVD is robust by applying them\nto different datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 23:13:04 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Yang", "Mingrui", ""], ["de Hoog", "Frank", ""], ["Fan", "Yuqi", ""], ["Hu", "Wen", ""]]}, {"id": "1512.00907", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Innovation Pursuit: A New Approach to Subspace Clustering", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 65, Issue: 23,\n  Dec.1, 1 2017 )", "doi": "10.1109/TSP.2017.2749206", "report-no": null, "categories": "cs.CV cs.IR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In subspace clustering, a group of data points belonging to a union of\nsubspaces are assigned membership to their respective subspaces. This paper\npresents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of\nsubspace clustering using a new geometrical idea whereby subspaces are\nidentified based on their relative novelties. We present two frameworks in\nwhich the idea of innovation pursuit is used to distinguish the subspaces.\nUnderlying the first framework is an iterative method that finds the subspaces\nconsecutively by solving a series of simple linear optimization problems, each\nsearching for a direction of innovation in the span of the data potentially\northogonal to all subspaces except for the one to be identified in one step of\nthe algorithm. A detailed mathematical analysis is provided establishing\nsufficient conditions for iPursuit to correctly cluster the data. The proposed\napproach can provably yield exact clustering even when the subspaces have\nsignificant intersections. It is shown that the complexity of the iterative\napproach scales only linearly in the number of data points and subspaces, and\nquadratically in the dimension of the subspaces. The second framework\nintegrates iPursuit with spectral clustering to yield a new variant of\nspectral-clustering-based algorithms. The numerical simulations with both real\nand synthetic data demonstrate that iPursuit can often outperform the\nstate-of-the-art subspace clustering algorithms, more so for subspaces with\nsignificant intersections, and that it significantly improves the\nstate-of-the-art result for subspace-segmentation-based face clustering.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 23:52:43 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 05:26:58 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 23:12:17 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 03:29:06 GMT"}, {"version": "v5", "created": "Sun, 26 Nov 2017 15:24:33 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1512.00932", "submitter": "S L Happy", "authors": "S L Happy, Priyadarshi Patnaik, Aurobinda Routray, and Rajlakshmi Guha", "title": "The Indian Spontaneous Expression Database for Emotion Recognition", "comments": "in IEEE Transactions on Affective Computing, 2016", "journal-ref": null, "doi": "10.1109/TAFFC.2015.2498174", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of spontaneous facial expressions is a major challenge\nin the field of affective computing. Head rotation, face pose, illumination\nvariation, occlusion etc. are the attributes that increase the complexity of\nrecognition of spontaneous expressions in practical applications. Effective\nrecognition of expressions depends significantly on the quality of the database\nused. Most well-known facial expression databases consist of posed expressions.\nHowever, currently there is a huge demand for spontaneous expression databases\nfor the pragmatic implementation of the facial expression recognition\nalgorithms. In this paper, we propose and establish a new facial expression\ndatabase containing spontaneous expressions of both male and female\nparticipants of Indian origin. The database consists of 428 segmented video\nclips of the spontaneous facial expressions of 50 participants. In our\nexperiment, emotions were induced among the participants by using emotional\nvideos and simultaneously their self-ratings were collected for each\nexperienced emotion. Facial expression clips were annotated carefully by four\ntrained decoders, which were further validated by the nature of stimuli used\nand self-report of emotions. An extensive analysis was carried out on the\ndatabase using several machine learning algorithms and the results are provided\nfor future reference. Such a spontaneous database will help in the development\nand validation of algorithms for recognition of spontaneous expressions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 02:51:08 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 02:01:16 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Happy", "S L", ""], ["Patnaik", "Priyadarshi", ""], ["Routray", "Aurobinda", ""], ["Guha", "Rajlakshmi", ""]]}, {"id": "1512.00939", "submitter": "Deepika Banchhor", "authors": "Siddharth Choubey, Deepika Banchhor", "title": "A Literature Survey of various Fingerprint De-noising Techniques to\n  justify the need of a new De-noising model based upon Pixel Component\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Preprocessing is a vital step in the field of image processing for\nbiometric pattern recognition. This paper studies and reviews various classical\nand modern fingerprint image de-noising models. The various model used for\nde-noising ranges widely from transform matrix using frequency, histogram model\nde-noising, de-noising by introducing Gabor filter and its types to enhance\nfingerprint images.\n  The output efficiency of various de-noising model proposed earlier is\ncalculated on the basis of SNR (signal to noise ratio) and MSE (mean square\nerror rate). Our simulated experimental results indicates that incorporating\nthe de-noising model based on Gabor filter inside domain of wavelet ranges with\ncomposite method only betters MSE (Mean Square Error). Improved MSE without\nsignificant improvement in SNR improves the fingerprint images only by a little\nmargin which is non-optimal in nature. Thus the objective of this research\npaper is to build an optimal de-noising model for fingerprint images so that\nits usage in biometric authentication can be more robust in nature.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 04:13:38 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Choubey", "Siddharth", ""], ["Banchhor", "Deepika", ""]]}, {"id": "1512.01003", "submitter": "Yuan Xie", "authors": "Yuan Xie and Shuhang Gu and Yan Liu and Wangmeng Zuo and Wensheng\n  Zhang and Lei Zhang", "title": "Weighted Schatten $p$-Norm Minimization for Image Denoising and\n  Background Subtraction", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TIP.2016.2599290", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank matrix approximation (LRMA), which aims to recover the underlying\nlow rank matrix from its degraded observation, has a wide range of applications\nin computer vision. The latest LRMA methods resort to using the nuclear norm\nminimization (NNM) as a convex relaxation of the nonconvex rank minimization.\nHowever, NNM tends to over-shrink the rank components and treats the different\nrank components equally, limiting its flexibility in practical applications. We\npropose a more flexible model, namely the Weighted Schatten $p$-Norm\nMinimization (WSNM), to generalize the NNM to the Schatten $p$-norm\nminimization with weights assigned to different singular values. The proposed\nWSNM not only gives better approximation to the original low-rank assumption,\nbut also considers the importance of different rank components. We analyze the\nsolution of WSNM and prove that, under certain weights permutation, WSNM can be\nequivalently transformed into independent non-convex $l_p$-norm subproblems,\nwhose global optimum can be efficiently solved by generalized iterated\nshrinkage algorithm. We apply WSNM to typical low-level vision problems, e.g.,\nimage denoising and background subtraction. Extensive experimental results\nshow, both qualitatively and quantitatively, that the proposed WSNM can more\neffectively remove noise, and model complex and dynamic scenes compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 09:24:20 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Xie", "Yuan", ""], ["Gu", "Shuhang", ""], ["Liu", "Yan", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Wensheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1512.01030", "submitter": "V S R Veeravasarapu", "authors": "V S R Veeravasarapu, Rudra Narayan Hota, Constantin Rothkopf, and\n  Ramesh Visvanathan", "title": "Simulations for Validation of Vision Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the computer vision matures into a systems science and engineering\ndiscipline, there is a trend in leveraging latest advances in computer graphics\nsimulations for performance evaluation, learning, and inference. However, there\nis an open question on the utility of graphics simulations for vision with\napparently contradicting views in the literature. In this paper, we place the\nresults from the recent literature in the context of performance\ncharacterization methodology outlined in the 90's and note that insights\nderived from simulations can be qualitative or quantitative depending on the\ndegree of fidelity of models used in simulation and the nature of the question\nposed by the experimenter. We describe a simulation platform that incorporates\nlatest graphics advances and use it for systematic performance characterization\nand trade-off analysis for vision system design. We verify the utility of the\nplatform in a case study of validating a generative model inspired vision\nhypothesis, Rank-Order consistency model, in the contexts of global and local\nillumination changes, and bad weather, and high-frequency noise. Our approach\nestablishes the link between alternative viewpoints, involving models with\nphysics based semantics and signal and perturbation semantics and confirms\ninsights in literature on robust change detection.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 10:53:32 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Veeravasarapu", "V S R", ""], ["Hota", "Rudra Narayan", ""], ["Rothkopf", "Constantin", ""], ["Visvanathan", "Ramesh", ""]]}, {"id": "1512.01055", "submitter": "Ibrahim Radwan Dr.", "authors": "Ibrahim Radwan, Abhinav Dhall, Roland Goecke", "title": "Occlusion-Aware Human Pose Estimation with Mixtures of Sub-Trees", "comments": "12 pages, 5 figures and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning a model for human pose\nestimation as mixtures of compositional sub-trees in two layers of prediction.\nThis involves estimating the pose of a sub-tree followed by identifying the\nrelationships between sub-trees that are used to handle occlusions between\ndifferent parts. The mixtures of the sub-trees are learnt utilising both\ngeometric and appearance distances. The Chow-Liu (CL) algorithm is recursively\napplied to determine the inter-relations between the nodes and to build the\nstructure of the sub-trees. These structures are used to learn the latent\nparameters of the sub-trees and the inference is done using a standard belief\npropagation technique. The proposed method handles occlusions during the\ninference process by identifying overlapping regions between different\nsub-trees and introducing a penalty term for overlapping parts. Experiments are\nperformed on three different datasets: the Leeds Sports, Image Parse and UIUC\nPeople datasets. The results show the robustness of the proposed method to\nocclusions over the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 12:25:33 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Radwan", "Ibrahim", ""], ["Dhall", "Abhinav", ""], ["Goecke", "Roland", ""]]}, {"id": "1512.01192", "submitter": "Saumya Jetley", "authors": "Saumya Jetley, Bernardino Romera-Paredes, Sadeep Jayasumana, Philip\n  Torr", "title": "Prototypical Priors: From Improving Classification to Zero-Shot Learning", "comments": "12 Pages, 6 Figures, 2 Tables, in British Machine Vision Conference\n  (BMVC), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on zero-shot learning make use of side information such as\nvisual attributes or natural language semantics to define the relations between\noutput visual classes and then use these relationships to draw inference on new\nunseen classes at test time. In a novel extension to this idea, we propose the\nuse of visual prototypical concepts as side information. For most real-world\nvisual object categories, it may be difficult to establish a unique prototype.\nHowever, in cases such as traffic signs, brand logos, flags, and even natural\nlanguage characters, these prototypical templates are available and can be\nleveraged for an improved recognition performance. The present work proposes a\nway to incorporate this prototypical information in a deep learning framework.\nUsing prototypes as prior information, the deepnet pipeline learns the input\nimage projections into the prototypical embedding space subject to minimization\nof the final classification loss. Based on our experiments with two different\ndatasets of traffic signs and brand logos, prototypical embeddings incorporated\nin a conventional convolutional neural network improve the recognition\nperformance. Recognition accuracy on the Belga logo dataset is especially\nnoteworthy and establishes a new state-of-the-art. In zero-shot learning\nscenarios, the same system can be directly deployed to draw inference on unseen\nclasses by simply adding the prototypical information for these new classes at\ntest time. Thus, unlike earlier approaches, testing on seen and unseen classes\nis handled using the same pipeline, and the system can be tuned for a trade-off\nof seen and unseen class performance as per task requirement. Comparison with\none of the latest works in the zero-shot learning domain yields top results on\nthe two datasets mentioned above.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 19:06:16 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 13:40:35 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Jetley", "Saumya", ""], ["Romera-Paredes", "Bernardino", ""], ["Jayasumana", "Sadeep", ""], ["Torr", "Philip", ""]]}, {"id": "1512.01289", "submitter": "Edward Grant", "authors": "Edward Grant, Stephan Sahm, Mariam Zabihi, Marcel van Gerven", "title": "Predicting and visualizing psychological attributions with a deep neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Judgments about personality based on facial appearance are strong effectors\nin social decision making, and are known to have impact on areas from\npresidential elections to jury decisions. Recent work has shown that it is\npossible to predict perception of memorability, trustworthiness, intelligence\nand other attributes in human face images. The most successful of these\napproaches require face images expertly annotated with key facial landmarks. We\ndemonstrate a Convolutional Neural Network (CNN) model that is able to perform\nthe same task without the need for landmark features, thereby greatly\nincreasing efficiency. The model has high accuracy, surpassing human-level\nperformance in some cases. Furthermore, we use a deconvolutional approach to\nvisualize important features for perception of 22 attributes and demonstrate a\nnew method for separately visualizing positive and negative features.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 00:24:16 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 01:06:35 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Grant", "Edward", ""], ["Sahm", "Stephan", ""], ["Zabihi", "Mariam", ""], ["van Gerven", "Marcel", ""]]}, {"id": "1512.01320", "submitter": "Ali Borji", "authors": "Ali Borji, Saeed Izadi, Laurent Itti", "title": "What can we learn about CNNs from a large scale controlled object\n  dataset?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tolerance to image variations (e.g. translation, scale, pose, illumination)\nis an important desired property of any object recognition system, be it human\nor machine. Moving towards increasingly bigger datasets has been trending in\ncomputer vision specially with the emergence of highly popular deep learning\nmodels. While being very useful for learning invariance to object inter- and\nintra-class shape variability, these large-scale wild datasets are not very\nuseful for learning invariance to other parameters forcing researchers to\nresort to other tricks for training a model. In this work, we introduce a\nlarge-scale synthetic dataset, which is freely and publicly available, and use\nit to answer several fundamental questions regarding invariance and selectivity\nproperties of convolutional neural networks. Our dataset contains two parts: a)\nobjects shot on a turntable: 16 categories, 8 rotation angles, 11 cameras on a\nsemicircular arch, 5 lighting conditions, 3 focus levels, variety of\nbackgrounds (23.4 per instance) generating 1320 images per instance (over 20\nmillion images in total), and b) scenes: in which a robot arm takes pictures of\nobjects on a 1:160 scale scene. We study: 1) invariance and selectivity of\ndifferent CNN layers, 2) knowledge transfer from one object category to\nanother, 3) systematic or random sampling of images to build a train set, 4)\ndomain adaptation from synthetic to natural scenes, and 5) order of knowledge\ndelivery to CNNs. We also explore how our analyses can lead the field to\ndevelop more efficient CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 05:48:09 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 16:56:11 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Borji", "Ali", ""], ["Izadi", "Saeed", ""], ["Itti", "Laurent", ""]]}, {"id": "1512.01325", "submitter": "Babak Saleh", "authors": "Babak Saleh, Ahmed Elgammal, Jacob Feldman, Ali Farhadi", "title": "Toward a Taxonomy and Computational Models of Abnormalities in Images", "comments": "To appear in the Thirtieth AAAI Conference on Artificial Intelligence\n  (AAAI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system can spot an abnormal image, and reason about what\nmakes it strange. This task has not received enough attention in computer\nvision. In this paper we study various types of atypicalities in images in a\nmore comprehensive way than has been done before. We propose a new dataset of\nabnormal images showing a wide range of atypicalities. We design human subject\nexperiments to discover a coarse taxonomy of the reasons for abnormality. Our\nexperiments reveal three major categories of abnormality: object-centric,\nscene-centric, and contextual. Based on this taxonomy, we propose a\ncomprehensive computational model that can predict all different types of\nabnormality in images and outperform prior arts in abnormality recognition.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 06:29:53 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Saleh", "Babak", ""], ["Elgammal", "Ahmed", ""], ["Feldman", "Jacob", ""], ["Farhadi", "Ali", ""]]}, {"id": "1512.01355", "submitter": "Luca Bertinetto", "authors": "Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, Philip\n  Torr", "title": "Staple: Complementary Learners for Real-Time Tracking", "comments": "To appear in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation Filter-based trackers have recently achieved excellent\nperformance, showing great robustness to challenging situations exhibiting\nmotion blur and illumination changes. However, since the model that they learn\ndepends strongly on the spatial layout of the tracked object, they are\nnotoriously sensitive to deformation. Models based on colour statistics have\ncomplementary traits: they cope well with variation in shape, but suffer when\nillumination is not consistent throughout a sequence. Moreover, colour\ndistributions alone can be insufficiently discriminative. In this paper, we\nshow that a simple tracker combining complementary cues in a ridge regression\nframework can operate faster than 80 FPS and outperform not only all entries in\nthe popular VOT14 competition, but also recent and far more sophisticated\ntrackers according to multiple benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 09:56:48 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 17:58:11 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Bertinetto", "Luca", ""], ["Valmadre", "Jack", ""], ["Golodetz", "Stuart", ""], ["Miksik", "Ondrej", ""], ["Torr", "Philip", ""]]}, {"id": "1512.01383", "submitter": "Thomas M\\'ollenhoff", "authors": "Thomas M\\\"ollenhoff and Emanuel Laude and Michael Moeller and Jan\n  Lellmann and Daniel Cremers", "title": "Sublabel-Accurate Relaxation of Nonconvex Energies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel spatially continuous framework for convex relaxations\nbased on functional lifting. Our method can be interpreted as a\nsublabel-accurate solution to multilabel problems. We show that previously\nproposed functional lifting methods optimize an energy which is linear between\ntwo labels and hence require (often infinitely) many labels for a faithful\napproximation. In contrast, the proposed formulation is based on a piecewise\nconvex approximation and therefore needs far fewer labels. In comparison to\nrecent MRF-based approaches, our method is formulated in a spatially continuous\nsetting and shows less grid bias. Moreover, in a local sense, our formulation\nis the tightest possible convex relaxation. It is easy to implement and allows\nan efficient primal-dual optimization on GPUs. We show the effectiveness of our\napproach on several computer vision problems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 12:16:13 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["M\u00f6llenhoff", "Thomas", ""], ["Laude", "Emanuel", ""], ["Moeller", "Michael", ""], ["Lellmann", "Jan", ""], ["Cremers", "Daniel", ""]]}, {"id": "1512.01400", "submitter": "Haibing Wu", "authors": "Haibing Wu and Xiaodong Gu", "title": "Max-Pooling Dropout for Regularization of Convolutional Neural Networks", "comments": "The journal version of this paper [arXiv:1512.00242] has been\n  published in Neural Networks,\n  http://www.sciencedirect.com/science/article/pii/S0893608015001446", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, dropout has seen increasing use in deep learning. For deep\nconvolutional neural networks, dropout is known to work well in fully-connected\nlayers. However, its effect in pooling layers is still not clear. This paper\ndemonstrates that max-pooling dropout is equivalent to randomly picking\nactivation based on a multinomial distribution at training time. In light of\nthis insight, we advocate employing our proposed probabilistic weighted\npooling, instead of commonly used max-pooling, to act as model averaging at\ntest time. Empirical evidence validates the superiority of probabilistic\nweighted pooling. We also compare max-pooling dropout and stochastic pooling,\nboth of which introduce stochasticity based on multinomial distributions at\npooling stage.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 13:18:37 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Wu", "Haibing", ""], ["Gu", "Xiaodong", ""]]}, {"id": "1512.01401", "submitter": "V S R Veeravasarapu", "authors": "V S R Veeravasarapu, Rudra Narayan Hota, Constantin Rothkopf, and\n  Ramesh Visvanathan", "title": "Model Validation for Vision Systems via Graphics Simulation", "comments": "arXiv admin note: text overlap with arXiv:1512.01030", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advances in computation, combined with latest advances in computer\ngraphics simulations have facilitated the development of vision systems and\ntraining them in virtual environments. One major stumbling block is in\ncertification of the designs and tuned parameters of these systems to work in\nreal world. In this paper, we begin to explore the fundamental question: Which\ntype of information transfer is more analogous to real world? Inspired from the\nperformance characterization methodology outlined in the 90's, we note that\ninsights derived from simulations can be qualitative or quantitative depending\non the degree of the fidelity of models used in simulations and the nature of\nthe questions posed by the experimenter. We adapt the methodology in the\ncontext of current graphics simulation tools for modeling data generation\nprocesses and, for systematic performance characterization and trade-off\nanalysis for vision system design leading to qualitative and quantitative\ninsights. In concrete, we examine invariance assumptions used in vision\nalgorithms for video surveillance settings as a case study and assess the\ndegree to which those invariance assumptions deviate as a function of\ncontextual variables on both graphics simulations and in real data. As computer\ngraphics rendering quality improves, we believe teasing apart the degree to\nwhich model assumptions are valid via systematic graphics simulation can be a\nsignificant aid to assisting more principled ways of approaching vision system\ndesign and performance modeling.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 13:20:23 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Veeravasarapu", "V S R", ""], ["Hota", "Rudra Narayan", ""], ["Rothkopf", "Constantin", ""], ["Visvanathan", "Ramesh", ""]]}, {"id": "1512.01413", "submitter": "Katherine Bouman", "authors": "Katherine L. Bouman, Michael D. Johnson, Daniel Zoran, Vincent L.\n  Fish, Sheperd S. Doeleman, William T. Freeman", "title": "Computational Imaging for VLBI Image Reconstruction", "comments": "Accepted for publication at CVPR 2016, Project Website:\n  http://vlbiimaging.csail.mit.edu/, Video of Oral Presentation at CVPR June\n  2016: https://www.youtube.com/watch?v=YgB6o_d4tL8", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2016, pp. 913-922", "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very long baseline interferometry (VLBI) is a technique for imaging celestial\nradio emissions by simultaneously observing a source from telescopes\ndistributed across Earth. The challenges in reconstructing images from fine\nangular resolution VLBI data are immense. The data is extremely sparse and\nnoisy, thus requiring statistical image models such as those designed in the\ncomputer vision community. In this paper we present a novel Bayesian approach\nfor VLBI image reconstruction. While other methods often require careful tuning\nand parameter selection for different types of data, our method (CHIRP)\nproduces good results under different settings such as low SNR or extended\nemission. The success of our method is demonstrated on realistic synthetic\nexperiments as well as publicly available real data. We present this problem in\na way that is accessible to members of the community, and provide a dataset\nwebsite (vlbiimaging.csail.mit.edu) that facilitates controlled comparisons\nacross algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 14:11:46 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 15:57:40 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Bouman", "Katherine L.", ""], ["Johnson", "Michael D.", ""], ["Zoran", "Daniel", ""], ["Fish", "Vincent L.", ""], ["Doeleman", "Sheperd S.", ""], ["Freeman", "William T.", ""]]}, {"id": "1512.01515", "submitter": "Or Litany", "authors": "Or Litany, Tal Remez, Daniel Freedman, Lior Shapira, Alex Bronstein,\n  Ran Gal", "title": "ASIST: Automatic Semantically Invariant Scene Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ASIST, a technique for transforming point clouds by replacing\nobjects with their semantically equivalent counterparts. Transformations of\nthis kind have applications in virtual reality, repair of fused scans, and\nrobotics. ASIST is based on a unified formulation of semantic labeling and\nobject replacement; both result from minimizing a single objective. We present\nnumerical tools for the efficient solution of this optimization problem. The\nmethod is experimentally assessed on new datasets of both synthetic and real\npoint clouds, and is additionally compared to two recent works on object\nreplacement on data from the corresponding papers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 19:14:57 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Litany", "Or", ""], ["Remez", "Tal", ""], ["Freedman", "Daniel", ""], ["Shapira", "Lior", ""], ["Bronstein", "Alex", ""], ["Gal", "Ran", ""]]}, {"id": "1512.01525", "submitter": "Yezhou Yang", "authors": "Yezhou Yang and Yiannis Aloimonos and Cornelia Fermuller and Eren\n  Erdal Aksoy", "title": "Learning the Semantics of Manipulation Action", "comments": null, "journal-ref": "The 53rd Annual Meeting of the Association for Computational\n  Linguistics (ACL) 1 (2015) 676-686", "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a formal computational framework for modeling\nmanipulation actions. The introduced formalism leads to semantics of\nmanipulation action and has applications to both observing and understanding\nhuman manipulation actions as well as executing them with a robotic mechanism\n(e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The\ngoal of the introduced framework is to: (1) represent manipulation actions with\nboth syntax and semantic parts, where the semantic part employs\n$\\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learn\nthe $\\lambda$-calculus representation of manipulation action from an annotated\naction corpus of videos; (3) use (1) and (2) to develop a system that visually\nobserves manipulation actions and understands their meaning while it can reason\nbeyond observations using propositional logic and axiom schemata. The\nexperiments conducted on a public available large manipulation action dataset\nvalidate the theoretical framework and our implementation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 20:00:08 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Yang", "Yezhou", ""], ["Aloimonos", "Yiannis", ""], ["Fermuller", "Cornelia", ""], ["Aksoy", "Eren Erdal", ""]]}, {"id": "1512.01533", "submitter": "Camille Goudeseune", "authors": "Camille Goudeseune", "title": "Motion trails from time-lapse video", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From an image sequence captured by a stationary camera, background\nsubtraction can detect moving foreground objects in the scene. Distinguishing\nforeground from background is further improved by various heuristics. Then each\nobject's motion can be emphasized by duplicating its positions as a motion\ntrail. These trails clarify the objects' spatial relationships. Also, adding\nmotion trails to a video before previewing it at high speed reduces the risk of\noverlooking transient events.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 20:28:27 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Goudeseune", "Camille", ""]]}, {"id": "1512.01596", "submitter": "Volodymyr Turchenko", "authors": "Volodymyr Turchenko, Artur Luczak", "title": "Creation of a Deep Convolutional Auto-Encoder in Caffe", "comments": "9 pages, 7 figures, 5 tables, 34 references in the list; Added\n  references, corrected Table 3, changed several paragraphs in the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of a deep (stacked) convolutional auto-encoder in the Caffe\ndeep learning framework is presented in this paper. We describe simple\nprinciples which we used to create this model in Caffe. The proposed model of\nconvolutional auto-encoder does not have pooling/unpooling layers yet. The\nresults of our experimental research show comparable accuracy of dimensionality\nreduction in comparison with a classic auto-encoder on the example of MNIST\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 23:58:47 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 01:51:14 GMT"}, {"version": "v3", "created": "Fri, 22 Apr 2016 03:20:41 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Turchenko", "Volodymyr", ""], ["Luczak", "Artur", ""]]}, {"id": "1512.01642", "submitter": "Keze Wang", "authors": "Liang Lin and Keze Wang and Wangmeng Zuo and Meng Wang and Jiebo Luo\n  and Lei Zhang", "title": "A Deep Structured Model with Radius-Margin Bound for 3D Human Activity\n  Recognition", "comments": "16 pages, 9 figures, to appear in International Journal of Computer\n  Vision 2015", "journal-ref": "International Journal of Computer Vision, Volume 118, Issue 2, pp\n  256-273 (June 2016)", "doi": "10.1007/s11263-015-0876-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human activity is very challenging even with the recently\ndeveloped 3D/depth sensors. To solve this problem, this work investigates a\nnovel deep structured model, which adaptively decomposes an activity instance\ninto temporal parts using the convolutional neural networks (CNNs). Our model\nadvances the traditional deep learning approaches in two aspects. First, { we\nincorporate latent temporal structure into the deep model, accounting for large\ntemporal variations of diverse human activities. In particular, we utilize the\nlatent variables to decompose the input activity into a number of temporally\nsegmented sub-activities, and accordingly feed them into the parts (i.e.\nsub-networks) of the deep architecture}. Second, we incorporate a radius-margin\nbound as a regularization term into our deep model, which effectively improves\nthe generalization performance for classification. For model training, we\npropose a principled learning algorithm that iteratively (i) discovers the\noptimal latent variables (i.e. the ways of activity decomposition) for all\ntraining instances, (ii) { updates the classifiers} based on the generated\nfeatures, and (iii) updates the parameters of multi-layer neural networks. In\nthe experiments, our approach is validated on several complex scenarios for\nhuman activity recognition and demonstrates superior performances over other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 09:02:02 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Lin", "Liang", ""], ["Wang", "Keze", ""], ["Zuo", "Wangmeng", ""], ["Wang", "Meng", ""], ["Luo", "Jiebo", ""], ["Zhang", "Lei", ""]]}, {"id": "1512.01655", "submitter": "Nicola Pezzotti", "authors": "Nicola Pezzotti, Boudewijn P.F. Lelieveldt, Laurens van der Maaten,\n  Thomas H\\\"ollt, Elmar Eisemann, and Anna Vilanova", "title": "Approximated and User Steerable tSNE for Progressive Visual Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progressive Visual Analytics aims at improving the interactivity in existing\nanalytics techniques by means of visualization as well as interaction with\nintermediate results. One key method for data analysis is dimensionality\nreduction, for example, to produce 2D embeddings that can be visualized and\nanalyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is a\nwell-suited technique for the visualization of several high-dimensional data.\ntSNE can create meaningful intermediate results but suffers from a slow\ninitialization that constrains its application in Progressive Visual Analytics.\nWe introduce a controllable tSNE approximation (A-tSNE), which trades off speed\nand accuracy, to enable interactive data exploration. We offer real-time\nvisualization techniques, including a density-based solution and a Magic Lens\nto inspect the degree of approximation. With this feedback, the user can decide\non local refinements and steer the approximation level during the analysis. We\ndemonstrate our technique with several datasets, in a real-world research\nscenario and for the real-time analysis of high-dimensional streams to\nillustrate its effectiveness for interactive data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 12:05:52 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 14:56:25 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 09:36:40 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Pezzotti", "Nicola", ""], ["Lelieveldt", "Boudewijn P. F.", ""], ["van der Maaten", "Laurens", ""], ["H\u00f6llt", "Thomas", ""], ["Eisemann", "Elmar", ""], ["Vilanova", "Anna", ""]]}, {"id": "1512.01680", "submitter": "Fatemeh Afghah", "authors": "Fatemeh Afghah, and Abolfazl Razi, and Kayvan Najarian", "title": "A Shapley Value Solution to Game Theoretic-based Feature Reduction in\n  False Alarm Detection", "comments": "Neural Information Processing Systems (NIPS'15), Workshop on Machine\n  Learning in Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  False alarm is one of the main concerns in intensive care units and can\nresult in care disruption, sleep deprivation, and insensitivity of care-givers\nto alarms. Several methods have been proposed to suppress the false alarm rate\nthrough improving the quality of physiological signals by filtering, and\ndeveloping more accurate sensors. However, significant intrinsic correlation\namong the extracted features limits the performance of most currently available\ndata mining techniques, as they often discard the predictors with low\nindividual impact that may potentially have strong discriminatory power when\ngrouped with others. We propose a model based on coalition game theory that\nconsiders the inter-features dependencies in determining the salient predictors\nin respect to false alarm, which results in improved classification accuracy.\nThe superior performance of this method compared to current methods is shown in\nsimulation results using PhysionNet's MIMIC II database.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 15:34:38 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Afghah", "Fatemeh", ""], ["Razi", "Abolfazl", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1512.01691", "submitter": "Rohit Pandey", "authors": "Rohit Kumar Pandey, Yingbo Zhou, Bhargava Urala Kota, Venu Govindaraju", "title": "Maximum Entropy Binary Encoding for Face Template Protection", "comments": "arXiv admin note: text overlap with arXiv:1506.04340", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a framework for secure identification using deep\nneural networks, and apply it to the task of template protection for face\nauthentication. We use deep convolutional neural networks (CNNs) to learn a\nmapping from face images to maximum entropy binary (MEB) codes. The mapping is\nrobust enough to tackle the problem of exact matching, yielding the same code\nfor new samples of a user as the code assigned during training. These codes are\nthen hashed using any hash function that follows the random oracle model (like\nSHA-512) to generate protected face templates (similar to text based password\nprotection). The algorithm makes no unrealistic assumptions and offers high\ntemplate security, cancelability, and state-of-the-art matching performance.\nThe efficacy of the approach is shown on CMU-PIE, Extended Yale B, and\nMulti-PIE face databases. We achieve high (~95%) genuine accept rates (GAR) at\nzero false accept rate (FAR) with up to 1024 bits of template security.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 17:55:04 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Pandey", "Rohit Kumar", ""], ["Zhou", "Yingbo", ""], ["Kota", "Bhargava Urala", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1512.01715", "submitter": "Hang Qi", "authors": "Hang Qi, Tianfu Wu, Mun-Wai Lee, Song-Chun Zhu", "title": "A Restricted Visual Turing Test for Deep Scene and Event Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a restricted visual Turing test (VTT) for story-line\nbased deep understanding in long-term and multi-camera captured videos. Given a\nset of videos of a scene (such as a multi-room office, a garden, and a parking\nlot.) and a sequence of story-line based queries, the task is to provide\nanswers either simply in binary form \"true/false\" (to a polar query) or in an\naccurate natural language description (to a non-polar query). Queries, polar or\nnon-polar, consist of view-based queries which can be answered from a\nparticular camera view and scene-centered queries which involves joint\ninference across different cameras. The story lines are collected to cover\nspatial, temporal and causal understanding of input videos. The data and\nqueries distinguish our VTT from recently proposed visual question answering in\nimages and video captioning. A vision system is proposed to perform joint video\nand query parsing which integrates different vision modules, a knowledge base\nand a query engine. The system provides unified interfaces for different\nmodules so that individual modules can be reconfigured to test a new method. We\nprovide a benchmark dataset and a toolkit for ontology guided story-line query\ngeneration which consists of about 93.5 hours videos captured in four different\nlocations and 3,426 queries split into 127 story lines. We also provide a\nbaseline implementation and result analyses.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 00:40:02 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 19:19:25 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Qi", "Hang", ""], ["Wu", "Tianfu", ""], ["Lee", "Mun-Wai", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1512.01722", "submitter": "Ali Borji", "authors": "Ali Borji, Mengyang Feng", "title": "Vanishing point attracts gaze in free-viewing and visual search tasks", "comments": "There is a major problem with the arguments and results unfortunately", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To investigate whether the vanishing point (VP) plays a significant role in\ngaze guidance, we ran two experiments. In the first one, we recorded fixations\nof 10 observers (4 female; mean age 22; SD=0.84) freely viewing 532 images, out\nof which 319 had VP (shuffled presentation; each image for 4 secs). We found\nthat the average number of fixations at a local region (80x80 pixels) centered\nat the VP is significantly higher than the average fixations at random\nlocations (t-test; n=319; p=1.8e-35). To address the confounding factor of\nsaliency, we learned a combined model of bottom-up saliency and VP. AUC score\nof our model (0.85; SD=0.01) is significantly higher than the original saliency\nmodel (e.g., 0.8 using AIM model by Bruce & Tsotsos (2009), t-test; p=\n3.14e-16) and the VP-only model (0.64, t-test; p= 4.02e-22). In the second\nexperiment, we asked 14 subjects (4 female, mean age 23.07, SD=1.26) to search\nfor a target character (T or L) placed randomly on a 3x3 imaginary grid\noverlaid on top of an image. Subjects reported their answers by pressing one of\ntwo keys. Stimuli consisted of 270 color images (180 with a single VP, 90\nwithout). The target happened with equal probability inside each cell (15 times\nL, 15 times T). We found that subjects were significantly faster (and more\naccurate) when target happened inside the cell containing the VP compared to\ncells without VP (median across 14 subjects 1.34 sec vs. 1.96; Wilcoxon\nrank-sum test; p = 0.0014). Response time at VP cells were also significantly\nlower than response time on images without VP (median 2.37; p= 4.77e-05). These\nfindings support the hypothesis that vanishing point, similar to face and text\n(Cerf et al., 2009) as well as gaze direction (Borji et al., 2014) attracts\nattention in free-viewing and visual search.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 01:59:51 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 20:37:09 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 18:15:44 GMT"}, {"version": "v4", "created": "Tue, 6 Sep 2016 16:01:13 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Borji", "Ali", ""], ["Feng", "Mengyang", ""]]}, {"id": "1512.01774", "submitter": "Or Litany", "authors": "Or Litany, Tal Remez, Alex Bronstein", "title": "Image reconstruction from dense binary pixels", "comments": "Signal Processing with Adaptive Sparse Structured Representations\n  (SPARS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the dense binary pixel Gigavision camera had been introduced,\nemulating a digital version of the photographic film. While seems to be a\npromising solution for HDR imaging, its output is not directly usable and\nrequires an image reconstruction process. In this work, we formulate this\nproblem as the minimization of a convex objective combining a\nmaximum-likelihood term with a sparse synthesis prior. We present MLNet - a\nnovel feed-forward neural network, producing acceptable output quality at a\nfixed complexity and is two orders of magnitude faster than iterative\nalgorithms. We present state of the art results in the abstract.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 10:59:36 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Litany", "Or", ""], ["Remez", "Tal", ""], ["Bronstein", "Alex", ""]]}, {"id": "1512.01789", "submitter": "Mark Sheinin", "authors": "Mark Sheinin, Yoav Y. Schechner", "title": "The Next Best Underwater View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To image in high resolution large and occlusion-prone scenes, a camera must\nmove above and around. Degradation of visibility due to geometric occlusions\nand distances is exacerbated by scattering, when the scene is in a\nparticipating medium. Moreover, underwater and in other media, artificial\nlighting is needed. Overall, data quality depends on the observed surface,\nmedium and the time-varying poses of the camera and light source. This work\nproposes to optimize camera/light poses as they move, so that the surface is\nscanned efficiently and the descattered recovery has the highest quality. The\nwork generalizes the next best view concept of robot vision to scattering media\nand cooperative movable lighting. It also extends descattering to platforms\nthat move optimally. The optimization criterion is information gain, taken from\ninformation theory. We exploit the existence of a prior rough 3D model, since\nunderwater such a model is routinely obtained using sonar. We demonstrate this\nprinciple in a scaled-down setup.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 13:17:14 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Sheinin", "Mark", ""], ["Schechner", "Yoav Y.", ""]]}, {"id": "1512.01815", "submitter": "Lior Wolf", "authors": "David Gadot, Lior Wolf", "title": "PatchBatch: a Batch Augmented Loss for Optical Flow", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new pipeline for optical flow computation, based on Deep\nLearning techniques. We suggest using a Siamese CNN to independently, and in\nparallel, compute the descriptors of both images. The learned descriptors are\nthen compared efficiently using the L2 norm and do not require network\nprocessing of patch pairs. The success of the method is based on an innovative\nloss function that computes higher moments of the loss distributions for each\ntraining batch. Combined with an Approximate Nearest Neighbor patch matching\nmethod and a flow interpolation technique, state of the art performance is\nobtained on the most challenging and competitive optical flow benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 18:30:28 GMT"}, {"version": "v2", "created": "Sun, 10 Apr 2016 14:17:18 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Gadot", "David", ""], ["Wolf", "Lior", ""]]}, {"id": "1512.01848", "submitter": "Basura Fernando", "authors": "Basura Fernando, Efstratios Gavves, Jose Oramas, Amir Ghodrati, Tinne\n  Tuytelaars", "title": "Rank Pooling for Action Recognition", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2558148", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a function-based temporal pooling method that captures the latent\nstructure of the video sequence data - e.g. how frame-level features evolve\nover time in a video. We show how the parameters of a function that has been\nfit to the video data can serve as a robust new video representation. As a\nspecific example, we learn a pooling function via ranking machines. By learning\nto rank the frame-level features of a video in chronological order, we obtain a\nnew representation that captures the video-wide temporal dynamics of a video,\nsuitable for action recognition. Other than ranking functions, we explore\ndifferent parametric models that could also explain the temporal changes in\nvideos. The proposed functional pooling methods, and rank pooling in\nparticular, is easy to interpret and implement, fast to compute and effective\nin recognizing a wide variety of actions. We evaluate our method on various\nbenchmarks for generic action, fine-grained action and gesture recognition.\nResults show that rank pooling brings an absolute improvement of 7-10 average\npooling baseline. At the same time, rank pooling is compatible with and\ncomplementary to several appearance and local motion based methods and\nfeatures, such as improved trajectories and deep learning features.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 22:30:53 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 00:41:05 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Fernando", "Basura", ""], ["Gavves", "Efstratios", ""], ["Oramas", "Jose", ""], ["Ghodrati", "Amir", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1512.01858", "submitter": "Ali Borji", "authors": "Mengyang Feng, Ali Borji, Huchuan Lu", "title": "Fixation prediction with a combined model of bottom-up saliency and\n  vanishing point", "comments": "arXiv admin note: text overlap with arXiv:1512.01722", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By predicting where humans look in natural scenes, we can understand how they\nperceive complex natural scenes and prioritize information for further\nhigh-level visual processing. Several models have been proposed for this\npurpose, yet there is a gap between best existing saliency models and human\nperformance. While many researchers have developed purely computational models\nfor fixation prediction, less attempts have been made to discover cognitive\nfactors that guide gaze. Here, we study the effect of a particular type of\nscene structural information, known as the vanishing point, and show that human\ngaze is attracted to the vanishing point regions. We record eye movements of 10\nobservers over 532 images, out of which 319 have vanishing points. We then\nconstruct a combined model of traditional saliency and a vanishing point\nchannel and show that our model outperforms state of the art saliency models\nusing three scores on our dataset.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 23:29:53 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Feng", "Mengyang", ""], ["Borji", "Ali", ""], ["Lu", "Huchuan", ""]]}, {"id": "1512.01881", "submitter": "Min Sun", "authors": "Cheng-Sheng Chan, Shou-Zhong Chen, Pei-Xuan Xie, Chiung-Chih Chang,\n  Min Sun", "title": "Recognition from Hand Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the study of a wrist-mounted camera system (referred to as\nHandCam) for recognizing activities of hands. HandCam has two unique properties\nas compared to egocentric systems (referred to as HeadCam): (1) it avoids the\nneed to detect hands; (2) it more consistently observes the activities of\nhands. By taking advantage of these properties, we propose a\ndeep-learning-based method to recognize hand states (free v.s. active hands,\nhand gestures, object categories), and discover object categories. Moreover, we\npropose a novel two-streams deep network to further take advantage of both\nHandCam and HeadCam. We have collected a new synchronized HandCam and HeadCam\ndataset with 20 videos captured in three scenes for hand states recognition.\nExperiments show that our HandCam system consistently outperforms a\ndeep-learning-based HeadCam method (with estimated manipulation regions) and a\ndense-trajectory-based HeadCam method in all tasks. We also show that HandCam\nvideos captured by different users can be easily aligned to improve free v.s.\nactive recognition accuracy (3.3% improvement) in across-scenes use case.\nMoreover, we observe that finetuning Convolutional Neural Network consistently\nimproves accuracy. Finally, our novel two-streams deep network combining\nHandCam and HeadCam features achieves the best performance in four out of five\ntasks. With more data, we believe a joint HandCam and HeadCam system can\nrobustly log hand states in daily life.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 02:06:29 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 16:04:40 GMT"}, {"version": "v3", "created": "Tue, 22 Mar 2016 09:12:04 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Chan", "Cheng-Sheng", ""], ["Chen", "Shou-Zhong", ""], ["Xie", "Pei-Xuan", ""], ["Chang", "Chiung-Chih", ""], ["Sun", "Min", ""]]}, {"id": "1512.01891", "submitter": "Yi Sun", "authors": "Yi Sun and Xiaogang Wang and Xiaoou Tang", "title": "Sparsifying Neural Network Connections for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to learn high-performance deep ConvNets with sparse\nneural connections, referred to as sparse ConvNets, for face recognition. The\nsparse ConvNets are learned in an iterative way, each time one additional layer\nis sparsified and the entire model is re-trained given the initial weights\nlearned in previous iterations. One important finding is that directly training\nthe sparse ConvNet from scratch failed to find good solutions for face\nrecognition, while using a previously learned denser model to properly\ninitialize a sparser model is critical to continue learning effective features\nfor face recognition. This paper also proposes a new neural correlation-based\nweight selection criterion and empirically verifies its effectiveness in\nselecting informative connections from previously learned models in each\niteration. When taking a moderately sparse structure (26%-76% of weights in the\ndense model), the proposed sparse ConvNet model significantly improves the face\nrecognition performance of the previous state-of-the-art DeepID2+ models given\nthe same training data, while it keeps the performance of the baseline model\nwith only 12% of the original parameters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 02:56:27 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Sun", "Yi", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1512.01927", "submitter": "Haoran Chen", "authors": "Haoran Chen and Yanfeng Sun and Junbin Gao and Yongli Hu", "title": "Fast Optimization Algorithm on Riemannian Manifolds and Its Application\n  in Low-Rank Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of optimizing a class of composite functions\non Riemannian manifolds and a new first order optimization algorithm (FOA) with\na fast convergence rate is proposed. Through the theoretical analysis for FOA,\nit has been proved that the algorithm has quadratic convergence. The\nexperiments in the matrix completion task show that FOA has better performance\nthan other first order optimization methods on Riemannian manifolds. A fast\nsubspace pursuit method based on FOA is proposed to solve the low-rank\nrepresentation model based on augmented Lagrange method on the low rank matrix\nvariety. Experimental results on synthetic and real data sets are presented to\ndemonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in\nterms of faster convergence and higher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 06:44:23 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Chen", "Haoran", ""], ["Sun", "Yanfeng", ""], ["Gao", "Junbin", ""], ["Hu", "Yongli", ""]]}, {"id": "1512.01979", "submitter": "Antonio Cicone", "authors": "Antonio Cicone, Jingfang Liu, Haomin Zhou", "title": "Hyperspectral Chemical Plume Detection Algorithms Based On\n  Multidimensional Iterative Filtering Decomposition", "comments": null, "journal-ref": null, "doi": "10.1098/rsta.2015.0196", "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemicals released in the air can be extremely dangerous for human beings and\nthe environment. Hyperspectral images can be used to identify chemical plumes,\nhowever the task can be extremely challenging. Assuming we know a priori that\nsome chemical plume, with a known frequency spectrum, has been photographed\nusing a hyperspectral sensor, we can use standard techniques like the so called\nmatched filter or adaptive cosine estimator, plus a properly chosen threshold\nvalue, to identify the position of the chemical plume. However, due to noise\nand sensors fault, the accurate identification of chemical pixels is not easy\neven in this apparently simple situation. In this paper we present a\npost-processing tool that, in a completely adaptive and data driven fashion,\nallows to improve the performance of any classification methods in identifying\nthe boundaries of a plume. This is done using the Multidimensional Iterative\nFiltering (MIF) algorithm (arXiv:1411.6051, arXiv:1507.07173), which is a\nnon-stationary signal decomposition method like the pioneering Empirical Mode\nDecomposition (EMD) method. Moreover, based on the MIF technique, we propose\nalso a pre-processing method that allows to decorrelate and mean-center a\nhyperspectral dataset. The Cosine Similarity measure, which often fails in\npractice, appears to become a successful and outperforming classifier when\nequipped with such pre-processing method. We show some examples of the proposed\nmethods when applied to real life problems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 11:06:10 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Cicone", "Antonio", ""], ["Liu", "Jingfang", ""], ["Zhou", "Haomin", ""]]}, {"id": "1512.02013", "submitter": "Etienne Gadeski", "authors": "Adrian Popescu, Etienne Gadeski, Herv\\'e Le Borgne", "title": "Scalable domain adaptation of convolutional neural networks", "comments": "technical report, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) tend to become a standard approach to\nsolve a wide array of computer vision problems. Besides important theoretical\nand practical advances in their design, their success is built on the existence\nof manually labeled visual resources, such as ImageNet. The creation of such\ndatasets is cumbersome and here we focus on alternatives to manual labeling. We\nhypothesize that new resources are of uttermost importance in domains which are\nnot or weakly covered by ImageNet, such as tourism photographs. We first\ncollect noisy Flickr images for tourist points of interest and apply automatic\nor weakly-supervised reranking techniques to reduce noise. Then, we learn\ndomain adapted models with a standard CNN architecture and compare them to a\ngeneric model obtained from ImageNet. Experimental validation is conducted with\npublicly available datasets, including Oxford5k, INRIA Holidays and Div150Cred.\nResults show that low-cost domain adaptation improves results compared to the\nuse of generic models but also compared to strong non-CNN baselines such as\ntriangulation embedding.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 12:31:32 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Popescu", "Adrian", ""], ["Gadeski", "Etienne", ""], ["Borgne", "Herv\u00e9 Le", ""]]}, {"id": "1512.02017", "submitter": "Aravindh Mahendran", "authors": "Aravindh Mahendran, Andrea Vedaldi", "title": "Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images", "comments": "A substantially extended version of\n  http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf.\n  arXiv admin note: text overlap with arXiv:1412.0035", "journal-ref": null, "doi": "10.1007/s11263-016-0911-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image representations, from SIFT and bag of visual words to Convolutional\nNeural Networks (CNNs) are a crucial component of almost all computer vision\nsystems. However, our understanding of them remains limited. In this paper we\nstudy several landmark representations, both shallow and deep, by a number of\ncomplementary visualization techniques. These visualizations are based on the\nconcept of \"natural pre-image\", namely a natural-looking image whose\nrepresentation has some notable property. We study in particular three such\nvisualizations: inversion, in which the aim is to reconstruct an image from its\nrepresentation, activation maximization, in which we search for patterns that\nmaximally stimulate a representation component, and caricaturization, in which\nthe visual patterns that a representation detects in an image are exaggerated.\nWe pose these as a regularized energy-minimization framework and demonstrate\nits generality and effectiveness. In particular, we show that this method can\ninvert representations such as HOG more accurately than recent alternatives\nwhile being applicable to CNNs too. Among our findings, we show that several\nlayers in CNNs retain photographically accurate information about the image,\nwith different degrees of geometric and photometric invariance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 12:38:13 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 17:01:51 GMT"}, {"version": "v3", "created": "Thu, 14 Apr 2016 15:05:28 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Mahendran", "Aravindh", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1512.02072", "submitter": "Zsuzsanna P\\\"usp\\\"oki", "authors": "Zsuzsanna P\\\"usp\\\"oki, John Paul Ward, Daniel Sage, Michael Unser", "title": "On The Continuous Steering of the Scale of Tight Wavelet Frames", "comments": null, "journal-ref": null, "doi": "10.1137/15M1033885", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analogy with steerable wavelets, we present a general construction of\nadaptable tight wavelet frames, with an emphasis on scaling operations. In\nparticular, the derived wavelets can be \"dilated\" by a procedure comparable to\nthe operation of steering steerable wavelets. The fundamental aspects of the\nconstruction are the same: an admissible collection of Fourier multipliers is\nused to extend a tight wavelet frame, and the \"scale\" of the wavelets is\nadapted by scaling the multipliers. As an application, the proposed wavelets\ncan be used to improve the frequency localization. Importantly, the localized\nfrequency bands specified by this construction can be scaled efficiently using\nmatrix multiplication.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 14:50:43 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["P\u00fcsp\u00f6ki", "Zsuzsanna", ""], ["Ward", "John Paul", ""], ["Sage", "Daniel", ""], ["Unser", "Michael", ""]]}, {"id": "1512.02097", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based\n  Parameter-Insensitive Clustering Method", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most density-based clustering methods largely rely on how well the underlying\ndensity is estimated. However, density estimation itself is also a challenging\nproblem, especially the determination of the kernel bandwidth. A large\nbandwidth could lead to the over-smoothed density estimation in which the\nnumber of density peaks could be less than the true clusters, while a small\nbandwidth could lead to the under-smoothed density estimation in which spurious\ndensity peaks, or called the \"ripple noise\", would be generated in the\nestimated density. In this paper, we propose a density-based hierarchical\nclustering method, called the Deep Nearest Neighbor Descent (D-NND), which\ncould learn the underlying density structure layer by layer and capture the\ncluster structure at the same time. The over-smoothed density estimation could\nbe largely avoided and the negative effect of the under-estimated cases could\nbe also largely reduced. Overall, D-NND presents not only the strong capability\nof discovering the underlying cluster structure but also the remarkable\nreliability due to its insensitivity to parameters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 15:47:49 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1512.02110", "submitter": "Vadim Holodovsky", "authors": "Vadim Holodovsky, Yoav Y. Schechner, Anat Levin, Aviad Levis, Amit\n  Aides", "title": "In-situ multi-scattering tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To recover the three dimensional (3D) volumetric distribution of matter in an\nobject, images of the object are captured from multiple directions and\nlocations. Using these images tomographic computations extract the\ndistribution. In highly scattering media and constrained, natural irradiance,\ntomography must explicitly account for off-axis scattering. Furthermore, the\ntomographic model and recovery must function when imaging is done in-situ, as\noccurs in medical imaging and ground-based atmospheric sensing. We formulate\ntomography that handles arbitrary orders of scattering, using a monte-carlo\nmodel. Moreover, the model is highly parallelizable in our formulation. This\nenables large scale rendering and recovery of volumetric scenes having a large\nnumber of variables. We solve stability and conditioning problems that stem\nfrom radiative transfer (RT) modeling in-situ.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 16:24:48 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Holodovsky", "Vadim", ""], ["Schechner", "Yoav Y.", ""], ["Levin", "Anat", ""], ["Levis", "Aviad", ""], ["Aides", "Amit", ""]]}, {"id": "1512.02134", "submitter": "Nikolaus Mayer", "authors": "Nikolaus Mayer, Eddy Ilg, Philip H\\\"ausser, Philipp Fischer, Daniel\n  Cremers, Alexey Dosovitskiy, Thomas Brox", "title": "A Large Dataset to Train Convolutional Networks for Disparity, Optical\n  Flow, and Scene Flow Estimation", "comments": "Includes supplementary material", "journal-ref": null, "doi": "10.1109/CVPR.2016.438", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that optical flow estimation can be formulated as a\nsupervised learning task and can be successfully solved with convolutional\nnetworks. Training of the so-called FlowNet was enabled by a large\nsynthetically generated dataset. The present paper extends the concept of\noptical flow estimation via convolutional networks to disparity and scene flow\nestimation. To this end, we propose three synthetic stereo video datasets with\nsufficient realism, variation, and size to successfully train large networks.\nOur datasets are the first large-scale datasets to enable training and\nevaluating scene flow methods. Besides the datasets, we present a convolutional\nnetwork for real-time disparity estimation that provides state-of-the-art\nresults. By combining a flow and disparity estimation network and training it\njointly, we demonstrate the first scene flow estimation with a convolutional\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 17:35:00 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Mayer", "Nikolaus", ""], ["Ilg", "Eddy", ""], ["H\u00e4usser", "Philip", ""], ["Fischer", "Philipp", ""], ["Cremers", "Daniel", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1512.02167", "submitter": "Bolei Zhou", "authors": "Bolei Zhou and Yuandong Tian and Sainbayar Sukhbaatar and Arthur Szlam\n  and Rob Fergus", "title": "Simple Baseline for Visual Question Answering", "comments": "One comparison method's scores are put into the correct column, and a\n  new experiment of generating attention map is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a very simple bag-of-words baseline for visual question\nanswering. This baseline concatenates the word features from the question and\nCNN features from the image to predict the answer. When evaluated on the\nchallenging VQA dataset [2], it shows comparable performance to many recent\napproaches using recurrent neural networks. To explore the strength and\nweakness of the trained model, we also provide an interactive web demo and\nopen-source code. .\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 19:00:54 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 05:17:49 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Zhou", "Bolei", ""], ["Tian", "Yuandong", ""], ["Sukhbaatar", "Sainbayar", ""], ["Szlam", "Arthur", ""], ["Fergus", "Rob", ""]]}, {"id": "1512.02188", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Yasuyuki Matsushita, In So Kweon, David Wipf", "title": "Pseudo-Bayesian Robust PCA: Algorithms and Analyses", "comments": "Journal version of NIPS 2016. Submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly used in computer vision and other applications, robust PCA\nrepresents an algorithmic attempt to reduce the sensitivity of classical PCA to\noutliers. The basic idea is to learn a decomposition of some data matrix of\ninterest into low rank and sparse components, the latter representing unwanted\noutliers. Although the resulting optimization problem is typically NP-hard,\nconvex relaxations provide a computationally-expedient alternative with\ntheoretical support. However, in practical regimes performance guarantees break\ndown and a variety of non-convex alternatives, including Bayesian-inspired\nmodels, have been proposed to boost estimation quality. Unfortunately though,\nwithout additional a priori knowledge none of these methods can significantly\nexpand the critical operational range such that exact principal subspace\nrecovery is possible. Into this mix we propose a novel pseudo-Bayesian\nalgorithm that explicitly compensates for design weaknesses in many existing\nnon-convex approaches leading to state-of-the-art performance with a sound\nanalytical foundation. Surprisingly, our algorithm can even outperform convex\nmatrix completion despite the fact that the latter is provided with perfect\nknowledge of which entries are not corrupted.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 19:43:54 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 16:08:25 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Matsushita", "Yasuyuki", ""], ["Kweon", "In So", ""], ["Wipf", "David", ""]]}, {"id": "1512.02311", "submitter": "Michael Maire", "authors": "Takuya Narihira, Michael Maire, Stella X. Yu", "title": "Direct Intrinsics: Learning Albedo-Shading Decomposition by\n  Convolutional Regression", "comments": "International Conference on Computer Vision (ICCV), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to intrinsic image decomposition, the task of\ndecomposing a single image into albedo and shading components. Our strategy,\nwhich we term direct intrinsics, is to learn a convolutional neural network\n(CNN) that directly predicts output albedo and shading channels from an input\nRGB image patch. Direct intrinsics is a departure from classical techniques for\nintrinsic image decomposition, which typically rely on physically-motivated\npriors and graph-based inference algorithms.\n  The large-scale synthetic ground-truth of the MPI Sintel dataset plays a key\nrole in training direct intrinsics. We demonstrate results on both the\nsynthetic images of Sintel and the real images of the classic MIT intrinsic\nimage dataset. On Sintel, direct intrinsics, using only RGB input, outperforms\nall prior work, including methods that rely on RGB+Depth input. Direct\nintrinsics also generalizes across modalities; it produces quite reasonable\ndecompositions on the real images of the MIT dataset. Our results indicate that\nthe marriage of CNNs with synthetic training data may be a powerful new\ntechnique for tackling classic problems in computer vision.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 03:38:52 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Narihira", "Takuya", ""], ["Maire", "Michael", ""], ["Yu", "Stella X.", ""]]}, {"id": "1512.02325", "submitter": "Wei Liu", "authors": "Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott\n  Reed, Cheng-Yang Fu, Alexander C. Berg", "title": "SSD: Single Shot MultiBox Detector", "comments": "ECCV 2016", "journal-ref": null, "doi": "10.1007/978-3-319-46448-0_2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for detecting objects in images using a single deep\nneural network. Our approach, named SSD, discretizes the output space of\nbounding boxes into a set of default boxes over different aspect ratios and\nscales per feature map location. At prediction time, the network generates\nscores for the presence of each object category in each default box and\nproduces adjustments to the box to better match the object shape. Additionally,\nthe network combines predictions from multiple feature maps with different\nresolutions to naturally handle objects of various sizes. Our SSD model is\nsimple relative to methods that require object proposals because it completely\neliminates proposal generation and subsequent pixel or feature resampling stage\nand encapsulates all computation in a single network. This makes SSD easy to\ntrain and straightforward to integrate into systems that require a detection\ncomponent. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets\nconfirm that SSD has comparable accuracy to methods that utilize an additional\nobject proposal step and is much faster, while providing a unified framework\nfor both training and inference. Compared to other single stage methods, SSD\nhas much better accuracy, even with a smaller input image size. For $300\\times\n300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan\nX and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a\ncomparable state of the art Faster R-CNN model. Code is available at\nhttps://github.com/weiliu89/caffe/tree/ssd .\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 04:46:38 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 21:17:34 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 18:31:25 GMT"}, {"version": "v4", "created": "Wed, 30 Nov 2016 09:54:02 GMT"}, {"version": "v5", "created": "Thu, 29 Dec 2016 19:05:11 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Liu", "Wei", ""], ["Anguelov", "Dragomir", ""], ["Erhan", "Dumitru", ""], ["Szegedy", "Christian", ""], ["Reed", "Scott", ""], ["Fu", "Cheng-Yang", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1512.02326", "submitter": "Dequan Wang", "authors": "Jie Shao, Dequan Wang, Xiangyang Xue, Zheng Zhang", "title": "Learning to Point and Count", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the problem of point-and-count as a test case to break\nthe what-and-where deadlock. Different from the traditional detection problem,\nthe goal is to discover key salient points as a way to localize and count the\nnumber of objects simultaneously. We propose two alternatives, one that counts\nfirst and then point, and another that works the other way around.\nFundamentally, they pivot around whether we solve \"what\" or \"where\" first. We\nevaluate their performance on dataset that contains multiple instances of the\nsame class, demonstrating the potentials and their synergies. The experiences\nderive a few important insights that explains why this is a much harder problem\nthan classification, including strong data bias and the inability to deal with\nobject scales robustly in state-of-art convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 04:48:52 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Shao", "Jie", ""], ["Wang", "Dequan", ""], ["Xue", "Xiangyang", ""], ["Zhang", "Zheng", ""]]}, {"id": "1512.02329", "submitter": "Qifei Wang", "authors": "Qifei Wang", "title": "Computational Models for Multiview Dense Depth Maps of Dynamic Scene", "comments": "4 pages, IEEE COMSOC MMTC E-Letter 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the recent progresses of the depth map generation for\ndynamic scene and its corresponding computational models. This paper mainly\ncovers the homogeneous ambiguity models in depth sensing, resolution models in\ndepth processing, and consistency models in depth optimization. We also\nsummarize the future work in the depth map generation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 05:04:57 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 06:18:35 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Wang", "Qifei", ""]]}, {"id": "1512.02355", "submitter": "Erkan Bostanci", "authors": "Erkan Bostanci", "title": "Is Hamming distance the only way for matching binary image feature\n  descriptors?", "comments": "2 pages, journal", "journal-ref": "Electronics Letters (2014), vol. 50, iss. 11, pp. 806-808", "doi": "10.1049/el.2014.0773", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brute force matching of binary image feature descriptors is conventionally\nperformed using the Hamming distance. This paper assesses the use of\nalternative metrics in order to see whether they can produce feature\ncorrespondences that yield more accurate homography matrices. Two statistical\ntests, namely ANOVA (Analysis of Variance) and McNemar's test were employed for\nevaluation. Results show that Jackard-Needham and Dice metrics can display\nbetter performance for some descriptors. Yet, these performance differences\nwere not found to be statistically significant.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 06:48:39 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Bostanci", "Erkan", ""]]}, {"id": "1512.02357", "submitter": "Masoud Aghamohamadian-Sharbaf", "authors": "Masoud Aghamohamadian-Sharbaf, Ahmadreza Heravi, Hamidreza Pourreza", "title": "Towards the Application of Linear Programming Methods For Multi-Camera\n  Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We presented a separation based optimization algorithm which, rather than\noptimization the entire variables altogether, This would allow us to employ: 1)\na class of nonlinear functions with three variables and 2) a convex quadratic\nmultivariable polynomial, for minimization of reprojection error. Neglecting\nthe inversion required to minimize the nonlinear functions, in this paper we\ndemonstrate how separation allows eradication of matrix inversion.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 07:25:29 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Aghamohamadian-Sharbaf", "Masoud", ""], ["Heravi", "Ahmadreza", ""], ["Pourreza", "Hamidreza", ""]]}, {"id": "1512.02413", "submitter": "Julian Yarkony", "authors": "Shaofei Wang, Steffen Wolf, Charless Fowlkes, Julian Yarkony", "title": "Tracking Objects with Higher Order Interactions using Delayed Column\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of multi-target tracking and data association in video.\nWe formulate this in terms of selecting a subset of high-quality tracks subject\nto the constraint that no pair of selected tracks is associated with a common\ndetection (of an object). This objective is equivalent to the classic NP-hard\nproblem of finding a maximum-weight set packing (MWSP) where tracks correspond\nto sets and is made further difficult since the number of candidate tracks\ngrows exponentially in the number of detections. We present a relaxation of\nthis combinatorial problem that uses a column generation formulation where the\npricing problem is solved via dynamic programming to efficiently explore the\nspace of tracks. We employ row generation to tighten the bound in such a way as\nto preserve efficient inference in the pricing problem. We show the practical\nutility of this algorithm for tracking problems in natural and biological video\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 11:41:30 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 04:10:10 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2016 05:44:51 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Wang", "Shaofei", ""], ["Wolf", "Steffen", ""], ["Fowlkes", "Charless", ""], ["Yarkony", "Julian", ""]]}, {"id": "1512.02497", "submitter": "Francisco Massa", "authors": "Francisco Massa, Bryan Russell, Mathieu Aubry", "title": "Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views", "comments": "To appear in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end convolutional neural network (CNN) for\n2D-3D exemplar detection. We demonstrate that the ability to adapt the features\nof natural images to better align with those of CAD rendered views is critical\nto the success of our technique. We show that the adaptation can be learned by\ncompositing rendered views of textured object models on natural images. Our\napproach can be naturally incorporated into a CNN detection pipeline and\nextends the accuracy and speed benefits from recent advances in deep learning\nto 2D-3D exemplar detection. We applied our method to two tasks: instance\ndetection, where we evaluated on the IKEA dataset, and object category\ndetection, where we out-perform Aubry et al. for \"chair\" detection on a subset\nof the Pascal VOC dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 15:04:46 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 13:14:22 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Massa", "Francisco", ""], ["Russell", "Bryan", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1512.02665", "submitter": "Feng Zhou", "authors": "Feng Zhou, Yuanqing Lin", "title": "Fine-grained Image Classification by Exploring Bipartite-Graph Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given a food image, can a fine-grained object recognition engine tell \"which\nrestaurant which dish\" the food belongs to? Such ultra-fine grained image\nrecognition is the key for many applications like search by images, but it is\nvery challenging because it needs to discern subtle difference between classes\nwhile dealing with the scarcity of training data. Fortunately, the ultra-fine\ngranularity naturally brings rich relationships among object classes. This\npaper proposes a novel approach to exploit the rich relationships through\nbipartite-graph labels (BGL). We show how to model BGL in an overall\nconvolutional neural networks and the resulting system can be optimized through\nback-propagation. We also show that it is computationally efficient in\ninference thanks to the bipartite structure. To facilitate the study, we\nconstruct a new food benchmark dataset, which consists of 37,885 food images\ncollected from 6 restaurants and totally 975 menus. Experimental results on\nthis new food and three other datasets demonstrates BGL advances previous works\nin fine-grained object recognition. An online demo is available at\nhttp://www.f-zhou.com/fg_demo/.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 21:18:35 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 18:49:54 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Zhou", "Feng", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1512.02736", "submitter": "Xingyu Zeng", "authors": "Xingyu Zeng, Wanli Ouyang, Xiaogang Wang", "title": "Window-Object Relationship Guided Representation Learning for Generic\n  Object Detections", "comments": "9 pages, including 1 reference page, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In existing works that learn representation for object detection, the\nrelationship between a candidate window and the ground truth bounding box of an\nobject is simplified by thresholding their overlap. This paper shows\ninformation loss in this simplification and picks up the relative location/size\ninformation discarded by thresholding. We propose a representation learning\npipeline to use the relationship as supervision for improving the learned\nrepresentation in object detection. Such relationship is not limited to object\nof the target category, but also includes surrounding objects of other\ncategories. We show that image regions with multiple contexts and multiple\nrotations are effective in capturing such relationship during the\nrepresentation learning process and in handling the semantic and visual\nvariation caused by different window-object configurations. Experimental\nresults show that the representation learned by our approach can improve the\nobject detection accuracy by 6.4% in mean average precision (mAP) on\nILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved\nby our single model and it is the best among published results. On PASCAL VOC,\nit outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute\nmAP.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 03:32:21 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Zeng", "Xingyu", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1512.02766", "submitter": "Erkan Bostanci", "authors": "Erkan Bostanci, Betul Bostanci, Nadia Kanwal, Adrian F. Clark", "title": "Sensor Fusion of Camera, GPS and IMU using Fuzzy Adaptive Multiple\n  Motion Models", "comments": "14 pages, journal", "journal-ref": null, "doi": "10.1007/s00500-017-2516-8", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tracking system that will be used for Augmented Reality (AR) applications\nhas two main requirements: accuracy and frame rate. The first requirement is\nrelated to the performance of the pose estimation algorithm and how accurately\nthe tracking system can find the position and orientation of the user in the\nenvironment. Accuracy problems of current tracking devices, considering that\nthey are low-cost devices, cause static errors during this motion estimation\nprocess. The second requirement is related to dynamic errors (the end-to-end\nsystem delay; occurring because of the delay in estimating the motion of the\nuser and displaying images based on this estimate. This paper investigates\ncombining the vision-based estimates with measurements from other sensors, GPS\nand IMU, in order to improve the tracking accuracy in outdoor environments. The\nidea of using Fuzzy Adaptive Multiple Models (FAMM) was investigated using a\nnovel fuzzy rule-based approach to decide on the model that results in improved\naccuracy and faster convergence for the fusion filter. Results show that the\ndeveloped tracking system is more accurate than a conventional GPS-IMU fusion\napproach due to additional estimates from a camera and fuzzy motion models. The\npaper also presents an application in cultural heritage context.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 06:25:09 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Bostanci", "Erkan", ""], ["Bostanci", "Betul", ""], ["Kanwal", "Nadia", ""], ["Clark", "Adrian F.", ""]]}, {"id": "1512.02767", "submitter": "Michael Maire", "authors": "Michael Maire, Takuya Narihira, Stella X. Yu", "title": "Affinity CNN: Learning Pixel-Centric Pairwise Relations for\n  Figure/Ground Embedding", "comments": "minor updates; extended version of CVPR 2016 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral embedding provides a framework for solving perceptual organization\nproblems, including image segmentation and figure/ground organization. From an\naffinity matrix describing pairwise relationships between pixels, it clusters\npixels into regions, and, using a complex-valued extension, orders pixels\naccording to layer. We train a convolutional neural network (CNN) to directly\npredict the pairwise relationships that define this affinity matrix. Spectral\nembedding then resolves these predictions into a globally-consistent\nsegmentation and figure/ground organization of the scene. Experiments\ndemonstrate significant benefit to this direct coupling compared to prior works\nwhich use explicit intermediate stages, such as edge detection, on the pathway\nfrom image to affinities. Our results suggest spectral embedding as a powerful\nalternative to the conditional random field (CRF)-based globalization schemes\ntypically coupled to deep neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 06:45:23 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 22:03:38 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Maire", "Michael", ""], ["Narihira", "Takuya", ""], ["Yu", "Stella X.", ""]]}, {"id": "1512.02895", "submitter": "Shaoting Zhang", "authors": "Xiaofan Zhang and Feng Zhou and Yuanqing Lin and Shaoting Zhang", "title": "Embedding Label Structures for Fine-Grained Feature Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent algorithms in convolutional neural networks (CNN) considerably advance\nthe fine-grained image classification, which aims to differentiate subtle\ndifferences among subordinate classes. However, previous studies have rarely\nfocused on learning a fined-grained and structured feature representation that\nis able to locate similar images at different levels of relevance, e.g.,\ndiscovering cars from the same make or the same model, both of which require\nhigh precision. In this paper, we propose two main contributions to tackle this\nproblem. 1) A multi-task learning framework is designed to effectively learn\nfine-grained feature representations by jointly optimizing both classification\nand similarity constraints. 2) To model the multi-level relevance, label\nstructures such as hierarchy or shared attributes are seamlessly embedded into\nthe framework by generalizing the triplet loss. Extensive and thorough\nexperiments have been conducted on three fine-grained datasets, i.e., the\nStanford car, the car-333, and the food datasets, which contain either\nhierarchical labels or shared attributes. Our proposed method has achieved very\ncompetitive performance, i.e., among state-of-the-art classification accuracy.\nMore importantly, it significantly outperforms previous fine-grained feature\nrepresentations for image retrieval at different levels of relevance.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 15:22:26 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 02:59:36 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Zhang", "Xiaofan", ""], ["Zhou", "Feng", ""], ["Lin", "Yuanqing", ""], ["Zhang", "Shaoting", ""]]}, {"id": "1512.02902", "submitter": "Makarand Tapaswi", "authors": "Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba,\n  Raquel Urtasun, Sanja Fidler", "title": "MovieQA: Understanding Stories in Movies through Question-Answering", "comments": "CVPR 2016, Spotlight presentation. Benchmark @\n  http://movieqa.cs.toronto.edu/ Code @\n  https://github.com/makarandtapaswi/MovieQA_CVPR2016/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the MovieQA dataset which aims to evaluate automatic story\ncomprehension from both video and text. The dataset consists of 14,944\nquestions about 408 movies with high semantic diversity. The questions range\nfrom simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain events\noccurred. Each question comes with a set of five possible answers; a correct\none and four deceiving answers provided by human annotators. Our dataset is\nunique in that it contains multiple sources of information -- video clips,\nplots, subtitles, scripts, and DVS. We analyze our data through various\nstatistics and methods. We further extend existing QA techniques to show that\nquestion-answering with such open-ended semantics is hard. We make this data\nset public along with an evaluation benchmark to encourage inspiring work in\nthis challenging domain.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 15:34:31 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 04:52:35 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Tapaswi", "Makarand", ""], ["Zhu", "Yukun", ""], ["Stiefelhagen", "Rainer", ""], ["Torralba", "Antonio", ""], ["Urtasun", "Raquel", ""], ["Fidler", "Sanja", ""]]}, {"id": "1512.02914", "submitter": "Christopher Marcum", "authors": "Christopher Steven Marcum", "title": "Yet Another Statistical Analysis of Bob Ross Paintings", "comments": "This version based off of arXiv compliant pdflatex source (which\n  result in lossy data). Original R-code, Sweave source, and data files are\n  available upon request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze a sample of clippings from paintings by the late\nartist Bob Ross. Previous work focused on the qualitative themes of his\npaintings (Hickey, 2014); here, we expand on that line of research by\nconsidering the colorspace and luminosity values as our data. Our results\ndemonstrate the subtle aesthetics of the average Ross painting, the common\nvariation shared by his paintings, and the structure of the relationships\nbetween each painting in our sample. We reveal, for the first time, renderings\nof the average paintings and introduce \"eigenross\" components to identify and\nevaluate shared variance. Additionally, all data and code are embedded in this\ndocument to encourage future research, and, in the spirit of Bob Ross, to teach\nothers how to do so.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 15:57:46 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Marcum", "Christopher Steven", ""]]}, {"id": "1512.02949", "submitter": "Rakshith Shetty", "authors": "Rakshith Shetty and Jorma Laaksonen", "title": "Video captioning with recurrent networks based on frame- and video-level\n  features and visual content classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the system for generating textual descriptions of\nshort video clips using recurrent neural networks (RNN), which we used while\nparticipating in the Large Scale Movie Description Challenge 2015 in ICCV 2015.\nOur work builds on static image captioning systems with RNN based language\nmodels and extends this framework to videos utilizing both static image\nfeatures and video-specific features. In addition, we study the usefulness of\nvisual content classifiers as a source of additional information for caption\ngeneration. With experimental results we show that utilizing keyframe based\nfeatures, dense trajectory video features and content classifier outputs\ntogether gives better performance than any one of them individually.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 17:17:29 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Shetty", "Rakshith", ""], ["Laaksonen", "Jorma", ""]]}, {"id": "1512.02972", "submitter": "Jorge Ortiz", "authors": "Jorge Ortiz (IBM Reserch) and Chien-Chin Huang (NYU Computer Science)\n  and Supriyo Chakraborty (IBM Research)", "title": "Get More With Less: Near Real-Time Image Clustering on Mobile Phones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms, in conjunction with user data, hold the promise\nof revolutionizing the way we interact with our phones, and indeed their\nwidespread adoption in the design of apps bear testimony to this promise.\nHowever, currently, the computationally expensive segments of the learning\npipeline, such as feature extraction and model training, are offloaded to the\ncloud, resulting in an over-reliance on the network and under-utilization of\ncomputing resources available on mobile platforms. In this paper, we show that\nby combining the computing power distributed over a number of phones, judicious\noptimization choices, and contextual information it is possible to execute the\nend-to-end pipeline entirely on the phones at the edge of the network,\nefficiently. We also show that by harnessing the power of this combination, it\nis possible to execute a computationally expensive pipeline at near real-time.\n  To demonstrate our approach, we implement an end-to-end image-processing\npipeline -- that includes feature extraction, vocabulary learning,\nvectorization, and image clustering -- on a set of mobile phones. Our results\nshow a 75% improvement over the standard, full pipeline implementation running\non the phones without modification -- reducing the time to one minute under\ncertain conditions. We believe that this result is a promising indication that\nfully distributed, infrastructure-less computing is possible on networks of\nmobile phones; enabling a new class of mobile applications that are less\nreliant on the cloud.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 18:08:59 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Ortiz", "Jorge", "", "IBM Reserch"], ["Huang", "Chien-Chin", "", "NYU Computer Science"], ["Chakraborty", "Supriyo", "", "IBM Research"]]}, {"id": "1512.03012", "submitter": "Manolis Savva", "authors": "Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan,\n  Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su,\n  Jianxiong Xiao, Li Yi, and Fisher Yu", "title": "ShapeNet: An Information-Rich 3D Model Repository", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ShapeNet: a richly-annotated, large-scale repository of shapes\nrepresented by 3D CAD models of objects. ShapeNet contains 3D models from a\nmultitude of semantic categories and organizes them under the WordNet taxonomy.\nIt is a collection of datasets providing many semantic annotations for each 3D\nmodel such as consistent rigid alignments, parts and bilateral symmetry planes,\nphysical sizes, keywords, as well as other planned annotations. Annotations are\nmade available through a public web-based interface to enable data\nvisualization of object attributes, promote data-driven geometric analysis, and\nprovide a large-scale quantitative benchmark for research in computer graphics\nand vision. At the time of this technical report, ShapeNet has indexed more\nthan 3,000,000 models, 220,000 models out of which are classified into 3,135\ncategories (WordNet synsets). In this report we describe the ShapeNet effort as\na whole, provide details for all currently available datasets, and summarize\nfuture plans.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 19:42:48 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Chang", "Angel X.", ""], ["Funkhouser", "Thomas", ""], ["Guibas", "Leonidas", ""], ["Hanrahan", "Pat", ""], ["Huang", "Qixing", ""], ["Li", "Zimo", ""], ["Savarese", "Silvio", ""], ["Savva", "Manolis", ""], ["Song", "Shuran", ""], ["Su", "Hao", ""], ["Xiao", "Jianxiong", ""], ["Yi", "Li", ""], ["Yu", "Fisher", ""]]}, {"id": "1512.03019", "submitter": "Alexandra Maria Radu", "authors": "Alexandra Maria Radu", "title": "Minimally Supervised Feature Selection for Classification (Master's\n  Thesis, University Politehnica of Bucharest)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the highly increasing number of features that are available\nnowadays we design a robust and fast method for feature selection. The method\ntries to select the most representative features that are independent from each\nother, but are strong together. We propose an algorithm that requires very\nlimited labeled data (as few as one labeled frame per class) and can\naccommodate as many unlabeled samples. We also present here the supervised\napproach from which we started. We compare our two formulations with\nestablished methods like AdaBoost, SVM, Lasso, Elastic Net and FoBa and show\nthat our method is much faster and it has constant training time. Moreover, the\nunsupervised approach outperforms all the methods with which we compared and\nthe difference might be quite prominent. The supervised approach is in most\ncases better than the other methods, especially when the number of training\nshots is very limited. All that the algorithm needs is to choose from a pool of\npositively correlated features. The methods are evaluated on the\nYoutube-Objects dataset of videos and on MNIST digits dataset, while at\ntraining time we also used features obtained on CIFAR10 dataset and others\npre-trained on ImageNet dataset. Thereby, we also proved that transfer learning\nis useful, even though the datasets differ very much: from low-resolution\ncentered images from 10 classes, to high-resolution images with objects from\n1000 classes occurring in different regions of the images or to very difficult\nvideos with very high intraclass variance. 7\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 19:49:29 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Radu", "Alexandra Maria", ""]]}, {"id": "1512.03131", "submitter": "Li Wang", "authors": "Li Wang and Dennis Sng", "title": "Deep Learning Algorithms with Applications to Video Analytics for A\n  Smart City: A Survey", "comments": "8 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently achieved very promising results in a wide range of\nareas such as computer vision, speech recognition and natural language\nprocessing. It aims to learn hierarchical representations of data by using deep\narchitecture models. In a smart city, a lot of data (e.g. videos captured from\nmany distributed sensors) need to be automatically processed and analyzed. In\nthis paper, we review the deep learning algorithms applied to video analytics\nof smart city in terms of different research topics: object detection, object\ntracking, face recognition, image classification and scene labeling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 03:23:54 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Wang", "Li", ""], ["Sng", "Dennis", ""]]}, {"id": "1512.03155", "submitter": "Erkan Bostanci", "authors": "Erkan Bostanci", "title": "Enhanced image feature coverage: Key-point selection using genetic\n  algorithms", "comments": "14 pages, journal", "journal-ref": null, "doi": "10.1080/13682199.2016.1254939", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coverage of image features play an important role in many vision algorithms\nsince their distribution affect the estimated homography. This paper presents a\nGenetic Algorithm (GA) in order to select the optimal set of features yielding\nmaximum coverage of the image which is measured by a robust method based on\nspatial statistics. It is shown with statistical tests on two datasets that the\nmetric yields better coverage and this is also confirmed by an accuracy test on\nthe computed homography for the original set and the newly selected set of\nfeatures. Results have demonstrated that the new set has similar performance in\nterms of the accuracy of the computed homography with the original one with an\nextra benefit of using fewer number of features ultimately reducing the time\nrequired for descriptor calculation and matching.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 06:51:28 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Bostanci", "Erkan", ""]]}, {"id": "1512.03156", "submitter": "Erkan Bostanci", "authors": "Erkan Bostanci", "title": "3D Reconstruction of Crime Scenes and Design Considerations for an\n  Interactive Investigation Tool", "comments": "9 pages, journal", "journal-ref": "International Journal of Information Security Science, Vol 4, No 2\n  (2015)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crime Scene Investigation (CSI) is a carefully planned systematic process\nwith the purpose of acquiring physical evidences to shed light upon the\nphysical reality of the crime and eventually detect the identity of the\ncriminal. Capturing images and videos of the crime scene is an important part\nof this process in order to conduct a deeper analysis on the digital evidence\nfor possible hints. This work brings this idea further to use the acquired\nfootage for generating a 3D model of the crime scene. Results show that\nrealistic reconstructions can be obtained using sophisticated computer vision\ntechniques. The paper also discusses a number of important design\nconsiderations describing key features that should be present in a powerful\ninteractive CSI analysis tool.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 07:20:43 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Bostanci", "Erkan", ""]]}, {"id": "1512.03384", "submitter": "Xintong Han", "authors": "Xintong Han, Bharat Singh, Vlad I. Morariu and Larry S. Davis", "title": "VRFP: On-the-fly Video Retrieval using Web Images and Fast Fisher Vector\n  Products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VRFP is a real-time video retrieval framework based on short text input\nqueries, which obtains weakly labeled training images from the web after the\nquery is known. The retrieved web images representing the query and each\ndatabase video are treated as unordered collections of images, and each\ncollection is represented using a single Fisher Vector built on CNN features.\nOur experiments show that a Fisher Vector is robust to noise present in web\nimages and compares favorably in terms of accuracy to other standard\nrepresentations. While a Fisher Vector can be constructed efficiently for a new\nquery, matching against the test set is slow due to its high dimensionality. To\nperform matching in real-time, we present a lossless algorithm that accelerates\nthe inner product computation between high dimensional Fisher Vectors. We prove\nthat the expected number of multiplications required decreases quadratically\nwith the sparsity of Fisher Vectors. We are not only able to construct and\napply query models in real-time, but with the help of a simple re-ranking\nscheme, we also outperform state-of-the-art automatic retrieval methods by a\nsignificant margin on TRECVID MED13 (3.5%), MED14 (1.3%) and CCV datasets\n(5.2%). We also provide a direct comparison on standard datasets between two\ndifferent paradigms for automatic video retrieval - zero-shot learning and\non-the-fly retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 19:50:50 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 01:25:42 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 17:28:16 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Han", "Xintong", ""], ["Singh", "Bharat", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1512.03385", "submitter": "Kaiming He", "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun", "title": "Deep Residual Learning for Image Recognition", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 19:51:55 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["He", "Kaiming", ""], ["Zhang", "Xiangyu", ""], ["Ren", "Shaoqing", ""], ["Sun", "Jian", ""]]}, {"id": "1512.03424", "submitter": "Fahimeh Rezazadegan", "authors": "Fahimeh Rezazadegan, Sareh Shirazi, Michael Milford, Ben Upcroft", "title": "Evaluation of Object Detection Proposals Under Condition Variations", "comments": "2 pages, 6 figures, CVPR Workshop, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a fundamental task in many computer vision applications,\ntherefore the importance of evaluating the quality of object detection is well\nacknowledged in this domain. This process gives insight into the capabilities\nof methods in handling environmental changes. In this paper, a new method for\nobject detection is introduced that combines the Selective Search and\nEdgeBoxes. We tested these three methods under environmental variations. Our\nexperiments demonstrate the outperformance of the combination method under\nillumination and view point variations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 06:31:59 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Rezazadegan", "Fahimeh", ""], ["Shirazi", "Sareh", ""], ["Milford", "Michael", ""], ["Upcroft", "Ben", ""]]}, {"id": "1512.03460", "submitter": "Yezhou Yang", "authors": "Yezhou Yang and Yi Li and Cornelia Fermuller and Yiannis Aloimonos", "title": "Neural Self Talk: Image Understanding via Continuous Questioning and\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of continuously discovering image\ncontents by actively asking image based questions and subsequently answering\nthe questions being asked. The key components include a Visual Question\nGeneration (VQG) module and a Visual Question Answering module, in which\nRecurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are\nused. Given a dataset that contains images, questions and their answers, both\nmodules are trained at the same time, with the difference being VQG uses the\nimages as input and the corresponding questions as output, while VQA uses\nimages and questions as input and the corresponding answers as output. We\nevaluate the self talk process subjectively using Amazon Mechanical Turk, which\nshow effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 21:58:46 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Yang", "Yezhou", ""], ["Li", "Yi", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1512.03526", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson and Carl Donovan", "title": "Randomized Low-Rank Dynamic Mode Decomposition for Motion Detection", "comments": "Preprint submitted to Journal of Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": "10.1016/j.cviu.2016.02.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a fast algorithm for randomized computation of a\nlow-rank Dynamic Mode Decomposition (DMD) of a matrix. Here we consider this\nmatrix to represent the development of a spatial grid through time e.g. data\nfrom a static video source. DMD was originally introduced in the fluid\nmechanics community, but is also suitable for motion detection in video streams\nand its use for background subtraction has received little previous\ninvestigation. In this study we present a comprehensive evaluation of\nbackground subtraction, using the randomized DMD and compare the results with\nleading robust principal component analysis algorithms. The results are\nconvincing and show the random DMD is an efficient and powerful approach for\nbackground modeling, allowing processing of high resolution videos in\nreal-time. Supplementary materials include implementations of the algorithms in\nPython.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 05:20:40 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Donovan", "Carl", ""]]}, {"id": "1512.03617", "submitter": "Weiya Ren", "authors": "Wei-Ya Ren", "title": "Robust Dictionary based Data Representation", "comments": "8 pages. 2015.12.10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustness to noise and outliers is an important issue in linear\nrepresentation in real applications. We focus on the problem that samples are\ngrossly corrupted, which is also the 'sample specific' corruptions problem. A\nreasonable assumption is that corrupted samples cannot be represented by the\ndictionary while clean samples can be well represented. This assumption is\nenforced in this paper by investigating the coefficients of corrupted samples.\nConcretely, we require the coefficients of corrupted samples be zero. In this\nway, the representation quality of clean data can be assured without the effect\nof corrupted data. At last, a robust dictionary based data representation\napproach and its sparse representation version are proposed, which have\ndirective significance for future applications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 12:17:35 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Ren", "Wei-Ya", ""]]}, {"id": "1512.03622", "submitter": "Guangrun Wang", "authors": "Shengyong Ding, Liang Lin, Guangrun Wang, Hongyang Chao", "title": "Deep Feature Learning with Relative Distance Comparison for Person\n  Re-identification", "comments": "29 pages, 9 figures, The code has been released.\n  http://vision.sysu.edu.cn/projects/deepreid/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the same individual across different scenes is an important yet\ndifficult task in intelligent video surveillance. Its main difficulty lies in\nhow to preserve similarity of the same person against large appearance and\nstructure variation while discriminating different individuals. In this paper,\nwe present a scalable distance driven feature learning framework based on the\ndeep neural network for person re-identification, and demonstrate its\neffectiveness to handle the existing challenges. Specifically, given the\ntraining images with the class labels (person IDs), we first produce a large\nnumber of triplet units, each of which contains three images, i.e. one person\nwith a matched reference and a mismatched reference. Treating the units as the\ninput, we build the convolutional neural network to generate the layered\nrepresentations, and follow with the $L2$ distance metric. By means of\nparameter optimization, our framework tends to maximize the relative distance\nbetween the matched pair and the mismatched pair for each triplet unit.\nMoreover, a nontrivial issue arising with the framework is that the triplet\norganization cubically enlarges the number of training triplets, as one image\ncan be involved into several triplet units. To overcome this problem, we\ndevelop an effective triplet generation scheme and an optimized gradient\ndescent algorithm, making the computational load mainly depends on the number\nof original images instead of the number of triplets. On several challenging\ndatabases, our approach achieves very promising results and outperforms other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 12:34:22 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Ding", "Shengyong", ""], ["Lin", "Liang", ""], ["Wang", "Guangrun", ""], ["Chao", "Hongyang", ""]]}, {"id": "1512.03706", "submitter": "Andrei Hossu", "authors": "Andrei Hossu and Daniela Andone", "title": "A New Approach of Gray Images Binarization with Threshold Methods", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.1.4391.8165", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents some aspects of the (gray level) image binarization\nmethods used in artificial vision systems. It is introduced a new approach of\ngray level image binarization for artificial vision systems dedicated to\nindustrial automation temporal thresholding. In the first part of the paper are\nextracted some limitations of using the global optimum thresholding in gray\nlevel image binarization. In the second part of this paper are presented some\naspects of the dynamic optimum thresholding method for gray level image\nbinarization. Starting from classic methods of global and dynamic optimal\nthresholding of the gray level images in the next section are introduced the\nconcepts of temporal histogram and temporal thresholding. In the final section\nare presented some practical aspects of the temporal thresholding method in\nartificial vision applications form the moving scene in robotic automation\nclass; pointing out the influence of the acquisition frequency on the methods\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 16:57:08 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Hossu", "Andrei", ""], ["Andone", "Daniela", ""]]}, {"id": "1512.03740", "submitter": "Zhenzhong Lan", "authors": "Zhenzhong Lan, Shoou-I Yu, Alexander G. Hauptmann", "title": "Improving Human Activity Recognition Through Ranking and Re-ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two well-motivated ranking-based methods to enhance the\nperformance of current state-of-the-art human activity recognition systems.\nFirst, as an improvement over the classic power normalization method, we\npropose a parameter-free ranking technique called rank normalization (RaN). RaN\nnormalizes each dimension of the video features to address the sparse and\nbursty distribution problems of Fisher Vectors and VLAD. Second, inspired by\ncurriculum learning, we introduce a training-free re-ranking technique called\nmulti-class iterative re-ranking (MIR). MIR captures relationships among action\nclasses by separating easy and typical videos from difficult ones and\nre-ranking the prediction scores of classifiers accordingly. We demonstrate\nthat our methods significantly improve the performance of state-of-the-art\nmotion features on six real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 17:41:53 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Lan", "Zhenzhong", ""], ["Yu", "Shoou-I", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1512.03958", "submitter": "Lior Wolf", "authors": "Guy Lev and Gil Sadeh and Benjamin Klein and Lior Wolf", "title": "RNN Fisher Vectors for Action Recognition and Image Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have had considerable success in classifying\nand predicting sequences. We demonstrate that RNNs can be effectively used in\norder to encode sequences and provide effective representations. The\nmethodology we use is based on Fisher Vectors, where the RNNs are the\ngenerative probabilistic models and the partial derivatives are computed using\nbackpropagation. State of the art results are obtained in two central but\ndistant tasks, which both rely on sequences: video action recognition and image\nannotation. We also show a surprising transfer learning result from the task of\nimage annotation to the task of video action recognition.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 20:24:43 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Lev", "Guy", ""], ["Sadeh", "Gil", ""], ["Klein", "Benjamin", ""], ["Wolf", "Lior", ""]]}, {"id": "1512.03980", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mahdyar Ravanbakhsh, Hossein Mousavi, Mohammad Rastegari, Vittorio\n  Murino, Larry S. Davis", "title": "Action Recognition with Image Based CNN Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of human actions consist of complex temporal compositions of more simple\nactions. Action recognition tasks usually relies on complex handcrafted\nstructures as features to represent the human action model. Convolutional\nNeural Nets (CNN) have shown to be a powerful tool that eliminate the need for\ndesigning handcrafted features. Usually, the output of the last layer in CNN (a\nlayer before the classification layer -known as fc7) is used as a generic\nfeature for images. In this paper, we show that fc7 features, per se, can not\nget a good performance for the task of action recognition, when the network is\ntrained only on images. We present a feature structure on top of fc7 features,\nwhich can capture the temporal variation in a video. To represent the temporal\ncomponents, which is needed to capture motion information, we introduced a\nhierarchical structure. The hierarchical model enables to capture sub-actions\nfrom a complex action. At the higher levels of the hierarchy, it represents a\ncoarse capture of action sequence and lower levels represent fine action\nelements. Furthermore, we introduce a method for extracting key-frames using\nbinary coding of each frame in a video, which helps to improve the performance\nof our hierarchical model. We experimented our method on several action\ndatasets and show that our method achieves superior results compared to other\nstate-of-the-arts methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 00:17:24 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""], ["Mousavi", "Hossein", ""], ["Rastegari", "Mohammad", ""], ["Murino", "Vittorio", ""], ["Davis", "Larry S.", ""]]}, {"id": "1512.03993", "submitter": "Meera Hahn", "authors": "Meera Hahn, Si Chen and Afshin Dehghan", "title": "Deep Tracking: Visual Tracking Using Deep Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a discriminatively trained deep convolutional network\nfor the task of visual tracking. Our tracker utilizes both motion and\nappearance features that are extracted from a pre-trained dual stream deep\nconvolution network. We show that the features extracted from our dual-stream\nnetwork can provide rich information about the target and this leads to\ncompetitive performance against state of the art tracking methods on a visual\ntracking benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 02:58:56 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Hahn", "Meera", ""], ["Chen", "Si", ""], ["Dehghan", "Afshin", ""]]}, {"id": "1512.04065", "submitter": "Yannis Kalantidis", "authors": "Yannis Kalantidis, Clayton Mellina, Simon Osindero", "title": "Cross-dimensional Weighting for Aggregated Deep Convolutional Features", "comments": "Accepted for publications at the 4th Workshop on Web-scale Vision and\n  Social Media (VSM), ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and straightforward way of creating powerful image\nrepresentations via cross-dimensional weighting and aggregation of deep\nconvolutional neural network layer outputs. We first present a generalized\nframework that encompasses a broad family of approaches and includes\ncross-dimensional pooling and weighting steps. We then propose specific\nnon-parametric schemes for both spatial- and channel-wise weighting that boost\nthe effect of highly active spatial responses and at the same time regulate\nburstiness effects. We experiment on different public datasets for image search\nand show that our approach outperforms the current state-of-the-art for\napproaches based on pre-trained networks. We also provide an easy-to-use, open\nsource implementation that reproduces our results.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 15:16:02 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 02:14:18 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Kalantidis", "Yannis", ""], ["Mellina", "Clayton", ""], ["Osindero", "Simon", ""]]}, {"id": "1512.04077", "submitter": "Mojmir Mutny", "authors": "Mojmir Mutny, Rahul Nair and Jens-Malte Gottfried", "title": "Learning the Correction for Multi-Path Deviations in Time-of-Flight\n  Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multipath effect in Time-of-Flight(ToF) cameras still remains to be a\nchallenging problem that hinders further processing of 3D data information.\nBased on the evidence from previous literature, we explored the possibility of\nusing machine learning techniques to correct this effect. Firstly, we created\ntwo new datasets of of ToF images rendered via ToF simulator of LuxRender.\nThese two datasets contain corners in multiple orientations and with different\nmaterial properties. We chose scenes with corners as multipath effects are most\npronounced in corners. Secondly, we used this dataset to construct a learning\nmodel to predict real valued corrections to the ToF data using Random Forests.\nWe found out that in our smaller dataset we were able to predict real valued\ncorrection and improve the quality of depth images significantly by removing\nmultipath bias. With our algorithm, we improved relative per-pixel error from\naverage value of 19% to 3%. Additionally, variance of the error was lowered by\nan order of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 16:31:14 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 11:17:58 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Mutny", "Mojmir", ""], ["Nair", "Rahul", ""], ["Gottfried", "Jens-Malte", ""]]}, {"id": "1512.04086", "submitter": "Neeraj Kumar", "authors": "Neeraj Kumar, Animesh Karmakar, Ranti Dev Sharma, Abhinav Mittal and\n  Amit Sethi", "title": "Deep Learning-Based Image Kernel for Inductive Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to classify images from target classes with a small\nnumber of training examples based on transfer learning from non-target classes.\nWithout using any more information than class labels for samples from\nnon-target classes, we train a Siamese net to estimate the probability of two\nimages to belong to the same class. With some post-processing, output of the\nSiamese net can be used to form a gram matrix of a Mercer kernel. Coupled with\na support vector machine (SVM), such a kernel gave reasonable classification\naccuracy on target classes without any fine-tuning. When the Siamese net was\nonly partially fine-tuned using a small number of samples from the target\nclasses, the resulting classifier outperformed the state-of-the-art and other\nalternatives. We share class separation capabilities and insights into the\nlearning process of such a kernel on MNIST, Dogs vs. Cats, and CIFAR-10\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 17:12:45 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 06:59:54 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2016 09:51:27 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Kumar", "Neeraj", ""], ["Karmakar", "Animesh", ""], ["Sharma", "Ranti Dev", ""], ["Mittal", "Abhinav", ""], ["Sethi", "Amit", ""]]}, {"id": "1512.04103", "submitter": "Yaser Souri", "authors": "Yaser Souri, Erfan Noury, Ehsan Adeli", "title": "Deep Relative Attributes", "comments": "ACCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attributes are great means of describing images or scenes, in a way\nboth humans and computers understand. In order to establish a correspondence\nbetween images and to be able to compare the strength of each property between\nimages, relative attributes were introduced. However, since their introduction,\nhand-crafted and engineered features were used to learn increasingly complex\nmodels for the problem of relative attributes. This limits the applicability of\nthose methods for more realistic cases. We introduce a deep neural network\narchitecture for the task of relative attribute prediction. A convolutional\nneural network (ConvNet) is adopted to learn the features by including an\nadditional layer (ranking layer) that learns to rank the images based on these\nfeatures. We adopt an appropriate ranking loss to train the whole network in an\nend-to-end fashion. Our proposed method outperforms the baseline and\nstate-of-the-art methods in relative attribute prediction on various coarse and\nfine-grained datasets. Our qualitative results along with the visualization of\nthe saliency maps show that the network is able to learn effective features for\neach specific attribute. Source code of the proposed method is available at\nhttps://github.com/yassersouri/ghiaseddin.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 19:10:16 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 08:21:43 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Souri", "Yaser", ""], ["Noury", "Erfan", ""], ["Adeli", "Ehsan", ""]]}, {"id": "1512.04115", "submitter": "Qifei Wang", "authors": "Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy", "title": "Unsupervised Temporal Segmentation of Repetitive Human Actions Based on\n  Kinematic Modeling and Frequency Analysis", "comments": "9 pages, International Conference on 3D Vision 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for temporal segmentation of human\nrepetitive actions based on frequency analysis of kinematic parameters,\nzero-velocity crossing detection, and adaptive k-means clustering. Since the\nhuman motion data may be captured with different modalities which have\ndifferent temporal sampling rate and accuracy (e.g., optical motion capture\nsystems vs. Microsoft Kinect), we first apply a generic full-body kinematic\nmodel with an unscented Kalman filter to convert the motion data into a unified\nrepresentation that is robust to noise. Furthermore, we extract the most\nrepresentative kinematic parameters via the primary frequency analysis. The\nsequences are segmented based on zero-velocity crossing of the selected\nparameters followed by an adaptive k-means clustering to identify the\nrepetition segments. Experimental results demonstrate that for the motion data\ncaptured by both the motion capture system and the Microsoft Kinect, our\nproposed algorithm obtains robust segmentation of repetitive action sequences.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 20:08:43 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Wang", "Qifei", ""], ["Kurillo", "Gregorij", ""], ["Ofli", "Ferda", ""], ["Bajcsy", "Ruzena", ""]]}, {"id": "1512.04118", "submitter": "Jiongxin Liu", "authors": "Jiongxin Liu, Yinxiao Li, Peter Allen, Peter Belhumeur", "title": "Articulated Pose Estimation Using Hierarchical Exemplar-Based Models", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based models have achieved great success on localizing the parts of\nsemi-rigid objects. However, their efficacy on highly articulated objects such\nas humans is yet to be explored. Inspired by hierarchical object representation\nand recent application of Deep Convolutional Neural Networks (DCNNs) on human\npose estimation, we propose a novel formulation that incorporates both\nhierarchical exemplar-based models and DCNNs in the spatial terms.\nSpecifically, we obtain more expressive spatial models by assuming independence\nbetween exemplars at different levels in the hierarchy; we also obtain stronger\nspatial constraints by inferring the spatial relations between parts at the\nsame level. As our method strikes a good balance between expressiveness and\nstrength of spatial models, it is both effective and generalizable, achieving\nstate-of-the-art results on different benchmarks: Leeds Sports Dataset and\nCUB-200-2011.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 20:37:10 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Liu", "Jiongxin", ""], ["Li", "Yinxiao", ""], ["Allen", "Peter", ""], ["Belhumeur", "Peter", ""]]}, {"id": "1512.04133", "submitter": "George Cushen", "authors": "George Cushen", "title": "A Person Re-Identification System For Mobile Devices", "comments": "Appearing in Proceedings of the 11th IEEE/ACM International\n  Conference on Signal Image Technology & Internet Systems (SITIS 2015)", "journal-ref": null, "doi": "10.1109/SITIS.2015.96", "report-no": null, "categories": "cs.CV cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is a critical security task for recognizing a person\nacross spatially disjoint sensors. Previous work can be computationally\nintensive and is mainly based on low-level cues extracted from RGB data and\nimplemented on a PC for a fixed sensor network (such as traditional CCTV). We\npresent a practical and efficient framework for mobile devices (such as smart\nphones and robots) where high-level semantic soft biometrics are extracted from\nRGB and depth data. By combining these cues, our approach attempts to provide\nrobustness to noise, illumination, and minor variations in clothing. This\nmobile approach may be particularly useful for the identification of persons in\nareas ill-served by fixed sensors or for tasks where the sensor position and\ndirection need to dynamically adapt to a target. Results on the BIWI dataset\nare preliminary but encouraging. Further evaluation and demonstration of the\nsystem will be available on our website.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 22:33:17 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Cushen", "George", ""]]}, {"id": "1512.04134", "submitter": "Qifei Wang", "authors": "Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy", "title": "Evaluation of Pose Tracking Accuracy in the First and Second Generations\n  of Microsoft Kinect", "comments": "10 pages, IEEE International Conference on Healthcare Informatics\n  2015 (ICHI 2015)", "journal-ref": null, "doi": "10.1109/ICHI.2015.54", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microsoft Kinect camera and its skeletal tracking capabilities have been\nembraced by many researchers and commercial developers in various applications\nof real-time human movement analysis. In this paper, we evaluate the accuracy\nof the human kinematic motion data in the first and second generation of the\nKinect system, and compare the results with an optical motion capture system.\nWe collected motion data in 12 exercises for 10 different subjects and from\nthree different viewpoints. We report on the accuracy of the joint localization\nand bone length estimation of Kinect skeletons in comparison to the motion\ncapture. We also analyze the distribution of the joint localization offsets by\nfitting a mixture of Gaussian and uniform distribution models to determine the\noutliers in the Kinect motion data. Our analysis shows that overall Kinect 2\nhas more robust and more accurate tracking of human pose as compared to Kinect\n1.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 22:58:55 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wang", "Qifei", ""], ["Kurillo", "Gregorij", ""], ["Ofli", "Ferda", ""], ["Bajcsy", "Ruzena", ""]]}, {"id": "1512.04143", "submitter": "Sean Bell", "authors": "Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick", "title": "Inside-Outside Net: Detecting Objects in Context with Skip Pooling and\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that contextual and multi-scale representations are\nimportant for accurate visual recognition. In this paper we present the\nInside-Outside Net (ION), an object detector that exploits information both\ninside and outside the region of interest. Contextual information outside the\nregion of interest is integrated using spatial recurrent neural networks.\nInside, we use skip pooling to extract information at multiple scales and\nlevels of abstraction. Through extensive experiments we evaluate the design\nspace and provide readers with an overview of what tricks of the trade are\nimportant. ION improves state-of-the-art on PASCAL VOC 2012 object detection\nfrom 73.9% to 76.4% mAP. On the new and more challenging MS COCO dataset, we\nimprove state-of-art-the from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection\nChallenge, our ION model won the Best Student Entry and finished 3rd place\noverall. As intuition suggests, our detection results provide strong evidence\nthat context and multi-scale representations improve small object detection.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 00:37:31 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Bell", "Sean", ""], ["Zitnick", "C. Lawrence", ""], ["Bala", "Kavita", ""], ["Girshick", "Ross", ""]]}, {"id": "1512.04150", "submitter": "Bolei Zhou", "authors": "Bolei Zhou and Aditya Khosla and Agata Lapedriza and Aude Oliva and\n  Antonio Torralba", "title": "Learning Deep Features for Discriminative Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we revisit the global average pooling layer proposed in [13],\nand shed light on how it explicitly enables the convolutional neural network to\nhave remarkable localization ability despite being trained on image-level\nlabels. While this technique was previously proposed as a means for\nregularizing training, we find that it actually builds a generic localizable\ndeep representation that can be applied to a variety of tasks. Despite the\napparent simplicity of global average pooling, we are able to achieve 37.1%\ntop-5 error for object localization on ILSVRC 2014, which is remarkably close\nto the 34.2% top-5 error achieved by a fully supervised CNN approach. We\ndemonstrate that our network is able to localize the discriminative image\nregions on a variety of tasks despite not being trained for them\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 01:32:33 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Zhou", "Bolei", ""], ["Khosla", "Aditya", ""], ["Lapedriza", "Agata", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""]]}, {"id": "1512.04205", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson and Steven L. Brunton and J. Nathan Kutz", "title": "Compressed Dynamic Mode Decomposition for Background Modeling", "comments": "Preprint submitted to Journal of Real-Time Image Processing", "journal-ref": null, "doi": "10.1007/s11554-016-0655-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the method of compressed dynamic mode decomposition (cDMD) for\nbackground modeling. The dynamic mode decomposition (DMD) is a regression\ntechnique that integrates two of the leading data analysis methods in use\ntoday: Fourier transforms and singular value decomposition. Borrowing ideas\nfrom compressed sensing and matrix sketching, cDMD eases the computational\nworkload of high resolution video processing. The key principal of cDMD is to\nobtain the decomposition on a (small) compressed matrix representation of the\nvideo feed. Hence, the cDMD algorithm scales with the intrinsic rank of the\nmatrix, rather then the size of the actual video (data) matrix. Selection of\nthe optimal modes characterizing the background is formulated as a\nsparsity-constrained sparse coding problem. Our results show, that the quality\nof the resulting background model is competitive, quantified by the F-measure,\nRecall and Precision. A GPU (graphics processing unit) accelerated\nimplementation is also presented which further boosts the computational\nefficiency of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 07:27:07 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 03:09:25 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Brunton", "Steven L.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1512.04208", "submitter": "Chenxia Wu", "authors": "Chenxia Wu, Jiemi Zhang, Bart Selman, Silvio Savarese, Ashutosh Saxena", "title": "Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten\n  Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robotic system that watches a human using a Kinect v2 RGB-D\nsensor, detects what he forgot to do while performing an activity, and if\nnecessary reminds the person using a laser pointer to point out the related\nobject. Our simple setup can be easily deployed on any assistive robot.\n  Our approach is based on a learning algorithm trained in a purely\nunsupervised setting, which does not require any human annotations. This makes\nour approach scalable and applicable to variant scenarios. Our model learns the\naction/object co-occurrence and action temporal relations in the activity, and\nuses the learned rich relationships to infer the forgotten action and the\nrelated object. We show that our approach not only improves the unsupervised\naction segmentation and action cluster assignment performance, but also\neffectively detects the forgotten actions on a challenging human activity RGB-D\nvideo dataset. In robotic experiments, we show that our robot is able to remind\npeople of forgotten actions successfully.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 07:50:22 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Wu", "Chenxia", ""], ["Zhang", "Jiemi", ""], ["Selman", "Bart", ""], ["Savarese", "Silvio", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1512.04219", "submitter": "Thomas Ruland", "authors": "Thomas Ruland", "title": "On the Relation between two Rotation Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their work \"Global Optimization through Rotation Space Search\", Richard\nHartley and Fredrik Kahl introduce a global optimization strategy for problems\nin geometric computer vision, based on rotation space search using a\nbranch-and-bound algorithm. In its core, Lemma 2 of their publication is the\nimportant foundation for a class of global optimization algorithms, which is\nadopted over a wide range of problems in subsequent publications. This lemma\nrelates a metric on rotations represented by rotation matrices with a metric on\nrotations in axis-angle representation. This work focuses on a proof for this\nrelationship, which is based on Rodrigues' Rotation Theorem for the composition\nof rotations in axis-angle representation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 08:50:44 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Ruland", "Thomas", ""]]}, {"id": "1512.04295", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Luca Benini", "title": "Origami: A 803 GOp/s/W Convolutional Network Accelerator", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2592330", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ever increasing number of computer vision and image/video processing\nchallenges are being approached using deep convolutional neural networks,\nobtaining state-of-the-art results in object recognition and detection,\nsemantic segmentation, action recognition, optical flow and superresolution.\nHardware acceleration of these algorithms is essential to adopt these\nimprovements in embedded and mobile computer vision systems. We present a new\narchitecture, design and implementation as well as the first reported silicon\nmeasurements of such an accelerator, outperforming previous work in terms of\npower-, area- and I/O-efficiency. The manufactured device provides up to 196\nGOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power\nefficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it\nthe first architecture scalable to TOp/s performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 13:06:43 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 22:56:41 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "1512.04354", "submitter": "Stefane Paris", "authors": "St\\'efane Paris (QGAR)", "title": "A proposal project for a blind image quality assessment by learning\n  distortions from the full reference image quality assessments", "comments": "International Workshop on Quality of Multimedia Experience, 2012,\n  Melbourne, Australia", "journal-ref": null, "doi": "10.1109/QoMEX.2012.6263876", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper presents a perspective plan to build a null reference image\nquality assessment. Its main goal is to deliver both the objective score and\nthe distortion map for a given distorted image without the knowledge of its\nreference image.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 12:21:04 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Paris", "St\u00e9fane", "", "QGAR"]]}, {"id": "1512.04407", "submitter": "Arjun Chandrasekaran", "authors": "Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit\n  Bansal, Dhruv Batra, C. Lawrence Zitnick and Devi Parikh", "title": "We Are Humor Beings: Understanding and Predicting Visual Humor", "comments": "17 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humor is an integral part of human lives. Despite being tremendously\nimpactful, it is perhaps surprising that we do not have a detailed\nunderstanding of humor yet. As interactions between humans and AI systems\nincrease, it is imperative that these systems are taught to understand\nsubtleties of human expressions such as humor. In this work, we are interested\nin the question - what content in a scene causes it to be funny? As a first\nstep towards understanding visual humor, we analyze the humor manifested in\nabstract scenes and design computational models for them. We collect two\ndatasets of abstract scenes that facilitate the study of humor at both the\nscene-level and the object-level. We analyze the funny scenes and explore the\ndifferent types of humor depicted in them via human studies. We model two tasks\nthat we believe demonstrate an understanding of some aspects of visual humor.\nThe tasks involve predicting the funniness of a scene and altering the\nfunniness of a scene. We show that our models perform well quantitatively, and\nqualitatively through human studies. Our datasets are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 16:59:35 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 02:12:49 GMT"}, {"version": "v3", "created": "Sun, 10 Apr 2016 22:15:43 GMT"}, {"version": "v4", "created": "Thu, 5 May 2016 21:36:13 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Chandrasekaran", "Arjun", ""], ["Vijayakumar", "Ashwin K.", ""], ["Antol", "Stanislaw", ""], ["Bansal", "Mohit", ""], ["Batra", "Dhruv", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""]]}, {"id": "1512.04412", "submitter": "Kaiming He", "authors": "Jifeng Dai, Kaiming He, Jian Sun", "title": "Instance-aware Semantic Segmentation via Multi-task Network Cascades", "comments": "Tech report. 1st-place winner of MS COCO 2015 segmentation\n  competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation research has recently witnessed rapid progress, but\nmany leading methods are unable to identify object instances. In this paper, we\npresent Multi-task Network Cascades for instance-aware semantic segmentation.\nOur model consists of three networks, respectively differentiating instances,\nestimating masks, and categorizing objects. These networks form a cascaded\nstructure, and are designed to share their convolutional features. We develop\nan algorithm for the nontrivial end-to-end training of this causal, cascaded\nstructure. Our solution is a clean, single-step training framework and can be\ngeneralized to cascades that have more stages. We demonstrate state-of-the-art\ninstance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our\nmethod takes only 360ms testing an image using VGG-16, which is two orders of\nmagnitude faster than previous systems for this challenging problem. As a by\nproduct, our method also achieves compelling object detection results which\nsurpass the competitive Fast/Faster R-CNN systems.\n  The method described in this paper is the foundation of our submissions to\nthe MS COCO 2015 segmentation competition, where we won the 1st place.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 17:17:23 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Dai", "Jifeng", ""], ["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1512.04418", "submitter": "Chia-Chen Lee", "authors": "Chia-Chen Lee, Wen-Liang Hwang", "title": "Sparse Representation of a Blur Kernel for Blind Image Restoration", "comments": "11 pages, 37 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blind image restoration is a non-convex problem which involves restoration of\nimages from an unknown blur kernel. The factors affecting the performance of\nthis restoration are how much prior information about an image and a blur\nkernel are provided and what algorithm is used to perform the restoration task.\nPrior information on images is often employed to restore the sharpness of the\nedges of an image. By contrast, no consensus is still present regarding what\nprior information to use in restoring from a blur kernel due to complex image\nblurring processes. In this paper, we propose modelling of a blur kernel as a\nsparse linear combinations of basic 2-D patterns. Our approach has a\ncompetitive edge over the existing blur kernel modelling methods because our\nmethod has the flexibility to customize the dictionary design, which makes it\nwell-adaptive to a variety of applications. As a demonstration, we construct a\ndictionary formed by basic patterns derived from the Kronecker product of\nGaussian sequences. We also compare our results with those derived by other\nstate-of-the-art methods, in terms of peak signal to noise ratio (PSNR).\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 17:32:41 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Lee", "Chia-Chen", ""], ["Hwang", "Wen-Liang", ""]]}, {"id": "1512.04509", "submitter": "Kumar Eswaran Dr.", "authors": "K.Eswaran and K.Damodhar Rao", "title": "On non-iterative training of a neural classifier", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently an algorithm, was discovered, which separates points in n-dimension\nby planes in such a manner that no two points are left un-separated by at least\none plane{[}1-3{]}. By using this new algorithm we show that there are two ways\nof classification by a neural network, for a large dimension feature space,\nboth of which are non-iterative and deterministic. To demonstrate the power of\nboth these methods we apply them exhaustively to the classical pattern\nrecognition problem: The Fisher-Anderson's, IRIS flower data set and present\nthe results.\n  It is expected these methods will now be widely used for the training of\nneural networks for Deep Learning not only because of their non-iterative and\ndeterministic nature but also because of their efficiency and speed and will\nsupersede other classification methods which are iterative in nature and rely\non error minimization.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 20:44:12 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 04:32:01 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Eswaran", "K.", ""], ["Rao", "K. Damodhar", ""]]}, {"id": "1512.04582", "submitter": "Jan Egger", "authors": "Jan Egger, Harald Busse, Philipp Brandmaier, Daniel Seider, Matthias\n  Gawlitza, Steffen Strocka, Philip Voglreiter, Mark Dokter, Michael Hofmann,\n  Bernhard Kainz, Alexander Hann, Xiaojun Chen, Tuomas Alhonnoro, Mika Pollari,\n  Dieter Schmalstieg, Michael Moche", "title": "Interactive Volumetry Of Liver Ablation Zones", "comments": "18 pages, 15 figures, 8 tables, 57 references", "journal-ref": "Sci. Rep. 5, 15373; doi: 10.1038/srep15373 (2015)", "doi": "10.1038/srep15373", "report-no": null, "categories": "cs.CV cs.GR cs.HC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Percutaneous radiofrequency ablation (RFA) is a minimally invasive technique\nthat destroys cancer cells by heat. The heat results from focusing energy in\nthe radiofrequency spectrum through a needle. Amongst others, this can enable\nthe treatment of patients who are not eligible for an open surgery. However,\nthe possibility of recurrent liver cancer due to incomplete ablation of the\ntumor makes post-interventional monitoring via regular follow-up scans\nmandatory. These scans have to be carefully inspected for any conspicuousness.\nWithin this study, the RF ablation zones from twelve post-interventional CT\nacquisitions have been segmented semi-automatically to support the visual\ninspection. An interactive, graph-based contouring approach, which prefers\nspherically shaped regions, has been applied. For the quantitative and\nqualitative analysis of the algorithm's results, manual slice-by-slice\nsegmentations produced by clinical experts have been used as the gold standard\n(which have also been compared among each other). As evaluation metric for the\nstatistical validation, the Dice Similarity Coefficient (DSC) has been\ncalculated. The results show that the proposed tool provides lesion\nsegmentation with sufficient accuracy much faster than manual segmentation. The\nvisual feedback and interactivity make the proposed tool well suitable for the\nclinical workflow.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 08:14:32 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Egger", "Jan", ""], ["Busse", "Harald", ""], ["Brandmaier", "Philipp", ""], ["Seider", "Daniel", ""], ["Gawlitza", "Matthias", ""], ["Strocka", "Steffen", ""], ["Voglreiter", "Philip", ""], ["Dokter", "Mark", ""], ["Hofmann", "Michael", ""], ["Kainz", "Bernhard", ""], ["Hann", "Alexander", ""], ["Chen", "Xiaojun", ""], ["Alhonnoro", "Tuomas", ""], ["Pollari", "Mika", ""], ["Schmalstieg", "Dieter", ""], ["Moche", "Michael", ""]]}, {"id": "1512.04605", "submitter": "Marian-Andrei Rizoiu", "authors": "Marian-Andrei Rizoiu, Julien Velcin, St\\'ephane Lallich", "title": "Semantic-enriched Visual Vocabulary Construction in a Weakly Supervised\n  Context", "comments": null, "journal-ref": "M.-A. Rizoiu, J. Velcin, and S. Lallich, \"Semantic-enriched Visual\n  Vocabulary Construction in a Weakly Supervised Context,\" Intelligent Data\n  Analysis, vol. 19, iss. 1, pp. 161-185, 2015", "doi": "10.3233/IDA-140702", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the prevalent learning tasks involving images is content-based image\nclassification. This is a difficult task especially because the low-level\nfeatures used to digitally describe images usually capture little information\nabout the semantics of the images. In this paper, we tackle this difficulty by\nenriching the semantic content of the image representation by using external\nknowledge. The underlying hypothesis of our work is that creating a more\nsemantically rich representation for images would yield higher machine learning\nperformances, without the need to modify the learning algorithms themselves.\nThe external semantic information is presented under the form of non-positional\nimage labels, therefore positioning our work in a weakly supervised context.\nTwo approaches are proposed: the first one leverages the labels into the visual\nvocabulary construction algorithm, the result being dedicated visual\nvocabularies. The second approach adds a filtering phase as a pre-processing of\nthe vocabulary construction. Known positive and known negative sets are\nconstructed and features that are unlikely to be associated with the objects\ndenoted by the labels are filtered. We apply our proposition to the task of\ncontent-based image classification and we show that semantically enriching the\nimage representation yields higher classification performances than the\nbaseline representation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 23:45:34 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Rizoiu", "Marian-Andrei", ""], ["Velcin", "Julien", ""], ["Lallich", "St\u00e9phane", ""]]}, {"id": "1512.04636", "submitter": "Alexander Wong", "authors": "Ameneh Boroomand, Mohammad Javad Shafiee, Farzad Khalvati, Masoom A.\n  Haider, and Alexander Wong", "title": "Noise-Compensated, Bias-Corrected Diffusion Weighted Endorectal Magnetic\n  Resonance Imaging via a Stochastically Fully-Connected Joint Conditional\n  Random Field Model", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion weighted magnetic resonance imaging (DW-MR) is a powerful tool in\nimaging-based prostate cancer screening and detection. Endorectal coils are\ncommonly used in DW-MR imaging to improve the signal-to-noise ratio (SNR) of\nthe acquisition, at the expense of significant intensity inhomogeneities (bias\nfield) that worsens as we move away from the endorectal coil. The presence of\nbias field can have a significant negative impact on the accuracy of different\nimage analysis tasks, as well as prostate tumor localization, thus leading to\nincreased inter- and intra-observer variability. Retrospective bias correction\napproaches are introduced as a more efficient way of bias correction compared\nto the prospective methods such that they correct for both of the scanner and\nanatomy-related bias fields in MR imaging. Previously proposed retrospective\nbias field correction methods suffer from undesired noise amplification that\ncan reduce the quality of bias-corrected DW-MR image. Here, we propose a\nunified data reconstruction approach that enables joint compensation of bias\nfield as well as data noise in DW-MR imaging. The proposed noise-compensated,\nbias-corrected (NCBC) data reconstruction method takes advantage of a novel\nstochastically fully connected joint conditional random field (SFC-JCRF) model\nto mitigate the effects of data noise and bias field in the reconstructed MR\ndata. The proposed NCBC reconstruction method was tested on synthetic DW-MR\ndata, physical DW-phantom as well as real DW-MR data all acquired using\nendorectal MR coil. Both qualitative and quantitative analysis illustrated that\nthe proposed NCBC method can achieve improved image quality when compared to\nother tested bias correction methods. As such, the proposed NCBC method may\nhave potential as a useful retrospective approach for improving the consistency\nof image interpretations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 03:44:28 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 16:47:37 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Boroomand", "Ameneh", ""], ["Shafiee", "Mohammad Javad", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1512.04785", "submitter": "Phong Vo", "authors": "Phong D. Vo, Alexandru Ginsca, Herv\\'e Le Borgne, Adrian Popescu", "title": "On Deep Representation Learning from Noisy Web Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The keep-growing content of Web images may be the next important data source\nto scale up deep neural networks, which recently obtained a great success in\nthe ImageNet classification challenge and related tasks. This prospect,\nhowever, has not been validated on convolutional networks (convnet) -- one of\nbest performing deep models -- because of their supervised regime. While\nunsupervised alternatives are not so good as convnet in generalizing the\nlearned model to new domains, we use convnet to leverage semi-supervised\nrepresentation learning. Our approach is to use massive amounts of unlabeled\nand noisy Web images to train convnets as general feature detectors despite\nchallenges coming from data such as high level of mislabeled data, outliers,\nand data biases. Extensive experiments are conducted at several data scales,\ndifferent network architectures, and data reranking techniques. The learned\nrepresentations are evaluated on nine public datasets of various topics. The\nbest results obtained by our convnets, trained on 3.14 million Web images,\noutperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is\nclosing the gap with VGG-16. These prominent results suggest a budget solution\nto use deep learning in practice and motivate more research in semi-supervised\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 13:57:39 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 20:50:54 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Vo", "Phong D.", ""], ["Ginsca", "Alexandru", ""], ["Borgne", "Herv\u00e9 Le", ""], ["Popescu", "Adrian", ""]]}, {"id": "1512.04958", "submitter": "Sarfaraz Hussein", "authors": "Sarfaraz Hussein, Aileen Green, Arjun Watane, Georgios Papadakis,\n  Medhat Osman and Ulas Bagci", "title": "Context Driven Label Fusion for segmentation of Subcutaneous and\n  Visceral Fat in CT Volumes", "comments": "ISBI 2016 submission, 5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of adipose tissue (fat) from computed tomography (CT) scans is\nconducted mostly through manual or semi-automated image segmentation algorithms\nwith limited efficacy. In this work, we propose a completely unsupervised and\nautomatic method to identify adipose tissue, and then separate Subcutaneous\nAdipose Tissue (SAT) from Visceral Adipose Tissue (VAT) at the abdominal\nregion. We offer a three-phase pipeline consisting of (1) Initial boundary\nestimation using gradient points, (2) boundary refinement using Geometric\nMedian Absolute Deviation and Appearance based Local Outlier Scores (3) Context\ndriven label fusion using Conditional Random Fields (CRF) to obtain the final\nboundary between SAT and VAT. We evaluate the proposed method on 151 abdominal\nCT scans and obtain state-of-the-art 94% and 91% dice similarity scores for SAT\nand VAT segmentation, as well as significant reduction in fat quantification\nerror measure.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 21:02:32 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Hussein", "Sarfaraz", ""], ["Green", "Aileen", ""], ["Watane", "Arjun", ""], ["Papadakis", "Georgios", ""], ["Osman", "Medhat", ""], ["Bagci", "Ulas", ""]]}, {"id": "1512.05010", "submitter": "Dejan Slep\\v{c}ev", "authors": "Slav Kirov and Dejan Slep\\v{c}ev", "title": "Multiple penalized principal curves: analysis and computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding the one-dimensional structure in a given data\nset. In other words we consider ways to approximate a given measure (data) by\ncurves. We consider an objective functional whose minimizers are a\nregularization of principal curves and introduce a new functional which allows\nfor multiple curves. We prove the existence of minimizers and establish their\nbasic properties. We develop an efficient algorithm for obtaining (near)\nminimizers of the functional. While both of the functionals used are nonconvex,\nwe argue that enlarging the configuration space to allow for multiple curves\nleads to a simpler energy landscape with fewer undesirable (high-energy) local\nminima. Furthermore we note that the approach proposed is able to find the\none-dimensional structure even for data with considerable amount of noise.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 23:26:50 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 13:19:05 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Kirov", "Slav", ""], ["Slep\u010dev", "Dejan", ""]]}, {"id": "1512.05227", "submitter": "Yin Cui", "authors": "Yin Cui, Feng Zhou, Yuanqing Lin, Serge Belongie", "title": "Fine-grained Categorization and Dataset Bootstrapping using Deep Metric\n  Learning with Humans in the Loop", "comments": "10 pages, 9 figures, CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing fine-grained visual categorization methods often suffer from three\nchallenges: lack of training data, large number of fine-grained categories, and\nhigh intraclass vs. low inter-class variance. In this work we propose a generic\niterative framework for fine-grained categorization and dataset bootstrapping\nthat handles these three challenges. Using deep metric learning with humans in\nthe loop, we learn a low dimensional feature embedding with anchor points on\nmanifolds for each category. These anchor points capture intra-class variances\nand remain discriminative between classes. In each round, images with high\nconfidence scores from our model are sent to humans for labeling. By comparing\nwith exemplar images, labelers mark each candidate image as either a \"true\npositive\" or a \"false positive\". True positives are added into our current\ndataset and false positives are regarded as \"hard negatives\" for our metric\nlearning model. Then the model is retrained with an expanded dataset and hard\nnegatives for the next round. To demonstrate the effectiveness of the proposed\nframework, we bootstrap a fine-grained flower dataset with 620 categories from\nInstagram images. The proposed deep metric learning scheme is evaluated on both\nour dataset and the CUB-200-2001 Birds dataset. Experimental evaluations show\nsignificant performance gain using dataset bootstrapping and demonstrate\nstate-of-the-art results achieved by the proposed deep metric learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 16:14:22 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 04:34:13 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Cui", "Yin", ""], ["Zhou", "Feng", ""], ["Lin", "Yuanqing", ""], ["Belongie", "Serge", ""]]}, {"id": "1512.05246", "submitter": "Calvin Murdock", "authors": "Calvin Murdock, Zhen Li, Howard Zhou, Tom Duerig", "title": "Blockout: Dynamic Model Selection for Hierarchical Deep Networks", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2016.283", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep architectures for image classification--even those that are trained\nto classify a large number of diverse categories--learn shared image\nrepresentations with a single model. Intuitively, however, categories that are\nmore similar should share more information than those that are very different.\nWhile hierarchical deep networks address this problem by learning separate\nfeatures for subsets of related categories, current implementations require\nsimplified models using fixed architectures specified via heuristic clustering\nmethods. Instead, we propose Blockout, a method for regularization and model\nselection that simultaneously learns both the model architecture and\nparameters. A generalization of Dropout, our approach gives a novel\nparametrization of hierarchical architectures that allows for structure\nlearning via back-propagation. To demonstrate its utility, we evaluate Blockout\non the CIFAR and ImageNet datasets, demonstrating improved classification\naccuracy, better regularization performance, faster training, and the clear\nemergence of hierarchical network structures.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 16:58:36 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Murdock", "Calvin", ""], ["Li", "Zhen", ""], ["Zhou", "Howard", ""], ["Duerig", "Tom", ""]]}, {"id": "1512.05278", "submitter": "Zhuo Hui", "authors": "Zhuo Hui, Aswin C Sankaranarayanan", "title": "Shape and Spatially-Varying Reflectance Estimation From Virtual\n  Exemplars", "comments": "PAMI minor revision. arXiv admin note: substantial text overlap with\n  arXiv:1503.04265", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of estimating the shape of objects that\nexhibit spatially-varying reflectance. We assume that multiple images of the\nobject are obtained under a fixed view-point and varying illumination, i.e.,\nthe setting of photometric stereo. At the core of our techniques is the\nassumption that the BRDF at each pixel lies in the non-negative span of a known\nBRDF dictionary.This assumption enables a per-pixel surface normal and BRDF\nestimation framework that is computationally tractable and requires no\ninitialization in spite of the underlying problem being non-convex. Our\nestimation framework first solves for the surface normal at each pixel using a\nvariant of example-based photometric stereo. We design an efficient multi-scale\nsearch strategy for estimating the surface normal and subsequently, refine this\nestimate using a gradient descent procedure. Given the surface normal estimate,\nwe solve for the spatially-varying BRDF by constraining the BRDF at each pixel\nto be in the span of the BRDF dictionary, here, we use additional priors to\nfurther regularize the solution. A hallmark of our approach is that it does not\nrequire iterative optimization techniques nor the need for careful\ninitialization, both of which are endemic to most state-of-the-art techniques.\nWe showcase the performance of our technique on a wide range of simulated and\nreal scenes where we outperform competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 18:48:38 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 22:06:48 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 00:10:38 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Hui", "Zhuo", ""], ["Sankaranarayanan", "Aswin C", ""]]}, {"id": "1512.05300", "submitter": "Evgeniya Ustinova", "authors": "Evgeniya Ustinova, Yaroslav Ganin, Victor Lempitsky", "title": "Multiregion Bilinear Convolutional Neural Networks for Person\n  Re-Identification", "comments": "in AVSS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new architecture for person re-identification. As\nthe task of re-identification is inherently associated with embedding learning\nand non-rigid appearance description, our architecture is based on the deep\nbilinear convolutional network (Bilinear-CNN) that has been proposed recently\nfor fine-grained classification of highly non-rigid objects. While the last\nstages of the original Bilinear-CNN architecture completely removes the\ngeometric information from consideration by performing orderless pooling, we\nobserve that a better embedding can be learned by performing bilinear pooling\nin a more local way, where each pooling is confined to a predefined region. Our\narchitecture thus represents a compromise between traditional convolutional\nnetworks and bilinear CNNs and strikes a balance between rigid matching and\ncompletely ignoring spatial information.\n  We perform the experimental validation of the new architecture on the three\npopular benchmark datasets (Market-1501, CUHK01, CUHK03), comparing it to\nbaselines that include Bilinear-CNN as well as prior art. The new architecture\noutperforms the baseline on all three datasets, while performing better than\nstate-of-the-art on two out of three. The code and the pretrained models of the\napproach can be found at https://github.com/madkn/MultiregionBilinearCNN-ReId.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 19:45:37 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 17:57:05 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 14:58:59 GMT"}, {"version": "v4", "created": "Fri, 29 Jul 2016 20:09:47 GMT"}, {"version": "v5", "created": "Wed, 6 Sep 2017 14:38:55 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ustinova", "Evgeniya", ""], ["Ganin", "Yaroslav", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1512.05421", "submitter": "Alexander Wong", "authors": "Jason Deglint, Farnoud Kazemzadeh, Daniel Cho, David A. Clausi, and\n  Alexander Wong", "title": "Numerical Demultiplexing of Color Image Sensor Measurements via\n  Non-linear Random Forest Modeling", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simultaneous capture of imaging data at multiple wavelengths across the\nelectromagnetic spectrum is highly challenging, requiring complex and costly\nmultispectral image sensors. In this study, we introduce a comprehensive\nframework for performing simultaneous multispectral imaging using conventional\nimage sensors with color filter arrays via numerical demultiplexing of the\ncolor image sensor measurements. A numerical forward model characterizing the\nformation of sensor measurements from light spectra hitting the sensor is\nconstructed based on a comprehensive spectral characterization of the sensor. A\nnumerical demultiplexer is then learned via non-linear random forest modeling\nbased on the forward model. Given the learned numerical demultiplexer, one can\nthen demultiplex simultaneously-acquired measurements made by the image sensor\ninto reflectance intensities at discrete selectable wavelengths, resulting in a\nhigher resolution reflectance spectrum. Simulation and real-world experimental\nresults demonstrate the efficacy of such a method for simultaneous\nmultispectral imaging.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 00:22:32 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Deglint", "Jason", ""], ["Kazemzadeh", "Farnoud", ""], ["Cho", "Daniel", ""], ["Clausi", "David A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1512.05430", "submitter": "Qian Yu", "authors": "Qian Yu, Christian Szegedy, Martin C. Stumpe, Liron Yatziv, Vinay\n  Shet, Julian Ibarz, Sacha Arnoud", "title": "Large Scale Business Discovery from Street Level Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search with local intent is becoming increasingly useful due to the\npopularity of the mobile device. The creation and maintenance of accurate\nlistings of local businesses worldwide is time consuming and expensive. In this\npaper, we propose an approach to automatically discover businesses that are\nvisible on street level imagery. Precise business store front detection enables\naccurate geo-location of businesses, and further provides input for business\ncategorization, listing generation, etc. The large variety of business\ncategories in different countries makes this a very challenging problem.\nMoreover, manual annotation is prohibitive due to the scale of this problem. We\npropose the use of a MultiBox based approach that takes input image pixels and\ndirectly outputs store front bounding boxes. This end-to-end learning approach\ninstead preempts the need for hand modeling either the proposal generation\nphase or the post-processing phase, leveraging large labelled training\ndatasets. We demonstrate our approach outperforms the state of the art\ndetection techniques with a large margin in terms of performance and run-time\nefficiency. In the evaluation, we show this approach achieves human accuracy in\nthe low-recall settings. We also provide an end-to-end evaluation of business\ndiscovery in the real world.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 01:15:11 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 07:24:29 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Yu", "Qian", ""], ["Szegedy", "Christian", ""], ["Stumpe", "Martin C.", ""], ["Yatziv", "Liron", ""], ["Shet", "Vinay", ""], ["Ibarz", "Julian", ""], ["Arnoud", "Sacha", ""]]}, {"id": "1512.05586", "submitter": "Zhouye Chen", "authors": "Zhouye Chen, Adrian Basarab, Denis Kouam\\'e", "title": "Reconstruction of Enhanced Ultrasound Images From Compressed\n  Measurements Using Simultaneous Direction Method of Multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution ultrasound image reconstruction from a reduced number of\nmeasurements is of great interest in ultrasound imaging, since it could enhance\nboth the frame rate and image resolution. Compressive deconvolution, combining\ncompressed sensing and image deconvolution, represents an interesting\npossibility to consider this challenging task. The model of compressive\ndeconvolution includes, in addition to the compressive sampling matrix, a 2D\nconvolution operator carrying the information on the system point spread\nfunction. Through this model, the resolution of reconstructed ultrasound images\nfrom compressed measurements mainly depends on three aspects: the acquisition\nsetup, i.e. the incoherence of the sampling matrix, the image regularization,\ni.e. the sparsity prior, and the optimization technique. In this paper, we\nmainly focused on the last two aspects. We proposed a novel simultaneous\ndirection method of multipliers-based optimization scheme to invert the linear\nmodel, including two regularization terms expressing the sparsity of the RF\nimages in a given basis and the generalized Gaussian statistical assumption on\ntissue reflectivity functions. The performance of the method is evaluated on\nboth simulated and in vivo data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 14:04:21 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Chen", "Zhouye", ""], ["Basarab", "Adrian", ""], ["Kouam\u00e9", "Denis", ""]]}, {"id": "1512.05653", "submitter": "Amelia Carolina Sparavigna", "authors": "A.C. Sparavigna and R. Marazzato", "title": "Effects of GIMP Retinex Filtering Evaluated by the Image Entropy", "comments": "Keywords: Image Processing, Foggy Images, Retinex, Shannon Entropy,\n  Generalized Entropies, Kaniadakis Entropy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A GIMP Retinex filtering can be used for enhancing images, with good results\non foggy images, as recently discussed. Since this filter has some parameters\nthat can be adjusted to optimize the output image, several approaches can be\ndecided according to desired results. Here, as a criterion for optimizing the\nfiltering parameters, we consider the maximization of the image entropy. We\nuse, besides the Shannon entropy, also a generalized entropy.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 10:45:17 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Sparavigna", "A. C.", ""], ["Marazzato", "R.", ""]]}, {"id": "1512.05830", "submitter": "Zhouchen Lin", "authors": "Li Shen and Zhouchen Lin and Qingming Huang", "title": "Relay Backpropagation for Effective Learning of Deep Convolutional\n  Neural Networks", "comments": "Technical report for our submissions to the ILSVRC 2015 Scene\n  Classification Challenge, where we won the first place", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning deeper convolutional neural networks becomes a tendency in recent\nyears. However, many empirical evidences suggest that performance improvement\ncannot be gained by simply stacking more layers. In this paper, we consider the\nissue from an information theoretical perspective, and propose a novel method\nRelay Backpropagation, that encourages the propagation of effective information\nthrough the network in training stage. By virtue of the method, we achieved the\nfirst place in ILSVRC 2015 Scene Classification Challenge. Extensive\nexperiments on two challenging large scale datasets demonstrate the\neffectiveness of our method is not restricted to a specific dataset or network\narchitecture. Our models will be available to the research community later.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 00:13:10 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 07:47:28 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Shen", "Li", ""], ["Lin", "Zhouchen", ""], ["Huang", "Qingming", ""]]}, {"id": "1512.05844", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Parthipan Siva, Paul Fieguth, and Alexander\n  Wong", "title": "Domain Adaptation and Transfer Learning in StochasticNets", "comments": null, "journal-ref": "Vision Letters, Vol. 1, No. 1, pp. VL115, 2015", "doi": "10.15353/vsnl.v1i1.44", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a recent field of machine learning research that aims to\nresolve the challenge of dealing with insufficient training data in the domain\nof interest. This is a particular issue with traditional deep neural networks\nwhere a large amount of training data is needed. Recently, StochasticNets was\nproposed to take advantage of sparse connectivity in order to decrease the\nnumber of parameters that needs to be learned, which in turn may relax training\ndata size requirements. In this paper, we study the efficacy of transfer\nlearning on StochasticNet frameworks. Experimental results show ~7% improvement\non StochasticNet performance when the transfer learning is applied in training\nstep.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 03:09:29 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Siva", "Parthipan", ""], ["Fieguth", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "1512.05986", "submitter": "Vlado Menkovski", "authors": "Vlado Menkovski, Zharko Aleksovski, Axel Saalbach, Hannes Nickisch", "title": "Can Pretrained Neural Networks Detect Anatomy?", "comments": "NIPS 2015 Workshop on Machine Learning in Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks demonstrated outstanding empirical results in\ncomputer vision and speech recognition tasks where labeled training data is\nabundant. In medical imaging, there is a huge variety of possible imaging\nmodalities and contrasts, where annotated data is usually very scarce. We\npresent two approaches to deal with this challenge. A network pretrained in a\ndifferent domain with abundant data is used as a feature extractor, while a\nsubsequent classifier is trained on a small target dataset; and a deep\narchitecture trained with heavy augmentation and equipped with sophisticated\nregularization methods. We test the approaches on a corpus of X-ray images to\ndesign an anatomy detection system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 15:16:31 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Menkovski", "Vlado", ""], ["Aleksovski", "Zharko", ""], ["Saalbach", "Axel", ""], ["Nickisch", "Hannes", ""]]}, {"id": "1512.05990", "submitter": "Jinhua Ma", "authors": "Andy J Ma, Pong C Yuen, Suchi Saria", "title": "Deformable Distributed Multiple Detector Fusion for Multi-Person\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses fully automated multi-person tracking in complex\nenvironments with challenging occlusion and extensive pose variations. Our\nsolution combines multiple detectors for a set of different regions of interest\n(e.g., full-body and head) for multi-person tracking. The use of multiple\ndetectors leads to fewer miss detections as it is able to exploit the\ncomplementary strengths of the individual detectors. While the number of false\npositives may increase with the increased number of bounding boxes detected\nfrom multiple detectors, we propose to group the detection outputs by bounding\nbox location and depth information. For robustness to significant pose\nvariations, deformable spatial relationship between detectors are learnt in our\nmulti-person tracking system. On RGBD data from a live Intensive Care Unit\n(ICU), we show that the proposed method significantly improves multi-person\ntracking performance over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 15:26:05 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Ma", "Andy J", ""], ["Yuen", "Pong C", ""], ["Saria", "Suchi", ""]]}, {"id": "1512.06009", "submitter": "Reuben Farrugia", "authors": "Reuben Farrugia, Christine Guillemot", "title": "Face Hallucination using Linear Models of Coupled Sparse Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most face super-resolution methods assume that low-resolution and\nhigh-resolution manifolds have similar local geometrical structure, hence learn\nlocal models on the lowresolution manifolds (e.g. sparse or locally linear\nembedding models), which are then applied on the high-resolution manifold.\nHowever, the low-resolution manifold is distorted by the oneto-many\nrelationship between low- and high- resolution patches. This paper presents a\nmethod which learns linear models based on the local geometrical structure on\nthe high-resolution manifold rather than on the low-resolution manifold. For\nthis, in a first step, the low-resolution patch is used to derive a globally\noptimal estimate of the high-resolution patch. The approximated solution is\nshown to be close in Euclidean space to the ground-truth but is generally\nsmooth and lacks the texture details needed by state-ofthe-art face\nrecognizers. This first estimate allows us to find the support of the\nhigh-resolution manifold using sparse coding (SC), which are then used as\nsupport for learning a local projection (or upscaling) model between the\nlow-resolution and the highresolution manifolds using Multivariate Ridge\nRegression (MRR). Experimental results show that the proposed method\noutperforms six face super-resolution methods in terms of both recognition and\nquality. These results also reveal that the recognition and quality are\nsignificantly affected by the method used for stitching all super-resolved\npatches together, where quilting was found to better preserve the texture\ndetails which helps to achieve higher recognition rates.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 16:01:55 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Farrugia", "Reuben", ""], ["Guillemot", "Christine", ""]]}, {"id": "1512.06014", "submitter": "Sabyasachi Mukhopadhyay", "authors": "Sabyasachi Mukhopadhyay, Sanket Nandan, Indrajit Kurmi", "title": "Multiclass Classification of Cervical Cancer Tissues by Hidden Markov\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report a hidden Markov model based multiclass\nclassification of cervical cancer tissues. This model has been validated\ndirectly over time series generated by the medium refractive index fluctuations\nextracted from differential interference contrast images of healthy and\ndifferent stages of cancer tissues. The method shows promising results for\nmulticlass classification with higher accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 16:17:35 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 20:42:48 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Nandan", "Sanket", ""], ["Kurmi", "Indrajit", ""]]}, {"id": "1512.06075", "submitter": "Yaser Yacoob", "authors": "Yaser Yacoob", "title": "Modeling Colors of Single Attribute Variations with Application to Food\n  Appearance", "comments": "9 Pages. Paper does not reference recent food-classification papers.\n  It is intended for wider scope", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the intra-image color-space of an object or a scene when\nthese are subject to a dominant single-source of variation. The source of\nvariation can be intrinsic or extrinsic (i.e., imaging conditions) to the\nobject. We observe that the quantized colors for such objects typically lie on\na planar subspace of RGB, and in some cases linear or polynomial curves on this\nplane are effective in capturing these color variations. We also observe that\nthe inter-image color sub-spaces are robust as long as drastic illumination\nchange is not involved.\n  We illustrate the use of this analysis for: discriminating between\nshading-change and reflectance-change for patches, and object detection,\nsegmentation and recognition based on a single exemplar. We focus on images of\nfood items to illustrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 18:55:21 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Yacoob", "Yaser", ""]]}, {"id": "1512.06216", "submitter": "Hao Zhang", "authors": "Hao Zhang, Zhiting Hu, Jinliang Wei, Pengtao Xie, Gunhee Kim, Qirong\n  Ho and Eric Xing", "title": "Poseidon: A System Architecture for Efficient GPU-based Deep Learning on\n  Multiple Machines", "comments": "14 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has achieved notable successes in many machine learning\ntasks. A number of frameworks have been developed to expedite the process of\ndesigning and training deep neural networks (DNNs), such as Caffe, Torch and\nTheano. Currently they can harness multiple GPUs on a single machine, but are\nunable to use GPUs that are distributed across multiple machines; as even\naverage-sized DNNs can take days to train on a single GPU with 100s of GBs to\nTBs of data, distributed GPUs present a prime opportunity for scaling up DL.\nHowever, the limited bandwidth available on commodity Ethernet networks\npresents a bottleneck to distributed GPU training, and prevents its trivial\nrealization.\n  To investigate how to adapt existing frameworks to efficiently support\ndistributed GPUs, we propose Poseidon, a scalable system architecture for\ndistributed inter-machine communication in existing DL frameworks. We integrate\nPoseidon with Caffe and evaluate its performance at training DNNs for object\nrecognition. Poseidon features three key contributions that accelerate DNN\ntraining on clusters: (1) a three-level hybrid architecture that allows\nPoseidon to support both CPU-only and GPU-equipped clusters, (2) a distributed\nwait-free backpropagation (DWBP) algorithm to improve GPU utilization and to\nbalance communication, and (3) a structure-aware communication protocol (SACP)\nto minimize communication overheads. We empirically show that Poseidon\nconverges to same objectives as a single machine, and achieves state-of-art\ntraining speedup across multiple models and well-established datasets using a\ncommodity GPU cluster of 8 nodes (e.g. 4.5x speedup on AlexNet, 4x on\nGoogLeNet, 4x on CIFAR-10). On the much larger ImageNet22K dataset, Poseidon\nwith 8 nodes achieves better speedup and competitive accuracy to recent\nCPU-based distributed systems such as Adam and Le et al., which use 10s to\n1000s of nodes.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 09:55:37 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Zhang", "Hao", ""], ["Hu", "Zhiting", ""], ["Wei", "Jinliang", ""], ["Xie", "Pengtao", ""], ["Kim", "Gunhee", ""], ["Ho", "Qirong", ""], ["Xing", "Eric", ""]]}, {"id": "1512.06223", "submitter": "Carlos Platero PhD", "authors": "Carlos Platero and M. Carmen Tobar", "title": "Combining patch-based strategies and non-rigid registration-based label\n  fusion methods", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this study is to develop a patch-based labeling method that\ncooperates with a label fusion using non-rigid registrations. We present a\nnovel patch-based label fusion method, whose selected patches and their weights\nare calculated from a combination of similarity measures between patches using\nintensity-based distances and labeling-based distances, where a previous\nlabeling of the target image is inferred through a label fusion method using\nnon-rigid registrations. These combined similarity measures result in better\nselection of the patches, and their weights are more robust, which improves the\nsegmentation results compared to other label fusion methods, including the\nconventional patch-based labeling method. To evaluate the performance and the\nrobustness of the proposed label fusion method, we employ two available\ndatabases of T1-weighted (T1W) magnetic resonance imaging (MRI) of human\nbrains. We compare our approach with other label fusion methods in the\nautomatic hippocampal segmentation from T1W-MRI.\n  Our label fusion method yields mean Dice coefficients of 0.847 and 0.798 for\nthe two databases used with mean times of approximately 180 and 320 seconds,\nrespectively. The collaboration between the patch-based labeling method and the\nlabel fusion using non-rigid registrations is given in the several levels: (a)\nThe pre-selection of the patches in the atlases are improved, (b) The weights\nof our selected patches are also more robust, (c) our approach imposes\ngeometrical restrictions, such as shape priors, and (d) the work-flow is very\nefficient. We show that the proposed approach is very competitive with respect\nto recently reported methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 10:30:51 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Platero", "Carlos", ""], ["Tobar", "M. Carmen", ""]]}, {"id": "1512.06235", "submitter": "Rajvi Shah", "authors": "Rajvi Shah and Aditya Deshpande and P J Narayanan", "title": "Multistage SFM: A Coarse-to-Fine Approach for 3D Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been proposed for large-scale 3D reconstruction from\nlarge, unorganized image collections. A large reconstruction problem is\ntypically divided into multiple components which are reconstructed\nindependently using structure from motion (SFM) and later merged together.\nIncremental SFM methods are most popular for the basic structure recovery of a\nsingle component. They are robust and effective but are strictly sequential in\nnature. We present a multistage approach for SFM reconstruction of a single\ncomponent that breaks the sequential nature of the incremental SFM methods. Our\napproach begins with quickly building a coarse 3D model using only a fraction\nof features from given images. The coarse model is then enriched by localizing\nremaining images and matching and triangulating remaining features in\nsubsequent stages. These stages are made efficient and highly parallel by\nleveraging the geometry of the coarse model. Our method produces similar\nquality models as compared to incremental SFM methods while being notably fast\nand parallel.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 12:30:34 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 07:18:57 GMT"}, {"version": "v3", "created": "Wed, 12 Oct 2016 12:41:47 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Shah", "Rajvi", ""], ["Deshpande", "Aditya", ""], ["Narayanan", "P J", ""]]}, {"id": "1512.06285", "submitter": "Min Xian", "authors": "Min Xian, Yingtao Zhang, H. D. Cheng, Fei Xu, Jianrui Ding", "title": "Neutro-Connectedness Cut", "comments": "15 pages, 14 figures, 4 tables, journal", "journal-ref": null, "doi": "10.1109/TIP.2016.2594485", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive image segmentation is a challenging task and receives increasing\nattention recently; however, two major drawbacks exist in interactive\nsegmentation approaches. First, the segmentation performance of ROI-based\nmethods is sensitive to the initial ROI: different ROIs may produce results\nwith great difference. Second, most seed-based methods need intense\ninteractions, and are not applicable in many cases. In this work, we generalize\nthe Neutro-Connectedness (NC) to be independent of top-down priors of objects\nand to model image topology with indeterminacy measurement on image regions,\npropose a novel method for determining object and background regions, which is\napplied to exclude isolated background regions and enforce label consistency,\nand put forward a hybrid interactive segmentation method, Neutro-Connectedness\nCut (NC-Cut), which can overcome the above two problems by utilizing both\npixel-wise appearance information and region-based NC properties. We evaluate\nthe proposed NC-Cut by employing two image datasets (265 images), and\ndemonstrate that the proposed approach outperforms state-of-the-art interactive\nimage segmentation methods (Grabcut, MILCut, One-Cut, MGC_max^sum and pPBC).\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 20:59:09 GMT"}, {"version": "v2", "created": "Sat, 6 Aug 2016 04:51:06 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Xian", "Min", ""], ["Zhang", "Yingtao", ""], ["Cheng", "H. D.", ""], ["Xu", "Fei", ""], ["Ding", "Jianrui", ""]]}, {"id": "1512.06337", "submitter": "Jiasong Wu", "authors": "Dan Wu, Jiasong Wu, Rui Zeng, Longyu Jiang, Lotfi Senhadji, Huazhong\n  Shu", "title": "Kernel principal component analysis network for image classification", "comments": "7 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to classify the nonlinear feature with linear classifier and improve\nthe classification accuracy, a deep learning network named kernel principal\ncomponent analysis network (KPCANet) is proposed. First, mapping the data into\nhigher space with kernel principal component analysis to make the data linearly\nseparable. Then building a two-layer KPCANet to obtain the principal components\nof image. Finally, classifying the principal components with linearly\nclassifier. Experimental results show that the proposed KPCANet is effective in\nface recognition, object recognition and hand-writing digits recognition, it\nalso outperforms principal component analysis network (PCANet) generally as\nwell. Besides, KPCANet is invariant to illumination and stable to occlusion and\nslight deformation.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 09:06:06 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Wu", "Dan", ""], ["Wu", "Jiasong", ""], ["Zeng", "Rui", ""], ["Jiang", "Longyu", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1512.06473", "submitter": "Jiaxiang Wu", "authors": "Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng", "title": "Quantized Convolutional Neural Networks for Mobile Devices", "comments": "Accepted by the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks (CNN) have demonstrated impressive\nperformance in various computer vision tasks. However, high performance\nhardware is typically indispensable for the application of CNN models due to\nthe high computation complexity, which prohibits their further extensions. In\nthis paper, we propose an efficient framework, namely Quantized CNN, to\nsimultaneously speed-up the computation and reduce the storage and memory\noverhead of CNN models. Both filter kernels in convolutional layers and\nweighting matrices in fully-connected layers are quantized, aiming at\nminimizing the estimation error of each layer's response. Extensive experiments\non the ILSVRC-12 benchmark demonstrate 4~6x speed-up and 15~20x compression\nwith merely one percentage loss of classification accuracy. With our quantized\nCNN model, even mobile devices can accurately classify images within one\nsecond.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 02:26:46 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 03:02:25 GMT"}, {"version": "v3", "created": "Mon, 16 May 2016 00:37:35 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Wu", "Jiaxiang", ""], ["Leng", "Cong", ""], ["Wang", "Yuhang", ""], ["Hu", "Qinghao", ""], ["Cheng", "Jian", ""]]}, {"id": "1512.06492", "submitter": "Qifei Wang", "authors": "Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy", "title": "Remote Health Coaching System and Human Motion Data Analysis for\n  Physical Therapy with Microsoft Kinect", "comments": "6 pages, Computer Vision for Accessible and Affordable HealthCare\n  Workshop (ICCV2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the recent progress we have made for the computer\nvision technologies in physical therapy with the accessible and affordable\ndevices. We first introduce the remote health coaching system we build with\nMicrosoft Kinect. Since the motion data captured by Kinect is noisy, we\ninvestigate the data accuracy of Kinect with respect to the high accuracy\nmotion capture system. We also propose an outlier data removal algorithm based\non the data distribution. In order to generate the kinematic parameter from the\nnoisy data captured by Kinect, we propose a kinematic filtering algorithm based\non Unscented Kalman Filter and the kinematic model of human skeleton. The\nproposed algorithm can obtain smooth kinematic parameter with reduced noise\ncompared to the kinematic parameter generated from the raw motion data from\nKinect.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 04:58:36 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Wang", "Qifei", ""], ["Kurillo", "Gregorij", ""], ["Ofli", "Ferda", ""], ["Bajcsy", "Ruzena", ""]]}, {"id": "1512.06498", "submitter": "Oruganti Ramana Mr", "authors": "O. V. Ramana Murthy and Roland Goecke", "title": "Harnessing the Deep Net Object Models for Enhancing Human Action\n  Recognition", "comments": "6 pages. arXiv admin note: text overlap with arXiv:1411.4006 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the influence of objects is investigated in the scenario of\nhuman action recognition with large number of classes. We hypothesize that the\nobjects the humans are interacting will have good say in determining the action\nbeing performed. Especially, if the objects are non-moving, such as objects\nappearing in the background, features such as spatio-temporal interest points,\ndense trajectories may fail to detect them. Hence we propose to detect objects\nusing pre-trained object detectors in every frame statically. Trained Deep\nnetwork models are used as object detectors. Information from different layers\nin conjunction with different encoding techniques is extensively studied to\nobtain the richest feature vectors. This technique is observed to yield\nstate-of-the-art performance on HMDB51 and UCF101 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 05:28:23 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 04:37:51 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Murthy", "O. V. Ramana", ""], ["Goecke", "Roland", ""]]}, {"id": "1512.06539", "submitter": "Ryuichi Tadano", "authors": "Ryuichi Tadano, Adithya Kumar Pediredla, Kaushik Mitra and Ashok\n  Veeraraghavan", "title": "Spatial Phase-Sweep: Increasing temporal resolution of transient imaging\n  using a light source array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient imaging or light-in-flight techniques capture the propagation of an\nultra-short pulse of light through a scene, which in effect captures the\noptical impulse response of the scene. Recently, it has been shown that we can\ncapture transient images using commercially available Time-of-Flight (ToF)\nsystems such as Photonic Mixer Devices (PMD). In this paper, we propose\n`spatial phase-sweep', a technique that exploits the speed of light to increase\nthe temporal resolution beyond the 100 picosecond limit imposed by current\nelectronics. Spatial phase-sweep uses a linear array of light sources with\nspatial separation of about 3 mm between them, thereby resulting in a time\nshift of about 10 picoseconds, which translates into 100 Gfps of transient\nimaging in theory. We demonstrate a prototype and transient imaging results\nusing spatial phase-sweep.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 09:25:18 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Tadano", "Ryuichi", ""], ["Pediredla", "Adithya Kumar", ""], ["Mitra", "Kaushik", ""], ["Veeraraghavan", "Ashok", ""]]}, {"id": "1512.06559", "submitter": "Samaneh Abbasi Sureshjani", "authors": "Marta Favali, Samaneh Abbasi-Sureshjani, Bart ter Haar Romeny,\n  Alessandro Sarti", "title": "Analysis of Vessel Connectivities in Retinal Images by Cortically\n  Inspired Spectral Clustering", "comments": "submitted to and accepted by JMIV", "journal-ref": "Journal of Mathematical Imaging and Vision, September 2016, Volume\n  56, Issue 1, pp 158-172", "doi": "10.1007/s10851-016-0640-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal images provide early signs of diabetic retinopathy, glaucoma, and\nhypertension. These signs can be investigated based on microaneurysms or\nsmaller vessels. The diagnostic biomarkers are the change of vessel widths and\nangles especially at junctions, which are investigated using the vessel\nsegmentation or tracking. Vessel paths may also be interrupted; crossings and\nbifurcations may be disconnected. This paper addresses a novel contextual\nmethod based on the geometry of the primary visual cortex (V1) to study these\ndifficulties. We have analyzed the specific problems at junctions with a\nconnectivity kernel obtained as the fundamental solution of the Fokker-Planck\nequation, which is usually used to represent the geometrical structure of\nmulti-orientation cortical connectivity. Using the spectral clustering on a\nlarge local affinity matrix constructed by both the connectivity kernel and the\nfeature of intensity, the vessels are identified successfully in a hierarchical\ntopology each representing an individual perceptual unit.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 10:09:23 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 11:37:55 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Favali", "Marta", ""], ["Abbasi-Sureshjani", "Samaneh", ""], ["Romeny", "Bart ter Haar", ""], ["Sarti", "Alessandro", ""]]}, {"id": "1512.06566", "submitter": "Marta Favali", "authors": "Marta Favali, Giovanna Citti, Alessandro Sarti", "title": "Local and global gestalt laws: A neurally based spectral approach", "comments": "submitted to Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mathematical model of figure-ground articulation is presented, taking into\naccount both local and global gestalt laws. The model is compatible with the\nfunctional architecture of the primary visual cortex (V1). Particularly the\nlocal gestalt law of good continuity is described by means of suitable\nconnectivity kernels, that are derived from Lie group theory and are neurally\nimplemented in long range connectivity in V1. Different kernels are compatible\nwith the geometric structure of cortical connectivity and they are derived as\nthe fundamental solutions of the Fokker Planck, the Sub-Riemannian Laplacian\nand the isotropic Laplacian equations. The kernels are used to construct\nmatrices of connectivity among the features present in a visual stimulus.\nGlobal gestalt constraints are then introduced in terms of spectral analysis of\nthe connectivity matrix, showing that this processing can be cortically\nimplemented in V1 by mean field neural equations. This analysis performs\ngrouping of local features and individuates perceptual units with the highest\nsaliency. Numerical simulations are performed and results are obtained applying\nthe technique to a number of stimuli.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 10:27:58 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 15:20:45 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Favali", "Marta", ""], ["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""]]}, {"id": "1512.06658", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Lu Fang, Mengqi Ji, Matti Strese, Yigitcan Ozer,\n  Eckehard Steinbach", "title": "Deep Learning for Surface Material Classification Using Haptic And\n  Visual Information", "comments": "8 pages, under review as a paper at Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a user scratches a hand-held rigid tool across an object surface, an\nacceleration signal can be captured, which carries relevant information about\nthe surface. More importantly, such a haptic signal is complementary to the\nvisual appearance of the surface, which suggests the combination of both\nmodalities for the recognition of the surface material. In this paper, we\npresent a novel deep learning method dealing with the surface material\nclassification problem based on a Fully Convolutional Network (FCN), which\ntakes as input the aforementioned acceleration signal and a corresponding image\nof the surface texture. Compared to previous surface material classification\nsolutions, which rely on a careful design of hand-crafted domain-specific\nfeatures, our method automatically extracts discriminative features utilizing\nthe advanced deep learning methodologies. Experiments performed on the TUM\nsurface material database demonstrate that our method achieves state-of-the-art\nclassification accuracy robustly and efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 15:22:16 GMT"}, {"version": "v2", "created": "Sun, 1 May 2016 07:00:56 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Zheng", "Haitian", ""], ["Fang", "Lu", ""], ["Ji", "Mengqi", ""], ["Strese", "Matti", ""], ["Ozer", "Yigitcan", ""], ["Steinbach", "Eckehard", ""]]}, {"id": "1512.06709", "submitter": "Xiaoxia Sun", "authors": "Xiaoxia Sun, Nasser M. Nasrabadi and Trac D. Tran", "title": "Sparse Coding with Fast Image Alignment via Large Displacement Optical\n  Flow", "comments": "ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation-based classifiers have shown outstanding accuracy and\nrobustness in image classification tasks even with the presence of intense\nnoise and occlusion. However, it has been discovered that the performance\ndegrades significantly either when test image is not aligned with the\ndictionary atoms or the dictionary atoms themselves are not aligned with each\nother, in which cases the sparse linear representation assumption fails. In\nthis paper, having both training and test images misaligned, we introduce a\nnovel sparse coding framework that is able to efficiently adapt the dictionary\natoms to the test image via large displacement optical flow. In the proposed\nalgorithm, every dictionary atom is automatically aligned with the input image\nand the sparse code is then recovered using the adapted dictionary atoms. A\ncorresponding supervised dictionary learning algorithm is also developed for\nthe proposed framework. Experimental results on digit datasets recognition\nverify the efficacy and robustness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 17:10:35 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Sun", "Xiaoxia", ""], ["Nasrabadi", "Nasser M.", ""], ["Tran", "Trac D.", ""]]}, {"id": "1512.06730", "submitter": "Shuchin Aeron", "authors": "Eric Kernfeld, Nathan Majumder, Shuchin Aeron, Misha Kilmer", "title": "Multilinear Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new model and an algorithm for unsupervised\nclustering of 2-D data such as images. We assume that the data comes from a\nunion of multilinear subspaces (UOMS) model, which is a specific structured\ncase of the much studied union of subspaces (UOS) model. For segmentation under\nthis model, we develop Multilinear Subspace Clustering (MSC) algorithm and\nevaluate its performance on the YaleB and Olivietti image data sets. We show\nthat MSC is highly competitive with existing algorithms employing the UOS model\nin terms of clustering performance while enjoying improvement in computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 17:53:35 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Kernfeld", "Eric", ""], ["Majumder", "Nathan", ""], ["Aeron", "Shuchin", ""], ["Kilmer", "Misha", ""]]}, {"id": "1512.06735", "submitter": "Ziyu Zhang", "authors": "Ziyu Zhang, Sanja Fidler, Raquel Urtasun", "title": "Instance-Level Segmentation for Autonomous Driving with Deep Densely\n  Connected MRFs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim is to provide a pixel-wise instance-level labeling of a monocular\nimage in the context of autonomous driving. We build on recent work [Zhang et\nal., ICCV15] that trained a convolutional neural net to predict instance\nlabeling in local image patches, extracted exhaustively in a stride from an\nimage. A simple Markov random field model using several heuristics was then\nproposed in [Zhang et al., ICCV15] to derive a globally consistent instance\nlabeling of the image. In this paper, we formulate the global labeling problem\nwith a novel densely connected Markov random field and show how to encode\nvarious intuitive potentials in a way that is amenable to efficient mean field\ninference [Kr\\\"ahenb\\\"uhl et al., NIPS11]. Our potentials encode the\ncompatibility between the global labeling and the patch-level predictions,\ncontrast-sensitive smoothness as well as the fact that separate regions form\ndifferent instances. Our experiments on the challenging KITTI benchmark [Geiger\net al., CVPR12] demonstrate that our method achieves a significant performance\nboost over the baseline [Zhang et al., ICCV15].\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 17:58:35 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 00:37:05 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Zhang", "Ziyu", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1512.06757", "submitter": "Jiaji Huang", "authors": "Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro", "title": "GraphConnect: A Regularization Framework for Neural Networks", "comments": "Theorems need more validation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have proved very successful in domains where large\ntraining sets are available, but when the number of training samples is small,\ntheir performance suffers from overfitting. Prior methods of reducing\noverfitting such as weight decay, Dropout and DropConnect are data-independent.\nThis paper proposes a new method, GraphConnect, that is data-dependent, and is\nmotivated by the observation that data of interest lie close to a manifold. The\nnew method encourages the relationships between the learned decisions to\nresemble a graph representing the manifold structure. Essentially GraphConnect\nis designed to learn attributes that are present in data samples in contrast to\nweight decay, Dropout and DropConnect which are simply designed to make it more\ndifficult to fit to random error or noise. Empirical Rademacher complexity is\nused to connect the generalization error of the neural network to spectral\nproperties of the graph learned from the input data. This framework is used to\nshow that GraphConnect is superior to weight decay. Experimental results on\nseveral benchmark datasets validate the theoretical analysis, and show that\nwhen the number of training samples is small, GraphConnect is able to\nsignificantly improve performance over weight decay.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 18:42:45 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 03:21:15 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Huang", "Jiaji", ""], ["Qiu", "Qiang", ""], ["Calderbank", "Robert", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1512.06785", "submitter": "Longqi Yang", "authors": "Longqi Yang, Cheng-Kang Hsieh, Deborah Estrin", "title": "Beyond Classification: Latent User Interests Profiling from Visual\n  Contents Analysis", "comments": "2015 IEEE 15th International Conference on Data Mining Workshops", "journal-ref": null, "doi": "10.1109/ICDMW.2015.160", "report-no": null, "categories": "cs.IR cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User preference profiling is an important task in modern online social\nnetworks (OSN). With the proliferation of image-centric social platforms, such\nas Pinterest, visual contents have become one of the most informative data\nstreams for understanding user preferences. Traditional approaches usually\ntreat visual content analysis as a general classification problem where one or\nmore labels are assigned to each image. Although such an approach simplifies\nthe process of image analysis, it misses the rich context and visual cues that\nplay an important role in people's perception of images. In this paper, we\nexplore the possibilities of learning a user's latent visual preferences\ndirectly from image contents. We propose a distance metric learning method\nbased on Deep Convolutional Neural Networks (CNN) to directly extract\nsimilarity information from visual contents and use the derived distance metric\nto mine individual users' fine-grained visual preferences. Through our\npreliminary experiments using data from 5,790 Pinterest users, we show that\neven for the images within the same category, each user possesses distinct and\nindividually-identifiable visual preferences that are consistent over their\nlifetime. Our results underscore the untapped potential of finer-grained visual\npreference profiling in understanding users' preferences.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 19:54:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Yang", "Longqi", ""], ["Hsieh", "Cheng-Kang", ""], ["Estrin", "Deborah", ""]]}, {"id": "1512.06790", "submitter": "Siddharth Mahendran", "authors": "Siddharth Mahendran and Ren\\'e Vidal", "title": "Car Segmentation and Pose Estimation using 3D Object Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation and 3D pose estimation are two key cogs in any algorithm\nfor scene understanding. However, state-of-the-art CRF-based models for image\nsegmentation rely mostly on 2D object models to construct top-down high-order\npotentials. In this paper, we propose new top-down potentials for image\nsegmentation and pose estimation based on the shape and volume of a 3D object\nmodel. We show that these complex top-down potentials can be easily decomposed\ninto standard forms for efficient inference in both the segmentation and pose\nestimation tasks. Experiments on a car dataset show that knowledge of\nsegmentation helps perform pose estimation better and vice versa.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 20:01:53 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 11:58:47 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Mahendran", "Siddharth", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1512.06925", "submitter": "Jiangbo Yuan", "authors": "Jiangbo Yuan and Xiuwen Liu", "title": "Transformed Residual Quantization for Approximate Nearest Neighbor\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of product quantization (PQ) for fast nearest neighbor search\ndepends on the exponentially reduced complexities of both storage and\ncomputation with respect to the codebook size. Recent efforts have been focused\non employing sophisticated optimization strategies, or seeking more effective\nmodels. Residual quantization (RQ) is such an alternative that holds the same\nproperty as PQ in terms of the aforementioned complexities. In addition to\nbeing a direct replacement of PQ, hybrids of PQ and RQ can yield more gains for\napproximate nearest neighbor search. This motivated us to propose a novel\napproach to optimizing RQ and the related hybrid models. With an observation of\nthe general randomness increase in a residual space, we propose a new strategy\nthat jointly learns a local transformation per residual cluster with an\nultimate goal to reduce overall quantization errors. We have shown that our\napproach can achieve significantly better accuracy on nearest neighbor search\nthan both the original and the optimized PQ on several very large scale\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 01:12:54 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Yuan", "Jiangbo", ""], ["Liu", "Xiuwen", ""]]}, {"id": "1512.06963", "submitter": "Zhou Ren", "authors": "Zhou Ren and Hailin Jin and Zhe Lin and Chen Fang and Alan Yuille", "title": "Multi-Instance Visual-Semantic Embedding", "comments": "9 pages, CVPR 2016 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-semantic embedding models have been recently proposed and shown to be\neffective for image classification and zero-shot learning, by mapping images\ninto a continuous semantic label space. Although several approaches have been\nproposed for single-label embedding tasks, handling images with multiple labels\n(which is a more general setting) still remains an open problem, mainly due to\nthe complex underlying corresponding relationship between image and its labels.\nIn this work, we present Multi-Instance visual-semantic Embedding model (MIE)\nfor embedding images associated with either single or multiple labels. Our\nmodel discovers and maps semantically-meaningful image subregions to their\ncorresponding labels. And we demonstrate the superiority of our method over the\nstate-of-the-art on two tasks, including multi-label image annotation and\nzero-shot learning.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 05:54:34 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Ren", "Zhou", ""], ["Jin", "Hailin", ""], ["Lin", "Zhe", ""], ["Fang", "Chen", ""], ["Yuille", "Alan", ""]]}, {"id": "1512.06974", "submitter": "Ishan Misra", "authors": "Ishan Misra and C. Lawrence Zitnick and Margaret Mitchell and Ross\n  Girshick", "title": "Seeing through the Human Reporting Bias: Visual Classifiers from Noisy\n  Human-Centric Labels", "comments": "To appear in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When human annotators are given a choice about what to label in an image,\nthey apply their own subjective judgments on what to ignore and what to\nmention. We refer to these noisy \"human-centric\" annotations as exhibiting\nhuman reporting bias. Examples of such annotations include image tags and\nkeywords found on photo sharing sites, or in datasets containing image\ncaptions. In this paper, we use these noisy annotations for learning visually\ncorrect image classifiers. Such annotations do not use consistent vocabulary,\nand miss a significant amount of the information present in an image; however,\nwe demonstrate that the noise in these annotations exhibits structure and can\nbe modeled. We propose an algorithm to decouple the human reporting bias from\nthe correct visually grounded labels. Our results are highly interpretable for\nreporting \"what's in the image\" versus \"what's worth saying.\" We demonstrate\nthe algorithm's efficacy along a variety of metrics and datasets, including MS\nCOCO and Yahoo Flickr 100M. We show significant improvements over traditional\nalgorithms for both image classification and image captioning, doubling the\nperformance of existing methods in some cases.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 07:28:06 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 19:58:29 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Misra", "Ishan", ""], ["Zitnick", "C. Lawrence", ""], ["Mitchell", "Margaret", ""], ["Girshick", "Ross", ""]]}, {"id": "1512.07030", "submitter": "Xiaojie Jin Mr.", "authors": "Xiaojie Jin, Chunyan Xu, Jiashi Feng, Yunchao Wei, Junjun Xiong,\n  Shuicheng Yan", "title": "Deep Learning with S-shaped Rectified Linear Activation Units", "comments": "Accepted by AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectified linear activation units are important components for\nstate-of-the-art deep convolutional networks. In this paper, we propose a novel\nS-shaped rectified linear activation unit (SReLU) to learn both convex and\nnon-convex functions, imitating the multiple function forms given by the two\nfundamental laws, namely the Webner-Fechner law and the Stevens law, in\npsychophysics and neural sciences. Specifically, SReLU consists of three\npiecewise linear functions, which are formulated by four learnable parameters.\nThe SReLU is learned jointly with the training of the whole deep network\nthrough back propagation. During the training phase, to initialize SReLU in\ndifferent layers, we propose a \"freezing\" method to degenerate SReLU into a\npredefined leaky rectified linear unit in the initial several training epochs\nand then adaptively learn the good initial values. SReLU can be universally\nused in the existing deep networks with negligible additional parameters and\ncomputation cost. Experiments with two popular CNN architectures, Network in\nNetwork and GoogLeNet on scale-various benchmarks including CIFAR10, CIFAR100,\nMNIST and ImageNet demonstrate that SReLU achieves remarkable improvement\ncompared to other activation functions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 10:54:26 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Jin", "Xiaojie", ""], ["Xu", "Chunyan", ""], ["Feng", "Jiashi", ""], ["Wei", "Yunchao", ""], ["Xiong", "Junjun", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1512.07041", "submitter": "Andrey Makarenko", "authors": "A.V. Makarenko, M.G. Volovik", "title": "Implementation of deep learning algorithm for automatic detection of\n  brain tumors using intraoperative IR-thermal mapping data", "comments": "7 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of deep machine learning for automatic delineation of tumor\nareas has been demonstrated for intraoperative neuronavigation using active\nIR-mapping with the use of the cold test. The proposed approach employs a\nmatrix IR-imager to remotely register the space-time distribution of surface\ntemperature pattern, which is determined by the dynamics of local cerebral\nblood flow. The advantages of this technique are non-invasiveness, zero risks\nfor the health of patients and medical staff, low implementation and\noperational costs, ease and speed of use. Traditional IR-diagnostic technique\nhas a crucial limitation - it involves a diagnostician who determines the\nboundaries of tumor areas, which gives rise to considerable uncertainty, which\ncan lead to diagnosis errors that are difficult to control. The current study\ndemonstrates that implementing deep learning algorithms allows to eliminate the\nexplained drawback.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 11:52:26 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Makarenko", "A. V.", ""], ["Volovik", "M. G.", ""]]}, {"id": "1512.07080", "submitter": "Toby Perrett", "authors": "Toby Perrett, Majid Mirmehdi, Eduardo Dias", "title": "Cost-based Feature Transfer for Vehicle Occupant Classification", "comments": "9 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of human presence and interaction in a vehicle is of growing\ninterest to vehicle manufacturers for design and safety purposes. We present a\nframework to perform the tasks of occupant detection and occupant\nclassification for automatic child locks and airbag suppression. It operates\nfor all passenger seats, using a single overhead camera. A transfer learning\ntechnique is introduced to make full use of training data from all seats whilst\nstill maintaining some control over the bias, necessary for a system designed\nto penalize certain misclassifications more than others. An evaluation is\nperformed on a challenging dataset with both weighted and unweighted\nclassifiers, demonstrating the effectiveness of the transfer process.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 13:35:10 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Perrett", "Toby", ""], ["Mirmehdi", "Majid", ""], ["Dias", "Eduardo", ""]]}, {"id": "1512.07108", "submitter": "Jiuxiang Gu Mr", "authors": "Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy,\n  Bing Shuai, Ting Liu, Xingxing Wang, Li Wang, Gang Wang, Jianfei Cai, Tsuhan\n  Chen", "title": "Recent Advances in Convolutional Neural Networks", "comments": "Pattern Recognition, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, deep learning has led to very good performance on a\nvariety of problems, such as visual recognition, speech recognition and natural\nlanguage processing. Among different types of deep neural networks,\nconvolutional neural networks have been most extensively studied. Leveraging on\nthe rapid growth in the amount of the annotated data and the great improvements\nin the strengths of graphics processor units, the research on convolutional\nneural networks has been emerged swiftly and achieved state-of-the-art results\non various tasks. In this paper, we provide a broad survey of the recent\nadvances in convolutional neural networks. We detailize the improvements of CNN\non different aspects, including layer design, activation function, loss\nfunction, regularization, optimization and fast computation. Besides, we also\nintroduce various applications of convolutional neural networks in computer\nvision, speech and natural language processing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 14:54:34 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 11:39:16 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 01:54:59 GMT"}, {"version": "v4", "created": "Sat, 6 Aug 2016 12:38:35 GMT"}, {"version": "v5", "created": "Thu, 5 Jan 2017 05:45:53 GMT"}, {"version": "v6", "created": "Thu, 19 Oct 2017 16:34:35 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Gu", "Jiuxiang", ""], ["Wang", "Zhenhua", ""], ["Kuen", "Jason", ""], ["Ma", "Lianyang", ""], ["Shahroudy", "Amir", ""], ["Shuai", "Bing", ""], ["Liu", "Ting", ""], ["Wang", "Xingxing", ""], ["Wang", "Li", ""], ["Wang", "Gang", ""], ["Cai", "Jianfei", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1512.07143", "submitter": "Marc Bola\\~nos", "authors": "Mariella Dimiccoli and Marc Bola\\~nos and Estefania Talavera and\n  Maedeh Aghaei and Stavri G. Nikolov and Petia Radeva", "title": "SR-Clustering: Semantic Regularized Clustering for Egocentric Photo\n  Streams Segmentation", "comments": "23 pages, 10 figures, 2 tables. In Press in Computer Vision and Image\n  Understanding Journal", "journal-ref": null, "doi": "10.1016/j.cviu.2016.10.005", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While wearable cameras are becoming increasingly popular, locating relevant\ninformation in large unstructured collections of egocentric images is still a\ntedious and time consuming processes. This paper addresses the problem of\norganizing egocentric photo streams acquired by a wearable camera into\nsemantically meaningful segments. First, contextual and semantic information is\nextracted for each image by employing a Convolutional Neural Networks approach.\nLater, by integrating language processing, a vocabulary of concepts is defined\nin a semantic space. Finally, by exploiting the temporal coherence in photo\nstreams, images which share contextual and semantic attributes are grouped\ntogether. The resulting temporal segmentation is particularly suited for\nfurther analysis, ranging from activity and event recognition to semantic\nindexing and summarization. Experiments over egocentric sets of nearly 17,000\nimages, show that the proposed approach outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 16:13:54 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 09:40:11 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Dimiccoli", "Mariella", ""], ["Bola\u00f1os", "Marc", ""], ["Talavera", "Estefania", ""], ["Aghaei", "Maedeh", ""], ["Nikolov", "Stavri G.", ""], ["Radeva", "Petia", ""]]}, {"id": "1512.07155", "submitter": "Sarah Adel Bargal", "authors": "Shugao Ma, Sarah Adel Bargal, Jianming Zhang, Leonid Sigal, Stan\n  Sclaroff", "title": "Do Less and Achieve More: Training CNNs for Action Recognition Utilizing\n  Action Images from the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, attempts have been made to collect millions of videos to train CNN\nmodels for action recognition in videos. However, curating such large-scale\nvideo datasets requires immense human labor, and training CNNs on millions of\nvideos demands huge computational resources. In contrast, collecting action\nimages from the Web is much easier and training on images requires much less\ncomputation. In addition, labeled web images tend to contain discriminative\naction poses, which highlight discriminative portions of a video's temporal\nprogression. We explore the question of whether we can utilize web action\nimages to train better CNN models for action recognition in videos. We collect\n23.8K manually filtered images from the Web that depict the 101 actions in the\nUCF101 action video dataset. We show that by utilizing web action images along\nwith videos in training, significant performance boosts of CNN models can be\nachieved. We then investigate the scalability of the process by leveraging\ncrawled web images (unfiltered) for UCF101 and ActivityNet. We replace 16.2M\nvideo frames by 393K unfiltered images and get comparable performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 16:52:19 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Ma", "Shugao", ""], ["Bargal", "Sarah Adel", ""], ["Zhang", "Jianming", ""], ["Sigal", "Leonid", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1512.07314", "submitter": "Moin Nabi", "authors": "Moin Nabi", "title": "Mid-level Representation for Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Recognition is one of the fundamental challenges in AI, where the goal\nis to understand the semantics of visual data. Employing mid-level\nrepresentation, in particular, shifted the paradigm in visual recognition. The\nmid-level image/video representation involves discovering and training a set of\nmid-level visual patterns (e.g., parts and attributes) and represent a given\nimage/video utilizing them. The mid-level patterns can be extracted from images\nand videos using the motion and appearance information of visual phenomenas.\nThis thesis targets employing mid-level representations for different\nhigh-level visual recognition tasks, namely (i)image understanding and\n(ii)video understanding.\n  In the case of image understanding, we focus on object detection/recognition\ntask. We investigate on discovering and learning a set of mid-level patches to\nbe used for representing the images of an object category. We specifically\nemploy the discriminative patches in a subcategory-aware webly-supervised\nfashion. We, additionally, study the outcomes provided by employing the\nsubcategory-based models for undoing dataset bias.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 00:45:41 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Nabi", "Moin", ""]]}, {"id": "1512.07331", "submitter": "Suhas Sreehari", "authors": "Suhas Sreehari, S. V. Venkatakrishnan, Brendt Wohlberg, Lawrence F.\n  Drummy, Jeffrey P. Simmons, Charles A. Bouman", "title": "Plug-and-Play Priors for Bright Field Electron Tomography and Sparse\n  Interpolation", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TCI.2016.2599778", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many material and biological samples in scientific imaging are characterized\nby non-local repeating structures. These are studied using scanning electron\nmicroscopy and electron tomography. Sparse sampling of individual pixels in a\n2D image acquisition geometry, or sparse sampling of projection images with\nlarge tilt increments in a tomography experiment, can enable high speed data\nacquisition and minimize sample damage caused by the electron beam.\n  In this paper, we present an algorithm for electron tomographic\nreconstruction and sparse image interpolation that exploits the non-local\nredundancy in images. We adapt a framework, termed plug-and-play (P&P) priors,\nto solve these imaging problems in a regularized inversion setting. The power\nof the P&P approach is that it allows a wide array of modern denoising\nalgorithms to be used as a \"prior model\" for tomography and image\ninterpolation. We also present sufficient mathematical conditions that ensure\nconvergence of the P&P approach, and we use these insights to design a new\nnon-local means denoising algorithm. Finally, we demonstrate that the algorithm\nproduces higher quality reconstructions on both simulated and real electron\nmicroscope data, along with improved convergence properties compared to other\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 02:06:29 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Sreehari", "Suhas", ""], ["Venkatakrishnan", "S. V.", ""], ["Wohlberg", "Brendt", ""], ["Drummy", "Lawrence F.", ""], ["Simmons", "Jeffrey P.", ""], ["Bouman", "Charles A.", ""]]}, {"id": "1512.07344", "submitter": "Xin Yuan", "authors": "Yunchen Pu, Xin Yuan, Andrew Stevens, Chunyuan Li, Lawrence Carin", "title": "A Deep Generative Deconvolutional Image Model", "comments": "10 pages, 7 figures. Appearing in Proceedings of the 19th\n  International Conference on Artificial Intelligence and Statistics (AISTATS)\n  2016, Cadiz, Spain. JMLR: W&CP volume 41", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep generative model is developed for representation and analysis of\nimages, based on a hierarchical convolutional dictionary-learning framework.\nStochastic {\\em unpooling} is employed to link consecutive layers in the model,\nyielding top-down image generation. A Bayesian support vector machine is linked\nto the top-layer features, yielding max-margin discrimination. Deep\ndeconvolutional inference is employed when testing, to infer the latent\nfeatures, and the top-layer features are connected with the max-margin\nclassifier for discrimination tasks. The model is efficiently trained using a\nMonte Carlo expectation-maximization (MCEM) algorithm, with implementation on\ngraphical processor units (GPUs) for efficient large-scale learning, and fast\ntesting. Excellent results are obtained on several benchmark datasets,\nincluding ImageNet, demonstrating that the proposed model achieves results that\nare highly competitive with similarly sized convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 03:10:29 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Pu", "Yunchen", ""], ["Yuan", "Xin", ""], ["Stevens", "Andrew", ""], ["Li", "Chunyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1512.07502", "submitter": "J.T. Turner", "authors": "J.T. Turner, David Aha, Leslie Smith, Kalyan Moy Gupta", "title": "Convolutional Architecture Exploration for Action Recognition and Image\n  Classification", "comments": "12 pages. 11 tables. 0 Images. Written Summer 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Architecture for Fast Feature Encoding (CAFFE) [11] is a\nsoftware package for the training, classifying, and feature extraction of\nimages. The UCF Sports Action dataset is a widely used machine learning dataset\nthat has 200 videos taken in 720x480 resolution of 9 different sporting\nactivities: diving, golf, swinging, kicking, lifting, horseback riding,\nrunning, skateboarding, swinging (various gymnastics), and walking. In this\nreport we report on a caffe feature extraction pipeline of images taken from\nthe videos of the UCF Sports Action dataset. A similar test was performed on\noverfeat, and results were inferior to caffe. This study is intended to explore\nthe architecture and hyper parameters needed for effective static analysis of\naction in videos and classification over a variety of image datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 14:54:43 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Turner", "J. T.", ""], ["Aha", "David", ""], ["Smith", "Leslie", ""], ["Gupta", "Kalyan Moy", ""]]}, {"id": "1512.07506", "submitter": "Rigas Kouskouridas", "authors": "Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malassiotis, Tae-Kyun\n  Kim", "title": "Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd", "comments": "CVPR 2016 accepted paper, project page:\n  http://www.iis.ee.ic.ac.uk/rkouskou/6D_NBV.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and 6D pose estimation in the crowd (scenes with multiple\nobject instances, severe foreground occlusions and background distractors), has\nbecome an important problem in many rapidly evolving technological areas such\nas robotics and augmented reality. Single shot-based 6D pose estimators with\nmanually designed features are still unable to tackle the above challenges,\nmotivating the research towards unsupervised feature learning and\nnext-best-view estimation. In this work, we present a complete framework for\nboth single shot-based 6D object pose estimation and next-best-view prediction\nbased on Hough Forests, the state of the art object pose estimator that\nperforms classification and regression jointly. Rather than using manually\ndesigned features we a) propose an unsupervised feature learnt from\ndepth-invariant patches using a Sparse Autoencoder and b) offer an extensive\nevaluation of various state of the art features. Furthermore, taking advantage\nof the clustering performed in the leaf nodes of Hough Forests, we learn to\nestimate the reduction of uncertainty in other views, formulating the problem\nof selecting the next-best-view. To further improve pose estimation, we propose\nan improved joint registration and hypotheses verification module as a final\nrefinement step to reject false detections. We provide two additional\nchallenging datasets inspired from realistic scenarios to extensively evaluate\nthe state of the art and our framework. One is related to domestic environments\nand the other depicts a bin-picking scenario mostly found in industrial\nsettings. We show that our framework significantly outperforms state of the art\nboth on public and on our datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 15:06:05 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 17:31:56 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Doumanoglou", "Andreas", ""], ["Kouskouridas", "Rigas", ""], ["Malassiotis", "Sotiris", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1512.07587", "submitter": "Rajasekaran Masatran", "authors": "Rajasekaran Masatran", "title": "A Latent-Variable Lattice Model", "comments": "6 pages, with 4 figures, 8 algorithms, and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random field (MRF) learning is intractable, and its approximation\nalgorithms are computationally expensive. We target a small subset of MRF that\nis used frequently in computer vision. We characterize this subset with three\nconcepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as\nan alternative. Our goal is robust learning from small datasets. Our learning\nalgorithm uses vector quantization and, at time complexity O(U log U) for a\ndataset of U pixels, is much faster than that of general-purpose MRF.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 19:01:03 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 16:57:50 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2016 08:48:46 GMT"}, {"version": "v4", "created": "Sat, 5 Mar 2016 13:07:09 GMT"}, {"version": "v5", "created": "Fri, 20 May 2016 08:30:02 GMT"}, {"version": "v6", "created": "Wed, 25 May 2016 09:17:23 GMT"}, {"version": "v7", "created": "Wed, 8 Jun 2016 03:25:09 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Masatran", "Rajasekaran", ""]]}, {"id": "1512.07711", "submitter": "Yongxi Lu", "authors": "Yongxi Lu, Tara Javidi, Svetlana Lazebnik", "title": "Adaptive Object Detection Using Adjacency and Zoom Prediction", "comments": "Accepted to CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art object detection systems rely on an accurate set of region\nproposals. Several recent methods use a neural network architecture to\nhypothesize promising object locations. While these approaches are\ncomputationally efficient, they rely on fixed image regions as anchors for\npredictions. In this paper we propose to use a search strategy that adaptively\ndirects computational resources to sub-regions likely to contain objects.\nCompared to methods based on fixed anchor locations, our approach naturally\nadapts to cases where object instances are sparse and small. Our approach is\ncomparable in terms of accuracy to the state-of-the-art Faster R-CNN approach\nwhile using two orders of magnitude fewer anchors on average. Code is publicly\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 04:20:16 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 05:51:12 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Lu", "Yongxi", ""], ["Javidi", "Tara", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1512.07712", "submitter": "Angshul Majumdar Dr.", "authors": "Anupriya Gogna and Angshul Majumdar", "title": "Fast Acquisition for Quantitative MRI Maps: Sparse Recovery from\n  Non-linear Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of estimating proton density and T1 maps from\ntwo partially sampled K-space scans such that the total acquisition time\nremains approximately the same as a single scan. Existing multi parametric non\nlinear curve fitting techniques require a large number (8 or more) of echoes to\nestimate the maps resulting in prolonged (clinically infeasible) acquisition\ntimes. Our simulation results show that our method yields very accurate and\nrobust results from only two partially sampled scans (total scan time being the\nsame as a single echo MRI). We model PD and T1 maps to be sparse in some\ntransform domain. The PD map is recovered via standard Compressed Sensing based\nrecovery technique. Estimating the T1 map requires solving an analysis prior\nsparse recovery problem from non linear measurements, since the relationship\nbetween T1 values and intensity values or K space samples is not linear. For\nthe first time in this work, we propose an algorithm for analysis prior sparse\nrecovery for non linear measurements. We have compared our approach with the\nonly existing technique based on matrix factorization from non linear\nmeasurements; our method yields considerably superior results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 04:24:27 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Gogna", "Anupriya", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1512.07729", "submitter": "Mahyar Najibi", "authors": "Mahyar Najibi, Mohammad Rastegari, Larry S. Davis", "title": "G-CNN: an Iterative Grid Based Object Detector", "comments": "To appear in Proceedings of IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2016. (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce G-CNN, an object detection technique based on CNNs which works\nwithout proposal algorithms. G-CNN starts with a multi-scale grid of fixed\nbounding boxes. We train a regressor to move and scale elements of the grid\ntowards objects iteratively. G-CNN models the problem of object detection as\nfinding a path from a fixed grid to boxes tightly surrounding the objects.\nG-CNN with around 180 boxes in a multi-scale grid performs comparably to Fast\nR-CNN which uses around 2K bounding boxes generated with a proposal technique.\nThis strategy makes detection faster by removing the object proposal stage as\nwell as reducing the number of boxes to be processed.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 07:02:37 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 20:40:11 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Najibi", "Mahyar", ""], ["Rastegari", "Mohammad", ""], ["Davis", "Larry S.", ""]]}, {"id": "1512.07815", "submitter": "Pankaj Pansari", "authors": "Pankaj Pansari, M. Pawan Kumar", "title": "Truncated Max-of-Convex Models", "comments": "Under review at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truncated convex models (TCM) are a special case of pairwise random fields\nthat have been widely used in computer vision. However, by restricting the\norder of the potentials to be at most two, they fail to capture useful image\nstatistics. We propose a natural generalization of TCM to high-order random\nfields, which we call truncated max-of-convex models (TMCM). The energy\nfunction of TMCM consistsof two types of potentials: (i) unary potential, which\nhas no restriction on its form; and (ii) clique potential, which is the sum of\nthe m largest truncated convex distances over all label pairs in a clique. The\nuse of a convex distance function encourages smoothness, while truncation\nallows for discontinuities in the labeling. By using m > 1, TMCM provides\nrobustness towards errors in the definition of the cliques. In order to\nminimize the energy function of a TMCM over all possible labelings, we design\nan efficient st-MINCUT based range expansion algorithm. We prove the accuracy\nof our algorithm by establishing strong multiplicative bounds for several\nspecial cases of interest. Using synthetic and standard real data sets, we\ndemonstrate the benefit of our high-order TMCM over pairwise TCM, as well as\nthe benefit of our range expansion algorithm over other st-MINCUT based\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 13:52:44 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 15:56:12 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Pansari", "Pankaj", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1512.07928", "submitter": "Seunghoon Hong", "authors": "Seunghoon Hong, Junhyuk Oh, Bohyung Han and Honglak Lee", "title": "Learning Transferrable Knowledge for Semantic Segmentation with Deep\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel weakly-supervised semantic segmentation algorithm based on\nDeep Convolutional Neural Network (DCNN). Contrary to existing\nweakly-supervised approaches, our algorithm exploits auxiliary segmentation\nannotations available for different categories to guide segmentations on images\nwith only image-level class labels. To make the segmentation knowledge\ntransferrable across categories, we design a decoupled encoder-decoder\narchitecture with attention model. In this architecture, the model generates\nspatial highlights of each category presented in an image using an attention\nmodel, and subsequently generates foreground segmentation for each highlighted\nregion using decoder. Combining attention model, we show that the decoder\ntrained with segmentation annotations in different categories can boost the\nperformance of weakly-supervised semantic segmentation. The proposed algorithm\ndemonstrates substantially improved performance compared to the\nstate-of-the-art weakly-supervised techniques in challenging PASCAL VOC 2012\ndataset when our model is trained with the annotations in 60 exclusive\ncategories in Microsoft COCO dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 22:33:27 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Hong", "Seunghoon", ""], ["Oh", "Junhyuk", ""], ["Han", "Bohyung", ""], ["Lee", "Honglak", ""]]}, {"id": "1512.07947", "submitter": "Alexander Wong", "authors": "Edward Li, Farzad Khalvati, Mohammad Javad Shafiee, Masoom A. Haider,\n  Alexander Wong", "title": "Sparse Reconstruction of Compressive Sensing MRI using Cross-Domain\n  Stochastically Fully Connected Conditional Random Fields", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is a crucial medical imaging technology for\nthe screening and diagnosis of frequently occurring cancers. However image\nquality may suffer by long acquisition times for MRIs due to patient motion, as\nwell as result in great patient discomfort. Reducing MRI acquisition time can\nreduce patient discomfort and as a result reduces motion artifacts from the\nacquisition process. Compressive sensing strategies, when applied to MRI, have\nbeen demonstrated to be effective at decreasing acquisition times significantly\nby sparsely sampling the \\emph{k}-space during the acquisition process.\nHowever, such a strategy requires advanced reconstruction algorithms to produce\nhigh quality and reliable images from compressive sensing MRI. This paper\nproposes a new reconstruction approach based on cross-domain stochastically\nfully connected conditional random fields (CD-SFCRF) for compressive sensing\nMRI. The CD-SFCRF introduces constraints in both \\emph{k}-space and spatial\ndomains within a stochastically fully connected graphical model to produce\nimproved MRI reconstruction. Experimental results using T2-weighted (T2w)\nimaging and diffusion-weighted imaging (DWI) of the prostate show strong\nperformance in preserving fine details and tissue structures in the\nreconstructed images when compared to other tested methods even at low sampling\nrates.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 02:58:53 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Li", "Edward", ""], ["Khalvati", "Farzad", ""], ["Shafiee", "Mohammad Javad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1512.07951", "submitter": "M. Avendi", "authors": "M. R. Avendi, A. Kheradvar, H. Jafarkhani", "title": "A Combined Deep-Learning and Deformable-Model Approach to Fully\n  Automatic Segmentation of the Left Ventricle in Cardiac MRI", "comments": "to appear in Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the left ventricle (LV) from cardiac magnetic resonance\nimaging (MRI) datasets is an essential step for calculation of clinical indices\nsuch as ventricular volume and ejection fraction. In this work, we employ deep\nlearning algorithms combined with deformable models to develop and evaluate a\nfully automatic segmentation tool for the LV from short-axis cardiac MRI\ndatasets. The method employs deep learning algorithms to learn the segmentation\ntask from the ground true data. Convolutional networks are employed to\nautomatically detect the LV chamber in MRI dataset. Stacked autoencoders are\nutilized to infer the shape of the LV. The inferred shape is incorporated into\ndeformable models to improve the accuracy and robustness of the segmentation.\nWe validated our method using 45 cardiac MR datasets taken from the MICCAI 2009\nLV segmentation challenge and showed that it outperforms the state-of-the art\nmethods. Excellent agreement with the ground truth was achieved. Validation\nmetrics, percentage of good contours, Dice metric, average perpendicular\ndistance and conformity, were computed as 96.69%, 0.94, 1.81mm and 0.86, versus\nthose of 79.2%-95.62%, 0.87-0.9, 1.76-2.97mm and 0.67-0.78, obtained by other\nmethods, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 03:35:15 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Avendi", "M. R.", ""], ["Kheradvar", "A.", ""], ["Jafarkhani", "H.", ""]]}, {"id": "1512.08047", "submitter": "Omar Al-Kadi", "authors": "Omar Sultan Al-Kadi", "title": "Assessment of texture measures susceptibility to noise in conventional\n  and contrast enhanced computed tomography lung tumour images", "comments": "10 pages, 9 figures", "journal-ref": "Computerized Medical Imaging and Graphics, vol.34, pp.494-503,\n  2010", "doi": "10.1016/j.compmedimag.2009.12.011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is one of the major problems that hinder an effective texture analysis\nof disease in medical images, which may cause variability in the reported\ndiagnosis. In this paper seven texture measurement methods (two wavelet, two\nmodel and three statistical based) were applied to investigate their\nsusceptibility to subtle noise caused by acquisition and reconstruction\ndeficiencies in computed tomography (CT) images. Features of lung tumours were\nextracted from two different conventional and contrast enhanced CT image\ndata-sets under filtered and noisy conditions. When measuring the noise in the\nbackground open-air region of the analysed CT images, noise of Gaussian and\nRayleigh distributions with varying mean and variance was encountered, and\nFisher distance was used to differentiate between an original extracted lung\ntumour region of interest (ROI) with the filtered and noisy reconstructed\nversions. It was determined that the wavelet packet (WP) and fractal dimension\nmeasures were the least affected, while the Gaussian Markov random field,\nrun-length and co-occurrence matrices were the most affected by noise.\nDepending on the selected ROI size, it was concluded that texture measures with\nfewer extracted features can decrease susceptibility to noise, with the WP and\nthe Gabor filter having a stable performance in both filtered and noisy CT\nversions and for both data-sets. Knowing how robust each texture measure under\nnoise presence is can assist physicians using an automated lung texture\nclassification system in choosing the appropriate feature extraction algorithm\nfor a more accurate diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 23:00:45 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Al-Kadi", "Omar Sultan", ""]]}, {"id": "1512.08049", "submitter": "Omar Al-Kadi", "authors": "Omar S. Al-Kadi", "title": "Texture measures combination for improved meningioma classification of\n  histopathological images", "comments": null, "journal-ref": "Pattern Recognition, vol.43, pp.2043-2053, 2010", "doi": "10.1016/j.patcog.2010.01.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing an improved technique which can assist pathologists in correctly\nclassifying meningioma tumours with a significant accuracy is our main\nobjective. The proposed technique, which is based on optimum texture measure\ncombination, inspects the separability of the RGB colour channels and selects\nthe channel which best segments the cell nuclei of the histopathological\nimages. The morphological gradient was applied to extract the region of\ninterest for each subtype and for elimination of possible noise (e.g. cracks)\nwhich might occur during biopsy preparation. Meningioma texture features are\nextracted by four different texture measures (two model-based and two\nstatistical-based) and then corresponding features are fused together in\ndifferent combinations after excluding highly correlated features, and a\nBayesian classifier was used for meningioma subtype discrimination. The\ncombined Gaussian Markov random field and run-length matrix texture measures\noutperformed all other combinations in terms of quantitatively characterising\nthe meningioma tissue, achieving an overall classification accuracy of 92.50%,\nimproving from 83.75% which is the best accuracy achieved if the texture\nmeasures are used individually.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 23:06:45 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Al-Kadi", "Omar S.", ""]]}, {"id": "1512.08051", "submitter": "Omar Al-Kadi", "authors": "Omar S. Al-Kadi", "title": "A Multiresolution Clinical Decision Support System Based on Fractal\n  Model Design for Classification of Histological Brain Tumours", "comments": null, "journal-ref": "Computerized Medical Imaging and Graphics, vol. 41, pp. 67-79,\n  2015", "doi": "10.1016/j.compmedimag.2014.05.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue texture is known to exhibit a heterogeneous or non-stationary nature,\ntherefore using a single resolution approach for optimum classification might\nnot suffice. A clinical decision support system that exploits the subband\ntextural fractal characteristics for best bases selection of meningioma brain\nhistopathological image classification is proposed. Each subband is analysed\nusing its fractal dimension instead of energy, which has the advantage of being\nless sensitive to image intensity and abrupt changes in tissue texture. The\nmost significant subband that best identifies texture discontinuities will be\nchosen for further decomposition, and its fractal characteristics would\nrepresent the optimal feature vector for classification. The performance was\ntested using the support vector machine (SVM), Bayesian and k-nearest neighbour\n(kNN) classifiers and a leave-one-patient-out method was employed for\nvalidation. Our method outperformed the classical energy based selection\napproaches, achieving for SVM, Bayesian and kNN classifiers an overall\nclassification accuracy of 94.12%, 92.50% and 79.70%, as compared to 86.31%,\n83.19% and 51.63% for the co-occurrence matrix, and 76.01%, 73.50% and 50.69%\nfor the energy texture signatures, respectively. These results indicate the\npotential usefulness as a decision support system that could complement\nradiologists diagnostic capability to discriminate higher order statistical\ntextural information, for which it would be otherwise difficult via ordinary\nhuman vision.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 23:21:06 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Al-Kadi", "Omar S.", ""]]}, {"id": "1512.08086", "submitter": "Dacheng Tao", "authors": "Shaoli Huang, Zhe Xu, Dacheng Tao, Ya Zhang", "title": "Part-Stacked CNN for Fine-Grained Visual Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of fine-grained visual categorization, the ability to\ninterpret models as human-understandable visual manuals is sometimes as\nimportant as achieving high classification accuracy. In this paper, we propose\na novel Part-Stacked CNN architecture that explicitly explains the fine-grained\nrecognition process by modeling subtle differences from object parts. Based on\nmanually-labeled strong part annotations, the proposed architecture consists of\na fully convolutional network to locate multiple object parts and a two-stream\nclassification network that en- codes object-level and part-level cues\nsimultaneously. By adopting a set of sharing strategies between the computation\nof multiple object parts, the proposed architecture is very efficient running\nat 20 frames/sec during inference. Experimental results on the CUB-200-2011\ndataset reveal the effectiveness of the proposed architecture, from both the\nperspective of classification accuracy and model interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 08:08:40 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Huang", "Shaoli", ""], ["Xu", "Zhe", ""], ["Tao", "Dacheng", ""], ["Zhang", "Ya", ""]]}, {"id": "1512.08103", "submitter": "Wei Liu", "authors": "Wei Liu, Yun Gu, Chunhua Shen, Xiaogang Chen, Qiang Wu and Jie Yang", "title": "Data Driven Robust Image Guided Depth Map Restoration", "comments": "9 pages, 9 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth maps captured by modern depth cameras such as Kinect and Time-of-Flight\n(ToF) are usually contaminated by missing data, noises and suffer from being of\nlow resolution. In this paper, we present a robust method for high-quality\nrestoration of a degraded depth map with the guidance of the corresponding\ncolor image. We solve the problem in an energy optimization framework that\nconsists of a novel robust data term and smoothness term. To accommodate not\nonly the noise but also the inconsistency between depth discontinuities and the\ncolor edges, we model both the data term and smoothness term with a robust\nexponential error norm function. We propose to use Iteratively Re-weighted\nLeast Squares (IRLS) methods for efficiently solving the resulting highly\nnon-convex optimization problem. More importantly, we further develop a\ndata-driven adaptive parameter selection scheme to properly determine the\nparameter in the model. We show that the proposed approach can preserve fine\ndetails and sharp depth discontinuities even for a large upsampling factor\n($8\\times$ for example). Experimental results on both simulated and real\ndatasets demonstrate that the proposed method outperforms recent\nstate-of-the-art methods in coping with the heavy noise, preserving sharp depth\ndiscontinuities and suppressing the texture copy artifacts.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 12:04:54 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Liu", "Wei", ""], ["Gu", "Yun", ""], ["Shen", "Chunhua", ""], ["Chen", "Xiaogang", ""], ["Wu", "Qiang", ""], ["Yang", "Jie", ""]]}, {"id": "1512.08212", "submitter": "Sina Honari", "authors": "David Rim, Sina Honari, Md Kamrul Hasan, Chris Pal", "title": "Improving Facial Analysis and Performance Driven Animation through\n  Disentangling Identity and Expression", "comments": "to appear in Image and Vision Computing Journal (IMAVIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present techniques for improving performance driven facial animation,\nemotion recognition, and facial key-point or landmark prediction using learned\nidentity invariant representations. Established approaches to these problems\ncan work well if sufficient examples and labels for a particular identity are\navailable and factors of variation are highly controlled. However, labeled\nexamples of facial expressions, emotions and key-points for new individuals are\ndifficult and costly to obtain. In this paper we improve the ability of\ntechniques to generalize to new and unseen individuals by explicitly modeling\npreviously seen variations related to identity and expression. We use a\nweakly-supervised approach in which identity labels are used to learn the\ndifferent factors of variation linked to identity separately from factors\nrelated to expression. We show how probabilistic modeling of these sources of\nvariation allows one to learn identity-invariant representations for\nexpressions which can then be used to identity-normalize various procedures for\nfacial expression analysis and animation control. We also show how to extend\nthe widely used techniques of active appearance models and constrained local\nmodels through replacing the underlying point distribution models which are\ntypically constructed using principal component analysis with\nidentity-expression factorized representations. We present a wide variety of\nexperiments in which we consistently improve performance on emotion\nrecognition, markerless performance-driven facial animation and facial\nkey-point tracking.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 12:34:26 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 23:01:37 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Rim", "David", ""], ["Honari", "Sina", ""], ["Hasan", "Md Kamrul", ""], ["Pal", "Chris", ""]]}, {"id": "1512.08413", "submitter": "Henry Y.T. Ngan", "authors": "Philip Lam, Lili Wang, Henry Y.T. Ngan, Nelson H.C. Yung, Anthony G.O.\n  Yeh", "title": "Outlier Detection In Large-scale Traffic Data By Na\\\"ive Bayes Method\n  and Gaussian Mixture Model Method", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is meaningful to detect outliers in traffic data for traffic management.\nHowever, this is a massive task for people from large-scale database to\ndistinguish outliers. In this paper, we present two methods: Kernel Smoothing\nNa\\\"ive Bayes (NB) method and Gaussian Mixture Model (GMM) method to\nautomatically detect any hardware errors as well as abnormal traffic events in\ntraffic data collected at a four-arm junction in Hong Kong. Traffic data was\nrecorded in a video format, and converted to spatial-temporal (ST) traffic\nsignals by statistics. The ST signals are then projected to a two-dimensional\n(2D) (x,y)-coordinate plane by Principal Component Analysis (PCA) for dimension\nreduction. We assume that inlier data are normal distributed. As such, the NB\nand GMM methods are successfully applied in outlier detection (OD) for traffic\ndata. The kernel smooth NB method assumes the existence of kernel distributions\nin traffic data and uses Bayes' Theorem to perform OD. In contrast, the GMM\nmethod believes the traffic data is formed by the mixture of Gaussian\ndistributions and exploits confidence region for OD. This paper would address\nthe modeling of each method and evaluate their respective performances.\nExperimental results show that the NB algorithm with Triangle kernel and GMM\nmethod achieve up to 93.78% and 94.50% accuracies, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 13:59:16 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Lam", "Philip", ""], ["Wang", "Lili", ""], ["Ngan", "Henry Y. T.", ""], ["Yung", "Nelson H. C.", ""], ["Yeh", "Anthony G. O.", ""]]}, {"id": "1512.08424", "submitter": "Martin Welk", "authors": "Martin Welk", "title": "Graph entropies in texture segmentation of images", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": "10.1002/9783527693245.ch7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the applicability of a set of texture descriptors introduced in\nrecent work by the author to texture-based segmentation of images. The texture\ndescriptors under investigation result from applying graph indices from\nquantitative graph theory to graphs encoding the local structure of images. The\nunderlying graphs arise from the computation of morphological amoebas as\nstructuring elements for adaptive morphology, either as weighted or unweighted\nDijkstra search trees or as edge-weighted pixel graphs within structuring\nelements. In the present paper we focus on texture descriptors in which the\ngraph indices are entropy-based, and use them in a geodesic active contour\nframework for image segmentation. Experiments on several synthetic and one\nreal-world image are shown to demonstrate texture segmentation by this\napproach. Forthermore, we undertake an attempt to analyse selected\nentropy-based texture descriptors with regard to what information about texture\nthey actually encode. Whereas this analysis uses some heuristic assumptions, it\nindicates that the graph-based texture descriptors are related to fractal\ndimension measures that have been proven useful in texture analysis.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 14:44:05 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Welk", "Martin", ""]]}, {"id": "1512.08475", "submitter": "Mohammad Reza Khosravi", "authors": "Mohammad Reza Khosravi, Mohammad Sharif-Yazd, Mohammad Kazem Moghimi,\n  Ahmad Keshavarz, Habib Rostami, Suleiman Mansouri", "title": "MRF-Based Multispectral Image Fusion Using an Adaptive Approach Based on\n  Edge-Guided Interpolation", "comments": "12 pages", "journal-ref": "Journal of Geographic Information System, vol. 9, no. 2, pp.\n  114-125 (2017)", "doi": "10.4236/jgis.2017.92008", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In interpretation of remote sensing images, it is possible that some images\nwhich are supplied by different sensors become incomprehensible. For better\nvisual perception of these images, it is essential to operate series of\npre-processing and elementary corrections and then operate a series of main\nprocessing steps for more precise analysis on the images. There are several\napproaches for processing which are depended on the type of remote sensing\nimages. The discussed approach in this article, i.e. image fusion, is the use\nof natural colors of an optical image for adding color to a grayscale satellite\nimage which gives us the ability for better observation of the HR image of OLI\nsensor of Landsat-8. This process with emphasis on details of fusion technique\nhas previously been performed; however, we are going to apply the concept of\nthe interpolation process. In fact, we see many important software tools such\nas ENVI and ERDAS as the most famous remote sensing image processing tools have\nonly classical interpolation techniques (such as bi-linear (BL) and\nbi-cubic/cubic convolution (CC)). Therefore, ENVI- and ERDAS-based researches\nin image fusion area and even other fusion researches often dont use new and\nbetter interpolators and are mainly concentrated on the fusion algorithms\ndetails for achieving a better quality, so we only focus on the interpolation\nimpact on fusion quality in Landsat-8 multispectral images. The important\nfeature of this approach is to use a statistical, adaptive, and edge-guided\ninterpolation method for improving the color quality in the images in practice.\nNumerical simulations show selecting the suitable interpolation techniques in\nMRF-based images creates better quality than the classical interpolators.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 18:18:43 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 11:27:15 GMT"}, {"version": "v3", "created": "Wed, 3 May 2017 16:25:52 GMT"}, {"version": "v4", "created": "Tue, 5 Dec 2017 13:31:21 GMT"}, {"version": "v5", "created": "Mon, 25 Mar 2019 08:46:12 GMT"}, {"version": "v6", "created": "Wed, 24 Apr 2019 09:53:32 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Khosravi", "Mohammad Reza", ""], ["Sharif-Yazd", "Mohammad", ""], ["Moghimi", "Mohammad Kazem", ""], ["Keshavarz", "Ahmad", ""], ["Rostami", "Habib", ""], ["Mansouri", "Suleiman", ""]]}, {"id": "1512.08512", "submitter": "Andrew Owens", "authors": "Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward\n  H. Adelson, William T. Freeman", "title": "Visually Indicated Sounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects make distinctive sounds when they are hit or scratched. These sounds\nreveal aspects of an object's material properties, as well as the actions that\nproduced them. In this paper, we propose the task of predicting what sound an\nobject makes when struck as a way of studying physical interactions within a\nvisual scene. We present an algorithm that synthesizes sound from silent videos\nof people hitting and scratching objects with a drumstick. This algorithm uses\na recurrent neural network to predict sound features from videos and then\nproduces a waveform from these features with an example-based synthesis\nprocedure. We show that the sounds predicted by our model are realistic enough\nto fool participants in a \"real or fake\" psychophysical experiment, and that\nthey convey significant information about material properties and physical\ninteractions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 20:56:50 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 03:03:04 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Owens", "Andrew", ""], ["Isola", "Phillip", ""], ["McDermott", "Josh", ""], ["Torralba", "Antonio", ""], ["Adelson", "Edward H.", ""], ["Freeman", "William T.", ""]]}, {"id": "1512.08648", "submitter": "Grzegorz Kurzejamski", "authors": "Grzegorz Kurzejamski, Jacek Zawistowski and Grzegorz Sarwas", "title": "A framework for robust object multi-detection with a vote aggregation\n  and a cascade filtering", "comments": "23rd International Conference in Central Europe on Computer Graphics,\n  Visualization and Computer Vision (WSCG) 2015 Short Paper. Computer Science\n  Research Notes CSRN 2502, ISSN 2464-4617, ISBN 978-80-86943-66-4, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework designed for the multi-object detection\npurposes and adjusted for the application of product search on the market\nshelves. The framework uses a single feedback loop and a pattern resizing\nmechanism to demonstrate the top effectiveness of the state-of-the-art local\nfeatures. A high detection rate with a low false detection chance can be\nachieved with use of only one pattern per object and no manual parameters\nadjustments. The method incorporates well known local features and a basic\nmatching process to create a reliable voting space. Further steps comprise of\nmetric transformations, graphical vote space representation, two-phase vote\naggregation process and a cascade of verifying filters.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 10:54:40 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Kurzejamski", "Grzegorz", ""], ["Zawistowski", "Jacek", ""], ["Sarwas", "Grzegorz", ""]]}, {"id": "1512.08669", "submitter": "Da-Han Wang", "authors": "Da-Han Wang, Hanzi Wang, Dong Zhang, Jonathan Li, David Zhang", "title": "Robust Scene Text Recognition Using Sparse Coding based Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective scene text recognition method using\nsparse coding based features, called Histograms of Sparse Codes (HSC) features.\nFor character detection, we use the HSC features instead of using the\nHistograms of Oriented Gradients (HOG) features. The HSC features are extracted\nby computing sparse codes with dictionaries that are learned from data using\nK-SVD, and aggregating per-pixel sparse codes to form local histograms. For\nword recognition, we integrate multiple cues including character detection\nscores and geometric contexts in an objective function. The final recognition\nresults are obtained by searching for the words which correspond to the maximum\nvalue of the objective function. The parameters in the objective function are\nlearned using the Minimum Classification Error (MCE) training method.\nExperiments on several challenging datasets demonstrate that the proposed\nHSC-based scene text recognition method outperforms HOG-based methods\nsignificantly and outperforms most state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 12:50:40 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Wang", "Da-Han", ""], ["Wang", "Hanzi", ""], ["Zhang", "Dong", ""], ["Li", "Jonathan", ""], ["Zhang", "David", ""]]}, {"id": "1512.08814", "submitter": "Omar Al-Kadi", "authors": "Omar Al-Kadi", "title": "Combined statistical and model based texture features for improved image\n  classification", "comments": "4th International Conference on Advances in Medical, Signal and\n  Information Processing, pp. 175-178, Italy, 2008", "journal-ref": null, "doi": "10.1049/cp:20080455", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to improve the accuracy of texture classification based on\nextracting texture features using five different texture methods and\nclassifying the patterns using a naive Bayesian classifier. Three\nstatistical-based and two model-based methods are used to extract texture\nfeatures from eight different texture images, then their accuracy is ranked\nafter using each method individually and in pairs. The accuracy improved up to\n97.01% when model based -Gaussian Markov random field (GMRF) and fractional\nBrownian motion (fBm) - were used together for classification as compared to\nthe highest achieved using each of the five different methods alone; and proved\nto be better in classifying as compared to statistical methods. Also, using\nGMRF with statistical based methods, such as Gray level co-occurrence (GLCM)\nand run-length (RLM) matrices, improved the overall accuracy to 96.94% and\n96.55%; respectively.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 23:28:13 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Al-Kadi", "Omar", ""]]}, {"id": "1512.09041", "submitter": "Chenliang Xu", "authors": "Chenliang Xu and Jason J. Corso", "title": "Actor-Action Semantic Segmentation with Grouping Process Models", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actor-action semantic segmentation made an important step toward advanced\nvideo understanding problems: what action is happening; who is performing the\naction; and where is the action in space-time. Current models for this problem\nare local, based on layered CRFs, and are unable to capture long-ranging\ninteraction of video parts. We propose a new model that combines these local\nlabeling CRFs with a hierarchical supervoxel decomposition. The supervoxels\nprovide cues for possible groupings of nodes, at various scales, in the CRFs to\nencourage adaptive, high-order groups for more effective labeling. Our model is\ndynamic and continuously exchanges information during inference: the local CRFs\ninfluence what supervoxels in the hierarchy are active, and these active nodes\ninfluence the connectivity in the CRF; we hence call it a grouping process\nmodel. The experimental results on a recent large-scale video dataset show a\nlarge margin of 60% relative improvement over the state of the art, which\ndemonstrates the effectiveness of the dynamic, bidirectional flow between\nlabeling and grouping.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 18:07:45 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Xu", "Chenliang", ""], ["Corso", "Jason J.", ""]]}, {"id": "1512.09049", "submitter": "Chenliang Xu", "authors": "Chenliang Xu and Jason J. Corso", "title": "LIBSVX: A Supervoxel Library and Benchmark for Early Video Processing", "comments": "In Review at International Journal of Computer Vision", "journal-ref": "Int J Comput Vis (2016) 119: 272", "doi": "10.1007/s11263-016-0906-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervoxel segmentation has strong potential to be incorporated into early\nvideo analysis as superpixel segmentation has in image analysis. However, there\nare many plausible supervoxel methods and little understanding as to when and\nwhere each is most appropriate. Indeed, we are not aware of a single\ncomparative study on supervoxel segmentation. To that end, we study seven\nsupervoxel algorithms, including both off-line and streaming methods, in the\ncontext of what we consider to be a good supervoxel: namely, spatiotemporal\nuniformity, object/region boundary detection, region compression and parsimony.\nFor the evaluation we propose a comprehensive suite of seven quality metrics to\nmeasure these desirable supervoxel characteristics. In addition, we evaluate\nthe methods in a supervoxel classification task as a proxy for subsequent\nhigh-level uses of the supervoxels in video analysis. We use six existing\nbenchmark video datasets with a variety of content-types and dense human\nannotations. Our findings have led us to conclusive evidence that the\nhierarchical graph-based (GBH), segmentation by weighted aggregation (SWA) and\ntemporal superpixels (TSP) methods are the top-performers among the seven\nmethods. They all perform well in terms of segmentation accuracy, but vary in\nregard to the other desiderata: GBH captures object boundaries best; SWA has\nthe best potential for region compression; and TSP achieves the best\nundersegmentation error.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 18:25:19 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Xu", "Chenliang", ""], ["Corso", "Jason J.", ""]]}, {"id": "1512.09194", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou and Jia-Nan Wu and Yuxin Wu and Xinyu Zhou", "title": "Exploiting Local Structures with the Kronecker Layer in Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study a technique to reduce the number of\nparameters and computation time in convolutional neural networks. We use\nKronecker product to exploit the local structures within convolution and\nfully-connected layers, by replacing the large weight matrices by combinations\nof multiple Kronecker products of smaller matrices. Just as the Kronecker\nproduct is a generalization of the outer product from vectors to matrices, our\nmethod is a generalization of the low rank approximation method for convolution\nneural networks. We also introduce combinations of different shapes of\nKronecker product to increase modeling capacity. Experiments on SVHN, scene\ntext recognition and ImageNet dataset demonstrate that we can achieve $3.3\n\\times$ speedup or $3.6 \\times$ parameter reduction with less than 1\\% drop in\naccuracy, showing the effectiveness and efficiency of our method. Moreover, the\ncomputation efficiency of Kronecker layer makes using larger feature map\npossible, which in turn enables us to outperform the previous state-of-the-art\non both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese character\nrecognition) datasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 01:32:16 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 01:19:38 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Zhou", "Shuchang", ""], ["Wu", "Jia-Nan", ""], ["Wu", "Yuxin", ""], ["Zhou", "Xinyu", ""]]}, {"id": "1512.09227", "submitter": "Zemin Zhang", "authors": "Zemin Zhang and Shuchin Aeron", "title": "Denoising and Completion of 3D Data via Multidimensional Dictionary\n  Learning", "comments": "9 pages, submitted to Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new dictionary learning algorithm for multidimensional data\nis proposed. Unlike most conventional dictionary learning methods which are\nderived for dealing with vectors or matrices, our algorithm, named KTSVD,\nlearns a multidimensional dictionary directly via a novel algebraic approach\nfor tensor factorization as proposed in [3, 12, 13]. Using this approach one\ncan define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-D\ndata to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, based\non the idea of sparse coding (using group-sparsity over multidimensional\ncoefficient vectors), alternates between estimating a compact representation\nand dictionary learning. We analyze our KTSVD algorithm and demonstrate its\nresult on video completion and multispectral image denoising.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 06:37:54 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Zhang", "Zemin", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1512.09272", "submitter": "Vijay Kumar  B G Dr", "authors": "Vijay Kumar B G, Gustavo Carneiro, Ian Reid", "title": "Learning Local Image Descriptors with Deep Siamese and Triplet\n  Convolutional Networks by Minimising Global Loss Functions", "comments": "IEEE Conference on Computer Vision and Pattern Recognition 2016 (CVPR\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent innovations in training deep convolutional neural network (ConvNet)\nmodels have motivated the design of new methods to automatically learn local\nimage descriptors. The latest deep ConvNets proposed for this task consist of a\nsiamese network that is trained by penalising misclassification of pairs of\nlocal image patches. Current results from machine learning show that replacing\nthis siamese by a triplet network can improve the classification accuracy in\nseveral problems, but this has yet to be demonstrated for local image\ndescriptor learning. Moreover, current siamese and triplet networks have been\ntrained with stochastic gradient descent that computes the gradient from\nindividual pairs or triplets of local image patches, which can make them prone\nto overfitting. In this paper, we first propose the use of triplet networks for\nthe problem of local image descriptor learning. Furthermore, we also propose\nthe use of a global loss that minimises the overall classification error in the\ntraining set, which can improve the generalisation capability of the model.\nUsing the UBC benchmark dataset for comparing local image descriptors, we show\nthat the triplet network produces a more accurate embedding than the siamese\nnetwork in terms of the UBC dataset errors. Moreover, we also demonstrate that\na combination of the triplet and global losses produces the best embedding in\nthe field, using this triplet network. Finally, we also show that the use of\nthe central-surround siamese network trained with the global loss produces the\nbest result of the field on the UBC dataset. Pre-trained models are available\nonline at https://github.com/vijaykbg/deep-patchmatch\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 12:36:28 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 06:47:57 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["G", "Vijay Kumar B", ""], ["Carneiro", "Gustavo", ""], ["Reid", "Ian", ""]]}, {"id": "1512.09300", "submitter": "Anders Boesen Lindbo Larsen", "authors": "Anders Boesen Lindbo Larsen, S{\\o}ren Kaae S{\\o}nderby, Hugo\n  Larochelle, Ole Winther", "title": "Autoencoding beyond pixels using a learned similarity metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an autoencoder that leverages learned representations to better\nmeasure similarities in data space. By combining a variational autoencoder with\na generative adversarial network we can use learned feature representations in\nthe GAN discriminator as basis for the VAE reconstruction objective. Thereby,\nwe replace element-wise errors with feature-wise errors to better capture the\ndata distribution while offering invariance towards e.g. translation. We apply\nour method to images of faces and show that it outperforms VAEs with\nelement-wise similarity measures in terms of visual fidelity. Moreover, we show\nthat the method learns an embedding in which high-level abstract visual\nfeatures (e.g. wearing glasses) can be modified using simple arithmetic.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 14:53:39 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 21:18:27 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Larsen", "Anders Boesen Lindbo", ""], ["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["Larochelle", "Hugo", ""], ["Winther", "Ole", ""]]}]