[{"id": "2002.00011", "submitter": "Chi Nok Enoch Kan", "authors": "Chi Nok Enoch Kan, Najibakram Maheenaboobacker, Dong Hye Ye", "title": "Age-Conditioned Synthesis of Pediatric Computed Tomography with\n  Auxiliary Classifier Generative Adversarial Networks", "comments": "Accepted for publication at IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2020", "journal-ref": "2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI\n  2020)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a popular and powerful tool in computed tomography (CT)\nimage processing such as organ segmentation, but its requirement of large\ntraining datasets remains a challenge. Even though there is a large anatomical\nvariability for children during their growth, the training datasets for\npediatric CT scans are especially hard to obtain due to risks of radiation to\nchildren. In this paper, we propose a method to conditionally synthesize\nrealistic pediatric CT images using a new auxiliary classifier generative\nadversarial network (ACGAN) architecture by taking age information into\naccount. The proposed network generated age-conditioned high-resolution CT\nimages to enrich pediatric training datasets.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 16:52:10 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Kan", "Chi Nok Enoch", ""], ["Maheenaboobacker", "Najibakram", ""], ["Ye", "Dong Hye", ""]]}, {"id": "2002.00022", "submitter": "Jun-Jie Huang", "authors": "Jun-Jie Huang and Pier Luigi Dragotti", "title": "Learning Deep Analysis Dictionaries -- Part II: Convolutional\n  Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a Deep Convolutional Analysis Dictionary Model\n(DeepCAM) by learning convolutional dictionaries instead of unstructured\ndictionaries as in the case of deep analysis dictionary model introduced in the\ncompanion paper. Convolutional dictionaries are more suitable for processing\nhigh-dimensional signals like for example images and have only a small number\nof free parameters. By exploiting the properties of a convolutional dictionary,\nwe present an efficient convolutional analysis dictionary learning approach. A\nL-layer DeepCAM consists of L layers of convolutional analysis dictionary and\nelement-wise soft-thresholding pairs and a single layer of convolutional\nsynthesis dictionary. Similar to DeepAM, each convolutional analysis dictionary\nis composed of a convolutional Information Preserving Analysis Dictionary\n(IPAD) and a convolutional Clustering Analysis Dictionary (CAD). The IPAD and\nthe CAD are learned using variations of the proposed learning algorithm. We\ndemonstrate that DeepCAM is an effective multilayer convolutional model and, on\nsingle image super-resolution, achieves performance comparable with other\nmethods while also showing good generalization capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 19:02:10 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Huang", "Jun-Jie", ""], ["Dragotti", "Pier Luigi", ""]]}, {"id": "2002.00065", "submitter": "V\\'itor Albiero", "authors": "V\\'itor Albiero, Krishnapriya K.S., Kushal Vangara, Kai Zhang, Michael\n  C. King, and Kevin W. Bowyer", "title": "Analysis of Gender Inequality In Face Recognition Accuracy", "comments": "Paper will appear at The 2nd Workshop on Demographic Variation in the\n  Performance of Biometric Systems at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive analysis of how and why face recognition accuracy\ndiffers between men and women. We show that accuracy is lower for women due to\nthe combination of (1) the impostor distribution for women having a skew toward\nhigher similarity scores, and (2) the genuine distribution for women having a\nskew toward lower similarity scores. We show that this phenomenon of the\nimpostor and genuine distributions for women shifting closer towards each other\nis general across datasets of African-American, Caucasian, and Asian faces. We\nshow that the distribution of facial expressions may differ between\nmale/female, but that the accuracy difference persists for image subsets rated\nconfidently as neutral expression. The accuracy difference also persists for\nimage subsets rated as close to zero pitch angle. Even when removing images\nwith forehead partially occluded by hair/hat, the same impostor/genuine\naccuracy difference persists. We show that the female genuine distribution\nimproves when only female images without facial cosmetics are used, but that\nthe female impostor distribution also degrades at the same time. Lastly, we\nshow that the accuracy difference persists even if a state-of-the-art deep\nlearning method is trained from scratch using training data explicitly balanced\nbetween male and female images and subjects.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 21:32:53 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Albiero", "V\u00edtor", ""], ["S.", "Krishnapriya K.", ""], ["Vangara", "Kushal", ""], ["Zhang", "Kai", ""], ["King", "Michael C.", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "2002.00092", "submitter": "Ao Luo", "authors": "Ao Luo, Fan Yang, Xin Li, Dong Nie, Zhicheng Jiao, Shangchen Zhou and\n  Hong Cheng", "title": "Hybrid Graph Neural Networks for Crowd Counting", "comments": "To appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is an important yet challenging task due to the large scale\nand density variation. Recent investigations have shown that distilling rich\nrelations among multi-scale features and exploiting useful information from the\nauxiliary task, i.e., localization, are vital for this task. Nevertheless, how\nto comprehensively leverage these relations within a unified network\narchitecture is still a challenging problem. In this paper, we present a novel\nnetwork structure called Hybrid Graph Neural Network (HyGnn) which targets to\nrelieve the problem by interweaving the multi-scale features for crowd density\nas well as its auxiliary task (localization) together and performing joint\nreasoning over a graph. Specifically, HyGnn integrates a hybrid graph to\njointly represent the task-specific feature maps of different scales as nodes,\nand two types of relations as edges:(i) multi-scale relations for capturing the\nfeature dependencies across scales and (ii) mutual beneficial relations\nbuilding bridges for the cooperation between counting and localization. Thus,\nthrough message passing, HyGnn can distill rich relations between the nodes to\nobtain more powerful representations, leading to robust and accurate results.\nOur HyGnn performs significantly well on four challenging datasets:\nShanghaiTech Part A, ShanghaiTech Part B, UCF_CC_50 and UCF_QNRF, outperforming\nthe state-of-the-art approaches by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 23:06:03 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Luo", "Ao", ""], ["Yang", "Fan", ""], ["Li", "Xin", ""], ["Nie", "Dong", ""], ["Jiao", "Zhicheng", ""], ["Zhou", "Shangchen", ""], ["Cheng", "Hong", ""]]}, {"id": "2002.00104", "submitter": "Jun Fang", "authors": "Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios\n  Georgiadis, Joseph Hassoun", "title": "Post-Training Piecewise Linear Quantization for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization plays an important role in the energy-efficient deployment of\ndeep neural networks on resource-limited devices. Post-training quantization is\nhighly desirable since it does not require retraining or access to the full\ntraining dataset. The well-established uniform scheme for post-training\nquantization achieves satisfactory results by converting neural networks from\nfull-precision to 8-bit fixed-point integers. However, it suffers from\nsignificant performance degradation when quantizing to lower bit-widths. In\nthis paper, we propose a piecewise linear quantization (PWLQ) scheme to enable\naccurate approximation for tensor values that have bell-shaped distributions\nwith long tails. Our approach breaks the entire quantization range into\nnon-overlapping regions for each tensor, with each region being assigned an\nequal number of quantization levels. Optimal breakpoints that divide the entire\nrange are found by minimizing the quantization error. Compared to\nstate-of-the-art post-training quantization methods, experimental results show\nthat our proposed method achieves superior performance on image classification,\nsemantic segmentation, and object detection with minor overhead.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 23:47:00 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 18:49:40 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Fang", "Jun", ""], ["Shafiee", "Ali", ""], ["Abdel-Aziz", "Hamzah", ""], ["Thorsley", "David", ""], ["Georgiadis", "Georgios", ""], ["Hassoun", "Joseph", ""]]}, {"id": "2002.00113", "submitter": "Hossein Talebi", "authors": "Hossein Talebi, Damien Kelly, Xiyang Luo, Ignacio Garcia Dorado, Feng\n  Yang, Peyman Milanfar and Michael Elad", "title": "Better Compression with Deep Pre-Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Could we compress images via standard codecs while avoiding visible\nartifacts? The answer is obvious -- this is doable as long as the bit budget is\ngenerous enough. What if the allocated bit-rate for compression is\ninsufficient? Then unfortunately, artifacts are a fact of life. Many attempts\nwere made over the years to fight this phenomenon, with various degrees of\nsuccess. In this work we aim to break the unholy connection between bit-rate\nand image quality, and propose a way to circumvent compression artifacts by\npre-editing the incoming image and modifying its content to fit the given bits.\nWe design this editing operation as a learned convolutional neural network, and\nformulate an optimization problem for its training. Our loss takes into account\na proximity between the original image and the edited one, a bit-budget penalty\nover the proposed image, and a no-reference image quality measure for forcing\nthe outcome to be visually pleasing. The proposed approach is demonstrated on\nthe popular JPEG compression, showing savings in bits and/or improvements in\nvisual quality, obtained with intricate editing effects.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 00:35:46 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 21:41:05 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 18:44:52 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Talebi", "Hossein", ""], ["Kelly", "Damien", ""], ["Luo", "Xiyang", ""], ["Dorado", "Ignacio Garcia", ""], ["Yang", "Feng", ""], ["Milanfar", "Peyman", ""], ["Elad", "Michael", ""]]}, {"id": "2002.00118", "submitter": "Xingzhe He", "authors": "Xingzhe He, Helen Lu Cao, Bo Zhu", "title": "AdvectiveNet: An Eulerian-Lagrangian Fluidic reservoir for Point Cloud\n  Processing", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel physics-inspired deep learning approach for point\ncloud processing motivated by the natural flow phenomena in fluid mechanics.\nOur learning architecture jointly defines data in an Eulerian world space,\nusing a static background grid, and a Lagrangian material space, using moving\nparticles. By introducing this Eulerian-Lagrangian representation, we are able\nto naturally evolve and accumulate particle features using flow velocities\ngenerated from a generalized, high-dimensional force field. We demonstrate the\nefficacy of this system by solving various point cloud classification and\nsegmentation problems with state-of-the-art performance. The entire geometric\nreservoir and data flow mimics the pipeline of the classic PIC/FLIP scheme in\nmodeling natural flow, bridging the disciplines of geometric machine learning\nand physical simulation.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 01:21:05 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 01:33:56 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 19:44:09 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["He", "Xingzhe", ""], ["Cao", "Helen Lu", ""], ["Zhu", "Bo", ""]]}, {"id": "2002.00120", "submitter": "Lori Dalton", "authors": "Ali Foroughi pour and Lori A. Dalton", "title": "On the Consistency of Optimal Bayesian Feature Selection in the Presence\n  of Correlations", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal Bayesian feature selection (OBFS) is a multivariate supervised\nscreening method designed from the ground up for biomarker discovery. In this\nwork, we prove that Gaussian OBFS is strongly consistent under mild conditions,\nand provide rates of convergence for key posteriors in the framework. These\nresults are of enormous importance, since they identify precisely what features\nare selected by OBFS asymptotically, characterize the relative rates of\nconvergence for posteriors on different types of features, provide conditions\nthat guarantee convergence, justify the use of OBFS when its internal\nassumptions are invalid, and set the stage for understanding the asymptotic\nbehavior of other algorithms based on the OBFS framework.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 01:41:08 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["pour", "Ali Foroughi", ""], ["Dalton", "Lori A.", ""]]}, {"id": "2002.00133", "submitter": "Zhengzhe Liu", "authors": "Zhengzhe Liu, Xiaojuan Qi, Philip Torr", "title": "Global Texture Enhancement for Fake Face Detection in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) can generate realistic fake face\nimages that can easily fool human beings.On the contrary, a common\nConvolutional Neural Network(CNN) discriminator can achieve more than 99.9%\naccuracyin discerning fake/real images. In this paper, we conduct an empirical\nstudy on fake/real faces, and have two important observations: firstly, the\ntexture of fake faces is substantially different from real ones; secondly,\nglobal texture statistics are more robust to image editing and transferable to\nfake faces from different GANs and datasets. Motivated by the above\nobservations, we propose a new architecture coined as Gram-Net, which leverages\nglobal image texture representations for robust fake image detection.\nExperimental results on several datasets demonstrate that our Gram-Net\noutperforms existing approaches. Especially, our Gram-Netis more robust to\nimage editings, e.g. down-sampling, JPEG compression, blur, and noise. More\nimportantly, our Gram-Net generalizes significantly better in detecting fake\nfaces from GAN models not seen in the training phase and can perform decently\nin detecting fake natural images.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 03:46:23 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 17:14:29 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 00:54:37 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Liu", "Zhengzhe", ""], ["Qi", "Xiaojuan", ""], ["Torr", "Philip", ""]]}, {"id": "2002.00137", "submitter": "Lijun Yu", "authors": "Lijun Yu, Peng Chen, Wenhe Liu, Guoliang Kang, Alexander G. Hauptmann", "title": "Training-free Monocular 3D Event Detection System for Traffic\n  Surveillance", "comments": "To be published in 2019 IEEE International Conference on Big Data\n  (Big Data), IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of detecting traffic events in a surveillance\nscenario, including the detection of both vehicle actions and traffic\ncollisions. Existing event detection systems are mostly learning-based and have\nachieved convincing performance when a large amount of training data is\navailable. However, in real-world scenarios, collecting sufficient labeled\ntraining data is expensive and sometimes impossible (e.g. for traffic collision\ndetection). Moreover, the conventional 2D representation of surveillance views\nis easily affected by occlusions and different camera views in nature. To deal\nwith the aforementioned problems, in this paper, we propose a training-free\nmonocular 3D event detection system for traffic surveillance. Our system\nfirstly projects the vehicles into the 3D Euclidean space and estimates their\nkinematic states. Then we develop multiple simple yet effective ways to\nidentify the events based on the kinematic patterns, which need no further\ntraining. Consequently, our system is robust to the occlusions and the\nviewpoint changes. Exclusive experiments report the superior result of our\nmethod on large-scale real-world surveillance datasets, which validates the\neffectiveness of our proposed system.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 04:42:57 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Yu", "Lijun", ""], ["Chen", "Peng", ""], ["Liu", "Wenhe", ""], ["Kang", "Guoliang", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "2002.00140", "submitter": "Yigit Alparslan", "authors": "Yigit Alparslan, Ken Alparslan, Mannika Kshettry, Louis Kratz", "title": "Towards Evaluating Gaussian Blurring in Perceptual Hashing as a Facial\n  Image Filter", "comments": "5 pages, fixed typos, added references in Introduction section, added\n  co-author due to post-publication contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the growth in social media, there is a huge amount of images of faces\navailable on the internet. Often, people use other people's pictures on their\nown profile. Perceptual hashing is often used to detect whether two images are\nidentical. Therefore, it can be used to detect whether people are misusing\nothers' pictures. In perceptual hashing, a hash is calculated for a given\nimage, and a new test image is mapped to one of the existing hashes if\nduplicate features are present. Therefore, it can be used as an image filter to\nflag banned image content or adversarial attacks --which are modifications that\nare made on purpose to deceive the filter-- even though the content might be\nchanged to deceive the filters. For this reason, it is critical for perceptual\nhashing to be robust enough to take transformations such as resizing, cropping,\nand slight pixel modifications into account. In this paper, we would like to\npropose to experiment with effect of gaussian blurring in perceptual hashing\nfor detecting misuse of personal images specifically for face images. We\nhypothesize that use of gaussian blurring on the image before calculating its\nhash will increase the accuracy of our filter that detects adversarial attacks\nwhich consist of image cropping, adding text annotation, and image rotation.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 04:52:41 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 22:00:06 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Alparslan", "Yigit", ""], ["Alparslan", "Ken", ""], ["Kshettry", "Mannika", ""], ["Kratz", "Louis", ""]]}, {"id": "2002.00153", "submitter": "Wenbin Li", "authors": "Wenbin Li, Lei Wang, Jing Huo, Yinghuan Shi, Yang Gao, and Jiebo Luo", "title": "Asymmetric Distribution Measure for Few-shot Learning", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core idea of metric-based few-shot image classification is to directly\nmeasure the relations between query images and support classes to learn\ntransferable feature embeddings. Previous work mainly focuses on image-level\nfeature representations, which actually cannot effectively estimate a class's\ndistribution due to the scarcity of samples. Some recent work shows that local\ndescriptor based representations can achieve richer representations than\nimage-level based representations. However, such works are still based on a\nless effective instance-level metric, especially a symmetric metric, to measure\nthe relations between query images and support classes. Given the natural\nasymmetric relation between a query image and a support class, we argue that an\nasymmetric measure is more suitable for metric-based few-shot learning. To that\nend, we propose a novel Asymmetric Distribution Measure (ADM) network for\nfew-shot learning by calculating a joint local and global asymmetric measure\nbetween two multivariate local distributions of queries and classes. Moreover,\na task-aware Contrastive Measure Strategy (CMS) is proposed to further enhance\nthe measure function. On popular miniImageNet and tieredImageNet, we achieve\n$3.02\\%$ and $1.56\\%$ gains over the state-of-the-art method on the $5$-way\n$1$-shot task, respectively, validating our innovative design of asymmetric\ndistribution measures for few-shot learning.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 06:41:52 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Li", "Wenbin", ""], ["Wang", "Lei", ""], ["Huo", "Jing", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""], ["Luo", "Jiebo", ""]]}, {"id": "2002.00169", "submitter": "Biao Gong", "authors": "Chenggang Yan, Biao Gong, Yuxuan Wei, Yue Gao", "title": "Deep Multi-View Enhancement Hashing for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing is an efficient method for nearest neighbor search in large-scale\ndata space by embedding high-dimensional feature descriptors into a similarity\npreserving Hamming space with a low dimension. However, large-scale high-speed\nretrieval through binary code has a certain degree of reduction in retrieval\naccuracy compared to traditional retrieval methods. We have noticed that\nmulti-view methods can well preserve the diverse characteristics of data.\nTherefore, we try to introduce the multi-view deep neural network into the hash\nlearning field, and design an efficient and innovative retrieval model, which\nhas achieved a significant improvement in retrieval performance. In this paper,\nwe propose a supervised multi-view hash model which can enhance the multi-view\ninformation through neural networks. This is a completely new hash learning\nmethod that combines multi-view and deep learning methods. The proposed method\nutilizes an effective view stability evaluation method to actively explore the\nrelationship among views, which will affect the optimization direction of the\nentire network. We have also designed a variety of multi-data fusion methods in\nthe Hamming space to preserve the advantages of both convolution and\nmulti-view. In order to avoid excessive computing resources on the enhancement\nprocedure during retrieval, we set up a separate structure called memory\nnetwork which participates in training together. The proposed method is\nsystematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and\nthe results show that our method significantly outperforms the state-of-the-art\nsingle-view and multi-view hashing methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 08:32:27 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 02:41:20 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Yan", "Chenggang", ""], ["Gong", "Biao", ""], ["Wei", "Yuxuan", ""], ["Gao", "Yue", ""]]}, {"id": "2002.00176", "submitter": "Jie Luo", "authors": "Bin Wen, Jie Luo, Xianglong Liu, Lei Huang", "title": "Unbiased Scene Graph Generation via Rich and Fair Semantic Extraction", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting graph representation of visual scenes in image is a challenging\ntask in computer vision. Although there has been encouraging progress of scene\ngraph generation in the past decade, we surprisingly find that the performance\nof existing approaches is largely limited by the strong biases, which mainly\nstem from (1) unconsciously assuming relations with certain semantic properties\nsuch as symmetric and (2) imbalanced annotations over different relations. To\nalleviate the negative effects of these biases, we proposed a new and simple\narchitecture named Rich and Fair semantic extraction network (RiFa for short),\nto not only capture rich semantic properties of the relations, but also fairly\npredict relations with different scale of annotations. Using pseudo-siamese\nnetworks, RiFa embeds the subject and object respectively to distinguish their\nsemantic differences and meanwhile preserve their underlying semantic\nproperties. Then, it further predicts subject-object relations based on both\nthe visual and semantic features of entities under certain contextual area, and\nfairly ranks the relation predictions for those with a few annotations.\nExperiments on the popular Visual Genome dataset show that RiFa achieves\nstate-of-the-art performance under several challenging settings of scene graph\ntask. Especially, it performs significantly better on capturing different\nsemantic properties of relations, and obtains the best overall per relation\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 09:28:44 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wen", "Bin", ""], ["Luo", "Jie", ""], ["Liu", "Xianglong", ""], ["Huang", "Lei", ""]]}, {"id": "2002.00179", "submitter": "Zifei Zhang", "authors": "Zifei Zhang, Kai Qiao, Lingyun Jiang, Linyuan Wang, and Bin Yan", "title": "AdvJND: Generating Adversarial Examples with Just Noticeable Difference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with traditional machine learning models, deep neural networks\nperform better, especially in image classification tasks. However, they are\nvulnerable to adversarial examples. Adding small perturbations on examples\ncauses a good-performance model to misclassify the crafted examples, without\ncategory differences in the human eyes, and fools deep models successfully.\nThere are two requirements for generating adversarial examples: the attack\nsuccess rate and image fidelity metrics. Generally, perturbations are increased\nto ensure the adversarial examples' high attack success rate; however, the\nadversarial examples obtained have poor concealment. To alleviate the tradeoff\nbetween the attack success rate and image fidelity, we propose a method named\nAdvJND, adding visual model coefficients, just noticeable difference\ncoefficients, in the constraint of a distortion function when generating\nadversarial examples. In fact, the visual subjective feeling of the human eyes\nis added as a priori information, which decides the distribution of\nperturbations, to improve the image quality of adversarial examples. We tested\nour method on the FashionMNIST, CIFAR10, and MiniImageNet datasets. Adversarial\nexamples generated by our AdvJND algorithm yield gradient distributions that\nare similar to those of the original inputs. Hence, the crafted noise can be\nhidden in the original inputs, thus improving the attack concealment\nsignificantly.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 09:55:27 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 09:34:17 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Zhang", "Zifei", ""], ["Qiao", "Kai", ""], ["Jiang", "Lingyun", ""], ["Wang", "Linyuan", ""], ["Yan", "Bin", ""]]}, {"id": "2002.00185", "submitter": "Hui-Chu Xiao", "authors": "Hui-Chu Xiao, Wan-Lei Zhao, Jie Lin, and Chong-Wah Ngo", "title": "Deeply Activated Salient Region for Instance Search", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of instance search depends heavily on the ability to locate\nand describe a wide variety of object instances in a video/image collection.\nDue to the lack of proper mechanism in locating instances and deriving feature\nrepresentation, instance search is generally only effective for retrieving\ninstances of known object categories. In this paper, a simple but effective\ninstance-level feature representation is presented. Different from other\napproaches, the issues in class-agnostic instance localization and distinctive\nfeature representation are considered. The former is achieved by detecting\nsalient instance regions from an image by a layer-wise back-propagation\nprocess. The back-propagation starts from the last convolution layer of a\npre-trained CNN that is originally used for classification. The\nback-propagation proceeds layer-by-layer until it reaches the input layer. This\nallows the salient instance regions in the input image from both known and\nunknown categories to be activated. Each activated salient region covers the\nfull or more usually a major range of an instance. The distinctive feature\nrepresentation is produced by average-pooling on the feature map of certain\nlayer with the detected instance region. Experiments show that such kind of\nfeature representation demonstrates considerably better performance over most\nof the existing approaches. In addition, we show that the proposed feature\ndescriptor is also suitable for content-based image search.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 10:25:59 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 02:29:46 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 02:57:02 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Xiao", "Hui-Chu", ""], ["Zhao", "Wan-Lei", ""], ["Lin", "Jie", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "2002.00199", "submitter": "Zhenghang Wu", "authors": "Zhenghang Wu, Yidong Cui", "title": "Large Hole Image Inpainting With Compress-Decompression Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting technology can patch images with missing pixels. Existing\nmethods propose convolutional neural networks to repair corrupted images. The\nnetworks focus on the valid pixels around the missing pixels, use the\nencoder-decoder structure to extract valuable information, and use the\ninformation to fix the vacancy. However, if the missing part is too large to\nprovide useful information, the result will exist blur, color mixing, and\nobject confusion. In order to patch the large hole image, we study the existing\napproaches and propose a new network, the compression-decompression network.\nThe compression network takes responsibility for inpainting and generating a\ndown-sample image. The decompression network takes responsibility for extending\nthe down-sample image into the original resolution. We construct the\ncompression network with the residual network and propose a similar texture\nselection algorithm to extend the image that is better than using the\nsuper-resolution network. We evaluate our model over Places2 and CelebA data\nset and use the similarity ratio as the metric. The result shows that our model\nhas better performance when the inpainting task has many conflicts.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 12:39:13 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Wu", "Zhenghang", ""], ["Cui", "Yidong", ""]]}, {"id": "2002.00216", "submitter": "Di Feng", "authors": "Di Feng, Yifan Cao, Lars Rosenbaum, Fabian Timm, Klaus Dietmayer", "title": "Leveraging Uncertainties for Deep Multi-modal Object Detection in\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a probabilistic deep neural network that combines LiDAR\npoint clouds and RGB camera images for robust, accurate 3D object detection. We\nexplicitly model uncertainties in the classification and regression tasks, and\nleverage uncertainties to train the fusion network via a sampling mechanism. We\nvalidate our method on three datasets with challenging real-world driving\nscenarios. Experimental results show that the predicted uncertainties reflect\ncomplex environmental uncertainty like difficulties of a human expert to label\nobjects. The results also show that our method consistently improves the\nAverage Precision by up to 7% compared to the baseline method. When sensors are\ntemporally misaligned, the sampling method improves the Average Precision by up\nto 20%, showing its high robustness against noisy sensor inputs.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 14:24:51 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Feng", "Di", ""], ["Cao", "Yifan", ""], ["Rosenbaum", "Lars", ""], ["Timm", "Fabian", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2002.00226", "submitter": "Xinsheng Wang", "authors": "Xinsheng Wang, Shanmin Pang, Jihua Zhu", "title": "Domain segmentation and adjustment for generalized zero-shot learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the generalized zero-shot learning, synthesizing unseen data with\ngenerative models has been the most popular method to address the imbalance of\ntraining data between seen and unseen classes. However, this method requires\nthat the unseen semantic information is available during the training stage,\nand training generative models is not trivial. Given that the generator of\nthese models can only be trained with seen classes, we argue that synthesizing\nunseen data may not be an ideal approach for addressing the domain shift caused\nby the imbalance of the training data. In this paper, we propose to realize the\ngeneralized zero-shot recognition in different domains. Thus, unseen (seen)\nclasses can avoid the effect of the seen (unseen) classes. In practice, we\npropose a threshold and probabilistic distribution joint method to segment the\ntesting instances into seen, unseen and uncertain domains. Moreover, the\nuncertain domain is further adjusted to alleviate the domain shift. Extensive\nexperiments on five benchmark datasets show that the proposed method exhibits\ncompetitive performance compared with that based on generative models.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 15:00:56 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wang", "Xinsheng", ""], ["Pang", "Shanmin", ""], ["Zhu", "Jihua", ""]]}, {"id": "2002.00250", "submitter": "Toamsz Kryjak", "authors": "Piotr Janus, Tomasz Kryjak, Marek Gorgon", "title": "Foreground object segmentation in RGB-D data implemented on GPU", "comments": "12 pages, 4 figures, submitted to KKA 2020 conference", "journal-ref": null, "doi": "10.1007/978-3-030-50936-1_68", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a GPU implementation of two foreground object\nsegmentation algorithms: Gaussian Mixture Model (GMM) and Pixel Based Adaptive\nSegmenter (PBAS) modified for RGB-D data support. The simultaneous use of\ncolour (RGB) and depth (D) data allows to improve segmentation accuracy,\nespecially in case of colour camouflage, illumination changes and occurrence of\nshadows. Three GPUs were used to accelerate calculations: embedded NVIDIA\nJetson TX2 (Maxwell architecture), mobile NVIDIA GeForce GTX 1050m (Pascal\narchitecture) and efficient NVIDIA RTX 2070 (Turing architecture). Segmentation\naccuracy comparable to previously published works was obtained. Moreover, the\nuse of a GPU platform allowed to get real-time image processing. In addition,\nthe system has been adapted to work with two RGB-D sensors: RealSense D415 and\nD435 from Intel.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 17:53:39 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Janus", "Piotr", ""], ["Kryjak", "Tomasz", ""], ["Gorgon", "Marek", ""]]}, {"id": "2002.00251", "submitter": "Alexander Schindler", "authors": "Alexander Schindler", "title": "Multi-Modal Music Information Retrieval: Augmenting Audio-Analysis with\n  Visual Computing for Improved Music Video Analysis", "comments": "Dissertation at TU Wien", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis combines audio-analysis with computer vision to approach Music\nInformation Retrieval (MIR) tasks from a multi-modal perspective. This thesis\nfocuses on the information provided by the visual layer of music videos and how\nit can be harnessed to augment and improve tasks of the MIR research domain.\nThe main hypothesis of this work is based on the observation that certain\nexpressive categories such as genre or theme can be recognized on the basis of\nthe visual content alone, without the sound being heard. This leads to the\nhypothesis that there exists a visual language that is used to express mood or\ngenre. In a further consequence it can be concluded that this visual\ninformation is music related and thus should be beneficial for the\ncorresponding MIR tasks such as music genre classification or mood recognition.\nA series of comprehensive experiments and evaluations are conducted which are\nfocused on the extraction of visual information and its application in\ndifferent MIR tasks. A custom dataset is created, suitable to develop and test\nvisual features which are able to represent music related information.\nEvaluations range from low-level visual features to high-level concepts\nretrieved by means of Deep Convolutional Neural Networks. Additionally, new\nvisual features are introduced capturing rhythmic visual patterns. In all of\nthese experiments the audio-based results serve as benchmark for the visual and\naudio-visual approaches. The experiments are conducted for three MIR tasks\nArtist Identification, Music Genre Classification and Cross-Genre\nClassification. Experiments show that an audio-visual approach harnessing\nhigh-level semantic information gained from visual concept detection,\noutperforms audio-only genre-classification accuracy by 16.43%.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 17:57:14 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Schindler", "Alexander", ""]]}, {"id": "2002.00264", "submitter": "Mahesh Kumar Krishna Reddy", "authors": "Mahesh Kumar Krishna Reddy, Mohammad Hossain, Mrigank Rochan and Yang\n  Wang", "title": "Few-Shot Scene Adaptive Crowd Counting Using Meta-Learning", "comments": "Accepted to WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of few-shot scene adaptive crowd counting. Given a\ntarget camera scene, our goal is to adapt a model to this specific scene with\nonly a few labeled images of that scene. The solution to this problem has\npotential applications in numerous real-world scenarios, where we ideally like\nto deploy a crowd counting model specially adapted to a target camera. We\naccomplish this challenge by taking inspiration from the recently introduced\nlearning-to-learn paradigm in the context of few-shot regime. In training, our\nmethod learns the model parameters in a way that facilitates the fast\nadaptation to the target scene. At test time, given a target scene with a small\nnumber of labeled data, our method quickly adapts to that scene with a few\ngradient updates to the learned parameters. Our extensive experimental results\nshow that the proposed approach outperforms other alternatives in few-shot\nscene adaptive crowd counting. Code is available at\nhttps://github.com/maheshkkumar/fscc.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 19:41:26 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 18:52:40 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 05:54:24 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Reddy", "Mahesh Kumar Krishna", ""], ["Hossain", "Mohammad", ""], ["Rochan", "Mrigank", ""], ["Wang", "Yang", ""]]}, {"id": "2002.00297", "submitter": "James Noraky", "authors": "James Noraky, Vivienne Sze", "title": "Depth Map Estimation of Dynamic Scenes Using Prior Depth Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth information is useful for many applications. Active depth sensors are\nappealing because they obtain dense and accurate depth maps. However, due to\nissues that range from power constraints to multi-sensor interference, these\nsensors cannot always be continuously used. To overcome this limitation, we\npropose an algorithm that estimates depth maps using concurrently collected\nimages and a previously measured depth map for dynamic scenes, where both the\ncamera and objects in the scene may be independently moving. To estimate depth\nin these scenarios, our algorithm models the dynamic scene motion using\nindependent and rigid motions. It then uses the previous depth map to\nefficiently estimate these rigid motions and obtain a new depth map. Our goal\nis to balance the acquisition of depth between the active depth sensor and\ncomputation, without incurring a large computational cost. Thus, we leverage\nthe prior depth information to avoid computationally expensive operations like\ndense optical flow estimation or segmentation used in similar approaches. Our\napproach can obtain dense depth maps at up to real-time (30 FPS) on a standard\nlaptop computer, which is orders of magnitude faster than similar approaches.\nWhen evaluated using RGB-D datasets of various dynamic scenes, our approach\nestimates depth maps with a mean relative error of 2.5% while reducing the\nactive depth sensor usage by over 90%.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 01:04:27 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Noraky", "James", ""], ["Sze", "Vivienne", ""]]}, {"id": "2002.00336", "submitter": "Arun CS Kumar", "authors": "Arun CS Kumar, Disha Ahuja, Ashwath Aithal", "title": "3D Object Detection on Point Clouds using Local Ground-aware and\n  Adaptive Representation of scenes' surface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A novel, adaptive ground-aware, and cost-effective 3D Object Detection\npipeline is proposed. The ground surface representation introduced in this\npaper, in comparison to its uni-planar counterparts (methods that model the\nsurface of a whole 3D scene using single plane), is far more accurate while\nbeing ~10x faster. The novelty of the ground representation lies both in the\nway in which the ground surface of the scene is represented in Lidar perception\nproblems, as well as in the (cost-efficient) way in which it is computed.\nFurthermore, the proposed object detection pipeline builds on the traditional\ntwo-stage object detection models by incorporating the ability to dynamically\nreason the surface of the scene, ultimately achieving a new state-of-the-art 3D\nobject detection performance among the two-stage Lidar Object Detection\npipelines.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 05:42:23 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 22:13:24 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kumar", "Arun CS", ""], ["Ahuja", "Disha", ""], ["Aithal", "Ashwath", ""]]}, {"id": "2002.00349", "submitter": "Matthias Fey", "authors": "Marian Kleineberg, Matthias Fey, Frank Weichert", "title": "Adversarial Generation of Continuous Implicit Shape Representations", "comments": "Published in Eurographics 2020 - Short Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a generative adversarial architecture for generating\nthree-dimensional shapes based on signed distance representations. While the\ndeep generation of shapes has been mostly tackled by voxel and surface point\ncloud approaches, our generator learns to approximate the signed distance for\nany point in space given prior latent information. Although structurally\nsimilar to generative point cloud approaches, this formulation can be evaluated\nwith arbitrary point density during inference, leading to fine-grained details\nin generated outputs. Furthermore, we study the effects of using either\nprogressively growing voxel- or point-processing networks as discriminators,\nand propose a refinement scheme to strengthen the generator's capabilities in\nmodeling the zero iso-surface decision boundary of shapes. We train our\napproach on the ShapeNet benchmark dataset and validate, both quantitatively\nand qualitatively, its performance in generating realistic 3D shapes.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 08:20:42 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 07:45:33 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Kleineberg", "Marian", ""], ["Fey", "Matthias", ""], ["Weichert", "Frank", ""]]}, {"id": "2002.00367", "submitter": "Joonatan Manttari", "authors": "Joonatan M\\\"antt\\\"ari, Sofia Broom\\'e, John Folkesson, Hedvig\n  Kjellstr\\\"om", "title": "Interpreting video features: a comparison of 3D convolutional networks\n  and convolutional LSTM networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of techniques for interpretability have been presented for deep\nlearning in computer vision, typically with the goal of understanding what the\nnetworks have based their classification on. However, interpretability for deep\nvideo architectures is still in its infancy and we do not yet have a clear\nconcept of how to decode spatiotemporal features. In this paper, we present a\nstudy comparing how 3D convolutional networks and convolutional LSTM networks\nlearn features across temporally dependent frames. This is the first comparison\nof two video models that both convolve to learn spatial features but have\nprincipally different methods of modeling time. Additionally, we extend the\nconcept of meaningful perturbation introduced by \\cite{MeaningFulPert} to the\ntemporal dimension, to identify the temporal part of a sequence most meaningful\nto the network for a classification decision. Our findings indicate that the 3D\nconvolutional model concentrates on shorter events in the input sequence, and\nplaces its spatial focus on fewer, contiguous areas.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 11:27:07 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 14:32:22 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["M\u00e4ntt\u00e4ri", "Joonatan", ""], ["Broom\u00e9", "Sofia", ""], ["Folkesson", "John", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "2002.00391", "submitter": "Biao Yang", "authors": "Biao Yang, Guocheng Yan, Pin Wang, Chingyao Chan, Xiang Song, and Yang\n  Chen", "title": "A Novel Graph based Trajectory Predictor with Pseudo Oracle", "comments": "17 apges, 8 figures", "journal-ref": "already published by TNNLS 2021", "doi": "10.1109/TNNLS.2021.3084143", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian trajectory prediction in dynamic scenes remains a challenging and\ncritical problem in numerous applications, such as self-driving cars and\nsocially aware robots. Challenges concentrate on capturing pedestrians' motion\npatterns and social interactions, as well as handling the future uncertainties.\nRecent studies focus on modeling pedestrians' motion patterns with recurrent\nneural networks, capturing social interactions with pooling-based or\ngraph-based methods, and handling future uncertainties by using random Gaussian\nnoise as the latent variable. However, they do not integrate specific obstacle\navoidance experience (OAE) that may improve prediction performance. For\nexample, pedestrians' future trajectories are always influenced by others in\nfront. Here we propose GTPPO (Graph-based Trajectory Predictor with Pseudo\nOracle), an encoder-decoder-based method conditioned on pedestrians' future\nbehaviors. Pedestrians' motion patterns are encoded with a long short-term\nmemory unit, which introduces the temporal attention to highlight specific time\nsteps. Their interactions are captured by a graph-based attention mechanism,\nwhich draws OAE into the data-driven learning process of graph attention.\nFuture uncertainties are handled by generating multi-modal outputs with an\ninformative latent variable. Such a variable is generated by a novel pseudo\noracle predictor, which minimizes the knowledge gap between historical and\nground-truth trajectories. Finally, the GTPPO is evaluated on ETH, UCY and\nStanford Drone datasets, and the results demonstrate state-of-the-art\nperformance. Besides, the qualitative evaluations show successful cases of\nhandling sudden motion changes in the future. Such findings indicate that GTPPO\ncan peek into the future.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 13:40:47 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 07:24:45 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yang", "Biao", ""], ["Yan", "Guocheng", ""], ["Wang", "Pin", ""], ["Chan", "Chingyao", ""], ["Song", "Xiang", ""], ["Chen", "Yang", ""]]}, {"id": "2002.00397", "submitter": "Fabio Poiesi", "authors": "Davide Boscaini and Fabio Poiesi", "title": "3D Shape Segmentation with Geometric Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30642-7_41", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic segmentation of 3D shapes with a high-density of vertices could\nbe impractical due to large memory requirements. To make this problem\ncomputationally tractable, we propose a neural-network based approach that\nproduces 3D augmented views of the 3D shape to solve the whole segmentation as\nsub-segmentation problems. 3D augmented views are obtained by projecting\nvertices and normals of a 3D shape onto 2D regular grids taken from different\nviewpoints around the shape. These 3D views are then processed by a\nConvolutional Neural Network to produce a probability distribution function\n(pdf) over the set of the semantic classes for each vertex. These pdfs are then\nre-projected on the original 3D shape and postprocessed using contextual\ninformation through Conditional Random Fields. We validate our approach using\n3D shapes of publicly available datasets and of real objects that are\nreconstructed using photogrammetry techniques. We compare our approach against\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 14:11:16 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Boscaini", "Davide", ""], ["Poiesi", "Fabio", ""]]}, {"id": "2002.00440", "submitter": "Guang Yang A", "authors": "Guang Yang, Jun Chen, Zhifan Gao, Shuo Li, Hao Ni, Elsa Angelini, Tom\n  Wong, Raad Mohiaddin, Eva Nyktari, Ricardo Wage, Lei Xu, Yanping Zhang,\n  Xiuquan Du, Heye Zhang, David Firmin, Jennifer Keegan", "title": "Simultaneous Left Atrium Anatomy and Scar Segmentations via Deep\n  Learning in Multiview Information with Attention", "comments": "34 pages, 10 figures, 7 tables, accepted by Future Generation\n  Computer Systems journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Three-dimensional late gadolinium enhanced (LGE) cardiac MR (CMR) of left\natrial scar in patients with atrial fibrillation (AF) has recently emerged as a\npromising technique to stratify patients, to guide ablation therapy and to\npredict treatment success. This requires a segmentation of the high intensity\nscar tissue and also a segmentation of the left atrium (LA) anatomy, the latter\nusually being derived from a separate bright-blood acquisition. Performing both\nsegmentations automatically from a single 3D LGE CMR acquisition would\neliminate the need for an additional acquisition and avoid subsequent\nregistration issues. In this paper, we propose a joint segmentation method\nbased on multiview two-task (MVTT) recursive attention model working directly\non 3D LGE CMR images to segment the LA (and proximal pulmonary veins) and to\ndelineate the scar on the same dataset. Using our MVTT recursive attention\nmodel, both the LA anatomy and scar can be segmented accurately (mean Dice\nscore of 93% for the LA anatomy and 87% for the scar segmentations) and\nefficiently (~0.27 seconds to simultaneously segment the LA anatomy and scars\ndirectly from the 3D LGE CMR dataset with 60-68 2D slices). Compared to\nconventional unsupervised learning and other state-of-the-art deep learning\nbased methods, the proposed MVTT model achieved excellent results, leading to\nan automatic generation of a patient-specific anatomical model combined with\nscar segmentation for patients in AF.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 18:03:44 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Yang", "Guang", ""], ["Chen", "Jun", ""], ["Gao", "Zhifan", ""], ["Li", "Shuo", ""], ["Ni", "Hao", ""], ["Angelini", "Elsa", ""], ["Wong", "Tom", ""], ["Mohiaddin", "Raad", ""], ["Nyktari", "Eva", ""], ["Wage", "Ricardo", ""], ["Xu", "Lei", ""], ["Zhang", "Yanping", ""], ["Du", "Xiuquan", ""], ["Zhang", "Heye", ""], ["Firmin", "David", ""], ["Keegan", "Jennifer", ""]]}, {"id": "2002.00460", "submitter": "Xingxing Zou", "authors": "Xingxing Zou, Zhizhong Li, Ke Bai, Dahua Lin, Waikeung Wong", "title": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build an outfit evaluation system which provides feedbacks\nconsisting of a judgment with a convincing explanation. The system is trained\nin a supervised manner which faithfully follows the domain knowledge in\nfashion. We create the EVALUATION3 dataset which is annotated with judgment,\nthe decisive reason for the judgment, and all corresponding attributes (e.g.\nprint, silhouette, and material \\etc.). In the training process, features of\nall attributes in an outfit are first extracted and then concatenated as the\ninput for the intra-factor compatibility net. Then, the inter-factor\ncompatibility net is used to compute the loss for judgment. We penalize the\ngradient of judgment loss of so that our Grad-CAM-like reason is regularized to\nbe consistent with the labeled reason. In inference, according to the obtained\ninformation of judgment, reason, and attributes, a user-friendly explanation\nsentence is generated by the pre-defined templates. The experimental results\nshow that the obtained network combines the advantages of high precision and\ngood interpretation.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 18:59:55 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Zou", "Xingxing", ""], ["Li", "Zhizhong", ""], ["Bai", "Ke", ""], ["Lin", "Dahua", ""], ["Wong", "Waikeung", ""]]}, {"id": "2002.00461", "submitter": "Sarwan Ali", "authors": "Asad Ullah, Sarwan Ali, Imdadullah Khan, Muhammad Asad Khan, Safiullah\n  Faizullah", "title": "Effect of Analysis Window and Feature Selection on Classification of\n  Hand Movements Using EMG Signal", "comments": "Accepted to Intelligent Systems Conference (IntelliSys) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electromyography (EMG) signals have been successfully employed for driving\nprosthetic limbs of a single or double degree of freedom. This principle works\nby using the amplitude of the EMG signals to decide between one or two simpler\nmovements. This method underperforms as compare to the contemporary advances\ndone at the mechanical, electronics, and robotics end, and it lacks intuition.\nRecently, research on myoelectric control based on pattern recognition (PR)\nshows promising results with the aid of machine learning classifiers. Using the\napproach termed as, EMG-PR, EMG signals are divided into analysis windows, and\nfeatures are extracted for each window. These features are then fed to the\nmachine learning classifiers as input. By offering multiple class movements and\nintuitive control, this method has the potential to power an amputated subject\nto perform everyday life movements. In this paper, we investigate the effect of\nthe analysis window and feature selection on classification accuracy of\ndifferent hand and wrist movements using time-domain features. We show that\neffective data preprocessing and optimum feature selection helps to improve the\nclassification accuracy of hand movements. We use publicly available hand and\nwrist gesture dataset of $40$ intact subjects for experimentation. Results\ncomputed using different classification algorithms show that the proposed\npreprocessing and features selection outperforms the baseline and achieve up to\n$98\\%$ classification accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 19:03:23 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 07:58:24 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 23:07:48 GMT"}, {"version": "v4", "created": "Tue, 11 Aug 2020 18:21:46 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Ullah", "Asad", ""], ["Ali", "Sarwan", ""], ["Khan", "Imdadullah", ""], ["Khan", "Muhammad Asad", ""], ["Faizullah", "Safiullah", ""]]}, {"id": "2002.00479", "submitter": "Alptekin Orbay", "authors": "Alptekin Orbay and Lale Akarun", "title": "Neural Sign Language Translation by Learning Tokenization", "comments": "8 pages, 2 figures, FG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Language Translation has attained considerable success recently, raising\nhopes for improved communication with the Deaf. A pre-processing step called\ntokenization improves the success of translations. Tokens can be learned from\nsign videos if supervised data is available. However, data annotation at the\ngloss level is costly, and annotated data is scarce. The paper utilizes\nAdversarial, Multitask, Transfer Learning to search for semi-supervised\ntokenization approaches without burden of additional labeling. It provides\nextensive experiments to compare all the methods in different settings to\nconduct a deeper analysis. In the case of no additional target annotation\nbesides sentences, the proposed methodology attains 13.25 BLUE-4 and 36.28\nROUGE scores which improves the current state-of-the-art by 4 points in BLUE-4\nand 5 points in ROUGE.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 19:59:30 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 15:28:18 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Orbay", "Alptekin", ""], ["Akarun", "Lale", ""]]}, {"id": "2002.00522", "submitter": "Chengwei Chen", "authors": "Chengwei Chen and Wang Yuan and Yuan Xie and Yanyun Qu and Yiqing Tao\n  and Haichuan Song and Lizhuang Ma", "title": "Novelty Detection via Non-Adversarial Generative Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class novelty detection is the process of determining if a query example\ndiffers from the training examples (the target class). Most of previous\nstrategies attempt to learn the real characteristics of target sample by using\ngenerative adversarial networks (GANs) methods. However, the training process\nof GANs remains challenging, suffering from instability issues such as mode\ncollapse and vanishing gradients. In this paper, by adopting non-adversarial\ngenerative networks, a novel decoder-encoder framework is proposed for novelty\ndetection task, insteading of classical encoder-decoder style. Under the\nnon-adversarial framework, both latent space and image reconstruction space are\njointly optimized, leading to a more stable training process with super fast\nconvergence and lower training losses. During inference, inspired by cycleGAN,\nwe design a new testing scheme to conduct image reconstruction, which is the\nreverse way of training sequence. Experiments show that our model has the clear\nsuperiority over cutting-edge novelty detectors and achieves the\nstate-of-the-art results on the datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 01:05:59 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Chen", "Chengwei", ""], ["Yuan", "Wang", ""], ["Xie", "Yuan", ""], ["Qu", "Yanyun", ""], ["Tao", "Yiqing", ""], ["Song", "Haichuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2002.00523", "submitter": "Luis Guerra", "authors": "Luis Guerra, Bohan Zhuang, Ian Reid, Tom Drummond", "title": "Automatic Pruning for Quantized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network quantization and pruning are two techniques commonly used to\nreduce the computational complexity and memory footprint of these models for\ndeployment. However, most existing pruning strategies operate on full-precision\nand cannot be directly applied to discrete parameter distributions after\nquantization. In contrast, we study a combination of these two techniques to\nachieve further network compression. In particular, we propose an effective\npruning strategy for selecting redundant low-precision filters. Furthermore, we\nleverage Bayesian optimization to efficiently determine the pruning ratio for\neach layer. We conduct extensive experiments on CIFAR-10 and ImageNet with\nvarious architectures and precisions. In particular, for ResNet-18 on ImageNet,\nwe prune 26.12% of the model size with Binarized Neural Network quantization,\nachieving a top-1 classification accuracy of 47.32% in a model of 2.47 MB and\n59.30% with a 2-bit DoReFa-Net in 4.36 MB.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 01:10:13 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Guerra", "Luis", ""], ["Zhuang", "Bohan", ""], ["Reid", "Ian", ""], ["Drummond", "Tom", ""]]}, {"id": "2002.00537", "submitter": "Jing Zhang", "authors": "Jing Zhang and Zhe Chen and Dacheng Tao", "title": "Towards High Performance Human Keypoint Detection", "comments": "Accepted by IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human keypoint detection from a single image is very challenging due to\nocclusion, blur, illumination and scale variance. In this paper, we address\nthis problem from three aspects by devising an efficient network structure,\nproposing three effective training strategies, and exploiting four useful\npostprocessing techniques. First, we find that context information plays an\nimportant role in reasoning human body configuration and invisible keypoints.\nInspired by this, we propose a cascaded context mixer (CCM), which efficiently\nintegrates spatial and channel context information and progressively refines\nthem. Then, to maximize CCM's representation capability, we develop a\nhard-negative person detection mining strategy and a joint-training strategy by\nexploiting abundant unlabeled data. It enables CCM to learn discriminative\nfeatures from massive diverse poses. Third, we present several sub-pixel\nrefinement techniques for postprocessing keypoint predictions to improve\ndetection accuracy. Extensive experiments on the MS COCO keypoint detection\nbenchmark demonstrate the superiority of the proposed method over\nrepresentative state-of-the-art (SOTA) methods. Our single model achieves\ncomparable performance with the winner of the 2018 COCO Keypoint Detection\nChallenge. The final ensemble model sets a new SOTA on this benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 02:24:51 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 02:23:25 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "Jing", ""], ["Chen", "Zhe", ""], ["Tao", "Dacheng", ""]]}, {"id": "2002.00552", "submitter": "Di Huang", "authors": "Di Huang, Xishan Zhang, Rui Zhang, Tian Zhi, Deyuan He, Jiaming Guo,\n  Chang Liu, Qi Guo, Zidong Du, Shaoli Liu, Tianshi Chen, Yunji Chen", "title": "DWM: A Decomposable Winograd Method for Convolution Acceleration", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Winograd's minimal filtering algorithm has been widely used in Convolutional\nNeural Networks (CNNs) to reduce the number of multiplications for faster\nprocessing. However, it is only effective on convolutions with kernel size as\n3x3 and stride as 1, because it suffers from significantly increased FLOPs and\nnumerical accuracy problem for kernel size larger than 3x3 and fails on\nconvolution with stride larger than 1. In this paper, we propose a novel\nDecomposable Winograd Method (DWM), which breaks through the limitation of\noriginal Winograd's minimal filtering algorithm to a wide and general\nconvolutions. DWM decomposes kernels with large size or large stride to several\nsmall kernels with stride as 1 for further applying Winograd method, so that\nDWM can reduce the number of multiplications while keeping the numerical\naccuracy. It enables the fast exploring of larger kernel size and larger stride\nvalue in CNNs for high performance and accuracy and even the potential for new\nCNNs. Comparing against the original Winograd, the proposed DWM is able to\nsupport all kinds of convolutions with a speedup of ~2, without affecting the\nnumerical accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 03:42:56 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Huang", "Di", ""], ["Zhang", "Xishan", ""], ["Zhang", "Rui", ""], ["Zhi", "Tian", ""], ["He", "Deyuan", ""], ["Guo", "Jiaming", ""], ["Liu", "Chang", ""], ["Guo", "Qi", ""], ["Du", "Zidong", ""], ["Liu", "Shaoli", ""], ["Chen", "Tianshi", ""], ["Chen", "Yunji", ""]]}, {"id": "2002.00555", "submitter": "Kai Han", "authors": "Chuanjian Liu, Kai Han, Yunhe Wang, Hanting Chen, Qi Tian, Chunjing Xu", "title": "Widening and Squeezing: Towards Accurate and Efficient QNNs", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization neural networks (QNNs) are very attractive to the industry\nbecause their extremely cheap calculation and storage overhead, but their\nperformance is still worse than that of networks with full-precision\nparameters. Most of existing methods aim to enhance performance of QNNs\nespecially binary neural networks by exploiting more effective training\ntechniques. However, we find the representation capability of quantization\nfeatures is far weaker than full-precision features by experiments. We address\nthis problem by projecting features in original full-precision networks to\nhigh-dimensional quantization features. Simultaneously, redundant quantization\nfeatures will be eliminated in order to avoid unrestricted growth of dimensions\nfor some datasets. Then, a compact quantization neural network but with\nsufficient representation ability will be established. Experimental results on\nbenchmark datasets demonstrate that the proposed method is able to establish\nQNNs with much less parameters and calculations but almost the same performance\nas that of full-precision baseline models, e.g. $29.9\\%$ top-1 error of binary\nResNet-18 on the ImageNet ILSVRC 2012 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 04:11:13 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 09:44:24 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Liu", "Chuanjian", ""], ["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Chen", "Hanting", ""], ["Tian", "Qi", ""], ["Xu", "Chunjing", ""]]}, {"id": "2002.00569", "submitter": "Chunhua Shen", "authors": "Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu,\n  Changming Sun, Dou Renyin", "title": "DiverseDepth: Affine-invariant Depth Prediction Using Diverse Data", "comments": "Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a method for depth estimation with monocular images, which can\npredict high-quality depth on diverse scenes up to an affine transformation,\nthus preserving accurate shapes of a scene. Previous methods that predict\nmetric depth often work well only for a specific scene. In contrast, learning\nrelative depth (information of being closer or further) can enjoy better\ngeneralization, with the price of failing to recover the accurate geometric\nshape of the scene. In this work, we propose a dataset and methods to tackle\nthis dilemma, aiming to predict accurate depth up to an affine transformation\nwith good generalization to diverse scenes. First we construct a large-scale\nand diverse dataset, termed Diverse Scene Depth dataset (DiverseDepth), which\nhas a broad range of scenes and foreground contents. Compared with previous\nlearning objectives, i.e., learning metric depth or relative depth, we propose\nto learn the affine-invariant depth using our diverse dataset to ensure both\ngeneralization and high-quality geometric shapes of scenes. Furthermore, in\norder to train the model on the complex dataset effectively, we propose a\nmulti-curriculum learning method. Experiments show that our method outperforms\nprevious methods on 8 datasets by a large margin with the zero-shot test\nsetting, demonstrating the excellent generalization capacity of the learned\nmodel to diverse scenes. The reconstructed point clouds with the predicted\ndepth show that our method can recover high-quality 3D shapes. Code and dataset\nare available at: https://tinyurl.com/DiverseDepth\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 05:38:33 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 01:16:42 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 08:26:57 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yin", "Wei", ""], ["Wang", "Xinlong", ""], ["Shen", "Chunhua", ""], ["Liu", "Yifan", ""], ["Tian", "Zhi", ""], ["Xu", "Songcen", ""], ["Sun", "Changming", ""], ["Renyin", "Dou", ""]]}, {"id": "2002.00573", "submitter": "Wei-Lun Chao", "authors": "Wei-Lun Chao, Han-Jia Ye, De-Chuan Zhan, Mark Campbell, Kilian Q.\n  Weinberger", "title": "Revisiting Meta-Learning as Supervised Learning", "comments": "An extended version of the paper titled \"A Meta Understanding of\n  Meta-Learning\" presented in ICML 2019 Workshop on Adaptive and Multitask\n  Learning: Algorithms & Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed an abundance of new publications and approaches\non meta-learning. This community-wide enthusiasm has sparked great insights but\nhas also created a plethora of seemingly different frameworks, which can be\nhard to compare and evaluate. In this paper, we aim to provide a principled,\nunifying framework by revisiting and strengthening the connection between\nmeta-learning and traditional supervised learning. By treating pairs of\ntask-specific data sets and target models as (feature, label) samples, we can\nreduce many meta-learning algorithms to instances of supervised learning. This\nview not only unifies meta-learning into an intuitive and practical framework\nbut also allows us to transfer insights from supervised learning directly to\nimprove meta-learning. For example, we obtain a better understanding of\ngeneralization properties, and we can readily transfer well-understood\ntechniques, such as model ensemble, pre-training, joint training, data\naugmentation, and even nearest neighbor based methods. We provide an intuitive\nanalogy of these methods in the context of meta-learning and show that they\ngive rise to significant improvements in model performance on few-shot\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 06:13:01 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Chao", "Wei-Lun", ""], ["Ye", "Han-Jia", ""], ["Zhan", "De-Chuan", ""], ["Campbell", "Mark", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "2002.00575", "submitter": "Siqi Yang", "authors": "Siqi Yang, Lin Wu, Arnold Wiliem and Brian C. Lovell", "title": "Unsupervised Domain Adaptive Object Detection using Forward-Backward\n  Cyclic Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to perform the unsupervised domain adaptation for\nobject detection through forward-backward cyclic (FBC) training. Recent\nadversarial training based domain adaptation methods have shown their\neffectiveness on minimizing domain discrepancy via marginal feature\ndistributions alignment. However, aligning the marginal feature distributions\ndoes not guarantee the alignment of class conditional distributions. This\nlimitation is more evident when adapting object detectors as the domain\ndiscrepancy is larger compared to the image classification task, e.g. various\nnumber of objects exist in one image and the majority of content in an image is\nthe background. This motivates us to learn domain invariance for category level\nsemantics via gradient alignment. Intuitively, if the gradients of two domains\npoint in similar directions, then the learning of one domain can improve that\nof another domain. To achieve gradient alignment, we propose Forward-Backward\nCyclic Adaptation, which iteratively computes adaptation from source to target\nvia backward hopping and from target to source via forward passing. In\naddition, we align low-level features for adapting holistic color/texture via\nadversarial training. However, the detector performs well on both domains is\nnot ideal for target domain. As such, in each cycle, domain diversity is\nenforced by maximum entropy regularization on the source domain to penalize\nconfident source-specific learning and minimum entropy regularization on target\ndomain to intrigue target-specific learning. Theoretical analysis of the\ntraining process is provided, and extensive experiments on challenging\ncross-domain object detection datasets have shown the superiority of our\napproach over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 06:24:58 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Yang", "Siqi", ""], ["Wu", "Lin", ""], ["Wiliem", "Arnold", ""], ["Lovell", "Brian C.", ""]]}, {"id": "2002.00580", "submitter": "Markus Uwe M\\\"uller", "authors": "M. U. M\\\"uller, N. Ekhtiari, R. M. Almeida, C. Rieke", "title": "Super-resolution of multispectral satellite images using convolutional\n  neural networks", "comments": "To be published in the ISPRS Annals of the Photogrammetry, Remote\n  Sensing and Spatial Information Sciences:\n  https://www.isprs.org/publications/annals.aspx, proceedings of the XXIV ISPRS\n  Congress, 14-20 June 2020, Nice, France", "journal-ref": "ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., V-1-2020,\n  33-40", "doi": "10.5194/isprs-annals-V-1-2020-33-2020", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution aims at increasing image resolution by algorithmic means and\nhas progressed over the recent years due to advances in the fields of computer\nvision and deep learning. Convolutional Neural Networks based on a variety of\narchitectures have been applied to the problem, e.g. autoencoders and residual\nnetworks. While most research focuses on the processing of photographs\nconsisting only of RGB color channels, little work can be found concentrating\non multi-band, analytic satellite imagery. Satellite images often include a\npanchromatic band, which has higher spatial resolution but lower spectral\nresolution than the other bands. In the field of remote sensing, there is a\nlong tradition of applying pan-sharpening to satellite images, i.e. bringing\nthe multispectral bands to the higher spatial resolution by merging them with\nthe panchromatic band. To our knowledge there are so far no approaches to\nsuper-resolution which take advantage of the panchromatic band. In this paper\nwe propose a method to train state-of-the-art CNNs using pairs of\nlower-resolution multispectral and high-resolution pan-sharpened image tiles in\norder to create super-resolved analytic images. The derived quality metrics\nshow that the method improves information content of the processed images. We\ncompare the results created by four CNN architectures, with RedNet30 performing\nbest.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 07:06:36 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 06:30:47 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["M\u00fcller", "M. U.", ""], ["Ekhtiari", "N.", ""], ["Almeida", "R. M.", ""], ["Rieke", "C.", ""]]}, {"id": "2002.00606", "submitter": "Jingwei Zhang", "authors": "Zihang Zhang, Jianping Gu", "title": "Facial Affect Recognition in the Wild Using Multi-Task Learning\n  Convolutional Network", "comments": "submitted to ABAW challenge in FG2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a neural network based method Multi-Task Affect\nNet(MTANet) submitted to the Affective Behavior Analysis in-the-Wild Challenge\nin FG2020. This method is a multi-task network and based on SE-ResNet modules.\nBy utilizing multi-task learning, this network can estimate and recognize three\nquantified affective models: valence and arousal, action units, and seven basic\nemotions simultaneously. MTANet achieve Concordance Correlation\nCoefficient(CCC) rates of 0.28 and 0.34 for valence and arousal, F1-score of\n0.427 and 0.32 for AUs detection and categorical emotion classification.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:02:26 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhang", "Zihang", ""], ["Gu", "Jianping", ""]]}, {"id": "2002.00614", "submitter": "B.S. Vivek", "authors": "B.S. Vivek, R. Venkatesh Babu", "title": "Regularizers for Single-step Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progress in the last decade has enabled machine learning models to\nachieve impressive performance across a wide range of tasks in Computer Vision.\nHowever, a plethora of works have demonstrated the susceptibility of these\nmodels to adversarial samples. Adversarial training procedure has been proposed\nto defend against such adversarial attacks. Adversarial training methods\naugment mini-batches with adversarial samples, and typically single-step\n(non-iterative) methods are used for generating these adversarial samples.\nHowever, models trained using single-step adversarial training converge to\ndegenerative minima where the model merely appears to be robust. The pseudo\nrobustness of these models is due to the gradient masking effect. Although\nmulti-step adversarial training helps to learn robust models, they are hard to\nscale due to the use of iterative methods for generating adversarial samples.\nTo address these issues, we propose three different types of regularizers that\nhelp to learn robust models using single-step adversarial training methods. The\nproposed regularizers mitigate the effect of gradient masking by harnessing on\nproperties that differentiate a robust model from that of a pseudo robust\nmodel. Performance of models trained using the proposed regularizers is on par\nwith models trained using computationally expensive multi-step adversarial\ntraining methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:21:04 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Vivek", "B. S.", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2002.00623", "submitter": "Iakov Karandashev M.", "authors": "Magomed Yu. Malsagov, Emil M. Khayrov, Maria M. Pushkareva, Iakov M.\n  Karandashev", "title": "Exponential discretization of weights of neural network connections in\n  pre-trained neural networks", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": "Optical Memory and Neural Networks (Inf. Opt.), V.28, N.4,\n  pp.262-270 (2019)", "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce random access memory (RAM) requirements and to increase speed of\nrecognition algorithms we consider a weight discretization problem for trained\nneural networks. We show that an exponential discretization is preferable to a\nlinear discretization since it allows one to achieve the same accuracy when the\nnumber of bits is 1 or 2 less. The quality of the neural network VGG-16 is\nalready satisfactory (top5 accuracy 69%) in the case of 3 bit exponential\ndiscretization. The ResNet50 neural network shows top5 accuracy 84% at 4 bits.\nOther neural networks perform fairly well at 5 bits (top5 accuracies of\nXception, Inception-v3, and MobileNet-v2 top5 were 87%, 90%, and 77%,\nrespectively). At less number of bits, the accuracy decreases rapidly.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:41:24 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Malsagov", "Magomed Yu.", ""], ["Khayrov", "Emil M.", ""], ["Pushkareva", "Maria M.", ""], ["Karandashev", "Iakov M.", ""]]}, {"id": "2002.00625", "submitter": "Ahmed Rasheed", "authors": "Ahmed Rasheed, Muhammad Shahzad Younis, Muhammad Bilal, and Maha\n  Rasheed", "title": "Classification of Chest Diseases using Wavelet Transforms and Transfer\n  Learning", "comments": "8 pages, 4 figures, Presented in International Conference On Medical\n  Imaging And Computer-Aided Diagnosis (MICAD 2020), proceeding will be\n  published with Springer in their \"Lecture Notes in Electrical Engineering\n  (LNEE)\" (ISSN: 1876-1100)", "journal-ref": null, "doi": "10.1007/978-981-15-5199-4_16", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray scan is a most often used modality by radiologists to diagnose\nmany chest related diseases in their initial stages. The proposed system aids\nthe radiologists in making decision about the diseases found in the scans more\nefficiently. Our system combines the techniques of image processing for feature\nenhancement and deep learning for classification among diseases. We have used\nthe ChestX-ray14 database in order to train our deep learning model on the 14\ndifferent labeled diseases found in it. The proposed research shows the\nsignificant improvement in the results by using wavelet transforms as\npre-processing technique.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:44:23 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Rasheed", "Ahmed", ""], ["Younis", "Muhammad Shahzad", ""], ["Bilal", "Muhammad", ""], ["Rasheed", "Maha", ""]]}, {"id": "2002.00647", "submitter": "Pegah Salehi", "authors": "Pegah Salehi, Abdolah Chalechale", "title": "Pix2Pix-based Stain-to-Stain Translation: A Solution for Robust Stain\n  Normalization in Histopathology Images Analysis", "comments": "7 pages, 6 figures, 4 table, The 11th Iranian and the first\n  International Conference on Machine Vision and Image Processing (MVIP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The diagnosis of cancer is mainly performed by visual analysis of the\npathologists, through examining the morphology of the tissue slices and the\nspatial arrangement of the cells. If the microscopic image of a specimen is not\nstained, it will look colorless and textured. Therefore, chemical staining is\nrequired to create contrast and help identify specific tissue components.\nDuring tissue preparation due to differences in chemicals, scanners, cutting\nthicknesses, and laboratory protocols, similar tissues are usually varied\nsignificantly in appearance. This diversity in staining, in addition to\nInterpretive disparity among pathologists more is one of the main challenges in\ndesigning robust and flexible systems for automated analysis. To address the\nstaining color variations, several methods for normalizing stain have been\nproposed. In our proposed method, a Stain-to-Stain Translation (STST) approach\nis used to stain normalization for Hematoxylin and Eosin (H&E) stained\nhistopathology images, which learns not only the specific color distribution\nbut also the preserves corresponding histopathological pattern. We perform the\nprocess of translation based on the pix2pix framework, which uses the\nconditional generator adversarial networks (cGANs). Our approach showed\nexcellent results, both mathematically and experimentally against the state of\nthe art methods. We have made the source code publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 11:19:01 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Salehi", "Pegah", ""], ["Chalechale", "Abdolah", ""]]}, {"id": "2002.00667", "submitter": "Sascha Wirges", "authors": "Sascha Wirges and Shuxiao Ding and Christoph Stiller", "title": "Single-Stage Object Detection from Top-View Grid Maps on Custom Sensor\n  Setups", "comments": "6 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our approach to unsupervised domain adaptation for single-stage\nobject detectors on top-view grid maps in automated driving scenarios. Our goal\nis to train a robust object detector on grid maps generated from custom sensor\ndata and setups. We first introduce a single-stage object detector for grid\nmaps based on RetinaNet. We then extend our model by image- and instance-level\ndomain classifiers at different feature pyramid levels which are trained in an\nadversarial manner. This allows us to train robust object detectors for\nunlabeled domains. We evaluate our approach quantitatively on the nuScenes and\nKITTI benchmarks and present qualitative domain adaptation results for\nunlabeled measurements recorded by our experimental vehicle. Our results\ndemonstrate that object detection accuracy for unlabeled domains can be\nimproved by applying our domain adaptation strategy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 12:05:20 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wirges", "Sascha", ""], ["Ding", "Shuxiao", ""], ["Stiller", "Christoph", ""]]}, {"id": "2002.00677", "submitter": "Devraj Mandal", "authors": "Devraj Mandal, Soma Biswas", "title": "A Novel Incremental Cross-Modal Hashing Approach", "comments": "20 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cross-modal retrieval deals with retrieving relevant items from one modality,\nwhen provided with a search query from another modality. Hashing techniques,\nwhere the data is represented as binary bits have specifically gained\nimportance due to the ease of storage, fast computations and high accuracy. In\nreal world, the number of data categories is continuously increasing, which\nrequires algorithms capable of handling this dynamic scenario. In this work, we\npropose a novel incremental cross-modal hashing algorithm termed \"iCMH\", which\ncan adapt itself to handle incoming data of new categories. The proposed\napproach consists of two sequential stages, namely, learning the hash codes and\ntraining the hash functions. At every stage, a small amount of old category\ndata termed \"exemplars\" is is used so as not to forget the old data while\ntrying to learn for the new incoming data, i.e. to avoid catastrophic\nforgetting. In the first stage, the hash codes for the exemplars is used, and\nsimultaneously, hash codes for the new data is computed such that it maintains\nthe semantic relations with the existing data. For the second stage, we propose\nboth a non-deep and deep architectures to learn the hash functions effectively.\nExtensive experiments across a variety of cross-modal datasets and comparisons\nwith state-of-the-art cross-modal algorithms shows the usefulness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 12:34:56 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Mandal", "Devraj", ""], ["Biswas", "Soma", ""]]}, {"id": "2002.00718", "submitter": "Fabio Cermelli", "authors": "Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bul\\`o, Elisa Ricci,\n  Barbara Caputo", "title": "Modeling the Background for Incremental Learning in Semantic\n  Segmentation", "comments": "Accepted at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their effectiveness in a wide range of tasks, deep architectures\nsuffer from some important limitations. In particular, they are vulnerable to\ncatastrophic forgetting, i.e. they perform poorly when they are required to\nupdate their model as new classes are available but the original training set\nis not retained. This paper addresses this problem in the context of semantic\nsegmentation. Current strategies fail on this task because they do not consider\na peculiar aspect of semantic segmentation: since each training step provides\nannotation only for a subset of all possible classes, pixels of the background\nclass (i.e. pixels that do not belong to any other classes) exhibit a semantic\ndistribution shift. In this work we revisit classical incremental learning\nmethods, proposing a new distillation-based framework which explicitly accounts\nfor this shift. Furthermore, we introduce a novel strategy to initialize\nclassifier's parameters, thus preventing biased predictions toward the\nbackground class. We demonstrate the effectiveness of our approach with an\nextensive evaluation on the Pascal-VOC 2012 and ADE20K datasets, significantly\noutperforming state of the art incremental learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 13:30:38 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 14:01:26 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Cermelli", "Fabio", ""], ["Mancini", "Massimiliano", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Ricci", "Elisa", ""], ["Caputo", "Barbara", ""]]}, {"id": "2002.00751", "submitter": "Johannes Hofmanninger", "authors": "Johannes Hofmanninger, Sebastian Roehrich, Helmut Prosch and Georg\n  Langs", "title": "Separation of target anatomical structure and occlusions in chest\n  radiographs", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiographs are commonly performed low-cost exams for screening and\ndiagnosis. However, radiographs are 2D representations of 3D structures causing\nconsiderable clutter impeding visual inspection and automated image analysis.\nHere, we propose a Fully Convolutional Network to suppress, for a specific\ntask, undesired visual structure from radiographs while retaining the relevant\nimage information such as lung-parenchyma. The proposed algorithm creates\nreconstructed radiographs and ground-truth data from high resolution CT-scans.\nResults show that removing visual variation that is irrelevant for a\nclassification task improves the performance of a classifier when only limited\ntraining data are available. This is particularly relevant because a low number\nof ground-truth cases is common in medical imaging.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 14:01:06 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Hofmanninger", "Johannes", ""], ["Roehrich", "Sebastian", ""], ["Prosch", "Helmut", ""], ["Langs", "Georg", ""]]}, {"id": "2002.00774", "submitter": "Yunjae Jung", "authors": "Yunjae Jung, Dahun Kim, Sanghyun Woo, Kyungsu Kim, Sungjin Kim, In So\n  Kweon", "title": "Hide-and-Tell: Learning to Bridge Photo Streams for Visual Storytelling", "comments": "AAAI 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual storytelling is a task of creating a short story based on photo\nstreams. Unlike existing visual captioning, storytelling aims to contain not\nonly factual descriptions, but also human-like narration and semantics.\nHowever, the VIST dataset consists only of a small, fixed number of photos per\nstory. Therefore, the main challenge of visual storytelling is to fill in the\nvisual gap between photos with narrative and imaginative story. In this paper,\nwe propose to explicitly learn to imagine a storyline that bridges the visual\ngap. During training, one or more photos is randomly omitted from the input\nstack, and we train the network to produce a full plausible story even with\nmissing photo(s). Furthermore, we propose for visual storytelling a\nhide-and-tell model, which is designed to learn non-local relations across the\nphoto streams and to refine and improve conventional RNN-based models. In\nexperiments, we show that our scheme of hide-and-tell, and the network design\nare indeed effective at storytelling, and that our model outperforms previous\nstate-of-the-art methods in automatic metrics. Finally, we qualitatively show\nthe learned ability to interpolate storyline over visual gaps.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 14:22:18 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Jung", "Yunjae", ""], ["Kim", "Dahun", ""], ["Woo", "Sanghyun", ""], ["Kim", "Kyungsu", ""], ["Kim", "Sungjin", ""], ["Kweon", "In So", ""]]}, {"id": "2002.00786", "submitter": "Sravan Mylavarapu", "authors": "Sravan Mylavarapu, Mahtab Sandhu, Priyesh Vijayan, K Madhava Krishna,\n  Balaraman Ravindran, Anoop Namboodiri", "title": "Towards Accurate Vehicle Behaviour Classification With Multi-Relational\n  Graph Convolutional Networks", "comments": "To appear in IV (IEEE Intelligent Vehicles Symposium) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding on-road vehicle behaviour from a temporal sequence of sensor\ndata is gaining in popularity. In this paper, we propose a pipeline for\nunderstanding vehicle behaviour from a monocular image sequence or video. A\nmonocular sequence along with scene semantics, optical flow and object labels\nare used to get spatial information about the object (vehicle) of interest and\nother objects (semantically contiguous set of locations) in the scene. This\nspatial information is encoded by a Multi-Relational Graph Convolutional\nNetwork (MR-GCN), and a temporal sequence of such encodings is fed to a\nrecurrent network to label vehicle behaviours. The proposed framework can\nclassify a variety of vehicle behaviours to high fidelity on datasets that are\ndiverse and include European, Chinese and Indian on-road scenes. The framework\nalso provides for seamless transfer of models across datasets without entailing\nre-annotation, retraining and even fine-tuning. We show comparative performance\ngain over baseline Spatio-temporal classifiers and detail a variety of\nablations to showcase the efficacy of the framework.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 14:34:28 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 16:30:32 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 17:49:11 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Mylavarapu", "Sravan", ""], ["Sandhu", "Mahtab", ""], ["Vijayan", "Priyesh", ""], ["Krishna", "K Madhava", ""], ["Ravindran", "Balaraman", ""], ["Namboodiri", "Anoop", ""]]}, {"id": "2002.00807", "submitter": "Akash Kumar", "authors": "Akash Kumar, Arnav Bhavasar", "title": "Syn2Real: Forgery Classification via Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, image manipulation is becoming increasingly more accessible,\nyielding more natural-looking images, owing to the modern tools in image\nprocessing and computer vision techniques. The task of the identification of\nforged images has become very challenging. Amongst different types of\nforgeries, the cases of Copy-Move forgery are increasing manifold, due to the\ndifficulties involved to detect this tampering. To tackle such problems,\npublicly available datasets are insufficient. In this paper, we propose to\ncreate a synthetic forged dataset using deep semantic image inpainting and\ncopy-move forgery algorithm. However, models trained on these datasets have a\nsignificant drop in performance when tested on more realistic data. To\nalleviate this problem, we use unsupervised domain adaptation networks to\ndetect copy-move forgery in new domains by mapping the feature space from our\nsynthetically generated dataset. Furthermore, we improvised the F1 score on\nCASIA and CoMoFoD dataset to 80.3% and 78.8%, respectively. Our approach can be\nhelpful in those cases where the classification of data is unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 15:02:58 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Kumar", "Akash", ""], ["Bhavasar", "Arnav", ""]]}, {"id": "2002.00815", "submitter": "Sebastian Mathias Keller", "authors": "Sebastian Mathias Keller, Maxim Samarin, Fabricio Arend Torres, Mario\n  Wieser, Volker Roth", "title": "Learning Extremal Representations with Deep Archetypal Analysis", "comments": "Under review for publication at the International Journal of Computer\n  Vision (IJCV). Extended version of our GCPR2019 paper \"Deep Archetypal\n  Analysis\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetypes are typical population representatives in an extremal sense, where\ntypicality is understood as the most extreme manifestation of a trait or\nfeature. In linear feature space, archetypes approximate the data convex hull\nallowing all data points to be expressed as convex mixtures of archetypes.\nHowever, it might not always be possible to identify meaningful archetypes in a\ngiven feature space. Learning an appropriate feature space and identifying\nsuitable archetypes simultaneously addresses this problem. This paper\nintroduces a generative formulation of the linear archetype model,\nparameterized by neural networks. By introducing the distance-dependent\narchetype loss, the linear archetype model can be integrated into the latent\nspace of a variational autoencoder, and an optimal representation with respect\nto the unknown archetypes can be learned end-to-end. The reformulation of\nlinear Archetypal Analysis as deep variational information bottleneck, allows\nthe incorporation of arbitrarily complex side information during training.\nFurthermore, an alternative prior, based on a modified Dirichlet distribution,\nis proposed. The real-world applicability of the proposed method is\ndemonstrated by exploring archetypes of female facial expressions while using\nmulti-rater based emotion scores of these expressions as side information. A\nsecond application illustrates the exploration of the chemical space of small\norganic molecules. In this experiment, it is demonstrated that exchanging the\nside information but keeping the same set of molecules, e. g. using as side\ninformation the heat capacity of each molecule instead of the band gap energy,\nwill result in the identification of different archetypes. As an application,\nthese learned representations of chemical space might reveal distinct starting\npoints for de novo molecular design.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 15:13:49 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Keller", "Sebastian Mathias", ""], ["Samarin", "Maxim", ""], ["Torres", "Fabricio Arend", ""], ["Wieser", "Mario", ""], ["Roth", "Volker", ""]]}, {"id": "2002.00842", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Tolga Aktas, Jiebo Luo", "title": "Mi YouTube es Su YouTube? Analyzing the Cultures using YouTube\n  Thumbnails of Popular Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  YouTube, a world-famous video sharing website, maintains a list of the top\ntrending videos on the platform. Due to its huge amount of users, it enables\nresearchers to understand people's preference by analyzing the trending videos.\nTrending videos vary from country to country. By analyzing such differences and\nchanges, we can tell how users' preferences differ over locations. Previous\nwork focuses on analyzing such culture preferences from videos' metadata, while\nthe culture information hidden within the visual content has not been\ndiscovered. In this study, we explore culture preferences among countries using\nthe thumbnails of YouTube trending videos. We first process the thumbnail\nimages of the videos using object detectors. The collected object information\nis then used for various statistical analysis. In particular, we examine the\ndata from three perspectives: geographical locations, video genres and users'\nreactions. Experimental results indicate that the users from similar cultures\nshares interests in watching similar videos on YouTube. Our study demonstrates\nthat discovering the culture preference through the thumbnails can be an\neffective mechanism for video social media analysis.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 20:15:57 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Zhang", "Songyang", ""], ["Aktas", "Tolga", ""], ["Luo", "Jiebo", ""]]}, {"id": "2002.00867", "submitter": "Peng Xu", "authors": "Peng Xu, Zeyu Song, Qiyue Yin, Yi-Zhe Song, Liang Wang", "title": "Deep Self-Supervised Representation Learning for Free-Hand Sketch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle for the first time, the problem of self-supervised\nrepresentation learning for free-hand sketches. This importantly addresses a\ncommon problem faced by the sketch community -- that annotated supervisory data\nare difficult to obtain. This problem is very challenging in that sketches are\nhighly abstract and subject to different drawing styles, making existing\nsolutions tailored for photos unsuitable. Key for the success of our\nself-supervised learning paradigm lies with our sketch-specific designs: (i) we\npropose a set of pretext tasks specifically designed for sketches that mimic\ndifferent drawing styles, and (ii) we further exploit the use of a textual\nconvolution network (TCN) in a dual-branch architecture for sketch feature\nlearning, as means to accommodate the sequential stroke nature of sketches. We\ndemonstrate the superiority of our sketch-specific designs through two\nsketch-related applications (retrieval and recognition) on a million-scale\nsketch dataset, and show that the proposed approach outperforms the\nstate-of-the-art unsupervised representation learning methods, and\nsignificantly narrows the performance gap between with supervised\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 16:28:29 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Xu", "Peng", ""], ["Song", "Zeyu", ""], ["Yin", "Qiyue", ""], ["Song", "Yi-Zhe", ""], ["Wang", "Liang", ""]]}, {"id": "2002.00883", "submitter": "Decky Aspandi", "authors": "Decky Aspandi, Adria Mallol-Ragolta, Bj\\\"orn Schuller, Xavier Binefa", "title": "Adversarial-based neural networks for affect estimations in the wild", "comments": "Paper for FG 2020 Affect Challenge\n  https://ibug.doc.ic.ac.uk/resources/fg-2020-competition-affective-behavior-analysis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in affective computing research nowadays given\nits crucial role in bridging humans with computers. This progress has been\nrecently accelerated due to the emergence of bigger data. One recent advance in\nthis field is the use of adversarial learning to improve model learning through\naugmented samples. However, the use of latent features, which is feasible\nthrough adversarial learning, is not largely explored, yet. This technique may\nalso improve the performance of affective models, as analogously demonstrated\nin related fields, such as computer vision. To expand this analysis, in this\nwork, we explore the use of latent features through our proposed\nadversarial-based networks for valence and arousal recognition in the wild.\nSpecifically, our models operate by aggregating several modalities to our\ndiscriminator, which is further conditioned to the extracted latent features by\nthe generator. Our experiments on the recently released SEWA dataset suggest\nthe progressive improvements of our results. Finally, we show our competitive\nresults on the Affective Behavior Analysis in-the-Wild (ABAW) challenge dataset\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 16:52:49 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 15:31:49 GMT"}, {"version": "v3", "created": "Sun, 9 Feb 2020 23:00:05 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Aspandi", "Decky", ""], ["Mallol-Ragolta", "Adria", ""], ["Schuller", "Bj\u00f6rn", ""], ["Binefa", "Xavier", ""]]}, {"id": "2002.00892", "submitter": "Victor Boutin", "authors": "Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent Perrinet", "title": "Effect of top-down connections in Hierarchical Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent\nmulti-dimensional, structured data such as images. The simplest solution to\nsolve this computationally hard problem is to decompose it into independent\nlayer-wise subproblems. However, neuroscientific evidence would suggest\ninter-connecting these subproblems as in the Predictive Coding (PC) theory,\nwhich adds top-down connections between consecutive layers. In this study, a\nnew model called 2-Layers Sparse Predictive Coding (2L-SPC) is introduced to\nassess the impact of this inter-layer feedback connection. In particular, the\n2L-SPC is compared with a Hierarchical Lasso (Hi-La) network made out of a\nsequence of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La\nnetworks are trained on 4 different databases and with different sparsity\nparameters on each layer. First, we show that the overall prediction error\ngenerated by 2L-SPC is lower thanks to the feedback mechanism as it transfers\nprediction error between layers. Second, we demonstrate that the inference\nstage of the 2L-SPC is faster to converge than for the Hi-La model. Third, we\nshow that the 2L-SPC also accelerates the learning process. Finally, the\nqualitative analysis of both models dictionaries, supported by their activation\nprobability, show that the 2L-SPC features are more generic and informative.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 17:12:01 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Boutin", "Victor", ""], ["Franciosini", "Angelo", ""], ["Ruffier", "Franck", ""], ["Perrinet", "Laurent", ""]]}, {"id": "2002.00899", "submitter": "Francesco Ragusa", "authors": "Francesco Ragusa, Antonino Furnari, Sebastiano Battiato, Giovanni\n  Signorello, Giovanni Maria Farinella", "title": "EGO-CH: Dataset and Fundamental Tasks for Visitors\n  BehavioralUnderstanding using Egocentric Vision", "comments": null, "journal-ref": "Pattern Recognition Letters 2020", "doi": "10.1016/j.patrec.2019.12.016", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equipping visitors of a cultural site with a wearable device allows to easily\ncollect information about their preferences which can be exploited to improve\nthe fruition of cultural goods with augmented reality. Moreover, egocentric\nvideo can be processed using computer vision and machine learning to enable an\nautomated analysis of visitors' behavior. The inferred information can be used\nboth online to assist the visitor and offline to support the manager of the\nsite. Despite the positive impact such technologies can have in cultural\nheritage, the topic is currently understudied due to the limited number of\npublic datasets suitable to study the considered problems. To address this\nissue, in this paper we propose EGOcentric-Cultural Heritage (EGO-CH), the\nfirst dataset of egocentric videos for visitors' behavior understanding in\ncultural sites. The dataset has been collected in two cultural sites and\nincludes more than $27$ hours of video acquired by $70$ subjects, with labels\nfor $26$ environments and over $200$ different Points of Interest. A large\nsubset of the dataset, consisting of $60$ videos, is associated with surveys\nfilled out by real visitors. To encourage research on the topic, we propose $4$\nchallenging tasks (room-based localization, point of interest/object\nrecognition, object retrieval and survey prediction) useful to understand\nvisitors' behavior and report baseline results on the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 17:25:23 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Ragusa", "Francesco", ""], ["Furnari", "Antonino", ""], ["Battiato", "Sebastiano", ""], ["Signorello", "Giovanni", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "2002.00911", "submitter": "Mathieu Gonzalez", "authors": "Mathieu Gonzalez, Amine Kacete, Albert Murienne, Eric Marchand", "title": "L6DNet: Light 6 DoF Network for Robust and Precise Object Pose\n  Estimation with Small Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 3D pose of an object is a challenging task that can be\nconsidered within augmented reality or robotic applications. In this paper, we\npropose a novel approach to perform 6 DoF object pose estimation from a single\nRGB-D image. We adopt a hybrid pipeline in two stages: data-driven and\ngeometric respectively. The data-driven step consists of a classification CNN\nto estimate the object 2D location in the image from local patches, followed by\na regression CNN trained to predict the 3D location of a set of keypoints in\nthe camera coordinate system. To extract the pose information, the geometric\nstep consists in aligning the 3D points in the camera coordinate system with\nthe corresponding 3D points in world coordinate system by minimizing a\nregistration error, thus computing the pose. Our experiments on the standard\ndataset LineMod show that our approach is more robust and accurate than\nstate-of-the-art methods. The approach is also validated to achieve a 6 DoF\npositioning task by visual servoing.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 17:41:29 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 17:02:45 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 07:47:38 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 14:03:03 GMT"}, {"version": "v5", "created": "Thu, 7 Jan 2021 08:18:10 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Gonzalez", "Mathieu", ""], ["Kacete", "Amine", ""], ["Murienne", "Albert", ""], ["Marchand", "Eric", ""]]}, {"id": "2002.00937", "submitter": "Alexandre Sablayrolles", "authors": "Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Herv\\'e\n  J\\'egou", "title": "Radioactive data: tracing through training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We want to detect whether a particular image dataset has been used to train a\nmodel. We propose a new technique, \\emph{radioactive data}, that makes\nimperceptible changes to this dataset such that any model trained on it will\nbear an identifiable mark. The mark is robust to strong variations such as\ndifferent architectures or optimization methods. Given a trained model, our\ntechnique detects the use of radioactive data and provides a level of\nconfidence (p-value). Our experiments on large-scale benchmarks (Imagenet),\nusing standard architectures (Resnet-18, VGG-16, Densenet-121) and training\nprocedures, show that we can detect usage of radioactive data with high\nconfidence (p<10^-4) even when only 1% of the data used to trained our model is\nradioactive. Our method is robust to data augmentation and the stochasticity of\ndeep network optimization. As a result, it offers a much higher signal-to-noise\nratio than data poisoning and backdoor methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 18:41:08 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Sablayrolles", "Alexandre", ""], ["Douze", "Matthijs", ""], ["Schmid", "Cordelia", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "2002.00952", "submitter": "Maria Ines Meyer", "authors": "Mattias Billast, Maria Ines Meyer, Diana M. Sima and David Robben", "title": "Improved inter-scanner MS lesion segmentation by adversarial training on\n  longitudinal data", "comments": "Added link to final authenticated publication\n  (https://doi.org/10.1007/978-3-030-46640-4_10)", "journal-ref": "Crimi A., Bakas S. (eds) Brainlesion: Glioma, Multiple Sclerosis,\n  Stroke and Traumatic Brain Injuries. BrainLes 2019. Lecture Notes in Computer\n  Science, vol 11992. Springer, Cham", "doi": "10.1007/978-3-030-46640-4_10", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of white matter lesion progression is an important biomarker\nin the follow-up of MS patients and plays a crucial role when deciding the\ncourse of treatment. Current automated lesion segmentation algorithms are\nsusceptible to variability in image characteristics related to MRI scanner or\nprotocol differences. We propose a model that improves the consistency of MS\nlesion segmentations in inter-scanner studies. First, we train a CNN base model\nto approximate the performance of icobrain, an FDA-approved clinically\navailable lesion segmentation software. A discriminator model is then trained\nto predict if two lesion segmentations are based on scans acquired using the\nsame scanner type or not, achieving a 78% accuracy in this task. Finally, the\nbase model and the discriminator are trained adversarially on multi-scanner\nlongitudinal data to improve the inter-scanner consistency of the base model.\nThe performance of the models is evaluated on an unseen dataset containing\nmanual delineations. The inter-scanner variability is evaluated on test-retest\ndata, where the adversarial network produces improved results over the base\nmodel and the FDA-approved solution.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 16:56:05 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 11:11:26 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Billast", "Mattias", ""], ["Meyer", "Maria Ines", ""], ["Sima", "Diana M.", ""], ["Robben", "David", ""]]}, {"id": "2002.01008", "submitter": "Zhengyu Zhao", "authors": "Zhengyu Zhao, Zhuoran Liu, Martha Larson", "title": "Adversarial Color Enhancement: Generating Unrestricted Adversarial\n  Images by Optimizing a Color Filter", "comments": "Accepted by BMVC 2020. Code is available at\n  https://github.com/ZhengyuZhao/ACE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce an approach that enhances images using a color filter in order\nto create adversarial effects, which fool neural networks into\nmisclassification. Our approach, Adversarial Color Enhancement (ACE), generates\nunrestricted adversarial images by optimizing the color filter via gradient\ndescent. The novelty of ACE is its incorporation of established practice for\nimage enhancement in a transparent manner. Experimental results validate the\nwhite-box adversarial strength and black-box transferability of ACE. A range of\nexamples demonstrates the perceptual quality of images that ACE produces. ACE\nmakes an important contribution to recent work that moves beyond $L_p$\nimperceptibility and focuses on unrestricted adversarial modifications that\nyield large perceptible perturbations, but remain non-suspicious, to the human\neye. The future potential of filter-based adversaries is also explored in two\ndirections: guiding ACE with common enhancement practices (e.g., Instagram\nfilters) towards specific attractive image styles and adapting ACE to image\nsemantics. Code is available at https://github.com/ZhengyuZhao/ACE.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 20:44:29 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 21:44:35 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2020 18:23:24 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhao", "Zhengyu", ""], ["Liu", "Zhuoran", ""], ["Larson", "Martha", ""]]}, {"id": "2002.01014", "submitter": "P. Jonathon Phillips", "authors": "P. Jonathon Phillips and Mark Przybocki", "title": "Four Principles of Explainable AI as Applied to Biometrics and Facial\n  Forensic Algorithms", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Traditionally, researchers in automatic face recognition and biometric\ntechnologies have focused on developing accurate algorithms. With this\ntechnology being integrated into operational systems, engineers and scientists\nare being asked, do these systems meet societal norms? The origin of this line\nof inquiry is `trust' of artificial intelligence (AI) systems. In this paper,\nwe concentrate on adapting explainable AI to face recognition and biometrics,\nand we present four principles of explainable AI to face recognition and\nbiometrics. The principles are illustrated by $\\it{four}$ case studies, which\nshow the challenges and issues in developing algorithms that can produce\nexplanations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 21:03:20 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Phillips", "P. Jonathon", ""], ["Przybocki", "Mark", ""]]}, {"id": "2002.01034", "submitter": "Bryar Shareef", "authors": "Bryar Shareef, Min Xian, Aleksandar Vakanski", "title": "Stan: Small tumor-aware network for breast ultrasound image segmentation", "comments": "5 pages, 3 figures, Accepted to 2020 ISBI conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast tumor segmentation provides accurate tumor boundary, and serves as a\nkey step toward further cancer quantification. Although deep learning-based\napproaches have been proposed and achieved promising results, existing\napproaches have difficulty in detecting small breast tumors. The capacity to\ndetecting small tumors is particularly important in finding early stage cancers\nusing computer-aided diagnosis (CAD) systems. In this paper, we propose a novel\ndeep learning architecture called Small Tumor-Aware Network (STAN), to improve\nthe performance of segmenting tumors with different size. The new architecture\nintegrates both rich context information and high-resolution image features. We\nvalidate the proposed approach using seven quantitative metrics on two public\nbreast ultrasound datasets. The proposed approach outperformed the\nstate-of-the-art approaches in segmenting small breast tumors. Index\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 22:25:01 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Shareef", "Bryar", ""], ["Xian", "Min", ""], ["Vakanski", "Aleksandar", ""]]}, {"id": "2002.01036", "submitter": "Thanuja Ambegoda", "authors": "Thanuja D. Ambegoda and Matthew Cook", "title": "Efficient 2D neuron boundary segmentation with local topological\n  constraints", "comments": "Presented at the Connectomics Workshop - Neural Information\n  Processing Systems (NIPS), December 10th, 2016 - Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for segmenting neuron membranes in 2D electron microscopy\nimagery. This segmentation task has been a bottleneck to reconstruction efforts\nof the brain's synaptic circuits. One common problem is the misclassification\nof blurry membrane fragments as cell interior, which leads to merging of two\nadjacent neuron sections into one via the blurry membrane region. Human\nannotators can easily avoid such errors by implicitly performing gap\ncompletion, taking into account the continuity of membranes.\n  Drawing inspiration from these human strategies, we formulate the\nsegmentation task as an edge labeling problem on a graph with local topological\nconstraints. We derive an integer linear program (ILP) that enforces membrane\ncontinuity, i.e. the absence of gaps. The cost function of the ILP is the\npixel-wise deviation of the segmentation from a priori membrane probabilities\nderived from the data.\n  Based on membrane probability maps obtained using random forest classifiers\nand convolutional neural networks, our method improves the neuron boundary\nsegmentation accuracy compared to a variety of standard segmentation\napproaches. Our method successfully performs gap completion and leads to fewer\ntopological errors. The method could potentially also be incorporated into\nother image segmentation pipelines with known topological constraints.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 22:28:39 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Ambegoda", "Thanuja D.", ""], ["Cook", "Matthew", ""]]}, {"id": "2002.01048", "submitter": "Hao Tang", "authors": "Hao Tang, Dan Xu, Yan Yan, Jason J. Corso, Philip H.S. Torr, Nicu Sebe", "title": "Multi-Channel Attention Selection GANs for Guided Image-to-Image\n  Translation", "comments": "An extended version of a paper published in CVPR2019. arXiv admin\n  note: substantial text overlap with arXiv:1904.06807", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model named Multi-Channel Attention Selection Generative\nAdversarial Network (SelectionGAN) for guided image-to-image translation, where\nwe translate an input image into another while respecting an external semantic\nguidance. The proposed SelectionGAN explicitly utilizes the semantic guidance\ninformation and consists of two stages. In the first stage, the input image and\nthe conditional semantic guidance are fed into a cycled semantic-guided\ngeneration network to produce initial coarse results. In the second stage, we\nrefine the initial results by using the proposed multi-scale spatial pooling \\&\nchannel selection module and the multi-channel attention selection module.\nMoreover, uncertainty maps automatically learned from attention maps are used\nto guide the pixel loss for better network optimization. Exhaustive experiments\non four challenging guided image-to-image translation tasks (face, hand, body\nand street view) demonstrate that our SelectionGAN is able to generate\nsignificantly better results than the state-of-the-art methods. Meanwhile, the\nproposed framework and modules are unified solutions and can be applied to\nsolve other generation tasks, such as semantic image synthesis. The code is\navailable at https://github.com/Ha0Tang/SelectionGAN.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 23:17:10 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Tang", "Hao", ""], ["Xu", "Dan", ""], ["Yan", "Yan", ""], ["Corso", "Jason J.", ""], ["Torr", "Philip H. S.", ""], ["Sebe", "Nicu", ""]]}, {"id": "2002.01053", "submitter": "Chirag Agarwal", "authors": "Chirag Agarwal, Shahin Khobahi, Arindam Bose, Mojtaba Soltanalian, Dan\n  Schonfeld", "title": "Deep-URL: A Model-Aware Approach To Blind Deconvolution Based On Deep\n  Unfolded Richardson-Lucy Network", "comments": "Accepted. 27th IEEE International Conference on Image Processing\n  (ICIP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of interpretability in current deep learning models causes serious\nconcerns as they are extensively used for various life-critical applications.\nHence, it is of paramount importance to develop interpretable deep learning\nmodels. In this paper, we consider the problem of blind deconvolution and\npropose a novel model-aware deep architecture that allows for the recovery of\nboth the blur kernel and the sharp image from the blurred image. In particular,\nwe propose the Deep Unfolded Richardson-Lucy (Deep-URL) framework -- an\ninterpretable deep-learning architecture that can be seen as an amalgamation of\nclassical estimation technique and deep neural network, and consequently leads\nto improved performance. Our numerical investigations demonstrate significant\nimprovement compared to state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 23:43:08 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 18:53:45 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2020 21:19:09 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Agarwal", "Chirag", ""], ["Khobahi", "Shahin", ""], ["Bose", "Arindam", ""], ["Soltanalian", "Mojtaba", ""], ["Schonfeld", "Dan", ""]]}, {"id": "2002.01075", "submitter": "Huabin Wang", "authors": "Huabin Wang and Rui Cheng and Jian Zhou and Liang Tao and Hon Keung\n  Kwan", "title": "Multistage Model for Robust Face Alignment Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ability to generalize unconstrained conditions such as severe occlusions\nand large pose variations remains a challenging goal to achieve in face\nalignment. In this paper, a multistage model based on deep neural networks is\nproposed which takes advantage of spatial transformer networks, hourglass\nnetworks and exemplar-based shape constraints. First, a spatial transformer -\ngenerative adversarial network which consists of convolutional layers and\nresidual units is utilized to solve the initialization issues caused by face\ndetectors, such as rotation and scale variations, to obtain improved face\nbounding boxes for face alignment. Then, stacked hourglass network is employed\nto obtain preliminary locations of landmarks as well as their corresponding\nscores. In addition, an exemplar-based shape dictionary is designed to\ndetermine landmarks with low scores based on those with high scores. By\nincorporating face shape constraints, misaligned landmarks caused by occlusions\nor cluttered backgrounds can be considerably improved. Extensive experiments\nbased on challenging benchmark datasets are performed to demonstrate the\nsuperior performance of the proposed method over other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 01:13:58 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Wang", "Huabin", ""], ["Cheng", "Rui", ""], ["Zhou", "Jian", ""], ["Tao", "Liang", ""], ["Kwan", "Hon Keung", ""]]}, {"id": "2002.01087", "submitter": "Chenhao Lin", "authors": "Chenhao Lin, Siwen Wang, Dongqi Xu, Yu Lu, Wayne Zhang", "title": "Object Instance Mining for Weakly Supervised Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection (WSOD) using only image-level annotations\nhas attracted growing attention over the past few years. Existing approaches\nusing multiple instance learning easily fall into local optima, because such\nmechanism tends to learn from the most discriminative object in an image for\neach category. Therefore, these methods suffer from missing object instances\nwhich degrade the performance of WSOD. To address this problem, this paper\nintroduces an end-to-end object instance mining (OIM) framework for weakly\nsupervised object detection. OIM attempts to detect all possible object\ninstances existing in each image by introducing information propagation on the\nspatial and appearance graphs, without any additional annotations. During the\niterative learning process, the less discriminative object instances from the\nsame class can be gradually detected and utilized for training. In addition, we\ndesign an object instance reweighted loss to learn larger portion of each\nobject instance to further improve the performance. The experimental results on\ntwo publicly available databases, VOC 2007 and 2012, demonstrate the efficacy\nof proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 02:11:39 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Lin", "Chenhao", ""], ["Wang", "Siwen", ""], ["Xu", "Dongqi", ""], ["Lu", "Yu", ""], ["Zhang", "Wayne", ""]]}, {"id": "2002.01096", "submitter": "Yongzhen Ke", "authors": "Yaoting Wang (1 and 2), Yongzhen Ke (1 and 2), Kai Wang (1 and 2),\n  Cuijiao Zhang (1 and 2), Fan Qin (3) ((1) School of computer science and\n  technology, Tiangong University, (2) Tianjin Key Laboratory of Autonomous\n  Intelligence Technology and Systems, (3) Business School, Nankai University)", "title": "Aesthetic Quality Assessment for Group photograph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image aesthetic quality assessment has got much attention in recent years,\nbut not many works have been done on a specific genre of photos: Group\nphotograph. In this work, we designed a set of high-level features based on the\nexperience and principles of group photography: Opened-eye, Gaze, Smile,\nOccluded faces, Face Orientation, Facial blur, Character center. Then we\ncombined them and 83 generic aesthetic features to build two aesthetic\nassessment models. We also constructed a large dataset of group photographs -\nGPD- annotated with the aesthetic score. The experimental result shows that our\nfeatures perform well for categorizing professional photos and snapshots and\npredicting the distinction of multiple group photographs of diverse human\nstates under the same scene.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 02:52:52 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Wang", "Yaoting", "", "1 and 2"], ["Ke", "Yongzhen", "", "1 and 2"], ["Wang", "Kai", "", "1 and 2"], ["Zhang", "Cuijiao", "", "1 and 2"], ["Qin", "Fan", ""]]}, {"id": "2002.01102", "submitter": "Hui Li", "authors": "Huai-Shui Tong, Xiao-Jun Wu, Hui Li", "title": "Improved dual channel pulse coupled neural network and its application\n  to multi-focus image fusion", "comments": "15 pages, 7 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an improved dual channel pulse coupled neural network\n(IDC-PCNN) model for image fusion. The model can overcome some defects of\nstandard PCNN model. In this fusion scheme, the multiplication rule is replaced\nby addition rule in the information fusion pool of dual channel PCNN (DC-PCNN)\nmodel. Meanwhile the sum of modified Laplacian (SML) measure is adopted, which\nis better than other focus measures. This method not only inherits the good\ncharacteristics of the standard PCNN model but also enhances the computing\nefficiency and fusion quality. The performance of the proposed method is\nevaluated by using four criteria including average cross entropy, root mean\nsquare error, peak value signal to noise ratio and structure similarity index.\nComparative studies show that the proposed fusion algorithm outperforms the\nstandard PCNN method and the DC-PCNN method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 03:26:31 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Tong", "Huai-Shui", ""], ["Wu", "Xiao-Jun", ""], ["Li", "Hui", ""]]}, {"id": "2002.01105", "submitter": "Yu Chen", "authors": "Xianpeng Ji, Yu Ding, Lincheng Li, Yu Chen, Changjie Fan", "title": "Multi-label Relation Modeling in Facial Action Units Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an approach to the facial action units detections. The\ninvolved action units (AU) include AU1 (Inner Brow Raiser), AU2 (Outer Brow\nRaiser), AU4 (Brow Lowerer), AU6 (Cheek Raise), AU12 (Lip Corner Puller), AU15\n(Lip Corner Depressor), AU20 (Lip Stretcher), and AU25 (Lip Part). Our work\nrelies on the dataset released by the FG-2020 Competition: Affective Behavior\nAnalysis In-the-Wild (ABAW). The proposed method consists of the data\npreprocessing, the feature extraction and the AU classification. The data\npreprocessing includes the detection of face texture and landmarks. The texture\nstatic and landmark dynamic features are extracted through neural networks and\nthen fused as the feature latent representation. Finally, the fused feature is\ntaken as the initial hidden state of a recurrent neural network with a\ntrainable lookup AU table. The output of the RNN is the results of AU\nclassification. The detected accuracy is evaluated with 0.5$\\times$accuracy +\n0.5$\\times$F1. Our method achieve 0.56 with the validation data that is\nspecified by the organization committee.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 03:33:00 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 10:39:30 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ji", "Xianpeng", ""], ["Ding", "Yu", ""], ["Li", "Lincheng", ""], ["Chen", "Yu", ""], ["Fan", "Changjie", ""]]}, {"id": "2002.01107", "submitter": "Chengwei Chen", "authors": "Chengwei Chen and Pan Chen and Lingyu Yang and Jinyuan Mo and Haichuan\n  Song and Yuan Xie and Lizhuang Ma", "title": "Acoustic anomaly detection via latent regularized gaussian mixture\n  generative adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic anomaly detection aims at distinguishing abnormal acoustic signals\nfrom the normal ones. It suffers from the class imbalance issue and the lacking\nin the abnormal instances. In addition, collecting all kinds of abnormal or\nunknown samples for training purpose is impractical and timeconsuming. In this\npaper, a novel Gaussian Mixture Generative Adversarial Network (GMGAN) is\nproposed under semi-supervised learning framework, in which the underlying\nstructure of training data is not only captured in spectrogram reconstruction\nspace, but also can be further restricted in the space of latent representation\nin a discriminant manner. Experiments show that our model has clear superiority\nover previous methods, and achieves the state-of-the-art results on DCASE\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 03:39:50 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 02:27:12 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Chen", "Chengwei", ""], ["Chen", "Pan", ""], ["Yang", "Lingyu", ""], ["Mo", "Jinyuan", ""], ["Song", "Haichuan", ""], ["Xie", "Yuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2002.01125", "submitter": "Mahdi Biparva", "authors": "Mahdi Biparva, John Tsotsos", "title": "Selective Segmentation Networks Using Top-Down Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks model the transformation of the input sensory\ndata at the bottom of a network hierarchy to the semantic information at the\ntop of the visual hierarchy. Feedforward processing is sufficient for some\nobject recognition tasks. Top-Down selection is potentially required in\naddition to the Bottom-Up feedforward pass. It can, in part, address the\nshortcoming of the loss of location information imposed by the hierarchical\nfeature pyramids. We propose a unified 2-pass framework for object segmentation\nthat augments Bottom-Up \\convnets with a Top-Down selection network. We utilize\nthe top-down selection gating activities to modulate the bottom-up hidden\nactivities for segmentation predictions. We develop an end-to-end multi-task\nframework with loss terms satisfying task requirements at the two ends of the\nnetwork. We evaluate the proposed network on benchmark datasets for semantic\nsegmentation, and show that networks with the Top-Down selection capability\noutperform the baseline model. Additionally, we shed light on the superior\naspects of the new segmentation paradigm and qualitatively and quantitatively\nsupport the efficiency of the novel framework over the baseline model that\nrelies purely on parametric skip connections.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:47:23 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Biparva", "Mahdi", ""], ["Tsotsos", "John", ""]]}, {"id": "2002.01132", "submitter": "Shikha Dubey", "authors": "Shikha Dubey, Abhijeet Boragule, Moongu Jeon", "title": "3D ResNet with Ranking Loss Function for Abnormal Activity Detection in\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormal activity detection is one of the most challenging tasks in the field\nof computer vision. This study is motivated by the recent state-of-art work of\nabnormal activity detection, which utilizes both abnormal and normal videos in\nlearning abnormalities with the help of multiple instance learning by providing\nthe data with video-level information. In the absence of temporal-annotations,\nsuch a model is prone to give a false alarm while detecting the abnormalities.\nFor this reason, in this paper, we focus on the task of minimizing the false\nalarm rate while performing an abnormal activity detection task. The mitigation\nof these false alarms and recent advancement of 3D deep neural network in video\naction recognition task collectively give us motivation to exploit the 3D\nResNet in our proposed method, which helps to extract spatial-temporal features\nfrom the videos. Afterwards, using these features and deep multiple instance\nlearning along with the proposed ranking loss, our model learns to predict the\nabnormality score at the video segment level. Therefore, our proposed method 3D\ndeep Multiple Instance Learning with ResNet (MILR) along with the new proposed\nranking loss function achieves the best performance on the UCF-Crime benchmark\ndataset, as compared to other state-of-art methods. The effectiveness of our\nproposed method is demonstrated on the UCF-Crime dataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 05:32:21 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Dubey", "Shikha", ""], ["Boragule", "Abhijeet", ""], ["Jeon", "Moongu", ""]]}, {"id": "2002.01144", "submitter": "Renlong Hang", "authors": "Renlong Hang, Zhu Li, Pedram Ghamisi, Danfeng Hong, Guiyu Xia, and\n  Qingshan Liu", "title": "Classification of Hyperspectral and LiDAR Data Using Coupled CNNs", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 2020", "doi": "10.1109/TGRS.2020.2969024", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient and effective framework to fuse\nhyperspectral and Light Detection And Ranging (LiDAR) data using two coupled\nconvolutional neural networks (CNNs). One CNN is designed to learn\nspectral-spatial features from hyperspectral data, and the other one is used to\ncapture the elevation information from LiDAR data. Both of them consist of\nthree convolutional layers, and the last two convolutional layers are coupled\ntogether via a parameter sharing strategy. In the fusion phase, feature-level\nand decision-level fusion methods are simultaneously used to integrate these\nheterogeneous features sufficiently. For the feature-level fusion, three\ndifferent fusion strategies are evaluated, including the concatenation\nstrategy, the maximization strategy, and the summation strategy. For the\ndecision-level fusion, a weighted summation strategy is adopted, where the\nweights are determined by the classification accuracy of each output. The\nproposed model is evaluated on an urban data set acquired over Houston, USA,\nand a rural one captured over Trento, Italy. On the Houston data, our model can\nachieve a new record overall accuracy of 96.03%. On the Trento data, it\nachieves an overall accuracy of 99.12%. These results sufficiently certify the\neffectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 06:23:36 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Hang", "Renlong", ""], ["Li", "Zhu", ""], ["Ghamisi", "Pedram", ""], ["Hong", "Danfeng", ""], ["Xia", "Guiyu", ""], ["Liu", "Qingshan", ""]]}, {"id": "2002.01147", "submitter": "Hanhan Li", "authors": "Hanhan Li, Pin Wang", "title": "Adversarially Robust Frame Sampling with Bounded Irregularities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, video analysis tools for automatically extracting meaningful\ninformation from videos are widely studied and deployed. Because most of them\nuse deep neural networks which are computationally expensive, feeding only a\nsubset of video frames into such algorithms is desired. Sampling the frames\nwith fixed rate is always attractive for its simplicity, representativeness,\nand interpretability. For example, a popular cloud video API generated video\nand shot labels by processing only the first frame of every second in a video.\nHowever, one can easily attack such strategies by placing chosen frames at the\nsampled locations. In this paper, we present an elegant solution to this\nsampling problem that is provably robust against adversarial attacks and\nintroduces bounded irregularities as well.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 06:33:43 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Li", "Hanhan", ""], ["Wang", "Pin", ""]]}, {"id": "2002.01154", "submitter": "Yuting Hu", "authors": "Yuting Hu, Zhen Wang, and Ghassan AlRegib", "title": "Texture Classification using Block Intensity and Gradient Difference\n  (BIGD) Descriptor", "comments": null, "journal-ref": null, "doi": "10.1016/j.image.2019.115770", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an efficient and distinctive local descriptor,\nnamely block intensity and gradient difference (BIGD). In an image patch, we\nrandomly sample multi-scale block pairs and utilize the intensity and gradient\ndifferences of pairwise blocks to construct the local BIGD descriptor. The\nrandom sampling strategy and the multi-scale framework help BIGD descriptors\ncapture the distinctive patterns of patches at different orientations and\nspatial granularity levels. We use vectors of locally aggregated descriptors\n(VLAD) or improved Fisher vector (IFV) to encode local BIGD descriptors into a\nfull image descriptor, which is then fed into a linear support vector machine\n(SVM) classifier for texture classification. We compare the proposed descriptor\nwith typical and state-of-the-art ones by evaluating their classification\nperformance on five public texture data sets including Brodatz, CUReT,\nKTH-TIPS, and KTH-TIPS-2a and -2b. Experimental results show that the proposed\nBIGD descriptor with stronger discriminative power yields 0.12% ~ 6.43% higher\nclassification accuracy than the state-of-the-art texture descriptor, dense\nmicroblock difference (DMD).\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 07:03:51 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Hu", "Yuting", ""], ["Wang", "Zhen", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2002.01155", "submitter": "Md Jahidul Islam", "authors": "Md Jahidul Islam, Peigen Luo and Junaed Sattar", "title": "Simultaneous Enhancement and Super-Resolution of Underwater Imagery for\n  Improved Visual Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce and tackle the simultaneous enhancement and\nsuper-resolution (SESR) problem for underwater robot vision and provide an\nefficient solution for near real-time applications. We present Deep SESR, a\nresidual-in-residual network-based generative model that can learn to restore\nperceptual image qualities at 2x, 3x, or 4x higher spatial resolution. We\nsupervise its training by formulating a multi-modal objective function that\naddresses the chrominance-specific underwater color degradation, lack of image\nsharpness, and loss in high-level feature representation. It is also supervised\nto learn salient foreground regions in the image, which in turn guides the\nnetwork to learn global contrast enhancement. We design an end-to-end training\npipeline to jointly learn the saliency prediction and SESR on a shared\nhierarchical feature space for fast inference. Moreover, we present UFO-120,\nthe first dataset to facilitate large-scale SESR learning; it contains over\n1500 training samples and a benchmark test set of 120 samples. By thorough\nexperimental evaluation on the UFO-120 and other standard datasets, we\ndemonstrate that Deep SESR outperforms the existing solutions for underwater\nimage enhancement and super-resolution. We also validate its generalization\nperformance on several test cases that include underwater images with diverse\nspectral and spatial degradation levels, and also terrestrial images with\nunseen natural objects. Lastly, we analyze its computational feasibility for\nsingle-board deployments and demonstrate its operational benefits for\nvisually-guided underwater robots. The model and dataset information will be\navailable at: https://github.com/xahidbuffon/Deep-SESR.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 07:07:08 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Islam", "Md Jahidul", ""], ["Luo", "Peigen", ""], ["Sattar", "Junaed", ""]]}, {"id": "2002.01176", "submitter": "Anastasiya Chirvonaya", "authors": "A. Sheshkus (4 and 6), A. Chirvonaya (2 and 6), D. Matveev (5 and 6),\n  D. Nikolaev (1 and 6), V.L. Arlazarov (3 and 4) ((1) Institute for\n  Information Transmission Problems (Kharkevich Institute) RAS, Moscow, Russia,\n  (2) National University of Science and Technology \"MISIS\", (3) Moscow\n  Institute for Physics and Technology, Moscow, Russia, (4) Institute for\n  Systems Analysis, Federal Research Center \"Computer Science and Control\" of\n  Russian Academy of Sciences, Moscow, Russia, (5) Lomonosov Moscow State\n  University, Moscow, Russia, (6) Smart Engines Service LLC, Moscow, Russia)", "title": "Vanishing Point Detection with Direct and Transposed Fast Hough\n  Transform inside the neural network", "comments": "9 pages, 9 figures, submitted to \"Computer Optics\"; extra experiment\n  added, new theorem proof added, references added; typos corrected", "journal-ref": "Computer Optics 2020; 44(5): 737-745", "doi": "10.18287/2412-6179-CO-676", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we suggest a new neural network architecture for vanishing\npoint detection in images. The key element is the use of the direct and\ntransposed Fast Hough Transforms separated by convolutional layer blocks with\nstandard activation functions. It allows us to get the answer in the\ncoordinates of the input image at the output of the network and thus to\ncalculate the coordinates of the vanishing point by simply selecting the\nmaximum. Besides, it was proved that calculation of the transposed Fast Hough\nTransform can be performed using the direct one. The use of integral operators\nenables the neural network to rely on global rectilinear features in the image,\nand so it is ideal for detecting vanishing points. To demonstrate the\neffectiveness of the proposed architecture, we use a set of images from a DVR\nand show its superiority over existing methods. Note, in addition, that the\nproposed neural network architecture essentially repeats the process of direct\nand back projection used, for example, in computed tomography.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 09:10:45 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 09:40:49 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 13:08:55 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Sheshkus", "A.", "", "4 and 6"], ["Chirvonaya", "A.", "", "2 and 6"], ["Matveev", "D.", "", "5 and 6"], ["Nikolaev", "D.", "", "1 and 6"], ["Arlazarov", "V. L.", "", "3 and 4"]]}, {"id": "2002.01177", "submitter": "Zhaowei Chen", "authors": "Tong Liu, Zhaowei Chen, Yi Yang, Zehao Wu and Haowei Li", "title": "Lane Detection in Low-light Conditions Using an Efficient Data\n  Enhancement : Light Conditions Style Transfer", "comments": "Accepted by IV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, deep learning techniques are widely used for lane detection, but\napplication in low-light conditions remains a challenge until this day.\nAlthough multi-task learning and contextual-information-based methods have been\nproposed to solve the problem, they either require additional manual\nannotations or introduce extra inference overhead respectively. In this paper,\nwe propose a style-transfer-based data enhancement method, which uses\nGenerative Adversarial Networks (GANs) to generate images in low-light\nconditions, that increases the environmental adaptability of the lane detector.\nOur solution consists of three parts: the proposed SIM-CycleGAN, light\nconditions style transfer and lane detection network. It does not require\nadditional manual annotations nor extra inference overhead. We validated our\nmethods on the lane detection benchmark CULane using ERFNet. Empirically, lane\ndetection model trained using our method demonstrated adaptability in low-light\nconditions and robustness in complex scenarios. Our code for this paper will be\npublicly available.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 09:14:13 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 09:39:45 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Liu", "Tong", ""], ["Chen", "Zhaowei", ""], ["Yang", "Yi", ""], ["Wu", "Zehao", ""], ["Li", "Haowei", ""]]}, {"id": "2002.01180", "submitter": "Arun Pandey", "authors": "Arun Pandey, Joachim Schreurs, Johan A. K. Suykens", "title": "Robust Generative Restricted Kernel Machines using Weighted Conjugate\n  Feature Duality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in generative models has grown tremendously in the past decade.\nHowever, their training performance can be adversely affected by contamination,\nwhere outliers are encoded in the representation of the model. This results in\nthe generation of noisy data. In this paper, we introduce weighted conjugate\nfeature duality in the framework of Restricted Kernel Machines (RKMs). The RKM\nformulation allows for an easy integration of methods from classical robust\nstatistics. This formulation is used to fine-tune the latent space of\ngenerative RKMs using a weighting function based on the Minimum Covariance\nDeterminant, which is a highly robust estimator of multivariate location and\nscatter. Experiments show that the weighted RKM is capable of generating clean\nimages when contamination is present in the training data. We further show that\nthe robust method also preserves uncorrelated feature learning through\nqualitative and quantitative experiments on standard datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 09:23:25 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 17:53:39 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 14:35:30 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Pandey", "Arun", ""], ["Schreurs", "Joachim", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "2002.01192", "submitter": "Kalun Ho", "authors": "Kalun Ho, Janis Keuper, Margret Keuper", "title": "Unsupervised Multiple Person Tracking using AutoEncoder-Based Lifted\n  Multicuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Object Tracking (MOT) is a long-standing task in computer vision.\nCurrent approaches based on the tracking by detection paradigm either require\nsome sort of domain knowledge or supervision to associate data correctly into\ntracks. In this work, we present an unsupervised multiple object tracking\napproach based on visual features and minimum cost lifted multicuts. Our method\nis based on straight-forward spatio-temporal cues that can be extracted from\nneighboring frames in an image sequences without superivison. Clustering based\non these cues enables us to learn the required appearance invariances for the\ntracking task at hand and train an autoencoder to generate suitable latent\nrepresentation. Thus, the resulting latent representations can serve as robust\nappearance cues for tracking even over large temporal distances where no\nreliable spatio-temporal features could be extracted. We show that, despite\nbeing trained without using the provided annotations, our model provides\ncompetitive results on the challenging MOT Benchmark for pedestrian tracking.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 09:42:34 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Ho", "Kalun", ""], ["Keuper", "Janis", ""], ["Keuper", "Margret", ""]]}, {"id": "2002.01205", "submitter": "Yangyang Qin", "authors": "Hefei Ling, Yangyang Qin, Li Zhang, Yuxuan Shi, Ping Li", "title": "Selective Convolutional Network: An Efficient Object Detector with\n  Ignoring Background", "comments": "Accepted at ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that attention mechanisms can effectively improve the\nperformance of many CNNs including object detectors. Instead of refining\nfeature maps prevalently, we reduce the prohibitive computational complexity by\na novel attempt at attention. Therefore, we introduce an efficient object\ndetector called Selective Convolutional Network (SCN), which selectively\ncalculates only on the locations that contain meaningful and conducive\ninformation. The basic idea is to exclude the insignificant background areas,\nwhich effectively reduces the computational cost especially during the feature\nextraction. To solve it, we design an elaborate structure with negligible\noverheads to guide the network where to look next. It's end-to-end trainable\nand easy-embedding. Without additional segmentation datasets, we explores two\ndifferent train strategies including direct supervision and indirect\nsupervision. Extensive experiments assess the performance on PASCAL VOC2007 and\nMS COCO detection datasets. Results show that SSD and Pelee integrated with our\nmethod averagely reduce the calculations in a range of 1/5 and 1/3 with slight\nloss of accuracy, demonstrating the feasibility of SCN.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 10:07:01 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Ling", "Hefei", ""], ["Qin", "Yangyang", ""], ["Zhang", "Li", ""], ["Shi", "Yuxuan", ""], ["Li", "Ping", ""]]}, {"id": "2002.01210", "submitter": "Tom Roussel", "authors": "Tom Roussel, Punarjay Chakravarty, Gaurav Pandey, Tinne Tuytelaars,\n  Luc Van Eycken", "title": "Deep-Geometric 6 DoF Localization from a Single Image in Topo-metric\n  Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a Deep-Geometric Localizer that is able to estimate the full 6\nDegree of Freedom (DoF) global pose of the camera from a single image in a\npreviously mapped environment. Our map is a topo-metric one, with discrete\ntopological nodes whose 6 DoF poses are known. Each topo-node in our map also\ncomprises of a set of points, whose 2D features and 3D locations are stored as\npart of the mapping process. For the mapping phase, we utilise a stereo camera\nand a regular stereo visual SLAM pipeline. During the localization phase, we\ntake a single camera image, localize it to a topological node using Deep\nLearning, and use a geometric algorithm (PnP) on the matched 2D features (and\ntheir 3D positions in the topo map) to determine the full 6 DoF globally\nconsistent pose of the camera. Our method divorces the mapping and the\nlocalization algorithms and sensors (stereo and mono), and allows accurate 6\nDoF pose estimation in a previously mapped environment using a single camera.\nWith potential VR/AR and localization applications in single camera devices\nsuch as mobile phones and drones, our hybrid algorithm compares favourably with\nthe fully Deep-Learning based Pose-Net that regresses pose from a single image\nin simulated as well as real environments.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 10:11:46 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Roussel", "Tom", ""], ["Chakravarty", "Punarjay", ""], ["Pandey", "Gaurav", ""], ["Tuytelaars", "Tinne", ""], ["Van Eycken", "Luc", ""]]}, {"id": "2002.01225", "submitter": "Nicolas Dobigeon", "authors": "Etienne Monier, Thomas Oberlin, Nathalie Brun, Xiaoyan Li, Marcel\n  Tenc\\'e, Nicolas Dobigeon", "title": "Fast reconstruction of atomic-scale STEM-EELS images from sparse\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cond-mat.mtrl-sci cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the reconstruction of partially sampled spectrum-images\nto accelerate the acquisition in scanning transmission electron microscopy\n(STEM). The problem of image reconstruction has been widely considered in the\nliterature for many imaging modalities, but only a few attempts handled 3D data\nsuch as spectral images acquired by STEM electron energy loss spectroscopy\n(EELS). Besides, among the methods proposed in the microscopy literature, some\nare fast but inaccurate while others provide accurate reconstruction but at the\nprice of a high computation burden. Thus none of the proposed reconstruction\nmethods fulfills our expectations in terms of accuracy and computation\ncomplexity. In this paper, we propose a fast and accurate reconstruction method\nsuited for atomic-scale EELS. This method is compared to popular solutions such\nas beta process factor analysis (BPFA) which is used for the first time on\nSTEM-EELS images. Experiments based on real as synthetic data will be\nconducted.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 11:07:56 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Monier", "Etienne", ""], ["Oberlin", "Thomas", ""], ["Brun", "Nathalie", ""], ["Li", "Xiaoyan", ""], ["Tenc\u00e9", "Marcel", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "2002.01238", "submitter": "Arunima Banerjee Dr.", "authors": "Prem Prakash, Arunima Banerjee, Pavan Kumar Perepu", "title": "Determination of the relative inclination and the viewing angle of an\n  interacting pair of galaxies using convolutional neural networks", "comments": "13 pages, 11 Figures, 15 tables (Accepted for publication in the\n  MNRAS)", "journal-ref": null, "doi": "10.1093/mnras/staa2109", "report-no": null, "categories": "astro-ph.GA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing dynamical models for interacting pair of galaxies as constrained\nby their observed structure and kinematics crucially depends on the correct\nchoice of the values of the relative inclination ($i$) between their galactic\nplanes as well as the viewing angle ($\\theta$), the angle between the line of\nsight and the normal to the plane of their orbital motion. We construct Deep\nConvolutional Neural Network (DCNN) models to determine the relative\ninclination ($i$) and the viewing angle ($\\theta$) of interacting galaxy pairs,\nusing N-body $+$ Smoothed Particle Hydrodynamics (SPH) simulation data from the\nGALMER database for training the same. In order to classify galaxy pairs based\non their $i$ values only, we first construct DCNN models for a (a) 2-class (\n$i$ = 0 $^{\\circ}$, 45$^{\\circ}$ ) and (b) 3-class ($i = 0^{\\circ}, 45^{\\circ}\n\\text{ and } 90^{\\circ}$) classification, obtaining $F_1$ scores of 99% and 98%\nrespectively. Further, for a classification based on both $i$ and $\\theta$\nvalues, we develop a DCNN model for a 9-class classification ($(i,\\theta) \\sim\n(0^{\\circ},15^{\\circ}) ,(0^{\\circ},45^{\\circ}), (0^{\\circ},90^{\\circ}),\n(45^{\\circ},15^{\\circ}), (45^{\\circ}, 45^{\\circ}), (45^{\\circ}, 90^{\\circ}),\n(90^{\\circ}, 15^{\\circ}), (90^{\\circ}, 45^{\\circ}), (90^{\\circ},90^{\\circ})$),\nand the $F_1$ score was 97$\\%$. Finally, we tested our 2-class model on real\ndata of interacting galaxy pairs from the Sloan Digital Sky Survey (SDSS) DR15,\nand achieve an $F_1$ score of 78%. Our DCNN models could be further extended to\ndetermine additional parameters needed to model dynamics of interacting galaxy\npairs, which is currently accomplished by trial and error method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 11:54:07 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 06:42:42 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Prakash", "Prem", ""], ["Banerjee", "Arunima", ""], ["Perepu", "Pavan Kumar", ""]]}, {"id": "2002.01276", "submitter": "Wenyang Hu", "authors": "Wenyang Hu, Xiaocong Cai, Jun Hou, Shuai Yi, Zhiping Lin", "title": "GTC: Guided Training of CTC Towards Efficient and Accurate Scene Text\n  Recognition", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectionist Temporal Classification (CTC) and attention mechanism are two\nmain approaches used in recent scene text recognition works. Compared with\nattention-based methods, CTC decoder has a much shorter inference time, yet a\nlower accuracy. To design an efficient and effective model, we propose the\nguided training of CTC (GTC), where CTC model learns a better alignment and\nfeature representations from a more powerful attentional guidance. With the\nbenefit of guided training, CTC model achieves robust and accurate prediction\nfor both regular and irregular scene text while maintaining a fast inference\nspeed. Moreover, to further leverage the potential of CTC decoder, a graph\nconvolutional network (GCN) is proposed to learn the local correlations of\nextracted features. Extensive experiments on standard benchmarks demonstrate\nthat our end-to-end model achieves a new state-of-the-art for regular and\nirregular scene text recognition and needs 6 times shorter inference time than\nattentionbased methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 13:26:14 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Hu", "Wenyang", ""], ["Cai", "Xiaocong", ""], ["Hou", "Jun", ""], ["Yi", "Shuai", ""], ["Lin", "Zhiping", ""]]}, {"id": "2002.01281", "submitter": "Cyprien Ruffino", "authors": "Cyprien Ruffino and Romain H\\'erault and Eric Laloy and Gilles Gasso", "title": "Pixel-wise Conditioned Generative Adversarial Networks for Image\n  Synthesis and Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have proven successful for\nunsupervised image generation. Several works have extended GANs to image\ninpainting by conditioning the generation with parts of the image to be\nreconstructed. Despite their success, these methods have limitations in\nsettings where only a small subset of the image pixels is known beforehand. In\nthis paper we investigate the effectiveness of conditioning GANs when very few\npixel values are provided. We propose a modelling framework which results in\nadding an explicit cost term to the GAN objective function to enforce\npixel-wise conditioning. We investigate the influence of this regularization\nterm on the quality of the generated images and the fulfillment of the given\npixel constraints. Using the recent PacGAN technique, we ensure that we keep\ndiversity in the generated samples. Conducted experiments on FashionMNIST show\nthat the regularization term effectively controls the trade-off between quality\nof the generated images and the conditioning. Experimental evaluation on the\nCIFAR-10 and CelebA datasets evidences that our method achieves accurate\nresults both visually and quantitatively in term of Fr\\'echet Inception\nDistance, while still enforcing the pixel conditioning. We also evaluate our\nmethod on a texture image generation task using fully-convolutional networks.\nAs a final contribution, we apply the method to a classical geological\nsimulation application.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 13:49:15 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Ruffino", "Cyprien", ""], ["H\u00e9rault", "Romain", ""], ["Laloy", "Eric", ""], ["Gasso", "Gilles", ""]]}, {"id": "2002.01284", "submitter": "Mario Alberto Gutierrez", "authors": "Mario A. Gutierrez-Mondragon, Dario Garcia-Gasulla, Sergio\n  Alvarez-Napagao, Jaume Brossa-Ordo\\~nez and Rafael Gimenez-Esteban", "title": "Obstruction level detection of sewer videos using convolutional neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worldwide, sewer networks are designed to transport wastewater to a\ncentralized treatment plant to be treated and returned to the environment. This\nprocess is critical for the current society, preventing waterborne illnesses,\nproviding safe drinking water and enhancing general sanitation. To keep a sewer\nnetwork perfectly operational, sampling inspections are performed constantly to\nidentify obstructions. Typically, a Closed-Circuit Television system is used to\nrecord the inside of pipes and report the obstruction level, which may trigger\na cleaning operative. Currently, the obstruction level assessment is done\nmanually, which is time-consuming and inconsistent. In this work, we design a\nmethodology to train a Convolutional Neural Network for identifying the level\nof obstruction in pipes, thus reducing the human effort required on such a\nfrequent and repetitive task. We gathered a database of videos that are\nexplored and adapted to generate useful frames to fed into the model. Our\nresulting classifier obtains deployment ready performances. To validate the\nconsistency of the approach and its industrial applicability, we integrate the\nLayer-wise Relevance Propagation explainability technique, which enables us to\nfurther understand the behavior of the neural network for this task. In the\nend, the proposed system can provide higher speed, accuracy, and consistency in\nthe process of sewer examination. Our analysis also uncovers some guidelines on\nhow to further improve the quality of the data gathering methodology.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 13:52:02 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Gutierrez-Mondragon", "Mario A.", ""], ["Garcia-Gasulla", "Dario", ""], ["Alvarez-Napagao", "Sergio", ""], ["Brossa-Ordo\u00f1ez", "Jaume", ""], ["Gimenez-Esteban", "Rafael", ""]]}, {"id": "2002.01325", "submitter": "Jae-Hyun Park", "authors": "Jae-Hyun Park, Woo-Jeoung Nam, Seong-Whan Lee", "title": "A Two-Stream Symmetric Network with Bidirectional Ensemble for Aerial\n  Image Matching", "comments": "20pages", "journal-ref": "Remote Sens. 12(3) (2020) 465-484", "doi": "10.3390/rs12030465", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method to precisely match two aerial images\nthat were obtained in different environments via a two-stream deep network. By\ninternally augmenting the target image, the network considers the two-stream\nwith the three input images and reflects the additional augmented pair in the\ntraining. As a result, the training process of the deep network is regularized\nand the network becomes robust for the variance of aerial images. Furthermore,\nwe introduce an ensemble method that is based on the bidirectional network,\nwhich is motivated by the isomorphic nature of the geometric transformation. We\nobtain two global transformation parameters without any additional network or\nparameters, which alleviate asymmetric matching results and enable significant\nimprovement in performance by fusing two outcomes. For the experiment, we adopt\naerial images from Google Earth and the International Society for\nPhotogrammetry and Remote Sensing (ISPRS). To quantitatively assess our result,\nwe apply the probability of correct keypoints (PCK) metric, which measures the\ndegree of matching. The qualitative and quantitative results show the sizable\ngap of performance compared to the conventional methods for matching the aerial\nimages. All code and our trained model, as well as the dataset are available\nonline.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 14:38:18 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Park", "Jae-Hyun", ""], ["Nam", "Woo-Jeoung", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2002.01368", "submitter": "Emile Engelbrecht Mr.", "authors": "Emile R. Engelbrecht, Johan A. du Preez", "title": "Semi-supervised learning with an open augmenting unknown class for\n  cost-effective training and reliable classifications", "comments": "9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to (a) train off partially labelled datasets and (b) ensure\nresulting networks separate data outside the domain of interest hugely expands\nthe practical and cost-effective applicability of neural network classifiers.\nWe design a classifier based off generative adversarial networks (GANs) that\ntrains off a practical and cost-saving semi-supervised criteria which,\nspecifically, allows novel classes within the unlabelled training set.\nFurthermore, we ensure the resulting classifier is capable of absolute novel\nclass detection, be these from the semi-supervised unlabelled training set or a\nso-called open set. Results are both state-of-the-art and a first of its kind.\nWe argue this technique greatly decreases training cost in respect to labelling\nwhile greatly improving the reliability of classifications.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 15:32:23 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 11:39:19 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 09:31:35 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Engelbrecht", "Emile R.", ""], ["Preez", "Johan A. du", ""]]}, {"id": "2002.01379", "submitter": "Bogdan Bugaev", "authors": "Bogdan Bugaev, Anton Kryshchenko, Roman Belov", "title": "Combining 3D Model Contour Energy and Keypoints for Object Tracking", "comments": "ECCV 2018 camera ready", "journal-ref": null, "doi": "10.1007/978-3-030-01258-8_4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new combined approach for monocular model-based 3D tracking. A\npreliminary object pose is estimated by using a keypoint-based technique. The\npose is then refined by optimizing the contour energy function. The energy\ndetermines the degree of correspondence between the contour of the model\nprojection and the image edges. It is calculated based on both the intensity\nand orientation of the raw image gradient. For optimization, we propose a\ntechnique and search area constraints that allow overcoming the local optima\nand taking into account information obtained through keypoint-based pose\nestimation. Owing to its combined nature, our method eliminates numerous issues\nof keypoint-based and edge-based approaches. We demonstrate the efficiency of\nour method by comparing it with state-of-the-art methods on a public benchmark\ndataset that includes videos with various lighting conditions, movement\npatterns, and speed.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 15:53:26 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Bugaev", "Bogdan", ""], ["Kryshchenko", "Anton", ""], ["Belov", "Roman", ""]]}, {"id": "2002.01449", "submitter": "Maheen Rashid", "authors": "Maheen Rashid, Hedvig Kjellstr\\\"om, Yong Jae Lee", "title": "Action Graphs: Weakly-supervised Action Localization with Graph\n  Convolution Networks", "comments": "Accepted at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for weakly-supervised action localization based on graph\nconvolutions. In order to find and classify video time segments that correspond\nto relevant action classes, a system must be able to both identify\ndiscriminative time segments in each video, and identify the full extent of\neach action. Achieving this with weak video level labels requires the system to\nuse similarity and dissimilarity between moments across videos in the training\ndata to understand both how an action appears, as well as the sub-actions that\ncomprise the action's full extent. However, current methods do not make\nexplicit use of similarity between video moments to inform the localization and\nclassification predictions. We present a novel method that uses graph\nconvolutions to explicitly model similarity between video moments. Our method\nutilizes similarity graphs that encode appearance and motion, and pushes the\nstate of the art on THUMOS '14, ActivityNet 1.2, and Charades for weakly\nsupervised action localization.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 18:21:10 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Rashid", "Maheen", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Lee", "Yong Jae", ""]]}, {"id": "2002.01461", "submitter": "Peng Sun", "authors": "Peng Sun, Rui Hou, Jerome Lynch", "title": "Measuring the Utilization of Public Open Spaces by Deep Learning: a\n  Benchmark Study at the Detroit Riverfront", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical activities and social interactions are essential activities that\nensure a healthy lifestyle. Public open spaces (POS), such as parks, plazas and\ngreenways, are key environments that encourage those activities. To evaluate a\nPOS, there is a need to study how humans use the facilities within it. However,\ntraditional approaches to studying use of POS are manual and therefore time and\nlabor intensive. They also may only provide qualitative insights. It is\nappealing to make use of surveillance cameras and to extract user-related\ninformation through computer vision. This paper proposes a proof-of-concept\ndeep learning computer vision framework for measuring human activities\nquantitatively in POS and demonstrates a case study of the proposed framework\nusing the Detroit Riverfront Conservancy (DRFC) surveillance camera network. A\ncustom image dataset is presented to train the framework; the dataset includes\n7826 fully annotated images collected from 18 cameras across the DRFC park\nspace under various illumination conditions. Dataset analysis is also provided\nas well as a baseline model for one-step user localization and activity\nrecognition. The mAP results are 77.5\\% for {\\it pedestrian} detection and\n81.6\\% for {\\it cyclist} detection. Behavioral maps are autonomously generated\nby the framework to locate different POS users and the average error for\nbehavioral localization is within 10 cm.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 18:38:19 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Sun", "Peng", ""], ["Hou", "Rui", ""], ["Lynch", "Jerome", ""]]}, {"id": "2002.01464", "submitter": "Jiayuan Mao", "authors": "Chi Han, Jiayuan Mao, Chuang Gan, Joshua B. Tenenbaum, Jiajun Wu", "title": "Visual Concept-Metaconcept Learning", "comments": "NeurIPS 2019. First two authors contributed equally. Project page:\n  http://vcml.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans reason with concepts and metaconcepts: we recognize red and green from\nvisual input; we also understand that they describe the same property of\nobjects (i.e., the color). In this paper, we propose the visual\nconcept-metaconcept learner (VCML) for joint learning of concepts and\nmetaconcepts from images and associated question-answer pairs. The key is to\nexploit the bidirectional connection between visual concepts and metaconcepts.\nVisual representations provide grounding cues for predicting relations between\nunseen pairs of concepts. Knowing that red and green describe the same property\nof objects, we generalize to the fact that cube and sphere also describe the\nsame property of objects, since they both categorize the shape of objects.\nMeanwhile, knowledge about metaconcepts empowers visual concept learning from\nlimited, noisy, and even biased data. From just a few examples of purple cubes\nwe can understand a new color purple, which resembles the hue of the cubes\ninstead of the shape of them. Evaluation on both synthetic and real-world\ndatasets validates our claims.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 18:42:30 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Han", "Chi", ""], ["Mao", "Jiayuan", ""], ["Gan", "Chuang", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "2002.01469", "submitter": "Sohrab Ferdowsi", "authors": "Sohrab Ferdowsi, Behrooz Razeghi, Taras Holotyak, Flavio P. Calmon,\n  Slava Voloshynovskiy", "title": "Privacy-Preserving Image Sharing via Sparsifying Layers on Convolutional\n  Groups", "comments": "Accepted as an oral presentation for ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a practical framework to address the problem of privacy-aware\nimage sharing in large-scale setups. We argue that, while compactness is always\ndesired at scale, this need is more severe when trying to furthermore protect\nthe privacy-sensitive content. We therefore encode images, such that, from one\nhand, representations are stored in the public domain without paying the huge\ncost of privacy protection, but ambiguated and hence leaking no discernible\ncontent from the images, unless a combinatorially-expensive guessing mechanism\nis available for the attacker. From the other hand, authorized users are\nprovided with very compact keys that can easily be kept secure. This can be\nused to disambiguate and reconstruct faithfully the corresponding\naccess-granted images. We achieve this with a convolutional autoencoder of our\ndesign, where feature maps are passed independently through sparsifying\ntransformations, providing multiple compact codes, each responsible for\nreconstructing different attributes of the image. The framework is tested on a\nlarge-scale database of images with public implementation available.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 18:54:52 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Ferdowsi", "Sohrab", ""], ["Razeghi", "Behrooz", ""], ["Holotyak", "Taras", ""], ["Calmon", "Flavio P.", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "2002.01599", "submitter": "Zohar Nussinov", "authors": "Brendon Lutnick, Wen Dong, Zohar Nussinov, and Pinaki Sarder", "title": "Unsupervised Community Detection with a Potts Model Hamiltonian, an\n  Efficient Algorithmic Solution, and Applications in Digital Pathology", "comments": "46 pages, 19 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised segmentation of large images using a Potts model Hamiltonian is\nunique in that segmentation is governed by a resolution parameter which scales\nthe sensitivity to small clusters. Here, the input image is first modeled as a\ngraph, which is then segmented by minimizing a Hamiltonian cost function\ndefined on the graph and the respective segments. However, there exists no\nclosed form solution of this optimization, and using previous iterative\nalgorithmic solution techniques, the problem scales quadratically in the Input\nLength. Therefore, while Potts model segmentation gives accurate segmentation,\nit is grossly underutilized as an unsupervised learning technique. We propose a\nfast statistical down-sampling of input image pixels based on the respective\ncolor features, and a new iterative method to minimize the Potts model energy\nconsidering pixel to segment relationship. This method is generalizable and can\nbe extended for image pixel texture features as well as spatial features. We\ndemonstrate that this new method is highly efficient, and outperforms existing\nmethods for Potts model based image segmentation. We demonstrate the\napplication of our method in medical microscopy image segmentation;\nparticularly, in segmenting renal glomerular micro-environment in renal\npathology. Our method is not limited to image segmentation, and can be extended\nto any image/data segmentation/clustering task for arbitrary datasets with\ndiscrete features.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 01:20:28 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Lutnick", "Brendon", ""], ["Dong", "Wen", ""], ["Nussinov", "Zohar", ""], ["Sarder", "Pinaki", ""]]}, {"id": "2002.01607", "submitter": "Chengwei Chen", "authors": "Chengwei Chen and Pan Chen and Haichuan Song and Yiqing Tao and Yuan\n  Xie and Shouhong Ding and Lizhuang Ma", "title": "Anomaly Detection by One Class Latent Regularized Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is a fundamental problem in computer vision area with many\nreal-world applications. Given a wide range of images belonging to the normal\nclass, emerging from some distribution, the objective of this task is to\nconstruct the model to detect out-of-distribution images belonging to abnormal\ninstances. Semi-supervised Generative Adversarial Networks (GAN)-based methods\nhave been gaining popularity in anomaly detection task recently. However, the\ntraining process of GAN is still unstable and challenging. To solve these\nissues, a novel adversarial dual autoencoder network is proposed, in which the\nunderlying structure of training data is not only captured in latent feature\nspace, but also can be further restricted in the space of latent representation\nin a discriminant manner, leading to a more accurate detector. In addition, the\nauxiliary autoencoder regarded as a discriminator could obtain an more stable\ntraining process. Experiments show that our model achieves the state-of-the-art\nresults on MNIST and CIFAR10 datasets as well as GTSRB stop signs dataset.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 02:21:52 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 06:30:49 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Chen", "Chengwei", ""], ["Chen", "Pan", ""], ["Song", "Haichuan", ""], ["Tao", "Yiqing", ""], ["Xie", "Yuan", ""], ["Ding", "Shouhong", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2002.01609", "submitter": "Byungseok Roh", "authors": "Byungseok Roh, Han-Cheol Cho, Myung-Ho Ju, Soon Hyung Pyo", "title": "BABO: Background Activation Black-Out for Efficient Object Detection", "comments": "14 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have enabled complex real-world use cases\ncomprised of multiple vision tasks and detection tasks are being shifted to the\nedge side as a pre-processing step of the entire workload. Since running a deep\nmodel on resource-constraint devices is challenging, techniques for efficient\ninference methods are demanded. In this paper, we present an objectness-aware\nobject detection method to reduce computational cost by sparsifying activation\nvalues on background regions where target objects don't exist. Sparsified\nactivation can be exploited to increase inference speed by software or hardware\naccelerated sparse convolution techniques. To accomplish this goal, we\nincorporate a light-weight objectness mask generation (OMG) network in front of\nan object detection (OD) network so that it can zero out unnecessary background\nareas of an input image before being fed into the OD network. In experiments,\nby switching background activation values to zero, the average number of zero\nvalues increases further from 36% to 68% on MobileNetV2-SSDLite even with ReLU\nactivation while maintaining accuracy on MS-COCO. This result indicates that\nthe total MAC including both OMG and OD networks can be reduced to 62% of the\noriginal OD model when only non-zero multiply-accumulate operations are\nconsidered. Moreover, we show a similar tendency in heavy networks (VGG and\nRetinaNet) and an additional dataset (PASCAL VOC).\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 02:25:08 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 12:03:31 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Roh", "Byungseok", ""], ["Cho", "Han-Cheol", ""], ["Ju", "Myung-Ho", ""], ["Pyo", "Soon Hyung", ""]]}, {"id": "2002.01612", "submitter": "Kumar Ayush", "authors": "Kumar Ayush, Burak Uzkent, Marshall Burke, David Lobell, Stefano Ermon", "title": "Generating Interpretable Poverty Maps using Object Detection in\n  Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate local-level poverty measurement is an essential task for governments\nand humanitarian organizations to track the progress towards improving\nlivelihoods and distribute scarce resources. Recent computer vision advances in\nusing satellite imagery to predict poverty have shown increasing accuracy, but\nthey do not generate features that are interpretable to policymakers,\ninhibiting adoption by practitioners. Here we demonstrate an interpretable\ncomputational framework to accurately predict poverty at a local level by\napplying object detectors to high resolution (30cm) satellite images. Using the\nweighted counts of objects as features, we achieve 0.539 Pearson's r^2 in\npredicting village-level poverty in Uganda, a 31% improvement over existing\n(and less interpretable) benchmarks. Feature importance and ablation analysis\nreveal intuitive relationships between object counts and poverty predictions.\nOur results suggest that interpretability does not have to come at the cost of\nperformance, at least in this important domain.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 02:50:01 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 02:02:57 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Ayush", "Kumar", ""], ["Uzkent", "Burak", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "2002.01618", "submitter": "Jonggi Hong", "authors": "Jonggi Hong, Kyungjun Lee, June Xu, Hernisa Kacorri", "title": "Crowdsourcing the Perception of Machine Teaching", "comments": "10 pages, 8 figures, 5 tables, CHI2020 conference", "journal-ref": "Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3313831.3376428", "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teachable interfaces can empower end-users to attune machine learning systems\nto their idiosyncratic characteristics and environment by explicitly providing\npertinent training examples. While facilitating control, their effectiveness\ncan be hindered by the lack of expertise or misconceptions. We investigate how\nusers may conceptualize, experience, and reflect on their engagement in machine\nteaching by deploying a mobile teachable testbed in Amazon Mechanical Turk.\nUsing a performance-based payment scheme, Mechanical Turkers (N = 100) are\ncalled to train, test, and re-train a robust recognition model in real-time\nwith a few snapshots taken in their environment. We find that participants\nincorporate diversity in their examples drawing from parallels to how humans\nrecognize objects independent of size, viewpoint, location, and illumination.\nMany of their misconceptions relate to consistency and model capabilities for\nreasoning. With limited variation and edge cases in testing, the majority of\nthem do not change strategies on a second training attempt.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 03:20:25 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Hong", "Jonggi", ""], ["Lee", "Kyungjun", ""], ["Xu", "June", ""], ["Kacorri", "Hernisa", ""]]}, {"id": "2002.01619", "submitter": "Yingjie Cai", "authors": "Yingjie Cai, Buyu Li, Zeyu Jiao, Hongsheng Li, Xingyu Zeng, Xiaogang\n  Wang", "title": "Monocular 3D Object Detection with Decoupled Structured Polygon\n  Estimation and Height-Guided Depth Estimation", "comments": "11 pages, 8 figures, AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D object detection task aims to predict the 3D bounding boxes of\nobjects based on monocular RGB images. Since the location recovery in 3D space\nis quite difficult on account of absence of depth information, this paper\nproposes a novel unified framework which decomposes the detection problem into\na structured polygon prediction task and a depth recovery task. Different from\nthe widely studied 2D bounding boxes, the proposed novel structured polygon in\nthe 2D image consists of several projected surfaces of the target object.\nCompared to the widely-used 3D bounding box proposals, it is shown to be a\nbetter representation for 3D detection. In order to inversely project the\npredicted 2D structured polygon to a cuboid in the 3D physical world, the\nfollowing depth recovery task uses the object height prior to complete the\ninverse projection transformation with the given camera projection matrix.\nMoreover, a fine-grained 3D box refinement scheme is proposed to further\nrectify the 3D detection results. Experiments are conducted on the challenging\nKITTI benchmark, in which our method achieves state-of-the-art detection\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 03:25:02 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 04:20:56 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cai", "Yingjie", ""], ["Li", "Buyu", ""], ["Jiao", "Zeyu", ""], ["Li", "Hongsheng", ""], ["Zeng", "Xingyu", ""], ["Wang", "Xiaogang", ""]]}, {"id": "2002.01625", "submitter": "Ziyue Zhang", "authors": "Ziyue Zhang, Richard YD Xu, Shuai Jiang, Yang Li, Congzhentao Huang,\n  Chen Deng", "title": "Illumination adaptive person reid based on teacher-student model and\n  adversarial training", "comments": "Accepted by ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing works in Person Re-identification (ReID) focus on settings\nwhere illumination either is kept the same or has very little fluctuation.\nHowever, the changes in the illumination degree may affect the robustness of a\nReID algorithm significantly. To address this problem, we proposed a Two-Stream\nNetwork that can separate ReID features from lighting features to enhance ReID\nperformance. Its innovations are threefold: (1) A discriminative entropy loss\nto ensure the ReID features contain no lighting information. (2) A ReID Teacher\nmodel trained by images under \"neutral\" lighting conditions to guide ReID\nclassification. (3) An illumination Teacher model trained by the differences\nbetween the illumination-adjusted and original images to guide illumination\nclassification. We construct two augmented datasets by synthetically changing a\nset of predefined lighting conditions in two of the most popular ReID\nbenchmarks: Market1501 and DukeMTMC-ReID. Experiments demonstrate that our\nalgorithm outperforms other state-of-the-art works and particularly potent in\nhandling images under extremely low light.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 03:49:10 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 07:32:22 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 10:20:21 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Zhang", "Ziyue", ""], ["Xu", "Richard YD", ""], ["Jiang", "Shuai", ""], ["Li", "Yang", ""], ["Huang", "Congzhentao", ""], ["Deng", "Chen", ""]]}, {"id": "2002.01642", "submitter": "Osman Tursun", "authors": "Osman Tursun, Simon Denman, Sridha Sridharan and Clinton Fookes", "title": "Learning Test-time Data Augmentation for Image Retrieval with\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-the-shelf convolutional neural network features achieve outstanding\nresults in many image retrieval tasks. However, their invariance is pre-defined\nby the network architecture and training data. Existing image retrieval\napproaches require fine-tuning or modification of the pre-trained networks to\nadapt to the variations in the target data. In contrast, our method enhances\nthe invariance of off-the-shelf features by aggregating features extracted from\nimages augmented with learned test-time augmentations. The optimal ensemble of\ntest-time augmentations is learned automatically through reinforcement\nlearning. Our training is time and resources efficient, and learns a diverse\ntest-time augmentations. Experiment results on trademark retrieval (METU\ntrademark dataset) and landmark retrieval (Oxford5k and Paris6k scene datasets)\ntasks show the learned ensemble of transformations is effective and\ntransferable. We also achieve state-of-the-art MAP@100 results on the METU\ntrademark dataset.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 05:08:41 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 07:54:06 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 06:32:56 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Tursun", "Osman", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2002.01646", "submitter": "Tao Zhuo", "authors": "Tao Zhuo and Mohan Kankanhalli", "title": "Solving Raven's Progressive Matrices with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raven's Progressive Matrices (RPM) have been widely used for Intelligence\nQuotient (IQ) test of humans. In this paper, we aim to solve RPM with neural\nnetworks in both supervised and unsupervised manners. First, we investigate\nstrategies to reduce over-fitting in supervised learning. We suggest the use of\na neural network with deep layers and pre-training on large-scale datasets to\nimprove model generalization. Experiments on the RAVEN dataset show that the\noverall accuracy of our supervised approach surpasses human-level performance.\nSecond, as an intelligent agent requires to automatically learn new skills to\nsolve new problems, we propose the first unsupervised method, Multilabel\nClassification with Pseudo Target (MCPT), for RPM problems. Based on the design\nof the pseudo target, MCPT converts the unsupervised learning problem to a\nsupervised task. Experiments show that MCPT doubles the testing accuracy of\nrandom guessing e.g. 28.50% vs. 12.5%. Finally, we discuss the problem of\nsolving RPM with unsupervised and explainable strategies in the future.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 05:18:02 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 13:58:52 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Zhuo", "Tao", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "2002.01650", "submitter": "Zhi Chen", "authors": "Zhi Chen, Yijie Bei and Cynthia Rudin", "title": "Concept Whitening for Interpretable Image Recognition", "comments": "Authors' pre-publication version of a 2020 Nature Machine\n  Intelligence article", "journal-ref": "Nature Machine Intelligence, Vol 2, Dec 2020, 772-782", "doi": "10.1038/s42256-020-00265-z", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does a neural network encode about a concept as we traverse through the\nlayers? Interpretability in machine learning is undoubtedly important, but the\ncalculations of neural networks are very challenging to understand. Attempts to\nsee inside their hidden layers can either be misleading, unusable, or rely on\nthe latent space to possess properties that it may not have. In this work,\nrather than attempting to analyze a neural network posthoc, we introduce a\nmechanism, called concept whitening (CW), to alter a given layer of the network\nto allow us to better understand the computation leading up to that layer. When\na concept whitening module is added to a CNN, the axes of the latent space are\naligned with known concepts of interest. By experiment, we show that CW can\nprovide us a much clearer understanding for how the network gradually learns\nconcepts over layers. CW is an alternative to a batch normalization layer in\nthat it normalizes, and also decorrelates (whitens) the latent space. CW can be\nused in any layer of the network without hurting predictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 05:28:09 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 16:55:49 GMT"}, {"version": "v3", "created": "Sat, 3 Oct 2020 05:06:19 GMT"}, {"version": "v4", "created": "Mon, 19 Oct 2020 14:13:19 GMT"}, {"version": "v5", "created": "Mon, 7 Dec 2020 19:09:35 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Chen", "Zhi", ""], ["Bei", "Yijie", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2002.01660", "submitter": "Xinrui Cui", "authors": "Dan Wang, Xinrui Cui, and Z. Jane Wang", "title": "CHAIN: Concept-harmonized Hierarchical Inference Interpretation of Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the great success of networks, it witnesses the increasing demand for\nthe interpretation of the internal network mechanism, especially for the net\ndecision-making logic. To tackle the challenge, the Concept-harmonized\nHierArchical INference (CHAIN) is proposed to interpret the net decision-making\nprocess. For net-decisions being interpreted, the proposed method presents the\nCHAIN interpretation in which the net decision can be hierarchically deduced\ninto visual concepts from high to low semantic levels. To achieve it, we\npropose three models sequentially, i.e., the concept harmonizing model, the\nhierarchical inference model, and the concept-harmonized hierarchical inference\nmodel. Firstly, in the concept harmonizing model, visual concepts from high to\nlow semantic-levels are aligned with net-units from deep to shallow layers.\nSecondly, in the hierarchical inference model, the concept in a deep layer is\ndisassembled into units in shallow layers. Finally, in the concept-harmonized\nhierarchical inference model, a deep-layer concept is inferred from its\nshallow-layer concepts. After several rounds, the concept-harmonized\nhierarchical inference is conducted backward from the highest semantic level to\nthe lowest semantic level. Finally, net decision-making is explained as a form\nof concept-harmonized hierarchical inference, which is comparable to human\ndecision-making. Meanwhile, the net layer structure for feature learning can be\nexplained based on the hierarchical visual concepts. In quantitative and\nqualitative experiments, we demonstrate the effectiveness of CHAIN at the\ninstance and class levels.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 06:45:23 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Wang", "Dan", ""], ["Cui", "Xinrui", ""], ["Wang", "Z. Jane", ""]]}, {"id": "2002.01690", "submitter": "Xiaofu Wu Dr", "authors": "Xiaofu Wu, Suofei hang, Quan Zhou, Zhen Yang, Chunming Zhao, Longin\n  Jan Latecki", "title": "Entropy Minimization vs. Diversity Maximization for Domain Adaptation", "comments": "submitted to IEEE T-IP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy minimization has been widely used in unsupervised domain adaptation\n(UDA). However, existing works reveal that entropy minimization only may result\ninto collapsed trivial solutions. In this paper, we propose to avoid trivial\nsolutions by further introducing diversity maximization. In order to achieve\nthe possible minimum target risk for UDA, we show that diversity maximization\nshould be elaborately balanced with entropy minimization, the degree of which\ncan be finely controlled with the use of deep embedded validation in an\nunsupervised manner. The proposed minimal-entropy diversity maximization (MEDM)\ncan be directly implemented by stochastic gradient descent without use of\nadversarial learning. Empirical evidence demonstrates that MEDM outperforms the\nstate-of-the-art methods on four popular domain adaptation datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 09:13:19 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Wu", "Xiaofu", ""], ["hang", "Suofei", ""], ["Zhou", "Quan", ""], ["Yang", "Zhen", ""], ["Zhao", "Chunming", ""], ["Latecki", "Longin Jan", ""]]}, {"id": "2002.01708", "submitter": "Nico Lang", "authors": "Daniel Laumer, Nico Lang, Natalie van Doorn, Oisin Mac Aodha, Pietro\n  Perona, Jan Dirk Wegner", "title": "Geocoding of trees from street addresses and street-level images", "comments": "Accepted for publication in ISPRS Journal of Photogrammetry and\n  Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach for updating older tree inventories with geographic\ncoordinates using street-level panorama images and a global optimization\nframework for tree instance matching. Geolocations of trees in inventories\nuntil the early 2000s where recorded using street addresses whereas newer\ninventories use GPS. Our method retrofits older inventories with geographic\ncoordinates to allow connecting them with newer inventories to facilitate\nlong-term studies on tree mortality etc. What makes this problem challenging is\nthe different number of trees per street address, the heterogeneous appearance\nof different tree instances in the images, ambiguous tree positions if viewed\nfrom multiple images and occlusions. To solve this assignment problem, we (i)\ndetect trees in Google street-view panoramas using deep learning, (ii) combine\nmulti-view detections per tree into a single representation, (iii) and match\ndetected trees with given trees per street address with a global optimization\napproach. Experiments for > 50000 trees in 5 cities in California, USA, show\nthat we are able to assign geographic coordinates to 38 % of the street trees,\nwhich is a good starting point for long-term studies on the ecosystem services\nvalue of street trees at large scale.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 10:13:43 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Laumer", "Daniel", ""], ["Lang", "Nico", ""], ["van Doorn", "Natalie", ""], ["Mac Aodha", "Oisin", ""], ["Perona", "Pietro", ""], ["Wegner", "Jan Dirk", ""]]}, {"id": "2002.01775", "submitter": "Inseop Chung", "authors": "Inseop Chung, SeongUk Park, Jangho Kim, Nojun Kwak", "title": "Feature-map-level Online Adversarial Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature maps contain rich information about image intensity and spatial\ncorrelation. However, previous online knowledge distillation methods only\nutilize the class probabilities. Thus in this paper, we propose an online\nknowledge distillation method that transfers not only the knowledge of the\nclass probabilities but also that of the feature map using the adversarial\ntraining framework. We train multiple networks simultaneously by employing\ndiscriminators to distinguish the feature map distributions of different\nnetworks. Each network has its corresponding discriminator which discriminates\nthe feature map from its own as fake while classifying that of the other\nnetwork as real. By training a network to fool the corresponding discriminator,\nit can learn the other network's feature map distribution. We show that our\nmethod performs better than the conventional direct alignment method such as L1\nand is more suitable for online distillation. Also, we propose a novel cyclic\nlearning scheme for training more than two networks together. We have applied\nour method to various network architectures on the classification task and\ndiscovered a significant improvement of performance especially in the case of\ntraining a pair of a small network and a large one.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 13:16:37 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 17:58:38 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 18:15:40 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chung", "Inseop", ""], ["Park", "SeongUk", ""], ["Kim", "Jangho", ""], ["Kwak", "Nojun", ""]]}, {"id": "2002.01779", "submitter": "Amir Aly", "authors": "Amir Aly", "title": "Human Posture Recognition and Gesture Imitation with a Humanoid Robot", "comments": "University of Paris 6 (UPMC), University of Sorbonne, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes different approaches for static and dynamic gesture\nanalysis and imitation with the social robot Nao\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 13:26:05 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 15:13:52 GMT"}, {"version": "v3", "created": "Sat, 21 Mar 2020 08:56:43 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Aly", "Amir", ""]]}, {"id": "2002.01793", "submitter": "Yacov Hel-Or", "authors": "Inbal Lav, Shai Avidan, Yoram Singer, Yacov Hel-Or", "title": "Proximity Preserving Binary Code using Signed Graph-Cut", "comments": null, "journal-ref": "AAAI Conference on Artificial Intelligence , Feb. 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a binary embedding framework, called Proximity Preserving Code\n(PPC), which learns similarity and dissimilarity between data points to create\na compact and affinity-preserving binary code. This code can be used to apply\nfast and memory-efficient approximation to nearest-neighbor searches. Our\nframework is flexible, enabling different proximity definitions between data\npoints. In contrast to previous methods that extract binary codes based on\nunsigned graph partitioning, our system models the attractive and repulsive\nforces in the data by incorporating positive and negative graph weights. The\nproposed framework is shown to boil down to finding the minimal cut of a signed\ngraph, a problem known to be NP-hard. We offer an efficient approximation and\nachieve superior results by constructing the code bit after bit. We show that\nthe proposed approximation is superior to the commonly used spectral methods\nwith respect to both accuracy and complexity. Thus, it is useful for many other\nproblems that can be translated into signed graph cut.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 13:58:41 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Lav", "Inbal", ""], ["Avidan", "Shai", ""], ["Singer", "Yoram", ""], ["Hel-Or", "Yacov", ""]]}, {"id": "2002.01827", "submitter": "Yue Fan", "authors": "Yue Fan, Yongqin Xian, Max Maria Losch, Bernt Schiele", "title": "Analyzing the Dependency of ConvNets on Spatial Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intuitively, image classification should profit from using spatial\ninformation. Recent work, however, suggests that this might be overrated in\nstandard CNNs. In this paper, we are pushing the envelope and aim to further\ninvestigate the reliance on spatial information. We propose spatial shuffling\nand GAP+FC to destroy spatial information during both training and testing\nphases. Interestingly, we observe that spatial information can be deleted from\nlater layers with small performance drops, which indicates spatial information\nat later layers is not necessary for good performance. For example, test\naccuracy of VGG-16 only drops by 0.03% and 2.66% with spatial information\ncompletely removed from the last 30% and 53% layers on CIFAR100, respectively.\nEvaluation on several object recognition datasets (CIFAR100, Small-ImageNet,\nImageNet) with a wide range of CNN architectures (VGG16, ResNet50, ResNet152)\nshows an overall consistent pattern.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 15:22:32 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Fan", "Yue", ""], ["Xian", "Yongqin", ""], ["Losch", "Max Maria", ""], ["Schiele", "Bernt", ""]]}, {"id": "2002.01852", "submitter": "Biao Yang", "authors": "Biao Yang, Guocheng Yan, Pin Wang, Ching-yao Chan, Xiaofeng Liu, and\n  Yang Chen", "title": "TPPO: A Novel Trajectory Predictor with Pseudo Oracle", "comments": "12 pages, 6 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:2002.00391", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting pedestrian trajectories in dynamic scenes remains a critical\nproblem with various applications, such as autonomous driving and socially\naware robots. Such forecasting is challenging due to human-human and\nhuman-object interactions and future uncertainties caused by human randomness.\nGenerative model-based methods handle future uncertainties by sampling a latent\nvariable. However, few previous studies carefully explored the generation of\nthe latent variable. In this work, we propose the Trajectory Predictor with\nPseudo Oracle (TPPO), which is a generative model-based trajectory predictor.\nThe first pseudo oracle is pedestrians' moving directions, and the second one\nis the latent variable estimated from observed trajectories. A social attention\nmodule is used to aggregate neighbors' interactions on the basis of the\ncorrelation between pedestrians' moving directions and their future\ntrajectories. This correlation is inspired by the fact that a pedestrian's\nfuture trajectory is often influenced by pedestrians in front. A latent\nvariable predictor is proposed to estimate latent variable distributions from\nobserved and ground-truth trajectories. Moreover, the gap between these two\ndistributions is minimized during training. Therefore, the latent variable\npredictor can estimate the latent variable from observed trajectories to\napproximate that estimated from ground-truth trajectories. We compare the\nperformance of TPPO with related methods on several public datasets. Results\ndemonstrate that TPPO outperforms state-of-the-art methods with low average and\nfinal displacement errors. Besides, the ablation study shows that the\nprediction performance will not dramatically decrease as sampling times decline\nduring tests.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 03:28:55 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Yang", "Biao", ""], ["Yan", "Guocheng", ""], ["Wang", "Pin", ""], ["Chan", "Ching-yao", ""], ["Liu", "Xiaofeng", ""], ["Chen", "Yang", ""]]}, {"id": "2002.01891", "submitter": "Yasuhiko Tachibana", "authors": "Yasuhiko Tachibana, Masataka Nishimori, Naoyuki Kitamura, Kensuke\n  Umehara, Junko Ota, Takayuki Obata, and Tatsuya Higashi", "title": "A neural network model that learns differences in diagnosis strategies\n  among radiologists has an improved area under the curve for aneurysm status\n  classification in magnetic resonance angiography image series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To construct a neural network model that can learn the different\ndiagnosing strategies of radiologists to better classify aneurysm status in\nmagnetic resonance angiography images. Materials and methods: This\nretrospective study included 3423 time-of-flight brain magnetic resonance\nangiography image series (subjects: male 1843 [mean age, 50.2 +/- 11.7 years],\nfemale 1580 [50.8 +/- 11.3 years]) recorded from November 2017 through January\n2019. The image series were read independently for aneurysm status by one of\nfour board-certified radiologists, who were assisted by an established deep\nlearning-based computer-assisted diagnosis (CAD) system. The constructed neural\nnetworks were trained to classify the aneurysm status of zero to five\naneurysm-suspicious areas suggested by the CAD system for each image series,\nand any additional aneurysm areas added by the radiologists, and this\nclassification was compared with the judgment of the annotating radiologist.\nImage series were randomly allocated to training and testing data in an 8:2\nratio. The accuracy of the classification was compared by receiver operating\ncharacteristic analysis between the control model that accepted only image data\nas input and the proposed model that additionally accepted the information of\nwho the annotating radiologist was. The DeLong test was used to compare areas\nunder the curves (P < 0.05 was considered significant). Results: The area under\nthe curve was larger in the proposed model (0.845) than in the control model\n(0.793), and the difference was significant (P < 0.0001). Conclusion: The\nproposed model improved classification accuracy by learning the diagnosis\nstrategies of individual annotating radiologists.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 19:19:57 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Tachibana", "Yasuhiko", ""], ["Nishimori", "Masataka", ""], ["Kitamura", "Naoyuki", ""], ["Umehara", "Kensuke", ""], ["Ota", "Junko", ""], ["Obata", "Takayuki", ""], ["Higashi", "Tatsuya", ""]]}, {"id": "2002.01913", "submitter": "Augusto Luis Ballardini PhD", "authors": "Augusto Luis Ballardini, Daniele Cattaneo, Rub\\'en Izquierdo, Ignacio\n  Parra Alonso, Andrea Piazzoni, Miguel \\'Angel Sotelo, Domenico Giorgio\n  Sorrenti", "title": "Vehicle Ego-Lane Estimation with Sensor Failure Modeling", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a probabilistic ego-lane estimation algorithm for highway-like\nscenarios that is designed to increase the accuracy of the ego-lane estimate,\nwhich can be obtained relying only on a noisy line detector and tracker. The\ncontribution relies on a Hidden Markov Model (HMM) with a transient failure\nmodel. The proposed algorithm exploits the OpenStreetMap (or other cartographic\nservices) road property lane number as the expected number of lanes and\nleverages consecutive, possibly incomplete, observations. The algorithm\neffectiveness is proven by employing different line detectors and showing we\ncould achieve much more usable, i.e. stable and reliable, ego-lane estimates\nover more than 100 Km of highway scenarios, recorded both in Italy and Spain.\nMoreover, as we could not find a suitable dataset for a quantitative comparison\nwith other approaches, we collected datasets and manually annotated the Ground\nTruth about the vehicle ego-lane. Such datasets are made publicly available for\nusage from the scientific community.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 18:32:00 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 15:06:49 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Ballardini", "Augusto Luis", ""], ["Cattaneo", "Daniele", ""], ["Izquierdo", "Rub\u00e9n", ""], ["Alonso", "Ignacio Parra", ""], ["Piazzoni", "Andrea", ""], ["Sotelo", "Miguel \u00c1ngel", ""], ["Sorrenti", "Domenico Giorgio", ""]]}, {"id": "2002.01975", "submitter": "Shadrokh Samavi", "authors": "Zahra Sobhaninia, Safiyeh Rezaei, Nader Karimi, Ali Emami, Shadrokh\n  Samavi", "title": "Brain Tumor Segmentation by Cascaded Deep Neural Networks Using Multiple\n  Image Scales", "comments": "5 pages and 4 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intracranial tumors are groups of cells that usually grow uncontrollably. One\nout of four cancer deaths is due to brain tumors. Early detection and\nevaluation of brain tumors is an essential preventive medical step that is\nperformed by magnetic resonance imaging (MRI). Many segmentation techniques\nexist for this purpose. Low segmentation accuracy is the main drawback of\nexisting methods. In this paper, we use a deep learning method to boost the\naccuracy of tumor segmentation in MR images. Cascade approach is used with\nmultiple scales of images to induce both local and global views and help the\nnetwork to reach higher accuracies. Our experimental results show that using\nmultiple scales and the utilization of two cascade networks is advantageous.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 20:00:40 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Sobhaninia", "Zahra", ""], ["Rezaei", "Safiyeh", ""], ["Karimi", "Nader", ""], ["Emami", "Ali", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2002.01981", "submitter": "Arie Agranonik", "authors": "Arie Agranonik, Maya Herman, Mark Last", "title": "Parallel 3DPIFCM Algorithm for Noisy Brain MRI Images", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we implemented the algorithm we developed in [1] called 3DPIFCM\nin a parallel environment by using CUDA on a GPU. In our previous work we\nintroduced 3DPIFCM which performs segmentation of images in noisy conditions\nand uses particle swarm optimization for finding the optimal algorithm\nparameters to account for noise. This algorithm achieved state of the art\nsegmentation accuracy when compared to FCM (Fuzzy C-Means), IFCMPSO (Improved\nFuzzy C-Means with Particle Swarm Optimization), GAIFCM (Genetic Algorithm\nImproved Fuzzy C-Means) on noisy MRI images of an adult Brain.\n  When using a genetic algorithm or PSO (Particle Swarm Optimization) on a\nsingle machine for optimization we witnessed long execution times for practical\nclinical usage. Therefore, in the current paper our goal was to speed up the\nexecution of 3DPIFCM by taking out parts of the algorithm and executing them as\nkernels on a GPU. The algorithm was implemented using the CUDA [13] framework\nfrom NVIDIA and experiments where performed on a server containing 64GB RAM , 8\ncores and a TITAN X GPU with 3072 SP cores and 12GB of GPU memory.\n  Our results show that the parallel version of the algorithm performs up to\n27x faster than the original sequential version and 68x faster than GAIFCM\nalgorithm. We show that the speedup of the parallel version increases as we\nincrease the size of the image due to better utilization of cores in the GPU.\nAlso, we show a speedup of up to 5x in our Brainweb experiment compared to\nother generic variants such as IFCMPSO and GAIFCM.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 20:30:29 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Agranonik", "Arie", ""], ["Herman", "Maya", ""], ["Last", "Mark", ""]]}, {"id": "2002.01985", "submitter": "Arie Agranonik", "authors": "Arie Agranonik, Maya Herman, Mark Last", "title": "3DPIFCM Novel Algorithm for Segmentation of Noisy Brain MRI Images", "comments": "16 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm named 3DPIFCM, for automatic segmentation of\nnoisy MRI Brain images. The algorithm is an extension of a well-known IFCM\n(Improved Fuzzy C-Means) algorithm. It performs fuzzy segmentation and\nintroduces a fitness function that is affected by proximity of the voxels and\nby the color intensity in 3D images. The 3DPIFCM algorithm uses PSO (Particle\nSwarm Optimization) in order to optimize the fitness function. In addition, the\n3DPIFCM uses 3D features of near voxels to better adjust the noisy artifacts.\nIn our experiments, we evaluate 3DPIFCM on T1 Brainweb dataset with noise\nlevels ranging from 1% to 20% and on a synthetic dataset with ground truth both\nin 3D. The analysis of the segmentation results shows a significant improvement\nin the segmentation quality of up to 28% compared to two generic variants in\nnoisy images and up to 60% when compared to the original FCM (Fuzzy C-Means).\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 20:48:51 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 19:22:59 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Agranonik", "Arie", ""], ["Herman", "Maya", ""], ["Last", "Mark", ""]]}, {"id": "2002.02033", "submitter": "Deying Kong", "authors": "Deying Kong, Haoyu Ma, Yifei Chen, Xiaohui Xie", "title": "Rotation-invariant Mixed Graphical Model Network for 2D Hand Pose\n  Estimation", "comments": "2020 IEEE Winter Conference on Applications of Computer Vision (WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new architecture named Rotation-invariant Mixed\nGraphical Model Network (R-MGMN) to solve the problem of 2D hand pose\nestimation from a monocular RGB image. By integrating a rotation net, the\nR-MGMN is invariant to rotations of the hand in the image. It also has a pool\nof graphical models, from which a combination of graphical models could be\nselected, conditioning on the input image. Belief propagation is performed on\neach graphical model separately, generating a set of marginal distributions,\nwhich are taken as the confidence maps of hand keypoint positions. Final\nconfidence maps are obtained by aggregating these confidence maps together. We\nevaluate the R-MGMN on two public hand pose datasets. Experiment results show\nour model outperforms the state-of-the-art algorithm which is widely used in 2D\nhand pose estimation by a noticeable margin.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 23:05:09 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Kong", "Deying", ""], ["Ma", "Haoyu", ""], ["Chen", "Yifei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "2002.02063", "submitter": "Qianwei Zhou", "authors": "Qianwei Zhou, Peng Tao, Xiaoxin Li, Shengyong Chen, Fan Zhang, Haigen\n  Hu", "title": "Residual-Recursion Autoencoder for Shape Illustration Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape illustration images (SIIs) are common and important in describing the\ncross-sections of industrial products. Same as MNIST, the handwritten digit\nimages, SIIs are gray or binary and containing shapes that are surrounded by\nlarge areas of blanks. In this work, Residual-Recursion Autoencoder (RRAE) has\nbeen proposed to extract low-dimensional features from SIIs while maintaining\nreconstruction accuracy as high as possible. RRAE will try to reconstruct the\noriginal image several times and recursively fill the latest residual image to\nthe reserved channel of the encoder's input before the next trial of\nreconstruction. As a kind of neural network training framework, RRAE can wrap\nover other autoencoders and increase their performance. From experiment\nresults, the reconstruction loss is decreased by 86.47% for convolutional\nautoencoder with high-resolution SIIs, 10.77% for variational autoencoder and\n8.06% for conditional variational autoencoder with MNIST.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 01:51:13 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Zhou", "Qianwei", ""], ["Tao", "Peng", ""], ["Li", "Xiaoxin", ""], ["Chen", "Shengyong", ""], ["Zhang", "Fan", ""], ["Hu", "Haigen", ""]]}, {"id": "2002.02077", "submitter": "Akshay Rangesh", "authors": "Akshay Rangesh, Bowen Zhang and Mohan M. Trivedi", "title": "Gaze Preserving CycleGANs for Eyeglass Removal & Persistent Gaze\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A driver's gaze is critical for determining their attention, state,\nsituational awareness, and readiness to take over control from partially\nautomated vehicles. Estimating the gaze direction is the most obvious way to\ngauge a driver's state under ideal conditions when limited to using\nnon-intrusive imaging sensors. Unfortunately, the vehicular environment\nintroduces a variety of challenges that are usually unaccounted for - harsh\nillumination, nighttime conditions, and reflective eyeglasses. Relying on head\npose alone under such conditions can prove to be unreliable and erroneous. In\nthis study, we offer solutions to address these problems encountered in the\nreal world. To solve issues with lighting, we demonstrate that using an\ninfrared camera with suitable equalization and normalization suffices. To\nhandle eyeglasses and their corresponding artifacts, we adopt image-to-image\ntranslation using generative adversarial networks to pre-process images prior\nto gaze estimation. Our proposed Gaze Preserving CycleGAN (GPCycleGAN) is\ntrained to preserve the driver's gaze while removing potential eyeglasses from\nface images. GPCycleGAN is based on the well-known CycleGAN approach - with the\naddition of a gaze classifier and a gaze consistency loss for additional\nsupervision. Our approach exhibits improved performance, interpretability,\nrobustness and superior qualitative results on challenging real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 02:45:25 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 21:20:47 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 19:24:00 GMT"}, {"version": "v4", "created": "Fri, 9 Oct 2020 00:36:01 GMT"}, {"version": "v5", "created": "Thu, 29 Oct 2020 19:34:41 GMT"}, {"version": "v6", "created": "Tue, 15 Jun 2021 21:41:54 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Rangesh", "Akshay", ""], ["Zhang", "Bowen", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "2002.02079", "submitter": "Ruiting Shao", "authors": "Ruiting Shao and Edward J. Delp", "title": "Forensic Scanner Identification Using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing availability and functionality of image editing tools,\nmany forensic techniques such as digital image authentication, source\nidentification and tamper detection are important for forensic image analysis.\nIn this paper, we describe a machine learning based system to address the\nforensic analysis of scanner devices. The proposed system uses deep-learning to\nautomatically learn the intrinsic features from various scanned images. Our\nexperimental results show that high accuracy can be achieved for source scanner\nidentification. The proposed system can also generate a reliability map that\nindicates the manipulated regions in an scanned image.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 02:48:09 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Shao", "Ruiting", ""], ["Delp", "Edward J.", ""]]}, {"id": "2002.02100", "submitter": "S.H. Shabbeer Basha", "authors": "S.H. Shabbeer Basha, Viswanath Pulabaigari, Snehasis Mukherjee", "title": "An Information-rich Sampling Technique over Spatio-Temporal CNN for\n  Classification of Human Actions in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel scheme for human action recognition in videos, using a\n3-dimensional Convolutional Neural Network (3D CNN) based classifier.\nTraditionally in deep learning based human activity recognition approaches,\neither a few random frames or every $k^{th}$ frame of the video is considered\nfor training the 3D CNN, where $k$ is a small positive integer, like 4, 5, or\n6. This kind of sampling reduces the volume of the input data, which speeds-up\ntraining of the network and also avoids over-fitting to some extent, thus\nenhancing the performance of the 3D CNN model. In the proposed video sampling\ntechnique, consecutive $k$ frames of a video are aggregated into a single frame\nby computing a Gaussian-weighted summation of the $k$ frames. The resulting\nframe (aggregated frame) preserves the information in a better way than the\nconventional approaches and experimentally shown to perform better. In this\npaper, a 3D CNN architecture is proposed to extract the spatio-temporal\nfeatures and follows Long Short-Term Memory (LSTM) to recognize human actions.\nThe proposed 3D CNN architecture is capable of handling the videos where the\ncamera is placed at a distance from the performer. Experiments are performed\nwith KTH and WEIZMANN human actions datasets, whereby it is shown to produce\ncomparable results with the state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 05:07:41 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 06:42:20 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Basha", "S. H. Shabbeer", ""], ["Pulabaigari", "Viswanath", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "2002.02112", "submitter": "Hyungrok Ham", "authors": "Hyungrok Ham, Tae Joon Jun, Daeyoung Kim", "title": "Unbalanced GANs: Pre-training the Generator of Generative Adversarial\n  Network using Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Unbalanced GANs, which pre-trains the generator of the generative\nadversarial network (GAN) using variational autoencoder (VAE). We guarantee the\nstable training of the generator by preventing the faster convergence of the\ndiscriminator at early epochs. Furthermore, we balance between the generator\nand the discriminator at early epochs and thus maintain the stabilized training\nof GANs. We apply Unbalanced GANs to well known public datasets and find that\nUnbalanced GANs reduce mode collapses. We also show that Unbalanced GANs\noutperform ordinary GANs in terms of stabilized learning, faster convergence\nand better image quality at early epochs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 06:03:04 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Ham", "Hyungrok", ""], ["Jun", "Tae Joon", ""], ["Kim", "Daeyoung", ""]]}, {"id": "2002.02143", "submitter": "Minyoung Chung", "authors": "Minyoung Chung, Minkyung Lee, Jioh Hong, Sanguk Park, Jusang Lee,\n  Jingyu Lee, Jeongjin Lee, Yeong-Gil Shin", "title": "Pose-Aware Instance Segmentation Framework from Cone Beam CT Images for\n  Tooth Segmentation", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": "10.1016/j.compbiomed.2020.103720", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Individual tooth segmentation from cone beam computed tomography (CBCT)\nimages is an essential prerequisite for an anatomical understanding of\northodontic structures in several applications, such as tooth reformation\nplanning and implant guide simulations. However, the presence of severe metal\nartifacts in CBCT images hinders the accurate segmentation of each individual\ntooth. In this study, we propose a neural network for pixel-wise labeling to\nexploit an instance segmentation framework that is robust to metal artifacts.\nOur method comprises of three steps: 1) image cropping and realignment by pose\nregressions, 2) metal-robust individual tooth detection, and 3) segmentation.\nWe first extract the alignment information of the patient by pose regression\nneural networks to attain a volume-of-interest (VOI) region and realign the\ninput image, which reduces the inter-overlapping area between tooth bounding\nboxes. Then, individual tooth regions are localized within a VOI realigned\nimage using a convolutional detector. We improved the accuracy of the detector\nby employing non-maximum suppression and multiclass classification metrics in\nthe region proposal network. Finally, we apply a convolutional neural network\n(CNN) to perform individual tooth segmentation by converting the pixel-wise\nlabeling task to a distance regression task. Metal-intensive image augmentation\nis also employed for a robust segmentation of metal artifacts. The result shows\nthat our proposed method outperforms other state-of-the-art methods, especially\nfor teeth with metal artifacts. The primary significance of the proposed method\nis two-fold: 1) an introduction of pose-aware VOI realignment followed by a\nrobust tooth detection and 2) a metal-robust CNN framework for accurate tooth\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 07:57:34 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chung", "Minyoung", ""], ["Lee", "Minkyung", ""], ["Hong", "Jioh", ""], ["Park", "Sanguk", ""], ["Lee", "Jusang", ""], ["Lee", "Jingyu", ""], ["Lee", "Jeongjin", ""], ["Shin", "Yeong-Gil", ""]]}, {"id": "2002.02159", "submitter": "Daisuke Iwai", "authors": "Daiki Tone, Daisuke Iwai, Shinsaku Hiura, Kosuke Sato", "title": "FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers\n  in Dynamic Projection Mapping", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": "10.1109/TVCG.2020.2973444", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel active marker for dynamic projection mapping (PM)\nthat emits a temporal blinking pattern of infrared (IR) light representing its\nID. We used a multi-material three dimensional (3D) printer to fabricate a\nprojection object with optical fibers that can guide IR light from LEDs\nattached on the bottom of the object. The aperture of an optical fiber is\ntypically very small; thus, it is unnoticeable to human observers under\nprojection and can be placed on a strongly curved part of a projection surface.\nIn addition, the working range of our system can be larger than previous\nmarker-based methods as the blinking patterns can theoretically be recognized\nby a camera placed at a wide range of distances from markers. We propose an\nautomatic marker placement algorithm to spread multiple active markers over the\nsurface of a projection object such that its pose can be robustly estimated\nusing captured images from arbitrary directions. We also propose an\noptimization framework for determining the routes of the optical fibers in such\na way that collisions of the fibers can be avoided while minimizing the loss of\nlight intensity in the fibers. Through experiments conducted using three\nfabricated objects containing strongly curved surfaces, we confirmed that the\nproposed method can achieve accurate dynamic PMs in a significantly wide\nworking range.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 08:56:46 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tone", "Daiki", ""], ["Iwai", "Daisuke", ""], ["Hiura", "Shinsaku", ""], ["Sato", "Kosuke", ""]]}, {"id": "2002.02194", "submitter": "Ying Huang", "authors": "Yan Yan, Ying Huang, Si Chen, Chunhua Shen, Hanzi Wang", "title": "Joint Deep Learning of Facial Expression Synthesis and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning based facial expression recognition (FER) methods\nhave attracted considerable attention and they usually require large-scale\nlabelled training data. Nonetheless, the publicly available facial expression\ndatabases typically contain a small amount of labelled data. In this paper, to\novercome the above issue, we propose a novel joint deep learning of facial\nexpression synthesis and recognition method for effective FER. More\nspecifically, the proposed method involves a two-stage learning procedure.\nFirstly, a facial expression synthesis generative adversarial network (FESGAN)\nis pre-trained to generate facial images with different facial expressions. To\nincrease the diversity of the training images, FESGAN is elaborately designed\nto generate images with new identities from a prior distribution. Secondly, an\nexpression recognition network is jointly learned with the pre-trained FESGAN\nin a unified framework. In particular, the classification loss computed from\nthe recognition network is used to simultaneously optimize the performance of\nboth the recognition network and the generator of FESGAN. Moreover, in order to\nalleviate the problem of data bias between the real images and the synthetic\nimages, we propose an intra-class loss with a novel real data-guided\nback-propagation (RDBP) algorithm to reduce the intra-class variations of\nimages from the same class, which can significantly improve the final\nperformance. Extensive experimental results on public facial expression\ndatabases demonstrate the superiority of the proposed method compared with\nseveral state-of-the-art FER methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 10:56:00 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Yan", "Yan", ""], ["Huang", "Ying", ""], ["Chen", "Si", ""], ["Shen", "Chunhua", ""], ["Wang", "Hanzi", ""]]}, {"id": "2002.02200", "submitter": "Jean Lahoud", "authors": "Jean Lahoud, Bernard Ghanem", "title": "RGB-based Semantic Segmentation Using Self-Supervised Depth Pre-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although well-known large-scale datasets, such as ImageNet, have driven image\nunderstanding forward, most of these datasets require extensive manual\nannotation and are thus not easily scalable. This limits the advancement of\nimage understanding techniques. The impact of these large-scale datasets can be\nobserved in almost every vision task and technique in the form of pre-training\nfor initialization. In this work, we propose an easily scalable and\nself-supervised technique that can be used to pre-train any semantic RGB\nsegmentation method. In particular, our pre-training approach makes use of\nautomatically generated labels that can be obtained using depth sensors. These\nlabels, denoted by HN-labels, represent different height and normal patches,\nwhich allow mining of local semantic information that is useful in the task of\nsemantic RGB segmentation. We show how our proposed self-supervised\npre-training with HN-labels can be used to replace ImageNet pre-training, while\nusing 25x less images and without requiring any manual labeling. We pre-train a\nsemantic segmentation network with our HN-labels, which resembles our final\ntask more than pre-training on a less related task, e.g. classification with\nImageNet. We evaluate on two datasets (NYUv2 and CamVid), and we show how the\nsimilarity in tasks is advantageous not only in speeding up the pre-training\nprocess, but also in achieving better final semantic segmentation accuracy than\nImageNet pre-training\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 11:16:24 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Lahoud", "Jean", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2002.02210", "submitter": "Javier Del Ser Dr.", "authors": "Ibai Lana, Javier J. Sanchez-Medina, Eleni I. Vlahogianni, Javier Del\n  Ser", "title": "From Data to Actions in Intelligent Transportation Systems: a\n  Prescription of Functional Requirements for Model Actionability", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Data Science permeate every field of Transportation Science and\nEngineering, resulting in developments in the transportation sector that {are}\ndata-driven. Nowadays, Intelligent Transportation Systems (ITS) could be\narguably approached as a ``story'' intensively producing and consuming large\namounts of data. A~diversity of sensing devices densely spread over the\ninfrastructure, vehicles or the travelers' personal devices act as sources of\ndata flows that are eventually fed {into} software running on automatic\ndevices, actuators or control systems producing, in~turn, complex information\nflows {among} users, traffic managers, data analysts, traffic modeling\nscientists, etc. These~information flows provide enormous opportunities to\nimprove model development and decision-making. This work aims to describe how\ndata, coming from diverse ITS sources, can be used to learn and adapt\ndata-driven models for efficiently operating ITS assets, systems and processes;\nin~other words, for data-based models to fully become \\emph{actionable}.\nGrounded in this described data modeling pipeline for ITS, we~define the\ncharacteristics, engineering requisites and challenges intrinsic to its three\ncompounding stages, namely, data fusion, adaptive learning and model\nevaluation. We~deliberately generalize model learning to be adaptive, since,\nin~the core of our paper is the firm conviction that most learners will have to\nadapt to the ever-changing phenomenon scenario underlying the majority of ITS\napplications. Finally, we~provide a prospect of current research lines within\nData Science that can bring notable advances to data-based ITS modeling, which\nwill eventually bridge the gap towards the practicality and actionability of\nsuch models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 12:02:30 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 14:33:07 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 15:17:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Lana", "Ibai", ""], ["Sanchez-Medina", "Javier J.", ""], ["Vlahogianni", "Eleni I.", ""], ["Del Ser", "Javier", ""]]}, {"id": "2002.02255", "submitter": "Cheng Chen", "authors": "Cheng Chen, Qi Dou, Hao Chen, Jing Qin, Pheng Ann Heng", "title": "Unsupervised Bidirectional Cross-Modality Adaptation via Deeply\n  Synergistic Image and Feature Alignment for Medical Image Segmentation", "comments": "IEEE TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation has increasingly gained interest in medical\nimage computing, aiming to tackle the performance degradation of deep neural\nnetworks when being deployed to unseen data with heterogeneous characteristics.\nIn this work, we present a novel unsupervised domain adaptation framework,\nnamed as Synergistic Image and Feature Alignment (SIFA), to effectively adapt a\nsegmentation network to an unlabeled target domain. Our proposed SIFA conducts\nsynergistic alignment of domains from both image and feature perspectives. In\nparticular, we simultaneously transform the appearance of images across domains\nand enhance domain-invariance of the extracted features by leveraging\nadversarial learning in multiple aspects and with a deeply supervised\nmechanism. The feature encoder is shared between both adaptive perspectives to\nleverage their mutual benefits via end-to-end learning. We have extensively\nevaluated our method with cardiac substructure segmentation and abdominal\nmulti-organ segmentation for bidirectional cross-modality adaptation between\nMRI and CT images. Experimental results on two different tasks demonstrate that\nour SIFA method is effective in improving segmentation performance on unlabeled\ntarget images, and outperforms the state-of-the-art domain adaptation\napproaches by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 13:49:47 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Chen", "Cheng", ""], ["Dou", "Qi", ""], ["Chen", "Hao", ""], ["Qin", "Jing", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "2002.02256", "submitter": "Abhijit Suprem", "authors": "Abhijit Suprem, Calton Pu", "title": "Looking GLAMORous: Vehicle Re-Id in Heterogeneous Cameras Networks with\n  Global and Local Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (re-id) is a fundamental problem for modern\nsurveillance camera networks. Existing approaches for vehicle re-id utilize\nglobal features and local features for re-id by combining multiple subnetworks\nand losses. In this paper, we propose GLAMOR, or Global and Local Attention\nMOdules for Re-id. GLAMOR performs global and local feature extraction\nsimultaneously in a unified model to achieve state-of-the-art performance in\nvehicle re-id across a variety of adversarial conditions and datasets (mAPs\n80.34, 76.48, 77.15 on VeRi-776, VRIC, and VeRi-Wild, respectively). GLAMOR\nintroduces several contributions: a better backbone construction method that\noutperforms recent approaches, group and layer normalization to address\nconflicting loss targets for re-id, a novel global attention module for global\nfeature extraction, and a novel local attention module for self-guided\npart-based local feature extraction that does not require supervision.\nAdditionally, GLAMOR is a compact and fast model that is 10x smaller while\ndelivering 25% better performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 13:51:00 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Suprem", "Abhijit", ""], ["Pu", "Calton", ""]]}, {"id": "2002.02265", "submitter": "Evin Pinar Ornek", "authors": "Evin Pinar Ornek", "title": "Zero-Shot Activity Recognition with Videos", "comments": "This is a research report done during master's studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examined the zero-shot activity recognition task with the\nusage of videos. We introduce an auto-encoder based model to construct a\nmultimodal joint embedding space between the visual and textual manifolds. On\nthe visual side, we used activity videos and a state-of-the-art 3D\nconvolutional action recognition network to extract the features. On the\ntextual side, we worked with GloVe word embeddings. The zero-shot recognition\nresults are evaluated by top-n accuracy. Then, the manifold learning ability is\nmeasured by mean Nearest Neighbor Overlap. In the end, we provide an extensive\ndiscussion over the results and the future directions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:33:10 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Ornek", "Evin Pinar", ""]]}, {"id": "2002.02295", "submitter": "Qize Yang", "authors": "Qize Yang, Ancong Wu, Wei-Shi Zheng", "title": "Person Re-identification by Contour Sketch under Moderate Clothing\n  Change", "comments": "To appear in TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2960509", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id), the process of matching pedestrian images\nacross different camera views, is an important task in visual surveillance.\nSubstantial development of re-id has recently been observed, and the majority\nof existing models are largely dependent on color appearance and assume that\npedestrians do not change their clothes across camera views. This limitation,\nhowever, can be an issue for re-id when tracking a person at different places\nand at different time if that person (e.g., a criminal suspect) changes his/her\nclothes, causing most existing methods to fail, since they are heavily relying\non color appearance and thus they are inclined to match a person to another\nperson wearing similar clothes. In this work, we call the person re-id under\nclothing change the \"cross-clothes person re-id\". In particular, we consider\nthe case when a person only changes his clothes moderately as a first attempt\nat solving this problem based on visible light images; that is we assume that a\nperson wears clothes of a similar thickness, and thus the shape of a person\nwould not change significantly when the weather does not change substantially\nwithin a short period of time. We perform cross-clothes person re-id based on a\ncontour sketch of person image to take advantage of the shape of the human body\ninstead of color information for extracting features that are robust to\nmoderate clothing change. Due to the lack of a large-scale dataset for\ncross-clothes person re-id, we contribute a new dataset that consists of 33698\nimages from 221 identities. Our experiments illustrate the challenges of\ncross-clothes person re-id and demonstrate the effectiveness of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 15:13:55 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Yang", "Qize", ""], ["Wu", "Ancong", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2002.02308", "submitter": "Ting-Kueu Hu", "authors": "Ting-Kuei Hu, Fernando Gama, Tianlong Chen, Zhangyang Wang, Alejandro\n  Ribeiro, Brian M. Sadler", "title": "VGAI: End-to-End Learning of Vision-Based Decentralized Controllers for\n  Robot Swarms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized coordination of a robot swarm requires addressing the tension\nbetween local perceptions and actions, and the accomplishment of a global\nobjective. In this work, we propose to learn decentralized controllers based on\nsolely raw visual inputs. For the first time, that integrates the learning of\ntwo key components: communication and visual perception, in one end-to-end\nframework. More specifically, we consider that each robot has access to a\nvisual perception of the immediate surroundings, and communication capabilities\nto transmit and receive messages from other neighboring robots. Our proposed\nlearning framework combines a convolutional neural network (CNN) for each robot\nto extract messages from the visual inputs, and a graph neural network (GNN)\nover the entire swarm to transmit, receive and process these messages in order\nto decide on actions. The use of a GNN and locally-run CNNs results naturally\nin a decentralized controller. We jointly train the CNNs and the GNN so that\neach robot learns to extract messages from the images that are adequate for the\nteam as a whole. Our experiments demonstrate the proposed architecture in the\nproblem of drone flocking and show its promising performance and scalability,\ne.g., achieving successful decentralized flocking for large-sized swarms\nconsisting of up to 75 drones.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 15:25:23 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 14:10:23 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hu", "Ting-Kuei", ""], ["Gama", "Fernando", ""], ["Chen", "Tianlong", ""], ["Wang", "Zhangyang", ""], ["Ribeiro", "Alejandro", ""], ["Sadler", "Brian M.", ""]]}, {"id": "2002.02318", "submitter": "Kun Ouyang", "authors": "Kun Ouyang, Yuxuan Liang, Ye Liu, Zekun Tong, Sijie Ruan, Yu Zheng,\n  and David S. Rosenblum", "title": "Fine-Grained Urban Flow Inference", "comments": "16 pages. arXiv admin note: substantial text overlap with\n  arXiv:1902.05377", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ubiquitous deployment of monitoring devices in urban flow monitoring\nsystems induces a significant cost for maintenance and operation. A technique\nis required to reduce the number of deployed devices, while preventing the\ndegeneration of data accuracy and granularity. In this paper, we present an\napproach for inferring the real-time and fine-grained crowd flows throughout a\ncity based on coarse-grained observations. This task exhibits two challenges:\nthe spatial correlations between coarse- and fine-grained urban flows, and the\ncomplexities of external impacts. To tackle these issues, we develop a model\nentitled UrbanFM which consists of two major parts: 1) an inference network to\ngenerate fine-grained flow distributions from coarse-grained inputs that uses a\nfeature extraction module and a novel distributional upsampling module; 2) a\ngeneral fusion subnet to further boost the performance by considering the\ninfluence of different external factors. This structure provides outstanding\neffectiveness and efficiency for small scale upsampling. However, the\nsingle-pass upsampling used by UrbanFM is insufficient at higher upscaling\nrates. Therefore, we further present UrbanPy, a cascading model for progressive\ninference of fine-grained urban flows by decomposing the original tasks into\nmultiple subtasks. Compared to UrbanFM, such an enhanced structure demonstrates\nfavorable performance for larger-scale inference tasks.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 01:11:24 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Ouyang", "Kun", ""], ["Liang", "Yuxuan", ""], ["Liu", "Ye", ""], ["Tong", "Zekun", ""], ["Ruan", "Sijie", ""], ["Zheng", "Yu", ""], ["Rosenblum", "David S.", ""]]}, {"id": "2002.02333", "submitter": "Li Weng", "authors": "Li Weng, Lingzhi Ye, Jiangmin Tian, Jiuwen Cao, and Jianzhong Wang", "title": "Random VLAD based Deep Hashing for Efficient Image Retrieval", "comments": "10 pages, 17 figures, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image hash algorithms generate compact binary representations that can be\nquickly matched by Hamming distance, thus become an efficient solution for\nlarge-scale image retrieval. This paper proposes RV-SSDH, a deep image hash\nalgorithm that incorporates the classical VLAD (vector of locally aggregated\ndescriptors) architecture into neural networks. Specifically, a novel neural\nnetwork component is formed by coupling a random VLAD layer with a latent hash\nlayer through a transform layer. This component can be combined with\nconvolutional layers to realize a hash algorithm. We implement RV-SSDH as a\npoint-wise algorithm that can be efficiently trained by minimizing\nclassification error and quantization loss. Comprehensive experiments show this\nnew architecture significantly outperforms baselines such as NetVLAD and SSDH,\nand offers a cost-effective trade-off in the state-of-the-art. In addition, the\nproposed random VLAD layer leads to satisfactory accuracy with low complexity,\nthus shows promising potentials as an alternative to NetVLAD.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 16:22:44 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Weng", "Li", ""], ["Ye", "Lingzhi", ""], ["Tian", "Jiangmin", ""], ["Cao", "Jiuwen", ""], ["Wang", "Jianzhong", ""]]}, {"id": "2002.02362", "submitter": "Runsheng Xu", "authors": "Andi Zang, Runsheng Xu, Zichen Li, David Doria", "title": "Lane Boundary Geometry Extraction from Satellite Imagery", "comments": null, "journal-ref": "In Proceedings of the 1st ACM SIGSPATIAL Workshop on\n  High-Precision Maps and Intelligent Applications for Autonomous Vehicles,\n  page 1. ACM, 2017", "doi": "10.1145/3149092.3149093", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving car is becoming more of a reality, as a key\ncomponent,high-definition(HD) maps shows its value in both market place and\nindustry. Even though HD maps generation from LiDAR or stereo/perspective\nimagery has achieved impressive success, its inherent defects cannot be\nignored. In this paper, we proposal a novel method for Highway HD maps modeling\nusing pixel-wise segmentation on satellite imagery and formalized hypotheses\nlinking, which is cheaper and faster than current HD maps modeling approaches\nfrom LiDAR point cloud and perspective view imagery, and let it becomes an\nideal complementary of state of the art. We also manual code/label an HD road\nmodel dataset as ground truth, aligned with Bing tile image server, to train,\ntest and evaluate our methodology. This dataset will be publish at same time to\ncontribute research in HD maps modeling from aerial imagery.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 17:10:35 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Zang", "Andi", ""], ["Xu", "Runsheng", ""], ["Li", "Zichen", ""], ["Doria", "David", ""]]}, {"id": "2002.02369", "submitter": "Michele Merler", "authors": "Michele Merler, Cicero Nogueira dos Santos, Mauro Martino, Alfio M.\n  Gliozzo, John R. Smith", "title": "Covering the News with (AI) Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a multi-modal discriminative and generative frame-work capable\nof assisting humans in producing visual content re-lated to a given theme,\nstarting from a collection of documents(textual, visual, or both). This\nframework can be used by edit or to generate images for articles, as well as\nbooks or music album covers. Motivated by a request from the The New York Times\n(NYT) seeking help to use AI to create art for their special section on\nArtificial Intelligence, we demonstrated the application of our system in\nproducing such image.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 22:57:51 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Merler", "Michele", ""], ["Santos", "Cicero Nogueira dos", ""], ["Martino", "Mauro", ""], ["Gliozzo", "Alfio M.", ""], ["Smith", "John R.", ""]]}, {"id": "2002.02413", "submitter": "Shreyank N Gowda", "authors": "Shreyank N Gowda, Chun Yuan", "title": "StegColNet: Steganalysis based on an ensemble colorspace approach", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-73973-7_30", "report-no": null, "categories": "eess.IV cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image steganography refers to the process of hiding information inside\nimages. Steganalysis is the process of detecting a steganographic image. We\nintroduce a steganalysis approach that uses an ensemble color space model to\nobtain a weighted concatenated feature activation map. The concatenated map\nhelps to obtain certain features explicit to each color space. We use a\nlevy-flight grey wolf optimization strategy to reduce the number of features\nselected in the map. We then use these features to classify the image into one\nof two classes: whether the given image has secret information stored or not.\nExtensive experiments have been done on a large scale dataset extracted from\nthe Bossbase dataset. Also, we show that the model can be transferred to\ndifferent datasets and perform extensive experiments on a mixture of datasets.\nOur results show that the proposed approach outperforms the recent state of the\nart deep learning steganalytical approaches by 2.32 percent on average for 0.2\nbits per channel (bpc) and 1.87 percent on average for 0.4 bpc.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 17:44:25 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 16:30:15 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Gowda", "Shreyank N", ""], ["Yuan", "Chun", ""]]}, {"id": "2002.02424", "submitter": "Youcheng Sun", "authors": "Youcheng Sun, Yifan Zhou, Simon Maskell, James Sharp, Xiaowei Huang", "title": "Reliability Validation of Learning Enabled Vehicle Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the reliability of a real-world learning-enabled system,\nwhich conducts dynamic vehicle tracking based on a high-resolution wide-area\nmotion imagery input. The system consists of multiple neural network components\n-- to process the imagery inputs -- and multiple symbolic (Kalman filter)\ncomponents -- to analyse the processed information for vehicle tracking. It is\nknown that neural networks suffer from adversarial examples, which make them\nlack robustness. However, it is unclear if and how the adversarial examples\nover learning components can affect the overall system-level reliability. By\nintegrating a coverage-guided neural network testing tool, DeepConcolic, with\nthe vehicle tracking system, we found that (1) the overall system can be\nresilient to some adversarial examples thanks to the existence of other\ncomponents, and (2) the overall system presents an extra level of uncertainty\nwhich cannot be determined by analysing the deep learning components only. This\nresearch suggests the need for novel verification and validation methods for\nlearning-enabled systems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 18:07:54 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Sun", "Youcheng", ""], ["Zhou", "Yifan", ""], ["Maskell", "Simon", ""], ["Sharp", "James", ""], ["Huang", "Xiaowei", ""]]}, {"id": "2002.02506", "submitter": "Zhangsihao Yang", "authors": "Zhangsihao Yang, Or Litany, Tolga Birdal, Srinath Sridhar, Leonidas\n  Guibas", "title": "Continuous Geodesic Convolutions for Learning on 3D Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of descriptor-based methods for geometric processing of\nnon-rigid shape rely on hand-crafted descriptors. Recently, learning-based\ntechniques have been shown effective, achieving state-of-the-art results in a\nvariety of tasks. Yet, even though these methods can in principle work directly\non raw data, most methods still rely on hand-crafted descriptors at the input\nlayer. In this work, we wish to challenge this practice and use a neural\nnetwork to learn descriptors directly from the raw mesh. To this end, we\nintroduce two modules into our neural architecture. The first is a local\nreference frame (LRF) used to explicitly make the features invariant to rigid\ntransformations. The second is continuous convolution kernels that provide\nrobustness to sampling. We show the efficacy of our proposed network in\nlearning on raw meshes using two cornerstone tasks: shape matching, and human\nbody parts segmentation. Our results show superior results over baseline\nmethods that use hand-crafted descriptors.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 20:37:31 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Yang", "Zhangsihao", ""], ["Litany", "Or", ""], ["Birdal", "Tolga", ""], ["Sridhar", "Srinath", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2002.02545", "submitter": "Lichen Wang", "authors": "Can Qin, Lichen Wang, Qianqian Ma, Yu Yin, Huan Wang, Yun Fu", "title": "Contradictory Structure Learning for Semi-supervised Domain Adaptation", "comments": "8 pages without citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current adversarial adaptation methods attempt to align the cross-domain\nfeatures, whereas two challenges remain unsolved: 1) the conditional\ndistribution mismatch and 2) the bias of the decision boundary towards the\nsource domain. To solve these challenges, we propose a novel framework for\nsemi-supervised domain adaptation by unifying the learning of opposite\nstructures (UODA). UODA consists of a generator and two classifiers (i.e., the\nsource-scattering classifier and the target-clustering classifier), which are\ntrained for contradictory purposes. The target-clustering classifier attempts\nto cluster the target features to improve intra-class density and enlarge\ninter-class divergence. Meanwhile, the source-scattering classifier is designed\nto scatter the source features to enhance the decision boundary's smoothness.\nThrough the alternation of source-feature expansion and target-feature\nclustering procedures, the target features are well-enclosed within the dilated\nboundary of the corresponding source features. This strategy can make the\ncross-domain features to be precisely aligned against the source bias\nsimultaneously. Moreover, to overcome the model collapse through training, we\nprogressively update the measurement of feature's distance and their\nrepresentation via an adversarial training paradigm. Extensive experiments on\nthe benchmarks of DomainNet and Office-home datasets demonstrate the\nsuperiority of our approach over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 22:58:20 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 19:58:09 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Qin", "Can", ""], ["Wang", "Lichen", ""], ["Ma", "Qianqian", ""], ["Yin", "Yu", ""], ["Wang", "Huan", ""], ["Fu", "Yun", ""]]}, {"id": "2002.02547", "submitter": "Didrik Nielsen", "authors": "Didrik Nielsen, Ole Winther", "title": "Closing the Dequantization Gap: PixelCNN as a Single-Layer Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow models have recently made great progress at modeling ordinal discrete\ndata such as images and audio. Due to the continuous nature of flow models,\ndequantization is typically applied when using them for such discrete data,\nresulting in lower bound estimates of the likelihood. In this paper, we\nintroduce subset flows, a class of flows that can tractably transform finite\nvolumes and thus allow exact computation of likelihoods for discrete data.\nBased on subset flows, we identify ordinal discrete autoregressive models,\nincluding WaveNets, PixelCNNs and Transformers, as single-layer flows. We use\nthe flow formulation to compare models trained and evaluated with either the\nexact likelihood or its dequantization lower bound. Finally, we study\nmultilayer flows composed of PixelCNNs and non-autoregressive coupling layers\nand demonstrate state-of-the-art results on CIFAR-10 for flow models trained\nwith dequantization.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 22:58:51 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 11:04:08 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 17:05:37 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Nielsen", "Didrik", ""], ["Winther", "Ole", ""]]}, {"id": "2002.02559", "submitter": "Youshan Zhang", "authors": "Youshan Zhang and Brian D. Davison", "title": "Impact of ImageNet Model Selection on Domain Adaptation", "comments": null, "journal-ref": "In 2020 IEEE Winter Applications of Computer Vision Workshops\n  (WACVW)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are widely used in image classification problems.\nHowever, little work addresses how features from different deep neural networks\naffect the domain adaptation problem. Existing methods often extract deep\nfeatures from one ImageNet model, without exploring other neural networks. In\nthis paper, we investigate how different ImageNet models affect transfer\naccuracy on domain adaptation problems. We extract features from sixteen\ndistinct pre-trained ImageNet models and examine the performance of twelve\nbenchmarking methods when using the features. Extensive experimental results\nshow that a higher accuracy ImageNet model produces better features, and leads\nto higher accuracy on domain adaptation problems (with a correlation\ncoefficient of up to 0.95). We also examine the architecture of each neural\nnetwork to find the best layer for feature extraction. Together, performance\nfrom our features exceeds that of the state-of-the-art in three benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 23:58:23 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Zhang", "Youshan", ""], ["Davison", "Brian D.", ""]]}, {"id": "2002.02582", "submitter": "Mohammad Hashir", "authors": "Mohammad Hashir, Hadrien Bertrand and Joseph Paul Cohen", "title": "Quantifying the Value of Lateral Views in Deep Learning for Chest X-rays", "comments": "Under review at MIDL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep learning models in chest X-ray prediction utilize the\nposteroanterior (PA) view due to the lack of other views available. PadChest is\na large-scale chest X-ray dataset that has almost 200 labels and multiple views\navailable. In this work, we use PadChest to explore multiple approaches to\nmerging the PA and lateral views for predicting the radiological labels\nassociated with the X-ray image. We find that different methods of merging the\nmodel utilize the lateral view differently. We also find that including the\nlateral view increases performance for 32 labels in the dataset, while being\nneutral for the others. The increase in overall performance is comparable to\nthe one obtained by using only the PA view with twice the amount of patients in\nthe training set.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 01:48:13 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Hashir", "Mohammad", ""], ["Bertrand", "Hadrien", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "2002.02585", "submitter": "Jing Wang", "authors": "Divinah Nyasaka, Jing Wang, Haron Tinega", "title": "Learning Hyperspectral Feature Extraction and Classification with\n  ResNeXt Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hyperspectral image (HSI) classification is a standard remote sensing\ntask, in which each image pixel is given a label indicating the physical\nland-cover on the earth's surface. The achievements of image semantic\nsegmentation and deep learning approaches on ordinary images have accelerated\nthe research on hyperspectral image classification. Moreover, the utilization\nof both the spectral and spatial cues in hyperspectral images has shown\nimproved classification accuracy in hyperspectral image classification. The use\nof only 3D Convolutional Neural Networks (3D-CNN) to extract both spatial and\nspectral cues from Hyperspectral images results in an explosion of parameters\nhence high computational cost. We propose network architecture called the\nMixedSN that utilizes the 3D convolutions to modeling spectral-spatial\ninformation in the early layers of the architecture and the 2D convolutions at\nthe top layers which majorly deal with semantic abstraction. We constrain our\narchitecture to ResNeXt block because of their performance and simplicity. Our\nmodel drastically reduced the number of parameters and achieved comparable\nclassification performance with state-of-the-art methods on Indian Pine (IP)\nscene dataset, Pavia University scene (PU) dataset, Salinas (SA) Scene dataset,\nand Botswana (BW) dataset.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 01:54:15 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Nyasaka", "Divinah", ""], ["Wang", "Jing", ""], ["Tinega", "Haron", ""]]}, {"id": "2002.02589", "submitter": "Shoudong Han", "authors": "Ziqing Yang, Shoudong Han and Jun Zhao", "title": "Poisson Kernel Avoiding Self-Smoothing in Graph Convolutional Networks", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional network (GCN) is now an effective tool to deal with\nnon-Euclidean data, such as social networks in social behavior analysis,\nmolecular structure analysis in the field of chemistry, and skeleton-based\naction recognition. Graph convolutional kernel is one of the most significant\nfactors in GCN to extract nodes' feature, and some improvements of it have\nreached promising performance theoretically and experimentally. However, there\nis limited research about how exactly different data types and graph structures\ninfluence the performance of these kernels. Most existing methods used an\nadaptive convolutional kernel to deal with a given graph structure, which still\nnot reveals the internal reasons. In this paper, we started from theoretical\nanalysis of the spectral graph and studied the properties of existing graph\nconvolutional kernels. While taking some designed datasets with specific\nparameters into consideration, we revealed the self-smoothing phenomenon of\nconvolutional kernels. After that, we proposed the Poisson kernel that can\navoid self-smoothing without training any adaptive kernel. Experimental results\ndemonstrate that our Poisson kernel not only works well on the benchmark\ndataset where state-of-the-art methods work fine, but also is evidently\nsuperior to them in synthetic datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 02:25:11 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 10:03:50 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Yang", "Ziqing", ""], ["Han", "Shoudong", ""], ["Zhao", "Jun", ""]]}, {"id": "2002.02598", "submitter": "Yihan Du", "authors": "Yihan Du, Yan Yan, Si Chen, Yang Hua", "title": "Object-Adaptive LSTM Network for Real-time Visual Tracking with\n  Adversarial Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning based visual tracking methods have obtained\ngreat success owing to the powerful feature representation ability of\nConvolutional Neural Networks (CNNs). Among these methods, classification-based\ntracking methods exhibit excellent performance while their speeds are heavily\nlimited by the expensive computation for massive proposal feature extraction.\nIn contrast, matching-based tracking methods (such as Siamese networks) possess\nremarkable speed superiority. However, the absence of online updating renders\nthese methods unadaptable to significant object appearance variations. In this\npaper, we propose a novel real-time visual tracking method, which adopts an\nobject-adaptive LSTM network to effectively capture the video sequential\ndependencies and adaptively learn the object appearance variations. For high\ncomputational efficiency, we also present a fast proposal selection strategy,\nwhich utilizes the matching-based tracking method to pre-estimate dense\nproposals and selects high-quality ones to feed to the LSTM network for\nclassification. This strategy efficiently filters out some irrelevant proposals\nand avoids the redundant computation for feature extraction, which enables our\nmethod to operate faster than conventional classification-based tracking\nmethods. In addition, to handle the problems of sample inadequacy and class\nimbalance during online tracking, we adopt a data augmentation technique based\non the Generative Adversarial Network (GAN) to facilitate the training of the\nLSTM network. Extensive experiments on four visual tracking benchmarks\ndemonstrate the state-of-the-art performance of our method in terms of both\ntracking accuracy and speed, which exhibits great potentials of recurrent\nstructures for visual tracking.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 03:06:07 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Du", "Yihan", ""], ["Yan", "Yan", ""], ["Chen", "Si", ""], ["Hua", "Yang", ""]]}, {"id": "2002.02603", "submitter": "Wx Yang", "authors": "Wanxiang Yang, Yan Yan, Si Chen", "title": "Adaptive Deep Metric Embeddings for Person Re-Identification under\n  Occlusions", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) under occlusions is a challenging problem in\nvideo surveillance. Most of existing person ReID methods take advantage of\nlocal features to deal with occlusions. However, these methods usually\nindependently extract features from the local regions of an image without\nconsidering the relationship among different local regions. In this paper, we\npropose a novel person ReID method, which learns the spatial dependencies\nbetween the local regions and extracts the discriminative feature\nrepresentation of the pedestrian image based on Long Short-Term Memory (LSTM),\ndealing with the problem of occlusions. In particular, we propose a novel loss\n(termed the adaptive nearest neighbor loss) based on the classification\nuncertainty to effectively reduce intra-class variations while enlarging\ninter-class differences within the adaptive neighborhood of the sample. The\nproposed loss enables the deep neural network to adaptively learn\ndiscriminative metric embeddings, which significantly improve the\ngeneralization capability of recognizing unseen person identities. Extensive\ncomparative evaluations on challenging person ReID datasets demonstrate the\nsignificantly improved performance of the proposed method compared with several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 03:18:10 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Yang", "Wanxiang", ""], ["Yan", "Yan", ""], ["Chen", "Si", ""]]}, {"id": "2002.02609", "submitter": "Zheng Hui", "authors": "Zheng Hui, Jie Li, Xiumei Wang, Xinbo Gao", "title": "Image Fine-grained Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting techniques have shown promising improvement with the\nassistance of generative adversarial networks (GANs) recently. However, most of\nthem often suffered from completed results with unreasonable structure or\nblurriness. To mitigate this problem, in this paper, we present a one-stage\nmodel that utilizes dense combinations of dilated convolutions to obtain larger\nand more effective receptive fields. Benefited from the property of this\nnetwork, we can more easily recover large regions in an incomplete image. To\nbetter train this efficient generator, except for frequently-used VGG feature\nmatching loss, we design a novel self-guided regression loss for concentrating\non uncertain areas and enhancing the semantic details. Besides, we devise a\ngeometrical alignment constraint item to compensate for the pixel-based\ndistance between prediction features and ground-truth ones. We also employ a\ndiscriminator with local and global branches to ensure local-global contents\nconsistency. To further improve the quality of generated images, discriminator\nfeature matching on the local branch is introduced, which dynamically minimizes\nthe similarity of intermediate features between synthetic and ground-truth\npatches. Extensive experiments on several public datasets demonstrate that our\napproach outperforms current state-of-the-art methods. Code is available at\nhttps://github.com/Zheng222/DMFN.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 03:45:25 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 03:52:51 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hui", "Zheng", ""], ["Li", "Jie", ""], ["Wang", "Xiumei", ""], ["Gao", "Xinbo", ""]]}, {"id": "2002.02624", "submitter": "Ryan Keisler", "authors": "Ryan Keisler, Samuel W. Skillman, Sunny Gonnabathula, Justin Poehnelt,\n  Xander Rudelis, Michael S. Warren", "title": "Visual search over billions of aerial and satellite images", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2019.07.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for performing visual search over billions of aerial and\nsatellite images. The purpose of visual search is to find images that are\nvisually similar to a query image. We define visual similarity using 512\nabstract visual features generated by a convolutional neural network that has\nbeen trained on aerial and satellite imagery. The features are converted to\nbinary values to reduce data and compute requirements. We employ a hash-based\nsearch using Bigtable, a scalable database service from Google Cloud. Searching\nthe continental United States at 1-meter pixel resolution, corresponding to\napproximately 2 billion images, takes approximately 0.1 seconds. This system\nenables real-time visual search over the surface of the earth, and an\ninteractive demo is available at https://search.descarteslabs.com.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 04:59:50 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Keisler", "Ryan", ""], ["Skillman", "Samuel W.", ""], ["Gonnabathula", "Sunny", ""], ["Poehnelt", "Justin", ""], ["Rudelis", "Xander", ""], ["Warren", "Michael S.", ""]]}, {"id": "2002.02634", "submitter": "Jing Yu Koh", "authors": "Jing Yu Koh, Duc Thanh Nguyen, Quang-Trung Truong, Sai-Kit Yeung,\n  Alexander Binder", "title": "SideInfNet: A Deep Neural Network for Semi-Automatic Semantic\n  Segmentation with Side Information", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully-automatic execution is the ultimate goal for many Computer Vision\napplications. However, this objective is not always realistic in tasks\nassociated with high failure costs, such as medical applications. For these\ntasks, semi-automatic methods allowing minimal effort from users to guide\ncomputer algorithms are often preferred due to desirable accuracy and\nperformance. Inspired by the practicality and applicability of the\nsemi-automatic approach, this paper proposes a novel deep neural network\narchitecture, namely SideInfNet that effectively integrates features learnt\nfrom images with side information extracted from user annotations. To evaluate\nour method, we applied the proposed network to three semantic segmentation\ntasks and conducted extensive experiments on benchmark datasets. Experimental\nresults and comparison with prior work have verified the superiority of our\nmodel, suggesting the generality and effectiveness of the model in\nsemi-automatic semantic segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 06:10:54 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 05:07:11 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 18:20:29 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 12:59:56 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Koh", "Jing Yu", ""], ["Nguyen", "Duc Thanh", ""], ["Truong", "Quang-Trung", ""], ["Yeung", "Sai-Kit", ""], ["Binder", "Alexander", ""]]}, {"id": "2002.02638", "submitter": "Arman Karimian", "authors": "Arman Karimian, Ziqi Yang, Roberto Tron", "title": "Statistical Outlier Identification in Multi-robot Visual SLAM using\n  Expectation Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel and distributed method for detecting inter-map\nloop closure outliers in simultaneous localization and mapping (SLAM). The\nproposed algorithm does not rely on a good initialization and can handle more\nthan two maps at a time. In multi-robot SLAM applications, maps made by\ndifferent agents have nonidentical spatial frames of reference which makes\ninitialization very difficult in the presence of outliers. This paper presents\na probabilistic approach for detecting incorrect orientation measurements prior\nto pose graph optimization by checking the geometric consistency of rotation\nmeasurements. Expectation-Maximization is used to fine-tune the model\nparameters. As ancillary contributions, a new approximate discrete inference\nprocedure is presented which uses evidence on loops in a graph and is based on\noptimization (Alternate Direction Method of Multipliers). This method yields\nsuperior results compared to Belief Propagation and has convergence guarantees.\nSimulation and experimental results are presented that evaluate the performance\nof the outlier detection method and the inference algorithm on synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 06:34:44 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Karimian", "Arman", ""], ["Yang", "Ziqi", ""], ["Tron", "Roberto", ""]]}, {"id": "2002.02651", "submitter": "Alexandros Stergiou MSc", "authors": "Alexandros Stergiou, Ronald Poppe, and Remco C. Veltkamp", "title": "Learning Class Regularized Features for Action Recognition", "comments": null, "journal-ref": null, "doi": "10.3390/app10186241", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Deep Convolutional Neural Networks (CNNs) is based on the notion of\nusing multiple kernels and non-linearities in their subsequent activations to\nextract useful features. The kernels are used as general feature extractors\nwithout specific correspondence to the target class. As a result, the extracted\nfeatures do not correspond to specific classes. Subtle differences between\nsimilar classes are modeled in the same way as large differences between\ndissimilar classes. To overcome the class-agnostic use of kernels in CNNs, we\nintroduce a novel method named Class Regularization that performs class-based\nregularization of layer activations. We demonstrate that this not only improves\nfeature search during training, but also allows an explicit assignment of\nfeatures per class during each stage of the feature extraction process. We show\nthat using Class Regularization blocks in state-of-the-art CNN architectures\nfor action recognition leads to systematic improvement gains of 1.8%, 1.2% and\n1.4% on the Kinetics, UCF-101 and HMDB-51 datasets, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 07:27:49 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Stergiou", "Alexandros", ""], ["Poppe", "Ronald", ""], ["Veltkamp", "Remco C.", ""]]}, {"id": "2002.02657", "submitter": "Davide La Torre Prof.", "authors": "D. Otero, D. La Torre, O. Michailovich, E.R. Vrscay", "title": "Optimization of Structural Similarity in Mathematical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.NA eess.IV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now generally accepted that Euclidean-based metrics may not always\nadequately represent the subjective judgement of a human observer. As a result,\nmany image processing methodologies have been recently extended to take\nadvantage of alternative visual quality measures, the most prominent of which\nis the Structural Similarity Index Measure (SSIM). The superiority of the\nlatter over Euclidean-based metrics have been demonstrated in several studies.\nHowever, being focused on specific applications, the findings of such studies\noften lack generality which, if otherwise acknowledged, could have provided a\nuseful guidance for further development of SSIM-based image processing\nalgorithms. Accordingly, instead of focusing on a particular image processing\ntask, in this paper, we introduce a general framework that encompasses a wide\nrange of imaging applications in which the SSIM can be employed as a fidelity\nmeasure. Subsequently, we show how the framework can be used to cast some\nstandard as well as original imaging tasks into optimization problems, followed\nby a discussion of a number of novel numerical strategies for their solution.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 07:46:31 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Otero", "D.", ""], ["La Torre", "D.", ""], ["Michailovich", "O.", ""], ["Vrscay", "E. R.", ""]]}, {"id": "2002.02698", "submitter": "Ge Song", "authors": "Ge Song, Jun Zhao, Xiaoyang Tan", "title": "Deep Robust Multilevel Semantic Cross-Modal Hashing", "comments": "11 pages, 9 figures, submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing based cross-modal retrieval has recently made significant progress.\nBut straightforward embedding data from different modalities into a joint\nHamming space will inevitably produce false codes due to the intrinsic modality\ndiscrepancy and noises. We present a novel Robust Multilevel Semantic Hashing\n(RMSH) for more accurate cross-modal retrieval. It seeks to preserve\nfine-grained similarity among data with rich semantics, while explicitly\nrequire distances between dissimilar points to be larger than a specific value\nfor strong robustness. For this, we give an effective bound of this value based\non the information coding-theoretic analysis, and the above goals are embodied\ninto a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via\nfusing multiple hash codes to explore seldom-seen semantics, alleviating the\nsparsity problem of similarity information. Experiments on three benchmarks\nshow the validity of the derived bounds, and our method achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 10:08:21 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 09:32:31 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Song", "Ge", ""], ["Zhao", "Jun", ""], ["Tan", "Xiaoyang", ""]]}, {"id": "2002.02705", "submitter": "Christian Haase-Sch\\\"utz", "authors": "Christian Haase-Sch\\\"utz, Rainer Stal, Heinz Hertlein and Bernhard\n  Sick", "title": "Iterative Label Improvement: Robust Training by Confidence Based\n  Filtering and Dataset Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art, high capacity deep neural networks not only require large\namounts of labelled training data, they are also highly susceptible to label\nerrors in this data, typically resulting in large efforts and costs and\ntherefore limiting the applicability of deep learning. To alleviate this issue,\nwe propose a novel meta training and labelling scheme that is able to use\ninexpensive unlabelled data by taking advantage of the generalization power of\ndeep neural networks. We show experimentally that by solely relying on one\nnetwork architecture and our proposed scheme of iterative training and\nprediction steps, both label quality and resulting model accuracy can be\nimproved significantly. Our method achieves state-of-the-art results, while\nbeing architecture agnostic and therefore broadly applicable. Compared to other\nmethods dealing with erroneous labels, our approach does neither require\nanother network to be trained, nor does it necessarily need an additional,\nhighly accurate reference label set. Instead of removing samples from a\nlabelled set, our technique uses additional sensor data without the need for\nmanual labelling. Furthermore, our approach can be used for semi-supervised\nlearning.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 10:42:26 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:00:16 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 10:13:54 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Haase-Sch\u00fctz", "Christian", ""], ["Stal", "Rainer", ""], ["Hertlein", "Heinz", ""], ["Sick", "Bernhard", ""]]}, {"id": "2002.02709", "submitter": "Hamd Ul Moqeet Riaz", "authors": "Hamd ul Moqeet Riaz, Nuri Benbarka and Andreas Zell", "title": "FourierNet: Compact mask representation for instance segmentation using\n  differentiable shape decoders", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FourierNet, a single shot, anchor-free, fully convolutional\ninstance segmentation method that predicts a shape vector. Consequently, this\nshape vector is converted into the masks' contour points using a fast numerical\ntransform. Compared to previous methods, we introduce a new training technique,\nwhere we utilize a differentiable shape decoder, which manages the automatic\nweight balancing of the shape vector's coefficients. We used the Fourier series\nas a shape encoder because of its coefficient interpretability and fast\nimplementation. FourierNet shows promising results compared to polygon\nrepresentation methods, achieving 30.6 mAP on the MS COCO 2017 benchmark. At\nlower image resolutions, it runs at 26.6 FPS with 24.3 mAP. It reaches 23.3 mAP\nusing just eight parameters to represent the mask (note that at least four\nparameters are needed for bounding box prediction only). Qualitative analysis\nshows that suppressing a reasonable proportion of higher frequencies of Fourier\nseries, still generates meaningful masks. These results validate our\nunderstanding that lower frequency components hold higher information for the\nsegmentation task, and therefore, we can achieve a compressed representation.\nCode is available at: github.com/cogsys-tuebingen/FourierNet.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 10:54:54 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 11:46:58 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Riaz", "Hamd ul Moqeet", ""], ["Benbarka", "Nuri", ""], ["Zell", "Andreas", ""]]}, {"id": "2002.02792", "submitter": "Prerana Mukherjee", "authors": "Laxman Kumarapu and Prerana Mukherjee", "title": "AnimePose: Multi-person 3D pose estimation and animation", "comments": "arXiv admin note: text overlap with arXiv:1907.11346 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D animation of humans in action is quite challenging as it involves using a\nhuge setup with several motion trackers all over the person's body to track the\nmovements of every limb. This is time-consuming and may cause the person\ndiscomfort in wearing exoskeleton body suits with motion sensors. In this work,\nwe present a trivial yet effective solution to generate 3D animation of\nmultiple persons from a 2D video using deep learning. Although significant\nimprovement has been achieved recently in 3D human pose estimation, most of the\nprior works work well in case of single person pose estimation and multi-person\npose estimation is still a challenging problem. In this work, we firstly\npropose a supervised multi-person 3D pose estimation and animation framework\nnamely AnimePose for a given input RGB video sequence. The pipeline of the\nproposed system consists of various modules: i) Person detection and\nsegmentation, ii) Depth Map estimation, iii) Lifting 2D to 3D information for\nperson localization iv) Person trajectory prediction and human pose tracking.\nOur proposed system produces comparable results on previous state-of-the-art 3D\nmulti-person pose estimation methods on publicly available datasets MuCo-3DHP\nand MuPoTS-3D datasets and it also outperforms previous state-of-the-art human\npose tracking methods by a significant margin of 11.7% performance gain on MOTA\nscore on Posetrack 2018 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 11:11:56 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Kumarapu", "Laxman", ""], ["Mukherjee", "Prerana", ""]]}, {"id": "2002.02814", "submitter": "Jianfeng Dong", "authors": "Zhe Ma, Jianfeng Dong, Yao Zhang, Zhongzi Long, Yuan He, Hui Xue,\n  Shouling Ji", "title": "Fine-Grained Fashion Similarity Learning by Attribute-Specific Embedding\n  Network", "comments": "16 pages, 13 figutes. Accepted by AAAI 2020. Code and data are\n  available at https://github.com/Maryeon/asen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives to learn fine-grained fashion similarity. In this\nsimilarity paradigm, one should pay more attention to the similarity in terms\nof a specific design/attribute among fashion items, which has potential values\nin many fashion related applications such as fashion copyright protection. To\nthis end, we propose an Attribute-Specific Embedding Network (ASEN) to jointly\nlearn multiple attribute-specific embeddings in an end-to-end manner, thus\nmeasure the fine-grained similarity in the corresponding space. With two\nattention modules, i.e., Attribute-aware Spatial Attention and Attribute-aware\nChannel Attention, ASEN is able to locate the related regions and capture the\nessential patterns under the guidance of the specified attribute, thus make the\nlearned attribute-specific embeddings better reflect the fine-grained\nsimilarity. Extensive experiments on four fashion-related datasets show the\neffectiveness of ASEN for fine-grained fashion similarity learning and its\npotential for fashion reranking.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 14:42:26 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Ma", "Zhe", ""], ["Dong", "Jianfeng", ""], ["Zhang", "Yao", ""], ["Long", "Zhongzi", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""], ["Ji", "Shouling", ""]]}, {"id": "2002.02815", "submitter": "Luis Guerra", "authors": "Luis Guerra, Bohan Zhuang, Ian Reid, Tom Drummond", "title": "Switchable Precision Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instantaneous and on demand accuracy-efficiency trade-off has been recently\nexplored in the context of neural networks slimming. In this paper, we propose\na flexible quantization strategy, termed Switchable Precision neural Networks\n(SP-Nets), to train a shared network capable of operating at multiple\nquantization levels. At runtime, the network can adjust its precision on the\nfly according to instant memory, latency, power consumption and accuracy\ndemands. For example, by constraining the network weights to 1-bit with\nswitchable precision activations, our shared network spans from BinaryConnect\nto Binarized Neural Network, allowing to perform dot-products using only\nsummations or bit operations. In addition, a self-distillation scheme is\nproposed to increase the performance of the quantized switches. We tested our\napproach with three different quantizers and demonstrate the performance of\nSP-Nets against independently trained quantized models in classification\naccuracy for Tiny ImageNet and ImageNet datasets using ResNet-18 and MobileNet\narchitectures.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 14:43:44 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Guerra", "Luis", ""], ["Zhuang", "Bohan", ""], ["Reid", "Ian", ""], ["Drummond", "Tom", ""]]}, {"id": "2002.02852", "submitter": "S\\'ebastien de Blois", "authors": "S\\'ebastien de Blois, Mathieu Garon, Christian Gagn\\'e,\n  Jean-Fran\\c{c}ois Lalonde", "title": "Input Dropout for Spatially Aligned Modalities", "comments": "Accepted in ICIP 2020. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision datasets containing multiple modalities such as color, depth,\nand thermal properties are now commonly accessible and useful for solving a\nwide array of challenging tasks. However, deploying multi-sensor heads is not\npossible in many scenarios. As such many practical solutions tend to be based\non simpler sensors, mostly for cost, simplicity and robustness considerations.\nIn this work, we propose a training methodology to take advantage of these\nadditional modalities available in datasets, even if they are not available at\ntest time. By assuming that the modalities have a strong spatial correlation,\nwe propose Input Dropout, a simple technique that consists in stochastic hiding\nof one or many input modalities at training time, while using only the\ncanonical (e.g. RGB) modalities at test time. We demonstrate that Input Dropout\ntrivially combines with existing deep convolutional architectures, and improves\ntheir performance on a wide range of computer vision tasks such as dehazing,\n6-DOF object tracking, pedestrian detection and object classification.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 15:39:34 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 11:35:17 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["de Blois", "S\u00e9bastien", ""], ["Garon", "Mathieu", ""], ["Gagn\u00e9", "Christian", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "2002.02857", "submitter": "Peter Hirsch", "authors": "Peter Hirsch, Dagmar Kainmueller", "title": "An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy\n  Images", "comments": "code available at: https://github.com/Kainmueller-Lab/aux_cpv_loss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of cell nuclei in microscopy images is a prevalent necessity in\ncell biology. Especially for three-dimensional datasets, manual segmentation is\nprohibitively time-consuming, motivating the need for automated methods.\nLearning-based methods trained on pixel-wise ground-truth segmentations have\nbeen shown to yield state-of-the-art results on 2d benchmark image data of\nnuclei, yet a respective benchmark is missing for 3d image data. In this work,\nwe perform a comparative evaluation of nuclei segmentation algorithms on a\ndatabase of manually segmented 3d light microscopy volumes. We propose a novel\nlearning strategy that boosts segmentation accuracy by means of a simple\nauxiliary task, thereby robustly outperforming each of our baselines.\nFurthermore, we show that one of our baselines, the popular three-label model,\nwhen trained with our proposed auxiliary task, outperforms the recent\nStarDist-3D. As an additional, practical contribution, we benchmark nuclei\nsegmentation against nuclei detection, i.e. the task of merely pinpointing\nindividual nuclei without generating respective pixel-accurate segmentations.\nFor learning nuclei detection, large 3d training datasets of manually annotated\nnuclei center points are available. However, the impact on detection accuracy\ncaused by training on such sparse ground truth as opposed to dense pixel-wise\nground truth has not yet been quantified. To this end, we compare nuclei\ndetection accuracy yielded by training on dense vs. sparse ground truth. Our\nresults suggest that training on sparse ground truth yields competitive nuclei\ndetection rates.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 15:47:55 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Hirsch", "Peter", ""], ["Kainmueller", "Dagmar", ""]]}, {"id": "2002.02909", "submitter": "Bin Kong", "authors": "Xian Zhang, Xin Wang, Bin Kong, Canghong Shi, Youbing Yin, Qi Song,\n  Siwei Lyu, Jiancheng Lv, Canghong Shi, Xiaojie Li", "title": "Domain Embedded Multi-model Generative Adversarial Networks for\n  Image-based Face Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior knowledge of face shape and structure plays an important role in face\ninpainting. However, traditional face inpainting methods mainly focus on the\ngenerated image resolution of the missing portion without consideration of the\nspecial particularities of the human face explicitly and generally produce\ndiscordant facial parts. To solve this problem, we present a domain embedded\nmulti-model generative adversarial model for inpainting of face images with\nlarge cropped regions. We firstly represent only face regions using the latent\nvariable as the domain knowledge and combine it with the non-face parts\ntextures to generate high-quality face images with plausible contents. Two\nadversarial discriminators are finally used to judge whether the generated\ndistribution is close to the real distribution or not. It can not only\nsynthesize novel image structures but also explicitly utilize the embedded face\ndomain knowledge to generate better predictions with consistency on structures\nand appearance. Experiments on both CelebA and CelebA-HQ face datasets\ndemonstrate that our proposed approach achieved state-of-the-art performance\nand generates higher quality inpainting results than existing ones.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 17:36:13 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 05:47:05 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhang", "Xian", ""], ["Wang", "Xin", ""], ["Kong", "Bin", ""], ["Shi", "Canghong", ""], ["Yin", "Youbing", ""], ["Song", "Qi", ""], ["Lyu", "Siwei", ""], ["Lv", "Jiancheng", ""], ["Shi", "Canghong", ""], ["Li", "Xiaojie", ""]]}, {"id": "2002.02917", "submitter": "Sharon Zhou", "authors": "Sharon Zhou, Jiequan Zhang, Hang Jiang, Torbjorn Lundh, Andrew Y. Ng", "title": "Data augmentation with Mobius transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has led to substantial improvements in the performance and\ngeneralization of deep models, and remain a highly adaptable method to evolving\nmodel architectures and varying amounts of data---in particular, extremely\nscarce amounts of available training data. In this paper, we present a novel\nmethod of applying Mobius transformations to augment input images during\ntraining. Mobius transformations are bijective conformal maps that generalize\nimage translation to operate over complex inversion in pixel space. As a\nresult, Mobius transformations can operate on the sample level and preserve\ndata labels. We show that the inclusion of Mobius transformations during\ntraining enables improved generalization over prior sample-level data\naugmentation techniques such as cutout and standard crop-and-flip\ntransformations, most notably in low data regimes.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 17:45:39 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 08:00:04 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Zhou", "Sharon", ""], ["Zhang", "Jiequan", ""], ["Jiang", "Hang", ""], ["Lundh", "Torbjorn", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "2002.02918", "submitter": "Qian Liu", "authors": "Qian Liu, Dongyang Cai, Jie Liu, Nan Ding, Tao Wang", "title": "iqiyi Submission to ActivityNet Challenge 2019 Kinetics-700 challenge:\n  Hierarchical Group-wise Attention", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, the method for the iqiyi submission to the task of\nActivityNet 2019 Kinetics-700 challenge is described. Three models are involved\nin the model ensemble stage: TSN, HG-NL and StNet. We propose the hierarchical\ngroup-wise non-local (HG-NL) module for frame-level features aggregation for\nvideo classification. The standard non-local (NL) module is effective in\naggregating frame-level features on the task of video classification but\npresents low parameters efficiency and high computational cost. The HG-NL\nmethod involves a hierarchical group-wise structure and generates multiple\nattention maps to enhance performance. Basing on this hierarchical group-wise\nstructure, the proposed method has competitive accuracy, fewer parameters and\nsmaller computational cost than the standard NL. For the task of ActivityNet\n2019 Kinetics-700 challenge, after model ensemble, we finally obtain an\naveraged top-1 and top-5 error percentage 28.444% on the test set.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 17:46:38 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Liu", "Qian", ""], ["Cai", "Dongyang", ""], ["Liu", "Jie", ""], ["Ding", "Nan", ""], ["Wang", "Tao", ""]]}, {"id": "2002.02921", "submitter": "Yidan Qin", "authors": "Yidan Qin, Sahba Aghajani Pedram, Seyedshams Feyzabadi, Max Allan, A.\n  Jonathan McLeod, Joel W. Burdick, Mahdi Azizian", "title": "Temporal Segmentation of Surgical Sub-tasks through Deep Learning with\n  Multiple Data Sources", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in robot-assisted surgeries (RAS) can be represented by\nfinite-state machines (FSMs), where each state represents either an action\n(such as picking up a needle) or an observation (such as bleeding). A crucial\nstep towards the automation of such surgical tasks is the temporal perception\nof the current surgical scene, which requires a real-time estimation of the\nstates in the FSMs. The objective of this work is to estimate the current state\nof the surgical task based on the actions performed or events occurred as the\ntask progresses. We propose Fusion-KVE, a unified surgical state estimation\nmodel that incorporates multiple data sources including the Kinematics, Vision,\nand system Events. Additionally, we examine the strengths and weaknesses of\ndifferent state estimation models in segmenting states with different\nrepresentative features or levels of granularity. We evaluate our model on the\nJHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), as well as a more\ncomplex dataset involving robotic intra-operative ultrasound (RIOUS) imaging,\ncreated using the da Vinci Xi surgical system. Our model achieves a superior\nframe-wise state estimation accuracy up to 89.4%, which improves the\nstate-of-the-art surgical state estimation models in both JIGSAWS suturing\ndataset and our RIOUS dataset.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 17:49:08 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Qin", "Yidan", ""], ["Pedram", "Sahba Aghajani", ""], ["Feyzabadi", "Seyedshams", ""], ["Allan", "Max", ""], ["McLeod", "A. Jonathan", ""], ["Burdick", "Joel W.", ""], ["Azizian", "Mahdi", ""]]}, {"id": "2002.02924", "submitter": "Marzieh Edraki", "authors": "Marzieh Edraki, Nazanin Rahnavard, Mubarak Shah", "title": "Subspace Capsule Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have become a key asset to most of\nfields in AI. Despite their successful performance, CNNs suffer from a major\ndrawback. They fail to capture the hierarchy of spatial relation among\ndifferent parts of an entity. As a remedy to this problem, the idea of capsules\nwas proposed by Hinton. In this paper, we propose the SubSpace Capsule Network\n(SCN) that exploits the idea of capsule networks to model possible variations\nin the appearance or implicitly defined properties of an entity through a group\nof capsule subspaces instead of simply grouping neurons to create capsules. A\ncapsule is created by projecting an input feature vector from a lower layer\nonto the capsule subspace using a learnable transformation. This transformation\nfinds the degree of alignment of the input with the properties modeled by the\ncapsule subspace. We show that SCN is a general capsule network that can\nsuccessfully be applied to both discriminative and generative models without\nincurring computational overhead compared to CNN during test time.\nEffectiveness of SCN is evaluated through a comprehensive set of experiments on\nsupervised image classification, semi-supervised image classification and\nhigh-resolution image generation tasks using the generative adversarial network\n(GAN) framework. SCN significantly improves the performance of the baseline\nmodels in all 3 tasks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 17:51:56 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Edraki", "Marzieh", ""], ["Rahnavard", "Nazanin", ""], ["Shah", "Mubarak", ""]]}, {"id": "2002.02927", "submitter": "Matthias Kirchner", "authors": "Matthias Kirchner and Cameron Johnson", "title": "SPN-CNN: Boosting Sensor-Based Source Camera Attribution With Deep\n  Learning", "comments": "Presented at the IEEE International Workshop on Information Forensics\n  and Security (WIFS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore means to advance source camera identification based on sensor\nnoise in a data-driven framework. Our focus is on improving the sensor pattern\nnoise (SPN) extraction from a single image at test time. Where existing works\nsuppress nuisance content with denoising filters that are largely agnostic to\nthe specific SPN signal of interest, we demonstrate that a~deep learning\napproach can yield a more suitable extractor that leads to improved source\nattribution. A series of extensive experiments on various public datasets\nconfirms the feasibility of our approach and its applicability to image\nmanipulation localization and video source attribution. A critical discussion\nof potential pitfalls completes the text.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 17:55:28 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Kirchner", "Matthias", ""], ["Johnson", "Cameron", ""]]}, {"id": "2002.02934", "submitter": "V\\'itor Albiero", "authors": "V\\'itor Albiero, Kai Zhang, and Kevin W. Bowyer", "title": "How Does Gender Balance In Training Data Affect Face Recognition\n  Accuracy?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have greatly increased the accuracy of face\nrecognition, but an old problem still persists: accuracy is usually higher for\nmen than women. It is often speculated that lower accuracy for women is caused\nby under-representation in the training data. This work investigates female\nunder-representation in the training data is truly the cause of lower accuracy\nfor females on test data. Using a state-of-the-art deep CNN, three different\nloss functions, and two training datasets, we train each on seven subsets with\ndifferent male/female ratios, totaling forty two trainings, that are tested on\nthree different datasets. Results show that (1) gender balance in the training\ndata does not translate into gender balance in the test accuracy, (2) the\n\"gender gap\" in test accuracy is not minimized by a gender-balanced training\nset, but by a training set with more male images than female images, and (3)\ntraining to minimize the accuracy gap does not result in highest female, male\nor average accuracy\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 18:11:01 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 21:30:35 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Albiero", "V\u00edtor", ""], ["Zhang", "Kai", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "2002.02942", "submitter": "Maneet Singh", "authors": "Richa Singh, Akshay Agarwal, Maneet Singh, Shruti Nagpal, Mayank Vatsa", "title": "On the Robustness of Face Recognition Algorithms Against Attacks and\n  Bias", "comments": "Accepted in Senior Member Track, AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition algorithms have demonstrated very high recognition\nperformance, suggesting suitability for real world applications. Despite the\nenhanced accuracies, robustness of these algorithms against attacks and bias\nhas been challenged. This paper summarizes different ways in which the\nrobustness of a face recognition algorithm is challenged, which can severely\naffect its intended working. Different types of attacks such as physical\npresentation attacks, disguise/makeup, digital adversarial attacks, and\nmorphing/tampering using GANs have been discussed. We also present a discussion\non the effect of bias on face recognition models and showcase that factors such\nas age and gender variations affect the performance of modern algorithms. The\npaper also presents the potential reasons for these challenges and some of the\nfuture research directions for increasing the robustness of face recognition\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 18:21:59 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Singh", "Richa", ""], ["Agarwal", "Akshay", ""], ["Singh", "Maneet", ""], ["Nagpal", "Shruti", ""], ["Vatsa", "Mayank", ""]]}, {"id": "2002.02949", "submitter": "Priyadarshini Panda", "authors": "Timothy Foldy-Porto, Yeshwanth Venkatesha, and Priyadarshini Panda", "title": "Activation Density driven Energy-Efficient Pruning in Training", "comments": "8 pages, 5 figures, 4 tables (Accepted in ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network pruning with suitable retraining can yield networks with\nconsiderably fewer parameters than the original with comparable degrees of\naccuracy. Typical pruning methods require large, fully trained networks as a\nstarting point from which they perform a time-intensive iterative pruning and\nretraining procedure to regain the original accuracy. We propose a novel\npruning method that prunes a network real-time during training, reducing the\noverall training time to achieve an efficient compressed network. We introduce\nan activation density based analysis to identify the optimal relative sizing or\ncompression for each layer of the network. Our method is architecture agnostic,\nallowing it to be employed on a wide variety of systems. For VGG-19 and\nResNet18 on CIFAR-10, CIFAR-100, and TinyImageNet, we obtain exceedingly sparse\nnetworks (up to $200 \\times$ reduction in parameters and over $60 \\times$\nreduction in inference compute operations in the best case) with accuracy\ncomparable to the baseline network. By reducing the network size periodically\nduring training, we achieve total training times that are shorter than those of\npreviously proposed pruning methods. Furthermore, training compressed networks\nat different epochs with our proposed method yields considerable reduction in\ntraining compute complexity ($1.6\\times$ to $3.2\\times$ lower) at near\niso-accuracy as compared to a baseline network trained entirely from scratch.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 18:34:31 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 12:16:25 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Foldy-Porto", "Timothy", ""], ["Venkatesha", "Yeshwanth", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2002.02957", "submitter": "Yuan-Hang Zhang", "authors": "Yuan-Hang Zhang, Rulin Huang, Jiabei Zeng, Shiguang Shan and Xilin\n  Chen", "title": "$M^3$T: Multi-Modal Continuous Valence-Arousal Estimation in the Wild", "comments": "6 pages, technical report; submission to ABAW Challenge at FG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes a multi-modal multi-task ($M^3$T) approach underlying\nour submission to the valence-arousal estimation track of the Affective\nBehavior Analysis in-the-wild (ABAW) Challenge, held in conjunction with the\nIEEE International Conference on Automatic Face and Gesture Recognition (FG)\n2020. In the proposed $M^3$T framework, we fuse both visual features from\nvideos and acoustic features from the audio tracks to estimate the valence and\narousal. The spatio-temporal visual features are extracted with a 3D\nconvolutional network and a bidirectional recurrent neural network. Considering\nthe correlations between valence / arousal, emotions, and facial actions, we\nalso explores mechanisms to benefit from other tasks. We evaluated the $M^3$T\nframework on the validation set provided by ABAW and it significantly\noutperforms the baseline method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 18:53:13 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Zhang", "Yuan-Hang", ""], ["Huang", "Rulin", ""], ["Zeng", "Jiabei", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2002.02959", "submitter": "Gamaleldin Elsayed", "authors": "Gamaleldin F. Elsayed, Prajit Ramachandran, Jonathon Shlens, Simon\n  Kornblith", "title": "Revisiting Spatial Invariance with Low-Rank Local Connectivity", "comments": null, "journal-ref": "International Conference on Machine Learning, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are among the most successful architectures in\ndeep learning with this success at least partially attributable to the efficacy\nof spatial invariance as an inductive bias. Locally connected layers, which\ndiffer from convolutional layers only in their lack of spatial invariance,\nusually perform poorly in practice. However, these observations still leave\nopen the possibility that some degree of relaxation of spatial invariance may\nyield a better inductive bias than either convolution or local connectivity. To\ntest this hypothesis, we design a method to relax the spatial invariance of a\nnetwork layer in a controlled manner; we create a \\textit{low-rank} locally\nconnected layer, where the filter bank applied at each position is constructed\nas a linear combination of basis set of filter banks with spatially varying\ncombining weights. By varying the number of basis filter banks, we can control\nthe degree of relaxation of spatial invariance. In experiments with small\nconvolutional networks, we find that relaxing spatial invariance improves\nclassification accuracy over both convolution and locally connected layers\nacross MNIST, CIFAR-10, and CelebA datasets, thus suggesting that spatial\ninvariance may be an overly restrictive prior.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 18:56:37 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 20:45:09 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Elsayed", "Gamaleldin F.", ""], ["Ramachandran", "Prajit", ""], ["Shlens", "Jonathon", ""], ["Kornblith", "Simon", ""]]}, {"id": "2002.02997", "submitter": "Sergul Aydore", "authors": "Liyan Chen, Philip Gautier, Sergul Aydore", "title": "DropCluster: A structured dropout for convolutional networks", "comments": "11 pages, 10 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout as a regularizer in deep neural networks has been less effective in\nconvolutional layers than in fully connected layers. This is due to the fact\nthat dropout drops features randomly. When features are spatially correlated as\nin the case of convolutional layers, information about the dropped pixels can\nstill propagate to the next layers via neighboring pixels. In order to address\nthis problem, more structured forms of dropout have been proposed. A drawback\nof these methods is that they do not adapt to the data. In this work, we\nintroduce a novel structured regularization for convolutional layers, which we\ncall DropCluster. Our regularizer relies on data-driven structure. It finds\nclusters of correlated features in convolutional layer outputs and drops the\nclusters randomly at each iteration. The clusters are learned and updated\nduring model training so that they adapt both to the data and to the model\nweights. Our experiments on the ResNet-50 architecture demonstrate that our\napproach achieves better performance than DropBlock or other existing\nstructured dropout variants. We also demonstrate the robustness of our approach\nwhen the size of training data is limited and when there is corruption in the\ndata at test time.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 20:02:47 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Chen", "Liyan", ""], ["Gautier", "Philip", ""], ["Aydore", "Sergul", ""]]}, {"id": "2002.02998", "submitter": "Ting-Wu Chin", "authors": "Ting-Wu Chin, Cha Zhang, Diana Marculescu", "title": "Renofeation: A Simple Transfer Learning Method for Improved Adversarial\n  Robustness", "comments": "2021 IEEE CVPR Workshop on Fair, Data Efficient and Trusted Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning through knowledge transfer from a pre-trained model on a\nlarge-scale dataset is a widely spread approach to effectively build models on\nsmall-scale datasets. In this work, we show that a recent adversarial attack\ndesigned for transfer learning via re-training the last linear layer can\nsuccessfully deceive models trained with transfer learning via end-to-end\nfine-tuning. This raises security concerns for many industrial applications. In\ncontrast, models trained with random initialization without transfer are much\nmore robust to such attacks, although these models often exhibit much lower\naccuracy. To this end, we propose noisy feature distillation, a new transfer\nlearning method that trains a network from random initialization while\nachieving clean-data performance competitive with fine-tuning. Code available\nat https://github.com/cmu-enyac/Renofeation.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 20:07:22 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 14:46:56 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chin", "Ting-Wu", ""], ["Zhang", "Cha", ""], ["Marculescu", "Diana", ""]]}, {"id": "2002.03024", "submitter": "Shane Mueller", "authors": "Shane T. Mueller", "title": "Cognitive Anthropomorphism of AI: How Humans and Computers Classify\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern AI image classifiers have made impressive advances in recent years,\nbut their performance often appears strange or violates expectations of users.\nThis suggests humans engage in cognitive anthropomorphism: expecting AI to have\nthe same nature as human intelligence. This mismatch presents an obstacle to\nappropriate human-AI interaction. To delineate this mismatch, I examine known\nproperties of human classification, in comparison to image classifier systems.\nBased on this examination, I offer three strategies for system design that can\naddress the mismatch between human and AI classification: explainable AI, novel\nmethods for training users, and new algorithms that match human cognition.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 21:49:58 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Mueller", "Shane T.", ""]]}, {"id": "2002.03040", "submitter": "Ricard Durall Lopez", "authors": "Ricard Durall, Franz-Josef Pfreundt, Janis Keuper", "title": "Local Facial Attribute Transfer through Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term attribute transfer refers to the tasks of altering images in such a\nway, that the semantic interpretation of a given input image is shifted towards\nan intended direction, which is quantified by semantic attributes. Prominent\nexample applications are photo realistic changes of facial features and\nexpressions, like changing the hair color, adding a smile, enlarging the nose\nor altering the entire context of a scene, like transforming a summer landscape\ninto a winter panorama. Recent advances in attribute transfer are mostly based\non generative deep neural networks, using various techniques to manipulate\nimages in the latent space of the generator.\n  In this paper, we present a novel method for the common sub-task of local\nattribute transfers, where only parts of a face have to be altered in order to\nachieve semantic changes (e.g. removing a mustache). In contrast to previous\nmethods, where such local changes have been implemented by generating new\n(global) images, we propose to formulate local attribute transfers as an\ninpainting problem. Removing and regenerating only parts of images, our\nAttribute Transfer Inpainting Generative Adversarial Network (ATI-GAN) is able\nto utilize local context information to focus on the attributes while keeping\nthe background unmodified resulting in visually sound results.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 22:57:01 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 09:07:54 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Durall", "Ricard", ""], ["Pfreundt", "Franz-Josef", ""], ["Keuper", "Janis", ""]]}, {"id": "2002.03053", "submitter": "Tuomo Valkonen", "authors": "Tuomo Valkonen", "title": "Predictive online optimisation with applications to optical flow", "comments": null, "journal-ref": "Journal of Mathematical Imaging and Vision (2021)", "doi": "10.1007/s10851-020-01000-4", "report-no": null, "categories": "math.OC cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online optimisation revolves around new data being introduced into a problem\nwhile it is still being solved; think of deep learning as more training samples\nbecome available. We adapt the idea to dynamic inverse problems such as video\nprocessing with optical flow. We introduce a corresponding predictive online\nprimal-dual proximal splitting method. The video frames now exactly correspond\nto the algorithm iterations. A user-prescribed predictor describes the\nevolution of the primal variable. To prove convergence we need a predictor for\nthe dual variable based on (proximal) gradient flow. This affects the model\nthat the method asymptotically minimises. We show that for inverse problems the\neffect is, essentially, to construct a new dynamic regulariser based on infimal\nconvolution of the static regularisers with the temporal coupling. We finish by\ndemonstrating excellent real-time performance of our method in computational\nimage stabilisation and convergence in terms of regularisation theory.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 00:21:01 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 17:50:32 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Valkonen", "Tuomo", ""]]}, {"id": "2002.03073", "submitter": "Yuxing Tang", "authors": "Jia Liang, Yuxing Tang, Youbao Tang, Jing Xiao, Ronald M. Summers", "title": "Bone Suppression on Chest Radiographs With Adversarial Learning", "comments": "Accepted by SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual-energy (DE) chest radiography provides the capability of selectively\nimaging two clinically relevant materials, namely soft tissues, and osseous\nstructures, to better characterize a wide variety of thoracic pathology and\npotentially improve diagnosis in posteroanterior (PA) chest radiographs.\nHowever, DE imaging requires specialized hardware and a higher radiation dose\nthan conventional radiography, and motion artifacts sometimes happen due to\ninvoluntary patient motion. In this work, we learn the mapping between\nconventional radiographs and bone suppressed radiographs. Specifically, we\npropose to utilize two variations of generative adversarial networks (GANs) for\nimage-to-image translation between conventional and bone suppressed radiographs\nobtained by DE imaging technique. We compare the effectiveness of training with\npatient-wisely paired and unpaired radiographs. Experiments show both training\nstrategies yield \"radio-realistic'' radiographs with suppressed bony structures\nand few motion artifacts on a hold-out test set. While training with paired\nimages yields slightly better performance than that of unpaired images when\nmeasuring with two objective image quality metrics, namely Structural\nSimilarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), training with\nunpaired images demonstrates better generalization ability on unseen\nanteroposterior (AP) radiographs than paired training.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 02:53:16 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liang", "Jia", ""], ["Tang", "Yuxing", ""], ["Tang", "Youbao", ""], ["Xiao", "Jing", ""], ["Summers", "Ronald M.", ""]]}, {"id": "2002.03080", "submitter": "Adam Dziedzic", "authors": "Adam Dziedzic, Sanjay Krishnan", "title": "Analysis of Random Perturbations for Robust Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has extensively shown that randomized perturbations of neural\nnetworks can improve robustness to adversarial attacks. The literature is,\nhowever, lacking a detailed compare-and-contrast of the latest proposals to\nunderstand what classes of perturbations work, when they work, and why they\nwork. We contribute a detailed evaluation that elucidates these questions and\nbenchmarks perturbation based defenses consistently. In particular, we show\nfive main results: (1) all input perturbation defenses, whether random or\ndeterministic, are equivalent in their efficacy, (2) attacks transfer between\nperturbation defenses so the attackers need not know the specific type of\ndefense -- only that it involves perturbations, (3) a tuned sequence of noise\nlayers across a network provides the best empirical robustness, (4)\nperturbation based defenses offer almost no robustness to adaptive attacks\nunless these perturbations are observed during training, and (5) adversarial\nexamples in a close neighborhood of original inputs show an elevated\nsensitivity to perturbations in first and second-order analyses.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 03:46:07 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 00:22:31 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 19:56:06 GMT"}, {"version": "v4", "created": "Sun, 7 Jun 2020 19:25:31 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Dziedzic", "Adam", ""], ["Krishnan", "Sanjay", ""]]}, {"id": "2002.03095", "submitter": "Lu Chen", "authors": "Lu Chen and Wei Xu", "title": "Attacking Optical Character Recognition (OCR) Systems with Adversarial\n  Watermarks", "comments": "9 pages, this http url http://aics.site/AICS2020/AICS20_paper_18.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical character recognition (OCR) is widely applied in real applications\nserving as a key preprocessing tool. The adoption of deep neural network (DNN)\nin OCR results in the vulnerability against adversarial examples which are\ncrafted to mislead the output of the threat model. Different from vanilla\ncolorful images, images of printed text have clear backgrounds usually.\nHowever, adversarial examples generated by most of the existing adversarial\nattacks are unnatural and pollute the background severely. To address this\nissue, we propose a watermark attack method to produce natural distortion that\nis in the disguise of watermarks and evade human eyes' detection. Experimental\nresults show that watermark attacks can yield a set of natural adversarial\nexamples attached with watermarks and attain similar attack performance to the\nstate-of-the-art methods in different attack scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 05:53:21 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Chen", "Lu", ""], ["Xu", "Wei", ""]]}, {"id": "2002.03125", "submitter": "Prajoy Podder", "authors": "Prajoy Podder, A.H.M Shahariar Parvez, Md. Mizanur Rahman, Tanvir\n  Zaman Khan", "title": "Ramifications and Diminution of Image Noise in Iris Recognition System", "comments": "Proceeding of 2018 IEEE International Conference on Current Trends\n  toward Converging Technologies, Coimbatore, India", "journal-ref": "Published in 2018 IEEE International Conference on Current Trends\n  toward Converging Technologies, Coimbatore, India", "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human Identity verification has always been an eye-catching goal in digital\nbased security system. Authentication or identification systems developed using\nhuman characteristics such as face, finger print, hand geometry, iris, and\nvoice are denoted as biometric systems. Among the various characteristics, Iris\nrecognition trusts on the idiosyncratic human iris patterns to find out and\ncorroborate the identity of a person. The image is normally contemplated as a\ngathering of information. Existence of noises in the input or processed image\neffects degradation in the image superiority. It should be paramount to restore\noriginal image from noises for attaining maximum amount of information from\ncorrupted images. Noisy images in biometric identification system cannot give\naccurate identity. So Image related data or information tends to loss or\ndamage. Images are affected by various sorts of noises. This paper mainly\nfocuses on Salt and Pepper noise, Gaussian noise, Uniform noise, Speckle noise.\nDifferent filtering techniques can be adapted for noise diminution to develop\nthe visual quality as well as understandability of images. In this paper, four\ntypes of noises have been undertaken and applied on some images. The filtering\nof these noises uses different types of filters like Mean, Median, Weiner,\nGaussian filter etc. A relative interpretation is performed using four\ndifferent categories of filter with finding the value of quality determined\nparameters like mean square error (MSE), peak signal to noise ratio (PSNR),\naverage difference value (AD) and maximum difference value (MD).\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 09:06:20 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Podder", "Prajoy", ""], ["Parvez", "A. H. M Shahariar", ""], ["Rahman", "Md. Mizanur", ""], ["Khan", "Tanvir Zaman", ""]]}, {"id": "2002.03131", "submitter": "Tengyu Ma", "authors": "Tengyu Ma, Joel Michelson, James Ainooson, Deepayan Sanyal, Xiaohan\n  Wang, Maithilee Kunda", "title": "Variable-Viewpoint Representations for 3D Object Recognition", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the problem of 3D object recognition, researchers using deep learning\nmethods have developed several very different input representations, including\n\"multi-view\" snapshots taken from discrete viewpoints around an object, as well\nas \"spherical\" representations consisting of a dense map of essentially\nray-traced samples of the object from all directions. These representations\noffer trade-offs in terms of what object information is captured and to what\ndegree of detail it is captured, but it is not clear how to measure these\ninformation trade-offs since the two types of representations are so different.\nWe demonstrate that both types of representations in fact exist at two extremes\nof a common representational continuum, essentially choosing to prioritize\neither the number of views of an object or the pixels (i.e., field of view)\nallotted per view. We identify interesting intermediate representations that\nlie at points in between these two extremes, and we show, through systematic\nempirical experiments, how accuracy varies along this continuum as a function\nof input information as well as the particular deep learning architecture that\nis used.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 10:06:30 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ma", "Tengyu", ""], ["Michelson", "Joel", ""], ["Ainooson", "James", ""], ["Sanyal", "Deepayan", ""], ["Wang", "Xiaohan", ""], ["Kunda", "Maithilee", ""]]}, {"id": "2002.03137", "submitter": "Xiaohan Wang", "authors": "Xiaohan Wang, Yu Wu, Linchao Zhu, Yi Yang", "title": "Symbiotic Attention with Privileged Information for Egocentric Action\n  Recognition", "comments": "AAAI-2020(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric video recognition is a natural testbed for diverse interaction\nreasoning. Due to the large action vocabulary in egocentric video datasets,\nrecent studies usually utilize a two-branch structure for action recognition,\nie, one branch for verb classification and the other branch for noun\nclassification. However, correlation studies between the verb and the noun\nbranches have been largely ignored. Besides, the two branches fail to exploit\nlocal features due to the absence of a position-aware attention mechanism. In\nthis paper, we propose a novel Symbiotic Attention framework leveraging\nPrivileged information (SAP) for egocentric video recognition. Finer\nposition-aware object detection features can facilitate the understanding of\nactor's interaction with the object. We introduce these features in action\nrecognition and regard them as privileged information. Our framework enables\nmutual communication among the verb branch, the noun branch, and the privileged\ninformation. This communication process not only injects local details into\nglobal features but also exploits implicit guidance about the spatio-temporal\nposition of an on-going action. We introduce novel symbiotic attention (SA) to\nenable effective communication. It first normalizes the detection guided\nfeatures on one branch to underline the action-relevant information from the\nother branch. SA adaptively enhances the interactions among the three sources.\nTo further catalyze this communication, spatial relations are uncovered for the\nselection of most action-relevant information. It identifies the most valuable\nand discriminative feature for classification. We validate the effectiveness of\nour SAP quantitatively and qualitatively. Notably, it achieves the\nstate-of-the-art on two large-scale egocentric video datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 10:48:43 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wang", "Xiaohan", ""], ["Wu", "Yu", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "2002.03138", "submitter": "Xiaodong Liu", "authors": "Hongwu Kuang, Xiaodong Liu, Jingwei Zhang, Zicheng Fang", "title": "Multi-Modality Cascaded Fusion Technology for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modality fusion is the guarantee of the stability of autonomous driving\nsystems. In this paper, we propose a general multi-modality cascaded fusion\nframework, exploiting the advantages of decision-level and feature-level\nfusion, utilizing target position, size, velocity, appearance and confidence to\nachieve accurate fusion results. In the fusion process, dynamic coordinate\nalignment(DCA) is conducted to reduce the error between sensors from different\nmodalities. In addition, the calculation of affinity matrix is the core module\nof sensor fusion, we propose an affinity loss that improves the performance of\ndeep affinity network(DAN). Last, the proposed step-by-step cascaded fusion\nframework is more interpretable and flexible compared to the end-toend fusion\nmethods. Extensive experiments on Nuscenes [2] dataset show that our approach\nachieves the state-of-theart performance.dataset show that our approach\nachieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 10:59:18 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Kuang", "Hongwu", ""], ["Liu", "Xiaodong", ""], ["Zhang", "Jingwei", ""], ["Fang", "Zicheng", ""]]}, {"id": "2002.03152", "submitter": "Qian Liu", "authors": "Qian Liu, Tao Wang, Jie Liu, Yang Guan, Qi Bu, Longfei Yang", "title": "CTM: Collaborative Temporal Modeling for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of digital multimedia, video understanding has\nbecome an important field. For action recognition, temporal dimension plays an\nimportant role, and this is quite different from image recognition. In order to\nlearn powerful feature of videos, we propose a Collaborative Temporal Modeling\n(CTM) block (Figure 1) to learn temporal information for action recognition.\nBesides a parameter-free identity shortcut, as a separate temporal modeling\nblock, CTM includes two collaborative paths: a spatial-aware temporal modeling\npath, which we propose the Temporal-Channel Convolution Module (TCCM) with\nunshared parameters for each spatial position (H*W) to build, and a\nspatial-unaware temporal modeling path. CTM blocks can seamlessly be inserted\ninto many popular networks to generate CTM Networks and bring the capability of\nlearning temporal information to 2D CNN backbone networks, which only capture\nspatial information. Experiments on several popular action recognition datasets\ndemonstrate that CTM blocks bring the performance improvements on 2D CNN\nbaselines, and our method achieves the competitive results against the\nstate-of-the-art methods. Code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 12:14:02 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liu", "Qian", ""], ["Wang", "Tao", ""], ["Liu", "Jie", ""], ["Guan", "Yang", ""], ["Bu", "Qi", ""], ["Yang", "Longfei", ""]]}, {"id": "2002.03157", "submitter": "Muzammil Behzad", "authors": "Muzammil Behzad, Nhat Vo, Xiaobai Li, Guoying Zhao", "title": "Towards Reading Beyond Faces for Sparsity-Aware 4D Affect Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a sparsity-aware deep network for automatic 4D\nfacial expression recognition (FER). Given 4D data, we first propose a novel\naugmentation method to combat the data limitation problem for deep learning.\nThis is achieved by projecting the input data into RGB and depth map images and\nthen iteratively performing randomized channel concatenation. Encoded in the\ngiven 3D landmarks, we also introduce an effective way to capture the facial\nmuscle movements from three orthogonal plans (TOP), the TOP-landmarks over\nmulti-views. Importantly, we then present a sparsity-aware deep network to\ncompute the sparse representations of convolutional features over multi-views.\nThis is not only effective for a higher recognition accuracy but is also\ncomputationally convenient. For training, the TOP-landmarks and sparse\nrepresentations are used to train a long short-term memory (LSTM) network. The\nrefined predictions are achieved when the learned features collaborate over\nmulti-views. Extensive experimental results achieved on the BU-4DFE dataset\nshow the significance of our method over the state-of-the-art methods by\nreaching a promising accuracy of 99.69% for 4D FER.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 13:09:11 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 23:42:00 GMT"}, {"version": "v3", "created": "Sun, 21 Jun 2020 09:56:36 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2020 11:02:47 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Behzad", "Muzammil", ""], ["Vo", "Nhat", ""], ["Li", "Xiaobai", ""], ["Zhao", "Guoying", ""]]}, {"id": "2002.03165", "submitter": "Chandra Sekhar Ravuri", "authors": "Chandra Sekhar Ravuri (1), Rajesh Sureddi (2), Sathya Veera Reddy\n  Dendi (2), Shanmuganathan Raman (1), Sumohana S. Channappayya (2) ((1)\n  Department of Electrical Engineering, Indian Institute of Technology\n  Gandhinagar, India., (2) Department of Electrical Engineering, Indian\n  Institute of Technology Hyderabad, India.)", "title": "Deep No-reference Tone Mapped Image Quality Assessment", "comments": "5 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of rendering high dynamic range (HDR) images to be viewed on\nconventional displays is called tone mapping. However, tone mapping introduces\ndistortions in the final image which may lead to visual displeasure. To\nquantify these distortions, we introduce a novel no-reference quality\nassessment technique for these tone mapped images. This technique is composed\nof two stages. In the first stage, we employ a convolutional neural network\n(CNN) to generate quality aware maps (also known as distortion maps) from tone\nmapped images by training it with the ground truth distortion maps. In the\nsecond stage, we model the normalized image and distortion maps using an\nAsymmetric Generalized Gaussian Distribution (AGGD). The parameters of the AGGD\nmodel are then used to estimate the quality score using support vector\nregression (SVR). We show that the proposed technique delivers competitive\nperformance relative to the state-of-the-art techniques. The novelty of this\nwork is its ability to visualize various distortions as quality maps\n(distortion maps), especially in the no-reference setting, and to use these\nmaps as features to estimate the quality score of tone mapped images.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 13:41:18 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ravuri", "Chandra Sekhar", ""], ["Sureddi", "Rajesh", ""], ["Dendi", "Sathya Veera Reddy", ""], ["Raman", "Shanmuganathan", ""], ["Channappayya", "Sumohana S.", ""]]}, {"id": "2002.03187", "submitter": "Hao Zhou", "authors": "Hao Zhou, Wengang Zhou, Yun Zhou, Houqiang Li", "title": "Spatial-Temporal Multi-Cue Network for Continuous Sign Language\n  Recognition", "comments": "Accepted as an oral presentation paper at AAAI 2020. (To appear in\n  Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of deep learning in continuous sign language\nrecognition (CSLR), deep models typically focus on the most discriminative\nfeatures, ignoring other potentially non-trivial and informative contents. Such\ncharacteristic heavily constrains their capability to learn implicit visual\ngrammars behind the collaboration of different visual cues (i,e., hand shape,\nfacial expression and body posture). By injecting multi-cue learning into\nneural network design, we propose a spatial-temporal multi-cue (STMC) network\nto solve the vision-based sequence learning problem. Our STMC network consists\nof a spatial multi-cue (SMC) module and a temporal multi-cue (TMC) module. The\nSMC module is dedicated to spatial representation and explicitly decomposes\nvisual features of different cues with the aid of a self-contained pose\nestimation branch. The TMC module models temporal correlations along two\nparallel paths, i.e., intra-cue and inter-cue, which aims to preserve the\nuniqueness and explore the collaboration of multiple cues. Finally, we design a\njoint optimization strategy to achieve the end-to-end sequence learning of the\nSTMC network. To validate the effectiveness, we perform experiments on three\nlarge-scale CSLR benchmarks: PHOENIX-2014, CSL and PHOENIX-2014-T. Experimental\nresults demonstrate that the proposed method achieves new state-of-the-art\nperformance on all three benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 15:38:44 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhou", "Hao", ""], ["Zhou", "Wengang", ""], ["Zhou", "Yun", ""], ["Li", "Houqiang", ""]]}, {"id": "2002.03196", "submitter": "Benjamin Cecchetto T", "authors": "Benjamin T. Cecchetto", "title": "Correction of Chromatic Aberration from a Single Image Using Keypoints", "comments": "Originally this paper was a project for a course in 2009 and has not\n  been published. It has been cited multiple times since then. The LaTeX code\n  was lost, so it has been revised in February 2020 to post on ArXiV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to correct for chromatic aberration in a\nsingle photograph. Our method replicates what a user would do in a photo\nediting program to account for this defect. We find matching keypoints in each\ncolour channel then align them as a user would.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 16:36:30 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Cecchetto", "Benjamin T.", ""]]}, {"id": "2002.03219", "submitter": "Gaowen Liu", "authors": "Gaowen Liu, Hao Tang, Hugo Latapie, Yan Yan", "title": "Exocentric to Egocentric Image Generation via Parallel Generative\n  Adversarial Network", "comments": "It has been accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view image generation has been recently proposed to generate images of\none view from another dramatically different view. In this paper, we\ninvestigate exocentric (third-person) view to egocentric (first-person) view\nimage generation. This is a challenging task since egocentric view sometimes is\nremarkably different from exocentric view. Thus, transforming the appearances\nacross the two views is a non-trivial task. To this end, we propose a novel\nParallel Generative Adversarial Network (P-GAN) with a novel cross-cycle loss\nto learn the shared information for generating egocentric images from\nexocentric view. We also incorporate a novel contextual feature loss in the\nlearning procedure to capture the contextual information in images. Extensive\nexperiments on the Exo-Ego datasets show that our model outperforms the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 19:10:36 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liu", "Gaowen", ""], ["Tang", "Hao", ""], ["Latapie", "Hugo", ""], ["Yan", "Yan", ""]]}, {"id": "2002.03226", "submitter": "Rizwan Ahmad", "authors": "Sizhuo Liu, Edward Reehorst, Philip Schniter, and Rizwan Ahmad", "title": "Free-breathing Cardiovascular MRI Using a Plug-and-Play Method with\n  Learned Denoiser", "comments": "IEEE ISBI 2020, International Symposium on Biomedical Imaging", "journal-ref": null, "doi": "10.1109/ISBI45749.2020.9098453", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac magnetic resonance imaging (CMR) is a noninvasive imaging modality\nthat provides a comprehensive evaluation of the cardiovascular system. The\nclinical utility of CMR is hampered by long acquisition times, however. In this\nwork, we propose and validate a plug-and-play (PnP) method for CMR\nreconstruction from undersampled multi-coil data. To fully exploit the rich\nimage structure inherent in CMR, we pair the PnP framework with a deep learning\n(DL)-based denoiser that is trained using spatiotemporal patches from\nhigh-quality, breath-held cardiac cine images. The resulting \"PnP-DL\" method\niterates over data consistency and denoising subroutines. We compare the\nreconstruction performance of PnP-DL to that of compressed sensing (CS) using\neight breath-held and ten real-time (RT) free-breathing cardiac cine datasets.\nWe find that, for breath-held datasets, PnP-DL offers more than one dB\nadvantage over commonly used CS methods. For RT free-breathing datasets, where\nground truth is not available, PnP-DL receives higher scores in qualitative\nevaluation. The results highlight the potential of PnP-DL to accelerate RT CMR.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 20:27:07 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Liu", "Sizhuo", ""], ["Reehorst", "Edward", ""], ["Schniter", "Philip", ""], ["Ahmad", "Rizwan", ""]]}, {"id": "2002.03228", "submitter": "Liang Liao", "authors": "Liang Liao and Stephen John Maybank", "title": "Intrinsic Dimension Estimation via Nearest Constrained Subspace\n  Classifier", "comments": "19 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of classification and intrinsic dimension estimation\non image data. A new subspace based classifier is proposed for supervised\nclassification or intrinsic dimension estimation. The distribution of the data\nin each class is modeled by a union of of a finite number ofaffine subspaces of\nthe feature space. The affine subspaces have a common dimension, which is\nassumed to be much less than the dimension of the feature space. The subspaces\nare found using regression based on the L0-norm. The proposed method is a\ngeneralisation of classical NN (Nearest Neighbor), NFL (Nearest Feature Line)\nclassifiers and has a close relationship to NS (Nearest Subspace) classifier.\nThe proposed classifier with an accurately estimated dimension parameter\ngenerally outperforms its competitors in terms of classification accuracy. We\nalso propose a fast version of the classifier using a neighborhood\nrepresentation to reduce its computational complexity. Experiments on publicly\navailable datasets corroborate these claims.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 20:54:42 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liao", "Liang", ""], ["Maybank", "Stephen John", ""]]}, {"id": "2002.03231", "submitter": "Aditya Kusupati", "authors": "Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman,\n  Prateek Jain, Sham Kakade, Ali Farhadi", "title": "Soft Threshold Weight Reparameterization for Learnable Sparsity", "comments": "19 pages, 10 figures, Published at International Conference on\n  Machine Learning (ICML) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus\nof maximizing prediction accuracy given an overall parameter budget. Existing\nmethods rely on uniform or heuristic non-uniform sparsity budgets which have\nsub-optimal layer-wise parameter allocation resulting in a) lower prediction\naccuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold\nReparameterization (STR), a novel use of the soft-threshold operator on DNN\nweights. STR smoothly induces sparsity while learning pruning thresholds\nthereby obtaining a non-uniform sparsity budget. Our method achieves\nstate-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and\nMobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that\nempirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy\nover existing results by up to 10% in the ultra sparse (99%) regime and can\nalso be used to induce low-rank (structured sparsity) in RNNs. In short, STR is\na simple mechanism which learns effective sparsity budgets that contrast with\npopular heuristics. Code, pretrained models and sparsity budgets are at\nhttps://github.com/RAIVNLab/STR.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 21:31:25 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 22:57:06 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 10:11:37 GMT"}, {"version": "v4", "created": "Wed, 11 Mar 2020 09:57:20 GMT"}, {"version": "v5", "created": "Fri, 17 Apr 2020 12:57:04 GMT"}, {"version": "v6", "created": "Wed, 22 Apr 2020 02:16:28 GMT"}, {"version": "v7", "created": "Tue, 28 Apr 2020 01:12:37 GMT"}, {"version": "v8", "created": "Sun, 10 May 2020 02:39:39 GMT"}, {"version": "v9", "created": "Mon, 22 Jun 2020 23:37:12 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kusupati", "Aditya", ""], ["Ramanujan", "Vivek", ""], ["Somani", "Raghav", ""], ["Wortsman", "Mitchell", ""], ["Jain", "Prateek", ""], ["Kakade", "Sham", ""], ["Farhadi", "Ali", ""]]}, {"id": "2002.03238", "submitter": "Ines Rieger", "authors": "Jaspar Pahl, Ines Rieger, Dominik Seuss", "title": "Multi-Label Class Balancing Algorithm for Action Unit Detection", "comments": "This submission is subject to the Action Unit detection task of the\n  Affective Behavior Analysis in-the-wild (ABAW) challenge at the IEEE\n  Conference on Face and Gesture Recognition 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isolated facial movements, so-called Action Units, can describe combined\nemotions or physical states such as pain. As datasets are limited and mostly\nimbalanced, we present an approach incorporating a multi-label class balancing\nalgorithm. This submission is subject to the Action Unit detection task of the\nAffective Behavior Analysis in-the-wild (ABAW) challenge at the IEEE Conference\non Face and Gesture Recognition 2020.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 21:56:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Pahl", "Jaspar", ""], ["Rieger", "Ines", ""], ["Seuss", "Dominik", ""]]}, {"id": "2002.03241", "submitter": "Chong Li", "authors": "Zhun Fan, Chong Li, Ying Chen, Paola Di Mascio, Xiaopeng Chen, Guijie\n  Zhu and Giuseppe Loprencipe", "title": "Ensemble of Deep Convolutional Neural Networks for Automatic Pavement\n  Crack Detection and Measurement", "comments": null, "journal-ref": null, "doi": "10.3390/coatings10020152", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated pavement crack detection and measurement are important road issues.\nAgencies have to guarantee the improvement of road safety. Conventional crack\ndetection and measurement algorithms can be extremely time-consuming and low\nefficiency. Therefore, recently, innovative algorithms have received increased\nattention from researchers. In this paper, we propose an ensemble of\nconvolutional neural networks (without a pooling layer) based on probability\nfusion for automated pavement crack detection and measurement. Specifically, an\nensemble of convolutional neural networks was employed to identify the\nstructure of small cracks with raw images. Secondly, outputs of the individual\nconvolutional neural network model for the ensemble were averaged to produce\nthe final crack probability value of each pixel, which can obtain a predicted\nprobability map. Finally, the predicted morphological features of the cracks\nwere measured by using the skeleton extraction algorithm. To validate the\nproposed method, some experiments were performed on two public crack databases\n(CFD and AigleRN) and the results of the different state-of-the-art methods\nwere compared. The experimental results show that the proposed method\noutperforms the other methods. For crack measurement, the crack length and\nwidth can be measure based on different crack types (complex, common, thin, and\nintersecting cracks.). The results show that the proposed algorithm can be\neffectively applied for crack measurement.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 22:15:11 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Fan", "Zhun", ""], ["Li", "Chong", ""], ["Chen", "Ying", ""], ["Di Mascio", "Paola", ""], ["Chen", "Xiaopeng", ""], ["Zhu", "Guijie", ""], ["Loprencipe", "Giuseppe", ""]]}, {"id": "2002.03264", "submitter": "Junnan Li Mr", "authors": "Junnan Li, Ziwei Xu, Yongkang Wong, Qi Zhao, Mohan Kankanhalli", "title": "GradMix: Multi-source Transfer across Domains and Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer vision community is witnessing an unprecedented rate of new\ntasks being proposed and addressed, thanks to the deep convolutional networks'\ncapability to find complex mappings from X to Y. The advent of each task often\naccompanies the release of a large-scale annotated dataset, for supervised\ntraining of deep network. However, it is expensive and time-consuming to\nmanually label sufficient amount of training data. Therefore, it is important\nto develop algorithms that can leverage off-the-shelf labeled dataset to learn\nuseful knowledge for the target task. While previous works mostly focus on\ntransfer learning from a single source, we study multi-source transfer across\ndomains and tasks (MS-DTT), in a semi-supervised setting. We propose GradMix, a\nmodel-agnostic method applicable to any model trained with gradient-based\nlearning rule, to transfer knowledge via gradient descent by weighting and\nmixing the gradients from all sources during training. GradMix follows a\nmeta-learning objective, which assigns layer-wise weights to the source\ngradients, such that the combined gradient follows the direction that minimize\nthe loss for a small set of samples from the target dataset. In addition, we\npropose to adaptively adjust the learning rate for each mini-batch based on its\nimportance to the target task, and a pseudo-labeling method to leverage the\nunlabeled samples in the target domain. We conduct MS-DTT experiments on two\ntasks: digit recognition and action recognition, and demonstrate the\nadvantageous performance of the proposed method against multiple baselines.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 02:10:22 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Li", "Junnan", ""], ["Xu", "Ziwei", ""], ["Wong", "Yongkang", ""], ["Zhao", "Qi", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "2002.03266", "submitter": "Junnan Li Dr", "authors": "Junnan Li, Jianquan Liu, Yongkang Wong, Shoji Nishimura, Mohan\n  Kankanhalli", "title": "Weakly-Supervised Multi-Person Action Recognition in 360$^{\\circ}$\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of commodity 360$^{\\circ}$ cameras have enabled a\nsingle video to capture an entire scene, which endows promising potentials in\nsurveillance scenarios. However, research in omnidirectional video analysis has\nlagged behind the hardware advances. In this work, we address the important\nproblem of action recognition in top-view 360$^{\\circ}$ videos. Due to the wide\nfiled-of-view, 360$^{\\circ}$ videos usually capture multiple people performing\nactions at the same time. Furthermore, the appearance of people are deformed.\nThe proposed framework first transforms omnidirectional videos into panoramic\nvideos, then it extracts spatial-temporal features using region-based 3D CNNs\nfor action recognition. We propose a weakly-supervised method based on\nmulti-instance multi-label learning, which trains the model to recognize and\nlocalize multiple actions in a video using only video-level action labels as\nsupervision. We perform experiments to quantitatively validate the efficacy of\nthe proposed method and qualitatively demonstrate action localization results.\nTo enable research in this direction, we introduce 360Action, the first\nomnidirectional video dataset for multi-person action recognition.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 02:17:46 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Li", "Junnan", ""], ["Liu", "Jianquan", ""], ["Wong", "Yongkang", ""], ["Nishimura", "Shoji", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "2002.03271", "submitter": "He-Feng Yin", "authors": "Zi-Qi Li, Jun Sun, Xiao-Jun Wu and He-Feng Yin", "title": "Learning efficient structured dictionary for image classification", "comments": "Journal of Electronic Imaging (major revision)", "journal-ref": null, "doi": "10.1117/1.JEI.29.3.033019", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the success of dictionary learning (DL) based\napproaches in the domain of pattern classification. In this paper, we present\nan efficient structured dictionary learning (ESDL) method which takes both the\ndiversity and label information of training samples into account. Specifically,\nESDL introduces alternative training samples into the process of dictionary\nlearning. To increase the discriminative capability of representation\ncoefficients for classification, an ideal regularization term is incorporated\ninto the objective function of ESDL. Moreover, in contrast with conventional DL\napproaches which impose computationally expensive L1-norm constraint on the\ncoefficient matrix, ESDL employs L2-norm regularization term. Experimental\nresults on benchmark databases (including four face databases and one scene\ndataset) demonstrate that ESDL outperforms previous DL approaches. More\nimportantly, ESDL can be applied in a wide range of pattern classification\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 03:12:49 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 01:58:51 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Zi-Qi", ""], ["Sun", "Jun", ""], ["Wu", "Xiao-Jun", ""], ["Yin", "He-Feng", ""]]}, {"id": "2002.03276", "submitter": "Haoyu Qin", "authors": "Haoyu Qin", "title": "Asymmetric Rejection Loss for Fairer Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition performance has seen a tremendous gain in recent years,\nmostly due to the availability of large-scale face images dataset that can be\nexploited by deep neural networks to learn powerful face representations.\nHowever, recent research has shown differences in face recognition performance\nacross different ethnic groups mostly due to the racial imbalance in the\ntraining datasets where Caucasian identities largely dominate other\nethnicities. This is actually symptomatic of the under-representation of\nnon-Caucasian ethnic groups in the celebdom from which face datasets are\nusually gathered, rendering the acquisition of labeled data of the\nunder-represented groups challenging. In this paper, we propose an Asymmetric\nRejection Loss, which aims at making full use of unlabeled images of those\nunder-represented groups, to reduce the racial bias of face recognition models.\nWe view each unlabeled image as a unique class, however as we cannot guarantee\nthat two unlabeled samples are from a distinct class we exploit both labeled\nand unlabeled data in an asymmetric manner in our loss formalism. Extensive\nexperiments show our method's strength in mitigating racial bias, outperforming\nstate-of-the-art semi-supervision methods. Performance on the under-represented\nethnicity groups increases while that on the well-represented group is nearly\nunchanged.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 04:01:03 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Qin", "Haoyu", ""]]}, {"id": "2002.03281", "submitter": "Min Zhang", "authors": "Min Zhang, Yifan Wang, Pranav Kadam, Shan Liu and C.-C. Jay Kuo", "title": "PointHop++: A Lightweight Learning Model on Point Sets for 3D\n  Classification", "comments": "4pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PointHop method was recently proposed by Zhang et al. for 3D point cloud\nclassification with unsupervised feature extraction. It has an extremely low\ntraining complexity while achieving state-of-the-art classification\nperformance. In this work, we improve the PointHop method furthermore in two\naspects: 1) reducing its model complexity in terms of the model parameter\nnumber and 2) ordering discriminant features automatically based on the\ncross-entropy criterion. The resulting method is called PointHop++. The first\nimprovement is essential for wearable and mobile computing while the second\nimprovement bridges statistics-based and optimization-based machine learning\nmethodologies. With experiments conducted on the ModelNet40 benchmark dataset,\nwe show that the PointHop++ method performs on par with deep neural network\n(DNN) solutions and surpasses other unsupervised feature extraction methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 04:49:32 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 03:56:54 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zhang", "Min", ""], ["Wang", "Yifan", ""], ["Kadam", "Pranav", ""], ["Liu", "Shan", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2002.03299", "submitter": "Shadrokh Samavi", "authors": "Morteza Mousa-Pasandi, Mohsen Hajabdollahi, Nader Karimi, Shadrokh\n  Samavi, Shahram Shirani", "title": "Convolutional Neural Network Pruning Using Filter Attenuation", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filters are the essential elements in convolutional neural networks (CNNs).\nFilters are corresponded to the feature maps and form the main part of the\ncomputational and memory requirement for the CNN processing. In filter pruning\nmethods, a filter with all of its components, including channels and\nconnections, are removed. The removal of a filter can cause a drastic change in\nthe network's performance. Also, the removed filters cannot come back to the\nnetwork structure. We want to address these problems in this paper. We propose\na CNN pruning method based on filter attenuation in which weak filters are not\ndirectly removed. Instead, weak filters are attenuated and gradually removed.\nIn the proposed attenuation approach, weak filters are not abruptly removed,\nand there is a chance for these filters to return to the network. The filter\nattenuation method is assessed using the VGG model for the Cifar10 image\nclassification task. Simulation results show that the filter attenuation works\nwith different pruning criteria, and better results are obtained in comparison\nwith the conventional pruning methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 06:31:24 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Mousa-Pasandi", "Morteza", ""], ["Hajabdollahi", "Mohsen", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Shirani", "Shahram", ""]]}, {"id": "2002.03302", "submitter": "Shadrokh Samavi", "authors": "Emad MalekHosseini, Mohsen Hajabdollahi, Nader Karimi, Shadrokh\n  Samavi, Shahram Shirani", "title": "Splitting Convolutional Neural Network Structures for Efficient\n  Inference", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For convolutional neural networks (CNNs) that have a large volume of input\ndata, memory management becomes a major concern. Memory cost reduction can be\nan effective way to deal with these problems that can be realized through\ndifferent techniques such as feature map pruning, input data splitting, etc.\nAmong various methods existing in this area of research, splitting the network\nstructure is an interesting research field, and there are a few works done in\nthis area. In this study, the problem of reducing memory utilization using\nnetwork structure splitting is addressed. A new technique is proposed to split\nthe network structure into small parts that consume lower memory than the\noriginal network. The split parts can be processed almost separately, which\nprovides an essential role for better memory management. The split approach has\nbeen tested on two well-known network structures of VGG16 and ResNet18 for the\nclassification of CIFAR10 images. Simulation results show that the splitting\nmethod reduces both the number of computational operations as well as the\namount of memory consumption.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 06:53:18 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["MalekHosseini", "Emad", ""], ["Hajabdollahi", "Mohsen", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Shirani", "Shahram", ""]]}, {"id": "2002.03308", "submitter": "Yang Zhang", "authors": "Yang Zhang, Ivor W.Tsang, Jun Li, Ping Liu, Xiaobo Lu, and Xin Yu", "title": "Face Hallucination with Finishing Touches", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": "10.1109/TIP.2020.3046918", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining a high-quality frontal face image from a low-resolution (LR)\nnon-frontal face image is primarily important for many facial analysis\napplications. However, mainstreams either focus on super-resolving near-frontal\nLR faces or frontalizing non-frontal high-resolution (HR) faces. It is\ndesirable to perform both tasks seamlessly for daily-life unconstrained face\nimages. In this paper, we present a novel Vivid Face Hallucination Generative\nAdversarial Network (VividGAN) for simultaneously super-resolving and\nfrontalizing tiny non-frontal face images. VividGAN consists of coarse-level\nand fine-level Face Hallucination Networks (FHnet) and two discriminators,\ni.e., Coarse-D and Fine-D. The coarse-level FHnet generates a frontal coarse HR\nface and then the fine-level FHnet makes use of the facial component appearance\nprior, i.e., fine-grained facial components, to attain a frontal HR face image\nwith authentic details. In the fine-level FHnet, we also design a facial\ncomponent-aware module that adopts the facial geometry guidance as clues to\naccurately align and merge the frontal coarse HR face and prior information.\nMeanwhile, two-level discriminators are designed to capture both the global\noutline of a face image as well as detailed facial characteristics. The\nCoarse-D enforces the coarsely hallucinated faces to be upright and complete\nwhile the Fine-D focuses on the fine hallucinated ones for sharper details.\nExtensive experiments demonstrate that our VividGAN achieves photo-realistic\nfrontal HR faces, reaching superior performance in downstream tasks, i.e., face\nrecognition and expression classification, compared with other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 07:33:48 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 03:10:44 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Zhang", "Yang", ""], ["Tsang", "Ivor W.", ""], ["Li", "Jun", ""], ["Liu", "Ping", ""], ["Lu", "Xiaobo", ""], ["Yu", "Xin", ""]]}, {"id": "2002.03312", "submitter": "Shenglan Liu", "authors": "Shenlan Liu, Xiang Liu, Gao Huang, Lin Feng, Lianyu Hu, Dong Jiang,\n  Aibin Zhang, Yang Liu, Hong Qiao", "title": "FSD-10: A Dataset for Competitive Sports Content Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is an important and challenging problem in video analysis.\nAlthough the past decade has witnessed progress in action recognition with the\ndevelopment of deep learning, such process has been slow in competitive sports\ncontent analysis. To promote the research on action recognition from\ncompetitive sports video clips, we introduce a Figure Skating Dataset (FSD-10)\nfor finegrained sports content analysis. To this end, we collect 1484 clips\nfrom the worldwide figure skating championships in 2017-2018, which consist of\n10 different actions in men/ladies programs. Each clip is at a rate of 30\nframes per second with resolution 1080 $\\times$ 720. These clips are then\nannotated by experts in type, grade of execution, skater info, .etc. To build a\nbaseline for action recognition in figure skating, we evaluate state-of-the-art\naction recognition methods on FSD-10. Motivated by the idea that domain\nknowledge is of great concern in sports field, we propose a keyframe based\ntemporal segment network (KTSN) for classification and achieve remarkable\nperformance. Experimental results demonstrate that FSD-10 is an ideal dataset\nfor benchmarking action recognition algorithms, as it requires to accurately\nextract action motions rather than action poses. We hope FSD-10, which is\ndesigned to have a large collection of finegrained actions, can serve as a new\nchallenge to develop more robust and advanced action recognition models.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 08:04:26 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liu", "Shenlan", ""], ["Liu", "Xiang", ""], ["Huang", "Gao", ""], ["Feng", "Lin", ""], ["Hu", "Lianyu", ""], ["Jiang", "Dong", ""], ["Zhang", "Aibin", ""], ["Liu", "Yang", ""], ["Qiao", "Hong", ""]]}, {"id": "2002.03314", "submitter": "Martianus Frederic Ezerman", "authors": "Alfred Marcel Bruckstein, Martianus Frederic Ezerman, Adamas Aqsa\n  Fahreza, and San Ling", "title": "Patch-Based Holographic Image Sensing", "comments": null, "journal-ref": "SIAM J. Imaging Sci., no. 14 vol. , pp. 198--223, 2021", "doi": "10.1137/20M1324041", "report-no": null, "categories": "cs.IT cs.CV eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holographic representations of data enable distributed storage with\nprogressive refinement when the stored packets of data are made available in\nany arbitrary order. In this paper, we propose and test patch-based transform\ncoding holographic sensing of image data. Our proposal is optimized for\nprogressive recovery under random order of retrieval of the stored data. The\ncoding of the image patches relies on the design of distributed projections\nensuring best image recovery, in terms of the $\\ell_2$ norm, at each retrieval\nstage. The performance depends only on the number of data packets that has been\nretrieved thus far. Several possible options to enhance the quality of the\nrecovery while changing the size and number of data packets are discussed and\ntested. This leads us to examine several interesting bit-allocation and\nrate-distortion trade offs, highlighted for a set of natural images with\nensemble estimated statistical properties.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 08:14:02 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 08:01:13 GMT"}, {"version": "v3", "created": "Sat, 19 Dec 2020 09:32:31 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Bruckstein", "Alfred Marcel", ""], ["Ezerman", "Martianus Frederic", ""], ["Fahreza", "Adamas Aqsa", ""], ["Ling", "San", ""]]}, {"id": "2002.03321", "submitter": "Shadrokh Samavi", "authors": "Sajjad Abbasi, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi,\n  Shahram Shirani", "title": "Unlabeled Data Deployment for Classification of Diabetic Retinopathy\n  Images Using Knowledge Transfer", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are extensively beneficial for medical\nimage processing. Medical images are plentiful, but there is a lack of\nannotated data. Transfer learning is used to solve the problem of lack of\nlabeled data and grants CNNs better training capability. Transfer learning can\nbe used in many different medical applications; however, the model under\ntransfer should have the same size as the original network. Knowledge\ndistillation is recently proposed to transfer the knowledge of a model to\nanother one and can be useful to cover the shortcomings of transfer learning.\nBut some parts of the knowledge may not be distilled by knowledge distillation.\nIn this paper, a novel knowledge distillation using transfer learning is\nproposed to transfer the whole knowledge of a model to another one. The\nproposed method can be beneficial and practical for medical image analysis in\nwhich a small number of labeled data are available. The proposed process is\ntested for diabetic retinopathy classification. Simulation results demonstrate\nthat using the proposed method, knowledge of an extensive network can be\ntransferred to a smaller model.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 09:01:11 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Abbasi", "Sajjad", ""], ["Hajabdollahi", "Mohsen", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Shirani", "Shahram", ""]]}, {"id": "2002.03322", "submitter": "Xingchen Zhang", "authors": "Xingchen Zhang, Ping Ye, Gang Xiao", "title": "VIFB: A Visible and Infrared Image Fusion Benchmark", "comments": "11 pages, 5 figures, 5 tables. Accepted to CVPRW2020. Compared to the\n  CVPRW2020 version, this version corrects minor mistakes in Table 4 and the\n  first paragraph of Section 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible and infrared image fusion is one of the most important areas in image\nprocessing due to its numerous applications. While much progress has been made\nin recent years with efforts on developing fusion algorithms, there is a lack\nof code library and benchmark which can gauge the state-of-the-art. In this\npaper, after briefly reviewing recent advances of visible and infrared image\nfusion, we present a visible and infrared image fusion benchmark (VIFB) which\nconsists of 21 image pairs, a code library of 20 fusion algorithms and 13\nevaluation metrics. We also carry out large scale experiments within the\nbenchmark to understand the performance of these algorithms. By analyzing\nqualitative and quantitative results, we identify effective algorithms for\nrobust image fusion and give some observations on the status and future\nprospects of this field.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 09:10:00 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 14:01:15 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 08:30:51 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 15:44:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Xingchen", ""], ["Ye", "Ping", ""], ["Xiao", "Gang", ""]]}, {"id": "2002.03328", "submitter": "Yufeng Zhang", "authors": "Yufeng Zhang, Wanwei Liu, Zhenbang Chen, Ji Wang, Zhiming Liu, Kenli\n  Li, Hongmei Wei", "title": "Out-of-Distribution Detection with Distance Guarantee in Deep Generative\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has revealed that deep generative models including flow-based\nmodels and Variational autoencoders may assign higher likelihood to\nout-of-distribution (OOD) data than in-distribution (ID) data. However, we\ncannot sample out OOD data from the model. This counterintuitive phenomenon has\nnot been satisfactorily explained. In this paper, we prove theorems to\ninvestigate the divergences in flow-based model and give two explanations to\nthe above phenomenon from divergence and geometric perspectives, respectively.\nBased on our analysis, we propose two group anomaly detection methods.\nFurthermore, we decompose the KL divergence and propose a point-wise anomaly\ndetection method. We have conducted extensive experiments on prevalent\nbenchmarks to evaluate our methods. For group anomaly detection (GAD), our\nmethod can achieve near 100\\% AUROC on all problems and has robustness against\ndata manipulations. On the contrary, the state-of-the-art (SOTA) GAD method\nperforms not better than random guessing for challenging problems and can be\nattacked by data manipulation in almost all cases. For point-wise anomaly\ndetection (PAD), our method is comparable to the SOTA PAD method on one\ncategory of problems and outperforms the baseline significantly on another\ncategory of problems.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 09:54:12 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 11:56:54 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 13:56:04 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Zhang", "Yufeng", ""], ["Liu", "Wanwei", ""], ["Chen", "Zhenbang", ""], ["Wang", "Ji", ""], ["Liu", "Zhiming", ""], ["Li", "Kenli", ""], ["Wei", "Hongmei", ""]]}, {"id": "2002.03335", "submitter": "Hila Levi", "authors": "Hila Levi, Shimon Ullman", "title": "Multi-Task Learning by a Top-Down Control Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the range of tasks performed by a general vision system expands, executing\nmultiple tasks accurately and efficiently in a single network has become an\nimportant and still open problem. Recent computer vision approaches address\nthis problem by branching networks, or by a channel-wise modulation of the\nnetwork feature-maps with task specific vectors. We present a novel\narchitecture that uses a dedicated top-down control network to modify the\nactivation of all the units in the main recognition network in a manner that\ndepends on the selected task, image content, and spatial location. We show the\neffectiveness of our scheme by achieving significantly better results than\nalternative state-of-the-art approaches on four datasets. We further\ndemonstrate our advantages in terms of task selectivity, scaling the number of\ntasks and interpretability.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 10:13:17 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 14:02:20 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 19:53:34 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Levi", "Hila", ""], ["Ullman", "Shimon", ""]]}, {"id": "2002.03342", "submitter": "Wenhao Wu", "authors": "Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang, Shilei Wen", "title": "Dynamic Inference: A New Approach Toward Efficient Video Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though action recognition in videos has achieved great success recently, it\nremains a challenging task due to the massive computational cost. Designing\nlightweight networks is a possible solution, but it may degrade the recognition\nperformance. In this paper, we innovatively propose a general dynamic inference\nidea to improve inference efficiency by leveraging the variation in the\ndistinguishability of different videos. The dynamic inference approach can be\nachieved from aspects of the network depth and the number of input video\nframes, or even in a joint input-wise and network depth-wise manner. In a\nnutshell, we treat input frames and network depth of the computational graph as\na 2-dimensional grid, and several checkpoints are placed on this grid in\nadvance with a prediction module. The inference is carried out progressively on\nthe grid by following some predefined route, whenever the inference process\ncomes across a checkpoint, an early prediction can be made depending on whether\nthe early stop criteria meets. For the proof-of-concept purpose, we instantiate\nthree dynamic inference frameworks using two well-known backbone CNNs. In these\ninstances, we overcome the drawback of limited temporal coverage resulted from\nan early prediction by a novel frame permutation scheme, and alleviate the\nconflict between progressive computation and video temporal relation modeling\nby introducing an online temporal shift module. Extensive experiments are\nconducted to thoroughly analyze the effectiveness of our ideas and to inspire\nfuture research efforts. Results on various datasets also evident the\nsuperiority of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 11:09:56 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wu", "Wenhao", ""], ["He", "Dongliang", ""], ["Tan", "Xiao", ""], ["Chen", "Shifeng", ""], ["Yang", "Yi", ""], ["Wen", "Shilei", ""]]}, {"id": "2002.03353", "submitter": "YiFeng Ding", "authors": "Yifeng Ding, Shaoguo Wen, Jiyang Xie, Dongliang Chang, Zhanyu Ma,\n  Zhongwei Si, Haibin Ling", "title": "Weakly Supervised Attention Pyramid Convolutional Neural Network for\n  Fine-Grained Visual Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3055617", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying the sub-categories of an object from the same super-category\n(e.g. bird species, car and aircraft models) in fine-grained visual\nclassification (FGVC) highly relies on discriminative feature representation\nand accurate region localization. Existing approaches mainly focus on\ndistilling information from high-level features. In this paper, however, we\nshow that by integrating low-level information (e.g. color, edge junctions,\ntexture patterns), performance can be improved with enhanced feature\nrepresentation and accurately located discriminative regions. Our solution,\nnamed Attention Pyramid Convolutional Neural Network (AP-CNN), consists of a) a\npyramidal hierarchy structure with a top-down feature pathway and a bottom-up\nattention pathway, and hence learns both high-level semantic and low-level\ndetailed feature representation, and b) an ROI guided refinement strategy with\nROI guided dropblock and ROI guided zoom-in, which refines features with\ndiscriminative local regions enhanced and background noises eliminated. The\nproposed AP-CNN can be trained end-to-end, without the need of additional\nbounding box/part annotations. Extensive experiments on three commonly used\nFGVC datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft) demonstrate that\nour approach can achieve state-of-the-art performance. Code available at\n\\url{http://dwz1.cc/ci8so8a}\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 12:33:23 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Ding", "Yifeng", ""], ["Wen", "Shaoguo", ""], ["Xie", "Jiyang", ""], ["Chang", "Dongliang", ""], ["Ma", "Zhanyu", ""], ["Si", "Zhongwei", ""], ["Ling", "Haibin", ""]]}, {"id": "2002.03366", "submitter": "Quande Liu", "authors": "Quande Liu, Qi Dou, Lequan Yu, Pheng Ann Heng", "title": "MS-Net: Multi-Site Network for Improving Prostate Segmentation with\n  Heterogeneous MRI Data", "comments": "IEEE TMI, 2020", "journal-ref": null, "doi": "10.1109/TMI.2020.2974574", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated prostate segmentation in MRI is highly demanded for\ncomputer-assisted diagnosis. Recently, a variety of deep learning methods have\nachieved remarkable progress in this task, usually relying on large amounts of\ntraining data. Due to the nature of scarcity for medical images, it is\nimportant to effectively aggregate data from multiple sites for robust model\ntraining, to alleviate the insufficiency of single-site samples. However, the\nprostate MRIs from different sites present heterogeneity due to the differences\nin scanners and imaging protocols, raising challenges for effective ways of\naggregating multi-site data for network training. In this paper, we propose a\nnovel multi-site network (MS-Net) for improving prostate segmentation by\nlearning robust representations, leveraging multiple sources of data. To\ncompensate for the inter-site heterogeneity of different MRI datasets, we\ndevelop Domain-Specific Batch Normalization layers in the network backbone,\nenabling the network to estimate statistics and perform feature normalization\nfor each site separately. Considering the difficulty of capturing the shared\nknowledge from multiple datasets, a novel learning paradigm, i.e.,\nMulti-site-guided Knowledge Transfer, is proposed to enhance the kernels to\nextract more generic representations from multi-site data. Extensive\nexperiments on three heterogeneous prostate MRI datasets demonstrate that our\nMS-Net improves the performance across all datasets consistently, and\noutperforms state-of-the-art methods for multi-site learning.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 14:11:50 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 06:21:01 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Liu", "Quande", ""], ["Dou", "Qi", ""], ["Yu", "Lequan", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "2002.03370", "submitter": "Jiaheng Liu", "authors": "Jiaheng Liu, Guo Lu, Zhihao Hu, Dong Xu", "title": "A Unified End-to-End Framework for Efficient Deep Image Compression", "comments": "We will released our code and training data", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compression is a widely used technique to reduce the spatial redundancy\nin images. Recently, learning based image compression has achieved significant\nprogress by using the powerful representation ability from neural networks.\nHowever, the current state-of-the-art learning based image compression methods\nsuffer from the huge computational cost, which limits their capacity for\npractical applications. In this paper, we propose a unified framework called\nEfficient Deep Image Compression (EDIC) based on three new technologies,\nincluding a channel attention module, a Gaussian mixture model and a\ndecoder-side enhancement module. Specifically, we design an auto-encoder style\nnetwork for learning based image compression. To improve the coding efficiency,\nwe exploit the channel relationship between latent representations by using the\nchannel attention module. Besides, the Gaussian mixture model is introduced for\nthe entropy model and improves the accuracy for bitrate estimation.\nFurthermore, we introduce the decoder-side enhancement module to further\nimprove image compression performance. Our EDIC method can also be readily\nincorporated with the Deep Video Compression (DVC) framework to further improve\nthe video compression performance. Simultaneously, our EDIC method boosts the\ncoding performance significantly while bringing slightly increased\ncomputational cost. More importantly, experimental results demonstrate that the\nproposed approach outperforms the current state-of-the-art image compression\nmethods and is up to more than 150 times faster in terms of decoding speed when\ncompared with Minnen's method. The proposed framework also successfully\nimproves the performance of the recent deep video compression system DVC. Our\ncode will be released at https://github.com/liujiaheng/compression.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 14:21:08 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 03:09:04 GMT"}, {"version": "v3", "created": "Sat, 23 May 2020 22:41:06 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Liu", "Jiaheng", ""], ["Lu", "Guo", ""], ["Hu", "Zhihao", ""], ["Xu", "Dong", ""]]}, {"id": "2002.03399", "submitter": "Felix Kuhnke", "authors": "Felix Kuhnke, Lars Rumberg, J\\\"orn Ostermann", "title": "Two-Stream Aural-Visual Affect Analysis in the Wild", "comments": "6 pages, 2 figures, Face and Gesture 2020 Workshop Paper (ABAW2020\n  competition)", "journal-ref": null, "doi": "10.1109/FG47880.2020.00056", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human affect recognition is an essential part of natural human-computer\ninteraction. However, current methods are still in their infancy, especially\nfor in-the-wild data. In this work, we introduce our submission to the\nAffective Behavior Analysis in-the-wild (ABAW) 2020 competition. We propose a\ntwo-stream aural-visual analysis model to recognize affective behavior from\nvideos. Audio and image streams are first processed separately and fed into a\nconvolutional neural network. Instead of applying recurrent architectures for\ntemporal analysis we only use temporal convolutions. Furthermore, the model is\ngiven access to additional features extracted during face-alignment. At\ntraining time, we exploit correlations between different emotion\nrepresentations to improve performance. Our model achieves promising results on\nthe challenging Aff-Wild2 database.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 16:59:56 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 13:59:01 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kuhnke", "Felix", ""], ["Rumberg", "Lars", ""], ["Ostermann", "J\u00f6rn", ""]]}, {"id": "2002.03401", "submitter": "Hamid Reza Boveiri", "authors": "Hamid Reza Boveiri, Raouf Khayami, Reza Javidan, Ali Reza MehdiZadeh", "title": "Medical Image Registration Using Deep Neural Networks: A Comprehensive\n  Review", "comments": "45 Pages, 39 Figures, 10 Tables, 2 Appendixes", "journal-ref": null, "doi": "10.1016/j.compeleceng.2020.106767", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-guided interventions are saving the lives of a large number of patients\nwhere the image registration problem should indeed be considered as the most\ncomplex and complicated issue to be tackled. On the other hand, the recently\nhuge progress in the field of machine learning made by the possibility of\nimplementing deep neural networks on the contemporary many-core GPUs opened up\na promising window to challenge with many medical applications, where the\nregistration is not an exception. In this paper, a comprehensive review on the\nstate-of-the-art literature known as medical image registration using deep\nneural networks is presented. The review is systematic and encompasses all the\nrelated works previously published in the field. Key concepts, statistical\nanalysis from different points of view, confiding challenges, novelties and\nmain contributions, key-enabling techniques, future directions and prospective\ntrends all are discussed and surveyed in details in this comprehensive review.\nThis review allows a deep understanding and insight for the readers active in\nthe field who are investigating the state-of-the-art and seeking to contribute\nthe future literature.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 17:22:05 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Boveiri", "Hamid Reza", ""], ["Khayami", "Raouf", ""], ["Javidan", "Reza", ""], ["MehdiZadeh", "Ali Reza", ""]]}, {"id": "2002.03463", "submitter": "Anirudh Chandrashekar", "authors": "Anirudh Chandrashekar, Ashok Handa, Natesh Shivakumar, Pierfrancesco\n  Lapolla, Vicente Grau, and Regent Lee", "title": "A Deep Learning Approach to Automate High-Resolution Blood Vessel\n  Reconstruction on Computerized Tomography Images With or Without the Use of\n  Contrast Agent", "comments": "18 pages, 10 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods to reconstruct vascular structures from a computed\ntomography (CT) angiogram rely on injection of intravenous contrast to enhance\nthe radio-density within the vessel lumen. However, pathological changes can be\npresent in the blood lumen, vessel wall or a combination of both that prevent\naccurate reconstruction. In the example of aortic aneurysmal disease, a blood\nclot or thrombus adherent to the aortic wall within the expanding aneurysmal\nsac is present in 70-80% of cases. These deformations prevent the automatic\nextraction of vital clinically relevant information by current methods. In this\nstudy, we implemented a modified U-Net architecture with attention-gating to\nestablish a high-throughput and automated segmentation pipeline of pathological\nblood vessels in CT images acquired with or without the use of a contrast\nagent. Twenty-six patients with paired non-contrast and contrast-enhanced CT\nimages within the ongoing Oxford Abdominal Aortic Aneurysm (OxAAA) study were\nrandomly selected, manually annotated and used for model training and\nevaluation (13/13). Data augmentation methods were implemented to diversify the\ntraining data set in a ratio of 10:1. The performance of our Attention-based\nU-Net in extracting both the inner lumen and the outer wall of the aortic\naneurysm from CT angiograms (CTA) was compared against a generic 3-D U-Net and\ndisplayed superior results. Subsequent implementation of this network\narchitecture within the aortic segmentation pipeline from both\ncontrast-enhanced CTA and non-contrast CT images has allowed for accurate and\nefficient extraction of the entire aortic volume. This extracted volume can be\nused to standardize current methods of aneurysmal disease management and sets\nthe foundation for subsequent complex geometric and morphological analysis.\nFurthermore, the proposed pipeline can be extended to other vascular\npathologies.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 22:32:37 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Chandrashekar", "Anirudh", ""], ["Handa", "Ashok", ""], ["Shivakumar", "Natesh", ""], ["Lapolla", "Pierfrancesco", ""], ["Grau", "Vicente", ""], ["Lee", "Regent", ""]]}, {"id": "2002.03480", "submitter": "Jeremy Nixon", "authors": "Jeremy Nixon, Jeremiah Liu, David Berthelot", "title": "Semi-Supervised Class Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One promising approach to dealing with datapoints that are outside of the\ninitial training distribution (OOD) is to create new classes that capture\nsimilarities in the datapoints previously rejected as uncategorizable. Systems\nthat generate labels can be deployed against an arbitrary amount of data,\ndiscovering classification schemes that through training create a higher\nquality representation of data. We introduce the Dataset Reconstruction\nAccuracy, a new and important measure of the effectiveness of a model's ability\nto create labels. We introduce benchmarks against this Dataset Reconstruction\nmetric. We apply a new heuristic, class learnability, for deciding whether a\nclass is worthy of addition to the training dataset. We show that our class\ndiscovery system can be successfully applied to vision and language, and we\ndemonstrate the value of semi-supervised learning in automatically discovering\nnovel classes.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 00:29:44 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 01:31:08 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Nixon", "Jeremy", ""], ["Liu", "Jeremiah", ""], ["Berthelot", "David", ""]]}, {"id": "2002.03482", "submitter": "Xi Zhang", "authors": "Xi Zhang and Xiaolin Wu", "title": "Ultra High Fidelity Image Compression with $\\ell_\\infty$-constrained\n  Encoding and Deep Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many professional fields, such as medicine, remote sensing and sciences,\nusers often demand image compression methods to be mathematically lossless. But\nlossless image coding has a rather low compression ratio (around 2:1 for\nnatural images). The only known technique to achieve significant compression\nwhile meeting the stringent fidelity requirements is the methodology of\n$\\ell_\\infty$-constrained coding that was developed and standardized in\nnineties. We make a major progress in $\\ell_\\infty$-constrained image coding\nafter two decades, by developing a novel CNN-based soft\n$\\ell_\\infty$-constrained decoding method. The new method repairs compression\ndefects by using a restoration CNN called the $\\ell_\\infty\\mbox{-SDNet}$ to map\na conventionally decoded image to the latent image. A unique strength of the\n$\\ell_\\infty\\mbox{-SDNet}$ is its ability to enforce a tight error bound on a\nper pixel basis. As such, no small distinctive structures of the original image\ncan be dropped or distorted, even if they are statistical outliers that are\notherwise sacrificed by mainstream CNN restoration methods. More importantly,\nthis research ushers in a new image compression system of\n$\\ell_\\infty$-constrained encoding and deep soft decoding\n($\\ell_\\infty\\mbox{-ED}^2$). The $\\ell_\\infty \\mbox{-ED}^2$ approach beats the\nbest of existing lossy image compression methods (e.g., BPG, WebP, etc.) not\nonly in $\\ell_\\infty$ but also in $\\ell_2$ error metric and perceptual quality,\nfor bit rates near the threshold of perceptually transparent reconstruction.\nOperationally, the new compression system is practical, with a low-complexity\nreal-time encoder and a cascade decoder consisting of a fast initial decoder\nand an optional CNN soft decoder.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 00:33:39 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2002.03500", "submitter": "Qing Guo", "authors": "Qing Guo and Felix Juefei-Xu and Xiaofei Xie and Lei Ma and Jian Wang\n  and Bing Yu and Wei Feng and Yang Liu", "title": "Watch out! Motion is Blurring the Vision of Your Deep Neural Networks", "comments": "19 pages, 16 figures. This paper has been accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art deep neural networks (DNNs) are vulnerable against\nadversarial examples with additive random-like noise perturbations. While such\nexamples are hardly found in the physical world, the image blurring effect\ncaused by object motion, on the other hand, commonly occurs in practice, making\nthe study of which greatly important especially for the widely adopted\nreal-time image processing tasks (e.g., object detection, tracking). In this\npaper, we initiate the first step to comprehensively investigate the potential\nhazards of the blur effect for DNN, caused by object motion. We propose a novel\nadversarial attack method that can generate visually natural motion-blurred\nadversarial examples, named motion-based adversarial blur attack (ABBA). To\nthis end, we first formulate the kernel-prediction-based attack where an input\nimage is convolved with kernels in a pixel-wise way, and the misclassification\ncapability is achieved by tuning the kernel weights. To generate visually more\nnatural and plausible examples, we further propose the saliency-regularized\nadversarial kernel prediction, where the salient region serves as a moving\nobject, and the predicted kernel is regularized to achieve naturally visual\neffects. Besides, the attack is further enhanced by adaptively tuning the\ntranslations of object and background. A comprehensive evaluation on the\nNeurIPS'17 adversarial competition dataset demonstrates the effectiveness of\nABBA by considering various kernel sizes, translations, and regions. The\nin-depth study further confirms that our method shows more effective\npenetrating capability to the state-of-the-art GAN-based deblurring mechanisms\ncompared with other blurring methods. We release the code to\nhttps://github.com/tsingqguo/ABBA.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 02:33:08 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 06:42:29 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 05:52:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Wang", "Jian", ""], ["Yu", "Bing", ""], ["Feng", "Wei", ""], ["Liu", "Yang", ""]]}, {"id": "2002.03501", "submitter": "Seunghyeok Back", "authors": "Seunghyeok Back, Jongwon Kim, Raeyoung Kang, Seungjun Choi, Kyoobin\n  Lee", "title": "Segmenting Unseen Industrial Components in a Heavy Clutter Using RGB-D\n  Fusion and Synthetic Data", "comments": "5 pages,6 figures, Accepted to ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of unseen industrial parts is essential for autonomous\nindustrial systems. However, industrial components are texture-less,\nreflective, and often found in cluttered and unstructured environments with\nheavy occlusion, which makes it more challenging to deal with unseen objects.\nTo tackle this problem, we present a synthetic data generation pipeline that\nrandomizes textures via domain randomization to focus on the shape information.\nIn addition, we propose an RGB-D Fusion Mask R-CNN with a confidence map\nestimator, which exploits reliable depth information in multiple feature\nlevels. We transferred the trained model to real-world scenarios and evaluated\nits performance by making comparisons with baselines and ablation studies. We\ndemonstrate that our methods, which use only synthetic data, could be effective\nsolutions for unseen industrial components segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 02:33:21 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 08:09:38 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 00:23:59 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Back", "Seunghyeok", ""], ["Kim", "Jongwon", ""], ["Kang", "Raeyoung", ""], ["Choi", "Seungjun", ""], ["Lee", "Kyoobin", ""]]}, {"id": "2002.03509", "submitter": "Shangbang Long", "authors": "Shangbang Long, Yushuo Guan, Kaigui Bian, Cong Yao", "title": "A New Perspective for Flexible Feature Gathering in Scene Text\n  Recognition Via Character Anchor Pooling", "comments": "To appear at ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irregular scene text recognition has attracted much attention from the\nresearch community, mainly due to the complexity of shapes of text in natural\nscene.\n  However, recent methods either rely on shape-sensitive modules such as\nbounding box regression, or discard sequence learning.\n  To tackle these issues, we propose a pair of coupling modules, termed as\nCharacter Anchoring Module (CAM) and Anchor Pooling Module (APM), to extract\nhigh-level semantics from two-dimensional space to form feature sequences.\n  The proposed CAM localizes the text in a shape-insensitive way by design by\nanchoring characters individually.\n  APM then interpolates and gathers features flexibly along the character\nanchors which enables sequence learning.\n  The complementary modules realize a harmonic unification of spatial\ninformation and sequence learning.\n  With the proposed modules, our recognition system surpasses previous\nstate-of-the-art scores on irregular and perspective text datasets, including,\nICDAR 2015, CUTE, and Total-Text, while paralleling state-of-the-art\nperformance on regular text datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 03:01:23 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Long", "Shangbang", ""], ["Guan", "Yushuo", ""], ["Bian", "Kaigui", ""], ["Yao", "Cong", ""]]}, {"id": "2002.03521", "submitter": "Shahriar Esmaeili", "authors": "Saeideh Roshanfekr, Shahriar Esmaeili, Hassan Ataeian, and Ali Amiri", "title": "UGRWO-Sampling: A modified random walk under-sampling approach based on\n  graphs to imbalanced data classification", "comments": "34 pages, 3 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we propose a new RWO-Sampling (Random Walk Over-Sampling)\nbased on graphs for imbalanced datasets. In this method, two figures based on\nunder-sampling and over-sampling methods are introduced to keep the proximity\ninformation, which is robust to noises and outliers. After the construction of\nthe first graph on minority class, RWO-Sampling will be implemented on selected\nsamples, and the rest of them will remain unchanged. The second graph is\nconstructed for the majority class, and the samples in a low-density area\n(outliers) are removed. In the proposed method, examples of the majority class\nin a high-density area are selected, and the rest of them are eliminated.\nFurthermore, utilizing RWO-sampling, the boundary of minority class is\nincreased though, the outliers are not raised. This method is tested, and the\nnumber of evaluation measures is compared to previous methods on nine\ncontinuous attribute datasets with different over-sampling rates. The\nexperimental results were an indicator of the high efficiency and flexibility\nof the proposed method for the classification of imbalanced data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 03:29:24 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 20:55:49 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Roshanfekr", "Saeideh", ""], ["Esmaeili", "Shahriar", ""], ["Ataeian", "Hassan", ""], ["Amiri", "Ali", ""]]}, {"id": "2002.03528", "submitter": "Gokul B. Nair", "authors": "Gokul B. Nair, Swapnil Daga, Rahul Sajnani, Anirudha Ramesh, Junaid\n  Ahmed Ansari, Krishna Murthy Jatavallabhula, K. Madhava Krishna", "title": "Multi-object Monocular SLAM for Dynamic Environments", "comments": "Accepted to IEEE Intelligent Vehicles Symposium 2020 (IV2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of multibody SLAM from a monocular\ncamera. The term multibody, implies that we track the motion of the camera, as\nwell as that of other dynamic participants in the scene. The quintessential\nchallenge in dynamic scenes is unobservability: it is not possible to\nunambiguously triangulate a moving object from a moving monocular camera.\nExisting approaches solve restricted variants of the problem, but the solutions\nsuffer relative scale ambiguity (i.e., a family of infinitely many solutions\nexist for each pair of motions in the scene). We solve this rather intractable\nproblem by leveraging single-view metrology, advances in deep learning, and\ncategory-level shape estimation. We propose a multi pose-graph optimization\nformulation, to resolve the relative and absolute scale factor ambiguities\ninvolved. This optimization helps us reduce the average error in trajectories\nof multiple bodies over real-world datasets, such as KITTI. To the best of our\nknowledge, our method is the first practical monocular multi-body SLAM system\nto perform dynamic multi-object and ego localization in a unified framework in\nmetric scale.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 03:49:16 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 11:42:42 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Nair", "Gokul B.", ""], ["Daga", "Swapnil", ""], ["Sajnani", "Rahul", ""], ["Ramesh", "Anirudha", ""], ["Ansari", "Junaid Ahmed", ""], ["Jatavallabhula", "Krishna Murthy", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2002.03549", "submitter": "Rahul Soni", "authors": "Rahul Soni, Naresh Shah, Chua Tat Seng, Jimmy D. Moore", "title": "Adversarial TCAV -- Robust and Effective Interpretation of Intermediate\n  Layers in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpreting neural network decisions and the information learned in\nintermediate layers is still a challenge due to the opaque internal state and\nshared non-linear interactions. Although (Kim et al, 2017) proposed to\ninterpret intermediate layers by quantifying its ability to distinguish a\nuser-defined concept (from random examples), the questions of robustness\n(variation against the choice of random examples) and effectiveness (retrieval\nrate of concept images) remain. We investigate these two properties and propose\nimprovements to make concept activations reliable for practical use.\n  Effectiveness: If the intermediate layer has effectively learned a\nuser-defined concept, it should be able to recall --- at the testing step ---\nmost of the images containing the proposed concept. For instance, we observed\nthat the recall rate of Tiger shark and Great white shark from the ImageNet\ndataset with \"Fins\" as a user-defined concept was only 18.35% for VGG16. To\nincrease the effectiveness of concept learning, we propose A-CAV --- the\nAdversarial Concept Activation Vector --- this results in larger margins\nbetween user concepts and (negative) random examples. This approach improves\nthe aforesaid recall to 76.83% for VGG16.\n  For robustness, we define it as the ability of an intermediate layer to be\nconsistent in its recall rate (the effectiveness) for different random seeds.\nWe observed that TCAV has a large variance in recalling a concept across\ndifferent random seeds. For example, the recall of cat images (from a layer\nlearning the concept of tail) varies from 18% to 86% with 20.85% standard\ndeviation on VGG16. We propose a simple and scalable modification that employs\na Gram-Schmidt process to sample random noise from concepts and learn an\naverage \"concept classifier\". This approach improves the aforesaid standard\ndeviation from 20.85% to 6.4%.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 05:15:03 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 18:15:30 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Soni", "Rahul", ""], ["Shah", "Naresh", ""], ["Seng", "Chua Tat", ""], ["Moore", "Jimmy D.", ""]]}, {"id": "2002.03554", "submitter": "Fuzhen Li", "authors": "Fuzhen Li, Zhenfeng Zhu, Xingxing Zhang, Jian Cheng, Yao Zhao", "title": "From Anchor Generation to Distribution Alignment: Learning a\n  Discriminative Embedding Space for Zero-Shot Recognition", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In zero-shot learning (ZSL), the samples to be classified are usually\nprojected into side information templates such as attributes. However, the\nirregular distribution of templates makes classification results confused. To\nalleviate this issue, we propose a novel framework called Discriminative Anchor\nGeneration and Distribution Alignment Model (DAGDA). Firstly, in order to\nrectify the distribution of original templates, a diffusion based graph\nconvolutional network, which can explicitly model the interaction between class\nand side information, is proposed to produce discriminative anchors. Secondly,\nto further align the samples with the corresponding anchors in anchor space,\nwhich aims to refine the distribution in a fine-grained manner, we introduce a\nsemantic relation regularization in anchor space. Following the way of\ninductive learning, our approach outperforms some existing state-of-the-art\nmethods, on several benchmark datasets, for both conventional as well as\ngeneralized ZSL setting. Meanwhile, the ablation experiments strongly\ndemonstrate the effectiveness of each component.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 05:25:33 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Li", "Fuzhen", ""], ["Zhu", "Zhenfeng", ""], ["Zhang", "Xingxing", ""], ["Cheng", "Jian", ""], ["Zhao", "Yao", ""]]}, {"id": "2002.03556", "submitter": "Anoop Toffy", "authors": "Akanksha Dwivedi, Anoop Toffy, Athul Suresh, Tarini Chandrashekhar", "title": "Vehicle Driving Assistant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Autonomous vehicles has been a common term in our day to day life with car\nmanufacturers like Tesla shipping cars that are SAE Level 3. While these\nvehicles include a slew of features such as parking assistance and cruise\ncontrol,they have mostly been tailored to foreign roads. Potholes, and the\nabundance of them, is something that is unique to our Indian roads. We believe\nthat successful detection of potholes from visual images can be applied in a\nvariety of scenarios. Moreover, the sheer variety in the color, shape and size\nof potholes makes this problem an apt candidate to be solved using modern\nmachine learning and image processing techniques.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 05:32:11 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Dwivedi", "Akanksha", ""], ["Toffy", "Anoop", ""], ["Suresh", "Athul", ""], ["Chandrashekhar", "Tarini", ""]]}, {"id": "2002.03557", "submitter": "Didan Deng", "authors": "Didan Deng, Zhaokang Chen, Bertram E. Shi", "title": "Multitask Emotion Recognition with Incomplete Labels", "comments": "Accepted by FG2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train a unified model to perform three tasks: facial action unit\ndetection, expression classification, and valence-arousal estimation. We\naddress two main challenges of learning the three tasks. First, most existing\ndatasets are highly imbalanced. Second, most existing datasets do not contain\nlabels for all three tasks. To tackle the first challenge, we apply data\nbalancing techniques to experimental datasets. To tackle the second challenge,\nwe propose an algorithm for the multitask model to learn from missing\n(incomplete) labels. This algorithm has two steps. We first train a teacher\nmodel to perform all three tasks, where each instance is trained by the ground\ntruth label of its corresponding task. Secondly, we refer to the outputs of the\nteacher model as the soft labels. We use the soft labels and the ground truth\nto train the student model. We find that most of the student models outperform\ntheir teacher model on all the three tasks. Finally, we use model ensembling to\nboost performance further on the three tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 05:32:12 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 11:52:37 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Deng", "Didan", ""], ["Chen", "Zhaokang", ""], ["Shi", "Bertram E.", ""]]}, {"id": "2002.03563", "submitter": "Seyed Mohammad Hadi Hosseini", "authors": "S. M. Hadi Hosseini, Hao Chen, Monica M. Jablonski", "title": "Automatic detection and counting of retina cell nuclei using deep\n  learning", "comments": "13 pages, 11 figures, 2 tables, SPIE. Medical Imaging 2020 Conference", "journal-ref": "Proceedings Volume 11317, Medical Imaging 2020: Biomedical\n  Applications in Molecular, Structural, and Functional Imaging; 113172I (2020)", "doi": "10.1117/12.2567454", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to automatically detect, classify, calculate the size, number,\nand grade of retinal cells and other biological objects is critically important\nin eye disease like age-related macular degeneration (AMD). In this paper, we\ndeveloped an automated tool based on deep learning technique and Mask R-CNN\nmodel to analyze large datasets of transmission electron microscopy (TEM)\nimages and quantify retinal cells with high speed and precision. We considered\nthree categories for outer nuclear layer (ONL) cells: live, intermediate, and\npyknotic. We trained the model using a dataset of 24 samples. We then optimized\nthe hyper-parameters using another set of 6 samples. The results of this\nresearch, after applying to the test datasets, demonstrated that our method is\nhighly accurate for automatically detecting, categorizing, and counting cell\nnuclei in the ONL of the retina. Performance of our model was tested using\ngeneral metrics: general mean average precision (mAP) for detection; and\nprecision, recall, F1-score, and accuracy for categorizing and counting.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 05:49:10 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Hosseini", "S. M. Hadi", ""], ["Chen", "Hao", ""], ["Jablonski", "Monica M.", ""]]}, {"id": "2002.03579", "submitter": "Jinlu Liu", "authors": "Jinlu Liu and Yongqiang Qin", "title": "Prototype Refinement Network for Few-Shot Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot segmentation targets to segment new classes with few annotated\nimages provided. It is more challenging than traditional semantic segmentation\ntasks that segment known classes with abundant annotated images. In this paper,\nwe propose a Prototype Refinement Network (PRNet) to attack the challenge of\nfew-shot segmentation. It firstly learns to bidirectionally extract prototypes\nfrom both support and query images of the known classes. Furthermore, to\nextract representative prototypes of the new classes, we use adaptation and\nfusion for prototype refinement. The step of adaptation makes the model to\nlearn new concepts which is directly implemented by retraining. Prototype\nfusion is firstly proposed which fuses support prototypes with query\nprototypes, incorporating the knowledge from both sides. It is effective in\nprototype refinement without importing extra learnable parameters. In this way,\nthe prototypes become more discriminative in low-data regimes. Experiments on\nPASAL-$5^i$ and COCO-$20^i$ demonstrate the superiority of our method.\nEspecially on COCO-$20^i$, PRNet significantly outperforms existing methods by\na large margin of 13.1\\% in 1-shot setting.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 07:06:09 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 07:17:59 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Liu", "Jinlu", ""], ["Qin", "Yongqiang", ""]]}, {"id": "2002.03592", "submitter": "Philipp Terh\\\"orst", "authors": "Philipp Terh\\\"orst, Jan Niklas Kolf, Naser Damer, Florian\n  Kirchbuchner, Arjan Kuijper", "title": "Post-Comparison Mitigation of Demographic Bias in Face Recognition Using\n  Fair Score Normalization", "comments": "Accepted in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Current face recognition systems achieve high progress on several benchmark\ntests. Despite this progress, recent works showed that these systems are\nstrongly biased against demographic sub-groups. Consequently, an easily\nintegrable solution is needed to reduce the discriminatory effect of these\nbiased systems. Previous work mainly focused on learning less biased face\nrepresentations, which comes at the cost of a strongly degraded overall\nrecognition performance. In this work, we propose a novel unsupervised fair\nscore normalization approach that is specifically designed to reduce the effect\nof bias in face recognition and subsequently lead to a significant overall\nperformance boost. Our hypothesis is built on the notation of individual\nfairness by designing a normalization approach that leads to treating similar\nindividuals similarly. Experiments were conducted on three publicly available\ndatasets captured under controlled and in-the-wild circumstances. Results\ndemonstrate that our solution reduces demographic biases, e.g. by up to 82.7%\nin the case when gender is considered. Moreover, it mitigates the bias more\nconsistently than existing works. In contrast to previous works, our fair\nnormalization approach enhances the overall performance by up to 53.2% at false\nmatch rate of 0.001 and up to 82.9% at a false match rate of 0.00001.\nAdditionally, it is easily integrable into existing recognition systems and not\nlimited to face biometrics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 08:17:26 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 15:27:52 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 14:57:23 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Terh\u00f6rst", "Philipp", ""], ["Kolf", "Jan Niklas", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2002.03605", "submitter": "Jan Kieseler", "authors": "Jan Kieseler", "title": "Object condensation: one-stage grid-free multi-object reconstruction in\n  physics detectors, graph and image data", "comments": null, "journal-ref": "Eur. Phys. J. C 80, 886 (2020)", "doi": "10.1140/epjc/s10052-020-08461-2", "report-no": null, "categories": "physics.data-an cs.CV eess.IV hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-energy physics detectors, images, and point clouds share many\nsimilarities in terms of object detection. However, while detecting an unknown\nnumber of objects in an image is well established in computer vision, even\nmachine learning assisted object reconstruction algorithms in particle physics\nalmost exclusively predict properties on an object-by-object basis. Traditional\napproaches from computer vision either impose implicit constraints on the\nobject size or density and are not well suited for sparse detector data or rely\non objects being dense and solid. The object condensation method proposed here\nis independent of assumptions on object size, sorting or object density, and\nfurther generalises to non-image-like data structures, such as graphs and point\nclouds, which are more suitable to represent detector signals. The pixels or\nvertices themselves serve as representations of the entire object, and a\ncombination of learnable local clustering in a latent space and confidence\nassignment allows one to collect condensates of the predicted object properties\nwith a simple algorithm. As proof of concept, the object condensation method is\napplied to a simple object classification problem in images and used to\nreconstruct multiple particles from detector signals. The latter results are\nalso compared to a classic particle flow approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 09:04:02 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 14:39:13 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 07:48:16 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kieseler", "Jan", ""]]}, {"id": "2002.03627", "submitter": "Shurun Wang", "authors": "Shurun Wang, Wenhan Yang, Shiqi Wang", "title": "End-to-End Facial Deep Learning Feature Compression with Teacher-Student\n  Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel end-to-end feature compression scheme by\nleveraging the representation and learning capability of deep neural networks,\ntowards intelligent front-end equipped analysis with promising accuracy and\nefficiency. In particular, the extracted features are compactly coded in an\nend-to-end manner by optimizing the rate-distortion cost to achieve\nfeature-in-feature representation. In order to further improve the compression\nperformance, we present a latent code level teacher-student enhancement model,\nwhich could efficiently transfer the low bit-rate representation into a high\nbit rate one. Such a strategy further allows us to adaptively shift the\nrepresentation cost to decoding computations, leading to more flexible feature\ncompression with enhanced decoding capability. We verify the effectiveness of\nthe proposed model with the facial feature, and experimental results reveal\nbetter compression performance in terms of rate-accuracy compared with existing\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 10:08:44 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wang", "Shurun", ""], ["Yang", "Wenhan", ""], ["Wang", "Shiqi", ""]]}, {"id": "2002.03642", "submitter": "Jongbin Ryu", "authors": "Jongbin Ryu, Jiun Bae, Jongwoo Lim", "title": "Collaborative Training of Balanced Random Forests for Open Set Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a collaborative training algorithm of balanced\nrandom forests with convolutional neural networks for domain adaptation tasks.\nIn real scenarios, most domain adaptation algorithms face the challenges from\nnoisy, insufficient training data and open set categorization. In such cases,\nconventional methods suffer from overfitting and fail to successfully transfer\nthe knowledge of the source to the target domain. To address these issues, the\nfollowing two techniques are proposed. First, we introduce the optimized\ndecision tree construction method with convolutional neural networks, in which\nthe data at each node are split into equal sizes while maximizing the\ninformation gain. It generates balanced decision trees on deep features because\nof the even-split constraint, which contributes to enhanced discrimination\npower and reduced overfitting problem. Second, to tackle the domain\nmisalignment problem, we propose the domain alignment loss which penalizes\nuneven splits of the source and target domain data. By collaboratively\noptimizing the information gain of the labeled source data as well as the\nentropy of unlabeled target data distributions, the proposed CoBRF algorithm\nachieves significantly better performance than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 10:43:20 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ryu", "Jongbin", ""], ["Bae", "Jiun", ""], ["Lim", "Jongwoo", ""]]}, {"id": "2002.03651", "submitter": "Suhwan Cho", "authors": "Suhwan Cho, MyeongAh Cho, Tae-young Chung, Heansung Lee, Sangyoun Lee", "title": "CRVOS: Clue Refining Network for Video Object Segmentation", "comments": "ICIP 2020. Code: https://github.com/suhwan-cho/CRVOS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The encoder-decoder based methods for semi-supervised video object\nsegmentation (Semi-VOS) have received extensive attention due to their superior\nperformances. However, most of them have complex intermediate networks which\ngenerate strong specifiers to be robust against challenging scenarios, and this\nis quite inefficient when dealing with relatively simple scenarios. To solve\nthis problem, we propose a real-time network, Clue Refining Network for Video\nObject Segmentation (CRVOS), that does not have any intermediate network to\nefficiently deal with these scenarios. In this work, we propose a simple\nspecifier, referred to as the Clue, which consists of the previous frame's\ncoarse mask and coordinates information. We also propose a novel refine module\nwhich shows the better performance compared with the general ones by using a\ndeconvolution layer instead of a bilinear upsampling layer. Our proposed method\nshows the fastest speed among the existing methods with a competitive accuracy.\nOn DAVIS 2016 validation set, our method achieves 63.5 fps and J&F score of\n81.6%.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 10:55:31 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 09:00:51 GMT"}, {"version": "v3", "created": "Sat, 30 May 2020 08:46:09 GMT"}, {"version": "v4", "created": "Tue, 2 Jun 2020 08:15:08 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Cho", "Suhwan", ""], ["Cho", "MyeongAh", ""], ["Chung", "Tae-young", ""], ["Lee", "Heansung", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2002.03662", "submitter": "Yuge Huang", "authors": "Yuge Huang, Pengcheng Shen, Ying Tai, Shaoxin Li, Xiaoming Liu, Jilin\n  Li, Feiyue Huang, Rongrong Ji", "title": "Improving Face Recognition from Hard Samples via Distribution\n  Distillation Loss", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large facial variations are the main challenge in face recognition. To this\nend, previous variation-specific methods make full use of task-related prior to\ndesign special network losses, which are typically not general among different\ntasks and scenarios. In contrast, the existing generic methods focus on\nimproving the feature discriminability to minimize the intra-class distance\nwhile maximizing the interclass distance, which perform well on easy samples\nbut fail on hard samples. To improve the performance on those hard samples for\ngeneral tasks, we propose a novel Distribution Distillation Loss to narrow the\nperformance gap between easy and hard samples, which is a simple, effective and\ngeneric for various types of facial variations. Specifically, we first adopt\nstate-of-the-art classifiers such as ArcFace to construct two similarity\ndistributions: teacher distribution from easy samples and student distribution\nfrom hard samples. Then, we propose a novel distribution-driven loss to\nconstrain the student distribution to approximate the teacher distribution,\nwhich thus leads to smaller overlap between the positive and negative pairs in\nthe student distribution. We have conducted extensive experiments on both\ngeneric large-scale face benchmarks and benchmarks with diverse variations on\nrace, resolution and pose. The quantitative results demonstrate the superiority\nof our method over strong baselines, e.g., Arcface and Cosface.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 11:25:22 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 08:19:16 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 15:00:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Huang", "Yuge", ""], ["Shen", "Pengcheng", ""], ["Tai", "Ying", ""], ["Li", "Shaoxin", ""], ["Liu", "Xiaoming", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""]]}, {"id": "2002.03663", "submitter": "Max Mehltretter", "authors": "Max Mehltretter", "title": "Uncertainty Estimation for End-To-End Learned Dense Stereo Matching via\n  Probabilistic Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need to identify erroneous disparity assignments, various\napproaches for uncertainty and confidence estimation of dense stereo matching\nhave been presented in recent years. As in many other fields, especially deep\nlearning based methods have shown convincing results. However, most of these\nmethods only model the uncertainty contained in the data, while ignoring the\nuncertainty of the employed dense stereo matching procedure. Additionally\nmodelling the latter, however, is particularly beneficial if the domain of the\ntraining data varies from that of the data to be processed. For this purpose,\nin the present work the idea of probabilistic deep learning is applied to the\ntask of dense stereo matching for the first time. Based on the well-known and\ncommonly employed GC-Net architecture, a novel probabilistic neural network is\npresented, for the task of joint depth and uncertainty estimation from epipolar\nrectified stereo image pairs. Instead of learning the network parameters\ndirectly, the proposed probabilistic neural network learns a probability\ndistribution from which parameters are sampled for every prediction. The\nvariations between multiple such predictions on the same image pair allow to\napproximate the model uncertainty. The quality of the estimated depth and\nuncertainty information is assessed in an extensive evaluation on three\ndifferent datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 11:27:52 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Mehltretter", "Max", ""]]}, {"id": "2002.03683", "submitter": "Longbiao Mao", "authors": "Longbiao Mao, Yan Yan, Jing-Hao Xue, and Hanzi Wang", "title": "Deep Multi-task Multi-label CNN for Effective Facial Attribute\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Attribute Classification (FAC) has attracted increasing attention in\ncomputer vision and pattern recognition. However, state-of-the-art FAC methods\nperform face detection/alignment and FAC independently. The inherent\ndependencies between these tasks are not fully exploited. In addition, most\nmethods predict all facial attributes using the same CNN network architecture,\nwhich ignores the different learning complexities of facial attributes. To\naddress the above problems, we propose a novel deep multi-task multi-label CNN,\ntermed DMM-CNN, for effective FAC. Specifically, DMM-CNN jointly optimizes two\nclosely-related tasks (i.e., facial landmark detection and FAC) to improve the\nperformance of FAC by taking advantage of multi-task learning. To deal with the\ndiverse learning complexities of facial attributes, we divide the attributes\ninto two groups: objective attributes and subjective attributes. Two different\nnetwork architectures are respectively designed to extract features for two\ngroups of attributes, and a novel dynamic weighting scheme is proposed to\nautomatically assign the loss weight to each facial attribute during training.\nFurthermore, an adaptive thresholding strategy is developed to effectively\nalleviate the problem of class imbalance for multi-label learning. Experimental\nresults on the challenging CelebA and LFWA datasets show the superiority of the\nproposed DMM-CNN method compared with several state-of-the-art FAC methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 12:34:16 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Mao", "Longbiao", ""], ["Yan", "Yan", ""], ["Xue", "Jing-Hao", ""], ["Wang", "Hanzi", ""]]}, {"id": "2002.03688", "submitter": "Dmitrii Lachinov", "authors": "Dmitrii Lachinov, Elena Shipunova and Vadim Turlapov", "title": "Knowledge Distillation for Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of brain tumors in multimodal MRIs is one of the most\nchallenging tasks in medical image analysis. The recent state of the art\nalgorithms solving this task is based on machine learning approaches and deep\nlearning in particular. The amount of data used for training such models and\nits variability is a keystone for building an algorithm with high\nrepresentation power. In this paper, we study the relationship between the\nperformance of the model and the amount of data employed during the training\nprocess. On the example of brain tumor segmentation challenge, we compare the\nmodel trained with labeled data provided by challenge organizers, and the same\nmodel trained in omni-supervised manner using additional unlabeled data\nannotated with the ensemble of heterogeneous models. As a result, a single\nmodel trained with additional data achieves performance close to the ensemble\nof multiple models and outperforms individual methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 12:44:07 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Lachinov", "Dmitrii", ""], ["Shipunova", "Elena", ""], ["Turlapov", "Vadim", ""]]}, {"id": "2002.03703", "submitter": "Shihua Zhang", "authors": "Chihao Zhang and Yang Yang and Wei Zhang and Shihua Zhang", "title": "Distributed Bayesian Matrix Decomposition for Big Data Mining and\n  Clustering", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix decomposition is one of the fundamental tools to discover knowledge\nfrom big data generated by modern applications. However, it is still\ninefficient or infeasible to process very big data using such a method in a\nsingle machine. Moreover, big data are often distributedly collected and stored\non different machines. Thus, such data generally bear strong heterogeneous\nnoise. It is essential and useful to develop distributed matrix decomposition\nfor big data analytics. Such a method should scale up well, model the\nheterogeneous noise, and address the communication issue in a distributed\nsystem. To this end, we propose a distributed Bayesian matrix decomposition\nmodel (DBMD) for big data mining and clustering. Specifically, we adopt three\nstrategies to implement the distributed computing including 1) the accelerated\ngradient descent, 2) the alternating direction method of multipliers (ADMM),\nand 3) the statistical inference. We investigate the theoretical convergence\nbehaviors of these algorithms. To address the heterogeneity of the noise, we\npropose an optimal plug-in weighted average that reduces the variance of the\nestimation. Synthetic experiments validate our theoretical results, and\nreal-world experiments show that our algorithms scale up well to big data and\nachieves superior or competing performance compared to other distributed\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 13:10:53 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhang", "Chihao", ""], ["Yang", "Yang", ""], ["Zhang", "Wei", ""], ["Zhang", "Shihua", ""]]}, {"id": "2002.03711", "submitter": "Yueyu Hu", "authors": "Yueyu Hu, Wenhan Yang, Zhan Ma, Jiaying Liu", "title": "Learning End-to-End Lossy Image Compression: A Benchmark", "comments": "Accepted for publication in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence. Website available at\n  https://huzi96.github.io/compression-bench.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compression is one of the most fundamental techniques and commonly used\napplications in the image and video processing field. Earlier methods built a\nwell-designed pipeline, and efforts were made to improve all modules of the\npipeline by handcrafted tuning. Later, tremendous contributions were made,\nespecially when data-driven methods revitalized the domain with their excellent\nmodeling capacities and flexibility in incorporating newly designed modules and\nconstraints. Despite great progress, a systematic benchmark and comprehensive\nanalysis of end-to-end learned image compression methods are lacking. In this\npaper, we first conduct a comprehensive literature survey of learned image\ncompression methods. The literature is organized based on several aspects to\njointly optimize the rate-distortion performance with a neural network, i.e.,\nnetwork architecture, entropy model and rate control. We describe milestones in\ncutting-edge learned image-compression methods, review a broad range of\nexisting works, and provide insights into their historical development routes.\nWith this survey, the main challenges of image compression methods are\nrevealed, along with opportunities to address the related issues with recent\nadvanced learning methods. This analysis provides an opportunity to take a\nfurther step towards higher-efficiency image compression. By introducing a\ncoarse-to-fine hyperprior model for entropy estimation and signal\nreconstruction, we achieve improved rate-distortion performance, especially on\nhigh-resolution images. Extensive benchmark experiments demonstrate the\nsuperiority of our model in rate-distortion performance and time complexity on\nmulti-core CPUs and GPUs. Our project website is available at\nhttps://huzi96.github.io/compression-bench.html.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 13:13:43 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 04:25:37 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 02:21:07 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 02:23:55 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hu", "Yueyu", ""], ["Yang", "Wenhan", ""], ["Ma", "Zhan", ""], ["Liu", "Jiaying", ""]]}, {"id": "2002.03720", "submitter": "Shengxin Zhu", "authors": "Binrui Shen, Qiang Niu and Shengxin Zhu", "title": "Fabricated Pictures Detection with Graph Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fabricating experimental pictures in research work is a serious academic\nmisconduct, which should better be detected in the reviewing process. However,\ndue to large number of submissions, the detection whether a picture is\nfabricated or reused is laborious for reviewers, and sometimes is indistinct\nwith human eyes. A tool for detecting similarity between images may help to\nalleviate this problem. Some methods based on local feature points matching\nwork for most of the time, while these methods may result in mess of matchings\ndue to ignorance of global relationship between features. We present a\nframework to detect similar, or perhaps fabricated, pictures with the graph\nmatching techniques. A new iterative method is proposed, and experiments show\nthat such a graph matching technique is better than the methods based only on\nlocal features for some cases.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 12:29:16 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Shen", "Binrui", ""], ["Niu", "Qiang", ""], ["Zhu", "Shengxin", ""]]}, {"id": "2002.03721", "submitter": "Matthias Perkonigg", "authors": "Matthias Perkonigg and Daniel Sobotka and Ahmed Ba-Ssalamah and Georg\n  Langs", "title": "Unsupervised deep clustering for predictive texture pattern discovery in\n  medical images", "comments": "Medical Imaging meets NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive marker patterns in imaging data are a means to quantify disease\nand progression, but their identification is challenging, if the underlying\nbiology is poorly understood. Here, we present a method to identify predictive\ntexture patterns in medical images in an unsupervised way. Based on deep\nclustering networks, we simultaneously encode and cluster medical image patches\nin a low-dimensional latent space. The resulting clusters serve as features for\ndisease staging, linking them to the underlying disease. We evaluate the method\non 70 T1-weighted magnetic resonance images of patients with different stages\nof liver steatosis. The deep clustering approach is able to find predictive\nclusters with a stable ranking, differentiating between low and high steatosis\nwith an F1-Score of 0.78.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 10:57:59 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Perkonigg", "Matthias", ""], ["Sobotka", "Daniel", ""], ["Ba-Ssalamah", "Ahmed", ""], ["Langs", "Georg", ""]]}, {"id": "2002.03723", "submitter": "Ying Huang", "authors": "Ying Huang, Wenwei Zhang, and Jinzhuo Wang", "title": "Deep Frequent Spatial Temporal Learning for Face Anti-Spoofing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is crucial for the security of face recognition system, by\navoiding invaded with presentation attack. Previous works have shown the\neffectiveness of using depth and temporal supervision for this task. However,\ndepth supervision is often considered only in a single frame, and temporal\nsupervision is explored by utilizing certain signals which is not robust to the\nchange of scenes. In this work, motivated by two stream ConvNets, we propose a\nnovel two stream FreqSaptialTemporalNet for face anti-spoofing which\nsimultaneously takes advantage of frequent, spatial and temporal information.\nCompared with existing methods which mine spoofing cues in multi-frame RGB\nimage, we make multi-frame spectrum image as one input stream for the\ndiscriminative deep neural network, encouraging the primary difference between\nlive and fake video to be automatically unearthed. Extensive experiments show\npromising improvement results using the proposed architecture. Meanwhile, we\nproposed a concise method to obtain a large amount of spoofing training data by\nutilizing a frequent augmentation pipeline, which contributes detail\nvisualization between live and fake images as well as data insufficiency issue\nwhen training large networks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 06:02:45 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Huang", "Ying", ""], ["Zhang", "Wenwei", ""], ["Wang", "Jinzhuo", ""]]}, {"id": "2002.03727", "submitter": "Akif Quddus Khan", "authors": "Akif Quddus Khan, Salman Khan", "title": "Durocmien: A deep framework for duroc skeleton extraction in constraint\n  environment", "comments": "8 Pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Farm animal behavior analysis is a crucial tasks for the industrial farming.\nIn an indoor farm setting, extracting Key joints of animal is essential for\ntracking the animal for longer period of time. In this paper, we proposed a\ndeep network named DUROCMIEN that exploit transfer learning to trained the\nnetwork for the Duroc, a domestic breed of pig, an end to end fashion. The\nbackbone of the architecture is based on hourglass stacked dense-net. In order\nto train the network, key frames are selected from the test data using K-mean\nsampler. In total, 9 Keypoints are annotated that gives a brief detailed\nbehavior analysis in the farm setting. Extensive experiments are conducted and\nthe quantitative results show that the network has the potential of increasing\nthe tracking performance by a substantial margin.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 14:49:36 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Khan", "Akif Quddus", ""], ["Khan", "Salman", ""]]}, {"id": "2002.03728", "submitter": "Rateb Jabbar Mr.", "authors": "Rateb Jabbar, Mohammed Shinoy, Mohamed Kharbeche, Khalifa Al-Khalifa,\n  Moez Krichen, Kamel Barkaoui", "title": "Driver Drowsiness Detection Model Using Convolutional Neural Networks\n  Techniques for Android Application", "comments": "6 pages, 5 figures, iociot2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A sleepy driver is arguably much more dangerous on the road than the one who\nis speeding as he is a victim of microsleeps. Automotive researchers and\nmanufacturers are trying to curb this problem with several technological\nsolutions that will avert such a crisis. This article focuses on the detection\nof such micro sleep and drowsiness using neural network based methodologies.\nOur previous work in this field involved using machine learning with\nmulti-layer perceptron to detect the same. In this paper, accuracy was\nincreased by utilizing facial landmarks which are detected by the camera and\nthat is passed to a Convolutional Neural Network (CNN) to classify drowsiness.\nThe achievement with this work is the capability to provide a lightweight\nalternative to heavier classification models with more than 88% for the\ncategory without glasses, more than 85% for the category night without glasses.\nOn average, more than 83% of accuracy was achieved in all categories. Moreover,\nas for model size, complexity and storage, there is a marked reduction in the\nnew proposed model in comparison to the benchmark model where the maximum size\nis 75 KB. The proposed CNN based model can be used to build a real-time driver\ndrowsiness detection system for embedded systems and Android devices with high\naccuracy and ease of use.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 12:39:50 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Jabbar", "Rateb", ""], ["Shinoy", "Mohammed", ""], ["Kharbeche", "Mohamed", ""], ["Al-Khalifa", "Khalifa", ""], ["Krichen", "Moez", ""], ["Barkaoui", "Kamel", ""]]}, {"id": "2002.03729", "submitter": "Sheng Quan Wang", "authors": "Shengquan Wang, Ang Li, Jiying Chen, Baoyu Zheng, Jiaxin Ji, Li\n  Xianglong", "title": "RSnet: An improvement for Darknet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, when we used this method to identify aircraft targets in remote\nsensing images, we found that there are some defects in our own YOLOv2 and\nDarknet-19 network. Characteristic in the images we identified are not very\nclear, thats why we couldn't get some much more good results. Then we replaced\nthe maxpooling in the yolov3 network as the global maxpooling. Under the same\ntest conditions, we got a higher. It achieves 76.9 AP50 in 100 ms on a\nGTX1050TI, compared to 80.5 AP50 in 627 ms by our net. Map.86% of Map was\nobtained by the improved network, higher than the former.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 09:38:50 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wang", "Shengquan", ""], ["Li", "Ang", ""], ["Chen", "Jiying", ""], ["Zheng", "Baoyu", ""], ["Ji", "Jiaxin", ""], ["Xianglong", "Li", ""]]}, {"id": "2002.03732", "submitter": "Subrata Goswami", "authors": "Subrata Goswami", "title": "Impact of Data Quality on Deep Neural Network Training", "comments": "5 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that data is critical for training neural networks. Lot have\nbeen written about quantities of data required to train networks well. However,\nthere is not much publications on how data quality effects convergence of such\nnetworks. There is dearth of information on what is considered good data ( for\nthe task ). This empirical experimental study explores some impacts of data\nquality. Specific results are shown in the paper how simple changes can have\nimpact on Mean Average Precision (mAP).\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 04:09:48 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Goswami", "Subrata", ""]]}, {"id": "2002.03733", "submitter": "Mingqing Yao", "authors": "Shanhui Sun, Jing Hu, Mingqing Yao, Jinrong Hu, Xiaodong Yang, Qi\n  Song, Xi Wu", "title": "Robust Multimodal Image Registration Using Deep Recurrent Reinforcement\n  Learning", "comments": null, "journal-ref": "Asian Conference on Computer Vision (ACCV). 2018. 511-526", "doi": "10.1007/978-3-030-20890-5_33", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The crucial components of a conventional image registration method are the\nchoice of the right feature representations and similarity measures. These two\ncomponents, although elaborately designed, are somewhat handcrafted using human\nknowledge. To this end, these two components are tackled in an end-to-end\nmanner via reinforcement learning in this work. Specifically, an artificial\nagent, which is composed of a combined policy and value network, is trained to\nadjust the moving image toward the right direction. We train this network using\nan asynchronous reinforcement learning algorithm, where a customized reward\nfunction is also leveraged to encourage robust image registration. This trained\nnetwork is further incorporated with a lookahead inference to improve the\nregistration capability. The advantage of this algorithm is fully demonstrated\nby our superior performance on clinical MR and CT image pairs to other\nstate-of-the-art medical image registration methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 12:22:09 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Sun", "Shanhui", ""], ["Hu", "Jing", ""], ["Yao", "Mingqing", ""], ["Hu", "Jinrong", ""], ["Yang", "Xiaodong", ""], ["Song", "Qi", ""], ["Wu", "Xi", ""]]}, {"id": "2002.03734", "submitter": "David Dehaene", "authors": "David Dehaene, Oriel Frigo, S\\'ebastien Combrexelle, Pierre Eline", "title": "Iterative energy-based projection on a normal data manifold for anomaly\n  localization", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoder reconstructions are widely used for the task of unsupervised\nanomaly localization. Indeed, an autoencoder trained on normal data is expected\nto only be able to reconstruct normal features of the data, allowing the\nsegmentation of anomalous pixels in an image via a simple comparison between\nthe image and its autoencoder reconstruction. In practice however, local\ndefects added to a normal image can deteriorate the whole reconstruction,\nmaking this segmentation challenging. To tackle the issue, we propose in this\npaper a new approach for projecting anomalous data on a autoencoder-learned\nnormal data manifold, by using gradient descent on an energy derived from the\nautoencoder's loss function. This energy can be augmented with regularization\nterms that model priors on what constitutes the user-defined optimal\nprojection. By iteratively updating the input of the autoencoder, we bypass the\nloss of high-frequency information caused by the autoencoder bottleneck. This\nallows to produce images of higher quality than classic reconstructions. Our\nmethod achieves state-of-the-art results on various anomaly localization\ndatasets. It also shows promising results at an inpainting task on the CelebA\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 13:35:41 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Dehaene", "David", ""], ["Frigo", "Oriel", ""], ["Combrexelle", "S\u00e9bastien", ""], ["Eline", "Pierre", ""]]}, {"id": "2002.03735", "submitter": "G C Nandi", "authors": "Sayantan Chatterjee, Faheem H. Zunjani, Souvik Sen and Gora C. Nandi", "title": "Real-Time Object Detection and Recognition on Low-Compute Humanoid\n  Robots using Deep Learning", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We envision that in the near future, humanoid robots would share home space\nand assist us in our daily and routine activities through object manipulations.\nOne of the fundamental technologies that need to be developed for robots is to\nenable them to detect objects and recognize them for effective manipulations\nand take real-time decisions involving those objects. In this paper, we\ndescribe a novel architecture that enables multiple low-compute NAO robots to\nperform real-time detection, recognition and localization of objects in its\ncamera view and take programmable actions based on the detected objects. The\nproposed algorithm for object detection and localization is an empirical\nmodification of YOLOv3, based on indoor experiments in multiple scenarios, with\na smaller weight size and lesser computational requirements. Quantization of\nthe weights and re-adjusting filter sizes and layer arrangements for\nconvolutions improved the inference time for low-resolution images from the\nrobot s camera feed. YOLOv3 was chosen after a comparative study of bounding\nbox algorithms was performed with an objective to choose one that strikes the\nperfect balance among information retention, low inference time and high\naccuracy for real-time object detection and localization. The architecture also\ncomprises of an effective end-to-end pipeline to feed the real-time frames from\nthe camera feed to the neural net and use its results for guiding the robot\nwith customizable actions corresponding to the detected class labels.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 05:24:58 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Chatterjee", "Sayantan", ""], ["Zunjani", "Faheem H.", ""], ["Sen", "Souvik", ""], ["Nandi", "Gora C.", ""]]}, {"id": "2002.03736", "submitter": "Yaozu Ye", "authors": "Yaozu Ye, Kailun Yang, Kaite Xiang, Juan Wang and Kaiwei Wang", "title": "Universal Semantic Segmentation for Fisheye Urban Driving Images", "comments": "SMC2020 recieved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a critical method in the field of autonomous\ndriving. When performing semantic image segmentation, a wider field of view\n(FoV) helps to obtain more information about the surrounding environment,\nmaking automatic driving safer and more reliable, which could be offered by\nfisheye cameras. However, large public fisheye datasets are not available, and\nthe fisheye images captured by the fisheye camera with large FoV comes with\nlarge distortion, so commonly-used semantic segmentation model cannot be\ndirectly utilized. In this paper, a seven degrees of freedom (DoF) augmentation\nmethod is proposed to transform rectilinear image to fisheye image in a more\ncomprehensive way. In the training process, rectilinear images are transformed\ninto fisheye images in seven DoF, which simulates the fisheye images taken by\ncameras of different positions, orientations and focal lengths. The result\nshows that training with the seven-DoF augmentation can improve the model's\naccuracy and robustness against different distorted fisheye data. This\nseven-DoF augmentation provides a universal semantic segmentation solution for\nfisheye cameras in different autonomous driving applications. Also, we provide\nspecific parameter settings of the augmentation for autonomous driving. At\nlast, we tested our universal semantic segmentation model on real fisheye\nimages and obtained satisfactory results. The code and configurations are\nreleased at https://github.com/Yaozhuwa/FisheyeSeg.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 11:19:00 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 13:02:09 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ye", "Yaozu", ""], ["Yang", "Kailun", ""], ["Xiang", "Kaite", ""], ["Wang", "Juan", ""], ["Wang", "Kaiwei", ""]]}, {"id": "2002.03737", "submitter": "Chuanguang Yang", "authors": "Chuanguang Yang, Zhulin An, Xiaolong Hu, Hui Zhu, Yongjun Xu", "title": "Localizing Interpretable Multi-scale informative Patches Derived from\n  Media Classification Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) always depend on wider receptive\nfield (RF) and more complex non-linearity to achieve state-of-the-art\nperformance, while suffering the increased difficult to interpret how relevant\npatches contribute the final prediction. In this paper, we construct an\ninterpretable AnchorNet equipped with our carefully designed RFs and linearly\nspatial aggregation to provide patch-wise interpretability of the input media\nmeanwhile localizing multi-scale informative patches only supervised on\nmedia-level labels without any extra bounding box annotations. Visualization of\nlocalized informative image and text patches show the superior multi-scale\nlocalization capability of AnchorNet. We further use localized patches for\ndownstream classification tasks across widely applied networks. Experimental\nresults demonstrate that replacing the original inputs with their patches for\nclassification can get a clear inference acceleration with only tiny\nperformance degradation, which proves that localized patches can indeed retain\nthe most semantics and evidences of the original inputs.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 10:04:18 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 08:14:52 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Yang", "Chuanguang", ""], ["An", "Zhulin", ""], ["Hu", "Xiaolong", ""], ["Zhu", "Hui", ""], ["Xu", "Yongjun", ""]]}, {"id": "2002.03740", "submitter": "Zijian Zhang", "authors": "Shuwen Xiao, Zhou Zhao, Zijian Zhang, Xiaohui Yan, Min Yang", "title": "Convolutional Hierarchical Attention Network for Query-Focused Video\n  Summarization", "comments": "Accepted by AAAI 2020 Conference", "journal-ref": null, "doi": "10.1609/aaai.v34i07.6929", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous approaches for video summarization mainly concentrate on finding the\nmost diverse and representative visual contents as video summary without\nconsidering the user's preference. This paper addresses the task of\nquery-focused video summarization, which takes user's query and a long video as\ninputs and aims to generate a query-focused video summary. In this paper, we\nconsider the task as a problem of computing similarity between video shots and\nquery. To this end, we propose a method, named Convolutional Hierarchical\nAttention Network (CHAN), which consists of two parts: feature encoding network\nand query-relevance computing module. In the encoding network, we employ a\nconvolutional network with local self-attention mechanism and query-aware\nglobal attention mechanism to learns visual information of each shot. The\nencoded features will be sent to query-relevance computing module to generate\nqueryfocused video summary. Extensive experiments on the benchmark dataset\ndemonstrate the competitive performance and show the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 04:30:14 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 02:44:09 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 03:26:30 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Xiao", "Shuwen", ""], ["Zhao", "Zhou", ""], ["Zhang", "Zijian", ""], ["Yan", "Xiaohui", ""], ["Yang", "Min", ""]]}, {"id": "2002.03741", "submitter": "Lu Yang", "authors": "Liang Zhang, Yufei Liu, Hang Xiao, Lu Yang, Guangming Zhu, Syed Afaq\n  Shah, Mohammed Bennamoun, and Peiyi Shen", "title": "Efficient Scene Text Detection with Textual Attention Tower", "comments": "Accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection has received attention for years and achieved an\nimpressive performance across various benchmarks. In this work, we propose an\nefficient and accurate approach to detect multioriented text in scene images.\nThe proposed feature fusion mechanism allows us to use a shallower network to\nreduce the computational complexity. A self-attention mechanism is adopted to\nsuppress false positive detections. Experiments on public benchmarks including\nICDAR 2013, ICDAR 2015 and MSRA-TD500 show that our proposed approach can\nachieve better or comparable performances with fewer parameters and less\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 09:50:08 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhang", "Liang", ""], ["Liu", "Yufei", ""], ["Xiao", "Hang", ""], ["Yang", "Lu", ""], ["Zhu", "Guangming", ""], ["Shah", "Syed Afaq", ""], ["Bennamoun", "Mohammed", ""], ["Shen", "Peiyi", ""]]}, {"id": "2002.03742", "submitter": "Mizanur Rahman", "authors": "Mizanur Rahman, Mhafuzul Islam, Jon C. Calhoun and Mashrur Chowdhury", "title": "Dynamic Error-bounded Lossy Compression (EBLC) to Reduce the Bandwidth\n  Requirement for Real-time Vision-based Pedestrian Safety Applications", "comments": "10 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As camera quality improves and their deployment moves to areas with limited\nbandwidth, communication bottlenecks can impair real-time constraints of an ITS\napplication, such as video-based real-time pedestrian detection. Video\ncompression reduces the bandwidth requirement to transmit the video but\ndegrades the video quality. As the quality level of the video decreases, it\nresults in the corresponding decreases in the accuracy of the vision-based\npedestrian detection model. Furthermore, environmental conditions (e.g., rain\nand darkness) alter the compression ratio and can make maintaining a high\npedestrian detection accuracy more difficult. The objective of this study is to\ndevelop a real-time error-bounded lossy compression (EBLC) strategy to\ndynamically change the video compression level depending on different\nenvironmental conditions in order to maintain a high pedestrian detection\naccuracy. We conduct a case study to show the efficacy of our dynamic EBLC\nstrategy for real-time vision-based pedestrian detection under adverse\nenvironmental conditions. Our strategy selects the error tolerances dynamically\nfor lossy compression that can maintain a high detection accuracy across a\nrepresentative set of environmental conditions. Analyses reveal that our\nstrategy increases pedestrian detection accuracy up to 14% and reduces the\ncommunication bandwidth up to 14x for adverse environmental conditions compared\nto the same conditions but without our dynamic EBLC strategy. Our dynamic EBLC\nstrategy is independent of detection models and environmental conditions\nallowing other detection models and environmental conditions to be easily\nincorporated in our strategy.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 17:21:51 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Rahman", "Mizanur", ""], ["Islam", "Mhafuzul", ""], ["Calhoun", "Jon C.", ""], ["Chowdhury", "Mashrur", ""]]}, {"id": "2002.03746", "submitter": "Riccardo Guidotti", "authors": "Riccardo Guidotti, Anna Monreale, Stan Matwin, Dino Pedreschi", "title": "Black Box Explanation by Learning Image Exemplars in the Latent Feature\n  Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to explain the decisions of black box models for image\nclassification. While using the black box to label images, our explanation\nmethod exploits the latent feature space learned through an adversarial\nautoencoder. The proposed method first generates exemplar images in the latent\nfeature space and learns a decision tree classifier. Then, it selects and\ndecodes exemplars respecting local decision rules. Finally, it visualizes them\nin a manner that shows to the user how the exemplars can be modified to either\nstay within their class, or to become counter-factuals by \"morphing\" into\nanother class. Since we focus on black box decision systems for image\nclassification, the explanation obtained from the exemplars also provides a\nsaliency map highlighting the areas of the image that contribute to its\nclassification, and areas of the image that push it into another class. We\npresent the results of an experimental evaluation on three datasets and two\nblack box models. Besides providing the most useful and interpretable\nexplanations, we show that the proposed method outperforms existing explainers\nin terms of fidelity, relevance, coherence, and stability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 15:42:14 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Guidotti", "Riccardo", ""], ["Monreale", "Anna", ""], ["Matwin", "Stan", ""], ["Pedreschi", "Dino", ""]]}, {"id": "2002.03749", "submitter": "Hartmut Feld", "authors": "Hartmut Feld, Bruno Mirbach, Jigyasa Katrolia, Mohamed Selim, Oliver\n  Wasenm\\\"uller, Didier Stricker", "title": "DFKI Cabin Simulator: A Test Platform for Visual In-Cabin Monitoring\n  Functions", "comments": "corrected typos and bad reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a test platform for visual in-cabin scene analysis and occupant\nmonitoring functions. The test platform is based on a driving simulator\ndeveloped at the DFKI, consisting of a realistic in-cabin mock-up and a\nwide-angle projection system for a realistic driving experience. The platform\nhas been equipped with a wide-angle 2D/3D camera system monitoring the entire\ninterior of the vehicle mock-up of the simulator. It is also supplemented with\na ground truth reference sensor system that allows to track and record the\noccupant's body movements synchronously with the 2D and 3D video streams of the\ncamera. Thus, the resulting test platform will serve as a basis to validate\nnumerous in-cabin monitoring functions, which are important for the realization\nof novel human-vehicle interfaces, advanced driver assistant systems, and\nautomated driving. Among the considered functions are occupant presence\ndetection, size and 3D-pose estimation and driver intention recognition. In\naddition, our platform will be the basis for the creation of large-scale\nin-cabin benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 07:15:50 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 12:27:42 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Feld", "Hartmut", ""], ["Mirbach", "Bruno", ""], ["Katrolia", "Jigyasa", ""], ["Selim", "Mohamed", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "2002.03750", "submitter": "Milad Taleby Ahvanooey", "authors": "Milad Taleby Ahvanooey, Qianmu Li", "title": "An Overview of Two Age Synthesis and Estimation Techniques", "comments": "16 pages, 5 figures. International Congress on Engineering Science\n  and Sustainable Urban Development Denmark, Copenhagen, September 2018", "journal-ref": null, "doi": null, "report-no": "GA-01089-AB", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Age estimation is a technique for predicting human ages from digital facial\nimages, which analyzes a person's face image and estimates his/her age based on\nthe year measure. Nowadays, intelligent age estimation and age synthesis have\nbecome particularly prevalent research topics in computer vision and face\nverification systems. Age synthesis is defined to render a facial image\naesthetically with rejuvenating and natural aging effects on the person's face.\nAge estimation is defined to label a facial image automatically with the age\ngroup (year range) or the exact age (year) of the person's face. In this case\nstudy, we overview the existing models, popular techniques, system\nperformances, and technical challenges related to the facial image-based age\nsynthesis and estimation topics. The main goal of this review is to provide an\neasy understanding and promising future directions with systematic discussions.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 06:07:49 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ahvanooey", "Milad Taleby", ""], ["Li", "Qianmu", ""]]}, {"id": "2002.03751", "submitter": "Yilan Li", "authors": "Yilan Li, Senem Velipasalar", "title": "Weighted Average Precision: Adversarial Example Detection in the Visual\n  Perception of Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that neural networks are vulnerable to carefully\ncrafted adversarial examples (AE). By adding small perturbations to input\nimages, AEs are able to make the victim model predicts incorrect outputs.\nSeveral research work in adversarial machine learning started to focus on the\ndetection of AEs in autonomous driving. However, the existing studies either\nuse preliminary assumption on outputs of detections or ignore the tracking\nsystem in the perception pipeline. In this paper, we firstly propose a novel\ndistance metric for practical autonomous driving object detection outputs.\nThen, we bridge the gap between the current AE detection research and the\nreal-world autonomous systems by providing a temporal detection algorithm,\nwhich takes the impact of tracking system into consideration. We perform\nevaluation on Berkeley Deep Drive (BDD) and CityScapes datasets to show how our\napproach outperforms existing single-frame-mAP based AE detections by\nincreasing 17.76% accuracy of performance.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 23:59:34 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 00:50:00 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Li", "Yilan", ""], ["Velipasalar", "Senem", ""]]}, {"id": "2002.03752", "submitter": "Vikram Shree", "authors": "Vikram Shree, Wei-Lun Chao and Mark Campbell", "title": "An Empirical Study of Person Re-Identification with Attributes", "comments": "Accepted by RO-MAN 2019, 28th IEEE International Conference on Robot\n  and Human Interactive Communication (RO-MAN). IEEE, 2019", "journal-ref": null, "doi": "10.1109/RO-MAN46459.2019.8956459", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims to identify a person from an image collection,\ngiven one image of that person as the query. There is, however, a plethora of\nreal-life scenarios where we may not have a priori library of query images and\ntherefore must rely on information from other modalities. In this paper, an\nattribute-based approach is proposed where the person of interest (POI) is\ndescribed by a set of visual attributes, which are used to perform the search.\nWe compare multiple algorithms and analyze how the quality of attributes\nimpacts the performance. While prior work mostly relies on high precision\nattributes annotated by experts, we conduct a human-subject study and reveal\nthat certain visual attributes could not be consistently described by human\nobservers, making them less reliable in real applications. A key conclusion is\nthat the performance achieved by non-expert attributes, instead of\nexpert-annotated ones, is a more faithful indicator of the status quo of\nattribute-based approaches for person re-identification.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 22:18:51 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Shree", "Vikram", ""], ["Chao", "Wei-Lun", ""], ["Campbell", "Mark", ""]]}, {"id": "2002.03754", "submitter": "Andrey Voynov", "authors": "Andrey Voynov, Artem Babenko", "title": "Unsupervised Discovery of Interpretable Directions in the GAN Latent\n  Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent spaces of GAN models often have semantically meaningful\ndirections. Moving in these directions corresponds to human-interpretable image\ntransformations, such as zooming or recoloring, enabling a more controllable\ngeneration process. However, the discovery of such directions is currently\nperformed in a supervised manner, requiring human labels, pretrained models, or\nsome form of self-supervision. These requirements severely restrict a range of\ndirections existing approaches can discover. In this paper, we introduce an\nunsupervised method to identify interpretable directions in the latent space of\na pretrained GAN model. By a simple model-agnostic procedure, we find\ndirections corresponding to sensible semantic manipulations without any form of\n(self-)supervision. Furthermore, we reveal several non-trivial findings, which\nwould be difficult to obtain by existing methods, e.g., a direction\ncorresponding to background removal. As an immediate practical benefit of our\nwork, we show how to exploit this finding to achieve competitive performance\nfor weakly-supervised saliency detection.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 13:57:14 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 10:08:49 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 12:12:14 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Voynov", "Andrey", ""], ["Babenko", "Artem", ""]]}, {"id": "2002.03761", "submitter": "Wenlin Zhuang", "authors": "Wenlin Zhuang, Congyi Wang, Siyu Xia, Jinxiang Chai, Yangang Wang", "title": "Music2Dance: DanceNet for Music-driven Dance Generation", "comments": "Our results are shown at https://youtu.be/bTHSrfEHcG8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesize human motions from music, i.e., music to dance, is appealing and\nattracts lots of research interests in recent years. It is challenging due to\nnot only the requirement of realistic and complex human motions for dance, but\nmore importantly, the synthesized motions should be consistent with the style,\nrhythm and melody of the music. In this paper, we propose a novel\nautoregressive generative model, DanceNet, to take the style, rhythm and melody\nof music as the control signals to generate 3D dance motions with high realism\nand diversity. To boost the performance of our proposed model, we capture\nseveral synchronized music-dance pairs by professional dancers, and build a\nhigh-quality music-dance pair dataset. Experiments have demonstrated that the\nproposed method can achieve the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 17:18:31 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 18:32:15 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Zhuang", "Wenlin", ""], ["Wang", "Congyi", ""], ["Xia", "Siyu", ""], ["Chai", "Jinxiang", ""], ["Wang", "Yangang", ""]]}, {"id": "2002.03763", "submitter": "Shenghua He", "authors": "Shenghua He and Weimin Zhou and Hua Li and Mark A. Anastasio", "title": "Learning Numerical Observers using Unsupervised Domain Adaptation", "comments": "SPIE Medical Imaging 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging systems are commonly assessed by use of objective image\nquality measures. Supervised deep learning methods have been investigated to\nimplement numerical observers for task-based image quality assessment. However,\nlabeling large amounts of experimental data to train deep neural networks is\ntedious, expensive, and prone to subjective errors. Computer-simulated image\ndata can potentially be employed to circumvent these issues; however, it is\noften difficult to computationally model complicated anatomical structures,\nnoise sources, and the response of real world imaging systems. Hence, simulated\nimage data will generally possess physical and statistical differences from the\nexperimental image data they seek to emulate. Within the context of machine\nlearning, these differences between the sets of two images is referred to as\ndomain shift. In this study, we propose and investigate the use of an\nadversarial domain adaptation method to mitigate the deleterious effects of\ndomain shift between simulated and experimental image data for deep\nlearning-based numerical observers (DL-NOs) that are trained on simulated\nimages but applied to experimental ones. In the proposed method, a DL-NO will\ninitially be trained on computer-simulated image data and subsequently adapted\nfor use with experimental image data, without the need for any labeled\nexperimental images. As a proof of concept, a binary signal detection task is\nconsidered. The success of this strategy as a function of the degree of domain\nshift present between the simulated and experimental image data is\ninvestigated.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 22:58:28 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 16:56:25 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["He", "Shenghua", ""], ["Zhou", "Weimin", ""], ["Li", "Hua", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "2002.03772", "submitter": "Gon\\c{c}alo Mordido", "authors": "Julian Niedermeier, Gon\\c{c}alo Mordido, Christoph Meinel", "title": "Improving the Evaluation of Generative Models with Fuzzy Logic", "comments": "AAAI 2020 Meta-Eval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective and interpretable metrics to evaluate current artificial\nintelligent systems are of great importance, not only to analyze the current\nstate of such systems but also to objectively measure progress in the future.\nIn this work, we focus on the evaluation of image generation tasks. We propose\na novel approach, called Fuzzy Topology Impact (FTI), that determines both the\nquality and diversity of an image set using topology representations combined\nwith fuzzy logic. When compared to current evaluation methods, FTI shows better\nand more stable performance on multiple experiments evaluating the sensitivity\nto noise, mode dropping and mode inventing.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 13:42:53 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Niedermeier", "Julian", ""], ["Mordido", "Gon\u00e7alo", ""], ["Meinel", "Christoph", ""]]}, {"id": "2002.03773", "submitter": "Kashif Ahmad", "authors": "Kashif Ahmad, Syed Zohaib, Nicola Conci and Ala Al-Fuqaha", "title": "Deriving Emotions and Sentiments from Visual Content: A Disaster\n  Analysis Use Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis aims to extract and express a person's perception,\nopinions and emotions towards an entity, object, product and a service,\nenabling businesses to obtain feedback from the consumers. The increasing\npopularity of the social networks and users' tendency towards sharing their\nfeelings, expressions and opinions in text, visual and audio content has opened\nnew opportunities and challenges in sentiment analysis. While sentiment\nanalysis of text streams has been widely explored in the literature, sentiment\nanalysis of images and videos is relatively new. This article introduces visual\nsentiment analysis and contrasts it with textual sentiment analysis with\nemphasis on the opportunities and challenges in this nascent research area. We\nalso propose a deep visual sentiment analyzer for disaster-related images as a\nuse-case, covering different aspects of visual sentiment analysis starting from\ndata collection, annotation, model selection, implementation and evaluations.\nWe believe such rigorous analysis will provide a baseline for future research\nin the domain.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 08:48:52 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ahmad", "Kashif", ""], ["Zohaib", "Syed", ""], ["Conci", "Nicola", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "2002.03776", "submitter": "Eduardo Soares Mr", "authors": "Plamen Angelov, Eduardo Soares", "title": "Towards Deep Machine Reasoning: a Prototype-based Deep Neural Network\n  with Decision Tree Inference", "comments": "Submitted to the IEEE Joint Conference on Neural Networks (IJCNN -\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the DMR -- a prototype-based method and network\narchitecture for deep learning which is using a decision tree (DT)-based\ninference and synthetic data to balance the classes. It builds upon the\nrecently introduced xDNN method addressing more complex multi-class problems,\nspecifically when classes are highly imbalanced. DMR moves away from a direct\ndecision based on all classes towards a layered DT of pair-wise class\ncomparisons. In addition, it forces the prototypes to be balanced between\nclasses regardless of possible class imbalances of the training data. It has\ntwo novel mechanisms, namely i) using a DT to determine the winning class\nlabel, and ii) balancing the classes by synthesizing data around the prototypes\ndetermined from the available training data. As a result, we improved\nsignificantly the performance of the resulting fully explainable DNN as\nevidenced by the best reported result on the well know benchmark problem\nCaltech-101 surpassing our own recently published \"world record\". Furthermore,\nwe also achieved another \"world record\" for another very hard benchmark\nproblem, namely Caltech-256 as well as surpassed the results of other\napproaches on Faces-1999 problem. In summary, we propose a new approach\nspecifically advantageous for imbalanced multi-class problems that achieved two\nworld records on well known hard benchmark problems and the best result on\nanother problem in terms of accuracy. Moreover, DMR offers full explainability,\ndoes not require GPUs and can continue to learn from new data by adding new\nprototypes preserving the previous ones but not requiring full retraining.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 14:11:07 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Angelov", "Plamen", ""], ["Soares", "Eduardo", ""]]}, {"id": "2002.03779", "submitter": "Hamidreza Kasaei", "authors": "S. Hamidreza Kasaei, Maryam Ghorbani, Jits Schilperoort, Wessel van\n  der Rest", "title": "Investigating the Importance of Shape Features, Color Constancy, Color\n  Spaces and Similarity Measures in Open-Ended 3D Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of state-of-the-art 3D object recognition\napproaches, service robots are frequently failed to recognize many objects in\nreal human-centric environments. For these robots, object recognition is a\nchallenging task due to the high demand for accurate and real-time response\nunder changing and unpredictable environmental conditions. Most of the recent\napproaches use either the shape information only and ignore the role of color\ninformation or vice versa. Furthermore, they mainly utilize the $L_n$ Minkowski\nfamily functions to measure the similarity of two object views, while there are\nvarious distance measures that are applicable to compare two object views. In\nthis paper, we explore the importance of shape information, color constancy,\ncolor spaces, and various similarity measures in open-ended 3D object\nrecognition. Towards this goal, we extensively evaluate the performance of\nobject recognition approaches in three different configurations, including\n\\textit{color-only}, \\textit{shape-only}, and \\textit{ combinations of color\nand shape}, in both offline and online settings. Experimental results\nconcerning scalability, memory usage, and object recognition performance show\nthat all of the \\textit{combinations of color and shape} yields significant\nimprovements over the \\textit{shape-only} and \\textit{color-only} approaches.\nThe underlying reason is that color information is an important feature to\ndistinguish objects that have very similar geometric properties with different\ncolors and vice versa. Moreover, by combining color and shape information, we\ndemonstrate that the robot can learn new object categories from very few\ntraining examples in a real-world setting.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 14:24:09 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 12:18:24 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kasaei", "S. Hamidreza", ""], ["Ghorbani", "Maryam", ""], ["Schilperoort", "Jits", ""], ["van der Rest", "Wessel", ""]]}, {"id": "2002.03781", "submitter": "Robin Yancey", "authors": "Robin Elizabeth Yancey", "title": "Multi-stream Faster RCNN for Mitosis Counting in Breast Cancer Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitotic count is a commonly used method to assess the level of progression of\nbreast cancer, which is now the fourth most prevalent cancer. Unfortunately,\ncounting mitosis is a tedious and subjective task with poor reproducibility,\nespecially for non-experts. Luckily, since the machine can read and compare\nmore data with greater efficiency this could be the next modern technique to\ncount mitosis. Furthermore, technological advancements in medicine have led to\nthe increase in image data available for use in training. In this work, we\npropose a network constructed using a similar approach to one that has been\nused for image fraud detection with the segmented image map as the second\nstream input to Faster RCNN. This region-based detection model combines a fully\nconvolutional Region Proposal Network to generate proposals and a\nclassification network to classify each of these proposals as containing\nmitosis or not. Features from both streams are fused in the bilinear pooling\nlayer to maintain the spatial concurrence of each. After training this model on\nthe ICPR 2014 MITOSIS contest dataset, we received an F-measure score of 0.507,\nhigher than both the winners score and scores from recent tests on the same\ndata. Our method is clinically applicable, taking only around five min per ten\nfull High Power Field slides when tested on a Quadro P6000 cloud GPU.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 20:20:00 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Yancey", "Robin Elizabeth", ""]]}, {"id": "2002.03786", "submitter": "Amin Mazloumian", "authors": "Amin Mazloumian (1), Matthias Rosenthal (1), Hans Gelke (1) ((1)\n  Institute of Embedded Systems, Zurich University of Applied Sciences)", "title": "Deep Learning for Classifying Food Waste", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One third of food produced in the world for human consumption --\napproximately 1.3 billion tons -- is lost or wasted every year. By classifying\nfood waste of individual consumers and raising awareness of the measures,\navoidable food waste can be significantly reduced. In this research, we use\ndeep learning to classify food waste in half a million images captured by\ncameras installed on top of food waste bins. We specifically designed a deep\nneural network that classifies food waste for every time food waste is thrown\nin the waste bins. Our method presents how deep learning networks can be\ntailored to best learn from available training data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 12:40:16 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Mazloumian", "Amin", ""], ["Rosenthal", "Matthias", ""], ["Gelke", "Hans", ""]]}, {"id": "2002.03797", "submitter": "Hannaneh Barahouei Pasandi", "authors": "Hannaneh Barahouei Pasandi, Tamer Nadeem", "title": "CONVINCE: Collaborative Cross-Camera Video Analytics at the Edge", "comments": "6 pages, 4 figures, 2020 IEEE International Conference on Pervasive\n  Computing and Communications Workshops (PerCom Workshops)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, video cameras are deployed in dense for monitoring physical places\ne.g., city, industrial, or agricultural sites. In the current systems, each\ncamera node sends its feed to a cloud server individually. However, this\napproach suffers from several hurdles including higher computation cost, large\nbandwidth requirement for analyzing the enormous data, and privacy concerns. In\ndense deployment, video nodes typically demonstrate a significant\nspatio-temporal correlation. To overcome these obstacles in current approaches,\nthis paper introduces CONVINCE, a new approach to look at the network cameras\nas a collective entity that enables collaborative video analytics pipeline\namong cameras. CONVINCE aims at 1) reducing the computation cost and bandwidth\nrequirements by leveraging spatio-temporal correlations among cameras in\neliminating redundant frames intelligently, and ii) improving vision\nalgorithms' accuracy by enabling collaborative knowledge sharing among relevant\ncameras. Our results demonstrate that CONVINCE achieves an object\nidentification accuracy of $\\sim$91\\%, by transmitting only about $\\sim$25\\% of\nall the recorded frames.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 23:55:45 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Pasandi", "Hannaneh Barahouei", ""], ["Nadeem", "Tamer", ""]]}, {"id": "2002.03807", "submitter": "Johanna \\\"Arje", "authors": "Johanna \\\"Arje, Claus Melvad, Mads Rosenh{\\o}j Jeppesen, Sigurd\n  Agerskov Madsen, Jenni Raitoharju, Maria Strandg{\\aa}rd Rasmussen, Alexandros\n  Iosifidis, Ville Tirronen, Kristian Meissner, Moncef Gabbouj, Toke Thomas\n  H{\\o}ye", "title": "Automatic image-based identification and biomass estimation of\n  invertebrates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how biological communities respond to environmental changes is\na key challenge in ecology and ecosystem management. The apparent decline of\ninsect populations necessitates more biomonitoring but the time-consuming\nsorting and identification of taxa pose strong limitations on how many insect\nsamples can be processed. In turn, this affects the scale of efforts to map\ninvertebrate diversity altogether. Given recent advances in computer vision, we\npropose to replace the standard manual approach of human expert-based sorting\nand identification with an automatic image-based technology. We describe a\nrobot-enabled image-based identification machine, which can automate the\nprocess of invertebrate identification, biomass estimation and sample sorting.\nWe use the imaging device to generate a comprehensive image database of\nterrestrial arthropod species. We use this database to test the classification\naccuracy i.e. how well the species identity of a specimen can be predicted from\nimages taken by the machine. We also test sensitivity of the classification\naccuracy to the camera settings (aperture and exposure time) in order to move\nforward with the best possible image quality. We use state-of-the-art Resnet-50\nand InceptionV3 CNNs for the classification task. The results for the initial\ndataset are very promising ($\\overline{ACC}=0.980$). The system is general and\ncan easily be used for other groups of invertebrates as well. As such, our\nresults pave the way for generating more data on spatial and temporal variation\nin invertebrate abundance, diversity and biomass.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 21:38:57 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["\u00c4rje", "Johanna", ""], ["Melvad", "Claus", ""], ["Jeppesen", "Mads Rosenh\u00f8j", ""], ["Madsen", "Sigurd Agerskov", ""], ["Raitoharju", "Jenni", ""], ["Rasmussen", "Maria Strandg\u00e5rd", ""], ["Iosifidis", "Alexandros", ""], ["Tirronen", "Ville", ""], ["Meissner", "Kristian", ""], ["Gabbouj", "Moncef", ""], ["H\u00f8ye", "Toke Thomas", ""]]}, {"id": "2002.03809", "submitter": "Andr\\'e Wyzykowski", "authors": "Andr\\'e Brasil Vieira Wyzykowski, Mauricio Pamplona Segundo, Rubisley\n  de Paula Lemes", "title": "Level Three Synthetic Fingerprint Generation", "comments": "Database are available at https://andrewyzy.github.io/L3-SF/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's legal restrictions that protect the privacy of biometric data are\nhampering fingerprint recognition researches. For instance, all high-resolution\nfingerprint databases ceased to be publicly available. To address this problem,\nwe present a novel hybrid approach to synthesize realistic, high-resolution\nfingerprints. First, we improved Anguli, a handcrafted fingerprint generator,\nto obtain dynamic ridge maps with sweat pores and scratches. Then, we trained a\nCycleGAN to transform these maps into realistic fingerprints. Unlike other\nCNN-based works, we can generate several images for the same identity. We used\nour approach to create a synthetic database with 7400 images in an attempt to\npropel further studies in this field without raising legal issues. We included\nsweat pore annotations in 740 images to encourage research developments in pore\ndetection. In our experiments, we employed two fingerprint matching approaches\nto confirm that real and synthetic databases have similar performance. We\nconducted a human perception analysis where sixty volunteers could hardly\ndiffer between real and synthesized fingerprints. Given that we also favorably\ncompare our results with the most advanced works in the literature, our\nexperimentation suggests that our approach is the new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 14:09:47 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 18:33:01 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 19:18:05 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wyzykowski", "Andr\u00e9 Brasil Vieira", ""], ["Segundo", "Mauricio Pamplona", ""], ["Lemes", "Rubisley de Paula", ""]]}, {"id": "2002.03830", "submitter": "David W. Romero", "authors": "David W. Romero, Erik J. Bekkers, Jakub M. Tomczak, Mark Hoogendoorn", "title": "Attentive Group Equivariant Convolutional Networks", "comments": "Proceedings of the 37th International Conference on Machine Learning\n  (ICML), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although group convolutional networks are able to learn powerful\nrepresentations based on symmetry patterns, they lack explicit means to learn\nmeaningful relationships among them (e.g., relative positions and poses). In\nthis paper, we present attentive group equivariant convolutions, a\ngeneralization of the group convolution, in which attention is applied during\nthe course of convolution to accentuate meaningful symmetry combinations and\nsuppress non-plausible, misleading ones. We indicate that prior work on visual\nattention can be described as special cases of our proposed framework and show\nempirically that our attentive group equivariant convolutional networks\nconsistently outperform conventional group convolutional networks on benchmark\nimage datasets. Simultaneously, we provide interpretability to the learned\nconcepts through the visualization of equivariant attention maps.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 14:06:24 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 12:34:17 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 07:41:35 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Romero", "David W.", ""], ["Bekkers", "Erik J.", ""], ["Tomczak", "Jakub M.", ""], ["Hoogendoorn", "Mark", ""]]}, {"id": "2002.03844", "submitter": "Palash Goyal", "authors": "Palash Goyal, Saurabh Sahu, Shalini Ghosh, Chul Lee", "title": "Exploiting Temporal Coherence for Multi-modal Video Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal ML models can process data in multiple modalities (e.g., video,\nimages, audio, text) and are useful for video content analysis in a variety of\nproblems (e.g., object detection, scene understanding). In this paper, we focus\non the problem of video categorization by using a multimodal approach. We have\ndeveloped a novel temporal coherence-based regularization approach, which\napplies to different types of models (e.g., RNN, NetVLAD, Transformer). We\ndemonstrate through experiments how our proposed multimodal video\ncategorization models with temporal coherence out-perform strong\nstate-of-the-art baseline models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 06:42:12 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 00:17:11 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Goyal", "Palash", ""], ["Sahu", "Saurabh", ""], ["Ghosh", "Shalini", ""], ["Lee", "Chul", ""]]}, {"id": "2002.03846", "submitter": "Felipe Giuste", "authors": "Felipe O. Giuste and Juan C. Vizcarra", "title": "CIFAR-10 Image Classification Using Feature Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification requires the generation of features capable of detecting\nimage patterns informative of group identity. The objective of this study was\nto classify images from the public CIFAR-10 image dataset by leveraging\ncombinations of disparate image feature sources from both manual and deep\nlearning approaches. Histogram of oriented gradients (HOG) and pixel\nintensities successfully inform classification (53% and 59% classification\naccuracy, respectively), yet there is much room for improvement. VGG16 with\nImageNet trained weights and a CIFAR-10 optimized model (CIFAR-VGG) further\nimprove upon image classification (60% and 93.43% accuracy, respectively). We\nfurther improved classification by utilizing transfer learning to re-establish\noptimal network weights for VGG16 (TL-VGG) and Inception ResNet v2\n(TL-Inception) resulting in significant performance increases (85% and 90.74%,\nrespectively), yet fail to surpass CIFAR-VGG. We hypothesized that if each\ngenerated feature set obtained some unique insight into the classification\nproblem, then combining these features would result in greater classification\naccuracy, surpassing that of CIFAR-VGG. Upon selection of the top 1000\nprincipal components from TL-VGG, TL-Inception, HOG, pixel intensities, and\nCIFAR-VGG, we achieved testing accuracy of 94.6%, lending support to our\nhypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 01:53:46 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 22:33:53 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Giuste", "Felipe O.", ""], ["Vizcarra", "Juan C.", ""]]}, {"id": "2002.03895", "submitter": "Stephen Hausler", "authors": "Stephen Hausler and Michael Milford", "title": "Hierarchical Multi-Process Fusion for Visual Place Recognition", "comments": "Pre-print version of article which will be presented at ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining multiple complementary techniques together has long been regarded\nas a way to improve performance. In visual localization, multi-sensor fusion,\nmulti-process fusion of a single sensing modality, and even combinations of\ndifferent localization techniques have been shown to result in improved\nperformance. However, merely fusing together different localization techniques\ndoes not account for the varying performance characteristics of different\nlocalization techniques. In this paper we present a novel, hierarchical\nlocalization system that explicitly benefits from three varying characteristics\nof localization techniques: the distribution of their localization hypotheses,\ntheir appearance- and viewpoint-invariant properties, and the resulting\ndifferences in where in an environment each system works well and fails. We\nshow how two techniques deployed hierarchically work better than in parallel\nfusion, how combining two different techniques works better than two levels of\na single technique, even when the single technique has superior individual\nperformance, and develop two and three-tier hierarchical structures that\nprogressively improve localization performance. Finally, we develop a stacked\nhierarchical framework where localization hypotheses from techniques with\ncomplementary characteristics are concatenated at each layer, significantly\nimproving retention of the correct hypothesis through to the final localization\nstage. Using two challenging datasets, we show the proposed system\noutperforming state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 00:34:39 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Hausler", "Stephen", ""], ["Milford", "Michael", ""]]}, {"id": "2002.03923", "submitter": "Piotr Koniusz", "authors": "Xin Yu and Zheyu Zhuang and Piotr Koniusz and Hongdong Li", "title": "6DoF Object Pose Estimation via Differentiable Proxy Voting Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a 6DOF object pose from a single image is very challenging due to\nocclusions or textureless appearances. Vector-field based keypoint voting has\ndemonstrated its effectiveness and superiority on tackling those issues.\nHowever, direct regression of vector-fields neglects that the distances between\npixels and keypoints also affect the deviations of hypotheses dramatically. In\nother words, small errors in direction vectors may generate severely deviated\nhypotheses when pixels are far away from a keypoint. In this paper, we aim to\nreduce such errors by incorporating the distances between pixels and keypoints\ninto our objective. To this end, we develop a simple yet effective\ndifferentiable proxy voting loss (DPVL) which mimics the hypothesis selection\nin the voting procedure. By exploiting our voting loss, we are able to train\nour network in an end-to-end manner. Experiments on widely used datasets, i.e.,\nLINEMOD and Occlusion LINEMOD, manifest that our DPVL improves pose estimation\nperformance significantly and speeds up the training convergence.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 16:33:33 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 22:24:55 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Yu", "Xin", ""], ["Zhuang", "Zheyu", ""], ["Koniusz", "Piotr", ""], ["Li", "Hongdong", ""]]}, {"id": "2002.03933", "submitter": "Hossam Isack", "authors": "Hossam Isack, Christian Haene, Cem Keskin, Sofien Bouaziz, Yuri\n  Boykov, Shahram Izadi and Sameh Khamis", "title": "RePose: Learning Deep Kinematic Priors for Fast Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel efficient and lightweight model for human pose estimation\nfrom a single image. Our model is designed to achieve competitive results at a\nfraction of the number of parameters and computational cost of various\nstate-of-the-art methods. To this end, we explicitly incorporate part-based\nstructural and geometric priors in a hierarchical prediction framework. At the\ncoarsest resolution, and in a manner similar to classical part-based\napproaches, we leverage the kinematic structure of the human body to propagate\nconvolutional feature updates between the keypoints or body parts. Unlike\nclassical approaches, we adopt end-to-end training to learn this geometric\nprior through feature updates from data. We then propagate the feature\nrepresentation at the coarsest resolution up the hierarchy to refine the\npredicted pose in a coarse-to-fine fashion. The final network effectively\nmodels the geometric prior and intuition within a lightweight deep neural\nnetwork, yielding state-of-the-art results for a model of this size on two\nstandard datasets, Leeds Sports Pose and MPII Human Pose.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 16:44:45 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Isack", "Hossam", ""], ["Haene", "Christian", ""], ["Keskin", "Cem", ""], ["Bouaziz", "Sofien", ""], ["Boykov", "Yuri", ""], ["Izadi", "Shahram", ""], ["Khamis", "Sameh", ""]]}, {"id": "2002.03982", "submitter": "Mirco Planamente", "authors": "Mirco Planamente, Andrea Bottino, Barbara Caputo", "title": "Self-Supervised Joint Encoding of Motion and Appearance for First Person\n  Action Recognition", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable cameras are becoming more and more popular in several applications,\nincreasing the interest of the research community in developing approaches for\nrecognizing actions from the first-person point of view. An open challenge in\negocentric action recognition is that videos lack detailed information about\nthe main actor's pose and thus tend to record only parts of the movement when\nfocusing on manipulation tasks. Thus, the amount of information about the\naction itself is limited, making crucial the understanding of the manipulated\nobjects and their context. Many previous works addressed this issue with\ntwo-stream architectures, where one stream is dedicated to modeling the\nappearance of objects involved in the action, and another to extracting motion\nfeatures from optical flow. In this paper, we argue that learning features\njointly from these two information channels is beneficial to capture the\nspatio-temporal correlations between the two better. To this end, we propose a\nsingle stream architecture able to do so, thanks to the addition of a\nself-supervised block that uses a pretext motion prediction task to intertwine\nmotion and appearance knowledge. Experiments on several publicly available\ndatabases show the power of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 17:51:13 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 18:50:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Planamente", "Mirco", ""], ["Bottino", "Andrea", ""], ["Caputo", "Barbara", ""]]}, {"id": "2002.03983", "submitter": "Martin Simon", "authors": "Kai Fischer, Martin Simon, Florian Oelsner, Stefan Milz, Horst-Michael\n  Gross, Patrick Maeder", "title": "StickyPillars: Robust and Efficient Feature Matching on Point Clouds\n  using Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust point cloud registration in real-time is an important prerequisite for\nmany mapping and localization algorithms. Traditional methods like ICP tend to\nfail without good initialization, insufficient overlap or in the presence of\ndynamic objects. Modern deep learning based registration approaches present\nmuch better results, but suffer from a heavy run-time. We overcome these\ndrawbacks by introducing StickyPillars, a fast, accurate and extremely robust\ndeep middle-end 3D feature matching method on point clouds. It uses graph\nneural networks and performs context aggregation on sparse 3D key-points with\nthe aid of transformer based multi-head self and cross-attention. The network\noutput is used as the cost for an optimal transport problem whose solution\nyields the final matching probabilities. The system does not rely on hand\ncrafted feature descriptors or heuristic matching strategies. We present\nstate-of-art art accuracy results on the registration problem demonstrated on\nthe KITTI dataset while being four times faster then leading deep methods.\nFurthermore, we integrate our matching system into a LiDAR odometry pipeline\nyielding most accurate results on the KITTI odometry dataset. Finally, we\ndemonstrate robustness on KITTI odometry. Our method remains stable in accuracy\nwhere state-of-the-art procedures fail on frame drops and higher speeds.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 17:53:41 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 06:27:41 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 09:18:05 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Fischer", "Kai", ""], ["Simon", "Martin", ""], ["Oelsner", "Florian", ""], ["Milz", "Stefan", ""], ["Gross", "Horst-Michael", ""], ["Maeder", "Patrick", ""]]}, {"id": "2002.03985", "submitter": "Luiz A. Zanlorensi", "authors": "Luiz A. Zanlorensi, Hugo Proen\\c{c}a, David Menotti", "title": "Unconstrained Periocular Recognition: Using Generative Deep Learning\n  Frameworks for Attribute Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ocular biometric systems working in unconstrained environments usually face\nthe problem of small within-class compactness caused by the multiple factors\nthat jointly degrade the quality of the obtained data. In this work, we propose\nan attribute normalization strategy based on deep learning generative\nframeworks, that reduces the variability of the samples used in pairwise\ncomparisons, without reducing their discriminability. The proposed method can\nbe seen as a preprocessing step that contributes for data regularization and\nimproves the recognition accuracy, being fully agnostic to the recognition\nstrategy used. As proof of concept, we consider the \"eyeglasses\" and \"gaze\"\nfactors, comparing the levels of performance of five different recognition\nmethods with/without using the proposed normalization strategy. Also, we\nintroduce a new dataset for unconstrained periocular recognition, composed of\nimages acquired by mobile devices, particularly suited to perceive the impact\nof \"wearing eyeglasses\" in recognition effectiveness. Our experiments were\nperformed in two different datasets, and support the usefulness of our\nattribute normalization scheme to improve the recognition performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 17:55:55 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zanlorensi", "Luiz A.", ""], ["Proen\u00e7a", "Hugo", ""], ["Menotti", "David", ""]]}, {"id": "2002.03989", "submitter": "Jun Liu", "authors": "Jun Liu, Xiangyue Wang, Xue-cheng Tai", "title": "Deep Convolutional Neural Networks with Spatial Regularization, Volume\n  and Star-shape Priori for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use Deep Convolutional Neural Networks (DCNNs) for image segmentation\nproblems. DCNNs can well extract the features from natural images. However, the\nclassification functions in the existing network architecture of CNNs are\nsimple and lack capabilities to handle important spatial information in a way\nthat have been done for many well-known traditional variational models. Prior\nsuch as spatial regularity, volume prior and object shapes cannot be well\nhandled by existing DCNNs. We propose a novel Soft Threshold Dynamics (STD)\nframework which can easily integrate many spatial priors of the classical\nvariational models into the DCNNs for image segmentation. The novelty of our\nmethod is to interpret the softmax activation function as a dual variable in a\nvariational problem, and thus many spatial priors can be imposed in the dual\nspace. From this viewpoint, we can build a STD based framework which can enable\nthe outputs of DCNNs to have many special priors such as spatial regularity,\nvolume constraints and star-shape priori. The proposed method is a general\nmathematical framework and it can be applied to any semantic segmentation\nDCNNs. To show the efficiency and accuracy of our method, we applied it to the\npopular DeepLabV3+ image segmentation network, and the experiments results show\nthat our method can work efficiently on data-driven image segmentation DCNNs.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 18:03:44 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liu", "Jun", ""], ["Wang", "Xiangyue", ""], ["Tai", "Xue-cheng", ""]]}, {"id": "2002.04023", "submitter": "Yao Xia", "authors": "Yao Xia", "title": "Upper, Middle and Lower Region Learning for Facial Action Unit Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action units (AUs) detection is fundamental to facial expression\nanalysis. As AU occurs only in a small area of the face, region-based learning\nhas been widely recognized useful for AU detection. Most region-based studies\nfocus on a small region where the AU occurs. Focusing on a specific region\nhelps eliminate the influence of identity, but bringing a risk for losing\ninformation. It is challenging to find balance. In this study, I propose a\nsimple strategy. I divide the face into three broad regions, upper, middle, and\nlower region, and group AUs based on where it occurs. I propose a new\nend-to-end deep learning framework named three regions based attention network\n(TRA-Net). After extracting the global feature, TRA-Net uses a hard attention\nmodule to extract three feature maps, each of which contains only a specific\nregion. Each region-specific feature map is fed to an independent branch. For\neach branch, three continuous soft attention modules are used to extract\nhigher-level features for final AU detection. In the DISFA dataset, this model\nachieves the highest F1 scores for the detection of AU1, AU2, and AU4, and\nproduces the highest accuracy in comparison with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 18:51:45 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 12:58:35 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Xia", "Yao", ""]]}, {"id": "2002.04034", "submitter": "Abolfazl Attar", "authors": "Mohammad reza Mohammadi, Mohammad Rahimzadeh and Abolfazl Attar", "title": "Sperm Detection and Tracking in Phase-Contrast Microscopy Image\n  Sequences using Deep Learning and Modified CSR-DCF", "comments": null, "journal-ref": null, "doi": null, "report-no": "arXiv:2002.04034", "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, computer-aided sperm analysis (CASA) systems have made a big leap\nin extracting the characteristics of spermatozoa for studies or measuring human\nfertility. The first step in sperm characteristics analysis is sperm detection\nin the frames of the video sample. In this article, we used RetinaNet, a deep\nfully convolutional neural network as the object detector. Sperms are small\nobjects with few attributes, that makes the detection more difficult in\nhigh-density samples and especially when there are other particles in semen,\nwhich could be like sperm heads. One of the main attributes of sperms is their\nmovement, but this attribute cannot be extracted when only one frame would be\nfed to the network. To improve the performance of the sperm detection network,\nwe concatenated some consecutive frames to use as the input of the network.\nWith this method, the motility attribute has also been extracted, and then with\nthe help of the deep convolutional network, we have achieved high accuracy in\nsperm detection. The second step is tracking the sperms, for extracting the\nmotility parameters that are essential for indicating fertility and other\nstudies on sperms. In the tracking phase, we modify the CSR-DCF algorithm. This\nmethod also has shown excellent results in sperm tracking even in high-density\nsperm samples, occlusions, sperm colliding, and when sperms exit from a frame\nand re-enter in the next frames. The average precision of the detection phase\nis 99.1%, and the F1 score of the tracking method evaluation is 96.61%. These\nresults can be a great help in studies investigating sperm behavior and\nanalyzing fertility possibility.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 00:38:47 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 19:54:46 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 23:16:23 GMT"}, {"version": "v4", "created": "Sat, 4 Apr 2020 06:21:26 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Mohammadi", "Mohammad reza", ""], ["Rahimzadeh", "Mohammad", ""], ["Attar", "Abolfazl", ""]]}, {"id": "2002.04051", "submitter": "James Bird", "authors": "James Bird, Linda Petzold, Philip Lubin, Julia Deacon", "title": "Advances in Deep Space Exploration via Simulators & Deep Learning", "comments": "16 pages, 7 figures, 2 tables, currently under review", "journal-ref": null, "doi": "10.1016/j.newast.2020.101517", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The StarLight program conceptualizes fast interstellar travel via small wafer\nsatellites (wafersats) that are propelled by directed energy. This process is\nwildly different from traditional space travel and trades large and slow\nspacecraft for small, fast, inexpensive, and fragile ones. The main goal of\nthese wafer satellites is to gather useful images during their deep space\njourney. We introduce and solve some of the main problems that accompany this\nconcept. First, we need an object detection system that can detect planets that\nwe have never seen before, some containing features that we may not even know\nexist in the universe. Second, once we have images of exoplanets, we need a way\nto take these images and rank them by importance. Equipment fails and data\nrates are slow, thus we need a method to ensure that the most important images\nto humankind are the ones that are prioritized for data transfer. Finally, the\nenergy on board is minimal and must be conserved and used sparingly. No\nexoplanet images should be missed, but using energy erroneously would be\ndetrimental. We introduce simulator-based methods that leverage artificial\nintelligence, mostly in the form of computer vision, in order to solve all\nthree of these issues. Our results confirm that simulators provide an extremely\nrich training environment that surpasses that of real images, and can be used\nto train models on features that have yet to be observed by humans. We also\nshow that the immersive and adaptable environment provided by the simulator,\ncombined with deep learning, lets us navigate and save energy in an otherwise\nimplausible way.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 19:07:54 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 19:21:36 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bird", "James", ""], ["Petzold", "Linda", ""], ["Lubin", "Philip", ""], ["Deacon", "Julia", ""]]}, {"id": "2002.04066", "submitter": "Misgina Tsighe Hagos", "authors": "Misgina Tsighe Hagos", "title": "Point-of-Care Diabetic Retinopathy Diagnosis: A Standalone Mobile\n  Application Approach", "comments": "Dissertation for a Masters of Technology in Data Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although deep learning research and applications have grown rapidly over the\npast decade, it has shown limitation in healthcare applications and its\nreachability to people in remote areas. One of the challenges of incorporating\ndeep learning in medical data classification or prediction is the shortage of\nannotated training data in the healthcare industry. Medical data sharing\nprivacy issues and limited patient population size can be stated as some of the\nreasons for training data insufficiency in healthcare. Methods to exploit deep\nlearning applications in healthcare have been proposed and implemented in this\ndissertation.\n  Traditional diagnosis of diabetic retinopathy requires trained\nophthalmologists and expensive imaging equipment to reach healthcare centres in\norder to provide facilities for treatment of preventable blindness. Diabetic\npeople residing in remote areas with shortage of healthcare services and\nophthalmologists usually fail to get periodical diagnosis of diabetic\nretinopathy thereby facing the probability of vision loss or impairment. Deep\nlearning and mobile application development have been integrated in this\ndissertation to provide an easy to use point-of-care smartphone based diagnosis\nof diabetic retinopathy. In order to solve the challenge of shortage of\nhealthcare centres and trained ophthalmologists, the standalone diagnostic\nservice was built so as to be operated by a non-expert without an internet\nconnection. This approach could be transferred to other areas of medical image\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 11:03:16 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Hagos", "Misgina Tsighe", ""]]}, {"id": "2002.04070", "submitter": "Peidong Liu Mr.", "authors": "Peidong Liu, Joel Janai, Marc Pollefeys, Torsten Sattler and Andreas\n  Geiger", "title": "Self-Supervised Linear Motion Deblurring", "comments": "Accepted by Robotics and Automation Letters (RA-L)", "journal-ref": null, "doi": "10.1109/LRA.2020.2972873", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion blurry images challenge many computer vision algorithms, e.g, feature\ndetection, motion estimation, or object recognition. Deep convolutional neural\nnetworks are state-of-the-art for image deblurring. However, obtaining training\ndata with corresponding sharp and blurry image pairs can be difficult. In this\npaper, we present a differentiable reblur model for self-supervised motion\ndeblurring, which enables the network to learn from real-world blurry image\nsequences without relying on sharp images for supervision. Our key insight is\nthat motion cues obtained from consecutive images yield sufficient information\nto inform the deblurring task. We therefore formulate deblurring as an inverse\nrendering problem, taking into account the physical image formation process: we\nfirst predict two deblurred images from which we estimate the corresponding\noptical flow. Using these predictions, we re-render the blurred images and\nminimize the difference with respect to the original blurry inputs. We use both\nsynthetic and real dataset for experimental evaluations. Our experiments\ndemonstrate that self-supervised single image deblurring is really feasible and\nleads to visually compelling results.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 20:15:21 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Liu", "Peidong", ""], ["Janai", "Joel", ""], ["Pollefeys", "Marc", ""], ["Sattler", "Torsten", ""], ["Geiger", "Andreas", ""]]}, {"id": "2002.04098", "submitter": "Yucheng Tang", "authors": "Yuchen Xu, Olivia Tang, Yucheng Tang, Ho Hin Lee, Yunqiang Chen,\n  Dashan Gao, Shizhong Han, Riqiang Gao, Michael R. Savona, Richard G.\n  Abramson, Yuankai Huo, Bennett A. Landman", "title": "Outlier Guided Optimization of Abdominal Segmentation", "comments": "SPIE2020 Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abdominal multi-organ segmentation of computed tomography (CT) images has\nbeen the subject of extensive research interest. It presents a substantial\nchallenge in medical image processing, as the shape and distribution of\nabdominal organs can vary greatly among the population and within an individual\nover time. While continuous integration of novel datasets into the training set\nprovides potential for better segmentation performance, collection of data at\nscale is not only costly, but also impractical in some contexts. Moreover, it\nremains unclear what marginal value additional data have to offer. Herein, we\npropose a single-pass active learning method through human quality assurance\n(QA). We built on a pre-trained 3D U-Net model for abdominal multi-organ\nsegmentation and augmented the dataset either with outlier data (e.g.,\nexemplars for which the baseline algorithm failed) or inliers (e.g., exemplars\nfor which the baseline algorithm worked). The new models were trained using the\naugmented datasets with 5-fold cross-validation (for outlier data) and withheld\noutlier samples (for inlier data). Manual labeling of outliers increased Dice\nscores with outliers by 0.130, compared to an increase of 0.067 with inliers\n(p<0.001, two-tailed paired t-test). By adding 5 to 37 inliers or outliers to\ntraining, we find that the marginal value of adding outliers is higher than\nthat of adding inliers. In summary, improvement on single-organ performance was\nobtained without diminishing multi-organ performance or significantly\nincreasing training time. Hence, identification and correction of baseline\nfailure cases present an effective and efficient method of selecting training\ndata to improve algorithm performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 21:41:52 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Xu", "Yuchen", ""], ["Tang", "Olivia", ""], ["Tang", "Yucheng", ""], ["Lee", "Ho Hin", ""], ["Chen", "Yunqiang", ""], ["Gao", "Dashan", ""], ["Han", "Shizhong", ""], ["Gao", "Riqiang", ""], ["Savona", "Michael R.", ""], ["Abramson", "Richard G.", ""], ["Huo", "Yuankai", ""], ["Landman", "Bennett A.", ""]]}, {"id": "2002.04102", "submitter": "Yucheng Tang", "authors": "Yuchen Xu, Olivia Tang, Yucheng Tang, Ho Hin Lee, Yunqiang Chen,\n  Dashan Gao, Shizhong Han, Riqiang Gao, Michael R. Savona, Richard G.\n  Abramson, Yuankai Huo, Bennett A. Landman", "title": "Validation and Optimization of Multi-Organ Segmentation on Clinical\n  Imaging Archives", "comments": "SPIE2020 Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of abdominal computed tomography(CT) provides spatial context,\nmorphological properties, and a framework for tissue-specific radiomics to\nguide quantitative Radiological assessment. A 2015 MICCAI challenge spurred\nsubstantial innovation in multi-organ abdominal CT segmentation with both\ntraditional and deep learning methods. Recent innovations in deep methods have\ndriven performance toward levels for which clinical translation is appealing.\nHowever, continued cross-validation on open datasets presents the risk of\nindirect knowledge contamination and could result in circular reasoning.\nMoreover, 'real world' segmentations can be challenging due to the wide\nvariability of abdomen physiology within patients. Herein, we perform two data\nretrievals to capture clinically acquired deidentified abdominal CT cohorts\nwith respect to a recently published variation on 3D U-Net (baseline\nalgorithm). First, we retrieved 2004 deidentified studies on 476 patients with\ndiagnosis codes involving spleen abnormalities (cohort A). Second, we retrieved\n4313 deidentified studies on 1754 patients without diagnosis codes involving\nspleen abnormalities (cohort B). We perform prospective evaluation of the\nexisting algorithm on both cohorts, yielding 13% and 8% failure rate,\nrespectively. Then, we identified 51 subjects in cohort A with segmentation\nfailures and manually corrected the liver and gallbladder labels. We re-trained\nthe model adding the manual labels, resulting in performance improvement of 9%\nand 6% failure rate for the A and B cohorts, respectively. In summary, the\nperformance of the baseline on the prospective cohorts was similar to that on\npreviously published datasets. Moreover, adding data from the first cohort\nsubstantively improved performance when evaluated on the second withheld\nvalidation cohort.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 21:49:42 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Xu", "Yuchen", ""], ["Tang", "Olivia", ""], ["Tang", "Yucheng", ""], ["Lee", "Ho Hin", ""], ["Chen", "Yunqiang", ""], ["Gao", "Dashan", ""], ["Han", "Shizhong", ""], ["Gao", "Riqiang", ""], ["Savona", "Michael R.", ""], ["Abramson", "Richard G.", ""], ["Huo", "Yuankai", ""], ["Landman", "Bennett A.", ""]]}, {"id": "2002.04114", "submitter": "Guan-An Wang", "authors": "Guan-An Wang, Tianzhu Zhang. Yang Yang, Jian Cheng, Jianlong Chang, Xu\n  Liang, Zengguang Hou", "title": "Cross-Modality Paired-Images Generation for RGB-Infrared Person\n  Re-Identification", "comments": "accepted by AAAI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-Infrared (IR) person re-identification is very challenging due to the\nlarge cross-modality variations between RGB and IR images. The key solution is\nto learn aligned features to the bridge RGB and IR modalities. However, due to\nthe lack of correspondence labels between every pair of RGB and IR images, most\nmethods try to alleviate the variations with set-level alignment by reducing\nthe distance between the entire RGB and IR sets. However, this set-level\nalignment may lead to misalignment of some instances, which limits the\nperformance for RGB-IR Re-ID. Different from existing methods, in this paper,\nwe propose to generate cross-modality paired-images and perform both global\nset-level and fine-grained instance-level alignments. Our proposed method\nenjoys several merits. First, our method can perform set-level alignment by\ndisentangling modality-specific and modality-invariant features. Compared with\nconventional methods, ours can explicitly remove the modality-specific features\nand the modality variation can be better reduced. Second, given cross-modality\nunpaired-images of a person, our method can generate cross-modality paired\nimages from exchanged images. With them, we can directly perform instance-level\nalignment by minimizing distances of every pair of images. Extensive\nexperimental results on two standard benchmarks demonstrate that the proposed\nmodel favourably against state-of-the-art methods. Especially, on SYSU-MM01\ndataset, our model can achieve a gain of 9.2% and 7.7% in terms of Rank-1 and\nmAP. Code is available at https://github.com/wangguanan/JSIA-ReID.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 22:15:19 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 00:03:01 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Wang", "Guan-An", ""], ["Yang", "Tianzhu Zhang. Yang", ""], ["Cheng", "Jian", ""], ["Chang", "Jianlong", ""], ["Liang", "Xu", ""], ["Hou", "Zengguang", ""]]}, {"id": "2002.04147", "submitter": "Ioannis Marras", "authors": "Ioannis Marras, Grigorios G. Chrysos, Ioannis Alexiou, Gregory\n  Slabaugh, Stefanos Zafeiriou", "title": "Reconstructing the Noise Manifold for Image Denoising", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have been successfully used in many\nlow-level vision problems like image denoising. Although the conditional image\ngeneration techniques have led to large improvements in this task, there has\nbeen little effort in providing conditional generative adversarial networks\n(cGAN)[42] with an explicit way of understanding the image noise for\nobject-independent denoising reliable for real-world applications. The task of\nleveraging structures in the target space is unstable due to the complexity of\npatterns in natural scenes, so the presence of unnatural artifacts or\nover-smoothed image areas cannot be avoided. To fill the gap, in this work we\nintroduce the idea of a cGAN which explicitly leverages structure in the image\nnoise space. By learning directly a low dimensional manifold of the image\nnoise, the generator promotes the removal from the noisy image only that\ninformation which spans this manifold. This idea brings many advantages while\nit can be appended at the end of any denoiser to significantly improve its\nperformance. Based on our experiments, our model substantially outperforms\nexisting state-of-the-art architectures, resulting in denoised images with less\noversmoothing and better detail.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 00:31:31 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 01:00:00 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Marras", "Ioannis", ""], ["Chrysos", "Grigorios G.", ""], ["Alexiou", "Ioannis", ""], ["Slabaugh", "Gregory", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2002.04162", "submitter": "Avinash Ravichandran", "authors": "Qing Liu, Orchid Majumder, Alessandro Achille, Avinash Ravichandran,\n  Rahul Bhotika, Stefano Soatto", "title": "Incremental Meta-Learning via Indirect Discriminant Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of the modern meta-learning methods for few-shot classification\ntasks operate in two phases: a meta-training phase where the meta-learner\nlearns a generic representation by solving multiple few-shot tasks sampled from\na large dataset and a testing phase, where the meta-learner leverages its\nlearnt internal representation for a specific few-shot task involving classes\nwhich were not seen during the meta-training phase. To the best of our\nknowledge, all such meta-learning methods use a single base dataset for\nmeta-training to sample tasks from and do not adapt the algorithm after\nmeta-training. This strategy may not scale to real-world use-cases where the\nmeta-learner does not potentially have access to the full meta-training dataset\nfrom the very beginning and we need to update the meta-learner in an\nincremental fashion when additional training data becomes available. Through\nour experimental setup, we develop a notion of incremental learning during the\nmeta-training phase of meta-learning and propose a method which can be used\nwith multiple existing metric-based meta-learning algorithms. Experimental\nresults on benchmark dataset show that our approach performs favorably at test\ntime as compared to training a model with the full meta-training set and incurs\nnegligible amount of catastrophic forgetting\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 01:39:12 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 18:19:18 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Liu", "Qing", ""], ["Majumder", "Orchid", ""], ["Achille", "Alessandro", ""], ["Ravichandran", "Avinash", ""], ["Bhotika", "Rahul", ""], ["Soatto", "Stefano", ""]]}, {"id": "2002.04170", "submitter": "Jie Yang", "authors": "Jie Yang, Zhiquan Qi, Yong Shi", "title": "Learning to Incorporate Structure Knowledge for Image Inpainting", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a multi-task learning framework that attempts to\nincorporate the image structure knowledge to assist image inpainting, which is\nnot well explored in previous works. The primary idea is to train a shared\ngenerator to simultaneously complete the corrupted image and corresponding\nstructures --- edge and gradient, thus implicitly encouraging the generator to\nexploit relevant structure knowledge while inpainting. In the meantime, we also\nintroduce a structure embedding scheme to explicitly embed the learned\nstructure features into the inpainting process, thus to provide possible\npreconditions for image completion. Specifically, a novel pyramid structure\nloss is proposed to supervise structure learning and embedding. Moreover, an\nattention mechanism is developed to further exploit the recurrent structures\nand patterns in the image to refine the generated structures and contents.\nThrough multi-task learning, structure embedding besides with attention, our\nframework takes advantage of the structure knowledge and outperforms several\nstate-of-the-art methods on benchmark datasets quantitatively and\nqualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 02:22:38 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 03:12:04 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Yang", "Jie", ""], ["Qi", "Zhiquan", ""], ["Shi", "Yong", ""]]}, {"id": "2002.04189", "submitter": "Rohit Jammula", "authors": "Rohit Jammula, Vishnu Rajan Tejus, Shreya Shankar", "title": "Optimal Transfer Learning Model for Binary Classification of Funduscopic\n  Images through Simple Heuristics", "comments": "5 pages. 4 tables. Accepted to present in Machine Learning in\n  Computational Biology (MLCB) 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have the capacity to fundamentally revolutionize medical\nimaging analysis, and they have particularly interesting applications in\ncomputer-aided diagnosis. We attempt to use deep learning neural networks to\ndiagnose funduscopic images, visual representations of the interior of the eye.\nRecently, a few robust deep learning approaches have performed binary\nclassification to infer the presence of a specific ocular disease, such as\nglaucoma or diabetic retinopathy. In an effort to broaden the applications of\ncomputer-aided ocular disease diagnosis, we propose a unifying model for\ndisease classification: low-cost inference of a fundus image to determine\nwhether it is healthy or diseased. To achieve this, we use transfer learning\ntechniques, which retain the more overarching capabilities of a pre-trained\nbase architecture but can adapt to another dataset. For comparisons, we then\ndevelop a custom heuristic equation and evaluation metric ranking system to\ndetermine the optimal base architecture and hyperparameters. The Xception base\narchitecture, Adam optimizer, and mean squared error loss function perform\nbest, achieving 90% accuracy, 94% sensitivity, and 86% specificity. For\nadditional ease of use, we contain the model in a web interface whose file\nchooser can access the local filesystem, allowing for use on any\ninternet-connected device: mobile, PC, or otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 03:49:14 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 07:33:06 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 21:41:36 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Jammula", "Rohit", ""], ["Tejus", "Vishnu Rajan", ""], ["Shankar", "Shreya", ""]]}, {"id": "2002.04205", "submitter": "Rahul Soni", "authors": "Rahul Soni, Naresh Shah, Jimmy D. Moore", "title": "Fine-grained Uncertainty Modeling in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing uncertainty modeling approaches try to detect an out-of-distribution\npoint from the in-distribution dataset. We extend this argument to detect\nfiner-grained uncertainty that distinguishes between (a). certain points, (b).\nuncertain points but within the data distribution, and (c). out-of-distribution\npoints. Our method corrects overconfident NN decisions, detects outlier points\nand learns to say ``I don't know'' when uncertain about a critical point\nbetween the top two predictions. In addition, we provide a mechanism to\nquantify class distributions overlap in the decision manifold and investigate\nits implications in model interpretability.\n  Our method is two-step: in the first step, the proposed method builds a class\ndistribution using Kernel Activation Vectors (kav) extracted from the Network.\nIn the second step, the algorithm determines the confidence of a test point by\na hierarchical decision rule based on the chi-squared distribution of squared\nMahalanobis distances.\n  Our method sits on top of a given Neural Network, requires a single scan of\ntraining data to estimate class distribution statistics, and is highly scalable\nto deep networks and wider pre-softmax layer. As a positive side effect, our\nmethod helps to prevent adversarial attacks without requiring any additional\ntraining. It is directly achieved when the Softmax layer is substituted by our\nrobust uncertainty layer at the evaluation phase.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 05:06:25 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Soni", "Rahul", ""], ["Shah", "Naresh", ""], ["Moore", "Jimmy D.", ""]]}, {"id": "2002.04206", "submitter": "George Ekladious", "authors": "George Ekladious, Hugo Lemoine, Eric Granger, Kaveh Kamali, Salim\n  Moudache", "title": "Dual-Triplet Metric Learning for Unsupervised Domain Adaptation in\n  Video-Based Face Recognition", "comments": "Submitted too IJCNN2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability and complexity of deep learning models remains a key issue in\nmany of visual recognition applications like, e.g., video surveillance, where\nfine tuning with labeled image data from each new camera is required to reduce\nthe domain shift between videos captured from the source domain, e.g., a\nlaboratory setting, and the target domain, i.e, an operational environment. In\nmany video surveillance applications, like face recognition (FR) and person\nre-identification, a pair-wise matcher is used to assign a query image captured\nusing a video camera to the corresponding reference images in a gallery. The\ndifferent configurations and operational conditions of video cameras can\nintroduce significant shifts in the pair-wise distance distributions, resulting\nin degraded recognition performance for new cameras. In this paper, a new deep\ndomain adaptation (DA) method is proposed to adapt the CNN embedding of a\nSiamese network using unlabeled tracklets captured with a new video cameras. To\nthis end, a dual-triplet loss is introduced for metric learning, where two\ntriplets are constructed using video data from a source camera, and a new\ntarget camera. In order to constitute the dual triplets, a mutual-supervised\nlearning approach is introduced where the source camera acts as a teacher,\nproviding the target camera with an initial embedding. Then, the student relies\non the teacher to iteratively label the positive and negative pairs collected\nduring, e.g., initial camera calibration. Both source and target embeddings\ncontinue to simultaneously learn such that their pair-wise distance\ndistributions become aligned. For validation, the proposed metric learning\ntechnique is used to train deep Siamese networks under different training\nscenarios, and is compared to state-of-the-art techniques for still-to-video FR\non the COX-S2V and a private video-based FR dataset.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 05:06:30 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Ekladious", "George", ""], ["Lemoine", "Hugo", ""], ["Granger", "Eric", ""], ["Kamali", "Kaveh", ""], ["Moudache", "Salim", ""]]}, {"id": "2002.04207", "submitter": "Ali Hatamizadeh", "authors": "Ali Hatamizadeh, Demetri Terzopoulos and Andriy Myronenko", "title": "Edge-Gated CNNs for Volumetric Semantic Segmentation of Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Textures and edges contribute different information to image recognition.\nEdges and boundaries encode shape information, while textures manifest the\nappearance of regions. Despite the success of Convolutional Neural Networks\n(CNNs) in computer vision and medical image analysis applications,\npredominantly only texture abstractions are learned, which often leads to\nimprecise boundary delineations. In medical imaging, expert manual segmentation\noften relies on organ boundaries; for example, to manually segment a liver, a\nmedical practitioner usually identifies edges first and subsequently fills in\nthe segmentation mask. Motivated by these observations, we propose a\nplug-and-play module, dubbed Edge-Gated CNNs (EG-CNNs), that can be used with\nexisting encoder-decoder architectures to process both edge and texture\ninformation. The EG-CNN learns to emphasize the edges in the encoder, to\npredict crisp boundaries by an auxiliary edge supervision, and to fuse its\noutput with the original CNN output. We evaluate the effectiveness of the\nEG-CNN with various mainstream CNNs on two publicly available datasets, BraTS\n19 and KiTS 19 for brain tumor and kidney semantic segmentation. We demonstrate\nhow the addition of EG-CNN consistently improves segmentation accuracy and\ngeneralization performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 05:08:21 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Hatamizadeh", "Ali", ""], ["Terzopoulos", "Demetri", ""], ["Myronenko", "Andriy", ""]]}, {"id": "2002.04219", "submitter": "Alperen Kantarc{\\i}", "authors": "Alperen Kantarc{\\i}, Haz{\\i}m Kemal Ekenel", "title": "Thermal to Visible Face Recognition Using Deep Autoencoders", "comments": "5 pages, 3 figures, 2019 International Conference of the Biometrics\n  Special Interest Group (BIOSIG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible face recognition systems achieve nearly perfect recognition\naccuracies using deep learning. However, in lack of light, these systems\nperform poorly. A way to deal with this problem is thermal to visible\ncross-domain face matching. This is a desired technology because of its\nusefulness in night time surveillance. Nevertheless, due to differences between\ntwo domains, it is a very challenging face recognition problem. In this paper,\nwe present a deep autoencoder based system to learn the mapping between visible\nand thermal face images. Also, we assess the impact of alignment in thermal to\nvisible face recognition. For this purpose, we manually annotate the facial\nlandmarks on the Carl and EURECOM datasets. The proposed approach is\nextensively tested on three publicly available datasets: Carl, UND-X1, and\nEURECOM. Experimental results show that the proposed approach improves the\nstate-of-the-art significantly. We observe that alignment increases the\nperformance by around 2%. Annotated facial landmark positions in this study can\nbe downloaded from the following link:\ngithub.com/Alpkant/Thermal-to-Visible-Face-Recognition-Using-Deep-Autoencoders .\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 11:58:36 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Kantarc\u0131", "Alperen", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "2002.04227", "submitter": "Yu Liu", "authors": "Haokui Zhang, Yu Liu, Bei Fang, Ying Li, Lingqiao Liu and Ian Reid", "title": "Hyperspectral Classification Based on 3D Asymmetric Inception Network\n  with Data Fusion Transfer Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral image(HSI) classification has been improved with convolutional\nneural network(CNN) in very recent years. Being different from the RGB\ndatasets, different HSI datasets are generally captured by various remote\nsensors and have different spectral configurations. Moreover, each HSI dataset\nonly contains very limited training samples and thus it is prone to overfitting\nwhen using deep CNNs. In this paper, we first deliver a 3D asymmetric inception\nnetwork, AINet, to overcome the overfitting problem. With the emphasis on\nspectral signatures over spatial contexts of HSI data, AINet can convey and\nclassify the features effectively. In addition, the proposed data fusion\ntransfer learning strategy is beneficial in boosting the classification\nperformance. Extensive experiments show that the proposed approach beat all of\nthe state-of-art methods on several HSI benchmarks, including Pavia University,\nIndian Pines and Kennedy Space Center(KSC). Code can be found at:\nhttps://github.com/UniLauX/AINet.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 06:37:34 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Zhang", "Haokui", ""], ["Liu", "Yu", ""], ["Fang", "Bei", ""], ["Li", "Ying", ""], ["Liu", "Lingqiao", ""], ["Reid", "Ian", ""]]}, {"id": "2002.04237", "submitter": "Sidharth Gupta", "authors": "Sidharth Gupta, Parijat Dube, Ashish Verma", "title": "Improving the affordability of robustness training for DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projected Gradient Descent (PGD) based adversarial training has become one of\nthe most prominent methods for building robust deep neural network models.\nHowever, the computational complexity associated with this approach, due to the\nmaximization of the loss function when finding adversaries, is a longstanding\nproblem and may be prohibitive when using larger and more complex models. In\nthis paper we show that the initial phase of adversarial training is redundant\nand can be replaced with natural training which significantly improves the\ncomputational efficiency. We demonstrate that this efficiency gain can be\nachieved without any loss in accuracy on natural and adversarial test samples.\nWe support our argument with insights on the nature of the adversaries and\ntheir relative strength during the training process. We show that our proposed\nmethod can reduce the training time by a factor of up to 2.5 with comparable or\nbetter model test accuracy and generalization on various strengths of\nadversarial attacks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 07:29:45 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 04:17:09 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Gupta", "Sidharth", ""], ["Dube", "Parijat", ""], ["Verma", "Ashish", ""]]}, {"id": "2002.04251", "submitter": "Ruisheng Su", "authors": "Ruisheng Su, Weiyi Xie, Tao Tan", "title": "2.75D: Boosting Learning Efficiency and Capability by Representing 3D\n  Features in 2D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, 3D convolutional neural networks (CNNs) have shown\nsuperior performance to 2D CNN in numerous deep learning tasks with high\ndimensional input, proving the added value of 3D spatial information in feature\nrepresentation. However, 3D CNN requires more training samples to converge, and\nmore computational resources and execution time for both training and\ninference. Meanwhile, applying transfer learning on 3D CNN is challenging due\nto a lack of publicly available pre-trained 3D networks. To tackle with these\nissues, we propose a novel 2D strategical representation of volumetric data,\nnamely 2.75D approach. In our method, the spatial information of 3D images was\ncaptured in a single 2D view by a spiral-spinning technique. Therefore, our CNN\nis intrinsically a 2D network, which can fully leverage pre-trained 2D CNNs for\ndownstream vision problems. We evaluated the proposed method on LUNA16 nodule\ndetection challenge, by comparing the proposed 2.75D method with 2D, 2.5D, 3D\ncounterparts in the nodule false positive reduction. Results show that the\nproposed method outperforms other counterparts when all methods were trained\nfrom scratch. Such performance gain is more pronounced when introducing\ntransfer learning or when training data is limited. In addition, our method\nachieves a substantial reduce in time consumption of training and inference\ncomparing with the 3D method. Our code will be publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 08:24:19 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 19:44:20 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Su", "Ruisheng", ""], ["Xie", "Weiyi", ""], ["Tan", "Tao", ""]]}, {"id": "2002.04264", "submitter": "Dongliang Chang", "authors": "Dongliang Chang, Yifeng Ding, Jiyang Xie, Ayan Kumar Bhunia, Xiaoxu\n  Li, Zhanyu Ma, Ming Wu, Jun Guo, Yi-Zhe Song", "title": "The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image\n  Classification", "comments": "TIP2020. Code available at\n  https://github.com/dongliangchang/Mutual-Channel-Loss", "journal-ref": null, "doi": "10.1109/TIP.2020.2973812", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key for solving fine-grained image categorization is finding discriminate and\nlocal regions that correspond to subtle visual traits. Great strides have been\nmade, with complex networks designed specifically to learn part-level\ndiscriminate feature representations. In this paper, we show it is possible to\ncultivate subtle details without the need for overly complicated network\ndesigns or training mechanisms -- a single loss is all it takes. The main trick\nlies with how we delve into individual feature channels early on, as opposed to\nthe convention of starting from a consolidated feature map. The proposed loss\nfunction, termed as mutual-channel loss (MC-Loss), consists of two\nchannel-specific components: a discriminality component and a diversity\ncomponent. The discriminality component forces all feature channels belonging\nto the same class to be discriminative, through a novel channel-wise attention\nmechanism. The diversity component additionally constraints channels so that\nthey become mutually exclusive on spatial-wise. The end result is therefore a\nset of feature channels that each reflects different locally discriminative\nregions for a specific class. The MC-Loss can be trained end-to-end, without\nthe need for any bounding-box/part annotations, and yields highly\ndiscriminative regions during inference. Experimental results show our MC-Loss\nwhen implemented on top of common base networks can achieve state-of-the-art\nperformance on all four fine-grained categorization datasets (CUB-Birds,\nFGVC-Aircraft, Flowers-102, and Stanford-Cars). Ablative studies further\ndemonstrate the superiority of MC-Loss when compared with other recently\nproposed general-purpose losses for visual classification, on two different\nbase networks. Code available at\nhttps://github.com/dongliangchang/Mutual-Channel-Loss\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 09:12:45 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 12:56:57 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Chang", "Dongliang", ""], ["Ding", "Yifeng", ""], ["Xie", "Jiyang", ""], ["Bhunia", "Ayan Kumar", ""], ["Li", "Xiaoxu", ""], ["Ma", "Zhanyu", ""], ["Wu", "Ming", ""], ["Guo", "Jun", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2002.04289", "submitter": "Alo\\\"is Pourchot", "authors": "Alo\\\"is Pourchot, Alexis Ducarouge, Olivier Sigaud", "title": "To Share or Not To Share: A Comprehensive Appraisal of Weight-Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight-sharing (WS) has recently emerged as a paradigm to accelerate the\nautomated search for efficient neural architectures, a process dubbed Neural\nArchitecture Search (NAS). Although very appealing, this framework is not\nwithout drawbacks and several works have started to question its capabilities\non small hand-crafted benchmarks. In this paper, we take advantage of the\n\\nasbench dataset to challenge the efficiency of WS on a representative search\nspace. By comparing a SOTA WS approach to a plain random search we show that,\ndespite decent correlations between evaluations using weight-sharing and\nstandalone ones, WS is only rarely significantly helpful to NAS. In particular\nwe highlight the impact of the search space itself on the benefits.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 10:29:31 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 09:11:20 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Pourchot", "Alo\u00efs", ""], ["Ducarouge", "Alexis", ""], ["Sigaud", "Olivier", ""]]}, {"id": "2002.04312", "submitter": "Saulo Martiello Mastelini", "authors": "Everton Jose Santana and Felipe Rodrigues dos Santos and Saulo\n  Martiello Mastelini and Fabio Luiz Melquiades and Sylvio Barbon Jr", "title": "Improved prediction of soil properties with Multi-target Stacked\n  Generalisation on EDXRF spectra", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) algorithms have been used for assessing soil quality\nparameters along with non-destructive methodologies. Among spectroscopic\nanalytical methodologies, energy dispersive X-ray fluorescence (EDXRF) is one\nof the more quick, environmentally friendly and less expensive when compared to\nconventional methods. However, some challenges in EDXRF spectral data analysis\nstill demand more efficient methods capable of providing accurate outcomes.\nUsing Multi-target Regression (MTR) methods, multiple parameters can be\npredicted, and also taking advantage of inter-correlated parameters the overall\npredictive performance can be improved. In this study, we proposed the\nMulti-target Stacked Generalisation (MTSG), a novel MTR method relying on\nlearning from different regressors arranged in stacking structure for a boosted\noutcome. We compared MTSG and 5 MTR methods for predicting 10 parameters of\nsoil fertility. Random Forest and Support Vector Machine (with linear and\nradial kernels) were used as learning algorithms embedded into each MTR method.\nResults showed the superiority of MTR methods over the Single-target Regression\n(the traditional ML method), reducing the predictive error for 5 parameters.\nParticularly, MTSG obtained the lowest error for phosphorus, total organic\ncarbon and cation exchange capacity. When observing the relative performance of\nSupport Vector Machine with a radial kernel, the prediction of base saturation\npercentage was improved in 19%. Finally, the proposed method was able to reduce\nthe average error from 0.67 (single-target) to 0.64 analysing all targets,\nrepresenting a global improvement of 4.48%.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 11:05:03 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Santana", "Everton Jose", ""], ["Santos", "Felipe Rodrigues dos", ""], ["Mastelini", "Saulo Martiello", ""], ["Melquiades", "Fabio Luiz", ""], ["Barbon", "Sylvio", "Jr"]]}, {"id": "2002.04315", "submitter": "Xin-Long Luo", "authors": "Xin-Long Luo and Geng Sun", "title": "Symplectic Geometric Methods for Matrix Differential Equations Arising\n  from Inertial Navigation Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article explores some geometric and algebraic properties of the\ndynamical system which is represented by matrix differential equations arising\nfrom inertial navigation problems, such as the symplecticity and the\northogonality. Furthermore, it extends the applicable fields of symplectic\ngeometric algorithms from the even dimensional Hamiltonian system to the odd\ndimensional dynamical system. Finally, some numerical experiments are presented\nand illustrate the theoretical results of this paper.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 11:08:52 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Luo", "Xin-Long", ""], ["Sun", "Geng", ""]]}, {"id": "2002.04355", "submitter": "\\c{S}eymanur Akt{\\i}", "authors": "\\c{S}eymanur Akt{\\i}, G\\\"ozde Ay\\c{s}e Tataro\\u{g}lu, Haz{\\i}m Kemal\n  Ekenel", "title": "Vision-based Fight Detection from Surveillance Cameras", "comments": "6 pages, 5 figures, 4 tables, International Conference on Image\n  Processing Theory, Tools and Applications, IPTA 2019", "journal-ref": null, "doi": "10.1109/IPTA.2019.8936070", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based action recognition is one of the most challenging research\ntopics of computer vision and pattern recognition. A specific application of\nit, namely, detecting fights from surveillance cameras in public areas,\nprisons, etc., is desired to quickly get under control these violent incidents.\nThis paper addresses this research problem and explores LSTM-based approaches\nto solve it. Moreover, the attention layer is also utilized. Besides, a new\ndataset is collected, which consists of fight scenes from surveillance camera\nvideos available at YouTube. This dataset is made publicly available. From the\nextensive experiments conducted on Hockey Fight, Peliculas, and the newly\ncollected fight datasets, it is observed that the proposed approach, which\nintegrates Xception model, Bi-LSTM, and attention, improves the\nstate-of-the-art accuracy for fight scene classification.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 12:56:29 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Akt\u0131", "\u015eeymanur", ""], ["Tataro\u011flu", "G\u00f6zde Ay\u015fe", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "2002.04380", "submitter": "Dominique Beaini", "authors": "Dominique Beaini, Sofiane Achiche, Alexandre Duperre, Maxime Raison", "title": "Saliency Enhancement using Gradient Domain Edges Merging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, there has been a rapid progress in solving the binary\nproblems in computer vision, such as edge detection which finds the boundaries\nof an image and salient object detection which finds the important object in an\nimage. This progress happened thanks to the rise of deep-learning and\nconvolutional neural networks (CNN) which allow to extract complex and abstract\nfeatures. However, edge detection and saliency are still two different fields\nand do not interact together, although it is intuitive for a human to detect\nsalient objects based on its boundaries. Those features are not well merged in\na CNN because edges and surfaces do not intersect since one feature represents\na region while the other represents boundaries between different regions. In\nthe current work, the main objective is to develop a method to merge the edges\nwith the saliency maps to improve the performance of the saliency. Hence, we\ndeveloped the gradient-domain merging (GDM) which can be used to quickly\ncombine the image-domain information of salient object detection with the\ngradient-domain information of the edge detection. This leads to our proposed\nsaliency enhancement using edges (SEE) with an average improvement of the\nF-measure of at least 3.4 times higher on the DUT-OMRON dataset and 6.6 times\nhigher on the ECSSD dataset, when compared to competing algorithm such as\ndenseCRF and BGOF. The SEE algorithm is split into 2 parts, SEE-Pre for\npreprocessing and SEE-Post pour postprocessing.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 14:04:56 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Beaini", "Dominique", ""], ["Achiche", "Sofiane", ""], ["Duperre", "Alexandre", ""], ["Raison", "Maxime", ""]]}, {"id": "2002.04392", "submitter": "Sven Koehler", "authors": "Sven Koehler and Animesh Tandon and Tarique Hussain and Heiner Latus\n  and Thomas Pickardt and Samir Sarikouch and Philipp Beerbaum and Gerald Greil\n  and Sandy Engelhardt and Ivo Wolf", "title": "How well do U-Net-based segmentation trained on adult cardiac magnetic\n  resonance imaging data generalise to rare congenital heart diseases for\n  surgical planning?", "comments": "Accepted for SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Planning the optimal time of intervention for pulmonary valve replacement\nsurgery in patients with the congenital heart disease Tetralogy of Fallot (TOF)\nis mainly based on ventricular volume and function according to current\nguidelines. Both of these two biomarkers are most reliably assessed by\nsegmentation of 3D cardiac magnetic resonance (CMR) images. In several grand\nchallenges in the last years, U-Net architectures have shown impressive results\non the provided data. However, in clinical practice, data sets are more diverse\nconsidering individual pathologies and image properties derived from different\nscanner properties. Additionally, specific training data for complex rare\ndiseases like TOF is scarce.\n  For this work, 1) we assessed the accuracy gap when using a publicly\navailable labelled data set (the Automatic Cardiac Diagnosis Challenge (ACDC)\ndata set) for training and subsequent applying it to CMR data of TOF patients\nand vice versa and 2) whether we can achieve similar results when applying the\nmodel to a more heterogeneous data base.\n  Multiple deep learning models were trained with four-fold cross validation.\nAfterwards they were evaluated on the respective unseen CMR images from the\nother collection. Our results confirm that current deep learning models can\nachieve excellent results (left ventricle dice of\n$0.951\\pm{0.003}$/$0.941\\pm{0.007}$ train/validation) within a single data\ncollection. But once they are applied to other pathologies, it becomes apparent\nhow much they overfit to the training pathologies (dice score drops between\n$0.072\\pm{0.001}$ for the left and $0.165\\pm{0.001}$ for the right ventricle).\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 08:50:51 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Koehler", "Sven", ""], ["Tandon", "Animesh", ""], ["Hussain", "Tarique", ""], ["Latus", "Heiner", ""], ["Pickardt", "Thomas", ""], ["Sarikouch", "Samir", ""], ["Beerbaum", "Philipp", ""], ["Greil", "Gerald", ""], ["Engelhardt", "Sandy", ""], ["Wolf", "Ivo", ""]]}, {"id": "2002.04405", "submitter": "Shahinur Alam", "authors": "Shahinur Alam, Md Sultan Mahmud, and Mohammed Yeasin", "title": "SafeNet: An Assistive Solution to Assess Incoming Threats for Premises", "comments": "arXiv admin note: text overlap with arXiv:1904.01178", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An assistive solution to assess incoming threats (e.g., robbery, burglary,\ngun violence) for homes will enhance the safety of the people with or without\ndisabilities. This paper presents \"SafeNet\"- an integrated assistive system to\ngenerate context-oriented image descriptions to assess incoming threats. The\nkey functionality of the system includes the detection and identification of\nhuman and generating image descriptions from the real-time video streams\nobtained from the cameras placed in strategic locations around the house. In\nthis paper, we focus on developing a robust model called \"SafeNet\" to generate\nimage descriptions. To interact with the system, we implemented a dialog\nenabled interface for creating a personalized profile from face images or\nvideos of friends/families. To improve computational efficiency, we apply\nchange detection to filter out frames that do not have any activity and use\nFaster-RCNN to detect the human presence and extract faces using Multitask\nCascaded Convolutional Networks (MTCNN). Subsequently, we apply LBP/FaceNet to\nidentify a person. SafeNet sends image descriptions to the users with an MMS\ncontaining a person's name if any match found or as \"Unknown\", scene image,\nfacial description, and contextual information. SafeNet identifies\nfriends/families/caregiver versus intruders/unknown with an average F-score\n0.97 and generates image descriptions from 10 classes with an average F-measure\n0.97.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 04:33:02 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Alam", "Shahinur", ""], ["Mahmud", "Md Sultan", ""], ["Yeasin", "Mohammed", ""]]}, {"id": "2002.04407", "submitter": "Dario Zanca", "authors": "Dario Zanca, Stefano Melacci, Marco Gori", "title": "Toward Improving the Evaluation of Visual Attention Models: a\n  Crowdsourcing Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual attention is a complex phenomenon. A computational modeling of\nthis phenomenon must take into account where people look in order to evaluate\nwhich are the salient locations (spatial distribution of the fixations), when\nthey look in those locations to understand the temporal development of the\nexploration (temporal order of the fixations), and how they move from one\nlocation to another with respect to the dynamics of the scene and the mechanics\nof the eyes (dynamics). State-of-the-art models focus on learning saliency maps\nfrom human data, a process that only takes into account the spatial component\nof the phenomenon and ignore its temporal and dynamical counterparts. In this\nwork we focus on the evaluation methodology of models of human visual\nattention. We underline the limits of the current metrics for saliency\nprediction and scanpath similarity, and we introduce a statistical measure for\nthe evaluation of the dynamics of the simulated eye movements. While deep\nlearning models achieve astonishing performance in saliency prediction, our\nanalysis shows their limitations in capturing the dynamics of the process. We\nfind that unsupervised gravitational models, despite of their simplicity,\noutperform all competitors. Finally, exploiting a crowd-sourcing platform, we\npresent a study aimed at evaluating how strongly the scanpaths generated with\nthe unsupervised gravitational models appear plausible to naive and expert\nhuman observers.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 14:27:47 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 13:34:36 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zanca", "Dario", ""], ["Melacci", "Stefano", ""], ["Gori", "Marco", ""]]}, {"id": "2002.04414", "submitter": "Xiaofu Wu Dr", "authors": "Xiaofu Wu, Ben Xie, Shiliang Zhao, Suofei Zhang, Yong Xiao, Ming Li", "title": "Diversity-Achieving Slow-DropBlock Network for Person Re-Identification", "comments": "submitted to IEEE TMM for possible publication. arXiv admin note:\n  substantial text overlap with arXiv:2001.07442", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A big challenge of person re-identification (Re-ID) using a multi-branch\nnetwork architecture is to learn diverse features from the ID-labeled dataset.\nThe 2-branch Batch DropBlock (BDB) network was recently proposed for achieving\ndiversity between the global branch and the feature-dropping branch. In this\npaper, we propose to move the dropping operation from the intermediate feature\nlayer towards the input (image dropping). Since it may drop a large portion of\ninput images, this makes the training hard to converge. Hence, we propose a\nnovel double-batch-split co-training approach for remedying this problem. In\nparticular, we show that the feature diversity can be well achieved with the\nuse of multiple dropping branches by setting individual dropping ratio for each\nbranch. Empirical evidence demonstrates that the proposed method performs\nsuperior to BDB on popular person Re-ID datasets, including Market-1501,\nDukeMTMC-reID and CUHK03 and the use of more dropping branches can further\nboost the performance.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 08:05:39 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Wu", "Xiaofu", ""], ["Xie", "Ben", ""], ["Zhao", "Shiliang", ""], ["Zhang", "Suofei", ""], ["Xiao", "Yong", ""], ["Li", "Ming", ""]]}, {"id": "2002.04433", "submitter": "Hossein Javidnia", "authors": "Hossein Javidnia, Fran\\c{c}ois Piti\\'e", "title": "Background Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current state of the art alpha matting methods mainly rely on the trimap\nas the secondary and only guidance to estimate alpha. This paper investigates\nthe effects of utilising the background information as well as trimap in the\nprocess of alpha calculation. To achieve this goal, a state of the art method,\nAlphaGan is adopted and modified to process the background information as an\nextra input channel. Extensive experiments are performed to analyse the effect\nof the background information in image and video matting such as training with\nmildly and heavily distorted backgrounds. Based on the quantitative evaluations\nperformed on Adobe Composition-1k dataset, the proposed pipeline significantly\noutperforms the state of the art methods using AlphaMatting benchmark metrics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 14:46:57 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Javidnia", "Hossein", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "2002.04439", "submitter": "Maurice Quach", "authors": "Maurice Quach, Giuseppe Valenzise and Frederic Dufaux", "title": "Folding-based compression of point cloud attributes", "comments": "Published in ICIP 2020. The source code can be found at\n  https://github.com/mauriceqch/pcc_attr_folding", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques to compress point cloud attributes leverage either\ngeometric or video-based compression tools. We explore a radically different\napproach inspired by recent advances in point cloud representation learning.\nPoint clouds can be interpreted as 2D manifolds in 3D space. Specifically, we\nfold a 2D grid onto a point cloud and we map attributes from the point cloud\nonto the folded 2D grid using a novel optimized mapping method. This mapping\nresults in an image, which opens a way to apply existing image processing\ntechniques on point cloud attributes. However, as this mapping process is lossy\nin nature, we propose several strategies to refine it so that attributes can be\nmapped to the 2D grid with minimal distortion. Moreover, this approach can be\nflexibly applied to point cloud patches in order to better adapt to local\ngeometric complexity. In this work, we consider point cloud attribute\ncompression; thus, we compress this image with a conventional 2D image codec.\nOur preliminary results show that the proposed folding-based coding scheme can\nalready reach performance similar to the latest MPEG Geometry-based PCC (G-PCC)\ncodec.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 14:55:58 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 09:04:51 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 07:17:57 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Quach", "Maurice", ""], ["Valenzise", "Giuseppe", ""], ["Dufaux", "Frederic", ""]]}, {"id": "2002.04455", "submitter": "Jiawei Li", "authors": "Jiawei Li, Jae Chul Koh, Won-Sook Lee", "title": "HRINet: Alternative Supervision Network for High-resolution CT image\n  Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image interpolation in medical area is of high importance as most 3D\nbiomedical volume images are sampled where the distance between consecutive\nslices significantly greater than the in-plane pixel size due to radiation dose\nor scanning time. Image interpolation creates a number of new slices between\nknown slices in order to obtain an isotropic volume image. The results can be\nused for the higher quality of 3D reconstruction and visualization of human\nbody structures. Semantic interpolation on the manifold has been proved to be\nvery useful for smoothing image interpolation. Nevertheless, all previous\nmethods focused on low-resolution image interpolation, and most of them work\npoorly on high-resolution image. We propose a novel network, High Resolution\nInterpolation Network (HRINet), aiming at producing high-resolution CT image\ninterpolations. We combine the idea of ACAI and GANs, and propose a novel idea\nof alternative supervision method by applying supervised and unsupervised\ntraining alternatively to raise the accuracy of human organ structures in CT\nwhile keeping high quality. We compare an MSE based and a perceptual based loss\noptimizing methods for high quality interpolation, and show the tradeoff\nbetween the structural correctness and sharpness. Our experiments show the\ngreat improvement on 256 2 and 5122 images quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 15:09:42 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 19:13:08 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Li", "Jiawei", ""], ["Koh", "Jae Chul", ""], ["Lee", "Won-Sook", ""]]}, {"id": "2002.04461", "submitter": "Alexander Tong", "authors": "Alexander Tong, Jessie Huang, Guy Wolf, David van Dijk, Smita\n  Krishnaswamy", "title": "TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular\n  Dynamics", "comments": "Presented at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly common to encounter data from dynamic processes captured\nby static cross-sectional measurements over time, particularly in biomedical\nsettings. Recent attempts to model individual trajectories from this data use\noptimal transport to create pairwise matchings between time points. However,\nthese methods cannot model continuous dynamics and non-linear paths that\nentities can take in these systems. To address this issue, we establish a link\nbetween continuous normalizing flows and dynamic optimal transport, that allows\nus to model the expected paths of points over time. Continuous normalizing\nflows are generally under constrained, as they are allowed to take an arbitrary\npath from the source to the target distribution. We present TrajectoryNet,\nwhich controls the continuous paths taken between distributions to produce\ndynamic optimal transport. We show how this is particularly applicable for\nstudying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq)\ntechnologies, and that TrajectoryNet improves upon recently proposed static\noptimal transport-based models that can be used for interpolating cellular\ndistributions.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 21:00:38 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 13:42:19 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Tong", "Alexander", ""], ["Huang", "Jessie", ""], ["Wolf", "Guy", ""], ["van Dijk", "David", ""], ["Krishnaswamy", "Smita", ""]]}, {"id": "2002.04479", "submitter": "Kevin Karsch", "authors": "Kevin Karsch, Ce Liu, Sing Bing Kang", "title": "Depth Extraction from Video Using Non-parametric Sampling", "comments": "arXiv admin note: text overlap with arXiv:2001.00987", "journal-ref": "ECCV 2012: Computer Vision ECCV 2012: Lecture Notes in Computer\n  Science, vol 7576 pp 775-788", "doi": "10.1007/978-3-642-33715-4_56", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a technique that automatically generates plausible depth maps\nfrom videos using non-parametric depth sampling. We demonstrate our technique\nin cases where past methods fail (non-translating cameras and dynamic scenes).\nOur technique is applicable to single images as well as videos. For videos, we\nuse local motion cues to improve the inferred depth maps, while optical flow is\nused to ensure temporal depth consistency. For training and evaluation, we use\na Kinect-based system to collect a large dataset containing stereoscopic videos\nwith known depths. We show that our depth estimation technique outperforms the\nstate-of-the-art on benchmark databases. Our technique can be used to\nautomatically convert a monoscopic video into stereo for 3D visualization, and\nwe demonstrate this through a variety of visually pleasing results for indoor\nand outdoor scenes, including results from the feature film Charade.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 23:36:50 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Karsch", "Kevin", ""], ["Liu", "Ce", ""], ["Kang", "Sing Bing", ""]]}, {"id": "2002.04487", "submitter": "Wout Boerdijk", "authors": "Wout Boerdijk, Martin Sundermeyer, Maximilian Durner and Rudolph\n  Triebel", "title": "Self-Supervised Object-in-Gripper Segmentation from Robotic Motions", "comments": "15 pages, 11 figures. Video:\n  https://www.youtube.com/watch?v=srEwuuIIgzI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate object segmentation is a crucial task in the context of robotic\nmanipulation. However, creating sufficient annotated training data for neural\nnetworks is particularly time consuming and often requires manual labeling. To\nthis end, we propose a simple, yet robust solution for learning to segment\nunknown objects grasped by a robot. Specifically, we exploit motion and\ntemporal cues in RGB video sequences. Using optical flow estimation we first\nlearn to predict segmentation masks of our given manipulator. Then, these\nannotations are used in combination with motion cues to automatically\ndistinguish between background, manipulator and unknown, grasped object. In\ncontrast to existing systems our approach is fully self-supervised and\nindependent of precise camera calibration, 3D models or potentially imperfect\ndepth data. We perform a thorough comparison with alternative baselines and\napproaches from literature. The object masks and views are shown to be suitable\ntraining data for segmentation networks that generalize to novel environments\nand also allow for watertight 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 15:44:46 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 11:14:49 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2020 10:31:16 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Boerdijk", "Wout", ""], ["Sundermeyer", "Martin", ""], ["Durner", "Maximilian", ""], ["Triebel", "Rudolph", ""]]}, {"id": "2002.04493", "submitter": "Zhengdong Zhang", "authors": "Zhengdong Zhang, Shuai Li, Ziyang Wang and Yun Lu", "title": "A Novel and Efficient Tumor Detection Framework for Pancreatic Cancer\n  via CT Images", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Deep Convolutional Neural Networks (DCNNs) have shown robust performance\nand results in medical image analysis, a number of deep-learning-based tumor\ndetection methods were developed in recent years. Nowadays, the automatic\ndetection of pancreatic tumors using contrast-enhanced Computed Tomography (CT)\nis widely applied for the diagnosis and staging of pancreatic cancer.\nTraditional hand-crafted methods only extract low-level features. Normal\nconvolutional neural networks, however, fail to make full use of effective\ncontext information, which causes inferior detection results. In this paper, a\nnovel and efficient pancreatic tumor detection framework aiming at fully\nexploiting the context information at multiple scales is designed. More\nspecifically, the contribution of the proposed method mainly consists of three\ncomponents: Augmented Feature Pyramid networks, Self-adaptive Feature Fusion\nand a Dependencies Computation (DC) Module. A bottom-up path augmentation to\nfully extract and propagate low-level accurate localization information is\nestablished firstly. Then, the Self-adaptive Feature Fusion can encode much\nricher context information at multiple scales based on the proposed regions.\nFinally, the DC Module is specifically designed to capture the interaction\ninformation between proposals and surrounding tissues. Experimental results\nachieve competitive performance in detection with the AUC of 0.9455, which\noutperforms other state-of-the-art methods to our best of knowledge,\ndemonstrating the proposed framework can detect the tumor of pancreatic cancer\nefficiently and accurately.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 15:48:22 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Zhang", "Zhengdong", ""], ["Li", "Shuai", ""], ["Wang", "Ziyang", ""], ["Lu", "Yun", ""]]}, {"id": "2002.04500", "submitter": "Wouter Bulten", "authors": "Wouter Bulten, Maschenka Balkenhol, Jean-Jo\\\"el Awoumou Belinga,\n  Am\\'erico Brilhante, Asl{\\i} \\c{C}ak{\\i}r, Xavier Farr\\'e, Katerina\n  Geronatsiou, Vincent Molini\\'e, Guilherme Pereira, Paromita Roy, G\\\"unter\n  Saile, Paulo Salles, Ewout Schaafsma, Jo\\\"elle Tschui, Anne-Marie Vos, Hester\n  van Boven, Robert Vink, Jeroen van der Laak, Christina Hulsbergen-van de Kaa,\n  Geert Litjens", "title": "Artificial Intelligence Assistance Significantly Improves Gleason\n  Grading of Prostate Biopsies by Pathologists", "comments": "21 pages, 5 figures", "journal-ref": "Modern Pathology, Available online 5 August 2020", "doi": "10.1038/s41379-020-0640-y", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the Gleason score is the most important prognostic marker for prostate\ncancer patients, it suffers from significant observer variability. Artificial\nIntelligence (AI) systems, based on deep learning, have proven to achieve\npathologist-level performance at Gleason grading. However, the performance of\nsuch systems can degrade in the presence of artifacts, foreign tissue, or other\nanomalies. Pathologists integrating their expertise with feedback from an AI\nsystem could result in a synergy that outperforms both the individual\npathologist and the system. Despite the hype around AI assistance, existing\nliterature on this topic within the pathology domain is limited. We\ninvestigated the value of AI assistance for grading prostate biopsies. A panel\nof fourteen observers graded 160 biopsies with and without AI assistance. Using\nAI, the agreement of the panel with an expert reference standard significantly\nincreased (quadratically weighted Cohen's kappa, 0.799 vs 0.872; p=0.018). Our\nresults show the added value of AI systems for Gleason grading, but more\nimportantly, show the benefits of pathologist-AI synergy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 16:00:31 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Bulten", "Wouter", ""], ["Balkenhol", "Maschenka", ""], ["Belinga", "Jean-Jo\u00ebl Awoumou", ""], ["Brilhante", "Am\u00e9rico", ""], ["\u00c7ak\u0131r", "Asl\u0131", ""], ["Farr\u00e9", "Xavier", ""], ["Geronatsiou", "Katerina", ""], ["Molini\u00e9", "Vincent", ""], ["Pereira", "Guilherme", ""], ["Roy", "Paromita", ""], ["Saile", "G\u00fcnter", ""], ["Salles", "Paulo", ""], ["Schaafsma", "Ewout", ""], ["Tschui", "Jo\u00eblle", ""], ["Vos", "Anne-Marie", ""], ["van Boven", "Hester", ""], ["Vink", "Robert", ""], ["van der Laak", "Jeroen", ""], ["de Kaa", "Christina Hulsbergen-van", ""], ["Litjens", "Geert", ""]]}, {"id": "2002.04571", "submitter": "Mohamed Mejri", "authors": "Mohamed Mejri, Antoine Richard, C\\'edric Pradalier", "title": "A Survey On 3D Inner Structure Prediction from its Outer Shape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The analysis of the internal structure of trees is highly important for both\nforest experts, biological scientists, and the wood industry. Traditionally,\nCT-scanners are considered as the most efficient way to get an accurate inner\nrepresentation of the tree. However, this method requires an important\ninvestment and reduces the cost-effectiveness of this operation. Our goal is to\ndesign neural-network-based methods to predict the internal density of the tree\nfrom its external bark shape. This paper compares different image-to-image(2D),\nvolume-to-volume(3D) and Convolutional Long Short Term Memory based neural\nnetwork architectures in the context of the prediction of the defect\ndistribution inside trees from their external bark shape. Those models are\ntrained on a synthetic dataset of 1800 CT-scanned look-like volumetric\nstructures of the internal density of the trees and their corresponding\nexternal surface.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 17:56:38 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Mejri", "Mohamed", ""], ["Richard", "Antoine", ""], ["Pradalier", "C\u00e9dric", ""]]}, {"id": "2002.04599", "submitter": "Florian Tram\\`er", "authors": "Florian Tram\\`er and Jens Behrmann and Nicholas Carlini and Nicolas\n  Papernot and J\\\"orn-Henrik Jacobsen", "title": "Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial\n  Perturbations", "comments": "ICML 2020 (Supersedes the workshop paper \"Exploiting Excessive\n  Invariance caused by Norm-Bounded Adversarial Robustness\", arXiv:1903.10484)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are malicious inputs crafted to induce\nmisclassification. Commonly studied sensitivity-based adversarial examples\nintroduce semantically-small changes to an input that result in a different\nmodel prediction. This paper studies a complementary failure mode,\ninvariance-based adversarial examples, that introduce minimal semantic changes\nthat modify an input's true label yet preserve the model's prediction. We\ndemonstrate fundamental tradeoffs between these two types of adversarial\nexamples.\n  We show that defenses against sensitivity-based attacks actively harm a\nmodel's accuracy on invariance-based attacks, and that new approaches are\nneeded to resist both attack types. In particular, we break state-of-the-art\nadversarially-trained and certifiably-robust models by generating small\nperturbations that the models are (provably) robust to, yet that change an\ninput's class according to human labelers. Finally, we formally show that the\nexistence of excessively invariant classifiers arises from the presence of\noverly-robust predictive features in standard datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 18:50:23 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 16:53:43 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Tram\u00e8r", "Florian", ""], ["Behrmann", "Jens", ""], ["Carlini", "Nicholas", ""], ["Papernot", "Nicolas", ""], ["Jacobsen", "J\u00f6rn-Henrik", ""]]}, {"id": "2002.04600", "submitter": "Qingyu Li", "authors": "Qingyu Li, Yilei Shi, Xin Huang, Xiao Xiang Zhu", "title": "Building Footprint Generation by IntegratingConvolution Neural Network\n  with Feature PairwiseConditional Random Field (FPCRF)", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.2973720", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building footprint maps are vital to many remote sensing applications, such\nas 3D building modeling, urban planning, and disaster management. Due to the\ncomplexity of buildings, the accurate and reliable generation of the building\nfootprint from remote sensing imagery is still a challenging task. In this\nwork, an end-to-end building footprint generation approach that integrates\nconvolution neural network (CNN) and graph model is proposed. CNN serves as the\nfeature extractor, while the graph model can take spatial correlation into\nconsideration. Moreover, we propose to implement the feature pairwise\nconditional random field (FPCRF) as a graph model to preserve sharp boundaries\nand fine-grained segmentation. Experiments are conducted on four different\ndatasets: (1) Planetscope satellite imagery of the cities of Munich, Paris,\nRome, and Zurich; (2) ISPRS benchmark data from the city of Potsdam, (3) Dstl\nKaggle dataset; and (4) Inria Aerial Image Labeling data of Austin, Chicago,\nKitsap County, Western Tyrol, and Vienna. It is found that the proposed\nend-to-end building footprint generation framework with the FPCRF as the graph\nmodel can further improve the accuracy of building footprint generation by\nusing only CNN, which is the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 18:51:19 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Li", "Qingyu", ""], ["Shi", "Yilei", ""], ["Huang", "Xin", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2002.04626", "submitter": "Jacob Reinhold", "authors": "Jacob C. Reinhold, Yufan He, Shizhong Han, Yunqiang Chen, Dashan Gao,\n  Junghoon Lee, Jerry L. Prince, Aaron Carass", "title": "Finding novelty with uncertainty", "comments": "SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images are often used to detect and characterize pathology and\ndisease; however, automatically identifying and segmenting pathology in medical\nimages is challenging because the appearance of pathology across diseases\nvaries widely. To address this challenge, we propose a Bayesian deep learning\nmethod that learns to translate healthy computed tomography images to magnetic\nresonance images and simultaneously calculates voxel-wise uncertainty. Since\nhigh uncertainty occurs in pathological regions of the image, this uncertainty\ncan be used for unsupervised anomaly segmentation. We show encouraging\nexperimental results on an unsupervised anomaly segmentation task by combining\ntwo types of uncertainty into a novel quantity we call scibilic uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 19:00:22 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Reinhold", "Jacob C.", ""], ["He", "Yufan", ""], ["Han", "Shizhong", ""], ["Chen", "Yunqiang", ""], ["Gao", "Dashan", ""], ["Lee", "Junghoon", ""], ["Prince", "Jerry L.", ""], ["Carass", "Aaron", ""]]}, {"id": "2002.04634", "submitter": "Bruno Iochins Grisci", "authors": "Jonas da Silveira Bohrer, Bruno Iochins Grisci and Marcio Dorn", "title": "Neuroevolution of Neural Network Architectures Using CoDeepNEAT and\n  Keras", "comments": "The original work was presented by Jonas da Silveira Bohrer in\n  partial fulfillment of the requirements for the degree of Bachelor in\n  Computer Engineering at the Institute of Informatics of the Federal\n  University of Rio Grande do Sul (UFRGS), Brazil, with Marcio Dorn as advisor\n  and Bruno Iochins Grisci as co-advisor. The original text is available at\n  Lume: https://lume.ufrgs.br/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a huge field of study in computer science and statistics\ndedicated to the execution of computational tasks through algorithms that do\nnot require explicit instructions but instead rely on learning patterns from\ndata samples to automate inferences. A large portion of the work involved in a\nmachine learning project is to define the best type of algorithm to solve a\ngiven problem. Neural networks - especially deep neural networks - are the\npredominant type of solution in the field. However, the networks themselves can\nproduce very different results according to the architectural choices made for\nthem. Finding the optimal network topology and configurations for a given\nproblem is a challenge that requires domain knowledge and testing efforts due\nto a large number of parameters that need to be considered. The purpose of this\nwork is to propose an adapted implementation of a well-established evolutionary\ntechnique from the neuroevolution field that manages to automate the tasks of\ntopology and hyperparameter selection. It uses a popular and accessible machine\nlearning framework - Keras - as the back-end, presenting results and proposed\nchanges concerning the original algorithm. The implementation is available at\nGitHub (https://github.com/sbcblab/Keras-CoDeepNEAT) with documentation and\nexamples to reproduce the experiments performed for this work.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 19:03:34 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Bohrer", "Jonas da Silveira", ""], ["Grisci", "Bruno Iochins", ""], ["Dorn", "Marcio", ""]]}, {"id": "2002.04639", "submitter": "Jacob Reinhold", "authors": "Jacob C. Reinhold, Yufan He, Shizhong Han, Yunqiang Chen, Dashan Gao,\n  Junghoon Lee, Jerry L. Prince, Aaron Carass", "title": "Validating uncertainty in medical image translation", "comments": "IEEE ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images are increasingly used as input to deep neural networks to\nproduce quantitative values that aid researchers and clinicians. However,\nstandard deep neural networks do not provide a reliable measure of uncertainty\nin those quantitative values. Recent work has shown that using dropout during\ntraining and testing can provide estimates of uncertainty. In this work, we\ninvestigate using dropout to estimate epistemic and aleatoric uncertainty in a\nCT-to-MR image translation task. We show that both types of uncertainty are\ncaptured, as defined, providing confidence in the output uncertainty estimates.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 19:06:54 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Reinhold", "Jacob C.", ""], ["He", "Yufan", ""], ["Han", "Shizhong", ""], ["Chen", "Yunqiang", ""], ["Gao", "Dashan", ""], ["Lee", "Junghoon", ""], ["Prince", "Jerry L.", ""], ["Carass", "Aaron", ""]]}, {"id": "2002.04658", "submitter": "Tong Qin", "authors": "Jun Hou, Tong Qin, Kailiang Wu, Dongbin Xiu", "title": "A Non-Intrusive Correction Algorithm for Classification Problems with\n  Corrupted Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel correction algorithm is proposed for multi-class classification\nproblems with corrupted training data. The algorithm is non-intrusive, in the\nsense that it post-processes a trained classification model by adding a\ncorrection procedure to the model prediction. The correction procedure can be\ncoupled with any approximators, such as logistic regression, neural networks of\nvarious architectures, etc. When training dataset is sufficiently large, we\nprove that the corrected models deliver correct classification results as if\nthere is no corruption in the training data. For datasets of finite size, the\ncorrected models produce significantly better recovery results, compared to the\nmodels without the correction algorithm. All of the theoretical findings in the\npaper are verified by our numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 20:07:05 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Hou", "Jun", ""], ["Qin", "Tong", ""], ["Wu", "Kailiang", ""], ["Xiu", "Dongbin", ""]]}, {"id": "2002.04672", "submitter": "Kevin Liang", "authors": "Yuewei Yang, Kevin J Liang, Lawrence Carin", "title": "Object Detection as a Positive-Unlabeled Problem", "comments": "Published as a conference paper in the British Machine Vision\n  Conference (BMVC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As with other deep learning methods, label quality is important for learning\nmodern convolutional object detectors. However, the potentially large number\nand wide diversity of object instances that can be found in complex image\nscenes makes constituting complete annotations a challenging task; objects\nmissing annotations can be observed in a variety of popular object detection\ndatasets. These missing annotations can be problematic, as the standard\ncross-entropy loss employed to train object detection models treats\nclassification as a positive-negative (PN) problem: unlabeled regions are\nimplicitly assumed to be background. As such, any object missing a bounding box\nresults in a confusing learning signal, the effects of which we observe\nempirically. To remedy this, we propose treating object detection as a\npositive-unlabeled (PU) problem, which removes the assumption that unlabeled\nregions must be negative. We demonstrate that our proposed PU classification\nloss outperforms the standard PN loss on PASCAL VOC and MS COCO across a range\nof label missingness, as well as on Visual Genome and DeepLesion with full\nlabels.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 20:49:34 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 18:25:47 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yang", "Yuewei", ""], ["Liang", "Kevin J", ""], ["Carin", "Lawrence", ""]]}, {"id": "2002.04685", "submitter": "Guoxi Huang", "authors": "Guoxi Huang and Adrian G. Bors", "title": "Learning spatio-temporal representations with temporal squeeze pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new video representation learning method, named\nTemporal Squeeze (TS) pooling, which can extract the essential movement\ninformation from a long sequence of video frames and map it into a set of few\nimages, named Squeezed Images. By embedding the Temporal Squeeze pooling as a\nlayer into off-the-shelf Convolution Neural Networks (CNN), we design a new\nvideo classification model, named Temporal Squeeze Network (TeSNet). The\nresulting Squeezed Images contain the essential movement information from the\nvideo frames, corresponding to the optimization of the video classification\ntask. We evaluate our architecture on two video classification benchmarks, and\nthe results achieved are compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:13:12 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Huang", "Guoxi", ""], ["Bors", "Adrian G.", ""]]}, {"id": "2002.04688", "submitter": "Jeremy Howard", "authors": "Jeremy Howard and Sylvain Gugger", "title": "fastai: A Layered API for Deep Learning", "comments": null, "journal-ref": "Information 2020, 11(2), 108", "doi": "10.3390/info11020108", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  fastai is a deep learning library which provides practitioners with\nhigh-level components that can quickly and easily provide state-of-the-art\nresults in standard deep learning domains, and provides researchers with\nlow-level components that can be mixed and matched to build new approaches. It\naims to do both things without substantial compromises in ease of use,\nflexibility, or performance. This is possible thanks to a carefully layered\narchitecture, which expresses common underlying patterns of many deep learning\nand data processing techniques in terms of decoupled abstractions. These\nabstractions can be expressed concisely and clearly by leveraging the dynamism\nof the underlying Python language and the flexibility of the PyTorch library.\nfastai includes: a new type dispatch system for Python along with a semantic\ntype hierarchy for tensors; a GPU-optimized computer vision library which can\nbe extended in pure Python; an optimizer which refactors out the common\nfunctionality of modern optimizers into two basic pieces, allowing optimization\nalgorithms to be implemented in 4-5 lines of code; a novel 2-way callback\nsystem that can access any part of the data, model, or optimizer and change it\nat any point during training; a new data block API; and much more. We have used\nthis library to successfully create a complete deep learning course, which we\nwere able to write more quickly than using previous approaches, and the code\nwas more clear. The library is already in wide use in research, industry, and\nteaching. NB: This paper covers fastai v2, which is currently in pre-release at\nhttp://dev.fast.ai/\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:16:48 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 18:17:51 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Howard", "Jeremy", ""], ["Gugger", "Sylvain", ""]]}, {"id": "2002.04698", "submitter": "Juan Pablo Munoz", "authors": "Juan Pablo Munoz and Scott Dexter", "title": "Improving Place Recognition Using Dynamic Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to place recognition well-suited to environments\nwith many dynamic objects--objects that may or may not be present in an agent's\nsubsequent visits. By incorporating an object-detecting preprocessing step, our\napproach yields high-quality place representations that incorporate object\ninformation. Not only does this result in significantly improved place\nrecognition in dynamic environments, it also significantly reduces\nmemory/storage requirements, which may increase the effectiveness of mobile\nagents with limited resources.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:39:24 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 04:58:38 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Munoz", "Juan Pablo", ""], ["Dexter", "Scott", ""]]}, {"id": "2002.04700", "submitter": "Ziyang Wang", "authors": "Ziyang Wang", "title": "A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for\n  Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing awareness of high-quality life, there is a growing need\nfor health monitoring devices running robust algorithms in home environment.\nHealth monitoring technologies enable real-time analysis of users' health\nstatus, offering long-term healthcare support and reducing hospitalization\ntime. The purpose of this work is twofold, the software focuses on the analysis\nof gait, which is widely adopted for joint correction and assessing any lower\nlimb or spinal problem. On the hardware side, we design a novel marker-less\ngait analysis device using a low-cost RGB camera mounted on a mobile\ntele-robot. As gait analysis with a single camera is much more challenging\ncompared to previous works utilizing multi-cameras, a RGB-D camera or wearable\nsensors, we propose using vision-based human pose estimation approaches. More\nspecifically, based on the output of two state-of-the-art human pose estimation\nmodels (Openpose and VNect), we devise measurements for four bespoke gait\nparameters: inversion/eversion, dorsiflexion/plantarflexion, ankle and foot\nprogression angles. We thereby classify walking patterns into normal,\nsupination, pronation and limp. We also illustrate how to run the purposed\nmachine learning models in low-resource environments such as a single\nentry-level CPU. Experiments show that our single RGB camera method achieves\ncompetitive performance compared to state-of-the-art methods based on depth\ncameras or multi-camera motion capture system, at smaller hardware costs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:42:22 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 16:24:28 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 14:53:55 GMT"}, {"version": "v4", "created": "Sun, 15 Mar 2020 03:27:52 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wang", "Ziyang", ""]]}, {"id": "2002.04741", "submitter": "Hao  Chen", "authors": "Hao Chen, Yali Wang, Guoyou Wang, Xiang Bai, and Yu Qiao", "title": "Progressive Object Transfer Detection", "comments": "TIP 2019", "journal-ref": null, "doi": "10.1109/TIP.2019.2938680", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development of object detection mainly depends on deep learning with\nlarge-scale benchmarks. However, collecting such fully-annotated data is often\ndifficult or expensive for real-world applications, which restricts the power\nof deep neural networks in practice. Alternatively, humans can detect new\nobjects with little annotation burden, since humans often use the prior\nknowledge to identify new objects with few elaborately-annotated examples, and\nsubsequently generalize this capacity by exploiting objects from wild images.\nInspired by this procedure of learning to detect, we propose a novel\nProgressive Object Transfer Detection (POTD) framework. Specifically, we make\nthree main contributions in this paper. First, POTD can leverage various object\nsupervision of different domains effectively into a progressive detection\nprocedure. Via such human-like learning, one can boost a target detection task\nwith few annotations. Second, POTD consists of two delicate transfer stages,\ni.e., Low-Shot Transfer Detection (LSTD), and Weakly-Supervised Transfer\nDetection (WSTD). In LSTD, we distill the implicit object knowledge of source\ndetector to enhance target detector with few annotations. It can effectively\nwarm up WSTD later on. In WSTD, we design a recurrent object labelling\nmechanism for learning to annotate weakly-labeled images. More importantly, we\nexploit the reliable object supervision from LSTD, which can further enhance\nthe robustness of target detector in the WSTD stage. Finally, we perform\nextensive experiments on a number of challenging detection benchmarks with\ndifferent settings. The results demonstrate that, our POTD outperforms the\nrecent state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 00:16:24 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 05:06:51 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Chen", "Hao", ""], ["Wang", "Yali", ""], ["Wang", "Guoyou", ""], ["Bai", "Xiang", ""], ["Qiao", "Yu", ""]]}, {"id": "2002.04752", "submitter": "Rachel Draelos", "authors": "Rachel Lea Draelos, David Dov, Maciej A. Mazurowski, Joseph Y. Lo,\n  Ricardo Henao, Geoffrey D. Rubin, Lawrence Carin", "title": "Machine-Learning-Based Multiple Abnormality Prediction with Large-Scale\n  Chest Computed Tomography Volumes", "comments": "20 pages, 3 figures, 5 tables (appendices additional). Published in\n  Medical Image Analysis (October 2020)", "journal-ref": null, "doi": "10.1016/j.media.2020.101857", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models for radiology benefit from large-scale data sets with\nhigh quality labels for abnormalities. We curated and analyzed a chest computed\ntomography (CT) data set of 36,316 volumes from 19,993 unique patients. This is\nthe largest multiply-annotated volumetric medical imaging data set reported. To\nannotate this data set, we developed a rule-based method for automatically\nextracting abnormality labels from free-text radiology reports with an average\nF-score of 0.976 (min 0.941, max 1.0). We also developed a model for\nmulti-organ, multi-disease classification of chest CT volumes that uses a deep\nconvolutional neural network (CNN). This model reached a classification\nperformance of AUROC greater than 0.90 for 18 abnormalities, with an average\nAUROC of 0.773 for all 83 abnormalities, demonstrating the feasibility of\nlearning from unfiltered whole volume CT data. We show that training on more\nlabels improves performance significantly: for a subset of 9 labels - nodule,\nopacity, atelectasis, pleural effusion, consolidation, mass, pericardial\neffusion, cardiomegaly, and pneumothorax - the model's average AUROC increased\nby 10% when the number of training labels was increased from 9 to all 83. All\ncode for volume preprocessing, automated label extraction, and the volume\nabnormality prediction model will be made publicly available. The 36,316 CT\nvolumes and labels will also be made publicly available pending institutional\napproval.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 00:59:23 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 17:39:03 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 23:57:17 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Draelos", "Rachel Lea", ""], ["Dov", "David", ""], ["Mazurowski", "Maciej A.", ""], ["Lo", "Joseph Y.", ""], ["Henao", "Ricardo", ""], ["Rubin", "Geoffrey D.", ""], ["Carin", "Lawrence", ""]]}, {"id": "2002.04776", "submitter": "Mohammad Saeed Abrishami", "authors": "Mohammad Saeed Abrishami, Amir Erfan Eshratifar, David Eigen, Yanzhi\n  Wang, Shahin Nazarian, Massoud Pedram", "title": "Efficient Training of Deep Convolutional Neural Networks by Augmentation\n  in Embedding Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the field of artificial intelligence have been made\npossible by deep neural networks. In applications where data are scarce,\ntransfer learning and data augmentation techniques are commonly used to improve\nthe generalization of deep learning models. However, fine-tuning a transfer\nmodel with data augmentation in the raw input space has a high computational\ncost to run the full network for every augmented input. This is particularly\ncritical when large models are implemented on embedded devices with limited\ncomputational and energy resources. In this work, we propose a method that\nreplaces the augmentation in the raw input space with an approximate one that\nacts purely in the embedding space. Our experimental results show that the\nproposed method drastically reduces the computation, while the accuracy of\nmodels is negligibly compromised.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 03:26:33 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Abrishami", "Mohammad Saeed", ""], ["Eshratifar", "Amir Erfan", ""], ["Eigen", "David", ""], ["Wang", "Yanzhi", ""], ["Nazarian", "Shahin", ""], ["Pedram", "Massoud", ""]]}, {"id": "2002.04780", "submitter": "Shuang Xu", "authors": "Shuang Xu and Xiaoli Wei and Chunxia Zhang and Junmin Liu and Jiangshe\n  Zhang", "title": "MFFW: A new dataset for multi-focus image fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-focus image fusion (MFF) is a fundamental task in the field of\ncomputational photography. Current methods have achieved significant\nperformance improvement. It is found that current methods are evaluated on\nsimulated image sets or Lytro dataset. Recently, a growing number of\nresearchers pay attention to defocus spread effect, a phenomenon of real-world\nmulti-focus images. Nonetheless, defocus spread effect is not obvious in\nsimulated or Lytro datasets, where popular methods perform very similar. To\ncompare their performance on images with defocus spread effect, this paper\nconstructs a new dataset called MFF in the wild (MFFW). It contains 19 pairs of\nmulti-focus images collected on the Internet. We register all pairs of source\nimages, and provide focus maps and reference images for part of pairs. Compared\nwith Lytro dataset, images in MFFW significantly suffer from defocus spread\neffect. In addition, the scenes of MFFW are more complex. The experiments\ndemonstrate that most state-of-the-art methods on MFFW dataset cannot robustly\ngenerate satisfactory fusion images. MFFW can be a new baseline dataset to test\nwhether an MMF algorithm is able to deal with defocus spread effect.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 03:35:37 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Xu", "Shuang", ""], ["Wei", "Xiaoli", ""], ["Zhang", "Chunxia", ""], ["Liu", "Junmin", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "2002.04791", "submitter": "Xin-Long Luo", "authors": "Xin-long Luo, Jia-hui Lv and Geng Sun", "title": "A Visual-inertial Navigation Method for High-Speed Unmanned Aerial\n  Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA cs.SY eess.SY math.DS math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the localization problem of high-speed high-altitude\nunmanned aerial vehicle (UAV) with a monocular camera and inertial navigation\nsystem. It proposes a navigation method utilizing the complementarity of vision\nand inertial devices to overcome the singularity which arises from the\nhorizontal flight of UAV. Furthermore, it modifies the mathematical model of\nlocalization problem via separating linear parts from nonlinear parts and\nreplaces a nonlinear least-squares problem with a linearly equality-constrained\noptimization problem. In order to avoid the ill-condition property near the\noptimal point of sequential unconstrained minimization techniques(penalty\nmethods), it constructs a semi-implicit continuous method with a trust-region\ntechnique based on a differential-algebraic dynamical system to solve the\nlinearly equality-constrained optimization problem. It also analyzes the global\nconvergence property of the semi-implicit continuous method in an infinity\nintegrated interval other than the traditional convergence analysis of\nnumerical methods for ordinary differential equations in a finite integrated\ninterval. Finally, the promising numerical results are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 04:28:11 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Luo", "Xin-long", ""], ["Lv", "Jia-hui", ""], ["Sun", "Geng", ""]]}, {"id": "2002.04821", "submitter": "Masoud Pourreza", "authors": "Mohammad Sabokrou, Masoud Pourreza, Xiaobai Li, Mahmood Fathy, Guoying\n  Zhao", "title": "Deep-HR: Fast Heart Rate Estimation from Face Video Under Realistic\n  Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for remote heart rate (HR) estimation.\nRecent studies have proved that blood pumping by the heart is highly correlated\nto the intense color of face pixels, and surprisingly can be utilized for\nremote HR estimation. Researchers successfully proposed several methods for\nthis task, but making it work in realistic situations is still a challenging\nproblem in computer vision community. Furthermore, learning to solve such a\ncomplex task on a dataset with very limited annotated samples is not\nreasonable. Consequently, researchers do not prefer to use the deep learning\napproaches for this problem. In this paper, we propose a simple yet efficient\napproach to benefit the advantages of the Deep Neural Network (DNN) by\nsimplifying HR estimation from a complex task to learning from very correlated\nrepresentation to HR. Inspired by previous work, we learn a component called\nFront-End (FE) to provide a discriminative representation of face videos,\nafterward a light deep regression auto-encoder as Back-End (BE) is learned to\nmap the FE representation to HR. Regression task on the informative\nrepresentation is simple and could be learned efficiently on limited training\nsamples. Beside of this, to be more accurate and work well on low-quality\nvideos, two deep encoder-decoder networks are trained to refine the output of\nFE. We also introduce a challenging dataset (HR-D) to show that our method can\nefficiently work in realistic conditions. Experimental results on HR-D and\nMAHNOB datasets confirm that our method could run as a real-time method and\nestimate the average HR better than state-of-the-art ones.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 07:00:07 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Sabokrou", "Mohammad", ""], ["Pourreza", "Masoud", ""], ["Li", "Xiaobai", ""], ["Fathy", "Mahmood", ""], ["Zhao", "Guoying", ""]]}, {"id": "2002.04829", "submitter": "Cong Geng", "authors": "Cong Geng, Jia Wang, Li Chen, Wenbo Bao, Chu Chu, Zhiyong Gao", "title": "Uniform Interpolation Constrained Geodesic Learning on Data Manifold", "comments": "submitted to NIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to learn a minimizing geodesic within a\ndata manifold. Along the learned geodesic, our method can generate high-quality\ninterpolations between two given data samples. Specifically, we use an\nautoencoder network to map data samples into latent space and perform\ninterpolation via an interpolation network. We add prior geometric information\nto regularize our autoencoder for the convexity of representations so that for\nany given interpolation approach, the generated interpolations remain within\nthe distribution of the data manifold. Before the learning of a geodesic, a\nproper Riemannianmetric should be defined. Therefore, we induce a Riemannian\nmetric by the canonical metric in the Euclidean space which the data manifold\nis isometrically immersed in. Based on this defined Riemannian metric, we\nintroduce a constant speed loss and a minimizing geodesic loss to regularize\nthe interpolation network to generate uniform interpolation along the learned\ngeodesic on the manifold. We provide a theoretical analysis of our model and\nuse image translation as an example to demonstrate the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 07:47:41 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 10:16:20 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 01:23:45 GMT"}, {"version": "v4", "created": "Fri, 14 Aug 2020 05:32:56 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Geng", "Cong", ""], ["Wang", "Jia", ""], ["Chen", "Li", ""], ["Bao", "Wenbo", ""], ["Chu", "Chu", ""], ["Gao", "Zhiyong", ""]]}, {"id": "2002.04831", "submitter": "Zi Yin", "authors": "Zi Yin, Valentin Yiu, Xiaolin Hu, Liang Tang", "title": "End-to-End Face Parsing via Interlinked Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face parsing is an important computer vision task that requires accurate\npixel segmentation of facial parts (such as eyes, nose, mouth, etc.), providing\na basis for further face analysis, modification, and other applications.\nInterlinked Convolutional Neural Networks (iCNN) was proved to be an effective\ntwo-stage model for face parsing. However, the original iCNN was trained\nseparately in two stages, limiting its performance. To solve this problem, we\nintroduce a simple, end-to-end face parsing framework: STN-aided\niCNN(STN-iCNN), which extends the iCNN by adding a Spatial Transformer Network\n(STN) between the two isolated stages. The STN-iCNN uses the STN to provide a\ntrainable connection to the original two-stage iCNN pipeline, making end-to-end\njoint training possible. Moreover, as a by-product, STN also provides more\nprecise cropped parts than the original cropper. Due to these two advantages,\nour approach significantly improves the accuracy of the original model. Our\nmodel achieved competitive performance on the Helen Dataset, the standard face\nparsing dataset. It also achieved superior performance on CelebAMask-HQ\ndataset, proving its good generalization. Our code has been released at\nhttps://github.com/aod321/STN-iCNN.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 08:03:03 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 19:27:52 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Yin", "Zi", ""], ["Yiu", "Valentin", ""], ["Hu", "Xiaolin", ""], ["Tang", "Liang", ""]]}, {"id": "2002.04836", "submitter": "Mehmet Burak Say{\\i}c{\\i}", "authors": "Mehmet Burak Say{\\i}c{\\i}, Rikiya Yamashita, Jeanne Shen", "title": "Analysis Of Multi Field Of View Cnn And Attention Cnn On H&E Stained\n  Whole-slide Images On Hepatocellular Carcinoma", "comments": "This paper has been withdrawn by the authors due to need for heavy\n  revise", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hepatocellular carcinoma (HCC) is a leading cause of cancer-related death\nworldwide. Whole-slide imaging which is a method of scanning glass slides have\nbeen employed for diagnosis of HCC. Using high resolution Whole-slide images is\ninfeasible for Convolutional Neural Network applications. Hence tiling the\nWhole-slide images is a common methodology for assigning Convolutional Neural\nNetworks for classification and segmentation. Determination of the tile size\naffects the performance of the algorithms since small field of view can not\ncapture the information on a larger scale and large field of view can not\ncapture the information on a cellular scale. In this work, the effect of tile\nsize on performance for classification problem is analysed. In addition, Multi\nField of View CNN is assigned for taking advantage of the information provided\nby different tile sizes and Attention CNN is assigned for giving the capability\nof voting most contributing tile size. It is found that employing more than one\ntile size significantly increases the performance of the classification by\n3.97% and both algorithms are found successful over the algorithm which uses\nonly one tile size.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 08:18:40 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 23:25:48 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Say\u0131c\u0131", "Mehmet Burak", ""], ["Yamashita", "Rikiya", ""], ["Shen", "Jeanne", ""]]}, {"id": "2002.04869", "submitter": "Guanglei Yang", "authors": "Guanglei Yang, Haifeng Xia, Mingli Ding, Zhengming Ding", "title": "Bi-Directional Generation for Unsupervised Domain Adaptation", "comments": "9 pages, 4 figures", "journal-ref": "Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI),\n  2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation facilitates the unlabeled target domain\nrelying on well-established source domain information. The conventional methods\nforcefully reducing the domain discrepancy in the latent space will result in\nthe destruction of intrinsic data structure. To balance the mitigation of\ndomain gap and the preservation of the inherent structure, we propose a\nBi-Directional Generation domain adaptation model with consistent classifiers\ninterpolating two intermediate domains to bridge source and target domains.\nSpecifically, two cross-domain generators are employed to synthesize one domain\nconditioned on the other. The performance of our proposed method can be further\nenhanced by the consistent classifiers and the cross-domain alignment\nconstraints. We also design two classifiers which are jointly optimized to\nmaximize the consistency on target sample prediction. Extensive experiments\nverify that our proposed model outperforms the state-of-the-art on standard\ncross domain visual benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 09:45:39 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Yang", "Guanglei", ""], ["Xia", "Haifeng", ""], ["Ding", "Mingli", ""], ["Ding", "Zhengming", ""]]}, {"id": "2002.04908", "submitter": "Feng Liu", "authors": "Haozhe Liu, Wentian Zhang, Guojie Liu and Feng Liu", "title": "A Zero-Shot based Fingerprint Presentation Attack Detection System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of presentation attacks, Automated Fingerprint\nRecognition Systems(AFRSs) are vulnerable to presentation attack. Thus,\nnumerous methods of presentation attack detection(PAD) have been proposed to\nensure the normal utilization of AFRS. However, the demand of large-scale\npresentation attack images and the low-level generalization ability always\nastrict existing PAD methods' actual performances. Therefore, we propose a\nnovel Zero-Shot Presentation Attack Detection Model to guarantee the\ngeneralization of the PAD model. The proposed ZSPAD-Model based on generative\nmodel does not utilize any negative samples in the process of establishment,\nwhich ensures the robustness for various types or materials based presentation\nattack. Different from other auto-encoder based model, the Fine-grained Map\narchitecture is proposed to refine the reconstruction error of the auto-encoder\nnetworks and a task-specific gaussian model is utilized to improve the quality\nof clustering. Meanwhile, in order to improve the performance of the proposed\nmodel, 9 confidence scores are discussed in this article. Experimental results\nshowed that the ZSPAD-Model is the state of the art for ZSPAD, and the MS-Score\nis the best confidence score. Compared with existing methods, the proposed\nZSPAD-Model performs better than the feature-based method and under the\nmulti-shot setting, the proposed method overperforms the learning based method\nwith little training data. When large training data is available, their results\nare similar.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 10:52:38 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Liu", "Haozhe", ""], ["Zhang", "Wentian", ""], ["Liu", "Guojie", ""], ["Liu", "Feng", ""]]}, {"id": "2002.04924", "submitter": "Mattias Nilsson", "authors": "Mattias Nilsson, Foteini Liwicki and Fredrik Sandin", "title": "Synaptic Integration of Spatiotemporal Features with a Dynamic\n  Neuromorphic Processor", "comments": "Copyright 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN),\n  2020, pp. 1-7", "doi": "10.1109/IJCNN48605.2020.9207210", "report-no": null, "categories": "cs.NE cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neurons can perform spatiotemporal feature detection by nonlinear\nsynaptic and dendritic integration of presynaptic spike patterns.\nMulticompartment models of non-linear dendrites and related neuromorphic\ncircuit designs enable faithful imitation of such dynamic integration\nprocesses, but these approaches are also associated with a relatively high\ncomputing cost or circuit size. Here, we investigate synaptic integration of\nspatiotemporal spike patterns with multiple dynamic synapses on point-neurons\nin the DYNAP-SE neuromorphic processor, which offers a complementary\nresource-efficient, albeit less flexible, approach to feature detection. We\ninvestigate how previously proposed excitatory--inhibitory pairs of dynamic\nsynapses can be combined to integrate multiple inputs, and we generalize that\nconcept to a case in which one inhibitory synapse is combined with multiple\nexcitatory synapses. We characterize the resulting delayed excitatory\npostsynaptic potentials (EPSPs) by measuring and analyzing the membrane\npotentials of the neuromorphic neuronal circuits. We find that biologically\nrelevant EPSP delays, with variability of order 10 milliseconds per neuron, can\nbe realized in the proposed manner by selecting different synapse combinations,\nthanks to device mismatch. Based on these results, we demonstrate that a single\npoint-neuron with dynamic synapses in the DYNAP-SE can respond selectively to\npresynaptic spikes with a particular spatiotemporal structure, which enables,\nfor instance, visual feature tuning of single neurons.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 11:26:35 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:05:32 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Nilsson", "Mattias", ""], ["Liwicki", "Foteini", ""], ["Sandin", "Fredrik", ""]]}, {"id": "2002.04932", "submitter": "Menglin Wang", "authors": "Menglin Wang, Baisheng Lai, Haokun Chen, Jianqiang Huang, Xiaojin\n  Gong, Xian-Sheng Hua", "title": "Towards Precise Intra-camera Supervised Person Re-identification", "comments": "Accepted by WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-camera supervision (ICS) for person re-identification (Re-ID) assumes\nthat identity labels are independently annotated within each camera view and no\ninter-camera identity association is labeled. It is a new setting proposed\nrecently to reduce the burden of annotation while expect to maintain desirable\nRe-ID performance. However, the lack of inter-camera labels makes the ICS Re-ID\nproblem much more challenging than the fully supervised counterpart. By\ninvestigating the characteristics of ICS, this paper proposes camera-specific\nnon-parametric classifiers, together with a hybrid mining quintuplet loss, to\nperform intra-camera learning. Then, an inter-camera learning module consisting\nof a graph-based ID association step and a Re-ID model updating step is\nconducted. Extensive experiments on three large-scale Re-ID datasets show that\nour approach outperforms all existing ICS works by a great margin. Our approach\nperforms even comparable to state-of-the-art fully supervised methods in two of\nthe datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 11:56:30 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 07:14:23 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Wang", "Menglin", ""], ["Lai", "Baisheng", ""], ["Chen", "Haokun", ""], ["Huang", "Jianqiang", ""], ["Gong", "Xiaojin", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2002.04967", "submitter": "Hao-Chiang Shao", "authors": "Hao-Chiang Shao, Chao-Yi Peng, Jun-Rei Wu, Chia-Wen Lin, Shao-Yun\n  Fang, Pin-Yen Tsai, Yan-Hsiu Liu", "title": "From IC Layout to Die Photo: A CNN-Based Data-Driven Approach", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": "10.1109/TCAD.2020.3015469", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning-based data-driven framework consisting of two\nconvolutional neural networks: i) LithoNet that predicts the shape deformations\non a circuit due to IC fabrication, and ii) OPCNet that suggests IC layout\ncorrections to compensate for such shape deformations. By learning the shape\ncorrespondences between pairs of layout design patterns and their scanning\nelectron microscope (SEM) images of the product wafer thereof, given an IC\nlayout pattern, LithoNet can mimic the fabrication process to predict its\nfabricated circuit shape. Furthermore, LithoNet can take the wafer fabrication\nparameters as a latent vector to model the parametric product variations that\ncan be inspected on SEM images. Besides, traditional optical proximity\ncorrection (OPC) methods used to suggest a correction on a lithographic\nphotomask is computationally expensive. Our proposed OPCNet mimics the OPC\nprocedure and efficiently generates a corrected photomask by collaborating with\nLithoNet to examine if the shape of a fabricated circuit optimally matches its\noriginal layout design. As a result, the proposed LithoNet-OPCNet framework can\nnot only predict the shape of a fabricated IC from its layout pattern, but also\nsuggests a layout correction according to the consistency between the predicted\nshape and the given layout. Experimental results with several benchmark layout\npatterns demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 04:02:04 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 13:16:34 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Shao", "Hao-Chiang", ""], ["Peng", "Chao-Yi", ""], ["Wu", "Jun-Rei", ""], ["Lin", "Chia-Wen", ""], ["Fang", "Shao-Yun", ""], ["Tsai", "Pin-Yen", ""], ["Liu", "Yan-Hsiu", ""]]}, {"id": "2002.04988", "submitter": "Yash Patel", "authors": "Yash Patel, Srikar Appalaraju, R. Manmatha", "title": "Saliency Driven Perceptual Image Compression", "comments": "WACV 2021 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new end-to-end trainable model for lossy image\ncompression, which includes several novel components. The method incorporates\n1) an adequate perceptual similarity metric; 2) saliency in the images; 3) a\nhierarchical auto-regressive model. This paper demonstrates that the popularly\nused evaluations metrics such as MS-SSIM and PSNR are inadequate for judging\nthe performance of image compression techniques as they do not align with the\nhuman perception of similarity. Alternatively, a new metric is proposed, which\nis learned on perceptual similarity data specific to image compression. The\nproposed compression model incorporates the salient regions and optimizes on\nthe proposed perceptual similarity metric. The model not only generates images\nwhich are visually better but also gives superior performance for subsequent\ncomputer vision tasks such as object detection and segmentation when compared\nto existing engineered or learned compression techniques.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 13:43:17 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 17:03:14 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Patel", "Yash", ""], ["Appalaraju", "Srikar", ""], ["Manmatha", "R.", ""]]}, {"id": "2002.04993", "submitter": "Anthony Cioppa", "authors": "Anthony Cioppa and Marc Van Droogenbroeck and Marc Braham", "title": "Real-Time Semantic Background Subtraction", "comments": "Accepted and Published at ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic background subtraction SBS has been shown to improve the performance\nof most background subtraction algorithms by combining them with semantic\ninformation, derived from a semantic segmentation network. However, SBS\nrequires high-quality semantic segmentation masks for all frames, which are\nslow to compute. In addition, most state-of-the-art background subtraction\nalgorithms are not real-time, which makes them unsuitable for real-world\napplications. In this paper, we present a novel background subtraction\nalgorithm called Real-Time Semantic Background Subtraction (denoted RT-SBS)\nwhich extends SBS for real-time constrained applications while keeping similar\nperformances. RT-SBS effectively combines a real-time background subtraction\nalgorithm with high-quality semantic information which can be provided at a\nslower pace, independently for each pixel. We show that RT-SBS coupled with\nViBe sets a new state of the art for real-time background subtraction\nalgorithms and even competes with the non real-time state-of-the-art ones. Note\nthat we provide python CPU and GPU implementations of RT-SBS at\nhttps://github.com/cioppaanthony/rt-sbs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 13:46:01 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 09:33:32 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 08:49:42 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Cioppa", "Anthony", ""], ["Van Droogenbroeck", "Marc", ""], ["Braham", "Marc", ""]]}, {"id": "2002.05000", "submitter": "Tao Zhou", "authors": "Tao Zhou, Huazhu Fu, Geng Chen, Jianbing Shen, and Ling Shao", "title": "Hi-Net: Hybrid-fusion Network for Multi-modal MR Image Synthesis", "comments": "has been accepted by IEEE TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) is a widely used neuroimaging technique that\ncan provide images of different contrasts (i.e., modalities). Fusing this\nmulti-modal data has proven particularly effective for boosting model\nperformance in many tasks. However, due to poor data quality and frequent\npatient dropout, collecting all modalities for every patient remains a\nchallenge. Medical image synthesis has been proposed as an effective solution\nto this, where any missing modalities are synthesized from the existing ones.\nIn this paper, we propose a novel Hybrid-fusion Network (Hi-Net) for\nmulti-modal MR image synthesis, which learns a mapping from multi-modal source\nimages (i.e., existing modalities) to target images (i.e., missing modalities).\nIn our Hi-Net, a modality-specific network is utilized to learn representations\nfor each individual modality, and a fusion network is employed to learn the\ncommon latent representation of multi-modal data. Then, a multi-modal synthesis\nnetwork is designed to densely combine the latent representation with\nhierarchical features from each modality, acting as a generator to synthesize\nthe target images. Moreover, a layer-wise multi-modal fusion strategy is\npresented to effectively exploit the correlations among multiple modalities, in\nwhich a Mixed Fusion Block (MFB) is proposed to adaptively weight different\nfusion strategies (i.e., element-wise summation, product, and maximization).\nExtensive experiments demonstrate that the proposed model outperforms other\nstate-of-the-art medical image synthesis methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 08:26:42 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Zhou", "Tao", ""], ["Fu", "Huazhu", ""], ["Chen", "Geng", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "2002.05028", "submitter": "Guillaume Boisson", "authors": "Tom\\'as V\\\"olker, Guillaume Boisson, Bertrand Chupeau", "title": "Learning light field synthesis with Multi-Plane Images: scene encoding\n  as a recurrent segmentation task", "comments": "Accepted to ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of view synthesis from large baseline\nlight fields, by turning a sparse set of input views into a Multi-plane Image\n(MPI). Because available datasets are scarce, we propose a lightweight network\nthat does not require extensive training. Unlike latest approaches, our model\ndoes not learn to estimate RGB layers but only encodes the scene geometry\nwithin MPI alpha layers, which comes down to a segmentation task. A Learned\nGradient Descent (LGD) framework is used to cascade the same convolutional\nnetwork in a recurrent fashion in order to refine the volumetric representation\nobtained. Thanks to its low number of parameters, our model trains successfully\non a small light field video dataset and provides visually appealing results.\nIt also exhibits convenient generalization properties regarding both the number\nof input views, the number of depth planes in the MPI, and the number of\nrefinement iterations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 14:35:54 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 09:28:52 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 11:25:09 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["V\u00f6lker", "Tom\u00e1s", ""], ["Boisson", "Guillaume", ""], ["Chupeau", "Bertrand", ""]]}, {"id": "2002.05046", "submitter": "Xiangping Zhu", "authors": "Xiangping Zhu, Xiatian Zhu, Minxian Li, Pietro Morerio, Vittorio\n  Murino, and Shaogang Gong", "title": "Intra-Camera Supervised Person Re-Identification", "comments": "Accepted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person re-identification (re-id) methods mostly exploit a large set\nof cross-camera identity labelled training data. This requires a tedious data\ncollection and annotation process, leading to poor scalability in practical\nre-id applications. On the other hand unsupervised re-id methods do not need\nidentity label information, but they usually suffer from much inferior and\ninsufficient model performance. To overcome these fundamental limitations, we\npropose a novel person re-identification paradigm based on an idea of\nindependent per-camera identity annotation. This eliminates the most\ntime-consuming and tedious inter-camera identity labelling process,\nsignificantly reducing the amount of human annotation efforts. Consequently, it\ngives rise to a more scalable and more feasible setting, which we call\nIntra-Camera Supervised (ICS) person re-id, for which we formulate a Multi-tAsk\nmulTi-labEl (MATE) deep learning method. Specifically, MATE is designed for\nself-discovering the cross-camera identity correspondence in a per-camera\nmulti-task inference framework. Extensive experiments demonstrate the\ncost-effectiveness superiority of our method over the alternative approaches on\nthree large person re-id datasets. For example, MATE yields 88.7% rank-1 score\non Market-1501 in the proposed ICS person re-id setting, significantly\noutperforming unsupervised learning models and closely approaching conventional\nfully supervised learning competitors.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:26:33 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 15:49:03 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 06:55:06 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zhu", "Xiangping", ""], ["Zhu", "Xiatian", ""], ["Li", "Minxian", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""], ["Gong", "Shaogang", ""]]}, {"id": "2002.05049", "submitter": "Christian Wachinger", "authors": "Christian Wachinger and Anna Rieckmann and Sebastian P\\\"olsterl", "title": "Detect and Correct Bias in Multi-Site Neuroimaging Datasets", "comments": null, "journal-ref": "Medical Image Analysis, 2020", "doi": "10.1016/j.media.2020.101879", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The desire to train complex machine learning algorithms and to increase the\nstatistical power in association studies drives neuroimaging research to use\never-larger datasets. The most obvious way to increase sample size is by\npooling scans from independent studies. However, simple pooling is often\nill-advised as selection, measurement, and confounding biases may creep in and\nyield spurious correlations. In this work, we combine 35,320 magnetic resonance\nimages of the brain from 17 studies to examine bias in neuroimaging. In the\nfirst experiment, Name That Dataset, we provide empirical evidence for the\npresence of bias by showing that scans can be correctly assigned to their\nrespective dataset with 71.5% accuracy. Given such evidence, we take a closer\nlook at confounding bias, which is often viewed as the main shortcoming in\nobservational studies. In practice, we neither know all potential confounders\nnor do we have data on them. Hence, we model confounders as unknown, latent\nvariables. Kolmogorov complexity is then used to decide whether the confounded\nor the causal model provides the simplest factorization of the graphical model.\nFinally, we present methods for dataset harmonization and study their ability\nto remove bias in imaging features. In particular, we propose an extension of\nthe recently introduced ComBat algorithm to control for global variation across\nimage features, inspired by adjusting for population stratification in\ngenetics. Our results demonstrate that harmonization can reduce\ndataset-specific information in image features. Further, confounding bias can\nbe reduced and even turned into a causal relationship. However, harmonziation\nalso requires caution as it can easily remove relevant subject-specific\ninformation. Code is available at https://github.com/ai-med/Dataset-Bias.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:32:24 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 20:11:25 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Wachinger", "Christian", ""], ["Rieckmann", "Anna", ""], ["P\u00f6lsterl", "Sebastian", ""]]}, {"id": "2002.05067", "submitter": "Chuhua Xian", "authors": "Chuhua Xian, Dongjiu Zhang, Chengkai Dai, Charlie C. L. Wang", "title": "Fast Generation of High Fidelity RGB-D Images by Deep-Learning with\n  Adaptive Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the raw data from consumer-level RGB-D cameras as input, we propose a\ndeep-learning based approach to efficiently generate RGB-D images with\ncompleted information in high resolution. To process the input images in low\nresolution with missing regions, new operators for adaptive convolution are\nintroduced in our deep-learning network that consists of three cascaded modules\n-- the completion module, the refinement module and the super-resolution\nmodule. The completion module is based on an architecture of encoder-decoder,\nwhere the features of input raw RGB-D will be automatically extracted by the\nencoding layers of a deep neural-network. The decoding layers are applied to\nreconstruct the completed depth map, which is followed by a refinement module\nto sharpen the boundary of different regions. For the super-resolution module,\nwe generate RGB-D images in high resolution by multiple layers for feature\nextraction and a layer for up-sampling. Benefited from the adaptive convolution\noperators newly proposed in this paper, our results outperform the existing\ndeep-learning based approaches for RGB-D image complete and super-resolution.\nAs an end-to-end approach, high fidelity RGB-D images can be generated\nefficiently at the rate of around 21 frames per second.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 16:14:38 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 06:52:49 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 04:10:34 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Xian", "Chuhua", ""], ["Zhang", "Dongjiu", ""], ["Dai", "Chengkai", ""], ["Wang", "Charlie C. L.", ""]]}, {"id": "2002.05070", "submitter": "Jianren Wang", "authors": "Jianren Wang, Zhaoyuan Fang, Hang Zhao", "title": "AlignNet: A Unifying Approach to Audio-Visual Alignment", "comments": "WACV2020. Project video and code are available at\n  https://jianrenw.github.io/AlignNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AlignNet, a model that synchronizes videos with reference audios\nunder non-uniform and irregular misalignments. AlignNet learns the end-to-end\ndense correspondence between each frame of a video and an audio. Our method is\ndesigned according to simple and well-established principles: attention,\npyramidal processing, warping, and affinity function. Together with the model,\nwe release a dancing dataset Dance50 for training and evaluation. Qualitative,\nquantitative and subjective evaluation results on dance-music alignment and\nspeech-lip alignment demonstrate that our method far outperforms the\nstate-of-the-art methods. Project video and code are available at\nhttps://jianrenw.github.io/AlignNet.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 16:19:28 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Wang", "Jianren", ""], ["Fang", "Zhaoyuan", ""], ["Zhao", "Hang", ""]]}, {"id": "2002.05104", "submitter": "Camila Kolling", "authors": "Camila Kolling, J\\^onatas Wehrmann, and Rodrigo C. Barros", "title": "Component Analysis for Visual Question Answering Architectures", "comments": null, "journal-ref": "2020 - The International Joint Conference on Neural Networks\n  (IJCNN)", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research advances in Computer Vision and Natural Language Processing\nhave introduced novel tasks that are paving the way for solving AI-complete\nproblems. One of those tasks is called Visual Question Answering (VQA). A VQA\nsystem must take an image and a free-form, open-ended natural language question\nabout the image, and produce a natural language answer as the output. Such a\ntask has drawn great attention from the scientific community, which generated a\nplethora of approaches that aim to improve the VQA predictive accuracy. Most of\nthem comprise three major components: (i) independent representation learning\nof images and questions; (ii) feature fusion so the model can use information\nfrom both sources to answer visual questions; and (iii) the generation of the\ncorrect answer in natural language. With so many approaches being recently\nintroduced, it became unclear the real contribution of each component for the\nultimate performance of the model. The main goal of this paper is to provide a\ncomprehensive analysis regarding the impact of each component in VQA models.\nOur extensive set of experiments cover both visual and textual elements, as\nwell as the combination of these representations in form of fusion and\nattention mechanisms. Our major contribution is to identify core components for\ntraining VQA models so as to maximize their predictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 17:25:50 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 01:08:38 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Kolling", "Camila", ""], ["Wehrmann", "J\u00f4natas", ""], ["Barros", "Rodrigo C.", ""]]}, {"id": "2002.05107", "submitter": "Steven Frank", "authors": "Steven J. Frank and Andrea M. Frank", "title": "Analysis of Dutch Master Paintings with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trained on the works of an artist under study and visually comparable works\nof other artists, convolutional neural networks can identify forgeries and\nprovide attributions. They can also assign classification probabilities within\na painting, revealing mixed authorship and identifying regions painted by\ndifferent hands.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 17:32:18 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 16:41:08 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 13:29:19 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Frank", "Steven J.", ""], ["Frank", "Andrea M.", ""]]}, {"id": "2002.05123", "submitter": "Roi Pony", "authors": "Roi Pony, Itay Naeh, Shie Mannor", "title": "Over-the-Air Adversarial Flickering Attacks against Video Recognition\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks for video classification, just like image classification\nnetworks, may be subjected to adversarial manipulation. The main difference\nbetween image classifiers and video classifiers is that the latter usually use\ntemporal information contained within the video. In this work we present a\nmanipulation scheme for fooling video classifiers by introducing a flickering\ntemporal perturbation that in some cases may be unnoticeable by human observers\nand is implementable in the real world. After demonstrating the manipulation of\naction classification of single videos, we generalize the procedure to make\nuniversal adversarial perturbation, achieving high fooling ratio. In addition,\nwe generalize the universal perturbation and produce a temporal-invariant\nperturbation, which can be applied to the video without synchronizing the\nperturbation to the input. The attack was implemented on several target models\nand the transferability of the attack was demonstrated. These properties allow\nus to bridge the gap between simulated environment and real-world application,\nas will be demonstrated in this paper for the first time for an over-the-air\nflickering attack.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 17:58:12 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 10:17:19 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 14:39:25 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 22:11:54 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Pony", "Roi", ""], ["Naeh", "Itay", ""], ["Mannor", "Shie", ""]]}, {"id": "2002.05158", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Elizabeth Munch, Paul Rosen", "title": "Fast and Scalable Complex Network Descriptor Using PageRank and\n  Persistent Homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PageRank of a graph is a scalar function defined on the node set of the\ngraph which encodes nodes centrality information of the graph. In this article,\nwe use the PageRank function along with persistent homology to obtain a\nscalable graph descriptor and utilize it to compare the similarities between\ngraphs. For a given graph $G(V,E)$, our descriptor can be computed in\n$O(|E|\\alpha(|V|))$, where $\\alpha$ is the inverse Ackermann function which\nmakes it scalable and computable on massive graphs. We show the effectiveness\nof our method by utilizing it on multiple shape mesh datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 05:08:48 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 03:33:20 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Hajij", "Mustafa", ""], ["Munch", "Elizabeth", ""], ["Rosen", "Paul", ""]]}, {"id": "2002.05235", "submitter": "Bowen Li", "authors": "Bowen Li, Xiaojuan Qi, Philip H. S. Torr, Thomas Lukasiewicz", "title": "Image-to-Image Translation with Text Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to embed controllable factors, i.e., natural\nlanguage descriptions, into image-to-image translation with generative\nadversarial networks, which allows text descriptions to determine the visual\nattributes of synthetic images. We propose four key components: (1) the\nimplementation of part-of-speech tagging to filter out non-semantic words in\nthe given description, (2) the adoption of an affine combination module to\neffectively fuse different modality text and image features, (3) a novel\nrefined multi-stage architecture to strengthen the differential ability of\ndiscriminators and the rectification ability of generators, and (4) a new\nstructure loss to further improve discriminators to better distinguish real and\nsynthetic images. Extensive experiments on the COCO dataset demonstrate that\nour method has a superior performance on both visual realism and semantic\nconsistency with given descriptions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 21:09:15 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Li", "Bowen", ""], ["Qi", "Xiaojuan", ""], ["Torr", "Philip H. S.", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2002.05242", "submitter": "Nataniel Ruiz", "authors": "Nataniel Ruiz, Mona Jalal, Vitaly Ablavsky, Danielle Allessio, John\n  Magee, Jacob Whitehill, Ivon Arroyo, Beverly Woolf, Stan Sclaroff, Margrit\n  Betke", "title": "Leveraging Affect Transfer Learning for Behavior Prediction in an\n  Intelligent Tutoring System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of building an intelligent tutoring system (ITS), which\nimproves student learning outcomes by intervention, we set out to improve\nprediction of student problem outcome. In essence, we want to predict the\noutcome of a student answering a problem in an ITS from a video feed by\nanalyzing their face and gestures. For this, we present a novel transfer\nlearning facial affect representation and a user-personalized training scheme\nthat unlocks the potential of this representation. We model the temporal\nstructure of video sequences of students solving math problems using a\nrecurrent neural network architecture. Additionally, we extend the largest\ndataset of student interactions with an intelligent online math tutor by a\nfactor of two. Our final model, coined ATL-BP (Affect Transfer Learning for\nBehavior Prediction) achieves an increase in mean F-score over state-of-the-art\nof 45% on this new dataset in the general case and 50% in a more challenging\nleave-users-out experimental setting when we use a user-personalized training\nscheme.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 21:30:34 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Ruiz", "Nataniel", ""], ["Jalal", "Mona", ""], ["Ablavsky", "Vitaly", ""], ["Allessio", "Danielle", ""], ["Magee", "John", ""], ["Whitehill", "Jacob", ""], ["Arroyo", "Ivon", ""], ["Woolf", "Beverly", ""], ["Sclaroff", "Stan", ""], ["Betke", "Margrit", ""]]}, {"id": "2002.05271", "submitter": "Min Chen", "authors": "Qianwen Wang, William Alexander, Jack Pegg, Huamin Qu, and Min Chen", "title": "HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine\n  Learning Models", "comments": "This article was submitted to EuroVis 2020 on 5 December 2020. It was\n  not accepted. Because the reviews have not identified any technical problems\n  that would undermine the novelty and validity of this work, we think that the\n  article is ready to be released as an arXiv report. The EuroVis 2020 reviews\n  and authors' short feedback can be found in the anc folder", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, Feb.\n  2021", "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a visual analytics tool for enabling\nhypothesis-based evaluation of machine learning (ML) models. We describe a\nnovel ML-testing framework that combines the traditional statistical hypothesis\ntesting (commonly used in empirical research) with logical reasoning about the\nconclusions of multiple hypotheses. The framework defines a controlled\nconfiguration for testing a number of hypotheses as to whether and how some\nextra information about a \"concept\" or \"feature\" may benefit or hinder a ML\nmodel. Because reasoning multiple hypotheses is not always straightforward, we\nprovide HypoML as a visual analysis tool, with which, the multi-thread testing\ndata is transformed to a visual representation for rapid observation of the\nconclusions and the logical flow between the testing data and hypotheses.We\nhave applied HypoML to a number of hypothesized concepts, demonstrating the\nintuitive and explainable nature of the visual analysis.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 23:03:44 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Wang", "Qianwen", ""], ["Alexander", "William", ""], ["Pegg", "Jack", ""], ["Qu", "Huamin", ""], ["Chen", "Min", ""]]}, {"id": "2002.05274", "submitter": "Han Zhang Mr", "authors": "Han Zhang, Fangyi Chen, Zhiqiang Shen, Qiqi Hao, Chenchen Zhu, Marios\n  Savvides", "title": "Solving Missing-Annotation Object Detection with Background\n  Recalibration Loss", "comments": "5 pages. Paper has been accepted by ICASSP 2020 for presentation in a\n  lecture (oral) session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a novel and challenging detection scenario: A majority\nof true objects/instances is unlabeled in the datasets, so these\nmissing-labeled areas will be regarded as the background during training.\nPrevious art on this problem has proposed to use soft sampling to re-weight the\ngradients of RoIs based on the overlaps with positive instances, while their\nmethod is mainly based on the two-stage detector (i.e. Faster RCNN) which is\nmore robust and friendly for the missing label scenario. In this paper, we\nintroduce a superior solution called Background Recalibration Loss (BRL) that\ncan automatically re-calibrate the loss signals according to the pre-defined\nIoU threshold and input image. Our design is built on the one-stage detector\nwhich is faster and lighter. Inspired by the Focal Loss formulation, we make\nseveral significant modifications to fit on the missing-annotation\ncircumstance. We conduct extensive experiments on the curated PASCAL VOC and MS\nCOCO datasets. The results demonstrate that our proposed method outperforms the\nbaseline and other state-of-the-arts by a large margin. Code available:\nhttps://github.com/Dwrety/mmdetection-selective-iou.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 23:11:46 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 19:21:26 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhang", "Han", ""], ["Chen", "Fangyi", ""], ["Shen", "Zhiqiang", ""], ["Hao", "Qiqi", ""], ["Zhu", "Chenchen", ""], ["Savvides", "Marios", ""]]}, {"id": "2002.05283", "submitter": "Xiangning Chen", "authors": "Xiangning Chen, Cho-Jui Hsieh", "title": "Stabilizing Differentiable Architecture Search via Perturbation-based\n  Regularization", "comments": "ICML 2020, code is available at\n  https://github.com/xiangning-chen/SmoothDARTS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable architecture search (DARTS) is a prevailing NAS solution to\nidentify architectures. Based on the continuous relaxation of the architecture\nspace, DARTS learns a differentiable architecture weight and largely reduces\nthe search cost. However, its stability has been challenged for yielding\ndeteriorating architectures as the search proceeds. We find that the\nprecipitous validation loss landscape, which leads to a dramatic performance\ndrop when distilling the final architecture, is an essential factor that causes\ninstability. Based on this observation, we propose a perturbation-based\nregularization - SmoothDARTS (SDARTS), to smooth the loss landscape and improve\nthe generalizability of DARTS-based methods. In particular, our new\nformulations stabilize DARTS-based methods by either random smoothing or\nadversarial attack. The search trajectory on NAS-Bench-1Shot1 demonstrates the\neffectiveness of our approach and due to the improved stability, we achieve\nperformance gain across various search spaces on 4 datasets. Furthermore, we\nmathematically show that SDARTS implicitly regularizes the Hessian norm of the\nvalidation loss, which accounts for a smoother loss landscape and improved\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 23:46:58 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 21:56:42 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 19:17:24 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chen", "Xiangning", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2002.05287", "submitter": "Bingzhe Wei", "authors": "Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, Bo Yang", "title": "Geom-GCN: Geometric Graph Convolutional Networks", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Message-passing neural networks (MPNNs) have been successfully applied to\nrepresentation learning on graphs in a variety of real-world applications.\nHowever, two fundamental weaknesses of MPNNs' aggregators limit their ability\nto represent graph-structured data: losing the structural information of nodes\nin neighborhoods and lacking the ability to capture long-range dependencies in\ndisassortative graphs. Few studies have noticed the weaknesses from different\nperspectives. From the observations on classical neural network and network\ngeometry, we propose a novel geometric aggregation scheme for graph neural\nnetworks to overcome the two weaknesses. The behind basic idea is the\naggregation on a graph can benefit from a continuous space underlying the\ngraph. The proposed aggregation scheme is permutation-invariant and consists of\nthree modules, node embedding, structural neighborhood, and bi-level\naggregation. We also present an implementation of the scheme in graph\nconvolutional networks, termed Geom-GCN (Geometric Graph Convolutional\nNetworks), to perform transductive learning on graphs. Experimental results\nshow the proposed Geom-GCN achieved state-of-the-art performance on a wide\nrange of open datasets of graphs. Code is available at\nhttps://github.com/graphdml-uiuc-jlu/geom-gcn.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 00:03:09 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 01:47:35 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Pei", "Hongbin", ""], ["Wei", "Bingzhe", ""], ["Chang", "Kevin Chen-Chuan", ""], ["Lei", "Yu", ""], ["Yang", "Bo", ""]]}, {"id": "2002.05293", "submitter": "Meng Li", "authors": "Meng Li and Yilei Li and Pierce Chuang and Liangzhen Lai and Vikas\n  Chandra", "title": "Improving Efficiency in Neural Network Accelerator Using Operands\n  Hamming Distance optimization", "comments": "12 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network accelerator is a key enabler for the on-device AI inference,\nfor which energy efficiency is an important metric. The data-path energy,\nincluding the computation energy and the data movement energy among the\narithmetic units, claims a significant part of the total accelerator energy. By\nrevisiting the basic physics of the arithmetic logic circuits, we show that the\ndata-path energy is highly correlated with the bit flips when streaming the\ninput operands into the arithmetic units, defined as the hamming distance of\nthe input operand matrices. Based on the insight, we propose a post-training\noptimization algorithm and a hamming-distance-aware training algorithm to\nco-design and co-optimize the accelerator and the network synergistically. The\nexperimental results based on post-layout simulation with MobileNetV2\ndemonstrate on average 2.85X data-path energy reduction and up to 8.51X\ndata-path energy reduction for certain layers.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 00:36:36 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Li", "Meng", ""], ["Li", "Yilei", ""], ["Chuang", "Pierce", ""], ["Lai", "Liangzhen", ""], ["Chandra", "Vikas", ""]]}, {"id": "2002.05299", "submitter": "Tyler Maunu", "authors": "Tyler Maunu and Gilad Lerman", "title": "Depth Descent Synchronization in $\\mathrm{SO}(D)$", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give robust recovery results for synchronization on the rotation group,\n$\\mathrm{SO}(D)$. In particular, we consider an adversarial corruption setting,\nwhere a limited percentage of the observations are arbitrarily corrupted. We\ngive a novel algorithm that exploits Tukey depth in the tangent space, which\nexactly recovers the underlying rotations up to an outlier percentage of\n$1/(D(D-1)+2)$. This corresponds to an outlier fraction of $1/4$ for\n$\\mathrm{SO}(2)$ and $1/8$ for $\\mathrm{SO}(3)$. In the case of $D=2$, we\ndemonstrate that a variant of this algorithm converges linearly to the ground\ntruth rotations. We finish by discussing this result in relation to a simpler\nnonconvex energy minimization framework based on least absolute deviations,\nwhich exhibits spurious fixed points.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 01:01:17 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 13:19:50 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Maunu", "Tyler", ""], ["Lerman", "Gilad", ""]]}, {"id": "2002.05316", "submitter": "Hongwei Yi", "authors": "Hongwei Yi, Shaoshuai Shi, Mingyu Ding, Jiankai Sun, Kui Xu, Hui Zhou,\n  Zhe Wang, Sheng Li, Guoping Wang", "title": "SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D\n  Vehicle Detection from Point Cloud", "comments": "Accepted by ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D vehicle detection based on point cloud is a challenging task in real-world\napplications such as autonomous driving. Despite significant progress has been\nmade, we observe two aspects to be further improved. First, the semantic\ncontext information in LiDAR is seldom explored in previous works, which may\nhelp identify ambiguous vehicles. Second, the distribution of point cloud on\nvehicles varies continuously with increasing depths, which may not be well\nmodeled by a single model. In this work, we propose a unified model SegVoxelNet\nto address the above two problems. A semantic context encoder is proposed to\nleverage the free-of-charge semantic segmentation masks in the bird's eye view.\nSuspicious regions could be highlighted while noisy regions are suppressed by\nthis module. To better deal with vehicles at different depths, a novel\ndepth-aware head is designed to explicitly model the distribution differences\nand each part of the depth-aware head is made to focus on its own target\ndetection range. Extensive experiments on the KITTI dataset show that the\nproposed method outperforms the state-of-the-art alternatives in both accuracy\nand efficiency with point cloud as input only.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 02:42:31 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Yi", "Hongwei", ""], ["Shi", "Shaoshuai", ""], ["Ding", "Mingyu", ""], ["Sun", "Jiankai", ""], ["Xu", "Kui", ""], ["Zhou", "Hui", ""], ["Wang", "Zhe", ""], ["Li", "Sheng", ""], ["Wang", "Guoping", ""]]}, {"id": "2002.05322", "submitter": "Ying Da Wang", "authors": "Ying Da Wang, Mehdi Shabaninejad, Ryan T. Armstrong, Peyman Mostaghimi", "title": "Physical Accuracy of Deep Neural Networks for 2D and 3D Multi-Mineral\n  Segmentation of Rock micro-CT Images", "comments": "16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of 3D micro-Computed Tomographic uCT) images of rock samples is\nessential for further Digital Rock Physics (DRP) analysis, however,\nconventional methods such as thresholding, watershed segmentation, and\nconverging active contours are susceptible to user-bias. Deep Convolutional\nNeural Networks (CNNs) have produced accurate pixelwise semantic segmentation\nresults with natural images and $\\mu$CT rock images, however, physical accuracy\nis not well documented. The performance of 4 CNN architectures is tested for 2D\nand 3D cases in 10 configurations. Manually segmented uCT images of Mt. Simon\nSandstone are treated as ground truth and used as training and validation data,\nwith a high voxelwise accuracy (over 99%) achieved. Downstream analysis is then\nused to validate physical accuracy. The topology of each segmented phase is\ncalculated, and the absolute permeability and multiphase flow is modelled with\ndirect simulation in single and mixed wetting cases. These physical measures of\nconnectivity, and flow characteristics show high variance and uncertainty, with\nmodels that achieve 95\\%+ in voxelwise accuracy possessing permeabilities and\nconnectivities orders of magnitude off. A new network architecture is also\nintroduced as a hybrid fusion of U-net and ResNet, combining short and long\nskip connections in a Network-in-Network configuration. The 3D implementation\noutperforms all other tested models in voxelwise and physical accuracy\nmeasures. The network architecture and the volume fraction in the dataset (and\nassociated weighting), are factors that not only influence the accuracy\ntrade-off in the voxelwise case, but is especially important in training a\nphysically accurate model for segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 03:14:17 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 11:14:47 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Da Wang", "Ying", ""], ["Shabaninejad", "Mehdi", ""], ["Armstrong", "Ryan T.", ""], ["Mostaghimi", "Peyman", ""]]}, {"id": "2002.05333", "submitter": "Xiaodong Liu", "authors": "Xiaodong Liu, Zhi Gao, and Ben M. Chen", "title": "MLFcGAN: Multi-level Feature Fusion based Conditional GAN for Underwater\n  Image Color Correction", "comments": "This paper has already been accepted to journal IEEE geoscience and\n  remote sensing letters", "journal-ref": null, "doi": "10.1109/LGRS.2019.2950056", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color correction for underwater images has received increasing interests, due\nto its critical role in facilitating available mature vision algorithms for\nunderwater scenarios. Inspired by the stunning success of deep convolutional\nneural networks (DCNNs) techniques in many vision tasks, especially the\nstrength in extracting features in multiple scales, we propose a deep\nmulti-scale feature fusion net based on the conditional generative adversarial\nnetwork (GAN) for underwater image color correction. In our network,\nmulti-scale features are extracted first, followed by augmenting local features\non each scale with global features. This design was verified to facilitate more\neffective and faster network learning, resulting in better performance in both\ncolor correction and detail preservation. We conducted extensive experiments\nand compared with the state-of-the-art approaches quantitatively and\nqualitatively, showing that our method achieves significant improvements.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 04:15:10 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Liu", "Xiaodong", ""], ["Gao", "Zhi", ""], ["Chen", "Ben M.", ""]]}, {"id": "2002.05347", "submitter": "Xialei Liu", "authors": "Xialei Liu, Hao Yang, Avinash Ravichandran, Rahul Bhotika, Stefano\n  Soatto", "title": "Multi-Task Incremental Learning for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-task learns multiple tasks, while sharing knowledge and computation\namong them. However, it suffers from catastrophic forgetting of previous\nknowledge when learned incrementally without access to the old data. Most\nexisting object detectors are domain-specific and static, while some are\nlearned incrementally but only within a single domain. Training an object\ndetector incrementally across various domains has rarely been explored. In this\nwork, we propose three incremental learning scenarios across various domains\nand categories for object detection. To mitigate catastrophic forgetting,\nattentive feature distillation is proposed to leverages both bottom-up and\ntop-down attentions to extract important information for distillation. We then\nsystematically analyze the proposed distillation method in different scenarios.\nWe find out that, contrary to common understanding, domain gaps have smaller\nnegative impact on incremental detection, while category differences are\nproblematic. For the difficult cases, where the domain gaps and especially\ncategory differences are large, we explore three different exemplar sampling\nmethods and show the proposed adaptive sampling method is effective to select\ndiverse and informative samples from entire datasets, to further prevent\nforgetting. Experimental results show that we achieve the significant\nimprovement in three different scenarios across seven object detection\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 04:58:37 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 09:47:31 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 20:31:18 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Liu", "Xialei", ""], ["Yang", "Hao", ""], ["Ravichandran", "Avinash", ""], ["Bhotika", "Rahul", ""], ["Soatto", "Stefano", ""]]}, {"id": "2002.05349", "submitter": "Suya You", "authors": "Zifan Yu and Suya You", "title": "Object Detection on Single Monocular Images through Canonical\n  Correlation Analysis", "comments": "17 pages; Research report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without using extra 3-D data like points cloud or depth images for providing\n3-D information, we retrieve the 3-D object information from single monocular\nimages. The high-quality predicted depth images are recovered from single\nmonocular images, and it is fed into the 2-D object proposal network with\ncorresponding monocular images. Most existing deep learning frameworks with\ntwo-streams input data always fuse separate data by concatenating or adding,\nwhich views every part of a feature map can contribute equally to the whole\ntask. However, when data are noisy, and too much information is redundant,\nthese methods no longer produce predictions or classifications efficiently. In\nthis report, we propose a two-dimensional CCA(canonical correlation analysis)\nframework to fuse monocular images and corresponding predicted depth images for\nbasic computer vision tasks like image classification and object detection.\nFirstly, we implemented different structures with one-dimensional CCA and\nAlexnet to test the performance on the image classification task. And then, we\napplied one of these structures with 2D-CCA for object detection. During these\nexperiments, we found that our proposed framework behaves better when taking\npredicted depth images as inputs with the model trained from ground truth\ndepth.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 05:03:42 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Yu", "Zifan", ""], ["You", "Suya", ""]]}, {"id": "2002.05350", "submitter": "Shuyuan Lin", "authors": "Shuyuan Lin, Guobao Xiao, Yan Yan, David Suter, Hanzi Wang", "title": "Hypergraph Optimization for Multi-structural Geometric Model Fitting", "comments": null, "journal-ref": null, "doi": "10.1609/aaai.v33i01.33018730", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, some hypergraph-based methods have been proposed to deal with the\nproblem of model fitting in computer vision, mainly due to the superior\ncapability of hypergraph to represent the complex relationship between data\npoints. However, a hypergraph becomes extremely complicated when the input data\ninclude a large number of data points (usually contaminated with noises and\noutliers), which will significantly increase the computational burden. In order\nto overcome the above problem, we propose a novel hypergraph optimization based\nmodel fitting (HOMF) method to construct a simple but effective hypergraph.\nSpecifically, HOMF includes two main parts: an adaptive inlier estimation\nalgorithm for vertex optimization and an iterative hyperedge optimization\nalgorithm for hyperedge optimization. The proposed method is highly efficient,\nand it can obtain accurate model fitting results within a few iterations.\nMoreover, HOMF can then directly apply spectral clustering, to achieve good\nfitting performance. Extensive experimental results show that HOMF outperforms\nseveral state-of-the-art model fitting methods on both synthetic data and real\nimages, especially in sampling efficiency and in handling data with severe\noutliers.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 05:07:11 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Lin", "Shuyuan", ""], ["Xiao", "Guobao", ""], ["Yan", "Yan", ""], ["Suter", "David", ""], ["Wang", "Hanzi", ""]]}, {"id": "2002.05388", "submitter": "Taro Kiritani", "authors": "Taro Kiritani, Koji Ono", "title": "Recurrent Attention Model with Log-Polar Mapping is Robust against\n  Adversarial Attacks", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are vulnerable to small $\\ell^p$ adversarial\nattacks, while the human visual system is not. Inspired by neural networks in\nthe eye and the brain, we developed a novel artificial neural network model\nthat recurrently collects data with a log-polar field of view that is\ncontrolled by attention. We demonstrate the effectiveness of this design as a\ndefense against SPSA and PGD adversarial attacks. It also has beneficial\nproperties observed in the animal visual system, such as reflex-like pathways\nfor low-latency inference, fixed amount of computation independent of image\nsize, and rotation and scale invariance. The code for experiments is available\nat https://gitlab.com/exwzd-public/kiritani_ono_2020.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 08:40:48 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Kiritani", "Taro", ""], ["Ono", "Koji", ""]]}, {"id": "2002.05447", "submitter": "Hanyu Liu", "authors": "Hanyu Liu, Jiabei Zeng, Shiguang Shan and Xilin Chen", "title": "Emotion Recognition for In-the-wild Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a brief introduction to our submission to the seven basic\nexpression classification track of Affective Behavior Analysis in-the-wild\nCompetition held in conjunction with the IEEE International Conference on\nAutomatic Face and Gesture Recognition (FG) 2020. Our method combines Deep\nResidual Network (ResNet) and Bidirectional Long Short-Term Memory Network\n(BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 11:29:46 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Liu", "Hanyu", ""], ["Zeng", "Jiabei", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2002.05459", "submitter": "Abdulkadir Gokce", "authors": "Yasin Almalioglu, Kutsev Bengisu Ozyoruk, Abdulkadir Gokce, Kagan\n  Incetan, Guliz Irem Gokceler, Muhammed Ali Simsek, Kivanc Ararat, Richard J.\n  Chen, Nicholas J. Durr, Faisal Mahmood, Mehmet Turan", "title": "EndoL2H: Deep Super-Resolution for Capsule Endoscopy", "comments": "23 pages, submitted to IEEE Transactions on Medical Imaging,\n  corresponding Author: Mehmet Turan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although wireless capsule endoscopy is the preferred modality for diagnosis\nand assessment of small bowel diseases, the poor camera resolution is a\nsubstantial limitation for both subjective and automated diagnostics.\nEnhanced-resolution endoscopy has shown to improve adenoma detection rate for\nconventional endoscopy and is likely to do the same for capsule endoscopy. In\nthis work, we propose and quantitatively validate a novel framework to learn a\nmapping from low-to-high resolution endoscopic images. We combine conditional\nadversarial networks with a spatial attention block to improve the resolution\nby up to factors of 8x, 10x, 12x, respectively. Quantitative and qualitative\nstudies performed demonstrate the superiority of EndoL2H over state-of-the-art\ndeep super-resolution methods DBPN, RCAN and SRGAN. MOS tests performed by 30\ngastroenterologists qualitatively assess and confirm the clinical relevance of\nthe approach. EndoL2H is generally applicable to any endoscopic capsule system\nand has the potential to improve diagnosis and better harness computational\napproaches for polyp detection and characterization. Our code and trained\nmodels are available at https://github.com/CapsuleEndoscope/EndoL2H.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 11:52:47 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 19:18:33 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Almalioglu", "Yasin", ""], ["Ozyoruk", "Kutsev Bengisu", ""], ["Gokce", "Abdulkadir", ""], ["Incetan", "Kagan", ""], ["Gokceler", "Guliz Irem", ""], ["Simsek", "Muhammed Ali", ""], ["Ararat", "Kivanc", ""], ["Chen", "Richard J.", ""], ["Durr", "Nicholas J.", ""], ["Mahmood", "Faisal", ""], ["Turan", "Mehmet", ""]]}, {"id": "2002.05493", "submitter": "Fabricio Breve", "authors": "Fabricio A Breve, Marcos G Quiles, Liang Zhao, and Elbert E. N. Macau", "title": "Chaotic Phase Synchronization and Desynchronization in an Oscillator\n  Network for Object Selection", "comments": null, "journal-ref": "BREVE, FA; ZHAO, L; QUILES, MG; MACAU, EEN. Chaotic Phase\n  Synchronization and Desynchronization in an Oscillator Network for Object\n  Selection. Neural Networks, v. 22, p. 728-737, 2009", "doi": "10.1016/j.neunet.2009.06.027", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object selection refers to the mechanism of extracting objects of interest\nwhile ignoring other objects and background in a given visual scene. It is a\nfundamental issue for many computer vision and image analysis techniques and it\nis still a challenging task to artificial visual systems. Chaotic phase\nsynchronization takes place in cases involving almost identical dynamical\nsystems and it means that the phase difference between the systems is kept\nbounded over the time, while their amplitudes remain chaotic and may be\nuncorrelated. Instead of complete synchronization, phase synchronization is\nbelieved to be a mechanism for neural integration in brain. In this paper, an\nobject selection model is proposed. Oscillators in the network representing the\nsalient object in a given scene are phase synchronized, while no phase\nsynchronization occurs for background objects. In this way, the salient object\ncan be extracted. In this model, a shift mechanism is also introduced to change\nattention from one object to another. Computer simulations show that the model\nproduces some results similar to those observed in natural vision systems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 13:39:25 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Breve", "Fabricio A", ""], ["Quiles", "Marcos G", ""], ["Zhao", "Liang", ""], ["Macau", "Elbert E. N.", ""]]}, {"id": "2002.05509", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Luc Van Gool, Radu Timofte", "title": "Replacing Mobile Camera ISP with a Single Deep Learning Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the popularity of mobile photography is growing constantly, lots of\nefforts are being invested now into building complex hand-crafted camera ISP\nsolutions. In this work, we demonstrate that even the most sophisticated ISP\npipelines can be replaced with a single end-to-end deep learning model trained\nwithout any prior knowledge about the sensor and optics used in a particular\ndevice. For this, we present PyNET, a novel pyramidal CNN architecture designed\nfor fine-grained image restoration that implicitly learns to perform all ISP\nsteps such as image demosaicing, denoising, white balancing, color and contrast\ncorrection, demoireing, etc. The model is trained to convert RAW Bayer data\nobtained directly from mobile camera sensor into photos captured with a\nprofessional high-end DSLR camera, making the solution independent of any\nparticular mobile ISP implementation. To validate the proposed approach on the\nreal data, we collected a large-scale dataset consisting of 10 thousand\nfull-resolution RAW-RGB image pairs captured in the wild with the Huawei P20\ncameraphone (12.3 MP Sony Exmor IMX380 sensor) and Canon 5D Mark IV DSLR. The\nexperiments demonstrate that the proposed solution can easily get to the level\nof the embedded P20's ISP pipeline that, unlike our approach, is combining the\ndata from two (RGB + B/W) camera sensors. The dataset, pre-trained models and\ncodes used in this paper are available on the project website.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 14:22:39 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Ignatov", "Andrey", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2002.05512", "submitter": "Yuanbo Xiangli", "authors": "Yuanbo Xiangli, Yubin Deng, Bo Dai, Chen Change Loy, Dahua Lin", "title": "Real or Not Real, that is the Question", "comments": "ICLR2020 spotlight. 1) train GAN by maximizing kl-divergence. 2)\n  train non-progressive GAN (DCGAN) architecture at 1024*1024 resolution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While generative adversarial networks (GAN) have been widely adopted in\nvarious topics, in this paper we generalize the standard GAN to a new\nperspective by treating realness as a random variable that can be estimated\nfrom multiple angles. In this generalized framework, referred to as\nRealnessGAN, the discriminator outputs a distribution as the measure of\nrealness. While RealnessGAN shares similar theoretical guarantees with the\nstandard GAN, it provides more insights on adversarial learning. Compared to\nmultiple baselines, RealnessGAN provides stronger guidance for the generator,\nachieving improvements on both synthetic and real-world datasets. Moreover, it\nenables the basic DCGAN architecture to generate realistic images at 1024*1024\nresolution when trained from scratch.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 18:41:55 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Xiangli", "Yuanbo", ""], ["Deng", "Yubin", ""], ["Dai", "Bo", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "2002.05534", "submitter": "Menghan Hu", "authors": "Yunlu Wang, Menghan Hu, Qingli Li, Xiao-Ping Zhang, Guangtao Zhai, Nan\n  Yao", "title": "Abnormal respiratory patterns classifier may contribute to large-scale\n  screening of people infected with COVID-19 in an accurate and unobtrusive\n  manner", "comments": "6 page, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research significance: The extended version of this paper has been accepted\nby IEEE Internet of Things journal (DOI: 10.1109/JIOT.2020.2991456), please\ncite the journal version. During the epidemic prevention and control period,\nour study can be helpful in prognosis, diagnosis and screening for the patients\ninfected with COVID-19 (the novel coronavirus) based on breathing\ncharacteristics. According to the latest clinical research, the respiratory\npattern of COVID-19 is different from the respiratory patterns of flu and the\ncommon cold. One significant symptom that occurs in the COVID-19 is Tachypnea.\nPeople infected with COVID-19 have more rapid respiration. Our study can be\nutilized to distinguish various respiratory patterns and our device can be\npreliminarily put to practical use. Demo videos of this method working in\nsituations of one subject and two subjects can be downloaded online. Research\ndetails: Accurate detection of the unexpected abnormal respiratory pattern of\npeople in a remote and unobtrusive manner has great significance. In this work,\nwe innovatively capitalize on depth camera and deep learning to achieve this\ngoal. The challenges in this task are twofold: the amount of real-world data is\nnot enough for training to get the deep model; and the intra-class variation of\ndifferent types of respiratory patterns is large and the outer-class variation\nis small. In this paper, considering the characteristics of actual respiratory\nsignals, a novel and efficient Respiratory Simulation Model (RSM) is first\nproposed to fill the gap between the large amount of training data and scarce\nreal-world data. The proposed deep model and the modeling ideas have the great\npotential to be extended to large scale applications such as public places,\nsleep scenario, and office environment.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 09:42:57 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 03:57:55 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wang", "Yunlu", ""], ["Hu", "Menghan", ""], ["Li", "Qingli", ""], ["Zhang", "Xiao-Ping", ""], ["Zhai", "Guangtao", ""], ["Yao", "Nan", ""]]}, {"id": "2002.05536", "submitter": "Yan Li", "authors": "Yang Li, Yan Li, Hua Tian", "title": "Deep Learning-based End-to-end Diagnosis System for Avascular Necrosis\n  of Femoral Head", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": "10.1109/JBHI.2020.3037079", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the first diagnostic imaging modality of avascular necrosis of the femoral\nhead (AVNFH), accurately staging AVNFH from a plain radiograph is critical yet\nchallenging for orthopedists. Thus, we propose a deep learning-based AVNFH\ndiagnosis system (AVN-net). The proposed AVN-net reads plain radiographs of the\npelvis, conducts diagnosis, and visualizes results automatically. Deep\nconvolutional neural networks are trained to provide an end-to-end diagnosis\nsolution, covering tasks of femoral head detection, exam-view identification,\nside classification, AVNFH diagnosis, and key clinical notes generation.\nAVN-net is able to obtain state-of-the-art testing AUC of 0.97 (95% CI:\n0.97-0.98) in AVNFH detection and significantly greater F1 scores than\nless-to-moderately experienced orthopedists in all diagnostic tests (p<0.01).\nFurthermore, two real-world pilot studies were conducted for diagnosis support\nand education assistance, respectively, to assess the utility of AVN-net. The\nexperimental results are promising. With the AVN-net diagnosis as a reference,\nthe diagnostic accuracy and consistency of all orthopedists considerably\nimproved while requiring only 1/4 of the time. Students self-studying the AVNFH\ndiagnosis using AVN-net can learn better and faster than the control group. To\nthe best of our knowledge, this study is the first research on the prospective\nuse of a deep learning-based diagnosis system for AVNFH by conducting two pilot\nstudies representing real-world application scenarios. We have demonstrated\nthat the proposed AVN-net achieves expert-level AVNFH diagnosis performance,\nprovides efficient support in clinical decision-making, and effectively passes\nclinical experience to students.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 05:55:50 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 14:06:56 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Li", "Yang", ""], ["Li", "Yan", ""], ["Tian", "Hua", ""]]}, {"id": "2002.05540", "submitter": "Hughes Perreault", "authors": "Hughes Perreault and Guillaume-Alexandre Bilodeau and Nicolas Saunier\n  and Maguelonne H\\'eritier", "title": "SpotNet: Self-Attention Multi-Task Network for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are very good at directing their visual attention toward relevant\nareas when they search for different types of objects. For instance, when we\nsearch for cars, we will look at the streets, not at the top of buildings. The\nmotivation of this paper is to train a network to do the same via a multi-task\nlearning approach. To train visual attention, we produce foreground/background\nsegmentation labels in a semi-supervised way, using background subtraction or\noptical flow. Using these labels, we train an object detection model to produce\nforeground/background segmentation maps as well as bounding boxes while sharing\nmost model parameters. We use those segmentation maps inside the network as a\nself-attention mechanism to weight the feature map used to produce the bounding\nboxes, decreasing the signal of non-relevant areas. We show that by using this\nmethod, we obtain a significant mAP improvement on two traffic surveillance\ndatasets, with state-of-the-art results on both UA-DETRAC and UAVDT.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 14:43:24 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 14:22:49 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Perreault", "Hughes", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Saunier", "Nicolas", ""], ["H\u00e9ritier", "Maguelonne", ""]]}, {"id": "2002.05544", "submitter": "Anderson Tavares", "authors": "Pedro H. C. Avelar, Anderson R. Tavares, Thiago L. T. da Silveira,\n  Cl\\'audio R. Jung, Lu\\'is C. Lamb", "title": "Superpixel Image Classification with Graph Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a methodology for image classification using Graph Neural\nNetwork (GNN) models. We transform the input images into region adjacency\ngraphs (RAGs), in which regions are superpixels and edges connect neighboring\nsuperpixels. Our experiments suggest that Graph Attention Networks (GATs),\nwhich combine graph convolutions with self-attention mechanisms, outperforms\nother GNN models. Although raw image classifiers perform better than GATs due\nto information loss during the RAG generation, our methodology opens an\ninteresting avenue of research on deep learning beyond rectangular-gridded\nimages, such as 360-degree field of view panoramas. Traditional convolutional\nkernels of current state-of-the-art methods cannot handle panoramas, whereas\nthe adapted superpixel algorithms and the resulting region adjacency graphs can\nnaturally feed a GNN, without topology issues.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 14:52:32 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 17:04:53 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Avelar", "Pedro H. C.", ""], ["Tavares", "Anderson R.", ""], ["da Silveira", "Thiago L. T.", ""], ["Jung", "Cl\u00e1udio R.", ""], ["Lamb", "Lu\u00eds C.", ""]]}, {"id": "2002.05556", "submitter": "Pedro Henrique Martins", "authors": "Pedro Henrique Martins, Vlad Niculae, Zita Marinho, Andr\\'e Martins", "title": "Sparse and Structured Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention mechanisms are widely used in multimodal tasks, as visual\nquestion answering (VQA). One drawback of softmax-based attention mechanisms is\nthat they assign some probability mass to all image regions, regardless of\ntheir adjacency structure and of their relevance to the text. In this paper, to\nbetter link the image structure with the text, we replace the traditional\nsoftmax attention mechanism with two alternative sparsity-promoting\ntransformations: sparsemax, which is able to select only the relevant regions\n(assigning zero weight to the rest), and a newly proposed Total-Variation\nSparse Attention (TVmax), which further encourages the joint selection of\nadjacent spatial locations. Experiments in VQA show gains in accuracy as well\nas higher similarity to human attention, which suggests better\ninterpretability.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 15:08:12 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 12:39:43 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Martins", "Pedro Henrique", ""], ["Niculae", "Vlad", ""], ["Marinho", "Zita", ""], ["Martins", "Andr\u00e9", ""]]}, {"id": "2002.05583", "submitter": "Haosheng Chen", "authors": "Haosheng Chen, Qiangqiang Wu, Yanjie Liang, Xinbo Gao, Hanzi Wang", "title": "Asynchronous Tracking-by-Detection on Adaptive Time Surfaces for\n  Event-based Object Tracking", "comments": "9 pages, 5 figures", "journal-ref": "Proceedings of the 27th ACM International Conference on Multimedia\n  (MM '19). 2019, Nice, France. ACM, New York, NY, USA", "doi": "10.1145/3343031.3350975", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras, which are asynchronous bio-inspired vision sensors, have shown\ngreat potential in a variety of situations, such as fast motion and low\nillumination scenes. However, most of the event-based object tracking methods\nare designed for scenarios with untextured objects and uncluttered backgrounds.\nThere are few event-based object tracking methods that support bounding\nbox-based object tracking. The main idea behind this work is to propose an\nasynchronous Event-based Tracking-by-Detection (ETD) method for generic\nbounding box-based object tracking. To achieve this goal, we present an\nAdaptive Time-Surface with Linear Time Decay (ATSLTD) event-to-frame conversion\nalgorithm, which asynchronously and effectively warps the spatio-temporal\ninformation of asynchronous retinal events to a sequence of ATSLTD frames with\nclear object contours. We feed the sequence of ATSLTD frames to the proposed\nETD method to perform accurate and efficient object tracking, which leverages\nthe high temporal resolution property of event cameras. We compare the proposed\nETD method with seven popular object tracking methods, that are based on\nconventional cameras or event cameras, and two variants of ETD. The\nexperimental results show the superiority of the proposed ETD method in\nhandling various challenging environments.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 15:58:31 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Chen", "Haosheng", ""], ["Wu", "Qiangqiang", ""], ["Liang", "Yanjie", ""], ["Gao", "Xinbo", ""], ["Wang", "Hanzi", ""]]}, {"id": "2002.05636", "submitter": "Ryan Steed", "authors": "Ryan Steed and Aylin Caliskan", "title": "A Set of Distinct Facial Traits Learned by Machines Is Not Predictive of\n  Appearance Bias in the Wild", "comments": "11 pages, 7 figures. Revision for AI Ethics", "journal-ref": "AI Ethics (2021)", "doi": "10.1007/s43681-020-00035-y", "report-no": null, "categories": "cs.CY cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Research in social psychology has shown that people's biased, subjective\njudgments about another's personality based solely on their appearance are not\npredictive of their actual personality traits. But researchers and companies\noften utilize computer vision models to predict similarly subjective\npersonality attributes such as \"employability.\" We seek to determine whether\nstate-of-the-art, black box face processing technology can learn human-like\nappearance biases. With features extracted with FaceNet, a widely used face\nrecognition framework, we train a transfer learning model on human subjects'\nfirst impressions of personality traits in other faces as measured by social\npsychologists. We find that features extracted with FaceNet can be used to\npredict human appearance bias scores for deliberately manipulated faces but not\nfor randomly generated faces scored by humans. Additionally, in contrast to\nwork with human biases in social psychology, the model does not find a\nsignificant signal correlating politicians' vote shares with perceived\ncompetence bias. With Local Interpretable Model-Agnostic Explanations (LIME),\nwe provide several explanations for this discrepancy. Our results suggest that\nsome signals of appearance bias documented in social psychology are not\nembedded by the machine learning techniques we investigate. We shed light on\nthe ways in which appearance bias could be embedded in face processing\ntechnology and cast further doubt on the practice of predicting subjective\ntraits based on appearances.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:09:27 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 12:39:57 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 17:15:05 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Steed", "Ryan", ""], ["Caliskan", "Aylin", ""]]}, {"id": "2002.05638", "submitter": "Samet Hicsonmez", "authors": "Samet Hicsonmez, Nermin Samet, Emre Akbas, Pinar Duygulu", "title": "GANILLA: Generative Adversarial Networks for Image to Illustration\n  Translation", "comments": "to be published in Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore illustrations in children's books as a new domain\nin unpaired image-to-image translation. We show that although the current\nstate-of-the-art image-to-image translation models successfully transfer either\nthe style or the content, they fail to transfer both at the same time. We\npropose a new generator network to address this issue and show that the\nresulting network strikes a better balance between style and content.\n  There are no well-defined or agreed-upon evaluation metrics for unpaired\nimage-to-image translation. So far, the success of image translation models has\nbeen based on subjective, qualitative visual comparison on a limited number of\nimages. To address this problem, we propose a new framework for the\nquantitative evaluation of image-to-illustration models, where both content and\nstyle are taken into account using separate classifiers. In this new evaluation\nframework, our proposed model performs better than the current state-of-the-art\nmodels on the illustrations dataset. Our code and pretrained models can be\nfound at https://github.com/giddyyupp/ganilla.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:12:09 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 09:46:35 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Hicsonmez", "Samet", ""], ["Samet", "Nermin", ""], ["Akbas", "Emre", ""], ["Duygulu", "Pinar", ""]]}, {"id": "2002.05654", "submitter": "S\\'ebastien Pi\\'erard", "authors": "S\\'ebastien Pi\\'erard and Marc Van Droogenbroeck", "title": "Summarizing the performances of a background subtraction algorithm\n  measured on several videos", "comments": "Copyright 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "ICIP 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist many background subtraction algorithms to detect motion in\nvideos. To help comparing them, datasets with ground-truth data such as CDNET\nor LASIESTA have been proposed. These datasets organize videos in categories\nthat represent typical challenges for background subtraction. The evaluation\nprocedure promoted by their authors consists in measuring performance\nindicators for each video separately and to average them hierarchically, within\na category first, then between categories, a procedure which we name\n\"summarization\". While the summarization by averaging performance indicators is\na valuable effort to standardize the evaluation procedure, it has no\ntheoretical justification and it breaks the intrinsic relationships between\nsummarized indicators. This leads to interpretation inconsistencies. In this\npaper, we present a theoretical approach to summarize the performances for\nmultiple videos that preserves the relationships between performance\nindicators. In addition, we give formulas and an algorithm to calculate\nsummarized performances. Finally, we showcase our observations on CDNET 2014.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:35:34 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 15:55:54 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Pi\u00e9rard", "S\u00e9bastien", ""], ["Van Droogenbroeck", "Marc", ""]]}, {"id": "2002.05665", "submitter": "Hatem Hajri", "authors": "Jeanine Harb, Nicolas R\\'eb\\'ena, Rapha\\\"el Chosidow, Gr\\'egoire\n  Roblin, Roman Potarusov and Hatem Hajri", "title": "FRSign: A Large-Scale Traffic Light Dataset for Autonomous Trains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the realm of autonomous transportation, there have been many initiatives\nfor open-sourcing self-driving cars datasets, but much less for alternative\nmethods of transportation such as trains. In this paper, we aim to bridge the\ngap by introducing FRSign, a large-scale and accurate dataset for vision-based\nrailway traffic light detection and recognition. Our recordings were made on\nselected running trains in France and benefited from carefully hand-labeled\nannotations. An illustrative dataset which corresponds to ten percent of the\nacquired data to date is published in open source with the paper. It contains\nmore than 100,000 images illustrating six types of French railway traffic\nlights and their possible color combinations, together with the relevant\ninformation regarding their acquisition such as date, time, sensor parameters,\nand bounding boxes. This dataset is published in open-source at the address\n\\url{https://frsign.irt-systemx.fr}. We compare, analyze various properties of\nthe dataset and provide metrics to express its variability. We also discuss\nspecific challenges and particularities related to autonomous trains in\ncomparison to autonomous cars.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 15:08:15 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Harb", "Jeanine", ""], ["R\u00e9b\u00e9na", "Nicolas", ""], ["Chosidow", "Rapha\u00ebl", ""], ["Roblin", "Gr\u00e9goire", ""], ["Potarusov", "Roman", ""], ["Hajri", "Hatem", ""]]}, {"id": "2002.05688", "submitter": "Gabriel Eilertsen", "authors": "Gabriel Eilertsen, Daniel J\\\"onsson, Timo Ropinski, Jonas Unger,\n  Anders Ynnerman", "title": "Classifying the classifier: dissecting the weight space of neural\n  networks", "comments": "ECAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an empirical study on the weights of neural networks,\nwhere we interpret each model as a point in a high-dimensional space -- the\nneural weight space. To explore the complex structure of this space, we sample\nfrom a diverse selection of training variations (dataset, optimization\nprocedure, architecture, etc.) of neural network classifiers, and train a large\nnumber of models to represent the weight space. Then, we use a machine learning\napproach for analyzing and extracting information from this space. Most\ncentrally, we train a number of novel deep meta-classifiers with the objective\nof classifying different properties of the training setup by identifying their\nfootprints in the weight space. Thus, the meta-classifiers probe for patterns\ninduced by hyper-parameters, so that we can quantify how much, where, and when\nthese are encoded through the optimization process. This provides a novel and\ncomplementary view for explainable AI, and we show how meta-classifiers can\nreveal a great deal of information about the training setup and optimization,\nby only considering a small subset of randomly selected consecutive weights. To\npromote further research on the weight space, we release the neural weight\nspace (NWS) dataset -- a collection of 320K weight snapshots from 16K\nindividually trained deep neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 18:12:02 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Eilertsen", "Gabriel", ""], ["J\u00f6nsson", "Daniel", ""], ["Ropinski", "Timo", ""], ["Unger", "Jonas", ""], ["Ynnerman", "Anders", ""]]}, {"id": "2002.05692", "submitter": "Petru-Daniel Tudosiu", "authors": "Petru-Daniel Tudosiu and Thomas Varsavsky and Richard Shaw and Mark\n  Graham and Parashkev Nachev and Sebastien Ourselin and Carole H. Sudre and M.\n  Jorge Cardoso", "title": "Neuromorphologicaly-preserving Volumetric data encoding using VQ-VAE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing efficiency and compactness of deep learning architectures,\ntogether with hardware improvements, have enabled the complex and\nhigh-dimensional modelling of medical volumetric data at higher resolutions.\nRecently, Vector-Quantised Variational Autoencoders (VQ-VAE) have been proposed\nas an efficient generative unsupervised learning approach that can encode\nimages to a small percentage of their initial size, while preserving their\ndecoded fidelity. Here, we show a VQ-VAE inspired network can efficiently\nencode a full-resolution 3D brain volume, compressing the data to $0.825\\%$ of\nthe original size while maintaining image fidelity, and significantly\noutperforming the previous state-of-the-art. We then demonstrate that VQ-VAE\ndecoded images preserve the morphological characteristics of the original data\nthrough voxel-based morphology and segmentation experiments. Lastly, we show\nthat such models can be pre-trained and then fine-tuned on different datasets\nwithout the introduction of bias.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 18:18:51 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Tudosiu", "Petru-Daniel", ""], ["Varsavsky", "Thomas", ""], ["Shaw", "Richard", ""], ["Graham", "Mark", ""], ["Nachev", "Parashkev", ""], ["Ourselin", "Sebastien", ""], ["Sudre", "Carole H.", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "2002.05702", "submitter": "Pietro Nardelli", "authors": "Pietro Nardelli, James C. Ross, Ra\\'ul San Jos\\'e Est\\'epar", "title": "Generative-based Airway and Vessel Morphology Quantification on Chest CT\n  Images", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": "10.1016/j.media.2020.101691", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately and precisely characterizing the morphology of small pulmonary\nstructures from Computed Tomography (CT) images, such as airways and vessels,\nis becoming of great importance for diagnosis of pulmonary diseases. The\nsmaller conducting airways are the major site of increased airflow resistance\nin chronic obstructive pulmonary disease (COPD), while accurately sizing\nvessels can help identify arterial and venous changes in lung regions that may\ndetermine future disorders. However, traditional methods are often limited due\nto image resolution and artifacts.\n  We propose a Convolutional Neural Regressor (CNR) that provides\ncross-sectional measurement of airway lumen, airway wall thickness, and vessel\nradius. CNR is trained with data created by a generative model of synthetic\nstructures which is used in combination with Simulated and Unsupervised\nGenerative Adversarial Network (SimGAN) to create simulated and refined airways\nand vessels with known ground-truth.\n  For validation, we first use synthetically generated airways and vessels\nproduced by the proposed generative model to compute the relative error and\ndirectly evaluate the accuracy of CNR in comparison with traditional methods.\nThen, in-vivo validation is performed by analyzing the association between the\npercentage of the predicted forced expiratory volume in one second (FEV1\\%) and\nthe value of the Pi10 parameter, two well-known measures of lung function and\nairway disease, for airways. For vessels, we assess the correlation between our\nestimate of the small-vessel blood volume and the lungs' diffusing capacity for\ncarbon monoxide (DLCO).\n  The results demonstrate that Convolutional Neural Networks (CNNs) provide a\npromising direction for accurately measuring vessels and airways on chest CT\nimages with physiological correlates.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 18:45:31 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 16:32:10 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Nardelli", "Pietro", ""], ["Ross", "James C.", ""], ["Est\u00e9par", "Ra\u00fal San Jos\u00e9", ""]]}, {"id": "2002.05709", "submitter": "Ting Chen", "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "comments": "ICML'2020. Code and pretrained models at\n  https://github.com/google-research/simclr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents SimCLR: a simple framework for contrastive learning of\nvisual representations. We simplify recently proposed contrastive\nself-supervised learning algorithms without requiring specialized architectures\nor a memory bank. In order to understand what enables the contrastive\nprediction tasks to learn useful representations, we systematically study the\nmajor components of our framework. We show that (1) composition of data\naugmentations plays a critical role in defining effective predictive tasks, (2)\nintroducing a learnable nonlinear transformation between the representation and\nthe contrastive loss substantially improves the quality of the learned\nrepresentations, and (3) contrastive learning benefits from larger batch sizes\nand more training steps compared to supervised learning. By combining these\nfindings, we are able to considerably outperform previous methods for\nself-supervised and semi-supervised learning on ImageNet. A linear classifier\ntrained on self-supervised representations learned by SimCLR achieves 76.5%\ntop-1 accuracy, which is a 7% relative improvement over previous\nstate-of-the-art, matching the performance of a supervised ResNet-50. When\nfine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy,\noutperforming AlexNet with 100X fewer labels.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 18:50:45 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 15:32:51 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 00:09:08 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Ting", ""], ["Kornblith", "Simon", ""], ["Norouzi", "Mohammad", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "2002.05712", "submitter": "Zhuliang Yao", "authors": "Zhuliang Yao, Yue Cao, Shuxin Zheng, Gao Huang, Stephen Lin", "title": "Cross-Iteration Batch Normalization", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known issue of Batch Normalization is its significantly reduced\neffectiveness in the case of small mini-batch sizes. When a mini-batch contains\nfew examples, the statistics upon which the normalization is defined cannot be\nreliably estimated from it during a training iteration. To address this\nproblem, we present Cross-Iteration Batch Normalization (CBN), in which\nexamples from multiple recent iterations are jointly utilized to enhance\nestimation quality. A challenge of computing statistics over multiple\niterations is that the network activations from different iterations are not\ncomparable to each other due to changes in network weights. We thus compensate\nfor the network weight changes via a proposed technique based on Taylor\npolynomials, so that the statistics can be accurately estimated and batch\nnormalization can be effectively applied. On object detection and image\nclassification with small mini-batch sizes, CBN is found to outperform the\noriginal batch normalization and a direct calculation of statistics over\nprevious iterations without the proposed compensation technique. Code is\navailable at https://github.com/Howal/Cross-iterationBatchNorm .\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 18:52:57 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 11:10:04 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 06:57:36 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yao", "Zhuliang", ""], ["Cao", "Yue", ""], ["Zheng", "Shuxin", ""], ["Huang", "Gao", ""], ["Lin", "Stephen", ""]]}, {"id": "2002.05714", "submitter": "Kai Han", "authors": "Kai Han and Sylvestre-Alvise Rebuffi and Sebastien Ehrhardt and Andrea\n  Vedaldi and Andrew Zisserman", "title": "Automatically Discovering and Learning New Visual Categories with\n  Ranking Statistics", "comments": "ICLR 2020, code: http://www.robots.ox.ac.uk/~vgg/research/auto_novel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of discovering novel classes in an image collection\ngiven labelled examples of other classes. This setting is similar to\nsemi-supervised learning, but significantly harder because there are no\nlabelled examples for the new classes. The challenge, then, is to leverage the\ninformation contained in the labelled images in order to learn a\ngeneral-purpose clustering model and use the latter to identify the new classes\nin the unlabelled data. In this work we address this problem by combining three\nideas: (1) we suggest that the common approach of bootstrapping an image\nrepresentation using the labeled data only introduces an unwanted bias, and\nthat this can be avoided by using self-supervised learning to train the\nrepresentation from scratch on the union of labelled and unlabelled data; (2)\nwe use rank statistics to transfer the model's knowledge of the labelled\nclasses to the problem of clustering the unlabelled images; and, (3) we train\nthe data representation by optimizing a joint objective function on the\nlabelled and unlabelled subsets of the data, improving both the supervised\nclassification of the labelled data, and the clustering of the unlabelled data.\nWe evaluate our approach on standard classification benchmarks and outperform\ncurrent methods for novel category discovery by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 18:53:32 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Han", "Kai", ""], ["Rebuffi", "Sylvestre-Alvise", ""], ["Ehrhardt", "Sebastien", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2002.05773", "submitter": "Yuemeng Li", "authors": "Yuemeng Li, Hongming Li, Yong Fan", "title": "ACEnet: Anatomical Context-Encoding Network for Neuroanatomy\n  Segmentation", "comments": null, "journal-ref": "Medical Image Analysis, 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of brain structures from magnetic resonance (MR) scans plays an\nimportant role in the quantification of brain morphology. Since 3D deep\nlearning models suffer from high computational cost, 2D deep learning methods\nare favored for their computational efficiency. However, existing 2D deep\nlearning methods are not equipped to effectively capture 3D spatial contextual\ninformation that is needed to achieve accurate brain structure segmentation. In\norder to overcome this limitation, we develop an Anatomical Context-Encoding\nNetwork (ACEnet) to incorporate 3D spatial and anatomical contexts in 2D\nconvolutional neural networks (CNNs) for efficient and accurate segmentation of\nbrain structures from MR scans, consisting of 1) an anatomical context encoding\nmodule to incorporate anatomical information in 2D CNNs and 2) a spatial\ncontext encoding module to integrate 3D image information in 2D CNNs. In\naddition, a skull stripping module is adopted to guide the 2D CNNs to attend to\nthe brain. Extensive experiments on three benchmark datasets have demonstrated\nthat our method achieves promising performance compared with state-of-the-art\nalternative methods for brain structure segmentation in terms of both\ncomputational efficiency and segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 20:48:46 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 18:58:13 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 22:30:59 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Yuemeng", ""], ["Li", "Hongming", ""], ["Fan", "Yong", ""]]}, {"id": "2002.05809", "submitter": "Konstantinos P. Panousis", "authors": "Konstantinos P. Panousis, Sotirios Chatzis, Sergios Theodoridis", "title": "Variational Conditional-Dependence Hidden Markov Models for Human Action\n  Recognition", "comments": "Under review ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov Models (HMMs) are a powerful generative approach for modeling\nsequential data and time-series in general. However, the commonly employed\nassumption of the dependence of the current time frame to a single or multiple\nimmediately preceding frames is unrealistic; more complicated dynamics\npotentially exist in real world scenarios. Human Action Recognition constitutes\nsuch a scenario, and has attracted increased attention with the advent of\nlow-cost 3D sensors. The naturally arising variations and complex temporal\ndependencies have established this task as a challenging problem in the\ncommunity. This paper revisits conventional sequential modeling approaches,\naiming to address the problem of capturing time-varying temporal dependency\npatterns. To this end, we propose a different formulation of HMMs, whereby the\ndependence on past frames is dynamically inferred from the data. Specifically,\nwe introduce a hierarchical extension by postulating an additional latent\nvariable layer; therein, the (time-varying) temporal dependence patterns are\ntreated as latent variables over which inference is performed. We leverage\nsolid arguments from the Variational Bayes framework and derive a tractable\ninference algorithm based on the forward-backward algorithm. As we\nexperimentally show using benchmark datasets, our approach yields competitive\nrecognition accuracy and can effectively handle data with missing values.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 23:18:52 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Panousis", "Konstantinos P.", ""], ["Chatzis", "Sotirios", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "2002.05844", "submitter": "Xin Yang", "authors": "Zhendong Liu, Xin Yang, Rui Gao, Shengfeng Liu, Haoran Dou, Shuangchi\n  He, Yuhao Huang, Yankai Huang, Huanjia Luo, Yuanji Zhang, Yi Xiong, Dong Ni", "title": "Remove Appearance Shift for Ultrasound Image Segmentation via Fast and\n  Universal Style Transfer", "comments": "IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) suffer from the performance degradation when\nimage appearance shift occurs, especially in ultrasound (US) image\nsegmentation. In this paper, we propose a novel and intuitive framework to\nremove the appearance shift, and hence improve the generalization ability of\nDNNs. Our work has three highlights. First, we follow the spirit of universal\nstyle transfer to remove appearance shifts, which was not explored before for\nUS images. Without sacrificing image structure details, it enables the\narbitrary style-content transfer. Second, accelerated with Adaptive Instance\nNormalization block, our framework achieved real-time speed required in the\nclinical US scanning. Third, an efficient and effective style image selection\nstrategy is proposed to ensure the target-style US image and testing content US\nimage properly match each other. Experiments on two large US datasets\ndemonstrate that our methods are superior to state-of-the-art methods on making\nDNNs robust against various appearance shifts.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 02:00:57 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Liu", "Zhendong", ""], ["Yang", "Xin", ""], ["Gao", "Rui", ""], ["Liu", "Shengfeng", ""], ["Dou", "Haoran", ""], ["He", "Shuangchi", ""], ["Huang", "Yuhao", ""], ["Huang", "Yankai", ""], ["Luo", "Huanjia", ""], ["Zhang", "Yuanji", ""], ["Xiong", "Yi", ""], ["Ni", "Dong", ""]]}, {"id": "2002.05878", "submitter": "Rongye Shi", "authors": "Zhicheng Gu, Zhihao Li, Xuan Di, Rongye Shi", "title": "An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset", "comments": null, "journal-ref": "Applied Sciences 10(6) 2046, 2020", "doi": "10.3390/app10062046", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Waymo Open Dataset has been released recently, providing a platform to\ncrowdsource some fundamental challenges for automated vehicles (AVs), such as\n3D detection and tracking. While~the dataset provides a large amount of\nhigh-quality and multi-source driving information, people in academia are more\ninterested in the underlying driving policy programmed in Waymo self-driving\ncars, which is inaccessible due to AV manufacturers' proprietary protection.\nAccordingly, academic researchers have to make various assumptions to implement\nAV components in their models or simulations, which may not represent the\nrealistic interactions in real-world traffic. Thus, this paper introduces an\napproach to learn a long short-term memory (LSTM)-based model for imitating the\nbehavior of Waymo's self-driving model. The proposed model has been evaluated\nbased on Mean Absolute Error (MAE). The experimental results show that our\nmodel outperforms several baseline models in driving action prediction. In\naddition, a visualization tool is presented for verifying the performance of\nthe model.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 05:28:15 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 16:25:20 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Gu", "Zhicheng", ""], ["Li", "Zhihao", ""], ["Di", "Xuan", ""], ["Shi", "Rongye", ""]]}, {"id": "2002.05895", "submitter": "Minyoung Chung", "authors": "Minyoung Chung, Jingyu Lee, Jeongjin Lee, and Yeong-Gil Shin", "title": "Liver Segmentation in Abdominal CT Images via Auto-Context Neural\n  Network and Self-Supervised Contour Attention", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": "10.1016/j.artmed.2021.102023", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate image segmentation of the liver is a challenging problem owing to\nits large shape variability and unclear boundaries. Although the applications\nof fully convolutional neural networks (CNNs) have shown groundbreaking\nresults, limited studies have focused on the performance of generalization. In\nthis study, we introduce a CNN for liver segmentation on abdominal computed\ntomography (CT) images that shows high generalization performance and accuracy.\nTo improve the generalization performance, we initially propose an auto-context\nalgorithm in a single CNN. The proposed auto-context neural network exploits an\neffective high-level residual estimation to obtain the shape prior. Identical\ndual paths are effectively trained to represent mutual complementary features\nfor an accurate posterior analysis of a liver. Further, we extend our network\nby employing a self-supervised contour scheme. We trained sparse contour\nfeatures by penalizing the ground-truth contour to focus more contour\nattentions on the failures. The experimental results show that the proposed\nnetwork results in better accuracy when compared to the state-of-the-art\nnetworks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CT\nimages for training and validation. Two-fold cross-validation is presented for\na comparison with the state-of-the-art neural networks. Novel multiple N-fold\ncross-validations are conducted to verify the performance of generalization.\nThe proposed network showed the best generalization performance among the\nnetworks. Additionally, we present a series of ablation experiments that\ncomprehensively support the importance of the underlying concepts.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 07:32:45 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chung", "Minyoung", ""], ["Lee", "Jingyu", ""], ["Lee", "Jeongjin", ""], ["Shin", "Yeong-Gil", ""]]}, {"id": "2002.05907", "submitter": "Bin Ren", "authors": "Bin Ren, Mengyuan Liu, Runwei Ding, Hong Liu", "title": "A Survey on 3D Skeleton-Based Action Recognition Using Learning Method", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D skeleton-based action recognition, owing to the latent advantages of\nskeleton, has been an active topic in computer vision. As a consequence, there\nare lots of impressive works including conventional handcraft feature based and\nlearned feature based have been done over the years. However, previous surveys\nabout action recognition mostly focus on the video or RGB data dominated\nmethods, and the scanty existing reviews related to skeleton data mainly\nindicate the representation of skeleton data or performance of some classic\ntechniques on a certain dataset. Besides, though deep learning methods has been\napplied to this field for years, there is no related reserach concern about an\nintroduction or review from the perspective of deep learning architectures. To\nbreak those limitations, this survey firstly highlight the necessity of action\nrecognition and the significance of 3D-skeleton data. Then a comprehensive\nintroduction about Recurrent Neural Network(RNN)-based, Convolutional Neural\nNetwork(CNN)-based and Graph Convolutional Network(GCN)-based main stream\naction recognition techniques are illustrated in a data-driven manner. Finally,\nwe give a brief talk about the biggest 3D skeleton dataset NTU-RGB+D and its\nnew edition called NTU-RGB+D 120, accompanied with several existing top rank\nalgorithms within those two datasets. To our best knowledge, this is the first\nresearch which give an overall discussion over deep learning-based action\nrecognitin using 3D skeleton data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 08:12:12 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Ren", "Bin", ""], ["Liu", "Mengyuan", ""], ["Ding", "Runwei", ""], ["Liu", "Hong", ""]]}, {"id": "2002.05911", "submitter": "Haosheng Chen", "authors": "Haosheng Chen, David Suter, Qiangqiang Wu, Hanzi Wang", "title": "End-to-end Learning of Object Motion Estimation from Retinal Events for\n  Event-based Object Tracking", "comments": "9 pages, 3 figures", "journal-ref": "Proceedings of the Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI '20). 2020, New York, USA. AAAI, New York, NY, USA", "doi": "10.1609/aaai.v34i07.6625", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras, which are asynchronous bio-inspired vision sensors, have shown\ngreat potential in computer vision and artificial intelligence. However, the\napplication of event cameras to object-level motion estimation or tracking is\nstill in its infancy. The main idea behind this work is to propose a novel deep\nneural network to learn and regress a parametric object-level motion/transform\nmodel for event-based object tracking. To achieve this goal, we propose a\nsynchronous Time-Surface with Linear Time Decay (TSLTD) representation, which\neffectively encodes the spatio-temporal information of asynchronous retinal\nevents into TSLTD frames with clear motion patterns. We feed the sequence of\nTSLTD frames to a novel Retinal Motion Regression Network (RMRNet) to perform\nan end-to-end 5-DoF object motion regression. Our method is compared with\nstate-of-the-art object tracking methods, that are based on conventional\ncameras or event cameras. The experimental results show the superiority of our\nmethod in handling various challenging environments such as fast motion and low\nillumination conditions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 08:19:50 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Chen", "Haosheng", ""], ["Suter", "David", ""], ["Wu", "Qiangqiang", ""], ["Wang", "Hanzi", ""]]}, {"id": "2002.05925", "submitter": "Onur Tasar", "authors": "Onur Tasar, S L Happy, Yuliya Tarabalka, Pierre Alliez", "title": "SemI2I: Semantically Consistent Image-to-Image Translation for Domain\n  Adaptation of Remote Sensing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although convolutional neural networks have been proven to be an effective\ntool to generate high quality maps from remote sensing images, their\nperformance significantly deteriorates when there exists a large domain shift\nbetween training and test data. To address this issue, we propose a new data\naugmentation approach that transfers the style of test data to training data\nusing generative adversarial networks. Our semantic segmentation framework\nconsists in first training a U-net from the real training data and then\nfine-tuning it on the test stylized fake training data generated by the\nproposed approach. Our experimental results prove that our framework\noutperforms the existing domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 09:07:09 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 09:21:35 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Tasar", "Onur", ""], ["Happy", "S L", ""], ["Tarabalka", "Yuliya", ""], ["Alliez", "Pierre", ""]]}, {"id": "2002.05928", "submitter": "Guangshuai Gao", "authors": "Guangshuai Gao, Qingjie Liu, Yunhong Wang", "title": "Counting dense objects in remote sensing images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating accurate number of interested objects from a given image is a\nchallenging yet important task. Significant efforts have been made to address\nthis problem and achieve great progress, yet counting number of ground objects\nfrom remote sensing images is barely studied. In this paper, we are interested\nin counting dense objects from remote sensing images. Compared with object\ncounting in natural scene, this task is challenging in following factors: large\nscale variation, complex cluttered background and orientation arbitrariness.\nMore importantly, the scarcity of data severely limits the development of\nresearch in this field. To address these issues, we first construct a\nlarge-scale object counting dataset based on remote sensing images, which\ncontains four kinds of objects: buildings, crowded ships in harbor,\nlarge-vehicles and small-vehicles in parking lot. We then benchmark the dataset\nby designing a novel neural network which can generate density map of an input\nimage. The proposed network consists of three parts namely convolution block\nattention module (CBAM), scale pyramid module (SPM) and deformable convolution\nmodule (DCM). Experiments on the proposed dataset and comparisons with state of\nthe art methods demonstrate the challenging of the proposed dataset, and\nsuperiority and effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 09:13:54 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Gao", "Guangshuai", ""], ["Liu", "Qingjie", ""], ["Wang", "Yunhong", ""]]}, {"id": "2002.05962", "submitter": "Jiawen Lin", "authors": "Jiawen Lyn", "title": "Multi-Level Feature Fusion Mechanism for Single Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution neural network (CNN) has been widely used in Single Image Super\nResolution (SISR) so that SISR has been a great success recently. As the\nnetwork deepens, the learning ability of network becomes more and more\npowerful. However, most SISR methods based on CNN do not make full use of\nhierarchical feature and the learning ability of network. These features cannot\nbe extracted directly by subsequent layers, so the previous layer hierarchical\ninformation has little impact on the output and performance of subsequent\nlayers relatively poor. To solve above problem, a novel Multi-Level Feature\nFusion network (MLRN) is proposed, which can take full use of global\nintermediate features. We also introduce Feature Skip Fusion Block (FSFblock)\nas basic module. Each block can be extracted directly to the raw multiscale\nfeature and fusion multi-level feature, then learn feature spatial correlation.\nThe correlation among the features of the holistic approach leads to a\ncontinuous global memory of information mechanism. Extensive experiments on\npublic datasets show that the method proposed by MLRN can be implemented, which\nis favorable performance for the most advanced methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 10:47:40 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Lyn", "Jiawen", ""]]}, {"id": "2002.05966", "submitter": "Hao Cheng", "authors": "Hao Cheng, Wentong Liao, Michael Ying Yang, Monika Sester, Bodo\n  Rosenhahn", "title": "MCENET: Multi-Context Encoder Network for Homogeneous Agent Trajectory\n  Prediction in Mixed Traffic", "comments": "8 pages, 5 figures, code is available on\n  https://github.com/haohao11/MCENET", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction in urban mixed-traffic zones (a.k.a. shared spaces) is\ncritical for many intelligent transportation systems, such as intent detection\nfor autonomous driving. However, there are many challenges to predict the\ntrajectories of heterogeneous road agents (pedestrians, cyclists and vehicles)\nat a microscopical level. For example, an agent might be able to choose\nmultiple plausible paths in complex interactions with other agents in varying\nenvironments. To this end, we propose an approach named Multi-Context Encoder\nNetwork (MCENET) that is trained by encoding both past and future scene\ncontext, interaction context and motion information to capture the patterns and\nvariations of the future trajectories using a set of stochastic latent\nvariables. In inference time, we combine the past context and motion\ninformation of the target agent with samplings of the latent variables to\npredict multiple realistic trajectories in the future. Through experiments on\nseveral datasets of varying scenes, our method outperforms some of the recent\nstate-of-the-art methods for mixed traffic trajectory prediction by a large\nmargin and more robust in a very challenging environment. The impact of each\ncontext is justified via ablation studies.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 11:04:41 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 15:53:02 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 13:39:05 GMT"}, {"version": "v4", "created": "Sun, 5 Apr 2020 12:08:51 GMT"}, {"version": "v5", "created": "Tue, 23 Jun 2020 13:06:17 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Cheng", "Hao", ""], ["Liao", "Wentong", ""], ["Yang", "Michael Ying", ""], ["Sester", "Monika", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2002.05981", "submitter": "Ahmed El Gazzar", "authors": "Ahmed El-Gazzar, Mirjam Quaak, Leonardo Cerliani, Peter Bloem, Guido\n  van Wingen and Rajat Mani Thomas", "title": "A Hybrid 3DCNN and 3DC-LSTM based model for 4D Spatio-temporal fMRI\n  data: An ABIDE Autism Classification study", "comments": "8pages", "journal-ref": "Second International Workshop, OR 2.0 2019, and Second\n  International Workshop, MLCN 2019, Held in Conjunction with MICCAI 2019,\n  Shenzhen, China, October 13 and 17, 2019, Proceedings", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Magnetic Resonance Imaging (fMRI) captures the temporal dynamics\nof neural activity as a function of spatial location in the brain. Thus, fMRI\nscans are represented as 4-Dimensional (3-space + 1-time) tensors. And it is\nwidely believed that the spatio-temporal patterns in fMRI manifests as\nbehaviour and clinical symptoms. Because of the high dimensionality ($\\sim$ 1\nMillion) of fMRI, and the added constraints of limited cardinality of data\nsets, extracting such patterns are challenging. A standard approach to overcome\nthese hurdles is to reduce the dimensionality of the data by either summarizing\nactivation over time or space at the expense of possible loss of useful\ninformation. Here, we introduce an end-to-end algorithm capable of extracting\nspatiotemporal features from the full 4-D data using 3-D CNNs and 3-D\nConvolutional LSTMs. We evaluate our proposed model on the publicly available\nABIDE dataset to demonstrate the capability of our model to classify Autism\nSpectrum Disorder (ASD) from resting-state fMRI data. Our results show that the\nproposed model achieves state of the art results on single sites with F1-scores\nof 0.78 and 0.7 on NYU and UM sites, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 11:52:00 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["El-Gazzar", "Ahmed", ""], ["Quaak", "Mirjam", ""], ["Cerliani", "Leonardo", ""], ["Bloem", "Peter", ""], ["van Wingen", "Guido", ""], ["Thomas", "Rajat Mani", ""]]}, {"id": "2002.05990", "submitter": "Dongxian Wu", "authors": "Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, Xingjun Ma", "title": "Skip Connections Matter: On the Transferability of Adversarial Examples\n  Generated with ResNets", "comments": "ICLR 2020 conference paper (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skip connections are an essential component of current state-of-the-art deep\nneural networks (DNNs) such as ResNet, WideResNet, DenseNet, and ResNeXt.\nDespite their huge success in building deeper and more powerful DNNs, we\nidentify a surprising security weakness of skip connections in this paper. Use\nof skip connections allows easier generation of highly transferable adversarial\nexamples. Specifically, in ResNet-like (with skip connections) neural networks,\ngradients can backpropagate through either skip connections or residual\nmodules. We find that using more gradients from the skip connections rather\nthan the residual modules according to a decay factor, allows one to craft\nadversarial examples with high transferability. Our method is termed Skip\nGradient Method(SGM). We conduct comprehensive transfer attacks against\nstate-of-the-art DNNs including ResNets, DenseNets, Inceptions,\nInception-ResNet, Squeeze-and-Excitation Network (SENet) and robustly trained\nDNNs. We show that employing SGM on the gradient flow can greatly improve the\ntransferability of crafted attacks in almost all cases. Furthermore, SGM can be\neasily combined with existing black-box attack techniques, and obtain high\nimprovements over state-of-the-art transferability methods. Our findings not\nonly motivate new research into the architectural vulnerability of DNNs, but\nalso open up further challenges for the design of secure DNN architectures.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 12:09:21 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Wu", "Dongxian", ""], ["Wang", "Yisen", ""], ["Xia", "Shu-Tao", ""], ["Bailey", "James", ""], ["Ma", "Xingjun", ""]]}, {"id": "2002.06001", "submitter": "Fabricio Breve", "authors": "Fabricio Breve", "title": "Building Networks for Image Segmentation using Particle Competition and\n  Cooperation", "comments": null, "journal-ref": "BREVE, FA. Building Networks for Image Segmentation Using Particle\n  Competition and Cooperation. Lecture Notes in Computer Science. Cham:\n  Springer International Publishing AG, 2017. v.10404. p.217 - 231", "doi": "10.1007/978-3-319-62392-4_16", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle competition and cooperation (PCC) is a graph-based semi-supervised\nlearning approach. When PCC is applied to interactive image segmentation tasks,\npixels are converted into network nodes, and each node is connected to its\nk-nearest neighbors, according to the distance between a set of features\nextracted from the image. Building a proper network to feed PCC is crucial to\nachieve good segmentation results. However, some features may be more important\nthan others to identify the segments, depending on the characteristics of the\nimage to be segmented. In this paper, an index to evaluate candidate networks\nis proposed. Thus, building the network becomes a problem of optimizing some\nfeature weights based on the proposed index. Computer simulations are performed\non some real-world images from the Microsoft GrabCut database, and the\nsegmentation results related in this paper show the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 12:45:12 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Breve", "Fabricio", ""]]}, {"id": "2002.06028", "submitter": "Leuleseged Alemu", "authors": "Alemu Leulseged Tesfaye", "title": "Constrained Dominant sets and Its applications in computer vision", "comments": "PhD dissertation. arXiv admin note: substantial text overlap with\n  arXiv:1608.00641 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we present new schemes which leverage a constrained\nclustering method to solve several computer vision tasks ranging from image\nretrieval, image segmentation and co-segmentation, to person re-identification.\nIn the last decades clustering methods have played a vital role in computer\nvision applications; herein, we focus on the extension, reformulation, and\nintegration of a well-known graph and game theoretic clustering method known as\nDominant Sets. Thus, we have demonstrated the validity of the proposed methods\nwith extensive experiments which are conducted on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 20:19:44 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Tesfaye", "Alemu Leulseged", ""]]}, {"id": "2002.06048", "submitter": "Youngmin Ro", "authors": "Youngmin Ro, Jin Young Choi", "title": "AutoLR: Layer-wise Pruning and Auto-tuning of Learning Rates in\n  Fine-tuning of Deep Networks", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing fine-tuning methods use a single learning rate over all layers. In\nthis paper, first, we discuss that trends of layer-wise weight variations by\nfine-tuning using a single learning rate do not match the well-known notion\nthat lower-level layers extract general features and higher-level layers\nextract specific features. Based on our discussion, we propose an algorithm\nthat improves fine-tuning performance and reduces network complexity through\nlayer-wise pruning and auto-tuning of layer-wise learning rates. The proposed\nalgorithm has verified the effectiveness by achieving state-of-the-art\nperformance on the image retrieval benchmark datasets (CUB-200, Cars-196,\nStanford online product, and Inshop). Code is available at\nhttps://github.com/youngminPIL/AutoLR.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 14:24:40 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 05:19:02 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 01:41:13 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ro", "Youngmin", ""], ["Choi", "Jin Young", ""]]}, {"id": "2002.06144", "submitter": "Rapha\\\"el Barman", "authors": "Rapha\\\"el Barman, Maud Ehrmann, Simon Clematide, Sofia Ares Oliveira,\n  Fr\\'ed\\'eric Kaplan", "title": "Combining Visual and Textual Features for Semantic Segmentation of\n  Historical Newspapers", "comments": null, "journal-ref": "Journal of Data Mining & Digital Humanities, HistoInformatics,\n  HistoInformatics (January 19, 2021) jdmdh:7097", "doi": "10.46298/jdmdh.6107", "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The massive amounts of digitized historical documents acquired over the last\ndecades naturally lend themselves to automatic processing and exploration.\nResearch work seeking to automatically process facsimiles and extract\ninformation thereby are multiplying with, as a first essential step, document\nlayout analysis. If the identification and categorization of segments of\ninterest in document images have seen significant progress over the last years\nthanks to deep learning techniques, many challenges remain with, among others,\nthe use of finer-grained segmentation typologies and the consideration of\ncomplex, heterogeneous documents such as historical newspapers. Besides, most\napproaches consider visual features only, ignoring textual signal. In this\ncontext, we introduce a multimodal approach for the semantic segmentation of\nhistorical newspapers that combines visual and textual features. Based on a\nseries of experiments on diachronic Swiss and Luxembourgish newspapers, we\ninvestigate, among others, the predictive power of visual and textual features\nand their capacity to generalize across time and sources. Results show\nconsistent improvement of multimodal models in comparison to a strong visual\nbaseline, as well as better robustness to high material variance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 17:56:18 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 11:59:18 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 07:45:29 GMT"}, {"version": "v4", "created": "Mon, 14 Dec 2020 16:56:29 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Barman", "Rapha\u00ebl", ""], ["Ehrmann", "Maud", ""], ["Clematide", "Simon", ""], ["Oliveira", "Sofia Ares", ""], ["Kaplan", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2002.06145", "submitter": "Huibing Wang", "authors": "Yuxiao Yan, Yang Yan, Jinjia Peng, Huibing Wang, Xianping Fu", "title": "Purifying Real Images with an Attention-guided Style Transfer Network\n  for Gaze Estimation", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.05820,\n  arXiv:1903.08152; and text overlap with arXiv:1603.08155 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the progress of learning-by-synthesis has proposed a training model\nfor synthetic images, which can effectively reduce the cost of human and\nmaterial resources. However, due to the different distribution of synthetic\nimages compared to real images, the desired performance cannot be achieved.\nReal images consist of multiple forms of light orientation, while synthetic\nimages consist of a uniform light orientation. These features are considered to\nbe characteristic of outdoor and indoor scenes, respectively. To solve this\nproblem, the previous method learned a model to improve the realism of the\nsynthetic image. Different from the previous methods, this paper try to purify\nreal image by extracting discriminative and robust features to convert outdoor\nreal images to indoor synthetic images. In this paper, we first introduce the\nsegmentation masks to construct RGB-mask pairs as inputs, then we design a\nattention-guided style transfer network to learn style features separately from\nthe attention and bkgd(background) region , learn content features from full\nand attention region. Moreover, we propose a novel region-level task-guided\nloss to restrain the features learnt from style and content. Experiments were\nperformed using mixed studies (qualitative and quantitative) methods to\ndemonstrate the possibility of purifying real images in complex directions. We\nevaluate the proposed method on three public datasets, including LPW, COCO and\nMPIIGaze. Extensive experimental results show that the proposed method is\neffective and achieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 04:29:32 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Yan", "Yuxiao", ""], ["Yan", "Yang", ""], ["Peng", "Jinjia", ""], ["Wang", "Huibing", ""], ["Fu", "Xianping", ""]]}, {"id": "2002.06228", "submitter": "Kevin Hernandez-Diaz", "authors": "Kevin Hernandez Diaz, Fernando Alonso-Fernandez, Josef Bigun", "title": "Spectrum Translation for Cross-Spectral Ocular Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-spectral verification remains a big issue in biometrics, especially for\nthe ocular area due to differences in the reflected features in the images\ndepending on the region and spectrum used.\n  In this paper, we investigate the use of Conditional Adversarial Networks for\nspectrum translation between near infra-red and visual light images for ocular\nbiometrics. We analyze the transformation based on the overall visual quality\nof the transformed images and the accuracy drop of the identification system\nwhen trained with opposing data.\n  We use the PolyU database and propose two different systems for biometric\nverification, the first one based on Siamese Networks trained with Softmax and\nCross-Entropy loss, and the second one a Triplet Loss network. We achieved an\nEER of 1\\% when using a Triplet Loss network trained for NIR and finding the\nEuclidean distance between the real NIR images and the fake ones translated\nfrom the visible spectrum. We also outperform previous results using baseline\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 19:30:31 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Diaz", "Kevin Hernandez", ""], ["Alonso-Fernandez", "Fernando", ""], ["Bigun", "Josef", ""]]}, {"id": "2002.06241", "submitter": "Jiachen Li", "authors": "Jiachen Li, Hengbo Ma, Zhihao Zhang, Masayoshi Tomizuka", "title": "Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein\n  Graph Double-Attention Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective understanding of the environment and accurate trajectory prediction\nof surrounding dynamic obstacles are indispensable for intelligent mobile\nsystems (like autonomous vehicles and social robots) to achieve safe and\nhigh-quality planning when they navigate in highly interactive and crowded\nscenarios. Due to the existence of frequent interactions and uncertainty in the\nscene evolution, it is desired for the prediction system to enable relational\nreasoning on different entities and provide a distribution of future\ntrajectories for each agent. In this paper, we propose a generic generative\nneural system (called Social-WaGDAT) for multi-agent trajectory prediction,\nwhich makes a step forward to explicit interaction modeling by incorporating\nrelational inductive biases with a dynamic graph representation and leverages\nboth trajectory and scene context information. We also employ an efficient\nkinematic constraint layer applied to vehicle trajectory prediction which not\nonly ensures physical feasibility but also enhances model performance. The\nproposed system is evaluated on three public benchmark datasets for trajectory\nprediction, where the agents cover pedestrians, cyclists and on-road vehicles.\nThe experimental results demonstrate that our model achieves better performance\nthan various baseline approaches in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 20:11:13 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Li", "Jiachen", ""], ["Ma", "Hengbo", ""], ["Zhang", "Zhihao", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2002.06260", "submitter": "Aaron Hertzmann", "authors": "Aaron Hertzmann", "title": "Why Do Line Drawings Work? A Realism Hypothesis", "comments": "Accepted to Perception", "journal-ref": "Perception. 49:4 (2020) 439-451", "doi": "10.1177/0301006620908207", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why is it that we can recognize object identity and 3D shape from line\ndrawings, even though they do not exist in the natural world? This paper\nhypothesizes that the human visual system perceives line drawings as if they\nwere approximately realistic images. Moreover, the techniques of line drawing\nare chosen to accurately convey shape to a human observer. Several implications\nand variants of this hypothesis are explored.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 21:41:00 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Hertzmann", "Aaron", ""]]}, {"id": "2002.06264", "submitter": "Yanfeng Liu", "authors": "Yanfeng Liu, Eric Psota, Lance P\\'erez", "title": "Layered Embeddings for Amodal Instance Segmentation", "comments": "International Conference on Image Analysis and Recognition. Springer,\n  Cham, 2019 (ICIAR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposed method extends upon the representational output of semantic\ninstance segmentation by explicitly including both visible and occluded parts.\nA fully convolutional network is trained to produce consistent pixel-level\nembedding across two layers such that, when clustered, the results convey the\nfull spatial extent and depth ordering of each instance. Results demonstrate\nthat the network can accurately estimate complete masks in the presence of\nocclusion and outperform leading top-down bounding-box approaches. Source code\navailable at https://github.com/yanfengliu/layered_embeddings\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 22:00:45 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Liu", "Yanfeng", ""], ["Psota", "Eric", ""], ["P\u00e9rez", "Lance", ""]]}, {"id": "2002.06274", "submitter": "Connor Parde", "authors": "Connor J. Parde, Y. Ivette Col\\'on, Matthew Q. Hill, Carlos D.\n  Castillo, Prithviraj Dhar, Alice J. O'Toole", "title": "Single Unit Status in Deep Convolutional Neural Network Codes for Face\n  Identification: Sparseness Redefined", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) trained for face identification\ndevelop representations that generalize over variable images, while retaining\nsubject (e.g., gender) and image (e.g., viewpoint) information. Identity,\ngender, and viewpoint codes were studied at the \"neural unit\" and ensemble\nlevels of a face-identification network. At the unit level, identification,\ngender classification, and viewpoint estimation were measured by deleting units\nto create variably-sized, randomly-sampled subspaces at the top network layer.\nIdentification of 3,531 identities remained high (area under the ROC\napproximately 1.0) as dimensionality decreased from 512 units to 16 (0.95), 4\n(0.80), and 2 (0.72) units. Individual identities separated statistically on\nevery top-layer unit. Cross-unit responses were minimally correlated,\nindicating that units code non-redundant identity cues. This \"distributed\" code\nrequires only a sparse, random sample of units to identify faces accurately.\nGender classification declined gradually and viewpoint estimation fell steeply\nas dimensionality decreased. Individual units were weakly predictive of gender\nand viewpoint, but ensembles proved effective predictors. Therefore,\ndistributed and sparse codes co-exist in the network units to represent\ndifferent face attributes. At the ensemble level, principal component analysis\nof face representations showed that identity, gender, and viewpoint information\nseparated into high-dimensional subspaces, ordered by explained variance.\nIdentity, gender, and viewpoint information contributed to all individual unit\nresponses, undercutting a neural tuning analogy for face attributes.\nInterpretation of neural-like codes from DCNNs, and by analogy, high-level\nvisual codes, cannot be inferred from single unit responses. Instead, \"meaning\"\nis encoded by directions in the high-dimensional space.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 22:42:02 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 09:58:01 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Parde", "Connor J.", ""], ["Col\u00f3n", "Y. Ivette", ""], ["Hill", "Matthew Q.", ""], ["Castillo", "Carlos D.", ""], ["Dhar", "Prithviraj", ""], ["O'Toole", "Alice J.", ""]]}, {"id": "2002.06289", "submitter": "Antoni Rosinol", "authors": "Antoni Rosinol, Arjun Gupta, Marcus Abate, Jingnan Shi, Luca Carlone", "title": "3D Dynamic Scene Graphs: Actionable Spatial Perception with Places,\n  Objects, and Humans", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified representation for actionable spatial perception: 3D\nDynamic Scene Graphs. Scene graphs are directed graphs where nodes represent\nentities in the scene (e.g. objects, walls, rooms), and edges represent\nrelations (e.g. inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs)\nextend this notion to represent dynamic scenes with moving agents (e.g. humans,\nrobots), and to include actionable information that supports planning and\ndecision-making (e.g. spatio-temporal relations, topology at different levels\nof abstraction). Our second contribution is to provide the first fully\nautomatic Spatial PerceptIon eNgine(SPIN) to build a DSG from visual-inertial\ndata. We integrate state-of-the-art techniques for object and human detection\nand pose estimation, and we describe how to robustly infer object, robot, and\nhuman nodes in crowded scenes. To the best of our knowledge, this is the first\npaper that reconciles visual-inertial SLAM and dense human mesh tracking.\nMoreover, we provide algorithms to obtain hierarchical representations of\nindoor environments (e.g. places, structures, rooms) and their relations. Our\nthird contribution is to demonstrate the proposed spatial perception engine in\na photo-realistic Unity-based simulator, where we assess its robustness and\nexpressiveness. Finally, we discuss the implications of our proposal on modern\nrobotics applications. 3D Dynamic Scene Graphs can have a profound impact on\nplanning and decision-making, human-robot interaction, long-term autonomy, and\nscene prediction. A video abstract is available at https://youtu.be/SWbofjhyPzI\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 00:46:32 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 22:39:39 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Rosinol", "Antoni", ""], ["Gupta", "Arjun", ""], ["Abate", "Marcus", ""], ["Shi", "Jingnan", ""], ["Carlone", "Luca", ""]]}, {"id": "2002.06300", "submitter": "James Philips", "authors": "James P. Philips and Nasseh Tabrizi", "title": "Historical Document Processing: Historical Document Processing: A Survey\n  of Techniques, Tools, and Trends", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical Document Processing is the process of digitizing written material\nfrom the past for future use by historians and other scholars. It incorporates\nalgorithms and software tools from various subfields of computer science,\nincluding computer vision, document analysis and recognition, natural language\nprocessing, and machine learning, to convert images of ancient manuscripts,\nletters, diaries, and early printed texts automatically into a digital format\nusable in data mining and information retrieval systems. Within the past twenty\nyears, as libraries, museums, and other cultural heritage institutions have\nscanned an increasing volume of their historical document archives, the need to\ntranscribe the full text from these collections has become acute. Since\nHistorical Document Processing encompasses multiple sub-domains of computer\nscience, knowledge relevant to its purpose is scattered across numerous\njournals and conference proceedings. This paper surveys the major phases of,\nstandard algorithms, tools, and datasets in the field of Historical Document\nProcessing, discusses the results of a literature review, and finally suggests\ndirections for further research.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 01:54:35 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 03:09:05 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Philips", "James P.", ""], ["Tabrizi", "Nasseh", ""]]}, {"id": "2002.06303", "submitter": "Joseph Robinson", "authors": "Joseph P. Robinson and Yu Yin and Zaid Khan and Ming Shao and Siyu Xia\n  and Michael Stopa and Samson Timoner and Matthew A. Turk and Rama Chellappa\n  and Yun Fu", "title": "Recognizing Families In the Wild: White Paper for the 4th Edition Data\n  Challenge", "comments": "White Paper for challenge in conjunction with 15th IEEE International\n  Conference on Automatic Face and Gesture Recognition (FG 2020)", "journal-ref": null, "doi": "10.1109/FG47880.2020.00138", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing Families In the Wild (RFIW): an annual large-scale, multi-track\nautomatic kinship recognition evaluation that supports various visual kin-based\nproblems on scales much higher than ever before. Organized in conjunction with\nthe 15th IEEE International Conference on Automatic Face and Gesture\nRecognition (FG) as a Challenge, RFIW provides a platform for publishing\noriginal work and the gathering of experts for a discussion of the next steps.\nThis paper summarizes the supported tasks (i.e., kinship verification,\ntri-subject verification, and search & retrieval of missing children) in the\nevaluation protocols, which include the practical motivation, technical\nbackground, data splits, metrics, and benchmark results. Furthermore, top\nsubmissions (i.e., leader-board stats) are listed and reviewed as a high-level\nanalysis on the state of the problem. In the end, the purpose of this paper is\nto describe the 2020 RFIW challenge, end-to-end, along with forecasts in\npromising future directions.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 02:22:42 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 18:56:29 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 05:02:32 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Robinson", "Joseph P.", ""], ["Yin", "Yu", ""], ["Khan", "Zaid", ""], ["Shao", "Ming", ""], ["Xia", "Siyu", ""], ["Stopa", "Michael", ""], ["Timoner", "Samson", ""], ["Turk", "Matthew A.", ""], ["Chellappa", "Rama", ""], ["Fu", "Yun", ""]]}, {"id": "2002.06345", "submitter": "Dongnan Liu", "authors": "Dongnan Liu, Donghao Zhang, Yang Song, Heng Huang, Weidong Cai", "title": "Panoptic Feature Fusion Net: A Novel Instance Segmentation Paradigm for\n  Biomedical and Biological Images", "comments": "Accepted to appear in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3050668", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is an important task for biomedical and biological\nimage analysis. Due to the complicated background components, the high\nvariability of object appearances, numerous overlapping objects, and ambiguous\nobject boundaries, this task still remains challenging. Recently, deep learning\nbased methods have been widely employed to solve these problems and can be\ncategorized into proposal-free and proposal-based methods. However, both\nproposal-free and proposal-based methods suffer from information loss, as they\nfocus on either global-level semantic or local-level instance features. To\ntackle this issue, we present a Panoptic Feature Fusion Net (PFFNet) that\nunifies the semantic and instance features in this work. Specifically, our\nproposed PFFNet contains a residual attention feature fusion mechanism to\nincorporate the instance prediction with the semantic features, in order to\nfacilitate the semantic contextual information learning in the instance branch.\nThen, a mask quality sub-branch is designed to align the confidence score of\neach object with the quality of the mask prediction. Furthermore, a consistency\nregularization mechanism is designed between the semantic segmentation tasks in\nthe semantic and instance branches, for the robust learning of both tasks.\nExtensive experiments demonstrate the effectiveness of our proposed PFFNet,\nwhich outperforms several state-of-the-art methods on various biomedical and\nbiological datasets.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 09:19:41 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 10:14:24 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Liu", "Dongnan", ""], ["Zhang", "Donghao", ""], ["Song", "Yang", ""], ["Huang", "Heng", ""], ["Cai", "Weidong", ""]]}, {"id": "2002.06349", "submitter": "Apostolos Modas", "authors": "Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen\n  Moosavi-Dezfooli, Pascal Frossard", "title": "Hold me tight! Influence of discriminative features on deep network\n  boundaries", "comments": "Accepted to the 34th Conference on Neural Information Processing\n  Systems (NeurIPS) 2020 (30 pages, 38 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important insights towards the explainability of neural networks reside in\nthe characteristics of their decision boundaries. In this work, we borrow tools\nfrom the field of adversarial robustness, and propose a new perspective that\nrelates dataset features to the distance of samples to the decision boundary.\nThis enables us to carefully tweak the position of the training samples and\nmeasure the induced changes on the boundaries of CNNs trained on large-scale\nvision datasets. We use this framework to reveal some intriguing properties of\nCNNs. Specifically, we rigorously confirm that neural networks exhibit a high\ninvariance to non-discriminative features, and show that the decision\nboundaries of a DNN can only exist as long as the classifier is trained with\nsome features that hold them together. Finally, we show that the construction\nof the decision boundary is extremely sensitive to small perturbations of the\ntraining samples, and that changes in certain directions can lead to sudden\ninvariances in the orthogonal ones. This is precisely the mechanism that\nadversarial training uses to achieve robustness.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 09:29:36 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 08:42:50 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 12:08:17 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 07:16:47 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Ortiz-Jimenez", "Guillermo", ""], ["Modas", "Apostolos", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "2002.06353", "submitter": "Huaishao Luo", "authors": "Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li,\n  Jason Li, Taroon Bharti, Ming Zhou", "title": "UniVL: A Unified Video and Language Pre-Training Model for Multimodal\n  Understanding and Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent success of the pre-training technique for NLP and\nimage-linguistic tasks, some video-linguistic pre-training works are gradually\ndeveloped to improve video-text related downstream tasks. However, most of the\nexisting multimodal models are pre-trained for understanding tasks, leading to\na pretrain-finetune discrepancy for generation tasks. This paper proposes\nUniVL: a Unified Video and Language pre-training model for both multimodal\nunderstanding and generation. It comprises four components, including two\nsingle-modal encoders, a cross encoder, and a decoder with the Transformer\nbackbone. Five objectives, including video-text joint, conditioned masked\nlanguage model (CMLM), conditioned masked frame model (CMFM), video-text\nalignment, and language reconstruction, are designed to train each of the\ncomponents. We further develop two pre-training strategies, stage by stage\npre-training (StagedP) and enhanced video representation (EnhancedV), to make\nthe training process of the UniVL more effective. The pre-train is carried out\non a sizeable instructional video dataset HowTo100M. Experimental results\ndemonstrate that the UniVL can learn strong video-text representation and\nachieves state-of-the-art results on five downstream tasks.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 10:03:25 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 14:21:43 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 13:27:13 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Luo", "Huaishao", ""], ["Ji", "Lei", ""], ["Shi", "Botian", ""], ["Huang", "Haoyang", ""], ["Duan", "Nan", ""], ["Li", "Tianrui", ""], ["Li", "Jason", ""], ["Bharti", "Taroon", ""], ["Zhou", "Ming", ""]]}, {"id": "2002.06378", "submitter": "Jingwei Xin", "authors": "Jingwei Xin, Nannan Wang, Jie Li, Xinbo Gao, Zhifeng Li", "title": "Video Face Super-Resolution with Motion-Adaptive Feedback Cell", "comments": "To appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR) methods have recently achieved a remarkable\nsuccess due to the development of deep convolutional neural networks (CNN).\nCurrent state-of-the-art CNN methods usually treat the VSR problem as a large\nnumber of separate multi-frame super-resolution tasks, at which a batch of low\nresolution (LR) frames is utilized to generate a single high resolution (HR)\nframe, and running a slide window to select LR frames over the entire video\nwould obtain a series of HR frames. However, duo to the complex temporal\ndependency between frames, with the number of LR input frames increase, the\nperformance of the reconstructed HR frames become worse. The reason is in that\nthese methods lack the ability to model complex temporal dependencies and hard\nto give an accurate motion estimation and compensation for VSR process. Which\nmakes the performance degrade drastically when the motion in frames is complex.\nIn this paper, we propose a Motion-Adaptive Feedback Cell (MAFC), a simple but\neffective block, which can efficiently capture the motion compensation and feed\nit back to the network in an adaptive way. Our approach efficiently utilizes\nthe information of the inter-frame motion, the dependence of the network on\nmotion estimation and compensation method can be avoid. In addition, benefiting\nfrom the excellent nature of MAFC, the network can achieve better performance\nin the case of extremely complex motion scenarios. Extensive evaluations and\ncomparisons validate the strengths of our approach, and the experimental\nresults demonstrated that the proposed framework is outperform the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 13:14:10 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Xin", "Jingwei", ""], ["Wang", "Nannan", ""], ["Li", "Jie", ""], ["Gao", "Xinbo", ""], ["Li", "Zhifeng", ""]]}, {"id": "2002.06423", "submitter": "Sudip Das", "authors": "Kinjal Dasgupta, Sudip Das, Ujjwal Bhattacharya", "title": "Scale-Invariant Multi-Oriented Text Detection in Wild Scene Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of scene texts in the wild is a challenging problem,\nparticularly due to the difficulties in handling (i) occlusions of varying\npercentages, (ii) widely different scales and orientations, (iii) severe\ndegradations in the image quality etc. In this article, we propose a fully\nconvolutional neural network architecture consisting of a novel Feature\nRepresentation Block (FRB) capable of efficient abstraction of information. The\nproposed network has been trained using curriculum learning with respect to\ndifficulties in image samples and gradual pixel-wise blurring. It is capable of\ndetecting texts of different scales and orientations suffered by blurring from\nmultiple possible sources, non-uniform illumination as well as partial\nocclusions of varying percentages. Text detection performance of the proposed\nframework on various benchmark sample databases including ICDAR 2015, ICDAR\n2017 MLT, COCO-Text and MSRA-TD500 improves respective state-of-the-art results\nsignificantly. Source code of the proposed architecture will be made available\nat github.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 18:34:15 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Dasgupta", "Kinjal", ""], ["Das", "Sudip", ""], ["Bhattacharya", "Ujjwal", ""]]}, {"id": "2002.06429", "submitter": "Sudip Das", "authors": "Sudip Das, Perla Sai Raj Kishore, Ujjwal Bhattacharya", "title": "An End-to-End Framework for Unsupervised Pose Estimation of Occluded\n  Pedestrians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose estimation in the wild is a challenging problem, particularly in\nsituations of (i) occlusions of varying degrees and (ii) crowded outdoor\nscenes. Most of the existing studies of pose estimation did not report the\nperformance in similar situations. Moreover, pose annotations for occluded\nparts of human figures have not been provided in any of the relevant standard\ndatasets which in turn creates further difficulties to the required studies for\npose estimation of the entire figure of occluded humans. Well known pedestrian\ndetection datasets such as CityPersons contains samples of outdoor scenes but\nit does not include pose annotations. Here, we propose a novel multi-task\nframework for end-to-end training towards the entire pose estimation of\npedestrians including in situations of any kind of occlusion. To tackle this\nproblem for training the network, we make use of a pose estimation dataset,\nMS-COCO, and employ unsupervised adversarial instance-level domain adaptation\nfor estimating the entire pose of occluded pedestrians. The experimental\nstudies show that the proposed framework outperforms the SOTA results for pose\nestimation, instance segmentation and pedestrian detection in cases of heavy\nocclusions (HO) and reasonable + heavy occlusions (R + HO) on the two benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 19:00:20 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Das", "Sudip", ""], ["Kishore", "Perla Sai Raj", ""], ["Bhattacharya", "Ujjwal", ""]]}, {"id": "2002.06460", "submitter": "Alfredo Kalaitzis", "authors": "Michel Deudon, Alfredo Kalaitzis, Israel Goytom, Md Rifat Arefin,\n  Zhichao Lin, Kris Sankaran, Vincent Michalski, Samira E. Kahou, Julien\n  Cornebise, Yoshua Bengio", "title": "HighRes-net: Recursive Fusion for Multi-Frame Super-Resolution of\n  Satellite Imagery", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative deep learning has sparked a new wave of Super-Resolution (SR)\nalgorithms that enhance single images with impressive aesthetic results, albeit\nwith imaginary details. Multi-frame Super-Resolution (MFSR) offers a more\ngrounded approach to the ill-posed problem, by conditioning on multiple\nlow-resolution views. This is important for satellite monitoring of human\nimpact on the planet -- from deforestation, to human rights violations -- that\ndepend on reliable imagery. To this end, we present HighRes-net, the first deep\nlearning approach to MFSR that learns its sub-tasks in an end-to-end fashion:\n(i) co-registration, (ii) fusion, (iii) up-sampling, and (iv)\nregistration-at-the-loss. Co-registration of low-resolution views is learned\nimplicitly through a reference-frame channel, with no explicit registration\nmechanism. We learn a global fusion operator that is applied recursively on an\narbitrary number of low-resolution pairs. We introduce a registered loss, by\nlearning to align the SR output to a ground-truth through ShiftNet. We show\nthat by learning deep representations of multiple views, we can super-resolve\nlow-resolution signals and enhance Earth Observation data at scale. Our\napproach recently topped the European Space Agency's MFSR competition on\nreal-world satellite imagery.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 22:17:47 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Deudon", "Michel", ""], ["Kalaitzis", "Alfredo", ""], ["Goytom", "Israel", ""], ["Arefin", "Md Rifat", ""], ["Lin", "Zhichao", ""], ["Sankaran", "Kris", ""], ["Michalski", "Vincent", ""], ["Kahou", "Samira E.", ""], ["Cornebise", "Julien", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2002.06468", "submitter": "Abdullah Nazib", "authors": "Abdullah Nazib, Clinton Fookes, Olivier Salvado, Dimitri Perrin", "title": "A Multiple Decoder CNN for Inverse Consistent 3D Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent application of deep learning technologies in medical image\nregistration has exponentially decreased the registration time and gradually\nincreased registration accuracy when compared to their traditional\ncounterparts. Most of the learning-based registration approaches considers this\ntask as a one directional problem. As a result, only correspondence from the\nmoving image to the target image is considered. However, in some medical\nprocedures bidirectional registration is required to be performed. Unlike other\nlearning-based registration, we propose a registration framework with inverse\nconsistency. The proposed method simultaneously learns forward transformation\nand backward transformation in an unsupervised manner. We perform training and\ntesting of the method on the publicly available LPBA40 MRI dataset and\ndemonstrate strong performance than baseline registration methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 23:23:09 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Nazib", "Abdullah", ""], ["Fookes", "Clinton", ""], ["Salvado", "Olivier", ""], ["Perrin", "Dimitri", ""]]}, {"id": "2002.06478", "submitter": "Tiange Luo", "authors": "Tiange Luo, Kaichun Mo, Zhiao Huang, Jiarui Xu, Siyu Hu, Liwei Wang,\n  Hao Su", "title": "Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen\n  Categories", "comments": "Accepted by ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of discovering 3D parts for objects in unseen\ncategories. Being able to learn the geometry prior of parts and transfer this\nprior to unseen categories pose fundamental challenges on data-driven shape\nsegmentation approaches. Formulated as a contextual bandit problem, we propose\na learning-based agglomerative clustering framework which learns a grouping\npolicy to progressively group small part proposals into bigger ones in a\nbottom-up fashion. At the core of our approach is to restrict the local context\nfor extracting part-level features, which encourages the generalizability to\nunseen categories. On the large-scale fine-grained 3D part dataset, PartNet, we\ndemonstrate that our method can transfer knowledge of parts learned from 3\ntraining categories to 21 unseen testing categories without seeing any\nannotated samples. Quantitative comparisons against four shape segmentation\nbaselines shows that our approach achieve the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 00:23:43 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 04:49:21 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 22:51:58 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Luo", "Tiange", ""], ["Mo", "Kaichun", ""], ["Huang", "Zhiao", ""], ["Xu", "Jiarui", ""], ["Hu", "Siyu", ""], ["Wang", "Liwei", ""], ["Su", "Hao", ""]]}, {"id": "2002.06483", "submitter": "Joseph Robinson", "authors": "Joseph P Robinson, Gennady Livitz, Yann Henon, Can Qin, Yun Fu, and\n  Samson Timoner", "title": "Face Recognition: Too Bias, or Not Too Bias?", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR)\n  Workshops, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reveal critical insights into problems of bias in state-of-the-art facial\nrecognition (FR) systems using a novel Balanced Faces In the Wild (BFW)\ndataset: data balanced for gender and ethnic groups. We show variations in the\noptimal scoring threshold for face-pairs across different subgroups. Thus, the\nconventional approach of learning a global threshold for all pairs resulting in\nperformance gaps among subgroups. By learning subgroup-specific thresholds, we\nnot only mitigate problems in performance gaps but also show a notable boost in\nthe overall performance. Furthermore, we do a human evaluation to measure the\nbias in humans, which supports the hypothesis that such a bias exists in human\nperception. For the BFW database, source code, and more, visit\ngithub.com/visionjo/facerec-bias-bfw.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 01:08:12 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 15:57:51 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 04:18:53 GMT"}, {"version": "v4", "created": "Tue, 21 Apr 2020 01:34:11 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Robinson", "Joseph P", ""], ["Livitz", "Gennady", ""], ["Henon", "Yann", ""], ["Qin", "Can", ""], ["Fu", "Yun", ""], ["Timoner", "Samson", ""]]}, {"id": "2002.06515", "submitter": "Xiaowen Shi", "authors": "Xiaowen Shi, Xin Li, Caili Wu, Shuchen Kong, Jing Yang, Liang He", "title": "A Real-Time Deep Network for Crowd Counting", "comments": null, "journal-ref": "IEEE ICASSP 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic analysis of highly crowded people has attracted extensive attention\nfrom computer vision research. Previous approaches for crowd counting have\nalready achieved promising performance across various benchmarks. However, to\ndeal with the real situation, we hope the model run as fast as possible while\nkeeping accuracy. In this paper, we propose a compact convolutional neural\nnetwork for crowd counting which learns a more efficient model with a small\nnumber of parameters. With three parallel filters executing the convolutional\noperation on the input image simultaneously at the front of the network, our\nmodel could achieve nearly real-time speed and save more computing resources.\nExperiments on two benchmarks show that our proposed method not only takes a\nbalance between performance and efficiency which is more suitable for actual\nscenes but also is superior to existing light-weight models in speed.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 06:09:22 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Shi", "Xiaowen", ""], ["Li", "Xin", ""], ["Wu", "Caili", ""], ["Kong", "Shuchen", ""], ["Yang", "Jing", ""], ["He", "Liang", ""]]}, {"id": "2002.06518", "submitter": "Jingwei Xin", "authors": "Jingwei Xin, Nannan Wang, Xinrui Jiang, Jie Li, Xinbo Gao, Zhifeng Li", "title": "Facial Attribute Capsules for Noise Face Super Resolution", "comments": "To appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing face super-resolution (SR) methods mainly assume the input image to\nbe noise-free. Their performance degrades drastically when applied to\nreal-world scenarios where the input image is always contaminated by noise. In\nthis paper, we propose a Facial Attribute Capsules Network (FACN) to deal with\nthe problem of high-scale super-resolution of noisy face image. Capsule is a\ngroup of neurons whose activity vector models different properties of the same\nentity. Inspired by the concept of capsule, we propose an integrated\nrepresentation model of facial information, which named Facial Attribute\nCapsule (FAC). In the SR processing, we first generated a group of FACs from\nthe input LR face, and then reconstructed the HR face from this group of FACs.\nAiming to effectively improve the robustness of FAC to noise, we generate FAC\nin semantic, probabilistic and facial attributes manners by means of integrated\nlearning strategy. Each FAC can be divided into two sub-capsules: Semantic\nCapsule (SC) and Probabilistic Capsule (PC). Them describe an explicit facial\nattribute in detail from two aspects of semantic representation and probability\ndistribution. The group of FACs model an image as a combination of facial\nattribute information in the semantic space and probabilistic space by an\nattribute-disentangling way. The diverse FACs could better combine the face\nprior information to generate the face images with fine-grained semantic\nattributes. Extensive benchmark experiments show that our method achieves\nsuperior hallucination results and outperforms state-of-the-art for very low\nresolution (LR) noise face image super resolution.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 06:22:28 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Xin", "Jingwei", ""], ["Wang", "Nannan", ""], ["Jiang", "Xinrui", ""], ["Li", "Jie", ""], ["Gao", "Xinbo", ""], ["Li", "Zhifeng", ""]]}, {"id": "2002.06575", "submitter": "Udit Singh Parihar", "authors": "Sai Shubodh Puligilla, Satyajit Tourani, Tushar Vaidya, Udit Singh\n  Parihar, Ravi Kiran Sarvadevabhatla and K. Madhava Krishna", "title": "Topological Mapping for Manhattan-like Repetitive Environments", "comments": "Accepted to ICRA 2020. Project Page:\n  https://github.com/Shubodh/ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We showcase a topological mapping framework for a challenging indoor\nwarehouse setting. At the most abstract level, the warehouse is represented as\na Topological Graph where the nodes of the graph represent a particular\nwarehouse topological construct (e.g. rackspace, corridor) and the edges denote\nthe existence of a path between two neighbouring nodes or topologies. At the\nintermediate level, the map is represented as a Manhattan Graph where the nodes\nand edges are characterized by Manhattan properties and as a Pose Graph at the\nlower-most level of detail. The topological constructs are learned via a Deep\nConvolutional Network while the relational properties between topological\ninstances are learnt via a Siamese-style Neural Network. In the paper, we show\nthat maintaining abstractions such as Topological Graph and Manhattan Graph\nhelp in recovering an accurate Pose Graph starting from a highly erroneous and\nunoptimized Pose Graph. We show how this is achieved by embedding topological\nand Manhattan relations as well as Manhattan Graph aided loop closure relations\nas constraints in the backend Pose Graph optimization framework. The recovery\nof near ground-truth Pose Graph on real-world indoor warehouse scenes vindicate\nthe efficacy of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 13:20:28 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 00:35:32 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2020 16:11:21 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Puligilla", "Sai Shubodh", ""], ["Tourani", "Satyajit", ""], ["Vaidya", "Tushar", ""], ["Parihar", "Udit Singh", ""], ["Sarvadevabhatla", "Ravi Kiran", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2002.06583", "submitter": "Arantxa Casanova", "authors": "Arantxa Casanova, Pedro O. Pinheiro, Negar Rostamzadeh, Christopher J.\n  Pal", "title": "Reinforced active learning for image segmentation", "comments": "Accepted to ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based approaches for semantic segmentation have two inherent\nchallenges. First, acquiring pixel-wise labels is expensive and time-consuming.\nSecond, realistic segmentation datasets are highly unbalanced: some categories\nare much more abundant than others, biasing the performance to the most\nrepresented ones. In this paper, we are interested in focusing human labelling\neffort on a small subset of a larger pool of data, minimizing this effort while\nmaximizing performance of a segmentation model on a hold-out set. We present a\nnew active learning strategy for semantic segmentation based on deep\nreinforcement learning (RL). An agent learns a policy to select a subset of\nsmall informative image regions -- opposed to entire images -- to be labeled,\nfrom a pool of unlabeled data. The region selection decision is made based on\npredictions and uncertainties of the segmentation model being trained. Our\nmethod proposes a new modification of the deep Q-network (DQN) formulation for\nactive learning, adapting it to the large-scale nature of semantic segmentation\nproblems. We test the proof of concept in CamVid and provide results in the\nlarge-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN\napproach requires roughly 30% less additional labeled data than our most\ncompetitive baseline to reach the same performance. Moreover, we find that our\nmethod asks for more labels of under-represented categories compared to the\nbaselines, improving their performance and helping to mitigate class imbalance.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 14:03:06 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Casanova", "Arantxa", ""], ["Pinheiro", "Pedro O.", ""], ["Rostamzadeh", "Negar", ""], ["Pal", "Christopher J.", ""]]}, {"id": "2002.06588", "submitter": "David Wood", "authors": "David A. Wood, Jeremy Lynch, Sina Kafiabadi, Emily Guilhem, Aisha Al\n  Busaidi, Antanas Montvila, Thomas Varsavsky, Juveria Siddiqui, Naveen Gadapa,\n  Matthew Townend, Martin Kiik, Keena Patel, Gareth Barker, Sebastian Ourselin,\n  James H. Cole, Thomas C. Booth", "title": "Automated Labelling using an Attention model for Radiology reports of\n  MRI scans (ALARM)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labelling large datasets for training high-capacity neural networks is a\nmajor obstacle to the development of deep learning-based medical imaging\napplications. Here we present a transformer-based network for magnetic\nresonance imaging (MRI) radiology report classification which automates this\ntask by assigning image labels on the basis of free-text expert radiology\nreports. Our model's performance is comparable to that of an expert\nradiologist, and better than that of an expert physician, demonstrating the\nfeasibility of this approach. We make code available online for researchers to\nlabel their own MRI datasets for medical imaging applications.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 15:04:52 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Wood", "David A.", ""], ["Lynch", "Jeremy", ""], ["Kafiabadi", "Sina", ""], ["Guilhem", "Emily", ""], ["Busaidi", "Aisha Al", ""], ["Montvila", "Antanas", ""], ["Varsavsky", "Thomas", ""], ["Siddiqui", "Juveria", ""], ["Gadapa", "Naveen", ""], ["Townend", "Matthew", ""], ["Kiik", "Martin", ""], ["Patel", "Keena", ""], ["Barker", "Gareth", ""], ["Ourselin", "Sebastian", ""], ["Cole", "James H.", ""], ["Booth", "Thomas C.", ""]]}, {"id": "2002.06597", "submitter": "Kui Jia", "authors": "Jiabao Lei and Kui Jia", "title": "Analytic Marching: An Analytic Meshing Solution from Deep Implicit\n  Surface Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a problem of learning surface mesh via implicit functions\nin an emerging field of deep learning surface reconstruction, where implicit\nfunctions are popularly implemented as multi-layer perceptrons (MLPs) with\nrectified linear units (ReLU). To achieve meshing from learned implicit\nfunctions, existing methods adopt the de-facto standard algorithm of marching\ncubes; while promising, they suffer from loss of precision learned in the MLPs,\ndue to the discretization nature of marching cubes. Motivated by the knowledge\nthat a ReLU based MLP partitions its input space into a number of linear\nregions, we identify from these regions analytic cells and analytic faces that\nare associated with zero-level isosurface of the implicit function, and\ncharacterize the theoretical conditions under which the identified analytic\nfaces are guaranteed to connect and form a closed, piecewise planar surface.\nBased on our theorem, we propose a naturally parallelizable algorithm of\nanalytic marching, which marches among analytic cells to exactly recover the\nmesh captured by a learned MLP. Experiments on deep learning mesh\nreconstruction verify the advantages of our algorithm over existing ones.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 15:36:19 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Lei", "Jiabao", ""], ["Jia", "Kui", ""]]}, {"id": "2002.06600", "submitter": "Ping Guo", "authors": "Jiali Xu, Qian Yin, Ping Guo, and Xin Zheng", "title": "Two-dimensional Multi-fiber Spectrum Image Correction Based on Machine\n  Learning Techniques", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": "10.1093/mnras/staa2883", "report-no": null, "categories": "astro-ph.IM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to limited size and imperfect of the optical components in a\nspectrometer, aberration has inevitably been brought into two-dimensional\nmulti-fiber spectrum image in LAMOST, which leads to obvious spacial variation\nof the point spread functions (PSFs). Consequently, if spatial variant PSFs are\nestimated directly , the huge storage and intensive computation requirements\nresult in deconvolutional spectral extraction method become intractable. In\nthis paper, we proposed a novel method to solve the problem of spatial\nvariation PSF through image aberration correction. When CCD image aberration is\ncorrected, PSF, the convolution kernel, can be approximated by one spatial\ninvariant PSF only. Specifically, machine learning techniques are adopted to\ncalibrate distorted spectral image, including Total Least Squares (TLS)\nalgorithm, intelligent sampling method, multi-layer feed-forward neural\nnetworks. The calibration experiments on the LAMOST CCD images show that the\ncalibration effect of proposed method is effectible. At the same time, the\nspectrum extraction results before and after calibration are compared, results\nshow the characteristics of the extracted one-dimensional waveform are more\nclose to an ideal optics system, and the PSF of the corrected object spectrum\nimage estimated by the blind deconvolution method is nearly central symmetry,\nwhich indicates that our proposed method can significantly reduce the\ncomplexity of spectrum extraction and improve extraction accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 15:39:09 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Xu", "Jiali", ""], ["Yin", "Qian", ""], ["Guo", "Ping", ""], ["Zheng", "Xin", ""]]}, {"id": "2002.06604", "submitter": "Yeongmin Ko", "authors": "Yeongmin Ko, Younkwan Lee, Shoaib Azam, Farzeen Munir, Moongu Jeon,\n  and Witold Pedrycz", "title": "Key Points Estimation and Point Instance Segmentation Approach for Lane\n  Detection", "comments": "Submitted to \"IEEE Transactions on Intelligent Transportation\n  Systems\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception techniques for autonomous driving should be adaptive to various\nenvironments. In the case of traffic line detection, an essential perception\nmodule, many condition should be considered, such as number of traffic lines\nand computing power of the target system. To address these problems, in this\npaper, we propose a traffic line detection method called Point Instance Network\n(PINet); the method is based on the key points estimation and instance\nsegmentation approach. The PINet includes several stacked hourglass networks\nthat are trained simultaneously. Therefore the size of the trained models can\nbe chosen according to the computing power of the target environment. We cast a\nclustering problem of the predicted key points as an instance segmentation\nproblem; the PINet can be trained regardless of the number of the traffic\nlines. The PINet achieves competitive accuracy and false positive on the\nTuSimple and Culane datasets, popular public datasets for lane detection. Our\ncode is available at https://github.com/koyeongmin/PINet_new\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 15:51:30 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 18:05:40 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 05:56:37 GMT"}, {"version": "v4", "created": "Mon, 14 Sep 2020 03:22:44 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ko", "Yeongmin", ""], ["Lee", "Younkwan", ""], ["Azam", "Shoaib", ""], ["Munir", "Farzeen", ""], ["Jeon", "Moongu", ""], ["Pedrycz", "Witold", ""]]}, {"id": "2002.06619", "submitter": "Mayanka Chandrashekar", "authors": "Mayanka Chandrashekar and Yugyung Lee", "title": "CRL: Class Representative Learning for Image Classification", "comments": "15 pages, Table 8, Figure 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building robust and real-time classifiers with diverse datasets are one of\nthe most significant challenges to deep learning researchers. It is because\nthere is a considerable gap between a model built with training (seen) data and\nreal (unseen) data in applications. Recent works including Zero-Shot Learning\n(ZSL), have attempted to deal with this problem of overcoming the apparent gap\nthrough transfer learning. In this paper, we propose a novel model, called\nClass Representative Learning Model (CRL), that can be especially effective in\nimage classification influenced by ZSL. In the CRL model, first, the learning\nstep is to build class representatives to represent classes in datasets by\naggregating prominent features extracted from a Convolutional Neural Network\n(CNN). Second, the inferencing step in CRL is to match between the class\nrepresentatives and new data. The proposed CRL model demonstrated superior\nperformance compared to the current state-of-the-art research in ZSL and mobile\ndeep learning. The proposed CRL model has been implemented and evaluated in a\nparallel environment, using Apache Spark, for both distributed learning and\nrecognition. An extensive experimental study on the benchmark datasets,\nImageNet-1K, CalTech-101, CalTech-256, CIFAR-100, shows that CRL can build a\nclass distribution model with drastic improvement in learning and recognition\nperformance without sacrificing accuracy compared to the state-of-the-art\nperformances in image classification.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 17:02:59 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Chandrashekar", "Mayanka", ""], ["Lee", "Yugyung", ""]]}, {"id": "2002.06626", "submitter": "Hubert Lin", "authors": "Hubert Lin, Paul Upchurch, Kavita Bala", "title": "Block Annotation: Better Image Annotation for Semantic Segmentation with\n  Sub-Image Decomposition", "comments": "ICCV 2019; http://www.cs.cornell.edu/~hubert/block_annotation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image datasets with high-quality pixel-level annotations are valuable for\nsemantic segmentation: labelling every pixel in an image ensures that rare\nclasses and small objects are annotated. However, full-image annotations are\nexpensive, with experts spending up to 90 minutes per image. We propose block\nsub-image annotation as a replacement for full-image annotation. Despite the\nattention cost of frequent task switching, we find that block annotations can\nbe crowdsourced at higher quality compared to full-image annotation with equal\nmonetary cost using existing annotation tools developed for full-image\nannotation. Surprisingly, we find that 50% pixels annotated with blocks allows\nsemantic segmentation to achieve equivalent performance to 100% pixels\nannotated. Furthermore, as little as 12% of pixels annotated allows performance\nas high as 98% of the performance with dense annotation. In weakly-supervised\nsettings, block annotation outperforms existing methods by 3-4% (absolute)\ngiven equivalent annotation time. To recover the necessary global structure for\napplications such as characterizing spatial context and affordance\nrelationships, we propose an effective method to inpaint block-annotated images\nwith high-quality labels without additional human effort. As such, fewer\nannotations can also be used for these applications compared to full-image\nannotation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 17:42:37 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Lin", "Hubert", ""], ["Upchurch", "Paul", ""], ["Bala", "Kavita", ""]]}, {"id": "2002.06650", "submitter": "Alejandro Flores Velazco", "authors": "Alejandro Flores-Velazco, David M. Mount", "title": "Coresets for the Nearest-Neighbor Rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a training set $P$ of labeled points, the nearest-neighbor rule\npredicts the class of an unlabeled query point as the label of its closest\npoint in the set. To improve the time and space complexity of classification, a\nnatural question is how to reduce the training set without significantly\naffecting the accuracy of the nearest-neighbor rule. Nearest-neighbor\ncondensation deals with finding a subset $R \\subseteq P$ such that for every\npoint $p \\in P$, $p$'s nearest-neighbor in $R$ has the same label as $p$. This\nrelates to the concept of coresets, which can be broadly defined as subsets of\nthe set, such that an exact result on the coreset corresponds to an approximate\nresult on the original set. However, the guarantees of a coreset hold for any\nquery point, and not only for the points of the training set.\n  This paper introduces the concept of coresets for nearest-neighbor\nclassification. We extend existing criteria used for condensation, and prove\nsufficient conditions to correctly classify any query point when using these\nsubsets. Additionally, we prove that finding such subsets of minimum\ncardinality is NP-hard, and propose quadratic-time approximation algorithms\nwith provable upper-bounds on the size of their selected subsets. Moreover, we\nshow how to improve one of these algorithms to have subquadratic runtime, being\nthe first of this kind for condensation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 19:00:48 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 21:07:11 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 19:14:41 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Flores-Velazco", "Alejandro", ""], ["Mount", "David M.", ""]]}, {"id": "2002.06661", "submitter": "Shweta Mahajan", "authors": "Shweta Mahajan, Iryna Gurevych, Stefan Roth", "title": "Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned joint representations of images and text form the backbone of several\nimportant cross-domain tasks such as image captioning. Prior work mostly maps\nboth domains into a common latent representation in a purely supervised\nfashion. This is rather restrictive, however, as the two domains follow\ndistinct generative processes. Therefore, we propose a novel semi-supervised\nframework, which models shared information between domains and domain-specific\ninformation separately. The information shared between the domains is aligned\nwith an invertible neural network. Our model integrates normalizing flow-based\npriors for the domain-specific information, which allows us to learn diverse\nmany-to-many mappings between the two domains. We demonstrate the effectiveness\nof our model on diverse tasks, including image captioning and text-to-image\nsynthesis.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 19:49:30 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Mahajan", "Shweta", ""], ["Gurevych", "Iryna", ""], ["Roth", "Stefan", ""]]}, {"id": "2002.06664", "submitter": "Rohan Chacko", "authors": "Sai Sagar Jinka, Rohan Chacko, Avinash Sharma and P. J. Narayanan", "title": "PeeledHuman: Robust Shape Representation for Textured 3D Human Body\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PeeledHuman - a novel shape representation of the human body\nthat is robust to self-occlusions. PeeledHuman encodes the human body as a set\nof Peeled Depth and RGB maps in 2D, obtained by performing ray-tracing on the\n3D body model and extending each ray beyond its first intersection. This\nformulation allows us to handle self-occlusions efficiently compared to other\nrepresentations. Given a monocular RGB image, we learn these Peeled maps in an\nend-to-end generative adversarial fashion using our novel framework - PeelGAN.\nWe train PeelGAN using a 3D Chamfer loss and other 2D losses to generate\nmultiple depth values per-pixel and a corresponding RGB field per-vertex in a\ndual-branch setup. In our simple non-parametric solution, the generated Peeled\nDepth maps are back-projected to 3D space to obtain a complete textured 3D\nshape. The corresponding RGB maps provide vertex-level texture details. We\ncompare our method with current parametric and non-parametric methods in 3D\nreconstruction and find that we achieve state-of-the-art-results. We\ndemonstrate the effectiveness of our representation on publicly available BUFF\nand MonoPerfCap datasets as well as loose clothing data collected by our\ncalibrated multi-Kinect setup.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 20:03:24 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 10:06:42 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Jinka", "Sai Sagar", ""], ["Chacko", "Rohan", ""], ["Sharma", "Avinash", ""], ["Narayanan", "P. J.", ""]]}, {"id": "2002.06666", "submitter": "Huynh Manh", "authors": "Manh Huynh, Gita Alaghband", "title": "AOL: Adaptive Online Learning for Human Trajectory Prediction in Dynamic\n  Video Scenes", "comments": "Accepted to BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel adaptive online learning (AOL) framework to predict human\nmovement trajectories in dynamic video scenes. Our framework learns and adapts\nto changes in the scene environment and generates best network weights for\ndifferent scenarios. The framework can be applied to prediction models and\nimprove their performance as it dynamically adjusts when it encounters changes\nin the scene and can apply the best training weights for predicting the next\nlocations. We demonstrate this by integrating our framework with two existing\nprediction models: LSTM [3] and Future Person Location (FPL) [1]. Furthermore,\nwe analyze the number of network weights for optimal performance and show that\nwe can achieve real-time with a fixed number of networks using the least\nrecently used (LRU) strategy for maintaining the most recently trained network\nweights. With extensive experiments, we show that our framework increases\nprediction accuracies of LSTM and FPL by ~17% and 28% on average, and up to\n~50% for FPL on the worst case while achieving real-time (20fps).\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 20:11:32 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 21:20:34 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Huynh", "Manh", ""], ["Alaghband", "Gita", ""]]}, {"id": "2002.06682", "submitter": "Nao Takano", "authors": "Nao Takano and Gita Alaghband", "title": "Generator From Edges: Reconstruction of Facial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications that involve supervised training require paired images.\nResearchers of single image super-resolution (SISR) create such images by\nartificially generating blurry input images from the corresponding ground\ntruth. Similarly we can create paired images with the canny edge. We propose\nGenerator From Edges (GFE) [Figure 2]. Our aim is to determine the best\narchitecture for GFE, along with reviews of perceptual loss [1, 2]. To this\nend, we conducted three experiments. First, we explored the effects of the\nadversarial loss often used in SISR. In particular, we uncovered that it is not\nan essential component to form a perceptual loss. Eliminating adversarial loss\nwill lead to a more effective architecture from the perspective of hardware\nresource. It also means that considerations for the problems pertaining to\ngenerative adversarial network (GAN) [3], such as mode collapse, are not\nnecessary. Second, we reexamined VGG loss and found that the mid-layers yield\nthe best results. By extracting the full potential of VGG loss, the overall\nperformance of perceptual loss improves significantly. Third, based on the\nfindings of the first two experiments, we reevaluated the dense network to\nconstruct GFE. Using GFE as an intermediate process, reconstructing a facial\nimage from a pencil sketch can become an easy task.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 21:18:04 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 00:10:52 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 04:44:05 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Takano", "Nao", ""], ["Alaghband", "Gita", ""]]}, {"id": "2002.06701", "submitter": "Chiranjib Sur", "authors": "Chiranjib Sur", "title": "Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic\n  Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we have introduced Gaussian Smoothen Semantic Features (GSSF)\nfor Better Semantic Selection for Indian regional language-based image\ncaptioning and introduced a procedure where we used the existing translation\nand English crowd-sourced sentences for training. We have shown that this\narchitecture is a promising alternative source, where there is a crunch in\nresources. Our main contribution of this work is the development of deep\nlearning architectures for the Bengali language (is the fifth widely spoken\nlanguage in the world) with a completely different grammar and language\nattributes. We have shown that these are working well for complex applications\nlike language generation from image contexts and can diversify the\nrepresentation through introducing constraints, more extensive features, and\nunique feature spaces. We also established that we could achieve absolute\nprecision and diversity when we use smoothened semantic tensor with the\ntraditional LSTM and feature decomposition networks. With better learning\narchitecture, we succeeded in establishing an automated algorithm and\nassessment procedure that can help in the evaluation of competent applications\nwithout the requirement for expertise and human intervention.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 23:03:32 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Sur", "Chiranjib", ""]]}, {"id": "2002.06735", "submitter": "George - Cosmin Poru\\c{s}niuc", "authors": "Alexandru \\c{S}erban, George Ila\\c{s}, George-Cosmin Poru\\c{s}niuc", "title": "SpotTheFake: An Initial Report on a New CNN-Enhanced Platform for\n  Counterfeit Goods Detection", "comments": "7 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The counterfeit goods trade represents nowadays more than 3.3% of the whole\nworld trade and thus it's a problem that needs now more than ever a lot of\nattention and a reliable solution that would reduce the negative impact it has\nover the modern society. This paper presents the design and early stage\ndevelopment of a novel counterfeit goods detection platform that makes use of\nthe outstsanding learning capabilities of the classical VGG16 convolutional\nmodel trained through the process of \"transfer learning\" and a multi-stage fake\ndetection procedure that proved to be not only reliable but also very robust in\nthe experiments we have conducted so far using an image dataset of various\ngoods which we gathered ourselves.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 01:51:22 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:36:31 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["\u015eerban", "Alexandru", ""], ["Ila\u015f", "George", ""], ["Poru\u015fniuc", "George-Cosmin", ""]]}, {"id": "2002.06736", "submitter": "Yingjie Yin", "authors": "Yingjie Yin, De Xu, Xingang Wang and Lei Zhang", "title": "Directional Deep Embedding and Appearance Learning for Fast Video Object\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent semi-supervised video object segmentation (VOS) methods rely on\nfine-tuning deep convolutional neural networks online using the given mask of\nthe first frame or predicted masks of subsequent frames. However, the online\nfine-tuning process is usually time-consuming, limiting the practical use of\nsuch methods. We propose a directional deep embedding and appearance learning\n(DDEAL) method, which is free of the online fine-tuning process, for fast VOS.\nFirst, a global directional matching module, which can be efficiently\nimplemented by parallel convolutional operations, is proposed to learn a\nsemantic pixel-wise embedding as an internal guidance. Second, an effective\ndirectional appearance model based statistics is proposed to represent the\ntarget and background on a spherical embedding space for VOS. Equipped with the\nglobal directional matching module and the directional appearance model\nlearning module, DDEAL learns static cues from the labeled first frame and\ndynamically updates cues of the subsequent frames for object segmentation. Our\nmethod exhibits state-of-the-art VOS performance without using online\nfine-tuning. Specifically, it achieves a J & F mean score of 74.8% on DAVIS\n2017 dataset and an overall score G of 71.3% on the large-scale YouTube-VOS\ndataset, while retaining a speed of 25 fps with a single NVIDIA TITAN Xp GPU.\nFurthermore, our faster version runs 31 fps with only a little accuracy loss.\nOur code and trained networks are available at\nhttps://github.com/YingjieYin/Directional-Deep-Embedding-and-Appearance-Learning-for-Fast-Video-Object-Segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 01:51:57 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Yin", "Yingjie", ""], ["Xu", "De", ""], ["Wang", "Xingang", ""], ["Zhang", "Lei", ""]]}, {"id": "2002.06753", "submitter": "Micah Goldblum", "authors": "Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia\n  Cherepanova, Tom Goldstein", "title": "Unraveling Meta-Learning: Understanding Feature Representations for\n  Few-Shot Tasks", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning algorithms produce feature extractors which achieve\nstate-of-the-art performance on few-shot classification. While the literature\nis rich with meta-learning methods, little is known about why the resulting\nfeature extractors perform so well. We develop a better understanding of the\nunderlying mechanics of meta-learning and the difference between models trained\nusing meta-learning and models which are trained classically. In doing so, we\nintroduce and verify several hypotheses for why meta-learned models perform\nbetter. Furthermore, we develop a regularizer which boosts the performance of\nstandard training routines for few-shot classification. In many cases, our\nroutine outperforms meta-learning while simultaneously running an order of\nmagnitude faster.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 03:18:45 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 22:50:45 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 13:59:50 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Goldblum", "Micah", ""], ["Reich", "Steven", ""], ["Fowl", "Liam", ""], ["Ni", "Renkun", ""], ["Cherepanova", "Valeriia", ""], ["Goldstein", "Tom", ""]]}, {"id": "2002.06765", "submitter": "Teppei Suzuki", "authors": "Teppei Suzuki", "title": "Superpixel Segmentation via Convolutional Neural Networks with\n  Regularized Information Maximization", "comments": "To appear in ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised superpixel segmentation method by optimizing a\nrandomly-initialized convolutional neural network (CNN) in inference time. Our\nmethod generates superpixels via CNN from a single image without any labels by\nminimizing a proposed objective function for superpixel segmentation in\ninference time. There are three advantages to our method compared with many of\nexisting methods: (i) leverages an image prior of CNN for superpixel\nsegmentation, (ii) adaptively changes the number of superpixels according to\nthe given images, and (iii) controls the property of superpixels by adding an\nauxiliary cost to the objective function. We verify the advantages of our\nmethod quantitatively and qualitatively on BSDS500 and SBD datasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 04:32:03 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 02:12:09 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 14:02:13 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Suzuki", "Teppei", ""]]}, {"id": "2002.06770", "submitter": "Wanyi Li", "authors": "Wanyi Li, Fuyu Li, Yongkang Luo, Peng Wang", "title": "Unsupervised Image-generation Enhanced Adaptation for Object Detection\n  in Thermal images", "comments": "5 pages, 4 figures, submit to ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in thermal images is an important computer vision task and\nhas many applications such as unmanned vehicles, robotics, surveillance and\nnight vision. Deep learning based detectors have achieved major progress, which\nusually need large amount of labelled training data. However, labelled data for\nobject detection in thermal images is scarce and expensive to collect. How to\ntake advantage of the large number labelled visible images and adapt them into\nthermal image domain, is expected to solve. This paper proposes an unsupervised\nimage-generation enhanced adaptation method for object detection in thermal\nimages. To reduce the gap between visible domain and thermal domain, the\nproposed method manages to generate simulated fake thermal images that are\nsimilar to the target images, and preserves the annotation information of the\nvisible source domain. The image generation includes a CycleGAN based\nimage-to-image translation and an intensity inversion transformation. Generated\nfake thermal images are used as renewed source domain. And then the\noff-the-shelf Domain Adaptive Faster RCNN is utilized to reduce the gap between\ngenerated intermediate domain and the thermal target domain. Experiments\ndemonstrate the effectiveness and superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 04:53:30 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Li", "Wanyi", ""], ["Li", "Fuyu", ""], ["Luo", "Yongkang", ""], ["Wang", "Peng", ""]]}, {"id": "2002.06797", "submitter": "Wanyi Li", "authors": "Wanyi Li, Fuyu Li, Yongkang Luo, Peng Wang and Jia sun", "title": "Deep Domain Adaptive Object Detection: a Survey", "comments": "Accepted by IEEE SSCI 2020, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) based object detection has achieved great progress. These\nmethods typically assume that large amount of labeled training data is\navailable, and training and test data are drawn from an identical distribution.\nHowever, the two assumptions are not always hold in practice. Deep domain\nadaptive object detection (DDAOD) has emerged as a new learning paradigm to\naddress the above mentioned challenges. This paper aims to review the\nstate-of-the-art progress on deep domain adaptive object detection approaches.\nFirstly, we introduce briefly the basic concepts of deep domain adaptation.\nSecondly, the deep domain adaptive detectors are classified into five\ncategories and detailed descriptions of representative methods in each category\nare provided. Finally, insights for future research trend are presented.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 06:40:19 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 08:38:35 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 06:29:01 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Li", "Wanyi", ""], ["Li", "Fuyu", ""], ["Luo", "Yongkang", ""], ["Wang", "Peng", ""], ["sun", "Jia", ""]]}, {"id": "2002.06800", "submitter": "A Mishra", "authors": "Aakansha Mishra, Ashish Anand and Prithwijit Guha", "title": "CQ-VQA: Visual Question Answering on Categorized Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes CQ-VQA, a novel 2-level hierarchical but end-to-end model\nto solve the task of visual question answering (VQA). The first level of\nCQ-VQA, referred to as question categorizer (QC), classifies questions to\nreduce the potential answer search space. The QC uses attended and fused\nfeatures of the input question and image. The second level, referred to as\nanswer predictor (AP), comprises of a set of distinct classifiers corresponding\nto each question category. Depending on the question category predicted by QC,\nonly one of the classifiers of AP remains active. The loss functions of QC and\nAP are aggregated together to make it an end-to-end model. The proposed model\n(CQ-VQA) is evaluated on the TDIUC dataset and is benchmarked against\nstate-of-the-art approaches. Results indicate competitive or better performance\nof CQ-VQA.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 06:45:29 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Mishra", "Aakansha", ""], ["Anand", "Ashish", ""], ["Guha", "Prithwijit", ""]]}, {"id": "2002.06806", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Efe Bozkir, Enkelejda Kasneci", "title": "Reinforcement learning for the privacy preservation and manipulation of\n  eye tracking data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach based on reinforcement learning for eye\ntracking data manipulation. It is based on two opposing agents, where one tries\nto classify the data correctly and the second agent looks for patterns in the\ndata, which get manipulated to hide specific information. We show that our\napproach is successfully applicable to preserve the privacy of the subjects.\nFor this purpose, we evaluate our approach iteratively to showcase the behavior\nof the reinforcement learning based approach. In addition, we evaluate the\nimportance of temporal, as well as spatial, information of eye tracking data\nfor specific classification goals. In the last part of our evaluation, we apply\nthe procedure to further public data sets without re-training the autoencoder\nor the data manipulator. The results show that the learned manipulation is\ngeneralized and applicable to unseen data as well.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 07:02:19 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 06:41:49 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Bozkir", "Efe", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2002.06810", "submitter": "Zhaohui Yang", "authors": "Zhaohui Yang, Yunhe Wang, Chang Xu, Peng Du, Chao Xu, Chunjing Xu, Qi\n  Tian", "title": "Discernible Image Compression", "comments": "Accepted by ACMMM 2020", "journal-ref": null, "doi": "10.1145/3394171.3413968", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compression, as one of the fundamental low-level image processing\ntasks, is very essential for computer vision. Tremendous computing and storage\nresources can be preserved with a trivial amount of visual information.\nConventional image compression methods tend to obtain compressed images by\nminimizing their appearance discrepancy with the corresponding original images,\nbut pay little attention to their efficacy in downstream perception tasks,\ne.g., image recognition and object detection. Thus, some of compressed images\ncould be recognized with bias. In contrast, this paper aims to produce\ncompressed images by pursuing both appearance and perceptual consistency. Based\non the encoder-decoder framework, we propose using a pre-trained CNN to extract\nfeatures of the original and compressed images, and making them similar. Thus\nthe compressed images are discernible to subsequent tasks, and we name our\nmethod as Discernible Image Compression (DIC). In addition, the maximum mean\ndiscrepancy (MMD) is employed to minimize the difference between feature\ndistributions. The resulting compression network can generate images with high\nimage quality and preserve the consistent perception in the feature domain, so\nthat these images can be well recognized by pre-trained machine learning\nmodels. Experiments on benchmarks demonstrate that images compressed by using\nthe proposed method can also be well recognized by subsequent visual\nrecognition and detection models. For instance, the mAP value of compressed\nimages by DIC is about 0.6% higher than that of using compressed images by\nconventional methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 07:35:08 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 13:54:31 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2020 00:44:12 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Yang", "Zhaohui", ""], ["Wang", "Yunhe", ""], ["Xu", "Chang", ""], ["Du", "Peng", ""], ["Xu", "Chao", ""], ["Xu", "Chunjing", ""], ["Tian", "Qi", ""]]}, {"id": "2002.06815", "submitter": "Minsung Hyun", "authors": "Minsung Hyun, Jisoo Jeong and Nojun Kwak", "title": "Class-Imbalanced Semi-Supervised Learning", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Supervised Learning (SSL) has achieved great success in overcoming the\ndifficulties of labeling and making full use of unlabeled data. However, SSL\nhas a limited assumption that the numbers of samples in different classes are\nbalanced, and many SSL algorithms show lower performance for the datasets with\nthe imbalanced class distribution. In this paper, we introduce a task of\nclass-imbalanced semi-supervised learning (CISSL), which refers to\nsemi-supervised learning with class-imbalanced data. In doing so, we consider\nclass imbalance in both labeled and unlabeled sets. First, we analyze existing\nSSL methods in imbalanced environments and examine how the class imbalance\naffects SSL methods. Then we propose Suppressed Consistency Loss (SCL), a\nregularization method robust to class imbalance. Our method shows better\nperformance than the conventional methods in the CISSL environment. In\nparticular, the more severe the class imbalance and the smaller the size of the\nlabeled data, the better our method performs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 07:48:47 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Hyun", "Minsung", ""], ["Jeong", "Jisoo", ""], ["Kwak", "Nojun", ""]]}, {"id": "2002.06816", "submitter": "Pamela K. Douglas", "authors": "Pamela K. Douglas, Farzad Vasheghani Farahani", "title": "On the Similarity of Deep Learning Representations Across Didactic and\n  Adversarial Examples", "comments": "2 figures", "journal-ref": "Med NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing use of deep neural networks (DNNs) has motivated a parallel\nendeavor: the design of adversaries that profit from successful\nmisclassifications. However, not all adversarial examples are crafted for\nmalicious purposes. For example, real world systems often contain physical,\ntemporal, and sampling variability across instrumentation. Adversarial examples\nin the wild may inadvertently prove deleterious for accurate predictive\nmodeling. Conversely, naturally occurring covariance of image features may\nserve didactic purposes. Here, we studied the stability of deep learning\nrepresentations for neuroimaging classification across didactic and adversarial\nconditions characteristic of MRI acquisition variability. We show that\nrepresentational similarity and performance vary according to the frequency of\nadversarial examples in the input space.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 07:49:20 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Douglas", "Pamela K.", ""], ["Farahani", "Farzad Vasheghani", ""]]}, {"id": "2002.06820", "submitter": "Zhanzhan Cheng", "authors": "Liang Qiao, Sanli Tang, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu\n  and Fei Wu", "title": "Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many approaches have recently been proposed to detect irregular scene text\nand achieved promising results. However, their localization results may not\nwell satisfy the following text recognition part mainly because of two reasons:\n1) recognizing arbitrary shaped text is still a challenging task, and 2)\nprevalent non-trainable pipeline strategies between text detection and text\nrecognition will lead to suboptimal performances. To handle this\nincompatibility problem, in this paper we propose an end-to-end trainable text\nspotting approach named Text Perceptron. Concretely, Text Perceptron first\nemploys an efficient segmentation-based text detector that learns the latent\ntext reading order and boundary information. Then a novel Shape Transform\nModule (abbr. STM) is designed to transform the detected feature regions into\nregular morphologies without extra parameters. It unites text detection and the\nfollowing recognition part into a whole framework, and helps the whole network\nachieve global optimization. Experiments show that our method achieves\ncompetitive performance on two standard text benchmarks, i.e., ICDAR 2013 and\nICDAR 2015, and also obviously outperforms existing methods on irregular text\nbenchmarks SCUT-CTW1500 and Total-Text.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 08:07:19 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Qiao", "Liang", ""], ["Tang", "Sanli", ""], ["Cheng", "Zhanzhan", ""], ["Xu", "Yunlu", ""], ["Niu", "Yi", ""], ["Pu", "Shiliang", ""], ["Wu", "Fei", ""]]}, {"id": "2002.06832", "submitter": "Hanyuan Zhang", "authors": "Hao Wu, Hanyuan Zhang, Xinyu Zhang, Weiwei Sun, Baihua Zheng, Yuning\n  Jiang", "title": "DeepDualMapper: A Gated Fusion Network for Automatic Map Extraction\n  using Aerial Images and Trajectories", "comments": "7 pages, AAAI 2020 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic map extraction is of great importance to urban computing and\nlocation-based services. Aerial image and GPS trajectory data refer to two\ndifferent data sources that could be leveraged to generate the map, although\nthey carry different types of information. Most previous works on data fusion\nbetween aerial images and data from auxiliary sensors do not fully utilize the\ninformation of both modalities and hence suffer from the issue of information\nloss. We propose a deep convolutional neural network called DeepDualMapper\nwhich fuses the aerial image and trajectory data in a more seamless manner to\nextract the digital map. We design a gated fusion module to explicitly control\nthe information flows from both modalities in a complementary-aware manner.\nMoreover, we propose a novel densely supervised refinement decoder to generate\nthe prediction in a coarse-to-fine way. Our comprehensive experiments\ndemonstrate that DeepDualMapper can fuse the information of images and\ntrajectories much more effectively than existing approaches, and is able to\ngenerate maps with higher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 08:33:46 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Wu", "Hao", ""], ["Zhang", "Hanyuan", ""], ["Zhang", "Xinyu", ""], ["Sun", "Weiwei", ""], ["Zheng", "Baihua", ""], ["Jiang", "Yuning", ""]]}, {"id": "2002.06838", "submitter": "Sheng Hu", "authors": "Sheng Hu, Yuqing Ma, Xianglong Liu, Yanlu Wei, Shihao Bai", "title": "Stratified Rule-Aware Network for Abstract Visual Reasoning", "comments": "AAAI 2021 paper. Code: https://github.com/husheng12345/SRAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract reasoning refers to the ability to analyze information, discover\nrules at an intangible level, and solve problems in innovative ways. Raven's\nProgressive Matrices (RPM) test is typically used to examine the capability of\nabstract reasoning. The subject is asked to identify the correct choice from\nthe answer set to fill the missing panel at the bottom right of RPM (e.g., a\n3$\\times$3 matrix), following the underlying rules inside the matrix. Recent\nstudies, taking advantage of Convolutional Neural Networks (CNNs), have\nachieved encouraging progress to accomplish the RPM test. However, they partly\nignore necessary inductive biases of RPM solver, such as order sensitivity\nwithin each row/column and incremental rule induction. To address this problem,\nin this paper we propose a Stratified Rule-Aware Network (SRAN) to generate the\nrule embeddings for two input sequences. Our SRAN learns multiple granularity\nrule embeddings at different levels, and incrementally integrates the\nstratified embedding flows through a gated fusion module. With the help of\nembeddings, a rule similarity metric is applied to guarantee that SRAN can not\nonly be trained using a tuplet loss but also infer the best answer efficiently.\nWe further point out the severe defects existing in the popular RAVEN dataset\nfor RPM test, which prevent from the fair evaluation of the abstract reasoning\nability. To fix the defects, we propose an answer set generation algorithm\ncalled Attribute Bisection Tree (ABT), forming an improved dataset named\nImpartial-RAVEN (I-RAVEN for short). Extensive experiments are conducted on\nboth PGM and I-RAVEN datasets, showing that our SRAN outperforms the\nstate-of-the-art models by a considerable margin.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 08:44:05 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 08:46:49 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Hu", "Sheng", ""], ["Ma", "Yuqing", ""], ["Liu", "Xianglong", ""], ["Wei", "Yanlu", ""], ["Bai", "Shihao", ""]]}, {"id": "2002.06862", "submitter": "Taro Langner", "authors": "Taro Langner, Robin Strand, H{\\aa}kan Ahlstr\\\"om, Joel Kullberg", "title": "Large-scale biometry with interpretable neural network regression on UK\n  Biobank body MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a large-scale medical examination, the UK Biobank study has successfully\nimaged more than 32,000 volunteer participants with magnetic resonance imaging\n(MRI). Each scan is linked to extensive metadata, providing a comprehensive\nmedical survey of imaged anatomy and related health states. Despite its\npotential for research, this vast amount of data presents a challenge to\nestablished methods of evaluation, which often rely on manual input. To date,\nthe range of reference values for cardiovascular and metabolic risk factors is\ntherefore incomplete. In this work, neural networks were trained for\nimage-based regression to infer various biological metrics from the\nneck-to-knee body MRI automatically. The approach requires no manual\nintervention or direct access to reference segmentations for training. The\nexamined fields span 64 variables derived from anthropometric measurements,\ndual-energy X-ray absorptiometry (DXA), atlas-based segmentations, and\ndedicated liver scans. With the ResNet50, the standardized framework achieves a\nclose fit to the target values (median R^2 > 0.97) in cross-validation.\nInterpretation of aggregated saliency maps suggests that the network correctly\ntargets specific body regions and limbs, and learned to emulate different\nmodalities. On several body composition metrics, the quality of the predictions\nis within the range of variability observed between established gold standard\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 09:47:58 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 10:23:26 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 08:59:16 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Langner", "Taro", ""], ["Strand", "Robin", ""], ["Ahlstr\u00f6m", "H\u00e5kan", ""], ["Kullberg", "Joel", ""]]}, {"id": "2002.06890", "submitter": "Terence Broad", "authors": "Terence Broad, Frederic Fol Leymarie, Mick Grierson", "title": "Amplifying The Uncanny", "comments": null, "journal-ref": "Proceedings of the Eighth Conference on Proceedings of the Eighth\n  Conference on Computation, Communication, Aesthetics & X, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep neural networks have become remarkably good at producing realistic\ndeepfakes, images of people that (to the untrained eye) are indistinguishable\nfrom real images. Deepfakes are produced by algorithms that learn to\ndistinguish between real and fake images and are optimised to generate samples\nthat the system deems realistic. This paper, and the resulting series of\nartworks Being Foiled explore the aesthetic outcome of inverting this process,\ninstead optimising the system to generate images that it predicts as being\nfake. This maximises the unlikelihood of the data and in turn, amplifies the\nuncanny nature of these machine hallucinations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 11:12:39 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 16:49:55 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 13:18:10 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Broad", "Terence", ""], ["Leymarie", "Frederic Fol", ""], ["Grierson", "Mick", ""]]}, {"id": "2002.06927", "submitter": "Mohamed Elmahdy", "authors": "Mohamed S. Elmahdy, Tanuj Ahuja, U. A. van der Heide, and Marius\n  Staring", "title": "Patient-Specific Finetuning of Deep Learning Models for Adaptive\n  Radiotherapy in Prostate CT", "comments": "IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contouring of the target volume and Organs-At-Risk (OARs) is a crucial step\nin radiotherapy treatment planning. In an adaptive radiotherapy setting,\nupdated contours need to be generated based on daily imaging. In this work, we\nleverage personalized anatomical knowledge accumulated over the treatment\nsessions, to improve the segmentation accuracy of a pre-trained Convolution\nNeural Network (CNN), for a specific patient. We investigate a transfer\nlearning approach, fine-tuning the baseline CNN model to a specific patient,\nbased on imaging acquired in earlier treatment fractions. The baseline CNN\nmodel is trained on a prostate CT dataset from one hospital of 379 patients.\nThis model is then fine-tuned and tested on an independent dataset of another\nhospital of 18 patients, each having 7 to 10 daily CT scans. For the prostate,\nseminal vesicles, bladder and rectum, the model fine-tuned on each specific\npatient achieved a Mean Surface Distance (MSD) of $1.64 \\pm 0.43$ mm, $2.38 \\pm\n2.76$ mm, $2.30 \\pm 0.96$ mm, and $1.24 \\pm 0.89$ mm, respectively, which was\nsignificantly better than the baseline model. The proposed personalized model\nadaptation is therefore very promising for clinical implementation in the\ncontext of adaptive radiotherapy of prostate cancer.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 12:53:37 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Elmahdy", "Mohamed S.", ""], ["Ahuja", "Tanuj", ""], ["van der Heide", "U. A.", ""], ["Staring", "Marius", ""]]}, {"id": "2002.06963", "submitter": "Dahyun Kim", "authors": "Dahyun Kim, Kunal Pratap Singh, Jonghyun Choi", "title": "Learning Architectures for Binary Networks", "comments": "The manuscript was changed to a one-column format along with minor\n  modifications to the content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Backbone architectures of most binary networks are well-known floating point\narchitectures such as the ResNet family. Questioning that the architectures\ndesigned for floating point networks would not be the best for binary networks,\nwe propose to search architectures for binary networks (BNAS) by defining a new\nsearch space for binary architectures and a novel search objective.\nSpecifically, based on the cell based search method, we define the new search\nspace of binary layer types, design a new cell template, and rediscover the\nutility of and propose to use the Zeroise layer instead of using it as a\nplaceholder. The novel search objective diversifies early search to learn\nbetter performing binary architectures. We show that our proposed method\nsearches architectures with stable training curves despite the quantization\nerror inherent in binary networks. Quantitative analyses demonstrate that our\nsearched architectures outperform the architectures used in state-of-the-art\nbinary networks and outperform or perform on par with state-of-the-art binary\nnetworks that employ various techniques other than architectural changes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 14:06:45 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 09:08:41 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Kim", "Dahyun", ""], ["Singh", "Kunal Pratap", ""], ["Choi", "Jonghyun", ""]]}, {"id": "2002.07040", "submitter": "Manu Tom", "authors": "Manu Tom, Roberto Aguilar, Pascal Imhof, Silvan Leinss, Emmanuel\n  Baltsavias and Konrad Schindler", "title": "Lake Ice Detection from Sentinel-1 SAR with Deep Learning", "comments": "Accepted for ISPRS Congress 2020, Nice, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lake ice, as part of the Essential Climate Variable (ECV) lakes, is an\nimportant indicator to monitor climate change and global warming. The\nspatio-temporal extent of lake ice cover, along with the timings of key\nphenological events such as freeze-up and break-up, provide important cues\nabout the local and global climate. We present a lake ice monitoring system\nbased on the automatic analysis of Sentinel-1 Synthetic Aperture Radar (SAR)\ndata with a deep neural network. In previous studies that used optical\nsatellite imagery for lake ice monitoring, frequent cloud cover was a main\nlimiting factor, which we overcome thanks to the ability of microwave sensors\nto penetrate clouds and observe the lakes regardless of the weather and\nillumination conditions. We cast ice detection as a two class (frozen,\nnon-frozen) semantic segmentation problem and solve it using a state-of-the-art\ndeep convolutional network (CNN). We report results on two winters ( 2016 - 17\nand 2017 - 18 ) and three alpine lakes in Switzerland. The proposed model\nreaches mean Intersection-over-Union (mIoU) scores >90% on average, and >84%\neven for the most difficult lake. Additionally, we perform cross-validation\ntests and show that our algorithm generalises well across unseen lakes and\nwinters.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 16:31:41 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 23:01:15 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Tom", "Manu", ""], ["Aguilar", "Roberto", ""], ["Imhof", "Pascal", ""], ["Leinss", "Silvan", ""], ["Baltsavias", "Emmanuel", ""], ["Schindler", "Konrad", ""]]}, {"id": "2002.07082", "submitter": "Shiv Ram Dubey", "authors": "Kancharagunta Kishan Babu and Shiv Ram Dubey", "title": "PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks\n  for Thermal and NIR to Visible Image Transformation", "comments": "Published in Neurocomputing Journal, Elsevier", "journal-ref": "Neurocomputing, 413:41-50, Nov 2020", "doi": "10.1016/j.neucom.2020.06.104", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real world scenarios, it is difficult to capture the images in the\nvisible light spectrum (VIS) due to bad lighting conditions. However, the\nimages can be captured in such scenarios using Near-Infrared (NIR) and Thermal\n(THM) cameras. The NIR and THM images contain the limited details. Thus, there\nis a need to transform the images from THM/NIR to VIS for better understanding.\nHowever, it is non-trivial task due to the large domain discrepancies and lack\nof abundant datasets. Nowadays, Generative Adversarial Network (GAN) is able to\ntransform the images from one domain to another domain. Most of the available\nGAN based methods use the combination of the adversarial and the pixel-wise\nlosses (like $L_1$ or $L_2$) as the objective function for training. The\nquality of transformed images in case of THM/NIR to VIS transformation is still\nnot up to the mark using such objective function. Thus, better objective\nfunctions are needed to improve the quality, fine details and realism of the\ntransformed images. A new model for THM/NIR to VIS image transformation called\nPerceptual Cyclic-Synthesized Generative Adversarial Network (PCSGAN) is\nintroduced to address these issues. The PCSGAN uses the combination of the\nperceptual (i.e., feature based) losses along with the pixel-wise and the\nadversarial losses. Both the quantitative and qualitative measures are used to\njudge the performance of the PCSGAN model over the WHU-IIP face and the RGB-NIR\nscene datasets. The proposed PCSGAN outperforms the state-of-the-art image\ntransformation models, including Pix2pix, DualGAN, CycleGAN, PS2GAN, and PAN in\nterms of the SSIM, MSE, PSNR and LPIPS evaluation measures. The code is\navailable at https://github.com/KishanKancharagunta/PCSGAN.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 11:55:03 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 11:50:33 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Babu", "Kancharagunta Kishan", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "2002.07088", "submitter": "Ryan Feng", "authors": "Ryan Feng, Jiefeng Chen, Earlence Fernandes, Somesh Jha, Atul Prakash", "title": "Robust Physical Hard-Label Attacks on Deep Learning Visual\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing physical adversarial examples for computer vision rely on white-box\naccess. In this work, we investigate physical examples in the black-box\nhard-label case -- where the attacker has only query access to the model and\nonly receives the top-1 class label without confidence information. This threat\nmodel is more realistic for cyber-physical systems -- the main target when\nconsidering physical attacks on computer vision. Key challenges in this setting\ninclude obtaining reliability against environmental variations and creating\narea-limited perturbations without access to model gradients. We base our work\non recent advances in gradient-free optimization and present GRAPHITE, the\nfirst algorithm for black-box hard-label physical attacks on computer vision\nmodels. We evaluate GRAPHITE on a traffic sign classifier and a\npublicly-available Automatic License Plate Recognition (ALPR) tool using only\nquery access. We successfully cause a Stop sign to be misclassified as a Speed\nLimit 30 in 92.9% of physical test images and cause errors in 95% of cases for\nthe ALPR tool.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 17:24:14 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 21:13:32 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 21:01:43 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Feng", "Ryan", ""], ["Chen", "Jiefeng", ""], ["Fernandes", "Earlence", ""], ["Jha", "Somesh", ""], ["Prakash", "Atul", ""]]}, {"id": "2002.07089", "submitter": "Samaneh Abbasi Sureshjani", "authors": "Samaneh Abbasi-Sureshjani, Sina Amirrajab, Cristian Lorenz, Juergen\n  Weese, Josien Pluim, Marcel Breeuwer", "title": "4D Semantic Cardiac Magnetic Resonance Image Synthesis on XCAT\n  Anatomical Model", "comments": "Accepted to MIDL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid controllable image generation method to synthesize\nanatomically meaningful 3D+t labeled Cardiac Magnetic Resonance (CMR) images.\nOur hybrid method takes the mechanistic 4D eXtended CArdiac Torso (XCAT) heart\nmodel as the anatomical ground truth and synthesizes CMR images via a\ndata-driven Generative Adversarial Network (GAN). We employ the\nstate-of-the-art SPatially Adaptive De-normalization (SPADE) technique for\nconditional image synthesis to preserve the semantic spatial information of\nground truth anatomy. Using the parameterized motion model of the XCAT heart,\nwe generate labels for 25 time frames of the heart for one cardiac cycle at 18\nlocations for the short axis view. Subsequently, realistic images are generated\nfrom these labels, with modality-specific features that are learned from real\nCMR image data. We demonstrate that style transfer from another cardiac image\ncan be accomplished by using a style encoder network. Due to the flexibility of\nXCAT in creating new heart models, this approach can result in a realistic\nvirtual population to address different challenges the medical image analysis\nresearch community is facing such as expensive data collection. Our proposed\nmethod has a great potential to synthesize 4D controllable CMR images with\nannotations and adaptable styles to be used in various supervised multi-site,\nmulti-vendor applications in medical image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 17:25:07 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 16:55:32 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 14:01:13 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Abbasi-Sureshjani", "Samaneh", ""], ["Amirrajab", "Sina", ""], ["Lorenz", "Cristian", ""], ["Weese", "Juergen", ""], ["Pluim", "Josien", ""], ["Breeuwer", "Marcel", ""]]}, {"id": "2002.07118", "submitter": "Joshua Rapp", "authors": "Joshua Rapp, Charles Saunders, Juli\\'an Tachella, John Murray-Bruce,\n  Yoann Altmann, Jean-Yves Tourneret, Stephen McLaughlin, Robin M. A. Dawson,\n  Franco N. C. Wong, Vivek K Goyal", "title": "Seeing Around Corners with Edge-Resolved Transient Imaging", "comments": "Includes manuscript (14 pages) and supplement (24 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-line-of-sight (NLOS) imaging is a rapidly growing field seeking to form\nimages of objects outside the field of view, with potential applications in\nsearch and rescue, reconnaissance, and even medical imaging. The critical\nchallenge of NLOS imaging is that diffuse reflections scatter light in all\ndirections, resulting in weak signals and a loss of directional information. To\naddress this problem, we propose a method for seeing around corners that\nderives angular resolution from vertical edges and longitudinal resolution from\nthe temporal response to a pulsed light source. We introduce an acquisition\nstrategy, scene response model, and reconstruction algorithm that enable the\nformation of 2.5-dimensional representations -- a plan view plus heights -- and\na 180$^{\\circ}$ field of view (FOV) for large-scale scenes. Our experiments\ndemonstrate accurate reconstructions of hidden rooms up to 3 meters in each\ndimension.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 18:33:48 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Rapp", "Joshua", ""], ["Saunders", "Charles", ""], ["Tachella", "Juli\u00e1n", ""], ["Murray-Bruce", "John", ""], ["Altmann", "Yoann", ""], ["Tourneret", "Jean-Yves", ""], ["McLaughlin", "Stephen", ""], ["Dawson", "Robin M. A.", ""], ["Wong", "Franco N. C.", ""], ["Goyal", "Vivek K", ""]]}, {"id": "2002.07136", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Ritchie Zhao, Weizhe Hua, Nayun Xu, G. Edward Suh, Zhiru\n  Zhang", "title": "Precision Gating: Improving Neural Network Efficiency with Dynamic\n  Dual-Precision Activations", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose precision gating (PG), an end-to-end trainable dynamic\ndual-precision quantization technique for deep neural networks. PG computes\nmost features in a low precision and only a small proportion of important\nfeatures in a higher precision to preserve accuracy. The proposed approach is\napplicable to a variety of DNN architectures and significantly reduces the\ncomputational cost of DNN execution with almost no accuracy loss. Our\nexperiments indicate that PG achieves excellent results on CNNs, including\nstatically compressed mobile-friendly networks such as ShuffleNet. Compared to\nthe state-of-the-art prediction-based quantization schemes, PG achieves the\nsame or higher accuracy with 2.4$\\times$ less compute on ImageNet. PG\nfurthermore applies to RNNs. Compared to 8-bit uniform quantization, PG obtains\na 1.2% improvement in perplexity per word with 2.7$\\times$ computational cost\nreduction on LSTM on the Penn Tree Bank dataset. Code is available at:\nhttps://github.com/cornell-zhang/dnn-gating\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 18:54:37 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 03:10:27 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Zhang", "Yichi", ""], ["Zhao", "Ritchie", ""], ["Hua", "Weizhe", ""], ["Xu", "Nayun", ""], ["Suh", "G. Edward", ""], ["Zhang", "Zhiru", ""]]}, {"id": "2002.07162", "submitter": "Wanling Gao", "authors": "Wanling Gao, Fei Tang, Jianfeng Zhan, Chuanxin Lan, Chunjie Luo, Lei\n  Wang, Jiahui Dai, Zheng Cao, Xiongwang Xiong, Zihan Jiang, Tianshu Hao, Fanda\n  Fan, Xu Wen, Fan Zhang, Yunyou Huang, Jianan Chen, Mengjia Du, Rui Ren, Chen\n  Zheng, Daoyi Zheng, Haoning Tang, Kunlin Zhan, Biao Wang, Defei Kong, Minghe\n  Yu, Chongkang Tan, Huan Li, Xinhui Tian, Yatao Li, Gang Lu, Junchao Shao,\n  Zhenyu Wang, Xiaoyu Wang, Hainan Ye", "title": "AIBench: An Agile Domain-specific Benchmarking Methodology and an AI\n  Benchmark Suite", "comments": "25 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:1908.08998", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific software and hardware co-design is encouraging as it is much\neasier to achieve efficiency for fewer tasks. Agile domain-specific\nbenchmarking speeds up the process as it provides not only relevant design\ninputs but also relevant metrics, and tools. Unfortunately, modern workloads\nlike Big data, AI, and Internet services dwarf the traditional one in terms of\ncode size, deployment scale, and execution path, and hence raise serious\nbenchmarking challenges.\n  This paper proposes an agile domain-specific benchmarking methodology.\nTogether with seventeen industry partners, we identify ten important end-to-end\napplication scenarios, among which sixteen representative AI tasks are\ndistilled as the AI component benchmarks. We propose the permutations of\nessential AI and non-AI component benchmarks as end-to-end benchmarks. An\nend-to-end benchmark is a distillation of the essential attributes of an\nindustry-scale application. We design and implement a highly extensible,\nconfigurable, and flexible benchmark framework, on the basis of which, we\npropose the guideline for building end-to-end benchmarks, and present the first\nend-to-end Internet service AI benchmark.\n  The preliminary evaluation shows the value of our benchmark suite---AIBench\nagainst MLPerf and TailBench for hardware and software designers,\nmicro-architectural researchers, and code developers. The specifications,\nsource code, testbed, and results are publicly available from the web site\n\\url{http://www.benchcouncil.org/AIBench/index.html}.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 07:29:05 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Gao", "Wanling", ""], ["Tang", "Fei", ""], ["Zhan", "Jianfeng", ""], ["Lan", "Chuanxin", ""], ["Luo", "Chunjie", ""], ["Wang", "Lei", ""], ["Dai", "Jiahui", ""], ["Cao", "Zheng", ""], ["Xiong", "Xiongwang", ""], ["Jiang", "Zihan", ""], ["Hao", "Tianshu", ""], ["Fan", "Fanda", ""], ["Wen", "Xu", ""], ["Zhang", "Fan", ""], ["Huang", "Yunyou", ""], ["Chen", "Jianan", ""], ["Du", "Mengjia", ""], ["Ren", "Rui", ""], ["Zheng", "Chen", ""], ["Zheng", "Daoyi", ""], ["Tang", "Haoning", ""], ["Zhan", "Kunlin", ""], ["Wang", "Biao", ""], ["Kong", "Defei", ""], ["Yu", "Minghe", ""], ["Tan", "Chongkang", ""], ["Li", "Huan", ""], ["Tian", "Xinhui", ""], ["Li", "Yatao", ""], ["Lu", "Gang", ""], ["Shao", "Junchao", ""], ["Wang", "Zhenyu", ""], ["Wang", "Xiaoyu", ""], ["Ye", "Hainan", ""]]}, {"id": "2002.07203", "submitter": "Dat Thanh Tran", "authors": "Dat Thanh Tran, Moncef Gabbouj, Alexandros Iosifidis", "title": "Multilinear Compressive Learning with Prior Knowledge", "comments": "15 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Multilinear Compressive Learning (MCL) framework\ncombines Multilinear Compressive Sensing and Machine Learning into an\nend-to-end system that takes into account the multidimensional structure of the\nsignals when designing the sensing and feature synthesis components. The key\nidea behind MCL is the assumption of the existence of a tensor subspace which\ncan capture the essential features from the signal for the downstream learning\ntask. Thus, the ability to find such a discriminative tensor subspace and\noptimize the system to project the signals onto that data manifold plays an\nimportant role in Multilinear Compressive Learning. In this paper, we propose a\nnovel solution to address both of the aforementioned requirements, i.e., How to\nfind those tensor subspaces in which the signals of interest are highly\nseparable? and How to optimize the sensing and feature synthesis components to\ntransform the original signals to the data manifold found in the first\nquestion? In our proposal, the discovery of a high-quality data manifold is\nconducted by training a nonlinear compressive learning system on the inference\ntask. Its knowledge of the data manifold of interest is then progressively\ntransferred to the MCL components via multi-stage supervised training with the\nsupervisory information encoding how the compressed measurements, the\nsynthesized features, and the predictions should be like. The proposed\nknowledge transfer algorithm also comes with a semi-supervised adaption that\nenables compressive learning models to utilize unlabeled data effectively.\nExtensive experiments demonstrate that the proposed knowledge transfer method\ncan effectively train MCL models to compressively sense and synthesize better\nfeatures for the learning tasks with improved performances, especially when the\ncomplexity of the learning task increases.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 19:06:05 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Tran", "Dat Thanh", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2002.07224", "submitter": "Garrett Bingham", "authors": "Garrett Bingham, William Macke, and Risto Miikkulainen", "title": "Evolutionary Optimization of Deep Learning Activation Functions", "comments": "8 pages; 9 figures/tables; GECCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of activation function can have a large effect on the performance\nof a neural network. While there have been some attempts to hand-engineer novel\nactivation functions, the Rectified Linear Unit (ReLU) remains the most\ncommonly-used in practice. This paper shows that evolutionary algorithms can\ndiscover novel activation functions that outperform ReLU. A tree-based search\nspace of candidate activation functions is defined and explored with mutation,\ncrossover, and exhaustive search. Experiments on training wide residual\nnetworks on the CIFAR-10 and CIFAR-100 image datasets show that this approach\nis effective. Replacing ReLU with evolved activation functions results in\nstatistically significant increases in network accuracy. Optimal performance is\nachieved when evolution is allowed to customize activation functions to a\nparticular task; however, these novel activation functions are shown to\ngeneralize, achieving high performance across tasks. Evolutionary optimization\nof activation functions is therefore a promising new dimension of metalearning\nin neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 19:54:26 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 15:53:12 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Bingham", "Garrett", ""], ["Macke", "William", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "2002.07227", "submitter": "Joseph Robinson", "authors": "Yu Yin and Songyao Jiang and Joseph P. Robinson and Yun Fu", "title": "Dual-Attention GAN for Large-Pose Face Frontalization", "comments": "The 15th IEEE International Conference on Automatic Face and Gesture\n  Recognition (FG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face frontalization provides an effective and efficient way for face data\naugmentation and further improves the face recognition performance in extreme\npose scenario. Despite recent advances in deep learning-based face synthesis\napproaches, this problem is still challenging due to significant pose and\nillumination discrepancy. In this paper, we present a novel Dual-Attention\nGenerative Adversarial Network (DA-GAN) for photo-realistic face frontalization\nby capturing both contextual dependencies and local consistency during GAN\ntraining. Specifically, a self-attention-based generator is introduced to\nintegrate local features with their long-range dependencies yielding better\nfeature representations, and hence generate faces that preserve identities\nbetter, especially for larger pose angles. Moreover, a novel\nface-attention-based discriminator is applied to emphasize local features of\nface regions, and hence reinforce the realism of synthetic frontal faces.\nGuided by semantic segmentation, four independent discriminators are used to\ndistinguish between different aspects of a face (\\ie skin, keypoints, hairline,\nand frontalized face). By introducing these two complementary attention\nmechanisms in generator and discriminator separately, we can learn a richer\nfeature representation and generate identity preserving inference of frontal\nviews with much finer details (i.e., more accurate facial appearance and\ntextures) comparing to the state-of-the-art. Quantitative and qualitative\nexperimental results demonstrate the effectiveness and efficiency of our DA-GAN\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 20:00:56 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Yin", "Yu", ""], ["Jiang", "Songyao", ""], ["Robinson", "Joseph P.", ""], ["Fu", "Yun", ""]]}, {"id": "2002.07269", "submitter": "Yu Liu", "authors": "Yu Liu, Jie Li, Qingsen Yan, Xia Yuan, Chunxia Zhao, Ian Reid and\n  Cesar Cadena", "title": "3D Gated Recurrent Fusion for Semantic Scene Completion", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper tackles the problem of data fusion in the semantic scene\ncompletion (SSC) task, which can simultaneously deal with semantic labeling and\nscene completion. RGB images contain texture details of the object(s) which are\nvital for semantic scene understanding. Meanwhile, depth images capture\ngeometric clues of high relevance for shape completion. Using both RGB and\ndepth images can further boost the accuracy of SSC over employing one modality\nin isolation. We propose a 3D gated recurrent fusion network (GRFNet), which\nlearns to adaptively select and fuse the relevant information from depth and\nRGB by making use of the gate and memory modules. Based on the single-stage\nfusion, we further propose a multi-stage fusion strategy, which could model the\ncorrelations among different stages within the network. Extensive experiments\non two benchmark datasets demonstrate the superior performance and the\neffectiveness of the proposed GRFNet for data fusion in SSC. Code will be made\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 21:45:43 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Liu", "Yu", ""], ["Li", "Jie", ""], ["Yan", "Qingsen", ""], ["Yuan", "Xia", ""], ["Zhao", "Chunxia", ""], ["Reid", "Ian", ""], ["Cadena", "Cesar", ""]]}, {"id": "2002.07358", "submitter": "Peisen Zhao", "authors": "Peisen Zhao, Lingxi Xie, Chen Ju, Ya Zhang, Yanfeng Wang, Qi Tian", "title": "Bottom-Up Temporal Action Localization with Mutual Regularization", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, temporal action localization (TAL), i.e., finding specific action\nsegments in untrimmed videos, has attracted increasing attentions of the\ncomputer vision community. State-of-the-art solutions for TAL involves\nevaluating the frame-level probabilities of three action-indicating phases,\ni.e. starting, continuing, and ending; and then post-processing these\npredictions for the final localization. This paper delves deep into this\nmechanism, and argues that existing methods, by modeling these phases as\nindividual classification tasks, ignored the potential temporal constraints\nbetween them. This can lead to incorrect and/or inconsistent predictions when\nsome frames of the video input lack sufficient discriminative information. To\nalleviate this problem, we introduce two regularization terms to mutually\nregularize the learning procedure: the Intra-phase Consistency (IntraC)\nregularization is proposed to make the predictions verified inside each phase;\nand the Inter-phase Consistency (InterC) regularization is proposed to keep\nconsistency between these phases. Jointly optimizing these two terms, the\nentire framework is aware of these potential constraints during an end-to-end\noptimization process. Experiments are performed on two popular TAL datasets,\nTHUMOS14 and ActivityNet1.3. Our approach clearly outperforms the baseline both\nquantitatively and qualitatively. The proposed regularization also generalizes\nto other TAL methods (e.g., TSA-Net and PGCN). code:\nhttps://github.com/PeisenZhao/Bottom-Up-TAL-with-MR\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 03:59:13 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 13:06:49 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 02:26:46 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zhao", "Peisen", ""], ["Xie", "Lingxi", ""], ["Ju", "Chen", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "2002.07362", "submitter": "Donghyun Kim", "authors": "Donghyun Kim, Tian Lan, Chuhang Zou, Ning Xu, Bryan A. Plummer, Stan\n  Sclaroff, Jayan Eledath, Gerard Medioni", "title": "MILA: Multi-Task Learning from Videos via Efficient Inter-Frame Local\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work in multi-task learning has mainly focused on predictions on a\nsingle image. In this work, we present a new approach for multi-task learning\nfrom videos via efficient inter-frame local attention (MILA). Our approach\ncontains a novel inter-frame attention module which allows learning of\ntask-specific attention across frames. We embed the attention module in a\n``slow-fast'' architecture, where the slower network runs on sparsely sampled\nkeyframes and the light-weight shallow network runs on non-keyframes at a high\nframe rate. We also propose an effective adversarial learning strategy to\nencourage the slow and fast network to learn similar features. Our approach\nensures low-latency multi-task learning while maintaining high quality\npredictions. Experiments show competitive accuracy compared to state-of-the-art\non two multi-task learning benchmarks while reducing the number of floating\npoint operations (FLOPs) by up to 70\\%. In addition, our attention based\nfeature propagation method (ILA) outperforms prior work in terms of task\naccuracy while also reducing up to 90\\% of FLOPs.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 04:25:58 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 16:02:21 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kim", "Donghyun", ""], ["Lan", "Tian", ""], ["Zou", "Chuhang", ""], ["Xu", "Ning", ""], ["Plummer", "Bryan A.", ""], ["Sclaroff", "Stan", ""], ["Eledath", "Jayan", ""], ["Medioni", "Gerard", ""]]}, {"id": "2002.07371", "submitter": "Yu Zhang", "authors": "Yu Zhang, Xin Sun, Junyu Dong, Changrui Chen, Yue Shen", "title": "High-Order Paired-ASPP Networks for Semantic Segmenation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current semantic segmentation models only exploit first-order statistics,\nwhile rarely exploring high-order statistics. However, common first-order\nstatistics are insufficient to support a solid unanimous representation. In\nthis paper, we propose High-Order Paired-ASPP Network to exploit high-order\nstatistics from various feature levels. The network first introduces a\nHigh-Order Representation module to extract the contextual high-order\ninformation from all stages of the backbone. They can provide more semantic\nclues and discriminative information than the first-order ones. Besides, a\nPaired-ASPP module is proposed to embed high-order statistics of the early\nstages into the last stage. It can further preserve the boundary-related and\nspatial context in the low-level features for final prediction. Our experiments\nshow that the high-order statistics significantly boost the performance on\nconfusing objects. Our method achieves competitive performance without bells\nand whistles on three benchmarks, i.e, Cityscapes, ADE20K and Pascal-Context\nwith the mIoU of 81.6%, 45.3% and 52.9%.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 04:50:06 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Zhang", "Yu", ""], ["Sun", "Xin", ""], ["Dong", "Junyu", ""], ["Chen", "Changrui", ""], ["Shen", "Yue", ""]]}, {"id": "2002.07376", "submitter": "Chaoqi Wang", "authors": "Chaoqi Wang, Guodong Zhang, Roger Grosse", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "comments": "Fix several typos", "journal-ref": "In Proceedings of the 8th International Conference on Learning\n  Representations (ICLR), 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overparameterization has been shown to benefit both the optimization and\ngeneralization of neural networks, but large networks are resource hungry at\nboth training and test time. Network pruning can reduce test-time resource\nrequirements, but is typically applied to trained networks and therefore cannot\navoid the expensive training process. We aim to prune networks at\ninitialization, thereby saving resources at training time as well.\nSpecifically, we argue that efficient training requires preserving the gradient\nflow through the network. This leads to a simple but effective pruning\ncriterion we term Gradient Signal Preservation (GraSP). We empirically\ninvestigate the effectiveness of the proposed method with extensive experiments\non CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet\narchitectures. Our method can prune 80% of the weights of a VGG-16 network on\nImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover,\nour method achieves significantly better performance than the baseline at\nextreme sparsity levels.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 05:14:47 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 00:02:33 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Wang", "Chaoqi", ""], ["Zhang", "Guodong", ""], ["Grosse", "Roger", ""]]}, {"id": "2002.07394", "submitter": "Junnan Li Dr", "authors": "Junnan Li, Richard Socher, Steven C.H. Hoi", "title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning", "comments": null, "journal-ref": "International Conference on Learning Representations, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to be annotation-hungry. Numerous efforts have\nbeen devoted to reducing the annotation cost when learning with deep networks.\nTwo prominent directions include learning with noisy labels and semi-supervised\nlearning by exploiting unlabeled data. In this work, we propose DivideMix, a\nnovel framework for learning with noisy labels by leveraging semi-supervised\nlearning techniques. In particular, DivideMix models the per-sample loss\ndistribution with a mixture model to dynamically divide the training data into\na labeled set with clean samples and an unlabeled set with noisy samples, and\ntrains the model on both the labeled and unlabeled data in a semi-supervised\nmanner. To avoid confirmation bias, we simultaneously train two diverged\nnetworks where each network uses the dataset division from the other network.\nDuring the semi-supervised training phase, we improve the MixMatch strategy by\nperforming label co-refinement and label co-guessing on labeled and unlabeled\nsamples, respectively. Experiments on multiple benchmark datasets demonstrate\nsubstantial improvements over state-of-the-art methods. Code is available at\nhttps://github.com/LiJunnan1992/DivideMix .\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 06:20:06 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Li", "Junnan", ""], ["Socher", "Richard", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2002.07405", "submitter": "Yao Qin", "authors": "Yao Qin, Nicholas Frosst, Colin Raffel, Garrison Cottrell and Geoffrey\n  Hinton", "title": "Deflecting Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an ongoing cycle where stronger defenses against adversarial\nattacks are subsequently broken by a more advanced defense-aware attack. We\npresent a new approach towards ending this cycle where we \"deflect''\nadversarial attacks by causing the attacker to produce an input that\nsemantically resembles the attack's target class. To this end, we first propose\na stronger defense based on Capsule Networks that combines three detection\nmechanisms to achieve state-of-the-art detection performance on both standard\nand defense-aware attacks. We then show that undetected attacks against our\ndefense often perceptually resemble the adversarial target class by performing\na human study where participants are asked to label images produced by the\nattack. These attack images can no longer be called \"adversarial'' because our\nnetwork classifies them the same way as humans do.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 06:59:13 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Qin", "Yao", ""], ["Frosst", "Nicholas", ""], ["Raffel", "Colin", ""], ["Cottrell", "Garrison", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "2002.07417", "submitter": "Linpu Fang", "authors": "Hang Xu, Linpu Fang, Xiaodan Liang, Wenxiong Kang, Zhenguo Li", "title": "Universal-RCNN: Universal Object Detector via Transferable Graph R-CNN", "comments": "Accepted by AAAI20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant object detection approaches treat each dataset separately and\nfit towards a specific domain, which cannot adapt to other domains without\nextensive retraining. In this paper, we address the problem of designing a\nuniversal object detection model that exploits diverse category granularity\nfrom multiple domains and predict all kinds of categories in one system.\nExisting works treat this problem by integrating multiple detection branches\nupon one shared backbone network. However, this paradigm overlooks the crucial\nsemantic correlations between multiple domains, such as categories hierarchy,\nvisual similarity, and linguistic relationship. To address these drawbacks, we\npresent a novel universal object detector called Universal-RCNN that\nincorporates graph transfer learning for propagating relevant semantic\ninformation across multiple datasets to reach semantic coherency. Specifically,\nwe first generate a global semantic pool by integrating all high-level semantic\nrepresentation of all the categories. Then an Intra-Domain Reasoning Module\nlearns and propagates the sparse graph representation within one dataset guided\nby a spatial-aware GCN. Finally, an InterDomain Transfer Module is proposed to\nexploit diverse transfer dependencies across all domains and enhance the\nregional feature representation by attending and transferring semantic contexts\nglobally. Extensive experiments demonstrate that the proposed method\nsignificantly outperforms multiple-branch models and achieves the\nstate-of-the-art results on multiple object detection benchmarks (mAP: 49.1% on\nCOCO).\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 07:57:45 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Xu", "Hang", ""], ["Fang", "Linpu", ""], ["Liang", "Xiaodan", ""], ["Kang", "Wenxiong", ""], ["Li", "Zhenguo", ""]]}, {"id": "2002.07421", "submitter": "Linpu Fang", "authors": "Linpu Fang, Hang Xu, Zhili Liu, Sarah Parisot, Zhenguo Li", "title": "EHSOD: CAM-Guided End-to-end Hybrid-Supervised Object Detection with\n  Cascade Refinement", "comments": "Accepted by AAAI20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detectors trained on fully-annotated data currently yield state of the\nart performance but require expensive manual annotations. On the other hand,\nweakly-supervised detectors have much lower performance and cannot be used\nreliably in a realistic setting. In this paper, we study the hybrid-supervised\nobject detection problem, aiming to train a high quality detector with only a\nlimited amount of fullyannotated data and fully exploiting cheap data with\nimagelevel labels. State of the art methods typically propose an iterative\napproach, alternating between generating pseudo-labels and updating a detector.\nThis paradigm requires careful manual hyper-parameter tuning for mining good\npseudo labels at each round and is quite time-consuming. To address these\nissues, we present EHSOD, an end-to-end hybrid-supervised object detection\nsystem which can be trained in one shot on both fully and weakly-annotated\ndata. Specifically, based on a two-stage detector, we proposed two modules to\nfully utilize the information from both kinds of labels: 1) CAMRPN module aims\nat finding foreground proposals guided by a class activation heat-map; 2)\nhybrid-supervised cascade module further refines the bounding-box position and\nclassification with the help of an auxiliary head compatible with image-level\ndata. Extensive experiments demonstrate the effectiveness of the proposed\nmethod and it achieves comparable results on multiple object detection\nbenchmarks with only 30% fully-annotated data, e.g. 37.5% mAP on COCO. We will\nrelease the code and the trained models.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 08:04:58 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Fang", "Linpu", ""], ["Xu", "Hang", ""], ["Liu", "Zhili", ""], ["Parisot", "Sarah", ""], ["Li", "Zhenguo", ""]]}, {"id": "2002.07442", "submitter": "Sheng Guo", "authors": "Shiwen Zhang and Sheng Guo and Weilin Huang and Matthew R. Scott and\n  Limin Wang", "title": "V4D:4D Convolutional Neural Networks for Video-level Representation\n  Learning", "comments": "To appear in ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing 3D CNNs for video representation learning are clip-based\nmethods, and thus do not consider video-level temporal evolution of\nspatio-temporal features. In this paper, we propose Video-level 4D\nConvolutional Neural Networks, referred as V4D, to model the evolution of\nlong-range spatio-temporal representation with 4D convolutions, and at the same\ntime, to preserve strong 3D spatio-temporal representation with residual\nconnections. Specifically, we design a new 4D residual block able to capture\ninter-clip interactions, which could enhance the representation power of the\noriginal clip-level 3D CNNs. The 4D residual blocks can be easily integrated\ninto the existing 3D CNNs to perform long-range modeling hierarchically. We\nfurther introduce the training and inference methods for the proposed V4D.\nExtensive experiments are conducted on three video recognition benchmarks,\nwhere V4D achieves excellent results, surpassing recent 3D CNNs by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 09:27:41 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Zhang", "Shiwen", ""], ["Guo", "Sheng", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""], ["Wang", "Limin", ""]]}, {"id": "2002.07464", "submitter": "Jihua Zhu", "authors": "Jihua Zhu, Jing Zhang, Huimin Lu, and Zhongyu Li", "title": "Registration of multi-view point sets under the perspective of\n  expectation-maximization", "comments": null, "journal-ref": "IEEE TIP 2020", "doi": "10.1109/TIP.2020.3024096", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration of multi-view point sets is a prerequisite for 3D model\nreconstruction. To solve this problem, most of previous approaches either\npartially explore available information or blindly utilize unnecessary\ninformation to align each point set, which may lead to the undesired results or\nintroduce extra computation complexity. To this end, this paper consider the\nmulti-view registration problem as a maximum likelihood estimation problem and\nproposes a novel multi-view registration approach under the perspective of\nExpectation-Maximization (EM). The basic idea of our approach is that different\ndata points are generated by the same number of Gaussian mixture models (GMMs).\nFor each data point in one point set, its nearest neighbors can be searched\nfrom other well-aligned point sets. Then, we can suppose this data point is\ngenerated by the special GMM, which is composed of each nearest neighbor\nadhered with one Gaussian distribution. Based on this assumption, it is\nreasonable to define the likelihood function including all rigid\ntransformations, which requires to be estimated for multi-view registration.\nSubsequently, the EM algorithm is utilized to maximize the likelihood function\nso as to estimate all rigid transformations. Finally, the proposed approach is\ntested on several bench mark data sets and compared with some state-of-the-art\nalgorithms. Experimental results illustrate its super performance on accuracy,\nrobustness and efficiency for the registration of multi-view point sets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:04:51 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 08:55:53 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhu", "Jihua", ""], ["Zhang", "Jing", ""], ["Lu", "Huimin", ""], ["Li", "Zhongyu", ""]]}, {"id": "2002.07468", "submitter": "Warasinee Chaisangmongkon", "authors": "Isarun Chamveha, Treethep Promwiset, Trongtum Tongdee, Pairash\n  Saiviroonporn and Warasinee Chaisangmongkon", "title": "Automated Cardiothoracic Ratio Calculation and Cardiomegaly Detection\n  using Deep Learning Approach", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for calculating the cardiothoracic ratio (CTR) from\nchest X-ray films. Our approach applies a deep learning model based on U-Net\nwith VGG16 encoder to extract lung and heart masks from chest X-ray images and\ncalculate CTR from the extents of obtained masks. Human radiologists evaluated\nour CTR measurements, and $76.5\\%$ were accepted to be included in medical\nreports without any need for adjustment. This result translates to a large\namount of time and labor saved for radiologists using our automated tools.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:10:28 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Chamveha", "Isarun", ""], ["Promwiset", "Treethep", ""], ["Tongdee", "Trongtum", ""], ["Saiviroonporn", "Pairash", ""], ["Chaisangmongkon", "Warasinee", ""]]}, {"id": "2002.07471", "submitter": "Sheng Guo", "authors": "Shiwen Zhang and Sheng Guo and Limin Wang and Weilin Huang and Matthew\n  R. Scott", "title": "Knowledge Integration Networks for Action Recognition", "comments": "To appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose Knowledge Integration Networks (referred as KINet)\nfor video action recognition. KINet is capable of aggregating meaningful\ncontext features which are of great importance to identifying an action, such\nas human information and scene context. We design a three-branch architecture\nconsisting of a main branch for action recognition, and two auxiliary branches\nfor human parsing and scene recognition which allow the model to encode the\nknowledge of human and scene for action recognition. We explore two pre-trained\nmodels as teacher networks to distill the knowledge of human and scene for\ntraining the auxiliary tasks of KINet. Furthermore, we propose a two-level\nknowledge encoding mechanism which contains a Cross Branch Integration (CBI)\nmodule for encoding the auxiliary knowledge into medium-level convolutional\nfeatures, and an Action Knowledge Graph (AKG) for effectively fusing high-level\ncontext information. This results in an end-to-end trainable framework where\nthe three tasks can be trained collaboratively, allowing the model to compute\nstrong context knowledge efficiently. The proposed KINet achieves the\nstate-of-the-art performance on a large-scale action recognition benchmark\nKinetics-400, with a top-1 accuracy of 77.8%. We further demonstrate that our\nKINet has strong capability by transferring the Kinetics-trained model to\nUCF-101, where it obtains 97.8% top-1 accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:20:30 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Zhang", "Shiwen", ""], ["Guo", "Sheng", ""], ["Wang", "Limin", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""]]}, {"id": "2002.07483", "submitter": "Shay Elmalem", "authors": "Shay Elmalem, Raja Giryes and Emanuel Marom", "title": "Motion Deblurring using Spatiotemporal Phase Aperture Coding", "comments": "10 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion blur is a known issue in photography, as it limits the exposure time\nwhile capturing moving objects. Extensive research has been carried to\ncompensate for it. In this work, a computational imaging approach for motion\ndeblurring is proposed and demonstrated. Using dynamic phase-coding in the lens\naperture during the image acquisition, the trajectory of the motion is encoded\nin an intermediate optical image. This encoding embeds both the motion\ndirection and extent by coloring the spatial blur of each object. The color\ncues serve as prior information for a blind deblurring process, implemented\nusing a convolutional neural network (CNN) trained to utilize such coding for\nimage restoration. We demonstrate the advantage of the proposed approach over\nblind-deblurring with no coding and other solutions that use coded acquisition,\nboth in simulation and real-world experiments.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:46:14 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Elmalem", "Shay", ""], ["Giryes", "Raja", ""], ["Marom", "Emanuel", ""]]}, {"id": "2002.07487", "submitter": "Florian Lemarchand", "authors": "Florian Lemarchand, Erwan Nogues and Maxime Pelcat", "title": "NoiseBreaker: Gradual Image Denoising Guided by Noise Analysis", "comments": "ACCEPTED by MMSP20", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully supervised deep-learning based denoisers are currently the most\nperforming image denoising solutions. However, they require clean reference\nimages. When the target noise is complex, e.g. composed of an unknown mixture\nof primary noises with unknown intensity, fully supervised solutions are\nlimited by the difficulty to build a suited training set for the problem. This\npaper proposes a gradual denoising strategy that iteratively detects the\ndominating noise in an image, and removes it using a tailored denoiser. The\nmethod is shown to keep up with state of the art blind denoisers on mixture\nnoises. Moreover, noise analysis is demonstrated to guide denoisers efficiently\nnot only on noise type, but also on noise intensity. The method provides an\ninsight on the nature of the encountered noise, and it makes it possible to\nextend an existing denoiser with new noise nature. This feature makes the\nmethod adaptive to varied denoising cases.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 11:09:03 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 13:46:09 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Lemarchand", "Florian", ""], ["Nogues", "Erwan", ""], ["Pelcat", "Maxime", ""]]}, {"id": "2002.07493", "submitter": "Michael Steininger", "authors": "Michael Steininger, Konstantin Kobs, Albin Zehe, Florian\n  Lautenschlager, Martin Becker, Andreas Hotho", "title": "MapLUR: Exploring a new Paradigm for Estimating Air Pollution using Deep\n  Learning on Map Images", "comments": "Accepted for publication in ACM TSAS - Special Issue on Deep Learning", "journal-ref": null, "doi": "10.1145/3380973", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land-use regression (LUR) models are important for the assessment of air\npollution concentrations in areas without measurement stations. While many such\nmodels exist, they often use manually constructed features based on restricted,\nlocally available data. Thus, they are typically hard to reproduce and\nchallenging to adapt to areas beyond those they have been developed for. In\nthis paper, we advocate a paradigm shift for LUR models: We propose the\nData-driven, Open, Global (DOG) paradigm that entails models based on purely\ndata-driven approaches using only openly and globally available data. Progress\nwithin this paradigm will alleviate the need for experts to adapt models to the\nlocal characteristics of the available data sources and thus facilitate the\ngeneralizability of air pollution models to new areas on a global scale. In\norder to illustrate the feasibility of the DOG paradigm for LUR, we introduce a\ndeep learning model called MapLUR. It is based on a convolutional neural\nnetwork architecture and is trained exclusively on globally and openly\navailable map data without requiring manual feature engineering. We compare our\nmodel to state-of-the-art baselines like linear regression, random forests and\nmulti-layer perceptrons using a large data set of modeled $\\text{NO}_2$\nconcentrations in Central London. Our results show that MapLUR significantly\noutperforms these approaches even though they are provided with manually\ntailored features. Furthermore, we illustrate that the automatic feature\nextraction inherent to models based on the DOG paradigm can learn features that\nare readily interpretable and closely resemble those commonly used in\ntraditional LUR approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 11:21:55 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Steininger", "Michael", ""], ["Kobs", "Konstantin", ""], ["Zehe", "Albin", ""], ["Lautenschlager", "Florian", ""], ["Becker", "Martin", ""], ["Hotho", "Andreas", ""]]}, {"id": "2002.07522", "submitter": "Yann Lifchitz", "authors": "Yann Lifchitz, Yannis Avrithis, Sylvaine Picard", "title": "Few-Shot Few-Shot Learning and the role of Spatial Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is often motivated by the ability of humans to learn new\ntasks from few examples. However, standard few-shot classification benchmarks\nassume that the representation is learned on a limited amount of base class\ndata, ignoring the amount of prior knowledge that a human may have accumulated\nbefore learning new tasks. At the same time, even if a powerful representation\nis available, it may happen in some domain that base class data are limited or\nnon-existent. This motivates us to study a problem where the representation is\nobtained from a classifier pre-trained on a large-scale dataset of a different\ndomain, assuming no access to its training process, while the base class data\nare limited to few examples per class and their role is to adapt the\nrepresentation to the domain at hand rather than learn from scratch. We adapt\nthe representation in two stages, namely on the few base class data if\navailable and on the even fewer data of new tasks. In doing so, we obtain from\nthe pre-trained classifier a spatial attention map that allows focusing on\nobjects and suppressing background clutter. This is important in the new\nproblem, because when base class data are few, the network cannot learn where\nto focus implicitly. We also show that a pre-trained network may be easily\nadapted to novel classes, without meta-learning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 12:32:01 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Lifchitz", "Yann", ""], ["Avrithis", "Yannis", ""], ["Picard", "Sylvaine", ""]]}, {"id": "2002.07613", "submitter": "Yiqiu Shen", "authors": "Yiqiu Shen, Nan Wu, Jason Phang, Jungkyu Park, Kangning Liu,\n  Sudarshini Tyagi, Laura Heacock, S. Gene Kim, Linda Moy, Kyunghyun Cho,\n  Krzysztof J. Geras", "title": "An interpretable classifier for high-resolution breast cancer screening\n  images utilizing weakly supervised localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images differ from natural images in significantly higher resolutions\nand smaller regions of interest. Because of these differences, neural network\narchitectures that work well for natural images might not be applicable to\nmedical image analysis. In this work, we extend the globally-aware multiple\ninstance classifier, a framework we proposed to address these unique properties\nof medical images. This model first uses a low-capacity, yet memory-efficient,\nnetwork on the whole image to identify the most informative regions. It then\napplies another higher-capacity network to collect details from chosen regions.\nFinally, it employs a fusion module that aggregates global and local\ninformation to make a final prediction. While existing methods often require\nlesion segmentation during training, our model is trained with only image-level\nlabels and can generate pixel-level saliency maps indicating possible malignant\nfindings. We apply the model to screening mammography interpretation:\npredicting the presence or absence of benign and malignant lesions. On the NYU\nBreast Cancer Screening Dataset, consisting of more than one million images,\nour model achieves an AUC of 0.93 in classifying breasts with malignant\nfindings, outperforming ResNet-34 and Faster R-CNN. Compared to ResNet-34, our\nmodel is 4.1x faster for inference while using 78.4% less GPU memory.\nFurthermore, we demonstrate, in a reader study, that our model surpasses\nradiologist-level AUC by a margin of 0.11. The proposed model is available\nonline: https://github.com/nyukat/GMIC.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 15:28:42 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Shen", "Yiqiu", ""], ["Wu", "Nan", ""], ["Phang", "Jason", ""], ["Park", "Jungkyu", ""], ["Liu", "Kangning", ""], ["Tyagi", "Sudarshini", ""], ["Heacock", "Laura", ""], ["Kim", "S. Gene", ""], ["Moy", "Linda", ""], ["Cho", "Kyunghyun", ""], ["Geras", "Krzysztof J.", ""]]}, {"id": "2002.07621", "submitter": "Steven Frank", "authors": "Steven J. Frank", "title": "Resource-Frugal Classification and Analysis of Pathology Slides Using\n  Image Entropy", "comments": null, "journal-ref": "Biomedical Signal Processing and Control, vol. 66, April 2021,\n  102388", "doi": "10.1016/j.bspc.2020.102388", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathology slides of lung malignancies are classified using resource-frugal\nconvolution neural networks (CNNs) that may be deployed on mobile devices. In\nparticular, the challenging task of distinguishing adenocarcinoma (LUAD) and\nsquamous-cell carcinoma (LUSC) lung cancer subtypes is approached in two\nstages. First, whole-slide histopathology images are downsampled to a size too\nlarge for CNN analysis but large enough to retain key anatomic detail. The\ndownsampled images are decomposed into smaller square tiles, which are sifted\nbased on their image entropies. A lightweight CNN produces tile-level\nclassifications that are aggregated to classify the slide. The resulting\naccuracies are comparable to those obtained with much more complex CNNs and\nlarger training sets. To allow clinicians to visually assess the basis for the\nclassification -- that is, to see the image regions that underlie it --\ncolor-coded probability maps are created by overlapping tiles and averaging the\ntile-level probabilities at a pixel level.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 18:42:36 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 19:33:45 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 19:26:41 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Frank", "Steven J.", ""]]}, {"id": "2002.07643", "submitter": "Sergey Berezin", "authors": "S. A. Berezin, V.M. Volkova", "title": "Neural arbitrary style transfer for portrait images using the attention\n  mechanism", "comments": "in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary style transfer is the task of synthesis of an image that has never\nbeen seen before, using two given images: content image and style image. The\ncontent image forms the structure, the basic geometric lines and shapes of the\nresulting image, while the style image sets the color and texture of the\nresult. The word \"arbitrary\" in this context means the absence of any one\npre-learned style. So, for example, convolutional neural networks capable of\ntransferring a new style only after training or retraining on a new amount of\ndata are not con-sidered to solve such a problem, while networks based on the\nattention mech-anism that are capable of performing such a transformation\nwithout retraining - yes. An original image can be, for example, a photograph,\nand a style image can be a painting of a famous artist. The resulting image in\nthis case will be the scene depicted in the original photograph, made in the\nstylie of this picture. Recent arbitrary style transfer algorithms make it\npossible to achieve good re-sults in this task, however, in processing portrait\nimages of people, the result of such algorithms is either unacceptable due to\nexcessive distortion of facial features, or weakly expressed, not bearing the\ncharacteristic features of a style image. In this paper, we consider an\napproach to solving this problem using the combined architecture of deep neural\nnetworks with a attention mechanism that transfers style based on the contents\nof a particular image segment: with a clear predominance of style over the form\nfor the background part of the im-age, and with the prevalence of content over\nthe form in the image part con-taining directly the image of a person.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 13:59:58 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Berezin", "S. A.", ""], ["Volkova", "V. M.", ""]]}, {"id": "2002.07662", "submitter": "Niels Ole Salscheider", "authors": "Niels Ole Salscheider", "title": "FeatureNMS: Non-Maximum Suppression by Learning Feature Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state of the art object detectors output multiple detections per object.\nThe duplicates are removed in a post-processing step called Non-Maximum\nSuppression. Classical Non-Maximum Suppression has shortcomings in scenes that\ncontain objects with high overlap: This heuristic assumes that a high overlap\nbetween two bounding boxes corresponds to a high probability of one being a\nduplicate. We propose FeatureNMS to solve this problem. FeatureNMS recognizes\nduplicates not only based on the intersection over union between the bounding\nboxes, but also based on the difference of feature vectors. These feature\nvectors can encode more information like visual appearance. Our approach\noutperforms classical NMS and derived approaches and achieves state of the art\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 15:50:37 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 15:54:06 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Salscheider", "Niels Ole", ""]]}, {"id": "2002.07686", "submitter": "Moran Shkolnik", "authors": "Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan,\n  Alex Bronstein, Uri Weiser", "title": "Robust Quantization: One Model to Rule Them All", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network quantization methods often involve simulating the quantization\nprocess during training, making the trained model highly dependent on the\ntarget bit-width and precise way quantization is performed. Robust quantization\noffers an alternative approach with improved tolerance to different classes of\ndata-types and quantization policies. It opens up new exciting applications\nwhere the quantization process is not static and can vary to meet different\ncircumstances and implementations. To address this issue, we propose a method\nthat provides intrinsic robustness to the model against a broad range of\nquantization processes. Our method is motivated by theoretical arguments and\nenables us to store a single generic model capable of operating at various\nbit-widths and quantization policies. We validate our method's effectiveness on\ndifferent ImageNet models.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:14:36 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 15:18:40 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 08:46:01 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Shkolnik", "Moran", ""], ["Chmiel", "Brian", ""], ["Banner", "Ron", ""], ["Shomron", "Gil", ""], ["Nahshan", "Yury", ""], ["Bronstein", "Alex", ""], ["Weiser", "Uri", ""]]}, {"id": "2002.07689", "submitter": "Patrick H\\\"ubner", "authors": "P. H\\\"ubner, M. Weinmann, S. Wursthorn", "title": "Voxel-Based Indoor Reconstruction From HoloLens Triangle Meshes", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current mobile augmented reality devices are often equipped with range\nsensors. The Microsoft HoloLens for instance is equipped with a Time-Of-Flight\n(ToF) range camera providing coarse triangle meshes that can be used in custom\napplications. We suggest to use the triangle meshes for the automatic\ngeneration of indoor models that can serve as basis for augmenting their\nphysical counterpart with location-dependent information. In this paper, we\npresent a novel voxel-based approach for automated indoor reconstruction from\nunstructured three-dimensional geometries like triangle meshes. After an\ninitial voxelization of the input data, rooms are detected in the resulting\nvoxel grid by segmenting connected voxel components of ceiling candidates and\nextruding them downwards to find floor candidates. Semantic class labels like\n'Wall', 'Wall Opening', 'Interior Object' and 'Empty Interior' are then\nassigned to the room voxels in-between ceiling and floor by a rule-based voxel\nsweep algorithm. Finally, the geometry of the detected walls and their openings\nis refined in voxel representation. The proposed approach is not restricted to\nManhattan World scenarios and does not rely on room surfaces being planar.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:15:17 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["H\u00fcbner", "P.", ""], ["Weinmann", "M.", ""], ["Wursthorn", "S.", ""]]}, {"id": "2002.07703", "submitter": "Ziyang Wang", "authors": "Ziyang Wang", "title": "Deep Learning in Medical Ultrasound Image Segmentation: a Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying machine learning technologies, especially deep learning, into\nmedical image segmentation is being widely studied because of its\nstate-of-the-art performance and results. It can be a key step to provide a\nreliable basis for clinical diagnosis, such as 3D reconstruction of human\ntissues, image-guided interventions, image analyzing and visualization. In this\nreview article, deep-learning-based methods for ultrasound image segmentation\nare categorized into six main groups according to their architectures and\ntraining at first. Secondly, for each group, several current representative\nalgorithms are selected, introduced, analyzed and summarized in detail. In\naddition, common evaluation methods for image segmentation and ultrasound image\nsegmentation datasets are summarized. Further, the performance of the current\nmethods and their evaluations are reviewed. In the end, the challenges and\npotential research directions for medical ultrasound image segmentation are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:33:22 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 18:29:26 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 00:05:17 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wang", "Ziyang", ""]]}, {"id": "2002.07705", "submitter": "Ujwal Bonde", "authors": "Ujwal Bonde and Pablo F. Alcantarilla and Stefan Leutenegger", "title": "Towards Bounding-Box Free Panoptic Segmentation", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we introduce a new Bounding-Box Free Network (BBFNet) for\npanoptic segmentation. Panoptic segmentation is an ideal problem for\nproposal-free methods as it already requires per-pixel semantic class labels.\nWe use this observation to exploit class boundaries from off-the-shelf semantic\nsegmentation networks and refine them to predict instance labels. Towards this\ngoal BBFNet predicts coarse watershed levels and uses them to detect large\ninstance candidates where boundaries are well defined. For smaller instances,\nwhose boundaries are less reliable, BBFNet also predicts instance centers by\nmeans of Hough voting followed by mean-shift to reliably detect small objects.\nA novel triplet loss network helps merging fragmented instances while refining\nboundary pixels. Our approach is distinct from previous works in panoptic\nsegmentation that rely on a combination of a semantic segmentation network with\na computationally costly instance segmentation network based on bounding box\nproposals, such as Mask R-CNN, to guide the prediction of instance labels using\na Mixture-of-Expert (MoE) approach. We benchmark our proposal-free method on\nCityscapes and Microsoft COCO datasets and show competitive performance with\nother MoE based approaches while outperforming existing non-proposal based\nmethods on the COCO dataset. We show the flexibility of our method using\ndifferent semantic segmentation backbones.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:34:01 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 14:17:23 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 17:48:08 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Bonde", "Ujwal", ""], ["Alcantarilla", "Pablo F.", ""], ["Leutenegger", "Stefan", ""]]}, {"id": "2002.07754", "submitter": "Elena Limonova", "authors": "Elena Limonova and Alexander Sheshkus and Dmitry Nikolaev", "title": "Computational optimization of convolutional neural networks using\n  separated filters architecture", "comments": "4 pages, 3 figures", "journal-ref": "International Journal of Applied Engineering Research (ISSN\n  0973-4562), Volume 11, Number 11 (2016), pp 7491-7494", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a convolutional neural network transformation that\nreduces computation complexity and thus speedups neural network processing.\nUsage of convolutional neural networks (CNN) is the standard approach to image\nrecognition despite the fact they can be too computationally demanding, for\nexample for recognition on mobile platforms or in embedded systems. In this\npaper we propose CNN structure transformation which expresses 2D convolution\nfilters as a linear combination of separable filters. It allows to obtain\nseparated convolutional filters by standard training algorithms. We study the\ncomputation efficiency of this structure transformation and suggest fast\nimplementation easily handled by CPU or GPU. We demonstrate that CNNs designed\nfor letter and digit recognition of proposed structure show 15% speedup without\naccuracy loss in industrial image recognition system. In conclusion, we discuss\nthe question of possible accuracy decrease and the application of proposed\ntransformation to different recognition problems. convolutional neural\nnetworks, computational optimization, separable filters, complexity reduction.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 17:42:13 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Limonova", "Elena", ""], ["Sheshkus", "Alexander", ""], ["Nikolaev", "Dmitry", ""]]}, {"id": "2002.07766", "submitter": "Matthew Willetts", "authors": "Alexander Camuto, Matthew Willetts, Brooks Paige, Chris Holmes,\n  Stephen Roberts", "title": "Learning Bijective Feature Maps for Linear ICA", "comments": "8 pages", "journal-ref": "AISTATS 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separating high-dimensional data like images into independent latent factors,\ni.e independent component analysis (ICA), remains an open research problem. As\nwe show, existing probabilistic deep generative models (DGMs), which are\ntailor-made for image data, underperform on non-linear ICA tasks. To address\nthis, we propose a DGM which combines bijective feature maps with a linear ICA\nmodel to learn interpretable latent structures for high-dimensional data. Given\nthe complexities of jointly training such a hybrid model, we introduce novel\ntheory that constrains linear ICA to lie close to the manifold of orthogonal\nrectangular matrices, the Stiefel manifold. By doing so we create models that\nconverge quickly, are easy to train, and achieve better unsupervised latent\nfactor discovery than flow-based models, linear ICA, and Variational\nAutoencoders on images.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 17:58:07 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 11:03:28 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 17:10:45 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2020 17:57:27 GMT"}, {"version": "v5", "created": "Fri, 29 Jan 2021 18:08:28 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Camuto", "Alexander", ""], ["Willetts", "Matthew", ""], ["Paige", "Brooks", ""], ["Holmes", "Chris", ""], ["Roberts", "Stephen", ""]]}, {"id": "2002.07772", "submitter": "Hussein Hazimeh", "authors": "Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, Rahul\n  Mazumder", "title": "The Tree Ensemble Layer: Differentiability meets Conditional Computation", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks and tree ensembles are state-of-the-art learners, each with\nits unique statistical and computational advantages. We aim to combine these\nadvantages by introducing a new layer for neural networks, composed of an\nensemble of differentiable decision trees (a.k.a. soft trees). While\ndifferentiable trees demonstrate promising results in the literature, they are\ntypically slow in training and inference as they do not support conditional\ncomputation. We mitigate this issue by introducing a new sparse activation\nfunction for sample routing, and implement true conditional computation by\ndeveloping specialized forward and backward propagation algorithms that exploit\nsparsity. Our efficient algorithms pave the way for jointly training over deep\nand wide tree ensembles using first-order methods (e.g., SGD). Experiments on\n23 classification datasets indicate over 10x speed-ups compared to the\ndifferentiable trees used in the literature and over 20x reduction in the\nnumber of parameters compared to gradient boosted trees, while maintaining\ncompetitive performance. Moreover, experiments on CIFAR, MNIST, and Fashion\nMNIST indicate that replacing dense layers in CNNs with our tree layer reduces\nthe test loss by 7-53% and the number of parameters by 8x. We provide an\nopen-source TensorFlow implementation with a Keras API.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:05:31 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 00:40:16 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Hazimeh", "Hussein", ""], ["Ponomareva", "Natalia", ""], ["Mol", "Petros", ""], ["Tan", "Zhenyu", ""], ["Mazumder", "Rahul", ""]]}, {"id": "2002.07793", "submitter": "Zihang Lai", "authors": "Zihang Lai, Erika Lu, Weidi Xie", "title": "MAST: A Memory-Augmented Self-supervised Tracker", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent interest in self-supervised dense tracking has yielded rapid progress,\nbut performance still remains far from supervised methods. We propose a dense\ntracking model trained on videos without any annotations that surpasses\nprevious self-supervised methods on existing benchmarks by a significant margin\n(+15%), and achieves performance comparable to supervised methods. In this\npaper, we first reassess the traditional choices used for self-supervised\ntraining and reconstruction loss by conducting thorough experiments that\nfinally elucidate the optimal choices. Second, we further improve on existing\nmethods by augmenting our architecture with a crucial memory component. Third,\nwe benchmark on large-scale semi-supervised video object segmentation(aka.\ndense tracking), and propose a new metric: generalizability. Our first two\ncontributions yield a self-supervised network that for the first time is\ncompetitive with supervised methods on standard evaluation metrics of dense\ntracking. When measuring generalizability, we show self-supervised approaches\nare actually superior to the majority of supervised methods. We believe this\nnew generalizability metric can better capture the real-world use-cases for\ndense tracking, and will spur new interest in this research direction.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:43:28 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 00:58:21 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Lai", "Zihang", ""], ["Lu", "Erika", ""], ["Xie", "Weidi", ""]]}, {"id": "2002.07798", "submitter": "Jerone Andrews", "authors": "Jerone T. A. Andrews, Yidan Zhang, Lewis D. Griffin", "title": "Conditional Adversarial Camera Model Anonymization", "comments": "ECCV 2020 - Advances in Image Manipulation workshop (AIM 2020)", "journal-ref": null, "doi": "10.1007/978-3-030-66823-5_13", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model of camera that was used to capture a particular photographic image\n(model attribution) is typically inferred from high-frequency model-specific\nartifacts present within the image. Model anonymization is the process of\ntransforming these artifacts such that the apparent capture model is changed.\nWe propose a conditional adversarial approach for learning such\ntransformations. In contrast to previous works, we cast model anonymization as\nthe process of transforming both high and low spatial frequency information. We\naugment the objective with the loss from a pre-trained dual-stream model\nattribution classifier, which constrains the generative network to transform\nthe full range of artifacts. Quantitative comparisons demonstrate the efficacy\nof our framework in a restrictive non-interactive black-box setting.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:53:21 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 00:35:06 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 13:42:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Andrews", "Jerone T. A.", ""], ["Zhang", "Yidan", ""], ["Griffin", "Lewis D.", ""]]}, {"id": "2002.07875", "submitter": "Manu Tom", "authors": "Rajanie Prabha, Manu Tom, Mathias Rothermel, Emmanuel Baltsavias,\n  Laura Leal-Taixe, Konrad Schindler", "title": "Lake Ice Monitoring with Webcams and Crowd-Sourced Images", "comments": "Accepted for ISPRS Congress 2020, Nice, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lake ice is a strong climate indicator and has been recognised as part of the\nEssential Climate Variables (ECV) by the Global Climate Observing System\n(GCOS). The dynamics of freezing and thawing, and possible shifts of freezing\npatterns over time, can help in understanding the local and global climate\nsystems. One way to acquire the spatio-temporal information about lake ice\nformation, independent of clouds, is to analyse webcam images. This paper\nintends to move towards a universal model for monitoring lake ice with freely\navailable webcam data. We demonstrate good performance, including the ability\nto generalise across different winters and different lakes, with a\nstate-of-the-art Convolutional Neural Network (CNN) model for semantic image\nsegmentation, Deeplab v3+. Moreover, we design a variant of that model, termed\nDeep-U-Lab, which predicts sharper, more correct segmentation boundaries. We\nhave tested the model's ability to generalise with data from multiple camera\nviews and two different winters. On average, it achieves\nintersection-over-union (IoU) values of ~71% across different cameras and ~69%\nacross different winters, greatly outperforming prior work. Going even further,\nwe show that the model even achieves 60% IoU on arbitrary images scraped from\nphoto-sharing web sites. As part of the work, we introduce a new benchmark\ndataset of webcam images, Photi-LakeIce, from multiple cameras and two\ndifferent winters, along with pixel-wise ground truth annotations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 20:46:46 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 03:02:35 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Prabha", "Rajanie", ""], ["Tom", "Manu", ""], ["Rothermel", "Mathias", ""], ["Baltsavias", "Emmanuel", ""], ["Leal-Taixe", "Laura", ""], ["Schindler", "Konrad", ""]]}, {"id": "2002.07877", "submitter": "Subhadip Maji", "authors": "Subhadip Maji and Smarajit Bose", "title": "CBIR using features derived by Deep Learning", "comments": "18 pages, 31 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Content Based Image Retrieval (CBIR) System, the task is to retrieve\nsimilar images from a large database given a query image. The usual procedure\nis to extract some useful features from the query image, and retrieve images\nwhich have similar set of features. For this purpose, a suitable similarity\nmeasure is chosen, and images with high similarity scores are retrieved.\nNaturally the choice of these features play a very important role in the\nsuccess of this system, and high level features are required to reduce the\nsemantic gap.\n  In this paper, we propose to use features derived from pre-trained network\nmodels from a deep-learning convolution network trained for a large image\nclassification problem. This approach appears to produce vastly superior\nresults for a variety of databases, and it outperforms many contemporary CBIR\nsystems. We analyse the retrieval time of the method, and also propose a\npre-clustering of the database based on the above-mentioned features which\nyields comparable results in a much shorter time in most of the cases.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 21:26:32 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Maji", "Subhadip", ""], ["Bose", "Smarajit", ""]]}, {"id": "2002.07891", "submitter": "Pu Zhao", "authors": "Pu Zhao, Pin-Yu Chen, Siyue Wang, Xue Lin", "title": "Towards Query-Efficient Black-Box Adversary with Zeroth-Order Natural\n  Gradient Descent", "comments": "accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great achievements of the modern deep neural networks (DNNs), the\nvulnerability/robustness of state-of-the-art DNNs raises security concerns in\nmany application domains requiring high reliability. Various adversarial\nattacks are proposed to sabotage the learning performance of DNN models. Among\nthose, the black-box adversarial attack methods have received special\nattentions owing to their practicality and simplicity. Black-box attacks\nusually prefer less queries in order to maintain stealthy and low costs.\nHowever, most of the current black-box attack methods adopt the first-order\ngradient descent method, which may come with certain deficiencies such as\nrelatively slow convergence and high sensitivity to hyper-parameter settings.\nIn this paper, we propose a zeroth-order natural gradient descent (ZO-NGD)\nmethod to design the adversarial attacks, which incorporates the zeroth-order\ngradient estimation technique catering to the black-box attack scenario and the\nsecond-order natural gradient descent to achieve higher query efficiency. The\nempirical evaluations on image classification datasets demonstrate that ZO-NGD\ncan obtain significantly lower model query complexities compared with\nstate-of-the-art attack methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 21:48:54 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Zhao", "Pu", ""], ["Chen", "Pin-Yu", ""], ["Wang", "Siyue", ""], ["Lin", "Xue", ""]]}, {"id": "2002.07897", "submitter": "Przemys{\\l}aw Spurek", "authors": "{\\L}ukasz Struski, Szymon Knop, Jacek Tabor, Wiktor Daniec,\n  Przemys{\\l}aw Spurek", "title": "LocoGAN -- Locally Convolutional GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper we construct a fully convolutional GAN model: LocoGAN, which\nlatent space is given by noise-like images of possibly different resolutions.\nThe learning is local, i.e. we process not the whole noise-like image, but the\nsub-images of a fixed size. As a consequence LocoGAN can produce images of\narbitrary dimensions e.g. LSUN bedroom data set. Another advantage of our\napproach comes from the fact that we use the position channels, which allows\nthe generation of fully periodic (e.g. cylindrical panoramic images) or almost\nperiodic ,,infinitely long\" images (e.g. wall-papers).\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 22:03:27 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Struski", "\u0141ukasz", ""], ["Knop", "Szymon", ""], ["Tabor", "Jacek", ""], ["Daniec", "Wiktor", ""], ["Spurek", "Przemys\u0142aw", ""]]}, {"id": "2002.07913", "submitter": "Le Hou", "authors": "Le Hou, Rajarsi Gupta, John S. Van Arnam, Yuwei Zhang, Kaustubh\n  Sivalenka, Dimitris Samaras, Tahsin M. Kurc, Joel H. Saltz", "title": "Dataset of Segmented Nuclei in Hematoxylin and Eosin Stained\n  Histopathology Images of 10 Cancer Types", "comments": null, "journal-ref": "Sci Data 7, 185 (2020)", "doi": "10.1038/s41597-020-0528-1", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution and appearance of nuclei are essential markers for the\ndiagnosis and study of cancer. Despite the importance of nuclear morphology,\nthere is a lack of large scale, accurate, publicly accessible nucleus\nsegmentation data. To address this, we developed an analysis pipeline that\nsegments nuclei in whole slide tissue images from multiple cancer types with a\nquality control process. We have generated nucleus segmentation results in\n5,060 Whole Slide Tissue images from 10 cancer types in The Cancer Genome\nAtlas. One key component of our work is that we carried out a multi-level\nquality control process (WSI-level and image patch-level), to evaluate the\nquality of our segmentation results. The image patch-level quality control used\nmanual segmentation ground truth data from 1,356 sampled image patches. The\ndatasets we publish in this work consist of roughly 5 billion quality\ncontrolled nuclei from more than 5,060 TCGA WSIs from 10 different TCGA cancer\ntypes and 1,356 manually segmented TCGA image patches from the same 10 cancer\ntypes plus additional 4 cancer types. Data is available at\nhttps://doi.org/10.7937/tcia.2019.4a4dkp9u\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 22:45:59 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 20:07:00 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Hou", "Le", ""], ["Gupta", "Rajarsi", ""], ["Van Arnam", "John S.", ""], ["Zhang", "Yuwei", ""], ["Sivalenka", "Kaustubh", ""], ["Samaras", "Dimitris", ""], ["Kurc", "Tahsin M.", ""], ["Saltz", "Joel H.", ""]]}, {"id": "2002.07920", "submitter": "Siyue Wang", "authors": "Xiao Wang, Siyue Wang, Pin-Yu Chen, Xue Lin, and Peter Chin", "title": "Block Switching: A Stochastic Approach for Deep Learning Security", "comments": "Accepted by AdvML19: Workshop on Adversarial Learning Methods for\n  Machine Learning and Data Mining at KDD, Anchorage, Alaska, USA, August 5th,\n  2019, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent study of adversarial attacks has revealed the vulnerability of modern\ndeep learning models. That is, subtly crafted perturbations of the input can\nmake a trained network with high accuracy produce arbitrary incorrect\npredictions, while maintain imperceptible to human vision system. In this\npaper, we introduce Block Switching (BS), a defense strategy against\nadversarial attacks based on stochasticity. BS replaces a block of model layers\nwith multiple parallel channels, and the active channel is randomly assigned in\nthe run time hence unpredictable to the adversary. We show empirically that BS\nleads to a more dispersed input gradient distribution and superior defense\neffectiveness compared with other stochastic defenses such as stochastic\nactivation pruning (SAP). Compared to other defenses, BS is also characterized\nby the following features: (i) BS causes less test accuracy drop; (ii) BS is\nattack-independent and (iii) BS is compatible with other defenses and can be\nused jointly with others.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 23:14:25 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Wang", "Xiao", ""], ["Wang", "Siyue", ""], ["Chen", "Pin-Yu", ""], ["Lin", "Xue", ""], ["Chin", "Peter", ""]]}, {"id": "2002.07953", "submitter": "Kuniaki Saito", "authors": "Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Kate Saenko", "title": "Universal Domain Adaptation through Self Supervision", "comments": "Accepted to NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation methods traditionally assume that all source\ncategories are present in the target domain. In practice, little may be known\nabout the category overlap between the two domains. While some methods address\ntarget settings with either partial or open-set categories, they assume that\nthe particular setting is known a priori. We propose a more universally\napplicable domain adaptation framework that can handle arbitrary category\nshift, called Domain Adaptative Neighborhood Clustering via Entropy\noptimization (DANCE). DANCE combines two novel ideas: First, as we cannot fully\nrely on source categories to learn features discriminative for the target, we\npropose a novel neighborhood clustering technique to learn the structure of the\ntarget domain in a self-supervised way. Second, we use entropy-based feature\nalignment and rejection to align target features with the source, or reject\nthem as unknown categories based on their entropy. We show through extensive\nexperiments that DANCE outperforms baselines across open-set, open-partial and\npartial domain adaptation settings. Implementation is available at\nhttps://github.com/VisionLearningGroup/DANCE.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 01:26:11 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 00:12:40 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 03:30:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Saito", "Kuniaki", ""], ["Kim", "Donghyun", ""], ["Sclaroff", "Stan", ""], ["Saenko", "Kate", ""]]}, {"id": "2002.07988", "submitter": "Lan Hu", "authors": "Lan Hu, Haomin Shi, and Laurent Kneip", "title": "Globally optimal point set registration by joint symmetry plane fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work proposes a solution to the challenging problem of\nregistering two partial point sets of the same object with very limited\noverlap. We leverage the fact that most objects found in man-made environments\ncontain a plane of symmetry. By reflecting the points of each set with respect\nto the plane of symmetry, we can largely increase the overlap between the sets\nand therefore boost the registration process. However, prior knowledge about\nthe plane of symmetry is generally unavailable or at least very hard to find,\nespecially with limited partial views, and finding this plane could strongly\nbenefit from a prior alignment of the partial point sets. We solve this\nchicken-and-egg problem by jointly optimizing the relative pose and symmetry\nplane parameters, and notably do so under global optimality by employing the\nbranch-and-bound (BnB) paradigm. Our results demonstrate a great improvement\nover the current state-of-the-art in globally optimal point set registration\nfor common objects. We furthermore show an interesting application of our\nmethod to dense 3D reconstruction of scenes with repetitive objects.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 03:40:04 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Hu", "Lan", ""], ["Shi", "Haomin", ""], ["Kneip", "Laurent", ""]]}, {"id": "2002.08005", "submitter": "Toru Tamaki", "authors": "Zhao Fangda, Toru Tamaki, Takio Kurita, Bisser Raytchev, Kazufumi\n  Kaneda", "title": "On-line non-overlapping camera calibration net", "comments": "7 pages", "journal-ref": "in Proc. of MIRU2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an easy-to-use non-overlapping camera calibration method. First,\nsuccessive images are fed to a PoseNet-based network to obtain ego-motion of\ncameras between frames. Next, the pose between cameras are estimated. Instead\nof using a batch method, we propose an on-line method of the inter-camera pose\nestimation. Furthermore, we implement the entire procedure on a computation\ngraph. Experiments with simulations and the KITTI dataset show the proposed\nmethod to be effective in simulation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 04:59:11 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Fangda", "Zhao", ""], ["Tamaki", "Toru", ""], ["Kurita", "Takio", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""]]}, {"id": "2002.08039", "submitter": "Abm Musa", "authors": "Abm Musa and Jakob Eriksson", "title": "Feasibility of Video-based Sub-meter Localization on\n  Resource-constrained Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the satellite-based Global Positioning System (GPS) is adequate for\nsome outdoor applications, many other applications are held back by its\nmulti-meter positioning errors and poor indoor coverage. In this paper, we\nstudy the feasibility of real-time video-based localization on\nresource-constrained platforms. Before commencing a localization task, a\nvideo-based localization system downloads an offline model of a restricted\ntarget environment, such as a set of city streets, or an indoor shopping mall.\nThe system is then able to localize the user within the model, using only video\nas input.\n  To enable such a system to run on resource-constrained embedded systems or\nsmartphones, we (a) propose techniques for efficiently building a 3D model of a\nsurveyed path, through frame selection and efficient feature matching, (b)\nsubstantially reduce model size by multiple compression techniques, without\nsacrificing localization accuracy, (c) propose efficient and concurrent\ntechniques for feature extraction and matching to enable online localization,\n(d) propose a method with interleaved feature matching and optical flow based\ntracking to reduce the feature extraction and matching time in online\nlocalization.\n  Based on an extensive set of both indoor and outdoor videos, manually\nannotated with location ground truth, we demonstrate that sub-meter accuracy,\nat real-time rates, is achievable on smart-phone type platforms, despite\nchallenging video conditions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 07:35:12 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Musa", "Abm", ""], ["Eriksson", "Jakob", ""]]}, {"id": "2002.08041", "submitter": "Hai Tran", "authors": "Hai H. Tran, Sumyeong Ahn, Taeyoung Lee, Yung Yi", "title": "Enlarging Discriminative Power by Adding an Extra Class in Unsupervised\n  Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of unsupervised domain adaptation that\naims at obtaining a prediction model for the target domain using labeled data\nfrom the source domain and unlabeled data from the target domain. There exists\nan array of recent research based on the idea of extracting features that are\nnot only invariant for both domains but also provide high discriminative power\nfor the target domain. In this paper, we propose an idea of empowering the\ndiscriminativeness: Adding a new, artificial class and training the model on\nthe data together with the GAN-generated samples of the new class. The trained\nmodel based on the new class samples is capable of extracting the features that\nare more discriminative by repositioning data of current classes in the target\ndomain and therefore drawing the decision boundaries more effectively. Our idea\nis highly generic so that it is compatible with many existing methods such as\nDANN, VADA, and DIRT-T. We conduct various experiments for the standard data\ncommonly used for the evaluation of unsupervised domain adaptations and\ndemonstrate that our algorithm achieves the SOTA performance for many\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 07:58:24 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Tran", "Hai H.", ""], ["Ahn", "Sumyeong", ""], ["Lee", "Taeyoung", ""], ["Yi", "Yung", ""]]}, {"id": "2002.08043", "submitter": "Yuan Xie", "authors": "Tong Wu, Yuan Xie, Yanyun Qu, Bicheng Dai, Shuxin Chen", "title": "Meta Segmentation Network for Ultra-Resolution Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress on semantic segmentation, there still exist huge\nchallenges in medical ultra-resolution image segmentation. The methods based on\nmulti-branch structure can make a good balance between computational burdens\nand segmentation accuracy. However, the fusion structure in these methods\nrequire to be designed elaborately to achieve desirable result, which leads to\nmodel redundancy. In this paper, we propose Meta Segmentation Network (MSN) to\nsolve this challenging problem. With the help of meta-learning, the fusion\nmodule of MSN is quite simple but effective. MSN can fast generate the weights\nof fusion layers through a simple meta-learner, requiring only a few training\nsamples and epochs to converge. In addition, to avoid learning all branches\nfrom scratch, we further introduce a particular weight sharing mechanism to\nrealize a fast knowledge adaptation and share the weights among multiple\nbranches, resulting in the performance improvement and significant parameters\nreduction. The experimental results on two challenging ultra-resolution medical\ndatasets BACH and ISIC show that MSN achieves the best performance compared\nwith the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 08:05:47 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Wu", "Tong", ""], ["Xie", "Yuan", ""], ["Qu", "Yanyun", ""], ["Dai", "Bicheng", ""], ["Chen", "Shuxin", ""]]}, {"id": "2002.08097", "submitter": "Subhajit Chaudhury", "authors": "Subhajit Chaudhury, Daiki Kimura, Phongtharin Vinayavekhin, Asim\n  Munawar, Ryuki Tachibana, Koji Ito, Yuki Inaba, Minoru Matsumoto, Shuji\n  Kidokoro and Hiroki Ozaki", "title": "Unsupervised Temporal Feature Aggregation for Event Detection in\n  Unstructured Sports Videos", "comments": "Accepted to IEEE International Symposium on Multimedia, 2019", "journal-ref": null, "doi": "10.1109/ISM46123.2019.00011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based sports analytics enable automatic retrieval of key events in a\ngame to speed up the analytics process for human experts. However, most\nexisting methods focus on structured television broadcast video datasets with a\nstraight and fixed camera having minimum variability in the capturing pose. In\nthis paper, we study the case of event detection in sports videos for\nunstructured environments with arbitrary camera angles. The transition from\nstructured to unstructured video analysis produces multiple challenges that we\naddress in our paper. Specifically, we identify and solve two major problems:\nunsupervised identification of players in an unstructured setting and\ngeneralization of the trained models to pose variations due to arbitrary\nshooting angles. For the first problem, we propose a temporal feature\naggregation algorithm using person re-identification features to obtain high\nplayer retrieval precision by boosting a weak heuristic scoring method.\nAdditionally, we propose a data augmentation technique, based on multi-modal\nimage translation model, to reduce bias in the appearance of training samples.\nExperimental evaluations show that our proposed method improves precision for\nplayer retrieval from 0.78 to 0.86 for obliquely angled videos. Additionally,\nwe obtain an improvement in F1 score for rally detection in table tennis videos\nfrom 0.79 in case of global frame-level features to 0.89 using our proposed\nplayer-level features. Please see the supplementary video submission at\nhttps://ibm.biz/BdzeZA.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 10:24:22 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Chaudhury", "Subhajit", ""], ["Kimura", "Daiki", ""], ["Vinayavekhin", "Phongtharin", ""], ["Munawar", "Asim", ""], ["Tachibana", "Ryuki", ""], ["Ito", "Koji", ""], ["Inaba", "Yuki", ""], ["Matsumoto", "Minoru", ""], ["Kidokoro", "Shuji", ""], ["Ozaki", "Hiroki", ""]]}, {"id": "2002.08098", "submitter": "Xiang Wang", "authors": "Xiang Wang, Sifei Liu, Huimin Ma, Ming-Hsuan Yang", "title": "Weakly-Supervised Semantic Segmentation by Iterative Affinity Learning", "comments": "IJCV 2020", "journal-ref": null, "doi": "10.1007/s11263-020-01293-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised semantic segmentation is a challenging task as no\npixel-wise label information is provided for training. Recent methods have\nexploited classification networks to localize objects by selecting regions with\nstrong response. While such response map provides sparse information, however,\nthere exist strong pairwise relations between pixels in natural images, which\ncan be utilized to propagate the sparse map to a much denser one. In this\npaper, we propose an iterative algorithm to learn such pairwise relations,\nwhich consists of two branches, a unary segmentation network which learns the\nlabel probabilities for each pixel, and a pairwise affinity network which\nlearns affinity matrix and refines the probability map generated from the unary\nnetwork. The refined results by the pairwise network are then used as\nsupervision to train the unary network, and the procedures are conducted\niteratively to obtain better segmentation progressively. To learn reliable\npixel affinity without accurate annotation, we also propose to mine confident\nregions. We show that iteratively training this framework is equivalent to\noptimizing an energy function with convergence to a local minimum. Experimental\nresults on the PASCAL VOC 2012 and COCO datasets demonstrate that the proposed\nalgorithm performs favorably against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 10:32:03 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Wang", "Xiang", ""], ["Liu", "Sifei", ""], ["Ma", "Huimin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2002.08104", "submitter": "Aleksandra Nowak", "authors": "Romuald A. Janik and Aleksandra Nowak", "title": "Analyzing Neural Networks Based on Random Graphs", "comments": "Added new results and discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a massive evaluation of neural networks with architectures\ncorresponding to random graphs of various types. We investigate various\nstructural and numerical properties of the graphs in relation to neural network\ntest accuracy. We find that none of the classical numerical graph invariants by\nitself allows to single out the best networks. Consequently, we introduce a new\nnumerical graph characteristic that selects a set of quasi-1-dimensional\ngraphs, which are a majority among the best performing networks. We also find\nthat networks with primarily short-range connections perform better than\nnetworks which allow for many long-range connections. Moreover, many resolution\nreducing pathways are beneficial. We provide a dataset of 1020 graphs and the\ntest accuracies of their corresponding neural networks at\nhttps://github.com/rmldj/random-graph-nn-paper\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 11:04:49 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 17:13:59 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 11:29:36 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Janik", "Romuald A.", ""], ["Nowak", "Aleksandra", ""]]}, {"id": "2002.08111", "submitter": "Sam Ringer", "authors": "Will Williams, Sam Ringer, Tom Ash, John Hughes, David MacLeod, Jamie\n  Dougherty", "title": "Hierarchical Quantized Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite progress in training neural networks for lossy image compression,\ncurrent approaches fail to maintain both perceptual quality and abstract\nfeatures at very low bitrates. Encouraged by recent success in learning\ndiscrete representations with Vector Quantized Variational Autoencoders\n(VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors\nof compression. We show that the combination of stochastic quantization and\nhierarchical latent structure aids likelihood-based image compression. This\nleads us to introduce a novel objective for training hierarchical VQ-VAEs. Our\nresulting scheme produces a Markovian series of latent variables that\nreconstruct images of high-perceptual quality which retain semantically\nmeaningful features. We provide qualitative and quantitative evaluations on the\nCelebA and MNIST datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 11:26:34 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 15:39:36 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 11:10:26 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Williams", "Will", ""], ["Ringer", "Sam", ""], ["Ash", "Tom", ""], ["Hughes", "John", ""], ["MacLeod", "David", ""], ["Dougherty", "Jamie", ""]]}, {"id": "2002.08118", "submitter": "Tony Duan", "authors": "Greg Yang, Tony Duan, J. Edward Hu, Hadi Salman, Ilya Razenshteyn,\n  Jerry Li", "title": "Randomized Smoothing of All Shapes and Sizes", "comments": "9 pages main text, 49 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized smoothing is the current state-of-the-art defense with provable\nrobustness against $\\ell_2$ adversarial attacks. Many works have devised new\nrandomized smoothing schemes for other metrics, such as $\\ell_1$ or\n$\\ell_\\infty$; however, substantial effort was needed to derive such new\nguarantees. This begs the question: can we find a general theory for randomized\nsmoothing?\n  We propose a novel framework for devising and analyzing randomized smoothing\nschemes, and validate its effectiveness in practice. Our theoretical\ncontributions are: (1) we show that for an appropriate notion of \"optimal\", the\noptimal smoothing distributions for any \"nice\" norms have level sets given by\nthe norm's *Wulff Crystal*; (2) we propose two novel and complementary methods\nfor deriving provably robust radii for any smoothing distribution; and, (3) we\nshow fundamental limits to current randomized smoothing techniques via the\ntheory of *Banach space cotypes*. By combining (1) and (2), we significantly\nimprove the state-of-the-art certified accuracy in $\\ell_1$ on standard\ndatasets. Meanwhile, we show using (3) that with only label statistics under\nrandom input perturbations, randomized smoothing cannot achieve nontrivial\ncertified accuracy against perturbations of $\\ell_p$-norm $\\Omega(\\min(1,\nd^{\\frac{1}{p} - \\frac{1}{2}}))$, when the input dimension $d$ is large. We\nprovide code in github.com/tonyduan/rs4a.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 11:41:09 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 19:33:48 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 06:59:55 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 05:27:53 GMT"}, {"version": "v5", "created": "Thu, 23 Jul 2020 21:20:51 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Yang", "Greg", ""], ["Duan", "Tony", ""], ["Hu", "J. Edward", ""], ["Salman", "Hadi", ""], ["Razenshteyn", "Ilya", ""], ["Li", "Jerry", ""]]}, {"id": "2002.08127", "submitter": "Xin-Yu Zhang", "authors": "Xin-Yu Zhang, Kai Zhao, Taihong Xiao, Ming-Ming Cheng, and Ming-Hsuan\n  Yang", "title": "Structured Sparsification with Joint Optimization of Group Convolution\n  and Channel Shuffle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in convolutional neural networks(CNNs) usually come with the\nexpense of excessive computational overhead and memory footprint. Network\ncompression aims to alleviate this issue by training compact models with\ncomparable performance. However, existing compression techniques either entail\ndedicated expert design or compromise with a moderate performance drop. In this\npaper, we propose a novel structured sparsification method for efficient\nnetwork compression. The proposed method automatically induces structured\nsparsity on the convolutional weights, thereby facilitating the implementation\nof the compressed model with the highly-optimized group convolution. We further\naddress the problem of inter-group communication with a learnable channel\nshuffle mechanism. The proposed approach can be easily applied to compress many\nnetwork architectures with a negligible performance drop. Extensive\nexperimental results and analysis demonstrate that our approach gives a\ncompetitive performance against the recent network compression counterparts\nwith a sound accuracy-complexity trade-off.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 12:03:10 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 05:50:42 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhang", "Xin-Yu", ""], ["Zhao", "Kai", ""], ["Xiao", "Taihong", ""], ["Cheng", "Ming-Ming", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2002.08158", "submitter": "Yibo Yang", "authors": "Yibo Yang, Robert Bamler and Stephan Mandt", "title": "Variational Bayesian Quantization", "comments": "9 pages + detailed supplement with additional full resolution\n  reconstructed images; ICML 2020 final camera-ready version, title changed to\n  \"Variational Bayesian Quantization\" following reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for quantizing continuous latent representations\nin trained models. Our approach applies to deep probabilistic models, such as\nvariational autoencoders (VAEs), and enables both data and model compression.\nUnlike current end-to-end neural compression methods that cater the model to a\nfixed quantization scheme, our algorithm separates model design and training\nfrom quantization. Consequently, our algorithm enables \"plug-and-play\"\ncompression with variable rate-distortion trade-off, using a single trained\nmodel. Our algorithm can be seen as a novel extension of arithmetic coding to\nthe continuous domain, and uses adaptive quantization accuracy based on\nestimates of posterior uncertainty. Our experimental results demonstrate the\nimportance of taking into account posterior uncertainties, and show that image\ncompression with the proposed algorithm outperforms JPEG over a wide range of\nbit rates using only a single standard VAE. Further experiments on Bayesian\nneural word embeddings demonstrate the versatility of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 00:15:37 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 22:25:12 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Yang", "Yibo", ""], ["Bamler", "Robert", ""], ["Mandt", "Stephan", ""]]}, {"id": "2002.08204", "submitter": "Fabian Timm", "authors": "Lukas Enderich and Fabian Timm and Wolfram Burgard", "title": "SYMOG: learning symmetric mixture of Gaussian modes for improved\n  fixed-point quantization", "comments": "Preprint submitted to Neurocomputing", "journal-ref": "Neurocomputing 2020", "doi": "10.1016/j.neucom.2019.11.114", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been proven to outperform classical methods\non several machine learning benchmarks. However, they have high computational\ncomplexity and require powerful processing units. Especially when deployed on\nembedded systems, model size and inference time must be significantly reduced.\nWe propose SYMOG (symmetric mixture of Gaussian modes), which significantly\ndecreases the complexity of DNNs through low-bit fixed-point quantization.\nSYMOG is a novel soft quantization method such that the learning task and the\nquantization are solved simultaneously. During training the weight distribution\nchanges from an unimodal Gaussian distribution to a symmetric mixture of\nGaussians, where each mean value belongs to a particular fixed-point mode. We\nevaluate our approach with different architectures (LeNet5, VGG7, VGG11,\nDenseNet) on common benchmark data sets (MNIST, CIFAR-10, CIFAR-100) and we\ncompare with state-of-the-art quantization approaches. We achieve excellent\nresults and outperform 2-bit state-of-the-art performance with an error rate of\nonly 5.71% on CIFAR-10 and 27.65% on CIFAR-100.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 14:17:32 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Enderich", "Lukas", ""], ["Timm", "Fabian", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2002.08214", "submitter": "Anusha Bhamidipati", "authors": "B.V.S Anusha, Sayan Banerjee, Subhasis Chaudhuri", "title": "DeFraudNet:End2End Fingerprint Spoof Detection using Patch Level\n  Attention", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, fingerprint recognition systems have made remarkable\nadvancements in the field of biometric security as it plays an important role\nin personal, national and global security. In spite of all these notable\nadvancements, the fingerprint recognition technology is still susceptible to\nspoof attacks which can significantly jeopardize the user security. The cross\nsensor and cross material spoof detection still pose a challenge with a myriad\nof spoof materials emerging every day, compromising sensor interoperability and\nrobustness. This paper proposes a novel method for fingerprint spoof detection\nusing both global and local fingerprint feature descriptors. These descriptors\nare extracted using DenseNet which significantly improves cross-sensor,\ncross-material and cross-dataset performance. A novel patch attention network\nis used for finding the most discriminative patches and also for network\nfusion. We evaluate our method on four publicly available datasets:LivDet 2011,\n2013, 2015 and 2017. A set of comprehensive experiments are carried out to\nevaluate cross-sensor, cross-material and cross-dataset performance over these\ndatasets. The proposed approach achieves an average accuracy of 99.52%, 99.16%\nand 99.72% on LivDet 2017,2015 and 2011 respectively outperforming the current\nstate-of-the-art results by 3% and 4% for LivDet 2015 and 2011 respectively.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 14:41:06 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Anusha", "B. V. S", ""], ["Banerjee", "Sayan", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2002.08219", "submitter": "Yeji Kim", "authors": "Ye-Ji Kim, Dong-Gyu Lee, Seong-Whan Lee", "title": "Three-Stream Fusion Network for First-Person Interaction Recognition", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-person interaction recognition is a challenging task because of\nunstable video conditions resulting from the camera wearer's movement. For\nhuman interaction recognition from a first-person viewpoint, this paper\nproposes a three-stream fusion network with two main parts: three-stream\narchitecture and three-stream correlation fusion. Thre three-stream\narchitecture captures the characteristics of the target appearance, target\nmotion, and camera ego-motion. Meanwhile the three-stream correlation fusion\ncombines the feature map of each of the three streams to consider the\ncorrelations among the target appearance, target motion and camera ego-motion.\nThe fused feature vector is robust to the camera movement and compensates for\nthe noise of the camera ego-motion. Short-term intervals are modeled using the\nfused feature vector, and a long short-term memory(LSTM) model considers the\ntemporal dynamics of the video. We evaluated the proposed method on two-public\nbenchmark datasets to validate the effectiveness of our approach. The\nexperimental results show that the proposed fusion method successfully\ngenerated a discriminative feature vector, and our network outperformed all\ncompeting activity recognition methods in first-person videos where\nconsiderable camera ego-motion occurs.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 14:47:05 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Kim", "Ye-Ji", ""], ["Lee", "Dong-Gyu", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2002.08239", "submitter": "Jorge Beltr\\'an", "authors": "Irene Cortes, Jorge Beltran, Arturo de la Escalera and Fernando Garcia", "title": "siaNMS: Non-Maximum Suppression with Siamese Networks for Multi-Camera\n  3D Object Detection", "comments": "Submitted to IEEE Intelligent Vehicles Symposium 2020 (IV2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of embedded hardware in autonomous vehicles broadens\ntheir computational capabilities, thus bringing the possibility to mount more\ncomplete sensor setups able to handle driving scenarios of higher complexity.\nAs a result, new challenges such as multiple detections of the same object have\nto be addressed. In this work, a siamese network is integrated into the\npipeline of a well-known 3D object detector approach to suppress duplicate\nproposals coming from different cameras via re-identification. Additionally,\nassociations are exploited to enhance the 3D box regression of the object by\naggregating their corresponding LiDAR frustums. The experimental evaluation on\nthe nuScenes dataset shows that the proposed method outperforms traditional NMS\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 15:32:38 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Cortes", "Irene", ""], ["Beltran", "Jorge", ""], ["de la Escalera", "Arturo", ""], ["Garcia", "Fernando", ""]]}, {"id": "2002.08242", "submitter": "Hai Xiao", "authors": "Hai Xiao, Jin Shang and Mengyuan Huang", "title": "AI Online Filters to Real World Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep artificial neural networks, trained with labeled data sets are widely\nused in numerous vision and robotics applications today. In terms of AI, these\nare called reflex models, referring to the fact that they do not self-evolve or\nactively adapt to environmental changes. As demand for intelligent robot\ncontrol expands to many high level tasks, reinforcement learning and state\nbased models play an increasingly important role. Herein, in computer vision\nand robotics domain, we study a novel approach to add reinforcement controls\nonto the image recognition reflex models to attain better overall performance,\nspecifically to a wider environment range beyond what is expected of the task\nreflex models. Follow a common infrastructure with environment sensing and AI\nbased modeling of self-adaptive agents, we implement multiple types of AI\ncontrol agents. To the end, we provide comparative results of these agents with\nbaseline, and an insightful analysis of their benefit to improve overall image\nrecognition performance in real world.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 08:23:14 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Xiao", "Hai", ""], ["Shang", "Jin", ""], ["Huang", "Mengyuan", ""]]}, {"id": "2002.08254", "submitter": "Michael Schmitt", "authors": "Michael Schmitt, Jonathan Prexl, Patrick Ebel, Lukas Liebel, Xiao\n  Xiang Zhu", "title": "Weakly Supervised Semantic Segmentation of Satellite Images for Land\n  Cover Mapping -- Challenges and Opportunities", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully automatic large-scale land cover mapping belongs to the core challenges\naddressed by the remote sensing community. Usually, the basis of this task is\nformed by (supervised) machine learning models. However, in spite of recent\ngrowth in the availability of satellite observations, accurate training data\nremains comparably scarce. On the other hand, numerous global land cover\nproducts exist and can be accessed often free-of-charge. Unfortunately, these\nmaps are typically of a much lower resolution than modern day satellite\nimagery. Besides, they always come with a significant amount of noise, as they\ncannot be considered ground truth, but are products of previous\n(semi-)automatic prediction tasks. Therefore, this paper seeks to make a case\nfor the application of weakly supervised learning strategies to get the most\nout of available data sources and achieve progress in high-resolution\nlarge-scale land cover mapping. Challenges and opportunities are discussed\nbased on the SEN12MS dataset, for which also some baseline results are shown.\nThese baselines indicate that there is still a lot of potential for dedicated\napproaches designed to deal with remote sensing-specific forms of weak\nsupervision.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 16:01:25 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 13:24:16 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Schmitt", "Michael", ""], ["Prexl", "Jonathan", ""], ["Ebel", "Patrick", ""], ["Liebel", "Lukas", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2002.08277", "submitter": "Yixiao Zhang", "authors": "Yixiao Zhang, Xiaosong Wang, Ziyue Xu, Qihang Yu, Alan Yuille, Daguang\n  Xu", "title": "When Radiology Report Generation Meets Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic radiology report generation has been an attracting research problem\ntowards computer-aided diagnosis to alleviate the workload of doctors in recent\nyears. Deep learning techniques for natural image captioning are successfully\nadapted to generating radiology reports. However, radiology image reporting is\ndifferent from the natural image captioning task in two aspects: 1) the\naccuracy of positive disease keyword mentions is critical in radiology image\nreporting in comparison to the equivalent importance of every single word in a\nnatural image caption; 2) the evaluation of reporting quality should focus more\non matching the disease keywords and their associated attributes instead of\ncounting the occurrence of N-gram. Based on these concerns, we propose to\nutilize a pre-constructed graph embedding module (modeled with a graph\nconvolutional neural network) on multiple disease findings to assist the\ngeneration of reports in this work. The incorporation of knowledge graph allows\nfor dedicated feature learning for each disease finding and the relationship\nmodeling between them. In addition, we proposed a new evaluation metric for\nradiology image reporting with the assistance of the same composed graph.\nExperimental results demonstrate the superior performance of the methods\nintegrated with the proposed graph embedding module on a publicly accessible\ndataset (IU-RR) of chest radiographs compared with previous approaches using\nboth the conventional evaluation metrics commonly adopted for image captioning\nand our proposed ones.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 16:39:42 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Zhang", "Yixiao", ""], ["Wang", "Xiaosong", ""], ["Xu", "Ziyue", ""], ["Yu", "Qihang", ""], ["Yuille", "Alan", ""], ["Xu", "Daguang", ""]]}, {"id": "2002.08289", "submitter": "Chitresh Bhushan", "authors": "Chitresh Bhushan, Zhaoyuan Yang, Nurali Virani, Naresh Iyer", "title": "Variational Encoder-based Reliable Classification", "comments": "Published in ICIP 2020. Typos fixed in revision", "journal-ref": "IEEE International Conference on Image Processing (2020) 1941-1945", "doi": "10.1109/ICIP40778.2020.9190836", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models provide statistically impressive results which might\nbe individually unreliable. To provide reliability, we propose an Epistemic\nClassifier (EC) that can provide justification of its belief using support from\nthe training dataset as well as quality of reconstruction. Our approach is\nbased on modified variational auto-encoders that can identify a semantically\nmeaningful low-dimensional space where perceptually similar instances are close\nin $\\ell_2$-distance too. Our results demonstrate improved reliability of\npredictions and robust identification of samples with adversarial attacks as\ncompared to baseline of softmax-based thresholding.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 17:05:32 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 13:51:37 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bhushan", "Chitresh", ""], ["Yang", "Zhaoyuan", ""], ["Virani", "Nurali", ""], ["Iyer", "Naresh", ""]]}, {"id": "2002.08325", "submitter": "Tejas Gokhale", "authors": "Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou Yang", "title": "VQA-LOL: Visual Question Answering under the Lens of Logic", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logical connectives and their implications on the meaning of a natural\nlanguage sentence are a fundamental aspect of understanding. In this paper, we\ninvestigate whether visual question answering (VQA) systems trained to answer a\nquestion about an image, are able to answer the logical composition of multiple\nsuch questions. When put under this \\textit{Lens of Logic}, state-of-the-art\nVQA models have difficulty in correctly answering these logically composed\nquestions. We construct an augmentation of the VQA dataset as a benchmark, with\nquestions containing logical compositions and linguistic transformations\n(negation, disjunction, conjunction, and antonyms). We propose our {Lens of\nLogic (LOL)} model which uses question-attention and logic-attention to\nunderstand logical connectives in the question, and a novel\nFr\\'echet-Compatibility Loss, which ensures that the answers of the component\nquestions and the composed question are consistent with the inferred logical\noperation. Our model shows substantial improvement in learning logical\ncompositions while retaining performance on VQA. We suggest this work as a move\ntowards robustness by embedding logical connectives in visual understanding.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 17:57:46 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 22:39:12 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Gokhale", "Tejas", ""], ["Banerjee", "Pratyay", ""], ["Baral", "Chitta", ""], ["Yang", "Yezhou", ""]]}, {"id": "2002.08327", "submitter": "Shawn Shan", "authors": "Shawn Shan, Emily Wenger, Jiayun Zhang, Huiying Li, Haitao Zheng, Ben\n  Y. Zhao", "title": "Fawkes: Protecting Privacy against Unauthorized Deep Learning Models", "comments": null, "journal-ref": "USENIX Security Symposium 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's proliferation of powerful facial recognition systems poses a real\nthreat to personal privacy. As Clearview.ai demonstrated, anyone can canvas the\nInternet for data and train highly accurate facial recognition models of\nindividuals without their knowledge. We need tools to protect ourselves from\npotential misuses of unauthorized facial recognition systems. Unfortunately, no\npractical or effective solutions exist.\n  In this paper, we propose Fawkes, a system that helps individuals inoculate\ntheir images against unauthorized facial recognition models. Fawkes achieves\nthis by helping users add imperceptible pixel-level changes (we call them\n\"cloaks\") to their own photos before releasing them. When used to train facial\nrecognition models, these \"cloaked\" images produce functional models that\nconsistently cause normal images of the user to be misidentified. We\nexperimentally demonstrate that Fawkes provides 95+% protection against user\nrecognition regardless of how trackers train their models. Even when clean,\nuncloaked images are \"leaked\" to the tracker and used for training, Fawkes can\nstill maintain an 80+% protection success rate. We achieve 100% success in\nexperiments against today's state-of-the-art facial recognition services.\nFinally, we show that Fawkes is robust against a variety of countermeasures\nthat try to detect or disrupt image cloaks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 18:00:22 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 03:54:20 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Shan", "Shawn", ""], ["Wenger", "Emily", ""], ["Zhang", "Jiayun", ""], ["Li", "Huiying", ""], ["Zheng", "Haitao", ""], ["Zhao", "Ben Y.", ""]]}, {"id": "2002.08331", "submitter": "Luiz Antonio Buschetto Macarini", "authors": "Luiz Antonio Buschetto Macarini, Aldo von Wangenheim, Felipe Perozzo\n  Dalto\\'e, Alexandre Sherlley Casimiro Onofre, Fabiana Botelho de Miranda\n  Onofre, Marcelo Ricardo Stemmer", "title": "Towards a Complete Pipeline for Segmenting Nuclei in Feulgen-Stained\n  Images", "comments": "7 pages, 8 figures (Figure 2 with \"a\" and \"b\"), conference paper\n  accepted for presentation in XI Computer on the Beach\n  (https://www.computeronthebeach.com.br/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cervical cancer is the second most common cancer type in women around the\nworld. In some countries, due to non-existent or inadequate screening, it is\noften detected at late stages, making standard treatment options often absent\nor unaffordable. It is a deadly disease that could benefit from early detection\napproaches. It is usually done by cytological exams which consist of visually\ninspecting the nuclei searching for morphological alteration. Since it is done\nby humans, naturally, some subjectivity is introduced. Computational methods\ncould be used to reduce this, where the first stage of the process would be the\nnuclei segmentation. In this context, we present a complete pipeline for the\nsegmentation of nuclei in Feulgen-stained images using Convolutional Neural\nNetworks. Here we show the entire process of segmentation, since the collection\nof the samples, passing through pre-processing, training the network,\npost-processing and results evaluation. We achieved an overall IoU of 0.78,\nshowing the affordability of the approach of nuclei segmentation on\nFeulgen-stained images. The code is available in:\nhttps://github.com/luizbuschetto/feulgen_nuclei_segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 18:14:57 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Macarini", "Luiz Antonio Buschetto", ""], ["von Wangenheim", "Aldo", ""], ["Dalto\u00e9", "Felipe Perozzo", ""], ["Onofre", "Alexandre Sherlley Casimiro", ""], ["Onofre", "Fabiana Botelho de Miranda", ""], ["Stemmer", "Marcelo Ricardo", ""]]}, {"id": "2002.08348", "submitter": "Ziyuan Liu", "authors": "Ziyuan Liu, Georg von Wichert", "title": "Extracting Semantic Indoor Maps from Occupancy Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary challenge for any autonomous system operating in realistic,\nrather unconstrained scenarios is to manage the complexity and uncertainty of\nthe real world. While it is unclear how exactly humans and other higher animals\nmaster these problems, it seems evident, that abstraction plays an important\nrole. The use of abstract concepts allows to define the system behavior on\nhigher levels. In this paper we focus on the semantic mapping of indoor\nenvironments. We propose a method to extract an abstracted floor plan from\ntypical grid maps using Bayesian reasoning. The result of this procedure is a\nprobabilistic generative model of the environment defined over abstract\nconcepts. It is well suited for higher-level reasoning and communication\npurposes. We demonstrate the effectiveness of the approach using real-world\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 18:52:27 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Liu", "Ziyuan", ""], ["von Wichert", "Georg", ""]]}, {"id": "2002.08357", "submitter": "Qijing Huang", "authors": "Qijing Huang, Dequan Wang, Yizhao Gao, Yaohui Cai, Zhen Dong, Bichen\n  Wu, Kurt Keutzer, John Wawrzynek", "title": "Algorithm-hardware Co-design for Deformable Convolution", "comments": null, "journal-ref": "NeurIPS EMC2 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGAs provide a flexible and efficient platform to accelerate\nrapidly-changing algorithms for computer vision. The majority of existing work\nfocuses on accelerating image classification, while other fundamental vision\nproblems, including object detection and instance segmentation, have not been\nadequately addressed. Compared with image classification, detection problems\nare more sensitive to the spatial variance of objects, and therefore, require\nspecialized convolutions to aggregate spatial information. To address this,\nrecent work proposes dynamic deformable convolution to augment regular\nconvolutions. Regular convolutions process a fixed grid of pixels across all\nthe spatial locations in an image, while dynamic deformable convolutions may\naccess arbitrary pixels in the image and the access pattern is input-dependent\nand varies per spatial location. These properties lead to inefficient memory\naccesses of inputs with existing hardware. In this work, we first investigate\nthe overhead of the deformable convolution on embedded FPGA SoCs, and then show\nthe accuracy-latency tradeoffs for a set of algorithm modifications including\nfull versus depthwise, fixed-shape, and limited-range. These modifications\nbenefit the energy efficiency for embedded devices in general as they reduce\nthe compute complexity. We then build an efficient object detection network\nwith modified deformable convolutions and quantize the network using\nstate-of-the-art quantization methods. We implement a unified hardware engine\non FPGA to support all the operations in the network. Preliminary experiments\nshow that little accuracy is compromised and speedup can be achieved with our\nco-design optimization for the deformable convolution.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 01:08:11 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Huang", "Qijing", ""], ["Wang", "Dequan", ""], ["Gao", "Yizhao", ""], ["Cai", "Yaohui", ""], ["Dong", "Zhen", ""], ["Wu", "Bichen", ""], ["Keutzer", "Kurt", ""], ["Wawrzynek", "John", ""]]}, {"id": "2002.08394", "submitter": "Krishna Murthy Jatavallabhula", "authors": "Kaustubh Mani, Swapnil Daga, Shubhika Garg, N. Sai Shankar, Krishna\n  Murthy Jatavallabhula, K. Madhava Krishna", "title": "MonoLayout: Amodal scene layout from a single image", "comments": "To be presented at WACV 2020 Video:\n  https://www.youtube.com/watch?v=HcroGyo6yRQ Project page:\n  https://hbutsuak95.github.io/monolayout", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the novel, highly challenging problem of estimating\nthe layout of a complex urban driving scenario. Given a single color image\ncaptured from a driving platform, we aim to predict the bird's-eye view layout\nof the road and other traffic participants. The estimated layout should reason\nbeyond what is visible in the image, and compensate for the loss of 3D\ninformation due to projection. We dub this problem amodal scene layout\nestimation, which involves \"hallucinating\" scene layout for even parts of the\nworld that are occluded in the image. To this end, we present MonoLayout, a\ndeep neural network for real-time amodal scene layout estimation from a single\nimage. We represent scene layout as a multi-channel semantic occupancy grid,\nand leverage adversarial feature learning to hallucinate plausible completions\nfor occluded image parts. Due to the lack of fair baseline methods, we extend\nseveral state-of-the-art approaches for road-layout estimation and vehicle\noccupancy estimation in bird's-eye view to the amodal setup for rigorous\nevaluation. By leveraging temporal sensor fusion to generate training labels,\nwe significantly outperform current art over a number of datasets. On the KITTI\nand Argoverse datasets, we outperform all baselines by a significant margin. We\nalso make all our annotations, and code publicly available. A video abstract of\nthis paper is available https://www.youtube.com/watch?v=HcroGyo6yRQ .\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 19:16:34 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Mani", "Kaustubh", ""], ["Daga", "Swapnil", ""], ["Garg", "Shubhika", ""], ["Shankar", "N. Sai", ""], ["Jatavallabhula", "Krishna Murthy", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2002.08397", "submitter": "Abhijeet Shenoi", "authors": "Abhijeet Shenoi, Mihir Patel, JunYoung Gwak, Patrick Goebel, Amir\n  Sadeghian, Hamid Rezatofighi, Roberto Mart\\'in-Mart\\'in, Silvio Savarese", "title": "JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset", "comments": "8 pages, 5 figures, 2 tables; Accepted at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Robots navigating autonomously need to perceive and track the motion of\nobjects and other agents in its surroundings. This information enables planning\nand executing robust and safe trajectories. To facilitate these processes, the\nmotion should be perceived in 3D Cartesian space. However, most recent\nmulti-object tracking (MOT) research has focused on tracking people and moving\nobjects in 2D RGB video sequences. In this work we present JRMOT, a novel 3D\nMOT system that integrates information from RGB images and 3D point clouds to\nachieve real-time, state-of-the-art tracking performance. Our system is built\nwith recent neural networks for re-identification, 2D and 3D detection and\ntrack description, combined into a joint probabilistic data-association\nframework within a multi-modal recursive Kalman architecture. As part of our\nwork, we release the JRDB dataset, a novel large scale 2D+3D dataset and\nbenchmark, annotated with over 2 million boxes and 3500 time consistent 2D+3D\ntrajectories across 54 indoor and outdoor scenes. JRDB contains over 60 minutes\nof data including 360 degree cylindrical RGB video and 3D pointclouds in social\nsettings that we use to develop, train and evaluate JRMOT. The presented 3D MOT\nsystem demonstrates state-of-the-art performance against competing methods on\nthe popular 2D tracking KITTI benchmark and serves as first 3D tracking\nsolution for our benchmark. Real-robot tests on our social robot JackRabbot\nindicate that the system is capable of tracking multiple pedestrians fast and\nreliably. We provide the ROS code of our tracker at\nhttps://sites.google.com/view/jrmot.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 19:21:33 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 07:24:15 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 05:23:40 GMT"}, {"version": "v4", "created": "Wed, 22 Jul 2020 07:29:39 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Shenoi", "Abhijeet", ""], ["Patel", "Mihir", ""], ["Gwak", "JunYoung", ""], ["Goebel", "Patrick", ""], ["Sadeghian", "Amir", ""], ["Rezatofighi", "Hamid", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Savarese", "Silvio", ""]]}, {"id": "2002.08402", "submitter": "Ziyuan Liu", "authors": "Ziyuan Liu, Georg von Wichert", "title": "A Generalizable Knowledge Framework for Semantic Indoor Mapping Based on\n  Markov Logic Networks and Data Driven MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generalizable knowledge framework for data\nabstraction, i.e. finding compact abstract model for input data using\npredefined abstract terms. Based on these abstract terms, intelligent\nautonomous systems, such as a robot, should be able to make inference according\nto specific knowledge base, so that they can better handle the complexity and\nuncertainty of the real world. We propose to realize this framework by\ncombining Markov logic networks (MLNs) and data driven MCMC sampling, because\nthe former are a powerful tool for modelling uncertain knowledge and the latter\nprovides an efficient way to draw samples from unknown complex distributions.\nFurthermore, we show in detail how to adapt this framework to a certain task,\nin particular, semantic robot mapping. Based on MLNs, we formulate\ntask-specific context knowledge as descriptive soft rules. Experiments on real\nworld data and simulated data confirm the usefulness of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 19:30:10 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Liu", "Ziyuan", ""], ["von Wichert", "Georg", ""]]}, {"id": "2002.08406", "submitter": "Weinan Song", "authors": "Weinan Song, Yuan Liang, Jiawei Yang, Kun Wang, Lei He", "title": "T-Net: Learning Feature Representation with Task-specific Supervision\n  for Biomedical Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The encoder-decoder network is widely used to learn deep feature\nrepresentations from pixel-wise annotations in biomedical image analysis. Under\nthis structure, the performance profoundly relies on the effectiveness of\nfeature extraction achieved by the encoding network. However, few models have\nconsidered adapting the attention of the feature extractor even in different\nkinds of tasks. In this paper, we propose a novel training strategy by adapting\nthe attention of the feature extractor according to different tasks for\neffective representation learning. Specifically, the framework, named T-Net,\nconsists of an encoding network supervised by task-specific attention maps and\na posterior network that takes in the learned features to predict the\ncorresponding results. The attention map is obtained by the transformation from\npixel-wise annotations according to the specific task, which is used as the\nsupervision to regularize the feature extractor to focus on different locations\nof the recognition object. To show the effectiveness of our method, we evaluate\nT-Net on two different tasks, i.e. , segmentation and localization. Extensive\nresults on three public datasets (BraTS-17, MoNuSeg and IDRiD) have indicated\nthe effectiveness and efficiency of our proposed supervision method, especially\nover the conventional encoding-decoding network.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 19:38:28 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 08:28:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Song", "Weinan", ""], ["Liang", "Yuan", ""], ["Yang", "Jiawei", ""], ["Wang", "Kun", ""], ["He", "Lei", ""]]}, {"id": "2002.08417", "submitter": "Ziyuan Liu", "authors": "Ziyuan Liu, Dong Chen, Kai M. Wurm, Georg von Wichert", "title": "Table-Top Scene Analysis Using Knowledge-Supervised MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a probabilistic method to generate abstract scene\ngraphs for table-top scenes from 6D object pose estimates. We explicitly make\nuse of task-specfic context knowledge by encoding this knowledge as descriptive\nrules in Markov logic networks. Our approach to generate scene graphs is\nprobabilistic: Uncertainty in the object poses is addressed by a probabilistic\nsensor model that is embedded in a data driven MCMC process. We apply Markov\nlogic inference to reason about hidden objects and to detect false estimates of\nobject poses. The effectiveness of our approach is demonstrated and evaluated\nin real world experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 20:10:38 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Liu", "Ziyuan", ""], ["Chen", "Dong", ""], ["Wurm", "Kai M.", ""], ["von Wichert", "Georg", ""]]}, {"id": "2002.08418", "submitter": "Cong Wang", "authors": "Cong Wang, Witold Pedrycz, ZhiWu Li, MengChu Zhou, Jun Zhao", "title": "Residual-Sparse Fuzzy $C$-Means Clustering Incorporating Morphological\n  Reconstruction and Wavelet frames", "comments": "12 pages, 11 figure", "journal-ref": "IEEE Transactions on Fuzzy Systems, 2020", "doi": "10.1109/TFUZZ.2020.3029296", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instead of directly utilizing an observed image including some outliers,\nnoise or intensity inhomogeneity, the use of its ideal value (e.g. noise-free\nimage) has a favorable impact on clustering. Hence, the accurate estimation of\nthe residual (e.g. unknown noise) between the observed image and its ideal\nvalue is an important task. To do so, we propose an $\\ell_0$\nregularization-based Fuzzy $C$-Means (FCM) algorithm incorporating a\nmorphological reconstruction operation and a tight wavelet frame transform. To\nachieve a sound trade-off between detail preservation and noise suppression,\nmorphological reconstruction is used to filter an observed image. By combining\nthe observed and filtered images, a weighted sum image is generated. Since a\ntight wavelet frame system has sparse representations of an image, it is\nemployed to decompose the weighted sum image, thus forming its corresponding\nfeature set. Taking it as data for clustering, we present an improved FCM\nalgorithm by imposing an $\\ell_0$ regularization term on the residual between\nthe feature set and its ideal value, which implies that the favorable\nestimation of the residual is obtained and the ideal value participates in\nclustering. Spatial information is also introduced into clustering since it is\nnaturally encountered in image segmentation. Furthermore, it makes the\nestimation of the residual more reliable. To further enhance the segmentation\neffects of the improved FCM algorithm, we also employ the morphological\nreconstruction to smoothen the labels generated by clustering. Finally, based\non the prototypes and smoothed labels, the segmented image is reconstructed by\nusing a tight wavelet frame reconstruction operation. Experimental results\nreported for synthetic, medical, and color images show that the proposed\nalgorithm is effective and efficient, and outperforms other algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 10:00:03 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wang", "Cong", ""], ["Pedrycz", "Witold", ""], ["Li", "ZhiWu", ""], ["Zhou", "MengChu", ""], ["Zhao", "Jun", ""]]}, {"id": "2002.08434", "submitter": "Vikram Shree", "authors": "Vikram Shree, Wei-Lun Chao and Mark Campbell", "title": "Interactive Natural Language-based Person Search", "comments": "8 pages, 12 figures, Published in IEEE Robotics and Automation\n  Letters (RA-L), \"Dataset at:\n  https://github.com/vikshree/QA_PersonSearchLanguageData\" , Video attachment\n  at: https://www.youtube.com/watch?v=Yyxu8uVUREE&feature=youtu.be", "journal-ref": "in IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.\n  1851-1858, April 2020", "doi": "10.1109/LRA.2020.2969921", "report-no": null, "categories": "cs.RO cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the problem of searching people in an unconstrained\nenvironment, with natural language descriptions. Specifically, we study how to\nsystematically design an algorithm to effectively acquire descriptions from\nhumans. An algorithm is proposed by adapting models, used for visual and\nlanguage understanding, to search a person of interest (POI) in a principled\nway, achieving promising results without the need to re-design another\ncomplicated model. We then investigate an iterative question-answering (QA)\nstrategy that enable robots to request additional information about the POI's\nappearance from the user. To this end, we introduce a greedy algorithm to rank\nquestions in terms of their significance, and equip the algorithm with the\ncapability to dynamically adjust the length of human-robot interaction\naccording to model's uncertainty. Our approach is validated not only on\nbenchmark datasets but on a mobile robot, moving in a dynamic and crowded\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 20:42:19 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Shree", "Vikram", ""], ["Chao", "Wei-Lun", ""], ["Campbell", "Mark", ""]]}, {"id": "2002.08438", "submitter": "Mina Amiri", "authors": "Mina Amiri, Rupert Brooks, Hassan Rivaz", "title": "Fine tuning U-Net for ultrasound image segmentation: which layers?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning a network which has been trained on a large dataset is an\nalternative to full training in order to overcome the problem of scarce and\nexpensive data in medical applications. While the shallow layers of the network\nare usually kept unchanged, deeper layers are modified according to the new\ndataset. This approach may not work for ultrasound images due to their\ndrastically different appearance. In this study, we investigated the effect of\nfine-tuning different layers of a U-Net which was trained on segmentation of\nnatural images in breast ultrasound image segmentation. Tuning the contracting\npart and fixing the expanding part resulted in substantially better results\ncompared to fixing the contracting part and tuning the expanding part.\nFurthermore, we showed that starting to fine-tune the U-Net from the shallow\nlayers and gradually including more layers will lead to a better performance\ncompared to fine-tuning the network from the deep layers moving back to shallow\nlayers. We did not observe the same results on segmentation of X-ray images,\nwhich have different salient features compared to ultrasound, it may therefore\nbe more appropriate to fine-tune the shallow layers rather than deep layers.\nShallow layers learn lower level features (including speckle pattern, and\nprobably the noise and artifact properties) which are critical in automatic\nsegmentation in this modality.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 20:45:40 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Amiri", "Mina", ""], ["Brooks", "Rupert", ""], ["Rivaz", "Hassan", ""]]}, {"id": "2002.08439", "submitter": "Siyue Wang", "authors": "Xiao Wang, Siyue Wang, Pin-Yu Chen, Xue Lin, Peter Chin", "title": "AdvMS: A Multi-source Multi-cost Defense Against Adversarial Attacks", "comments": "Accepted by 45th International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing effective defense against adversarial attacks is a crucial topic as\ndeep neural networks have been proliferated rapidly in many security-critical\ndomains such as malware detection and self-driving cars. Conventional defense\nmethods, although shown to be promising, are largely limited by their\nsingle-source single-cost nature: The robustness promotion tends to plateau\nwhen the defenses are made increasingly stronger while the cost tends to\namplify. In this paper, we study principles of designing multi-source and\nmulti-cost schemes where defense performance is boosted from multiple defending\ncomponents. Based on this motivation, we propose a multi-source and multi-cost\ndefense scheme, Adversarially Trained Model Switching (AdvMS), that inherits\nadvantages from two leading schemes: adversarial training and random model\nswitching. We show that the multi-source nature of AdvMS mitigates the\nperformance plateauing issue and the multi-cost nature enables improving\nrobustness at a flexible and adjustable combination of costs over different\nfactors which can better suit specific restrictions and needs in practice.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 20:46:54 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Wang", "Xiao", ""], ["Wang", "Siyue", ""], ["Chen", "Pin-Yu", ""], ["Lin", "Xue", ""], ["Chin", "Peter", ""]]}, {"id": "2002.08440", "submitter": "Ehsan Emad Marvasti", "authors": "Ehsan Emad Marvasti, Arash Raftari, Amir Emad Marvasti, Yaser P.\n  Fallah, Rui Guo, HongSheng Lu", "title": "Cooperative LIDAR Object Detection via Feature Sharing in Deep Networks", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent advancements in communication and computational systems has led to\nsignificant improvement of situational awareness in connected and autonomous\nvehicles. Computationally efficient neural networks and high speed wireless\nvehicular networks have been some of the main contributors to this improvement.\nHowever, scalability and reliability issues caused by inherent limitations of\nsensory and communication systems are still challenging problems. In this\npaper, we aim to mitigate the effects of these limitations by introducing the\nconcept of feature sharing for cooperative object detection (FS-COD). In our\nproposed approach, a better understanding of the environment is achieved by\nsharing partially processed data between cooperative vehicles while maintaining\na balance between computation and communication load. This approach is\ndifferent from current methods of map sharing, or sharing of raw data which are\nnot scalable. The performance of the proposed approach is verified through\nexperiments on Volony dataset. It is shown that the proposed approach has\nsignificant performance superiority over the conventional single-vehicle object\ndetection approaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 20:47:09 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Marvasti", "Ehsan Emad", ""], ["Raftari", "Arash", ""], ["Marvasti", "Amir Emad", ""], ["Fallah", "Yaser P.", ""], ["Guo", "Rui", ""], ["Lu", "HongSheng", ""]]}, {"id": "2002.08448", "submitter": "Samik Banerjee", "authors": "Samik Banerjee, Sukhendu Das", "title": "SD-GAN: Structural and Denoising GAN reveals facial parts under\n  occlusion", "comments": "Recommended for revision in Neurocomputing, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain facial parts are salient (unique) in appearance, which substantially\ncontribute to the holistic recognition of a subject. Occlusion of these salient\nparts deteriorates the performance of face recognition algorithms. In this\npaper, we propose a generative model to reconstruct the missing parts of the\nface which are under occlusion. The proposed generative model (SD-GAN)\nreconstructs a face preserving the illumination variation and identity of the\nface. A novel adversarial training algorithm has been designed for a bimodal\nmutually exclusive Generative Adversarial Network (GAN) model, for faster\nconvergence. A novel adversarial \"structural\" loss function is also proposed,\ncomprising of two components: a holistic and a local loss, characterized by\nSSIM and patch-wise MSE. Ablation studies on real and synthetically occluded\nface datasets reveal that our proposed technique outperforms the competing\nmethods by a considerable margin, even for boosting the performance of Face\nRecognition.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 21:12:49 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Banerjee", "Samik", ""], ["Das", "Sukhendu", ""]]}, {"id": "2002.08473", "submitter": "Karsten Roth", "authors": "Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bj\\\"orn\n  Ommer, Joseph Paul Cohen", "title": "Revisiting Training Strategies and Generalization Performance in Deep\n  Metric Learning", "comments": "ICML 2020. Main paper 8.25 pages, 26 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Metric Learning (DML) is arguably one of the most influential lines of\nresearch for learning visual similarities with many proposed approaches every\nyear. Although the field benefits from the rapid progress, the divergence in\ntraining protocols, architectures, and parameter choices make an unbiased\ncomparison difficult. To provide a consistent reference point, we revisit the\nmost widely used DML objective functions and conduct a study of the crucial\nparameter choices as well as the commonly neglected mini-batch sampling\nprocess. Under consistent comparison, DML objectives show much higher\nsaturation than indicated by literature. Further based on our analysis, we\nuncover a correlation between the embedding space density and compression to\nthe generalization performance of DML models. Exploiting these insights, we\npropose a simple, yet effective, training regularization to reliably boost the\nperformance of ranking-based DML models on various standard benchmark datasets.\nCode and a publicly accessible WandB-repo are available at\nhttps://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 22:16:12 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 15:57:55 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 00:54:49 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2020 20:09:58 GMT"}, {"version": "v5", "created": "Sat, 11 Apr 2020 15:13:06 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2020 16:51:25 GMT"}, {"version": "v7", "created": "Sat, 9 May 2020 17:59:18 GMT"}, {"version": "v8", "created": "Thu, 18 Jun 2020 13:18:06 GMT"}, {"version": "v9", "created": "Sat, 1 Aug 2020 16:14:33 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Roth", "Karsten", ""], ["Milbich", "Timo", ""], ["Sinha", "Samarth", ""], ["Gupta", "Prateek", ""], ["Ommer", "Bj\u00f6rn", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "2002.08490", "submitter": "Micha{\\l} Ku\\'zba", "authors": "Piotr Wo\\'znicki, Micha{\\l} Ku\\'zba, Piotr Migda{\\l}", "title": "Modelling response to trypophobia trigger using intermediate layers of\n  ImageNet networks", "comments": "extended abstract submitted to Eastern European Machine Learning\n  2019, 3 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we approach the problem of detecting trypophobia triggers\nusing Convolutional neural networks. We show that standard architectures such\nas VGG or ResNet are capable of recognizing trypophobia patterns. We also\nconduct experiments to analyze the nature of this phenomenon. To do that, we\ndissect the network decreasing the number of its layers and parameters. We\nprove, that even significantly reduced networks have accuracy above 91% and\nfocus their attention on the trypophobia patterns as presented on the visual\nexplanations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 22:58:58 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 01:05:23 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Wo\u017anicki", "Piotr", ""], ["Ku\u017aba", "Micha\u0142", ""], ["Migda\u0142", "Piotr", ""]]}, {"id": "2002.08510", "submitter": "Tianlang Chen", "authors": "Tianlang Chen, Jiebo Luo", "title": "Expressing Objects just like Words: Recurrent Visual Embedding for\n  Image-Text Matching", "comments": "Accepted by AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image-text matching approaches typically infer the similarity of an\nimage-text pair by capturing and aggregating the affinities between the text\nand each independent object of the image. However, they ignore the connections\nbetween the objects that are semantically related. These objects may\ncollectively determine whether the image corresponds to a text or not. To\naddress this problem, we propose a Dual Path Recurrent Neural Network (DP-RNN)\nwhich processes images and sentences symmetrically by recurrent neural networks\n(RNN). In particular, given an input image-text pair, our model reorders the\nimage objects based on the positions of their most related words in the text.\nIn the same way as extracting the hidden features from word embeddings, the\nmodel leverages RNN to extract high-level object features from the reordered\nobject inputs. We validate that the high-level object features contain useful\njoint information of semantically related objects, which benefit the retrieval\ntask. To compute the image-text similarity, we incorporate a Multi-attention\nCross Matching Model into DP-RNN. It aggregates the affinity between objects\nand words with cross-modality guided attention and self-attention. Our model\nachieves the state-of-the-art performance on Flickr30K dataset and competitive\nperformance on MS-COCO dataset. Extensive experiments demonstrate the\neffectiveness of our model.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 00:51:01 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Chen", "Tianlang", ""], ["Luo", "Jiebo", ""]]}, {"id": "2002.08546", "submitter": "Jian Liang", "authors": "Jian Liang, Dapeng Hu, and Jiashi Feng", "title": "Do We Really Need to Access the Source Data? Source Hypothesis Transfer\n  for Unsupervised Domain Adaptation", "comments": "ICML2020. Fix the typos for Digits. Code is available at\n  https://github.com/tim-learn/SHOT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned\nfrom a labeled source dataset to solve similar tasks in a new unlabeled domain.\nPrior UDA methods typically require to access the source data when learning to\nadapt the model, making them risky and inefficient for decentralized private\ndata. This work tackles a practical setting where only a trained source model\nis available and investigates how we can effectively utilize such a model\nwithout source data to solve UDA problems. We propose a simple yet generic\nrepresentation learning framework, named \\emph{Source HypOthesis Transfer}\n(SHOT). SHOT freezes the classifier module (hypothesis) of the source model and\nlearns the target-specific feature extraction module by exploiting both\ninformation maximization and self-supervised pseudo-labeling to implicitly\nalign representations from the target domains to the source hypothesis. To\nverify its versatility, we evaluate SHOT in a variety of adaptation cases\nincluding closed-set, partial-set, and open-set domain adaptation. Experiments\nindicate that SHOT yields state-of-the-art results among multiple domain\nadaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 03:13:58 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 17:31:59 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 17:03:14 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 09:48:24 GMT"}, {"version": "v5", "created": "Fri, 23 Oct 2020 03:22:32 GMT"}, {"version": "v6", "created": "Tue, 1 Jun 2021 09:06:00 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Liang", "Jian", ""], ["Hu", "Dapeng", ""], ["Feng", "Jiashi", ""]]}, {"id": "2002.08547", "submitter": "Han Hu", "authors": "Qing Zhu, Lin Chen, Han Hu, Binzhi Xu, Yeting Zhang, Haifeng Li", "title": "Deep Fusion of Local and Non-Local Features for Precision Landslide\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision mapping of landslide inventory is crucial for hazard mitigation.\nMost landslides generally co-exist with other confusing geological features,\nand the presence of such areas can only be inferred unambiguously at a large\nscale. In addition, local information is also important for the preservation of\nobject boundaries. Aiming to solve this problem, this paper proposes an\neffective approach to fuse both local and non-local features to surmount the\ncontextual problem. Built upon the U-Net architecture that is widely adopted in\nthe remote sensing community, we utilize two additional modules. The first one\nuses dilated convolution and the corresponding atrous spatial pyramid pooling,\nwhich enlarged the receptive field without sacrificing spatial resolution or\nincreasing memory usage. The second uses a scale attention mechanism to guide\nthe up-sampling of features from the coarse level by a learned weight map. In\nimplementation, the computational overhead against the original U-Net was only\na few convolutional layers. Experimental evaluations revealed that the proposed\nmethod outperformed state-of-the-art general-purpose semantic segmentation\napproaches. Furthermore, ablation studies have shown that the two models\nafforded extensive enhancements in landslide-recognition performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 03:18:59 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Zhu", "Qing", ""], ["Chen", "Lin", ""], ["Hu", "Han", ""], ["Xu", "Binzhi", ""], ["Zhang", "Yeting", ""], ["Li", "Haifeng", ""]]}, {"id": "2002.08549", "submitter": "Han Hu", "authors": "Han Hu, Libin Wang, Mier Zhang, Yulin Ding, Qing Zhu", "title": "Fast and Regularized Reconstruction of Building Fa\\c{c}ades from\n  Street-View Images using Binary Integer Programming", "comments": null, "journal-ref": null, "doi": "10.5194/isprs-annals-V-2-2020-365-2020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized arrangement of primitives on building fa\\c{c}ades to aligned\nlocations and consistent sizes is important towards structured reconstruction\nof urban environment. Mixed integer linear programing was used to solve the\nproblem, however, it is extreamly time consuming even for state-of-the-art\ncommercial solvers. Aiming to alleviate this issue, we cast the problem into\nbinary integer programming, which omits the requirements for real value\nparameters and is more efficient to be solved . Firstly, the bounding boxes of\nthe primitives are detected using the YOLOv3 architecture in real-time.\nSecondly, the coordinates of the upper left corners and the sizes of the\nbounding boxes are automatically clustered in a binary integer programming\noptimization, which jointly considers the geometric fitness, regularity and\nadditional constraints; this step does not require \\emph{a priori} knowledge,\nsuch as the number of clusters or pre-defined grammars. Finally, the\nregularized bounding boxes can be directly used to guide the fa\\c{c}ade\nreconstruction in an interactive envinronment. Experimental evaluations have\nrevealed that the accuracies for the extraction of primitives are above 0.82,\nwhich is sufficient for the following 3D reconstruction. The proposed approach\nonly takes about $ 10\\% $ to $ 20\\% $ of the runtime than previous approach and\nreduces the diversity of the bounding boxes to about $20\\%$ to $50\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 03:29:52 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 10:52:47 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 10:21:39 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Hu", "Han", ""], ["Wang", "Libin", ""], ["Zhang", "Mier", ""], ["Ding", "Yulin", ""], ["Zhu", "Qing", ""]]}, {"id": "2002.08555", "submitter": "Zhang Liao", "authors": "Liao Zhang, Yan Yan, Lin Cheng, and Hanzi Wang", "title": "Learning Object Scale With Click Supervision for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised object detection has recently attracted increasing\nattention since it only requires image-levelannotations. However, the\nperformance obtained by existingmethods is still far from being satisfactory\ncompared with fully-supervised object detection methods. To achieve a good\ntrade-off between annotation cost and object detection performance,we propose a\nsimple yet effective method which incorporatesCNN visualization with click\nsupervision to generate the pseudoground-truths (i.e., bounding boxes). These\npseudo ground-truthscan be used to train a fully-supervised detector. To\nestimatethe object scale, we firstly adopt a proposal selection algorithmto\npreserve high-quality proposals, and then generate ClassActivation Maps (CAMs)\nfor these preserved proposals by theproposed CNN visualization algorithm called\nSpatial AttentionCAM. Finally, we fuse these CAMs together to generate\npseudoground-truths and train a fully-supervised object detector withthese\nground-truths. Experimental results on the PASCAL VOC2007 and VOC 2012 datasets\nshow that the proposed methodcan obtain much higher accuracy for estimating the\nobject scale,compared with the state-of-the-art image-level based methodsand\nthe center-click based method\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 03:59:46 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Zhang", "Liao", ""], ["Yan", "Yan", ""], ["Cheng", "Lin", ""], ["Wang", "Hanzi", ""]]}, {"id": "2002.08565", "submitter": "Yinan Zhao", "authors": "Danna Gurari, Yinan Zhao, Meng Zhang, Nilavra Bhattacharya", "title": "Captioning Images Taken by People Who Are Blind", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While an important problem in the vision community is to design algorithms\nthat can automatically caption images, few publicly-available datasets for\nalgorithm development directly address the interests of real users. Observing\nthat people who are blind have relied on (human-based) image captioning\nservices to learn about images they take for nearly a decade, we introduce the\nfirst image captioning dataset to represent this real use case. This new\ndataset, which we call VizWiz-Captions, consists of over 39,000 images\noriginating from people who are blind that are each paired with five captions.\nWe analyze this dataset to (1) characterize the typical captions, (2)\ncharacterize the diversity of content found in the images, and (3) compare its\ncontent to that found in eight popular vision datasets. We also analyze modern\nimage captioning algorithms to identify what makes this new dataset challenging\nfor the vision community. We publicly-share the dataset with captioning\nchallenge instructions at https://vizwiz.org\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 04:36:39 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 15:48:35 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Gurari", "Danna", ""], ["Zhao", "Yinan", ""], ["Zhang", "Meng", ""], ["Bhattacharya", "Nilavra", ""]]}, {"id": "2002.08587", "submitter": "Ke Mei", "authors": "Ke Mei, Chuang Zhu, Lei Jiang, Jun Liu, Yuanyuan Qiao", "title": "Cross-stained Segmentation from Renal Biopsy Images Using Multi-level\n  Adversarial Learning", "comments": "Accepted by ICASSP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation from renal pathological images is a key step in automatic\nanalyzing the renal histological characteristics. However, the performance of\nmodels varies significantly in different types of stained datasets due to the\nappearance variations. In this paper, we design a robust and flexible model for\ncross-stained segmentation. It is a novel multi-level deep adversarial network\narchitecture that consists of three sub-networks: (i) a segmentation network;\n(ii) a pair of multi-level mirrored discriminators for guiding the segmentation\nnetwork to extract domain-invariant features; (iii) a shape discriminator that\nis utilized to further identify the output of the segmentation network and the\nground truth. Experimental results on glomeruli segmentation from renal biopsy\nimages indicate that our network is able to improve segmentation performance on\ntarget type of stained images and use unlabeled data to achieve similar\naccuracy to labeled data. In addition, this method can be easily applied to\nother tasks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 06:49:48 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Mei", "Ke", ""], ["Zhu", "Chuang", ""], ["Jiang", "Lei", ""], ["Liu", "Jun", ""], ["Qiao", "Yuanyuan", ""]]}, {"id": "2002.08595", "submitter": "Yingtao Tian", "authors": "Yingtao Tian, Chikahiko Suzuki, Tarin Clanuwat, Mikel Bober-Irizar,\n  Alex Lamb, Asanobu Kitamoto", "title": "KaoKore: A Pre-modern Japanese Art Facial Expression Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From classifying handwritten digits to generating strings of text, the\ndatasets which have received long-time focus from the machine learning\ncommunity vary greatly in their subject matter. This has motivated a renewed\ninterest in building datasets which are socially and culturally relevant, so\nthat algorithmic research may have a more direct and immediate impact on\nsociety. One such area is in history and the humanities, where better and\nrelevant machine learning models can accelerate research across various fields.\nTo this end, newly released benchmarks and models have been proposed for\ntranscribing historical Japanese cursive writing, yet for the field as a whole\nusing machine learning for historical Japanese artworks still remains largely\nuncharted. To bridge this gap, in this work we propose a new dataset KaoKore\nwhich consists of faces extracted from pre-modern Japanese artwork. We\ndemonstrate its value as both a dataset for image classification as well as a\ncreative and artistic dataset, which we explore using generative models.\nDataset available at https://github.com/rois-codh/kaokore\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 07:22:13 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Tian", "Yingtao", ""], ["Suzuki", "Chikahiko", ""], ["Clanuwat", "Tarin", ""], ["Bober-Irizar", "Mikel", ""], ["Lamb", "Alex", ""], ["Kitamoto", "Asanobu", ""]]}, {"id": "2002.08619", "submitter": "Tianyu Pang", "authors": "Tianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Jun Zhu, Hang Su", "title": "Boosting Adversarial Training with Hypersphere Embedding", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) is one of the most effective defenses against\nadversarial attacks for deep learning models. In this work, we advocate\nincorporating the hypersphere embedding (HE) mechanism into the AT procedure by\nregularizing the features onto compact manifolds, which constitutes a\nlightweight yet effective module to blend in the strength of representation\nlearning. Our extensive analyses reveal that AT and HE are well coupled to\nbenefit the robustness of the adversarially trained models from several\naspects. We validate the effectiveness and adaptability of HE by embedding it\ninto the popular AT frameworks including PGD-AT, ALP, and TRADES, as well as\nthe FreeAT and FastAT strategies. In the experiments, we evaluate our methods\nunder a wide range of adversarial attacks on the CIFAR-10 and ImageNet\ndatasets, which verifies that integrating HE can consistently enhance the model\nrobustness for each AT framework with little extra computation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 08:42:29 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 13:27:17 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 16:18:38 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Pang", "Tianyu", ""], ["Yang", "Xiao", ""], ["Dong", "Yinpeng", ""], ["Xu", "Kun", ""], ["Zhu", "Jun", ""], ["Su", "Hang", ""]]}, {"id": "2002.08623", "submitter": "Junyu Gao", "authors": "Tao Han, Junyu Gao, Yuan Yuan, Qi Wang", "title": "Focus on Semantic Consistency for Cross-domain Crowd Understanding", "comments": "Accpeted by ICASSP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For pixel-level crowd understanding, it is time-consuming and laborious in\ndata collection and annotation. Some domain adaptation algorithms try to\nliberate it by training models with synthetic data, and the results in some\nrecent works have proved the feasibility. However, we found that a mass of\nestimation errors in the background areas impede the performance of the\nexisting methods. In this paper, we propose a domain adaptation method to\neliminate it. According to the semantic consistency, a similar distribution in\ndeep layer's features of the synthetic and real-world crowd area, we first\nintroduce a semantic extractor to effectively distinguish crowd and background\nin high-level semantic information. Besides, to further enhance the adapted\nmodel, we adopt adversarial learning to align features in the semantic space.\nExperiments on three representative real datasets show that the proposed domain\nadaptation scheme achieves the state-of-the-art for cross-domain counting\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 08:51:05 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Han", "Tao", ""], ["Gao", "Junyu", ""], ["Yuan", "Yuan", ""], ["Wang", "Qi", ""]]}, {"id": "2002.08641", "submitter": "Tanya Motwani", "authors": "Tanya Motwani and Manojkumar Parmar", "title": "A Novel Framework for Selection of GANs for an Application", "comments": "11 pages, 1 figure, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Network (GAN) is a current focal point of research.\nThe body of knowledge is fragmented, leading to a trial-error method while\nselecting an appropriate GAN for a given scenario. We provide a comprehensive\nsummary of the evolution of GANs starting from its inception addressing issues\nlike mode collapse, vanishing gradient, unstable training and non-convergence.\nWe also provide a comparison of various GANs from the application point of\nview, its behaviour and implementation details. We propose a novel framework to\nidentify candidate GANs for a specific use case based on architecture, loss,\nregularization and divergence. We also discuss application of the framework\nusing an example, and we demonstrate a significant reduction in search space.\nThis efficient way to determine potential GANs lowers unit economics of AI\ndevelopment for organizations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 09:51:48 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 09:48:42 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Motwani", "Tanya", ""], ["Parmar", "Manojkumar", ""]]}, {"id": "2002.08670", "submitter": "Jiaming Wang", "authors": "Jiaming Wang and Jun Du and Jianshu Zhang", "title": "Stroke Constrained Attention Network for Online Handwritten Mathematical\n  Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel stroke constrained attention network (SCAN)\nwhich treats stroke as the basic unit for encoder-decoder based online\nhandwritten mathematical expression recognition (HMER). Unlike previous methods\nwhich use trace points or image pixels as basic units, SCAN makes full use of\nstroke-level information for better alignment and representation. The proposed\nSCAN can be adopted in both single-modal (online or offline) and multi-modal\nHMER. For single-modal HMER, SCAN first employs a CNN-GRU encoder to extract\npoint-level features from input traces in online mode and employs a CNN encoder\nto extract pixel-level features from input images in offline mode, then use\nstroke constrained information to convert them into online and offline\nstroke-level features. Using stroke-level features can explicitly group points\nor pixels belonging to the same stroke, therefore reduces the difficulty of\nsymbol segmentation and recognition via the decoder with attention mechanism.\nFor multi-modal HMER, other than fusing multi-modal information in decoder,\nSCAN can also fuse multi-modal information in encoder by utilizing the stroke\nbased alignments between online and offline modalities. The encoder fusion is a\nbetter way for combining multi-modal information as it implements the\ninformation interaction one step before the decoder fusion so that the\nadvantages of multiple modalities can be exploited earlier and more adequately\nwhen training the encoder-decoder model. Evaluated on a benchmark published by\nCROHME competition, the proposed SCAN achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 11:01:12 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Wang", "Jiaming", ""], ["Du", "Jun", ""], ["Zhang", "Jianshu", ""]]}, {"id": "2002.08675", "submitter": "You-Wei Luo", "authors": "You-Wei Luo, Chuan-Xian Ren, Pengfei Ge, Ke-Kun Huang, Yu-Feng Yu", "title": "Unsupervised Domain Adaptation via Discriminative Manifold Embedding and\n  Alignment", "comments": "Accepted to AAAI 2020. Code available:\n  \\<https://github.com/LavieLuo/DRMEA>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation is effective in leveraging the rich\ninformation from the source domain to the unsupervised target domain. Though\ndeep learning and adversarial strategy make an important breakthrough in the\nadaptability of features, there are two issues to be further explored. First,\nthe hard-assigned pseudo labels on the target domain are risky to the intrinsic\ndata structure. Second, the batch-wise training manner in deep learning limits\nthe description of the global structure. In this paper, a Riemannian manifold\nlearning framework is proposed to achieve transferability and discriminability\nconsistently. As to the first problem, this method establishes a probabilistic\ndiscriminant criterion on the target domain via soft labels. Further, this\ncriterion is extended to a global approximation scheme for the second issue;\nsuch approximation is also memory-saving. The manifold metric alignment is\nexploited to be compatible with the embedding space. A theoretical error bound\nis derived to facilitate the alignment. Extensive experiments have been\nconducted to investigate the proposal and results of the comparison study\nmanifest the superiority of consistent manifold learning framework.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 11:06:41 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 16:36:53 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Luo", "You-Wei", ""], ["Ren", "Chuan-Xian", ""], ["Ge", "Pengfei", ""], ["Huang", "Ke-Kun", ""], ["Yu", "Yu-Feng", ""]]}, {"id": "2002.08679", "submitter": "Alexander Kozlov", "authors": "Alexander Kozlov and Ivan Lazarevich and Vasily Shamporov and Nikolay\n  Lyalyushkin and Yury Gorbachev", "title": "Neural Network Compression Framework for fast model inference", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we present a new framework for neural networks compression with\nfine-tuning, which we called Neural Network Compression Framework (NNCF). It\nleverages recent advances of various network compression methods and implements\nsome of them, such as sparsity, quantization, and binarization. These methods\nallow getting more hardware-friendly models which can be efficiently run on\ngeneral-purpose hardware computation units (CPU, GPU) or special Deep Learning\naccelerators. We show that the developed methods can be successfully applied to\na wide range of models to accelerate the inference time while keeping the\noriginal accuracy. The framework can be used within the training samples, which\nare supplied with it, or as a standalone package that can be seamlessly\nintegrated into the existing training code with minimal adaptations. Currently,\na PyTorch version of NNCF is available as a part of OpenVINO Training\nExtensions at https://github.com/openvinotoolkit/nncf.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 11:24:01 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 12:52:08 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 10:14:12 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 08:17:23 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kozlov", "Alexander", ""], ["Lazarevich", "Ivan", ""], ["Shamporov", "Vasily", ""], ["Lyalyushkin", "Nikolay", ""], ["Gorbachev", "Yury", ""]]}, {"id": "2002.08681", "submitter": "Yabin Zhang", "authors": "Yabin Zhang, Bin Deng, Hui Tang, Lei Zhang, and Kui Jia", "title": "Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and\n  Practice", "comments": "CVPR extension; TPAMI camera ready version:\n  https://ieeexplore.ieee.org/document/9253700; IEEE copyright; Codes are\n  available at: https://github.com/YBZh/MultiClassDA", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI),10 November 2020", "doi": "10.1109/TPAMI.2020.3036956", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the formalism of unsupervised multi-class domain\nadaptation (multi-class UDA), which underlies a few recent algorithms whose\nlearning objectives are only motivated empirically. Multi-Class Scoring\nDisagreement (MCSD) divergence is presented by aggregating the absolute margin\nviolations in multi-class classification, and this proposed MCSD is able to\nfully characterize the relations between any pair of multi-class scoring\nhypotheses. By using MCSD as a measure of domain distance, we develop a new\ndomain adaptation bound for multi-class UDA; its data-dependent, probably\napproximately correct bound is also developed that naturally suggests\nadversarial learning objectives to align conditional feature distributions\nacross source and target domains. Consequently, an algorithmic framework of\nMulti-class Domain-adversarial learning Networks (McDalNets) is developed, and\nits different instantiations via surrogate learning objectives either coincide\nwith or resemble a few recently popular methods, thus (partially) underscoring\ntheir practical effectiveness. Based on our identical theory for multi-class\nUDA, we also introduce a new algorithm of Domain-Symmetric Networks (SymmNets),\nwhich is featured by a novel adversarial strategy of domain confusion and\ndiscrimination. SymmNets affords simple extensions that work equally well under\nthe problem settings of either closed set, partial, or open set UDA. We conduct\ncareful empirical studies to compare different algorithms of McDalNets and our\nnewly introduced SymmNets. Experiments verify our theoretical analysis and show\nthe efficacy of our proposed SymmNets. In addition, we have made our\nimplementation code publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 11:26:45 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 09:36:34 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Yabin", ""], ["Deng", "Bin", ""], ["Tang", "Hui", ""], ["Zhang", "Lei", ""], ["Jia", "Kui", ""]]}, {"id": "2002.08688", "submitter": "Jordi Pons", "authors": "Berkan Kadioglu, Michael Horgan, Xiaoyu Liu, Jordi Pons, Dan Darcy,\n  and Vivek Kumar", "title": "An empirical study of Conv-TasNet", "comments": "In proceedings of ICASSP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conv-TasNet is a recently proposed waveform-based deep neural network that\nachieves state-of-the-art performance in speech source separation. Its\narchitecture consists of a learnable encoder/decoder and a separator that\noperates on top of this learned space. Various improvements have been proposed\nto Conv-TasNet. However, they mostly focus on the separator, leaving its\nencoder/decoder as a (shallow) linear operator. In this paper, we conduct an\nempirical study of Conv-TasNet and propose an enhancement to the\nencoder/decoder that is based on a (deep) non-linear variant of it. In\naddition, we experiment with the larger and more diverse LibriTTS dataset and\ninvestigate the generalization capabilities of the studied models when trained\non a much larger dataset. We propose cross-dataset evaluation that includes\nassessing separations from the WSJ0-2mix, LibriTTS and VCTK databases. Our\nresults show that enhancements to the encoder/decoder can improve average\nSI-SNR performance by more than 1 dB. Furthermore, we offer insights into the\ngeneralization capabilities of Conv-TasNet and the potential value of\nimprovements to the encoder/decoder.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 11:51:43 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 15:00:22 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Kadioglu", "Berkan", ""], ["Horgan", "Michael", ""], ["Liu", "Xiaoyu", ""], ["Pons", "Jordi", ""], ["Darcy", "Dan", ""], ["Kumar", "Vivek", ""]]}, {"id": "2002.08694", "submitter": "Henghui Ding", "authors": "Xiaohong Wang, Xudong Jiang, Henghui Ding, and Jun Liu", "title": "Bi-directional Dermoscopic Feature Learning and Multi-scale Consistent\n  Decision Fusion for Skin Lesion Segmentation", "comments": "Accepted to TIP", "journal-ref": null, "doi": "10.1109/TIP.2019.2955297", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of skin lesion from dermoscopic images is a crucial\npart of computer-aided diagnosis of melanoma. It is challenging due to the fact\nthat dermoscopic images from different patients have non-negligible lesion\nvariation, which causes difficulties in anatomical structure learning and\nconsistent skin lesion delineation. In this paper, we propose a novel\nbi-directional dermoscopic feature learning (biDFL) framework to model the\ncomplex correlation between skin lesions and their informative context. By\ncontrolling feature information passing through two complementary directions, a\nsubstantially rich and discriminative feature representation is achieved.\nSpecifically, we place biDFL module on the top of a CNN network to enhance\nhigh-level parsing performance. Furthermore, we propose a multi-scale\nconsistent decision fusion (mCDF) that is capable of selectively focusing on\nthe informative decisions generated from multiple classification layers. By\nanalysis of the consistency of the decision at each position, mCDF\nautomatically adjusts the reliability of decisions and thus allows a more\ninsightful skin lesion delineation. The comprehensive experimental results show\nthe effectiveness of the proposed method on skin lesion segmentation, achieving\nstate-of-the-art performance consistently on two publicly available dermoscopic\nimage databases.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 12:00:24 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Wang", "Xiaohong", ""], ["Jiang", "Xudong", ""], ["Ding", "Henghui", ""], ["Liu", "Jun", ""]]}, {"id": "2002.08700", "submitter": "Ruobing Zheng", "authors": "Ruobing Zheng, Zhou Zhu, Bo Song, Changjiang Ji", "title": "A Neural Lip-Sync Framework for Synthesizing Photorealistic Virtual News\n  Anchors", "comments": "Accepted by ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip sync has emerged as a promising technique for generating mouth movements\nfrom audio signals. However, synthesizing a high-resolution and photorealistic\nvirtual news anchor is still challenging. Lack of natural appearance, visual\nconsistency, and processing efficiency are the main problems with existing\nmethods. This paper presents a novel lip-sync framework specially designed for\nproducing high-fidelity virtual news anchors. A pair of Temporal Convolutional\nNetworks are used to learn the cross-modal sequential mapping from audio\nsignals to mouth movements, followed by a neural rendering network that\ntranslates the synthetic facial map into a high-resolution and photorealistic\nappearance. This fully trainable framework provides end-to-end processing that\noutperforms traditional graphics-based methods in many low-delay applications.\nExperiments also show the framework has advantages over modern neural-based\nmethods in both visual appearance and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 12:26:20 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 10:01:18 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zheng", "Ruobing", ""], ["Zhu", "Zhou", ""], ["Song", "Bo", ""], ["Ji", "Changjiang", ""]]}, {"id": "2002.08721", "submitter": "Lars Schmarje", "authors": "Lars Schmarje, Monty Santarossa, Simon-Martin Schr\\\"oder, and Reinhard\n  Koch", "title": "A survey on Semi-, Self- and Unsupervised Learning for Image\n  Classification", "comments": "Accepted to IEEE Access 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning strategies achieve outstanding results in computer vision\ntasks, one issue remains: The current strategies rely heavily on a huge amount\nof labeled data. In many real-world problems, it is not feasible to create such\nan amount of labeled training data. Therefore, it is common to incorporate\nunlabeled data into the training process to reach equal results with fewer\nlabels. Due to a lot of concurrent research, it is difficult to keep track of\nrecent developments. In this survey, we provide an overview of often used ideas\nand methods in image classification with fewer labels. We compare 34 methods in\ndetail based on their performance and their commonly used ideas rather than a\nfine-grained taxonomy. In our analysis, we identify three major trends that\nlead to future research opportunities. 1. State-of-the-art methods are\nscaleable to real-world applications in theory but issues like class imbalance,\nrobustness, or fuzzy labels are not considered. 2. The degree of supervision\nwhich is needed to achieve comparable results to the usage of all labels is\ndecreasing and therefore methods need to be extended to settings with a\nvariable number of classes. 3. All methods share some common ideas but we\nidentify clusters of methods that do not share many ideas. We show that\ncombining ideas from different clusters can lead to better performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 13:29:27 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 13:33:38 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 11:44:02 GMT"}, {"version": "v4", "created": "Thu, 11 Feb 2021 14:16:54 GMT"}, {"version": "v5", "created": "Tue, 25 May 2021 12:29:55 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Schmarje", "Lars", ""], ["Santarossa", "Monty", ""], ["Schr\u00f6der", "Simon-Martin", ""], ["Koch", "Reinhard", ""]]}, {"id": "2002.08725", "submitter": "Maxime Lafarge", "authors": "Maxime W. Lafarge, Erik J. Bekkers, Josien P.W. Pluim, Remco Duits,\n  Mitko Veta", "title": "Roto-Translation Equivariant Convolutional Networks: Application to\n  Histopathology Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation-invariance is a desired property of machine-learning models for\nmedical image analysis and in particular for computational pathology\napplications. We propose a framework to encode the geometric structure of the\nspecial Euclidean motion group SE(2) in convolutional networks to yield\ntranslation and rotation equivariance via the introduction of SE(2)-group\nconvolution layers. This structure enables models to learn feature\nrepresentations with a discretized orientation dimension that guarantees that\ntheir outputs are invariant under a discrete set of rotations. Conventional\napproaches for rotation invariance rely mostly on data augmentation, but this\ndoes not guarantee the robustness of the output when the input is rotated. At\nthat, trained conventional CNNs may require test-time rotation augmentation to\nreach their full capability. This study is focused on histopathology image\nanalysis applications for which it is desirable that the arbitrary global\norientation information of the imaged tissues is not captured by the machine\nlearning models. The proposed framework is evaluated on three different\nhistopathology image analysis tasks (mitosis detection, nuclei segmentation and\ntumor classification). We present a comparative analysis for each problem and\nshow that consistent increase of performances can be achieved when using the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 13:44:29 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Lafarge", "Maxime W.", ""], ["Bekkers", "Erik J.", ""], ["Pluim", "Josien P. W.", ""], ["Duits", "Remco", ""], ["Veta", "Mitko", ""]]}, {"id": "2002.08729", "submitter": "Ke Quan", "authors": "Ke Quan", "title": "Bimodal Distribution Removal and Genetic Algorithm in Neural Network for\n  Breast Cancer Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis of breast cancer has been well studied in the past. Multiple linear\nprogramming models have been devised to approximate the relationship between\ncell features and tumour malignancy. However, these models are less capable in\nhandling non-linear correlations. Neural networks instead are powerful in\nprocessing complex non-linear correlations. It is thus certainly beneficial to\napproach this cancer diagnosis problem with a model based on neural network.\nParticularly, introducing bias to neural network training process is deemed as\nan important means to increase training efficiency. Out of a number of popular\nproposed methods for introducing artificial bias, Bimodal Distribution Removal\n(BDR) presents ideal efficiency improvement results and fair simplicity in\nimplementation. However, this paper examines the effectiveness of BDR against\nthe target cancer diagnosis classification problem and shows that BDR process\nin fact negatively impacts classification performance. In addition, this paper\nalso explores genetic algorithm as an efficient tool for feature selection and\nproduced significantly better results comparing to baseline model that without\nany feature selection in place\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 13:51:40 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Quan", "Ke", ""]]}, {"id": "2002.08742", "submitter": "Joon Son Chung", "authors": "Arsha Nagrani, Joon Son Chung, Samuel Albanie, Andrew Zisserman", "title": "Disentangled Speech Embeddings using Cross-modal Self-supervision", "comments": "ICASSP 2020. The first three authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to learn representations of speaker identity\nwithout access to manually annotated data. To do so, we develop a\nself-supervised learning objective that exploits the natural cross-modal\nsynchrony between faces and audio in video. The key idea behind our approach is\nto tease apart--without annotation--the representations of linguistic content\nand speaker identity. We construct a two-stream architecture which: (1) shares\nlow-level features common to both representations; and (2) provides a natural\nmechanism for explicitly disentangling these factors, offering the potential\nfor greater generalisation to novel combinations of content and identity and\nultimately producing speaker identity representations that are more robust. We\ntrain our method on a large-scale audio-visual dataset of talking heads `in the\nwild', and demonstrate its efficacy by evaluating the learned speaker\nrepresentations for standard speaker recognition performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 14:13:12 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 15:01:49 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Nagrani", "Arsha", ""], ["Chung", "Joon Son", ""], ["Albanie", "Samuel", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2002.08749", "submitter": "Henghui Ding", "authors": "Jianhan Mei, Henghui Ding, Xudong Jiang", "title": "Object 6D Pose Estimation with Non-local Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the challenging task of estimating 6D object pose\nfrom a single RGB image. Motivated by the deep learning based object detection\nmethods, we propose a concise and efficient network that integrate 6D object\npose parameter estimation into the object detection framework. Furthermore, for\nmore robust estimation to occlusion, a non-local self-attention module is\nintroduced. The experimental results show that the proposed method reaches the\nstate-of-the-art performance on the YCB-video and the Linemod datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 14:23:32 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Mei", "Jianhan", ""], ["Ding", "Henghui", ""], ["Jiang", "Xudong", ""]]}, {"id": "2002.08797", "submitter": "Soufiane Hayou", "authors": "Soufiane Hayou, Jean-Francois Ton, Arnaud Doucet, Yee Whye Teh", "title": "Robust Pruning at Initialization", "comments": "37 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overparameterized Neural Networks (NN) display state-of-the-art performance.\nHowever, there is a growing need for smaller, energy-efficient, neural networks\ntobe able to use machine learning applications on devices with limited\ncomputational resources. A popular approach consists of using pruning\ntechniques. While these techniques have traditionally focused on pruning\npre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et\nal. (2018) has shown promising results when pruning at initialization. However,\nfor Deep NNs, such procedures remain unsatisfactory as the resulting pruned\nnetworks can be difficult to train and, for instance, they do not prevent one\nlayer from being fully pruned. In this paper, we provide a comprehensive\ntheoretical analysis of Magnitude and Gradient based pruning at initialization\nand training of sparse architectures. This allows us to propose novel\nprincipled approaches which we validate experimentally on a variety of NN\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 17:09:50 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 18:26:19 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 11:14:29 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 17:12:50 GMT"}, {"version": "v5", "created": "Wed, 19 May 2021 22:43:36 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hayou", "Soufiane", ""], ["Ton", "Jean-Francois", ""], ["Doucet", "Arnaud", ""], ["Teh", "Yee Whye", ""]]}, {"id": "2002.08820", "submitter": "Vishwesh Nath", "authors": "Vishwesh Nath, Sudhir K. Pathak, Kurt G. Schilling, Walt Schneider,\n  Bennett A. Landman", "title": "Deep Learning Estimation of Multi-Tissue Constrained Spherical\n  Deconvolution with Limited Single Shell DW-MRI", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion-weighted magnetic resonance imaging (DW-MRI) is the only\nnon-invasive approach for estimation of intra-voxel tissue microarchitecture\nand reconstruction of in vivo neural pathways for the human brain. With\nimprovement in accelerated MRI acquisition technologies, DW-MRI protocols that\nmake use of multiple levels of diffusion sensitization have gained popularity.\nA well-known advanced method for reconstruction of white matter microstructure\nthat uses multi-shell data is multi-tissue constrained spherical deconvolution\n(MT-CSD). MT-CSD substantially improves the resolution of intra-voxel structure\nover the traditional single shell version, constrained spherical deconvolution\n(CSD). Herein, we explore the possibility of using deep learning on single\nshell data (using the b=1000 s/mm2 from the Human Connectome Project (HCP)) to\nestimate the information content captured by 8th order MT-CSD using the full\nthree shell data (b=1000, 2000, and 3000 s/mm2 from HCP). Briefly, we examine\ntwo network architectures: 1.) Sequential network of fully connected dense\nlayers with a residual block in the middle (ResDNN), 2.) Patch based\nconvolutional neural network with a residual block (ResCNN). For both networks\nan additional output block for estimation of voxel fraction was used with a\nmodified loss function. Each approach was compared against the baseline of\nusing MT-CSD on all data on 15 subjects from the HCP divided into 5 training, 2\nvalidation, and 8 testing subjects with a total of 6.7 million voxels. The\nfiber orientation distribution function (fODF) can be recovered with high\ncorrelation (0.77 vs 0.74 and 0.65) as compared to the ground truth of MT-CST,\nwhich was derived from the multi-shell DW-MRI acquisitions. Source code and\nmodels have been made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 15:59:03 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Nath", "Vishwesh", ""], ["Pathak", "Sudhir K.", ""], ["Schilling", "Kurt G.", ""], ["Schneider", "Walt", ""], ["Landman", "Bennett A.", ""]]}, {"id": "2002.08822", "submitter": "Matthias Minderer", "authors": "Matthias Minderer, Olivier Bachem, Neil Houlsby, Michael Tschannen", "title": "Automatic Shortcut Removal for Self-Supervised Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In self-supervised visual representation learning, a feature extractor is\ntrained on a \"pretext task\" for which labels can be generated cheaply, without\nhuman annotation. A central challenge in this approach is that the feature\nextractor quickly learns to exploit low-level visual features such as color\naberrations or watermarks and then fails to learn useful semantic\nrepresentations. Much work has gone into identifying such \"shortcut\" features\nand hand-designing schemes to reduce their effect. Here, we propose a general\nframework for mitigating the effect shortcut features. Our key assumption is\nthat those features which are the first to be exploited for solving the pretext\ntask may also be the most vulnerable to an adversary trained to make the task\nharder. We show that this assumption holds across common pretext tasks and\ndatasets by training a \"lens\" network to make small image changes that\nmaximally reduce performance in the pretext task. Representations learned with\nthe modified images outperform those learned without in all tested cases.\nAdditionally, the modifications made by the lens reveal how the choice of\npretext task and dataset affects the features learned by self-supervision.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:00:18 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 17:31:56 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 11:15:48 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Minderer", "Matthias", ""], ["Bachem", "Olivier", ""], ["Houlsby", "Neil", ""], ["Tschannen", "Michael", ""]]}, {"id": "2002.08859", "submitter": "Eitan Richardson", "authors": "Eitan Richardson and Yair Weiss", "title": "A Bayes-Optimal View on Adversarial Examples", "comments": "Minor revision per journal review, 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Since the discovery of adversarial examples - the ability to fool modern CNN\nclassifiers with tiny perturbations of the input, there has been much\ndiscussion whether they are a \"bug\" that is specific to current neural\narchitectures and training methods or an inevitable \"feature\" of high\ndimensional geometry. In this paper, we argue for examining adversarial\nexamples from the perspective of Bayes-Optimal classification. We construct\nrealistic image datasets for which the Bayes-Optimal classifier can be\nefficiently computed and derive analytic conditions on the distributions under\nwhich these classifiers are provably robust against any adversarial attack even\nin high dimensions. Our results show that even when these \"gold standard\"\noptimal classifiers are robust, CNNs trained on the same datasets consistently\nlearn a vulnerable classifier, indicating that adversarial examples are often\nan avoidable \"bug\". We further show that RBF SVMs trained on the same data\nconsistently learn a robust classifier. The same trend is observed in\nexperiments with real images in different datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:43:47 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 09:47:10 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Richardson", "Eitan", ""], ["Weiss", "Yair", ""]]}, {"id": "2002.08900", "submitter": "M Sadegh Riazi", "authors": "M. Sadegh Riazi and Seyed M. Chavoshian and Farinaz Koushanfar", "title": "SynFi: Automatic Synthetic Fingerprint Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authentication and identification methods based on human fingerprints are\nubiquitous in several systems ranging from government organizations to consumer\nproducts. The performance and reliability of such systems directly rely on the\nvolume of data on which they have been verified. Unfortunately, a large volume\nof fingerprint databases is not publicly available due to many privacy and\nsecurity concerns.\n  In this paper, we introduce a new approach to automatically generate\nhigh-fidelity synthetic fingerprints at scale. Our approach relies on (i)\nGenerative Adversarial Networks to estimate the probability distribution of\nhuman fingerprints and (ii) Super-Resolution methods to synthesize fine-grained\ntextures. We rigorously test our system and show that our methodology is the\nfirst to generate fingerprints that are computationally indistinguishable from\nreal ones, a task that prior art could not accomplish.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 07:45:29 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Riazi", "M. Sadegh", ""], ["Chavoshian", "Seyed M.", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "2002.08916", "submitter": "Aidan Boyd", "authors": "Aidan Boyd, Adam Czajka, Kevin Bowyer", "title": "Deep Learning-Based Feature Extraction in Iris Recognition: Use Existing\n  Models, Fine-tune or Train From Scratch?", "comments": "Presented at BTAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning techniques can be employed to generate effective feature\nextractors for the task of iris recognition. The question arises: should we\ntrain such structures from scratch on a relatively large iris image dataset, or\nit is better to fine-tune the existing models to adapt them to a new domain? In\nthis work we explore five different sets of weights for the popular ResNet-50\narchitecture to find out whether iris-specific feature extractors perform\nbetter than models trained for non-iris tasks. Features are extracted from each\nconvolutional layer and the classification accuracy achieved by a Support\nVector Machine is measured on a dataset that is disjoint from the samples used\nin training of the ResNet-50 model. We show that the optimal training strategy\nis to fine-tune an off-the-shelf set of weights to the iris recognition domain.\nThis approach results in greater accuracy than both off-the-shelf weights and a\nmodel trained from scratch. The winning, fine-tuned approach also shows an\nincrease in performance when compared to previous work, in which only\noff-the-shelf (not fine-tuned) models were used in iris feature extraction. We\nmake the best-performing ResNet-50 model, fine-tuned with more than 360,000\niris images, publicly available along with this paper.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 18:00:33 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Boyd", "Aidan", ""], ["Czajka", "Adam", ""], ["Bowyer", "Kevin", ""]]}, {"id": "2002.08935", "submitter": "Timo S\\\"amann", "authors": "Timo S\\\"amann, Peter Schlicht, Fabian H\\\"uger", "title": "Strategy to Increase the Safety of a DNN-based Perception for HAD\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety is one of the most important development goals for highly automated\ndriving (HAD) systems. This applies in particular to the perception function\ndriven by Deep Neural Networks (DNNs). For these, large parts of the\ntraditional safety processes and requirements are not fully applicable or\nsufficient. The aim of this paper is to present a framework for the description\nand mitigation of DNN insufficiencies and the derivation of relevant safety\nmechanisms to increase the safety of DNNs. To assess the effectiveness of these\nsafety mechanisms, we present a categorization scheme for evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 18:32:53 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["S\u00e4mann", "Timo", ""], ["Schlicht", "Peter", ""], ["H\u00fcger", "Fabian", ""]]}, {"id": "2002.08945", "submitter": "Ehsan Adeli", "authors": "Bingbin Liu, Ehsan Adeli, Zhangjie Cao, Kuan-Hui Lee, Abhijeet Shenoi,\n  Adrien Gaidon, Juan Carlos Niebles", "title": "Spatiotemporal Relationship Reasoning for Pedestrian Intent Prediction", "comments": "Accepted at ICRA 2020 and IEEE Robotics and Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning over visual data is a desirable capability for robotics and\nvision-based applications. Such reasoning enables forecasting of the next\nevents or actions in videos. In recent years, various models have been\ndeveloped based on convolution operations for prediction or forecasting, but\nthey lack the ability to reason over spatiotemporal data and infer the\nrelationships of different objects in the scene. In this paper, we present a\nframework based on graph convolution to uncover the spatiotemporal\nrelationships in the scene for reasoning about pedestrian intent. A scene graph\nis built on top of segmented object instances within and across video frames.\nPedestrian intent, defined as the future action of crossing or not-crossing the\nstreet, is a very crucial piece of information for autonomous vehicles to\nnavigate safely and more smoothly. We approach the problem of intent prediction\nfrom two different perspectives and anticipate the intention-to-cross within\nboth pedestrian-centric and location-centric scenarios. In addition, we\nintroduce a new dataset designed specifically for autonomous-driving scenarios\nin areas with dense pedestrian populations: the Stanford-TRI Intent Prediction\n(STIP) dataset. Our experiments on STIP and another benchmark dataset show that\nour graph modeling framework is able to predict the intention-to-cross of the\npedestrians with an accuracy of 79.10% on STIP and 79.28% on \\rev{Joint\nAttention for Autonomous Driving (JAAD) dataset up to one second earlier than\nwhen the actual crossing happens. These results outperform the baseline and\nprevious work. Please refer to http://stip.stanford.edu/ for the dataset and\ncode.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 18:50:44 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Liu", "Bingbin", ""], ["Adeli", "Ehsan", ""], ["Cao", "Zhangjie", ""], ["Lee", "Kuan-Hui", ""], ["Shenoi", "Abhijeet", ""], ["Gaidon", "Adrien", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "2002.08959", "submitter": "Aidan Boyd", "authors": "Aidan Boyd, Adam Czajka, Kevin Bowyer", "title": "Are Gabor Kernels Optimal for Iris Recognition?", "comments": "To appear at IJCB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gabor kernels are widely accepted as dominant filters for iris recognition.\nIn this work we investigate, given the current interest in neural networks, if\nGabor kernels are the only family of functions performing best in iris\nrecognition, or if better filters can be learned directly from iris data. We\nuse (on purpose) a single-layer convolutional neural network as it mimics an\niris code-based algorithm. We learn two sets of data-driven kernels; one\nstarting from randomly initialized weights and the other from open-source set\nof Gabor kernels. Through experimentation, we show that the network does not\nconverge on Gabor kernels, instead converging on a mix of edge detectors, blob\ndetectors and simple waves. In our experiments carried out with three\nsubject-disjoint datasets we found that the performance of these learned\nkernels is comparable to the open-source Gabor kernels. These lead us to two\nconclusions: (a) a family of functions offering optimal performance in iris\nrecognition is wider than Gabor kernels, and (b) we probably hit the maximum\nperformance for an iris coding algorithm that uses a single convolutional\nlayer, yet with multiple filters. Released with this work is a framework to\nlearn data-driven kernels that can be easily transplanted into open-source iris\nrecognition software (for instance, OSIRIS -- Open Source IRIS).\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:51:11 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Boyd", "Aidan", ""], ["Czajka", "Adam", ""], ["Bowyer", "Kevin", ""]]}, {"id": "2002.08973", "submitter": "Raphael Gontijo-Lopes", "authors": "Raphael Gontijo-Lopes, Sylvia J. Smullin, Ekin D. Cubuk, Ethan Dyer", "title": "Affinity and Diversity: Quantifying Mechanisms of Data Augmentation", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though data augmentation has become a standard component of deep neural\nnetwork training, the underlying mechanism behind the effectiveness of these\ntechniques remains poorly understood. In practice, augmentation policies are\noften chosen using heuristics of either distribution shift or augmentation\ndiversity. Inspired by these, we seek to quantify how data augmentation\nimproves model generalization. To this end, we introduce interpretable and\neasy-to-compute measures: Affinity and Diversity. We find that augmentation\nperformance is predicted not by either of these alone but by jointly optimizing\nthe two.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 19:02:02 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 19:04:48 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Gontijo-Lopes", "Raphael", ""], ["Smullin", "Sylvia J.", ""], ["Cubuk", "Ekin D.", ""], ["Dyer", "Ethan", ""]]}, {"id": "2002.08975", "submitter": "Aria Wang", "authors": "Aria Yuan Wang and Michael J. Tarr", "title": "Learning Intermediate Features of Object Affordances with a\n  Convolutional Neural Network", "comments": "Published on 2018 Conference on Cognitive Computational Neuroscience.\n  See <https://ccneuro.org/2018/Papers/ViewPapers.asp?PaperNum=1134>", "journal-ref": null, "doi": "10.32470/CCN.2018.1134-0", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our ability to interact with the world around us relies on being able to\ninfer what actions objects afford -- often referred to as affordances. The\nneural mechanisms of object-action associations are realized in the visuomotor\npathway where information about both visual properties and actions is\nintegrated into common representations. However, explicating these mechanisms\nis particularly challenging in the case of affordances because there is hardly\nany one-to-one mapping between visual features and inferred actions. To better\nunderstand the nature of affordances, we trained a deep convolutional neural\nnetwork (CNN) to recognize affordances from images and to learn the underlying\nfeatures or the dimensionality of affordances. Such features form an underlying\ncompositional structure for the general representation of affordances which can\nthen be tested against human neural data. We view this representational\nanalysis as the first step towards a more formal account of how humans perceive\nand interact with the environment.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 19:04:40 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Wang", "Aria Yuan", ""], ["Tarr", "Michael J.", ""]]}, {"id": "2002.08981", "submitter": "Stathi Fotiadis", "authors": "Stathi Fotiadis, Eduardo Pignatelli, Mario Lino Valencia, Chris\n  Cantwell, Amos Storkey, Anil A. Bharath", "title": "Comparing recurrent and convolutional neural networks for predicting\n  wave propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical systems can be modelled by partial differential equations and\nnumerical computations are used everywhere in science and engineering. In this\nwork, we investigate the performance of recurrent and convolutional deep neural\nnetwork architectures to predict the surface waves. The system is governed by\nthe Saint-Venant equations. We improve on the long-term prediction over\nprevious methods while keeping the inference time at a fraction of numerical\nsimulations. We also show that convolutional networks perform at least as well\nas recurrent networks in this task. Finally, we assess the generalisation\ncapability of each network by extrapolating in longer time-frames and in\ndifferent physical settings.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 19:15:04 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 18:05:20 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 14:28:56 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Fotiadis", "Stathi", ""], ["Pignatelli", "Eduardo", ""], ["Valencia", "Mario Lino", ""], ["Cantwell", "Chris", ""], ["Storkey", "Amos", ""], ["Bharath", "Anil A.", ""]]}, {"id": "2002.08988", "submitter": "Thu Nguyen-Phuoc", "authors": "Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-Liang Yang, Niloy\n  Mitra", "title": "BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled\n  Images", "comments": "For project page, see https://www.monkeyoverflow.com/#/blockgan/\n  Accepted to Conference on Neural Information Processing Systemsm, NeurIPS\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BlockGAN, an image generative model that learns object-aware 3D\nscene representations directly from unlabelled 2D images. Current work on scene\nrepresentation learning either ignores scene background or treats the whole\nscene as one object. Meanwhile, work that considers scene compositionality\ntreats scene objects only as image patches or 2D layers with alpha maps.\nInspired by the computer graphics pipeline, we design BlockGAN to learn to\nfirst generate 3D features of background and foreground objects, then combine\nthem into 3D features for the wholes cene, and finally render them into\nrealistic images. This allows BlockGAN to reason over occlusion and interaction\nbetween objects' appearance, such as shadow and lighting, and provides control\nover each object's 3D pose and identity, while maintaining image realism.\nBlockGAN is trained end-to-end, using only unlabelled single images, without\nthe need for 3D geometry, pose labels, object masks, or multiple views of the\nsame scene. Our experiments show that using explicit 3D features to represent\nobjects allows BlockGAN to learn disentangled representations both in terms of\nobjects (foreground and background) and their properties (pose and identity).\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 19:41:06 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 18:06:29 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 20:28:23 GMT"}, {"version": "v4", "created": "Wed, 2 Dec 2020 11:57:32 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Nguyen-Phuoc", "Thu", ""], ["Richardt", "Christian", ""], ["Mai", "Long", ""], ["Yang", "Yong-Liang", ""], ["Mitra", "Niloy", ""]]}, {"id": "2002.08991", "submitter": "Keno Bressem", "authors": "Keno K. Bressem, Lisa Adams, Christoph Erxleben, Bernd Hamm, Stefan\n  Niehues, Janis Vahldiek", "title": "Comparing Different Deep Learning Architectures for Classification of\n  Chest Radiographs", "comments": "15 pages, 6 figures, 3 tables", "journal-ref": null, "doi": "10.1038/s41598-020-70479-z", "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chest radiographs are among the most frequently acquired images in radiology\nand are often the subject of computer vision research. However, most of the\nmodels used to classify chest radiographs are derived from openly available\ndeep neural networks, trained on large image-datasets. These datasets routinely\ndiffer from chest radiographs in that they are mostly color images and contain\nseveral possible image classes, while radiographs are greyscale images and\noften only contain fewer image classes. Therefore, very deep neural networks,\nwhich can represent more complex relationships in image-features, might not be\nrequired for the comparatively simpler task of classifying grayscale chest\nradiographs. We compared fifteen different architectures of artificial neural\nnetworks regarding training-time and performance on the openly available\nCheXpert dataset to identify the most suitable models for deep learning tasks\non chest radiographs. We could show, that smaller networks such as ResNet-34,\nAlexNet or VGG-16 have the potential to classify chest radiographs as precisely\nas deeper neural networks such as DenseNet-201 or ResNet-151, while being less\ncomputationally demanding.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 19:47:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Bressem", "Keno K.", ""], ["Adams", "Lisa", ""], ["Erxleben", "Christoph", ""], ["Hamm", "Bernd", ""], ["Niehues", "Stefan", ""], ["Vahldiek", "Janis", ""]]}, {"id": "2002.09003", "submitter": "Francisco Delgado", "authors": "Javier Finat, Francisco Delgado-del-Hoyo", "title": "Complete Endomorphisms in Computer Vision", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondences between k-tuples of points are key in multiple view geometry\nand motion analysis. Regular transformations are posed by homographies between\ntwo projective planes that serves as structural models for images. Such\ntransformations can not include degenerate situations. Fundamental or essential\nmatrices expand homographies with structural information by using degenerate\nbilinear maps. The projectivization of the endomorphisms of a three-dimensional\nvector space includes all of them. Hence, they are able to explain a wider\nrange of eventually degenerate transformations between arbitrary pairs of\nviews. To include these degenerate situations, this paper introduces a\ncompletion of bilinear maps between spaces given by an equivariant\ncompactification of regular transformations. This completion is extensible to\nthe varieties of fundamental and essential matrices, where most methods based\non regular transformations fail. The construction of complete endomorphisms\nmanages degenerate projection maps using a simultaneous action on source and\ntarget spaces. In such way, this mathematical construction provides a robust\nframework to relate corresponding views in multiple view geometry.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 20:28:08 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Finat", "Javier", ""], ["Delgado-del-Hoyo", "Francisco", ""]]}, {"id": "2002.09023", "submitter": "Xin Guo", "authors": "Xin Guo and Luisa F. Polan\\'ia and Kenneth E. Barner", "title": "Audio-video Emotion Recognition in the Wild using Deep Hybrid Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an audiovisual-based emotion recognition hybrid network.\nWhile most of the previous work focuses either on using deep models or\nhand-engineered features extracted from images, we explore multiple deep models\nbuilt on both images and audio signals. Specifically, in addition to\nconvolutional neural networks (CNN) and recurrent neutral networks (RNN)\ntrained on facial images, the hybrid network also contains one SVM classifier\ntrained on holistic acoustic feature vectors, one long short-term memory\nnetwork (LSTM) trained on short-term feature sequences extracted from segmented\naudio clips, and one Inception(v2)-LSTM network trained on image-like maps,\nwhich are built based on short-term acoustic feature sequences. Experimental\nresults show that the proposed hybrid network outperforms the baseline method\nby a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 21:18:17 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Guo", "Xin", ""], ["Polan\u00eda", "Luisa F.", ""], ["Barner", "Kenneth E.", ""]]}, {"id": "2002.09034", "submitter": "Narciso L\\'opez-L\\'opez", "authors": "Narciso L\\'opez-L\\'opez, Andrea V\\'azquez, Cyril Poupon,\n  Jean-Fran\\c{c}ois Mangin, Pamela Guevara", "title": "Cortical surface parcellation based on intra-subject white matter fiber\n  clustering", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941, CONICYT PFCHA/ DOCTORADO\n  NACIONAL/2016-21160342, CONICYT FONDECYT 1190701, CONICYT PIA/Anillo de\n  Investigaci\\'on en Ciencia y Tecnolog\\'ia ACT172121 and CONICYT Basal Center\n  FB0008", "journal-ref": null, "doi": "10.1109/CHILECON47746.2019.8988066", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid method that performs the complete parcellation of the\ncerebral cortex of an individual, based on the connectivity information of the\nwhite matter fibers from a whole-brain tractography dataset. The method\nconsists of five steps, first intra-subject clustering is performed on the\nbrain tractography. The fibers that make up each cluster are then intersected\nwith the cortical mesh and then filtered to discard outliers. In addition, the\nmethod resolves the overlapping between the different intersection regions\n(sub-parcels) throughout the cortex efficiently. Finally, a post-processing is\ndone to achieve more uniform sub-parcels. The output is the complete labeling\nof cortical mesh vertices, representing the different cortex sub-parcels, with\nstrong connections to other sub-parcels. We evaluated our method with measures\nof brain connectivity such as functional segregation (clustering coefficient),\nfunctional integration (characteristic path length) and small-world. Results in\nfive subjects from ARCHI database show a good individual cortical parcellation\nfor each one, composed of about 200 subparcels per hemisphere and complying\nwith these connectivity measures.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 19:14:39 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["L\u00f3pez-L\u00f3pez", "Narciso", ""], ["V\u00e1zquez", "Andrea", ""], ["Poupon", "Cyril", ""], ["Mangin", "Jean-Fran\u00e7ois", ""], ["Guevara", "Pamela", ""]]}, {"id": "2002.09045", "submitter": "Sheng He", "authors": "Sheng He, Randy L. Gollub, Shawn N. Murphy, Juan David Perez, Sanjay\n  Prabhu, Rudolph Pienaar, Richard L. Robertson, P. Ellen Grant, Yangming Ou", "title": "Brain Age Estimation Using LSTM on Children's Brain MRI", "comments": "ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain age prediction based on children's brain MRI is an important biomarker\nfor brain health and brain development analysis. In this paper, we consider the\n3D brain MRI volume as a sequence of 2D images and propose a new framework\nusing the recurrent neural network for brain age estimation. The proposed\nmethod is named as 2D-ResNet18+Long short-term memory (LSTM), which consists of\nfour parts: 2D ResNet18 for feature extraction on 2D images, a pooling layer\nfor feature reduction over the sequences, an LSTM layer, and a final regression\nlayer. We apply the proposed method on a public multisite NIH-PD dataset and\nevaluate generalization on a second multisite dataset, which shows that the\nproposed 2D-ResNet18+LSTM method provides better results than traditional 3D\nbased neural network for brain age estimation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 22:27:52 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["He", "Sheng", ""], ["Gollub", "Randy L.", ""], ["Murphy", "Shawn N.", ""], ["Perez", "Juan David", ""], ["Prabhu", "Sanjay", ""], ["Pienaar", "Rudolph", ""], ["Robertson", "Richard L.", ""], ["Grant", "P. Ellen", ""], ["Ou", "Yangming", ""]]}, {"id": "2002.09048", "submitter": "Manashi Chakraborty", "authors": "Manashi Chakraborty, Mayukh Roy, Prabir Kumar Biswas, Pabitra Mitra", "title": "Unsupervised Pre-trained, Texture Aware And Lightweight Model for Deep\n  Learning-Based Iris Recognition Under Limited Annotated Data", "comments": "Under review at ICIP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a texture aware lightweight deep learning framework\nfor iris recognition. Our contributions are primarily three fold. Firstly, to\naddress the dearth of labelled iris data, we propose a reconstruction loss\nguided unsupervised pre-training stage followed by supervised refinement. This\ndrives the network weights to focus on discriminative iris texture patterns.\nNext, we propose several texture aware improvisations inside a Convolution\nNeural Net to better leverage iris textures. Finally, we show that our\nsystematic training and architectural choices enable us to design an efficient\nframework with upto 100X fewer parameters than contemporary deep learning\nbaselines yet achieve better recognition performance for within and cross\ndataset evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 22:30:38 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Chakraborty", "Manashi", ""], ["Roy", "Mayukh", ""], ["Biswas", "Prabir Kumar", ""], ["Mitra", "Pabitra", ""]]}, {"id": "2002.09049", "submitter": "Xingchao Liu", "authors": "Xingchao Liu, Mao Ye, Dengyong Zhou, Qiang Liu", "title": "Post-training Quantization with Multiple Points: Mixed Precision without\n  Mixed Precision", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the post-training quantization problem, which discretizes the\nweights of pre-trained deep neural networks without re-training the model. We\npropose multipoint quantization, a quantization method that approximates a\nfull-precision weight vector using a linear combination of multiple vectors of\nlow-bit numbers; this is in contrast to typical quantization methods that\napproximate each weight using a single low precision number. Computationally,\nwe construct the multipoint quantization with an efficient greedy selection\nprocedure, and adaptively decides the number of low precision points on each\nquantized weight vector based on the error of its output. This allows us to\nachieve higher precision levels for important weights that greatly influence\nthe outputs, yielding an 'effect of mixed precision' but without physical mixed\nprecision implementations (which requires specialized hardware accelerators).\nEmpirically, our method can be implemented by common operands, bringing almost\nno memory and computation overhead. We show that our method outperforms a range\nof state-of-the-art methods on ImageNet classification and it can be\ngeneralized to more challenging tasks like PASCAL VOC object detection.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 22:37:45 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 07:20:56 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 15:25:38 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Liu", "Xingchao", ""], ["Ye", "Mao", ""], ["Zhou", "Dengyong", ""], ["Liu", "Qiang", ""]]}, {"id": "2002.09053", "submitter": "Wenhao Wang", "authors": "Wenhao Wang", "title": "Adapted Center and Scale Prediction: More Stable and More Accurate", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection benefits from deep learning technology and gains rapid\ndevelopment in recent years. Most of detectors follow general object detection\nframe, i.e. default boxes and two-stage process. Recently, anchor-free and\none-stage detectors have been introduced into this area. However, their\naccuracies are unsatisfactory. Therefore, in order to enjoy the simplicity of\nanchor-free detectors and the accuracy of two-stage ones simultaneously, we\npropose some adaptations based on a detector, Center and Scale Prediction(CSP).\nThe main contributions of our paper are: (1) We improve the robustness of CSP\nand make it easier to train. (2) We propose a novel method to predict width,\nnamely compressing width. (3) We achieve the second best performance on\nCityPersons benchmark, i.e. 9.3% log-average miss rate(MR) on reasonable set,\n8.7% MR on partial set and 5.6% MR on bare set, which shows an anchor-free and\none-stage detector can still have high accuracy. (4) We explore some\ncapabilities of Switchable Normalization which are not mentioned in its\noriginal paper.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 22:49:50 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 02:32:54 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Wang", "Wenhao", ""]]}, {"id": "2002.09068", "submitter": "Sudipta Banerjee", "authors": "Sudipta Banerjee and Arun Ross", "title": "Face Phylogeny Tree Using Basis Functions", "comments": "Updated paper particulalrly Section 4.2.7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photometric transformations, such as brightness and contrast adjustment, can\nbe applied to a face image repeatedly creating a set of near-duplicate images.\nIdentifying the original image from a set of such near-duplicates and deducing\nthe relationship between them are important in the context of digital image\nforensics. This is commonly done by generating an image phylogeny tree\n\\textemdash \\hspace{0.08cm} a hierarchical structure depicting the relationship\nbetween a set of near-duplicate images. In this work, we utilize three\ndifferent families of basis functions to model pairwise relationships between\nnear-duplicate images. The basis functions used in this work are orthogonal\npolynomials, wavelet basis functions and radial basis functions. We perform\nextensive experiments to assess the performance of the proposed method across\nthree different modalities, namely, face, fingerprint and iris images; across\ndifferent image phylogeny tree configurations; and across different types of\nphotometric transformations. We also utilize the same basis functions to model\ngeometric transformations and deep-learning based transformations. We also\nperform extensive analysis of each basis function with respect to its ability\nto model arbitrary transformations and to distinguish between the original and\nthe transformed images. Finally, we utilize the concept of approximate von\nNeumann graph entropy to explain the success and failure cases of the proposed\nIPT generation algorithm. Experiments indicate that the proposed algorithm\ngeneralizes well across different scenarios thereby suggesting the merits of\nusing basis functions to model the relationship between photometrically and\ngeometrically modified images.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 00:13:21 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 20:35:06 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Banerjee", "Sudipta", ""], ["Ross", "Arun", ""]]}, {"id": "2002.09085", "submitter": "Han Hu", "authors": "Qing Zhu, Zhendong Wang, Han Hu, Linfu Xie, Xuming Ge, Yeting Zhang", "title": "Leveraging Photogrammetric Mesh Models for Aerial-Ground Feature Point\n  Matching Toward Integrated 3D Reconstruction", "comments": "Accepted for publication in ISPRS Journal of Photogrammetry and\n  Remote Sensing", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2020.05.024", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration of aerial and ground images has been proved as an efficient\napproach to enhance the surface reconstruction in urban environments. However,\nas the first step, the feature point matching between aerial and ground images\nis remarkably difficult, due to the large differences in viewpoint and\nillumination conditions. Previous studies based on geometry-aware image\nrectification have alleviated this problem, but the performance and convenience\nof this strategy is limited by several flaws, e.g. quadratic image pairs,\nsegregated extraction of descriptors and occlusions. To address these problems,\nwe propose a novel approach: leveraging photogrammetric mesh models for\naerial-ground image matching. The methods of this proposed approach have linear\ntime complexity with regard to the number of images, can explicitly handle low\noverlap using multi-view images and can be directly injected into off-the-shelf\nstructure-from-motion (SfM) and multi-view stereo (MVS) solutions. First,\naerial and ground images are reconstructed separately and initially\nco-registered through weak georeferencing data. Second, aerial models are\nrendered to the initial ground views, in which the color, depth and normal\nimages are obtained. Then, the synthesized color images and the corresponding\nground images are matched by comparing the descriptors, filtered by local\ngeometrical information, and then propagated to the aerial views using depth\nimages and patch-based matching. Experimental evaluations using various\ndatasets confirm the superior performance of the proposed methods in\naerial-ground image matching. In addition, incorporation of the existing SfM\nand MVS solutions into these methods enables more complete and accurate models\nto be directly obtained.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 01:47:59 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 07:41:15 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhu", "Qing", ""], ["Wang", "Zhendong", ""], ["Hu", "Han", ""], ["Xie", "Linfu", ""], ["Ge", "Xuming", ""], ["Zhang", "Yeting", ""]]}, {"id": "2002.09103", "submitter": "Arsenii Ashukha", "authors": "Dmitry Molchanov, Alexander Lyzhov, Yuliya Molchanova, Arsenii\n  Ashukha, Dmitry Vetrov", "title": "Greedy Policy Search: A Simple Baseline for Learnable Test-Time\n  Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Test-time data augmentation$-$averaging the predictions of a machine learning\nmodel across multiple augmented samples of data$-$is a widely used technique\nthat improves the predictive performance. While many advanced learnable data\naugmentation techniques have emerged in recent years, they are focused on the\ntraining phase. Such techniques are not necessarily optimal for test-time\naugmentation and can be outperformed by a policy consisting of simple crops and\nflips. The primary goal of this paper is to demonstrate that test-time\naugmentation policies can be successfully learned too. We introduce greedy\npolicy search (GPS), a simple but high-performing method for learning a policy\nof test-time augmentation. We demonstrate that augmentation policies learned\nwith GPS achieve superior predictive performance on image classification\nproblems, provide better in-domain uncertainty estimation, and improve the\nrobustness to domain shift.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 02:57:13 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 13:10:23 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Molchanov", "Dmitry", ""], ["Lyzhov", "Alexander", ""], ["Molchanova", "Yuliya", ""], ["Ashukha", "Arsenii", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "2002.09107", "submitter": "Iretiayo Akinola", "authors": "Iretiayo Akinola, Jacob Varley and Dmitry Kalashnikov", "title": "Learning Precise 3D Manipulation from Multiple Uncalibrated Cameras", "comments": "Accepted at International Conference on Robotics and Automation (ICRA\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an effective multi-view approach to closed-loop\nend-to-end learning of precise manipulation tasks that are 3D in nature. Our\nmethod learns to accomplish these tasks using multiple statically placed but\nuncalibrated RGB camera views without building an explicit 3D representation\nsuch as a pointcloud or voxel grid. This multi-camera approach achieves\nsuperior task performance on difficult stacking and insertion tasks compared to\nsingle-view baselines. Single view robotic agents struggle from occlusion and\nchallenges in estimating relative poses between points of interest. While full\n3D scene representations (voxels or pointclouds) are obtainable from registered\noutput of multiple depth sensors, several challenges complicate operating off\nsuch explicit 3D representations. These challenges include imperfect camera\ncalibration, poor depth maps due to object properties such as reflective\nsurfaces, and slower inference speeds over 3D representations compared to 2D\nimages. Our use of static but uncalibrated cameras does not require\ncamera-robot or camera-camera calibration making the proposed approach easy to\nsetup and our use of \\textit{sensor dropout} during training makes it resilient\nto the loss of camera-views after deployment.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 03:28:42 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 18:48:24 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Akinola", "Iretiayo", ""], ["Varley", "Jacob", ""], ["Kalashnikov", "Dmitry", ""]]}, {"id": "2002.09120", "submitter": "Nhu-Tai Do Mr", "authors": "Nhu-Tai Do, Tram-Tran Nguyen-Quynh and Soo-Hyung Kim", "title": "Affective Expression Analysis in-the-wild using Multi-Task Temporal\n  Statistical Deep Learning Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Affective behavior analysis plays an important role in human-computer\ninteraction, customer marketing, health monitoring. ABAW Challenge and\nAff-Wild2 dataset raise the new challenge for classifying basic emotions and\nregression valence-arousal value under in-the-wild environments. In this paper,\nwe present an affective expression analysis model that deals with the above\nchallenges. Our approach includes STAT and Temporal Module for fine-tuning\nagain face feature model. We experimented on Aff-Wild2 dataset, a large-scale\ndataset for ABAW Challenge with the annotations for both the categorical and\nvalence-arousal emotion. We achieved the expression score 0.543 and\nvalence-arousal score 0.534 on the validation set.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 04:06:03 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 07:57:34 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 08:23:46 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Do", "Nhu-Tai", ""], ["Nguyen-Quynh", "Tram-Tran", ""], ["Kim", "Soo-Hyung", ""]]}, {"id": "2002.09131", "submitter": "Wonmin Byeon", "authors": "Jiahao Su, Wonmin Byeon, Jean Kossaifi, Furong Huang, Jan Kautz,\n  Animashree Anandkumar", "title": "Convolutional Tensor-Train LSTM for Spatio-temporal Learning", "comments": "Jiahao Su and Wonmin Byeon contributed equally to this work. 22\n  pages, 14 figures, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from spatio-temporal data has numerous applications such as\nhuman-behavior analysis, object tracking, video compression, and physics\nsimulation.However, existing methods still perform poorly on challenging video\ntasks such as long-term forecasting. This is because these kinds of challenging\ntasks require learning long-term spatio-temporal correlations in the video\nsequence. In this paper, we propose a higher-order convolutional LSTM model\nthat can efficiently learn these correlations, along with a succinct\nrepresentations of the history. This is accomplished through a novel tensor\ntrain module that performs prediction by combining convolutional features\nacross time. To make this feasible in terms of computation and memory\nrequirements, we propose a novel convolutional tensor-train decomposition of\nthe higher-order model. This decomposition reduces the model complexity by\njointly approximating a sequence of convolutional kernels asa low-rank\ntensor-train factorization. As a result, our model outperforms existing\napproaches, but uses only a fraction of parameters, including the baseline\nmodels.Our results achieve state-of-the-art performance in a wide range of\napplications and datasets, including the multi-steps video prediction on the\nMoving-MNIST-2and KTH action datasets as well as early activity recognition on\nthe Something-Something V2 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 05:00:01 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 14:31:38 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2020 19:27:44 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2020 18:30:12 GMT"}, {"version": "v5", "created": "Sun, 4 Oct 2020 23:14:31 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Su", "Jiahao", ""], ["Byeon", "Wonmin", ""], ["Kossaifi", "Jean", ""], ["Huang", "Furong", ""], ["Kautz", "Jan", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "2002.09136", "submitter": "Yuanyi Zhong", "authors": "Yuanyi Zhong, Alexander Schwing, Jian Peng", "title": "Disentangling Controllable Object through Video Prediction Improves\n  Visual Reinforcement Learning", "comments": "Accepted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many vision-based reinforcement learning (RL) problems, the agent controls\na movable object in its visual field, e.g., the player's avatar in video games\nand the robotic arm in visual grasping and manipulation. Leveraging\naction-conditioned video prediction, we propose an end-to-end learning\nframework to disentangle the controllable object from the observation signal.\nThe disentangled representation is shown to be useful for RL as additional\nobservation channels to the agent. Experiments on a set of Atari games with the\npopular Double DQN algorithm demonstrate improved sample efficiency and game\nperformance (from 222.8% to 261.4% measured in normalized game scores, with\nprediction bonus reward).\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 05:43:34 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Zhong", "Yuanyi", ""], ["Schwing", "Alexander", ""], ["Peng", "Jian", ""]]}, {"id": "2002.09137", "submitter": "Zhaoyuan Fang", "authors": "Zhaoyuan Fang, Adam Czajka, Kevin W. Bowyer", "title": "Robust Iris Presentation Attack Detection Fusing 2D and 3D Information", "comments": "Accepted to IEEE T-IFS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity and unpredictability of artifacts potentially presented to an iris\nsensor calls for presentation attack detection methods that are agnostic to\nspecificity of presentation attack instruments. This paper proposes a method\nthat combines two-dimensional and three-dimensional properties of the observed\niris to address the problem of spoof detection in case when some properties of\nartifacts are unknown. The 2D (textural) iris features are extracted by a\nstate-of-the-art method employing Binary Statistical Image Features (BSIF) and\nan ensemble of classifiers is used to deliver 2D modality-related decision. The\n3D (shape) iris features are reconstructed by a photometric stereo method from\nonly two images captured under near-infrared illumination placed at two\ndifferent angles, as in many current commercial iris recognition sensors. The\nmap of normal vectors is used to assess the convexity of the observed iris\nsurface. The combination of these two approaches has been applied to detect\nwhether a subject is wearing a textured contact lens to disguise their\nidentity. Extensive experiments with NDCLD'15 dataset, and a newly collected\nNDIris3D dataset show that the proposed method is highly robust under various\nopen-set testing scenarios, and that it outperforms all available open-source\niris PAD methods tested in identical scenarios. The source code and the newly\nprepared benchmark are made available along with this paper.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 05:44:38 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 17:38:41 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Fang", "Zhaoyuan", ""], ["Czajka", "Adam", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "2002.09147", "submitter": "Jilin Mei", "authors": "Yancheng Pan, Biao Gao, Jilin Mei, Sibo Geng, Chengkun Li and Huijing\n  Zhao", "title": "SemanticPOSS: A Point Cloud Dataset with Large Quantity of Dynamic\n  Instances", "comments": "submited to IEEE Intelligent Vehicles Symposium(2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D semantic segmentation is one of the key tasks for autonomous driving\nsystem. Recently, deep learning models for 3D semantic segmentation task have\nbeen widely researched, but they usually require large amounts of training\ndata. However, the present datasets for 3D semantic segmentation are lack of\npoint-wise annotation, diversiform scenes and dynamic objects.\n  In this paper, we propose the SemanticPOSS dataset, which contains 2988\nvarious and complicated LiDAR scans with large quantity of dynamic instances.\nThe data is collected in Peking University and uses the same data format as\nSemanticKITTI. In addition, we evaluate several typical 3D semantic\nsegmentation models on our SemanticPOSS dataset. Experimental results show that\nSemanticPOSS can help to improve the prediction accuracy of dynamic objects as\npeople, car in some degree. SemanticPOSS will be published at\n\\url{www.poss.pku.edu.cn}.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 06:10:34 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Pan", "Yancheng", ""], ["Gao", "Biao", ""], ["Mei", "Jilin", ""], ["Geng", "Sibo", ""], ["Li", "Chengkun", ""], ["Zhao", "Huijing", ""]]}, {"id": "2002.09161", "submitter": "Xinwei Shen", "authors": "Xinwei Shen, Tong Zhang, Kani Chen", "title": "Bidirectional Generative Modeling Using Adversarial Gradient Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the general $f$-divergence formulation of bidirectional\ngenerative modeling, which includes VAE and BiGAN as special cases. We present\na new optimization method for this formulation, where the gradient is computed\nusing an adversarially learned discriminator. In our framework, we show that\ndifferent divergences induce similar algorithms in terms of gradient\nevaluation, except with different scaling. Therefore this paper gives a general\nrecipe for a class of principled $f$-divergence based generative modeling\nmethods. Theoretical justifications and extensive empirical studies are\nprovided to demonstrate the advantage of our approach over existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 07:28:56 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 10:43:09 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 03:59:02 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Shen", "Xinwei", ""], ["Zhang", "Tong", ""], ["Chen", "Kani", ""]]}, {"id": "2002.09168", "submitter": "Yujun Shen", "authors": "Mengya Gao, Yujun Shen, Quanquan Li, Chen Change Loy", "title": "Residual Knowledge Distillation", "comments": "9 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is one of the most potent ways for model\ncompression. The key idea is to transfer the knowledge from a deep teacher\nmodel (T) to a shallower student (S). However, existing methods suffer from\nperformance degradation due to the substantial gap between the learning\ncapacities of S and T. To remedy this problem, this work proposes Residual\nKnowledge Distillation (RKD), which further distills the knowledge by\nintroducing an assistant (A). Specifically, S is trained to mimic the feature\nmaps of T, and A aids this process by learning the residual error between them.\nIn this way, S and A complement with each other to get better knowledge from T.\nFurthermore, we devise an effective method to derive S and A from a given model\nwithout increasing the total computational cost. Extensive experiments show\nthat our approach achieves appealing results on popular classification\ndatasets, CIFAR-100 and ImageNet, surpassing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 07:49:26 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Gao", "Mengya", ""], ["Shen", "Yujun", ""], ["Li", "Quanquan", ""], ["Loy", "Chen Change", ""]]}, {"id": "2002.09181", "submitter": "Philipp Terh\\\"orst", "authors": "Philipp Terh\\\"orst, Marco Huber, Naser Damer, Florian Kirchbuchner,\n  Arjan Kuijper", "title": "Unsupervised Enhancement of Soft-biometric Privacy with Negative Face\n  Recognition", "comments": "Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research on soft-biometrics showed that privacy-sensitive information\ncan be deduced from biometric templates of an individual. Since for many\napplications, these templates are expected to be used for recognition purposes\nonly, this raises major privacy issues. Previous works focused on supervised\nprivacy-enhancing solutions that require privacy-sensitive information about\nindividuals and limit their application to the suppression of single and\npre-defined attributes. Consequently, they do not take into account attributes\nthat are not considered in the training. In this work, we present Negative Face\nRecognition (NFR), a novel face recognition approach that enhances the\nsoft-biometric privacy on the template-level by representing face templates in\na complementary (negative) domain. While ordinary templates characterize facial\nproperties of an individual, negative templates describe facial properties that\ndoes not exist for this individual. This suppresses privacy-sensitive\ninformation from stored templates. Experiments are conducted on two publicly\navailable datasets captured under controlled and uncontrolled scenarios on\nthree privacy-sensitive attributes. The experiments demonstrate that our\nproposed approach reaches higher suppression rates than previous work, while\nmaintaining higher recognition performances as well. Unlike previous works, our\napproach does not require privacy-sensitive labels and offers a more\ncomprehensive privacy-protection not limited to pre-defined attributes.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 08:37:16 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Terh\u00f6rst", "Philipp", ""], ["Huber", "Marco", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2002.09202", "submitter": "Kushal Vaghani", "authors": "Kushal Vaghani", "title": "Curating Social Media Data", "comments": "Masters by Research Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms have empowered the democratization of the pulse of\npeople in the modern era. Due to its immense popularity and high usage, data\npublished on social media sites (e.g., Twitter, Facebook and Tumblr) is a rich\nocean of information. Therefore data-driven analytics of social imprints has\nbecome a vital asset for organisations and governments to further improve their\nproducts and services. However, due to the dynamic and noisy nature of social\nmedia data, performing accurate analysis on raw data is a challenging task. A\nkey requirement is to curate the raw data before fed into analytics pipelines.\nThis curation process transforms the raw data into contextualized data and\nknowledge. We propose a data curation pipeline, namely CrowdCorrect, to enable\nanalysts cleansing and curating social data and preparing it for reliable\nanalytics. Our pipeline provides an automatic feature extraction from a corpus\nof social media data using existing in-house tools. Further, we offer a\ndual-correction mechanism using both automated and crowd-sourced approaches.\nThe implementation of this pipeline also includes a set of tools for\nautomatically creating micro-tasks to facilitate the contribution of crowd\nusers in curating the raw data. For the purposes of this research, we use\nTwitter as our motivational social media data platform due to its popularity.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 10:07:15 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Vaghani", "Kushal", ""]]}, {"id": "2002.09211", "submitter": "Jia Peng", "authors": "Peng Jia, Qiang Liu, Yongyang Sun", "title": "Detection and Classification of Astronomical Targets with Deep Neural\n  Networks in Wide Field Small Aperture Telescopes", "comments": "Accepted by Astronomical Journal. The complete code can be downloaded\n  from https://doi.org/10.12149/101016. This code can be directly used to\n  process images obtained by WFSATs. Images obtained by ordinary sky survey\n  telescopes can also be processed with this code, however more annotated\n  images are required to train the neural network", "journal-ref": null, "doi": "10.3847/1538-3881/ab800a", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide field small aperture telescopes are widely used for optical transient\nobservations. Detection and classification of astronomical targets in observed\nimages are the most important and basic step. In this paper, we propose an\nastronomical targets detection and classification framework based on deep\nneural networks. Our framework adopts the concept of the Faster R-CNN and uses\na modified Resnet-50 as backbone network and a Feature Pyramid Network to\nextract features from images of different astronomical targets. To increase the\ngeneralization ability of our framework, we use both simulated and real\nobservation images to train the neural network. After training, the neural\nnetwork could detect and classify astronomical targets automatically. We test\nthe performance of our framework with simulated data and find that our\nframework has almost the same detection ability as that of the traditional\nmethod for bright and isolated sources and our framework has 2 times better\ndetection ability for dim targets, albeit all celestial objects detected by the\ntraditional method can be classified correctly. We also use our framework to\nprocess real observation data and find that our framework can improve 25 %\ndetection ability than that of the traditional method when the threshold of our\nframework is 0.6. Rapid discovery of transient targets is quite important and\nwe further propose to install our framework in embedded devices such as the\nNvidia Jetson Xavier to achieve real-time astronomical targets detection and\nclassification abilities.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 10:35:31 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 13:21:56 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Jia", "Peng", ""], ["Liu", "Qiang", ""], ["Sun", "Yongyang", ""]]}, {"id": "2002.09219", "submitter": "Jean-Yves Franceschi", "authors": "Jean-Yves Franceschi (MLIA), Edouard Delasalles (MLIA), Micka\\\"el Chen\n  (MLIA), Sylvain Lamprier (MLIA), Patrick Gallinari (MLIA)", "title": "Stochastic Latent Residual Video Prediction", "comments": null, "journal-ref": "Thirty-seventh International Conference on Machine Learning,\n  International Machine Learning Society, Jul 2020, Vienne, Austria. pp.89--102", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing video prediction models that account for the inherent uncertainty\nof the future is challenging. Most works in the literature are based on\nstochastic image-autoregressive recurrent networks, which raises several\nperformance and applicability issues. An alternative is to use fully latent\ntemporal models which untie frame synthesis and temporal dynamics. However, no\nsuch model for stochastic video prediction has been proposed in the literature\nyet, due to design and training difficulties. In this paper, we overcome these\ndifficulties by introducing a novel stochastic temporal model whose dynamics\nare governed in a latent space by a residual update rule. This first-order\nscheme is motivated by discretization schemes of differential equations. It\nnaturally models video dynamics as it allows our simpler, more interpretable,\nlatent model to outperform prior state-of-the-art methods on challenging\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 10:44:01 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 15:50:43 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 14:34:16 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2020 14:37:21 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Franceschi", "Jean-Yves", "", "MLIA"], ["Delasalles", "Edouard", "", "MLIA"], ["Chen", "Micka\u00ebl", "", "MLIA"], ["Lamprier", "Sylvain", "", "MLIA"], ["Gallinari", "Patrick", "", "MLIA"]]}, {"id": "2002.09237", "submitter": "S\\\"oren Klemm", "authors": "Karim Huesmann, Soeren Klemm, Lars Linsen and Benjamin Risse", "title": "Exploiting the Full Capacity of Deep Neural Networks while Avoiding\n  Overfitting by Targeted Sparsity Regularization", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Overfitting is one of the most common problems when training deep neural\nnetworks on comparatively small datasets. Here, we demonstrate that neural\nnetwork activation sparsity is a reliable indicator for overfitting which we\nutilize to propose novel targeted sparsity visualization and regularization\nstrategies. Based on these strategies we are able to understand and counteract\noverfitting caused by activation sparsity and filter correlation in a targeted\nlayer-by-layer manner. Our results demonstrate that targeted sparsity\nregularization can efficiently be used to regularize well-known datasets and\narchitectures with a significant increase in image classification performance\nwhile outperforming both dropout and batch normalization. Ultimately, our study\nreveals novel insights into the contradicting concepts of activation sparsity\nand network capacity by demonstrating that targeted sparsity regularization\nenables salient and discriminative feature learning while exploiting the full\ncapacity of deep models without suffering from overfitting, even when trained\nexcessively.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 11:38:17 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Huesmann", "Karim", ""], ["Klemm", "Soeren", ""], ["Linsen", "Lars", ""], ["Risse", "Benjamin", ""]]}, {"id": "2002.09249", "submitter": "Marcell Beregi-Kov\\'acs", "authors": "Marcell Beregi-Kov\\'acs, \\'Agnes Baran and Andr\\'as Hajdu", "title": "Efficient Learning of Model Weights via Changing Features During\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a machine learning model, which dynamically changes\nthe features during training. Our main motivation is to update the model in a\nsmall content during the training process with replacing less descriptive\nfeatures to new ones from a large pool. The main benefit is coming from the\nfact that opposite to the common practice we do not start training a new model\nfrom the scratch, but can keep the already learned weights. This procedure\nallows the scan of a large feature pool which together with keeping the\ncomplexity of the model leads to an increase of the model accuracy within the\nsame training time. The efficiency of our approach is demonstrated in several\nclassic machine learning scenarios including linear regression and neural\nnetwork-based training. As a specific analysis towards signal processing, we\nhave successfully tested our approach on the database MNIST for digit\nclassification considering single pixel and pixel-pairs intensities as possible\nfeatures.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 12:38:14 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Beregi-Kov\u00e1cs", "Marcell", ""], ["Baran", "\u00c1gnes", ""], ["Hajdu", "Andr\u00e1s", ""]]}, {"id": "2002.09274", "submitter": "Yu-Jhe Li", "authors": "Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Yu-Chiang Frank Wang", "title": "Cross-Resolution Adversarial Dual Network for Person Re-Identification\n  and Beyond", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). 17 pages. arXiv admin note: substantial text overlap\n  with arXiv:1908.06052", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) aims at matching images of the same person\nacross camera views. Due to varying distances between cameras and persons of\ninterest, resolution mismatch can be expected, which would degrade re-ID\nperformance in real-world scenarios. To overcome this problem, we propose a\nnovel generative adversarial network to address cross-resolution person re-ID,\nallowing query images with varying resolutions. By advancing adversarial\nlearning techniques, our proposed model learns resolution-invariant image\nrepresentations while being able to recover the missing details in\nlow-resolution input images. The resulting features can be jointly applied for\nimproving re-ID performance due to preserving resolution invariance and\nrecovering re-ID oriented discriminative details. Extensive experimental\nresults on five standard person re-ID benchmarks confirm the effectiveness of\nour method and the superiority over the state-of-the-art approaches, especially\nwhen the input resolutions are not seen during training. Furthermore, the\nexperimental results on two vehicle re-ID benchmarks also confirm the\ngeneralization of our model on cross-resolution visual tasks. The extensions of\nsemi-supervised settings further support the use of our proposed approach to\nreal-world scenarios and applications.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 07:21:38 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:01:01 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Li", "Yu-Jhe", ""], ["Chen", "Yun-Chun", ""], ["Lin", "Yen-Yu", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2002.09280", "submitter": "Mohamed Abbas Hedjazi", "authors": "Mohamed Abbas Hedjazi, Yakup Genc", "title": "Learning to Inpaint by Progressively Growing the Mask Regions", "comments": "ICCV Workshop on Should we preregister experiments in computer\n  vision?, Seoul, South Korea, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting is one of the most challenging tasks in computer vision.\nRecently, generative-based image inpainting methods have been shown to produce\nvisually plausible images. However, they still have difficulties to generate\nthe correct structures and colors as the masked region grows large. This\ndrawback is due to the training stability issue of the generative models. This\nwork introduces a new curriculum-style training approach in the context of\nimage inpainting. The proposed method increases the masked region size\nprogressively in training time, during test time the user gives variable size\nand multiple holes at arbitrary locations. Incorporating such an approach in\nGANs may stabilize the training and provides better color consistencies and\ncaptures object continuities. We validate our approach on the MSCOCO and CelebA\ndatasets. We report qualitative and quantitative comparisons of our training\napproach in different models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 13:33:05 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Hedjazi", "Mohamed Abbas", ""], ["Genc", "Yakup", ""]]}, {"id": "2002.09285", "submitter": "Maxime Martineau", "authors": "Maxime Martineau, Romain Raveaux, Donatello Conte, Gilles Venturini", "title": "A Convolutional Neural Network into graph space", "comments": "arXiv admin note: text overlap with arXiv:1611.08402 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs), in a few decades, have outperformed the\nexisting state of the art methods in classification context. However, in the\nway they were formalised, CNNs are bound to operate on euclidean spaces.\nIndeed, convolution is a signal operation that are defined on euclidean spaces.\nThis has restricted deep learning main use to euclidean-defined data such as\nsound or image. And yet, numerous computer application fields (among which\nnetwork analysis, computational social science, chemo-informatics or computer\ngraphics) induce non-euclideanly defined data such as graphs, networks or\nmanifolds. In this paper we propose a new convolution neural network\narchitecture, defined directly into graph space. Convolution and pooling\noperators are defined in graph domain. We show its usability in a\nback-propagation context. Experimental results show that our model performance\nis at state of the art level on simple tasks. It shows robustness with respect\nto graph domain changes and improvement with respect to other euclidean and\nnon-euclidean convolutional architectures.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 15:14:21 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 12:59:14 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Martineau", "Maxime", ""], ["Raveaux", "Romain", ""], ["Conte", "Donatello", ""], ["Venturini", "Gilles", ""]]}, {"id": "2002.09298", "submitter": "Hanan Salam", "authors": "Ahmed Rachid Hazourli and Amine Djeghri and Hanan Salam and Alice\n  Othmani", "title": "Deep Multi-Facial Patches Aggregation Network For Facial Expression\n  Recognition", "comments": "This article arXiv:2002.09298 is an updated version of\n  arXiv:1909.10305", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach for Facial Expressions Recognition\n(FER) based on a deep multi-facial patches aggregation network. Deep features\nare learned from facial patches using deep sub-networks and aggregated within\none deep architecture for expression classification . Several problems may\naffect the performance of deep-learning based FER approaches, in particular,\nthe small size of existing FER datasets which might not be sufficient to train\nlarge deep learning networks. Moreover, it is extremely time-consuming to\ncollect and annotate a large number of facial images. To account for this, we\npropose two data augmentation techniques for facial expression generation to\nexpand FER labeled training datasets. We evaluate the proposed framework on\nthree FER datasets. Results show that the proposed approach achieves\nstate-of-art FER deep learning approaches performance when the model is trained\nand tested on images from the same dataset. Moreover, the proposed data\naugmentation techniques improve the expression recognition rate, and thus can\nbe a solution for training deep learning FER models using small datasets. The\naccuracy degrades significantly when testing for dataset bias.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:57:06 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hazourli", "Ahmed Rachid", ""], ["Djeghri", "Amine", ""], ["Salam", "Hanan", ""], ["Othmani", "Alice", ""]]}, {"id": "2002.09303", "submitter": "Thomas Deselaers", "authors": "Philippe Gervais and Thomas Deselaers and Emre Aksan and Otmar\n  Hilliges", "title": "The DIDI dataset: Digital Ink Diagram data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We are releasing a dataset of diagram drawings with dynamic drawing\ninformation. The dataset aims to foster research in interactive graphical\nsymbolic understanding. The dataset was obtained using a prompted data\ncollection effort.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:16:28 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 11:56:21 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gervais", "Philippe", ""], ["Deselaers", "Thomas", ""], ["Aksan", "Emre", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2002.09315", "submitter": "Kangming Yan", "authors": "Yuan Zhou and Kangming Yan", "title": "Domain Adaptive Adversarial Learning Based on Physics Model Feedback for\n  Underwater Image Enhancement", "comments": "arXiv admin note: substantial text overlap with arXiv:1808.00605,\n  arXiv:1807.03528 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to refraction, absorption, and scattering of light by suspended\nparticles in water, raw underwater images suffer from low contrast, blurred\ndetails, and color distortion. These characteristics can significantly\ninterfere with the visibility of underwater images and the result of visual\ntasks, such as segmentation and tracking. To address this problem, we propose a\nnew robust adversarial learning framework via physics model based feedback\ncontrol and domain adaptation mechanism for enhancing underwater images to get\nrealistic results. A new method for simulating underwater-like training dataset\nfrom RGB-D data by underwater image formation model is proposed. Upon the\nsynthetic dataset, a novel enhancement framework, which introduces a domain\nadaptive mechanism as well as a physics model constraint feedback control, is\ntrained to enhance the underwater scenes. Final enhanced results on synthetic\nand real underwater images demonstrate the superiority of the proposed method,\nwhich outperforms nondeep and deep learning methods in both qualitative and\nquantitative evaluations. Furthermore, we perform an ablation study to show the\ncontributions of each component we proposed.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 07:50:00 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Zhou", "Yuan", ""], ["Yan", "Kangming", ""]]}, {"id": "2002.09317", "submitter": "Yi Zhao", "authors": "Yi Zhao, Nils Wandel, Magdalena Landl, Andrea Schnepf, Sven Behnke", "title": "3D U-Net for Segmentation of Plant Root MRI Images in Super-Resolution", "comments": "6 pages, 5 figures, in the 28th European Symposium on Artificial\n  Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) enables plant scientists to non-invasively\nstudy root system development and root-soil interaction. Challenging recording\nconditions, such as low resolution and a high level of noise hamper the\nperformance of traditional root extraction algorithms, though. We propose to\nincrease signal-to-noise ratio and resolution by segmenting the scanned volumes\ninto root and soil in super-resolution using a 3D U-Net. Tests on real data\nshow that the trained network is capable to detect most roots successfully and\neven finds roots that were missed by human annotators. Our experiments show\nthat the segmentation performance can be further improved with modifications of\nthe loss function.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 14:12:57 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Zhao", "Yi", ""], ["Wandel", "Nils", ""], ["Landl", "Magdalena", ""], ["Schnepf", "Andrea", ""], ["Behnke", "Sven", ""]]}, {"id": "2002.09406", "submitter": "Gregory Palmer", "authors": "Gregory Palmer, Benjamin Schnieders, Rahul Savani, Karl Tuyls,\n  Joscha-David Fossel, Harry Flore", "title": "The Automated Inspection of Opaque Liquid Vaccines", "comments": "8 pages, 5 Figures, 3 Tables, ECAI 2020 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the pharmaceutical industry the screening of opaque vaccines containing\nsuspensions is currently a manual task carried out by trained human visual\ninspectors. We show that deep learning can be used to effectively automate this\nprocess. A moving contrast is required to distinguish anomalies from other\nparticles, reflections and dust resting on a vial's surface. We train\n3D-ConvNets to predict the likelihood of 20-frame video samples containing\nanomalies. Our unaugmented dataset consists of hand-labelled samples, recorded\nusing vials provided by the HAL Allergy Group, a pharmaceutical company. We\ntrained ten randomly initialized 3D-ConvNets to provide a benchmark, observing\nmean AUROC scores of 0.94 and 0.93 for positive samples (containing anomalies)\nand negative (anomaly-free) samples, respectively. Using Frame-Completion\nGenerative Adversarial Networks we: (i) introduce an algorithm for computing\nsaliency maps, which we use to verify that the 3D-ConvNets are indeed\nidentifying anomalies; (ii) propose a novel self-training approach using the\nsaliency maps to determine if multiple networks agree on the location of\nanomalies. Our self-training approach allows us to augment our data set by\nlabelling 217,888 additional samples. 3D-ConvNets trained with our augmented\ndataset improve on the results we get when we train only on the unaugmented\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 16:45:29 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Palmer", "Gregory", ""], ["Schnieders", "Benjamin", ""], ["Savani", "Rahul", ""], ["Tuyls", "Karl", ""], ["Fossel", "Joscha-David", ""], ["Flore", "Harry", ""]]}, {"id": "2002.09423", "submitter": "David Torpey", "authors": "David Torpey and Turgay Celik", "title": "Human Action Recognition using Local Two-Stream Convolution Neural\n  Network Features and Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple yet effective method for human action\nrecognition in video. The proposed method separately extracts local appearance\nand motion features using state-of-the-art three-dimensional convolutional\nneural networks from sampled snippets of a video. These local features are then\nconcatenated to form global representations which are then used to train a\nlinear SVM to perform the action classification using full context of the\nvideo, as partial context as used in previous works. The videos undergo two\nsimple proposed preprocessing techniques, optical flow scaling and crop\nfilling. We perform an extensive evaluation on three common benchmark dataset\nto empirically show the benefit of the SVM, and the two preprocessing steps.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 17:26:32 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Torpey", "David", ""], ["Celik", "Turgay", ""]]}, {"id": "2002.09424", "submitter": "David Torpey", "authors": "Ziyad Jappie and David Torpey and Turgay Celik", "title": "SummaryNet: A Multi-Stage Deep Learning Model for Automatic Video\n  Summarisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarisation can be posed as the task of extracting important parts of\na video in order to create an informative summary of what occurred in the\nvideo. In this paper we introduce SummaryNet as a supervised learning framework\nfor automated video summarisation. SummaryNet employs a two-stream\nconvolutional network to learn spatial (appearance) and temporal (motion)\nrepresentations. It utilizes an encoder-decoder model to extract the most\nsalient features from the learned video representations. Lastly, it uses a\nsigmoid regression network with bidirectional long short-term memory cells to\npredict the probability of a frame being a summary frame. Experimental results\non benchmark datasets show that the proposed method achieves comparable or\nsignificantly better results than the state-of-the-art video summarisation\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 18:24:35 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Jappie", "Ziyad", ""], ["Torpey", "David", ""], ["Celik", "Turgay", ""]]}, {"id": "2002.09437", "submitter": "Viveka Kulharia", "authors": "Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz,\n  Philip H.S. Torr, Puneet K. Dokania", "title": "Calibrating Deep Neural Networks using Focal Loss", "comments": "This paper was accepted at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Miscalibration - a mismatch between a model's confidence and its correctness\n- of Deep Neural Networks (DNNs) makes their predictions hard to rely on.\nIdeally, we want networks to be accurate, calibrated and confident. We show\nthat, as opposed to the standard cross-entropy loss, focal loss [Lin et. al.,\n2017] allows us to learn models that are already very well calibrated. When\ncombined with temperature scaling, whilst preserving accuracy, it yields\nstate-of-the-art calibrated models. We provide a thorough analysis of the\nfactors causing miscalibration, and use the insights we glean from this to\njustify the empirically excellent performance of focal loss. To facilitate the\nuse of focal loss in practice, we also provide a principled approach to\nautomatically select the hyperparameter involved in the loss function. We\nperform extensive experiments on a variety of computer vision and NLP datasets,\nand with a wide variety of network architectures, and show that our approach\nachieves state-of-the-art calibration without compromising on accuracy in\nalmost all cases. Code is available at\nhttps://github.com/torrvision/focal_calibration.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 17:35:50 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 14:22:17 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mukhoti", "Jishnu", ""], ["Kulharia", "Viveka", ""], ["Sanyal", "Amartya", ""], ["Golodetz", "Stuart", ""], ["Torr", "Philip H. S.", ""], ["Dokania", "Puneet K.", ""]]}, {"id": "2002.09461", "submitter": "Peng Xu", "authors": "Peng Xu, Kun Liu, Tao Xiang, Timothy M. Hospedales, Zhanyu Ma, Jun\n  Guo, Yi-Zhe Song", "title": "Fine-Grained Instance-Level Sketch-Based Video Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing sketch-analysis work studies sketches depicting static objects or\nscenes. In this work, we propose a novel cross-modal retrieval problem of\nfine-grained instance-level sketch-based video retrieval (FG-SBVR), where a\nsketch sequence is used as a query to retrieve a specific target video\ninstance. Compared with sketch-based still image retrieval, and coarse-grained\ncategory-level video retrieval, this is more challenging as both visual\nappearance and motion need to be simultaneously matched at a fine-grained\nlevel. We contribute the first FG-SBVR dataset with rich annotations. We then\nintroduce a novel multi-stream multi-modality deep network to perform FG-SBVR\nunder both strong and weakly supervised settings. The key component of the\nnetwork is a relation module, designed to prevent model over-fitting given\nscarce training data. We show that this model significantly outperforms a\nnumber of existing state-of-the-art models designed for video analysis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 18:28:35 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Xu", "Peng", ""], ["Liu", "Kun", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""], ["Ma", "Zhanyu", ""], ["Guo", "Jun", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2002.09474", "submitter": "Elena Limonova", "authors": "Elena Limonova and Arseny Terekhin and Dmitry Nikolaev and Vladimir\n  Arlazarov", "title": "Fast Implementation of Morphological Filtering Using ARM NEON Extension", "comments": "6 pages, 4 figures", "journal-ref": "International Journal of Applied Engineering Research (ISSN\n  0973-4562), Volume 11, Number 24 (2016), pp. 11675-11680", "doi": null, "report-no": null, "categories": "cs.DC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider speedup potential of morphological image filtering\non ARM processors. Morphological operations are widely used in image analysis\nand recognition and their speedup in some cases can significantly reduce\noverall execution time of recognition. More specifically, we propose fast\nimplementation of erosion and dilation using ARM SIMD extension NEON. These\noperations with the rectangular structuring element are separable. They were\nimplemented using the advantages of separability as sequential horizontal and\nvertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm\nfor large windows and low-constant linear complexity algorithm for small\nwindows. Final implementation was improved with SIMD and used a combination of\nthese methods. We also considered fast transpose implementation of 8x8 and\n16x16 matrices using ARM NEON to get additional computational gain for\nmorphological operations. Experiments showed 3 times efficiency increase for\nfinal implementation of erosion and dilation compared to van Herk/Gil-Werman\nalgorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times\nspeedup for 16x16 matrix transpose compared to transpose without SIMD.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 12:55:34 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Limonova", "Elena", ""], ["Terekhin", "Arseny", ""], ["Nikolaev", "Dmitry", ""], ["Arlazarov", "Vladimir", ""]]}, {"id": "2002.09479", "submitter": "Cong Wang", "authors": "Cong Wang, Witold Pedrycz, ZhiWu Li, MengChu Zhou", "title": "Kullback-Leibler Divergence-Based Fuzzy $C$-Means Clustering\n  Incorporating Morphological Reconstruction and Wavelet Frames for Image\n  Segmentation", "comments": "This paper has been withdrawn by the author due to a crucial\n  definition error of objective function", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although spatial information of images usually enhance the robustness of the\nFuzzy C-Means (FCM) algorithm, it greatly increases the computational costs for\nimage segmentation. To achieve a sound trade-off between the segmentation\nperformance and the speed of clustering, we come up with a Kullback-Leibler\n(KL) divergence-based FCM algorithm by incorporating a tight wavelet frame\ntransform and a morphological reconstruction operation. To enhance FCM's\nrobustness, an observed image is first filtered by using the morphological\nreconstruction. A tight wavelet frame system is employed to decompose the\nobserved and filtered images so as to form their feature sets. Considering\nthese feature sets as data of clustering, an modified FCM algorithm is\nproposed, which introduces a KL divergence term in the partition matrix into\nits objective function. The KL divergence term aims to make membership degrees\nof each image pixel closer to those of its neighbors, which brings that the\nmembership partition becomes more suitable and the parameter setting of FCM\nbecomes simplified. On the basis of the obtained partition matrix and\nprototypes, the segmented feature set is reconstructed by minimizing the\ninverse process of the modified objective function. To modify abnormal features\nproduced in the reconstruction process, each reconstructed feature is\nreassigned to the closest prototype. As a result, the segmentation accuracy of\nKL divergence-based FCM is further improved. What's more, the segmented image\nis reconstructed by using a tight wavelet frame reconstruction operation.\nFinally, supporting experiments coping with synthetic, medical and color images\nare reported. Experimental results exhibit that the proposed algorithm works\nwell and comes with better segmentation performance than other comparative\nalgorithms. Moreover, the proposed algorithm requires less time than most of\nthe FCM-related algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 05:19:10 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 02:58:29 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Wang", "Cong", ""], ["Pedrycz", "Witold", ""], ["Li", "ZhiWu", ""], ["Zhou", "MengChu", ""]]}, {"id": "2002.09536", "submitter": "Madhavan Seshadri", "authors": "Madhavan Seshadri, Malavika Srikanth and Mikhail Belov", "title": "Image to Language Understanding: Captioning approach", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting context from visual representations is of utmost importance in the\nadvancement of Computer Science. Representation of such a format in Natural\nLanguage has a huge variety of applications such as helping the visually\nimpaired etc. Such an approach is a combination of Computer Vision and Natural\nLanguage techniques which is a hard problem to solve. This project aims to\ncompare different approaches for solving the image captioning problem. In\nspecific, the focus was on comparing two different types of models:\nEncoder-Decoder approach and a Multi-model approach. In the encoder-decoder\napproach, inject and merge architectures were compared against a multi-modal\nimage captioning approach based primarily on object detection. These approaches\nhave been compared on the basis on state of the art sentence comparison metrics\nsuch as BLEU, GLEU, Meteor, and Rouge on a subset of the Google Conceptual\ncaptions dataset which contains 100k images. On the basis of this comparison,\nwe observed that the best model was the Inception injected encoder model. This\nbest approach has been deployed as a web-based system. On uploading an image,\nsuch a system will output the best caption associated with the image.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 20:15:33 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Seshadri", "Madhavan", ""], ["Srikanth", "Malavika", ""], ["Belov", "Mikhail", ""]]}, {"id": "2002.09554", "submitter": "Ziyuan Liu", "authors": "Ziyuan Liu, Dongheui Lee, Wolfgang Sepp", "title": "Particle Filter Based Monocular Human Tracking with a 3D Cardbox Model\n  and a Novel Deterministic Resampling Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of markerless human motion tracking is the high dimensionality\nof the search space. Thus, efficient exploration in the search space is of\ngreat significance. In this paper, a motion capturing algorithm is proposed for\nupper body motion tracking. The proposed system tracks human motion based on\nmonocular silhouette-matching, and it is built on the top of a hierarchical\nparticle filter, within which a novel deterministic resampling strategy (DRS)\nis applied. The proposed system is evaluated quantitatively with the ground\ntruth data measured by an inertial sensor system. In addition, we compare the\nDRS with the stratified resampling strategy (SRS). It is shown in experiments\nthat DRS outperforms SRS with the same amount of particles. Moreover, a new 3D\narticulated human upper body model with the name 3D cardbox model is created\nand is proven to work successfully for motion tracking. Experiments show that\nthe proposed system can robustly track upper body motion without\nself-occlusion. Motions towards the camera can also be well tracked.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 21:21:58 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Liu", "Ziyuan", ""], ["Lee", "Dongheui", ""], ["Sepp", "Wolfgang", ""]]}, {"id": "2002.09558", "submitter": "Jonathan Ventura", "authors": "Wesley Khademi, Sonia Rao, Clare Minnerath, Guy Hagen, and Jonathan\n  Ventura", "title": "Self-Supervised Poisson-Gaussian Denoising", "comments": "to appear in IEEE WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the blindspot model for self-supervised denoising to handle\nPoisson-Gaussian noise and introduce an improved training scheme that avoids\nhyperparameters and adapts the denoiser to the test data. Self-supervised\nmodels for denoising learn to denoise from only noisy data and do not require\ncorresponding clean images, which are difficult or impossible to acquire in\nsome application areas of interest such as low-light microscopy. We introduce a\nnew training strategy to handle Poisson-Gaussian noise which is the standard\nnoise model for microscope images. Our new strategy eliminates hyperparameters\nfrom the loss function, which is important in a self-supervised regime where no\nground truth data is available to guide hyperparameter tuning. We show how our\ndenoiser can be adapted to the test data to improve performance. Our\nevaluations on microscope image denoising benchmarks validate our approach.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 21:34:33 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 01:13:33 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Khademi", "Wesley", ""], ["Rao", "Sonia", ""], ["Minnerath", "Clare", ""], ["Hagen", "Guy", ""], ["Ventura", "Jonathan", ""]]}, {"id": "2002.09564", "submitter": "Shadab Khan", "authors": "Prateek Munjal, Nasir Hayat, Munawar Hayat, Jamshid Sourati, Shadab\n  Khan", "title": "Towards Robust and Reproducible Active Learning Using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning (AL) is a promising ML paradigm that has the potential to\nparse through large unlabeled data and help reduce annotation cost in domains\nwhere labeling entire data can be prohibitive. Recently proposed neural network\nbased AL methods use different heuristics to accomplish this goal. In this\nstudy, we show that recent AL methods offer a gain over random baseline under a\nbrittle combination of experimental conditions. We demonstrate that such\nmarginal gains vanish when experimental factors are changed, leading to\nreproducibility issues and suggesting that AL methods lack robustness. We also\nobserve that with a properly tuned model, which employs recently proposed\nregularization techniques, the performance significantly improves for all AL\nmethods including the random sampling baseline, and performance differences\namong the AL methods become negligible. Based on these observations, we suggest\na set of experiments that are critical to assess the true effectiveness of an\nAL method. To facilitate these experiments we also present an open source\ntoolkit. We believe our findings and recommendations will help advance\nreproducible research in robust AL using neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 22:01:47 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Munjal", "Prateek", ""], ["Hayat", "Nasir", ""], ["Hayat", "Munawar", ""], ["Sourati", "Jamshid", ""], ["Khan", "Shadab", ""]]}, {"id": "2002.09571", "submitter": "Nicholas Cheney", "authors": "Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O.\n  Stanley, Jeff Clune, Nick Cheney", "title": "Learning to Continually Learn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual lifelong learning requires an agent or model to learn many\nsequentially ordered tasks, building on previous knowledge without\ncatastrophically forgetting it. Much work has gone towards preventing the\ndefault tendency of machine learning models to catastrophically forget, yet\nvirtually all such work involves manually-designed solutions to the problem. We\ninstead advocate meta-learning a solution to catastrophic forgetting, allowing\nAI to learn to continually learn. Inspired by neuromodulatory processes in the\nbrain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It\ndifferentiates through a sequential learning process to meta-learn an\nactivation-gating function that enables context-dependent selective activation\nwithin a deep neural network. Specifically, a neuromodulatory (NM) neural\nnetwork gates the forward pass of another (otherwise normal) neural network\ncalled the prediction learning network (PLN). The NM network also thus\nindirectly controls selective plasticity (i.e. the backward pass of) the PLN.\nANML enables continual learning without catastrophic forgetting at scale: it\nproduces state-of-the-art continual learning performance, sequentially learning\nas many as 600 classes (over 9,000 SGD updates).\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 22:52:00 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 03:22:48 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Beaulieu", "Shawn", ""], ["Frati", "Lapo", ""], ["Miconi", "Thomas", ""], ["Lehman", "Joel", ""], ["Stanley", "Kenneth O.", ""], ["Clune", "Jeff", ""], ["Cheney", "Nick", ""]]}, {"id": "2002.09576", "submitter": "Scott Freitas", "authors": "Scott Freitas, Shang-Tse Chen, Zijie J. Wang, Duen Horng Chau", "title": "UnMask: Adversarial Detection and Defense Through Robust Feature\n  Alignment", "comments": "Accepted into IEEE Big Data 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are being integrated into a wide range of high-impact,\nsecurity-critical systems, from self-driving cars to medical diagnosis.\nHowever, recent research has demonstrated that many of these deep learning\narchitectures are vulnerable to adversarial attacks--highlighting the vital\nneed for defensive techniques to detect and mitigate these attacks before they\noccur. To combat these adversarial attacks, we developed UnMask, an adversarial\ndetection and defense framework based on robust feature alignment. The core\nidea behind UnMask is to protect these models by verifying that an image's\npredicted class (\"bird\") contains the expected robust features (e.g., beak,\nwings, eyes). For example, if an image is classified as \"bird\", but the\nextracted features are wheel, saddle and frame, the model may be under attack.\nUnMask detects such attacks and defends the model by rectifying the\nmisclassification, re-classifying the image based on its robust features. Our\nextensive evaluation shows that UnMask (1) detects up to 96.75% of attacks, and\n(2) defends the model by correctly classifying up to 93% of adversarial images\nproduced by the current strongest attack, Projected Gradient Descent, in the\ngray-box setting. UnMask provides significantly better protection than\nadversarial training across 8 attack vectors, averaging 31.18% higher accuracy.\nWe open source the code repository and data with this paper:\nhttps://github.com/safreita1/unmask.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 23:20:23 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 20:21:11 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Freitas", "Scott", ""], ["Chen", "Shang-Tse", ""], ["Wang", "Zijie J.", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2002.09600", "submitter": "Shousheng Luo", "authors": "Shousheng Luo and Xue-Cheng Tai and Yang Wang", "title": "Convex Shape Representation with Binary Labels for Image Segmentation:\n  Models and Fast Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel and effective binary representation for convex shapes. We\nshow the equivalence between the shape convexity and some properties of the\nassociated indicator function. The proposed method has two advantages. Firstly,\nthe representation is based on a simple inequality constraint on the binary\nfunction rather than the definition of convex shapes, which allows us to obtain\nefficient algorithms for various applications with convexity prior. Secondly,\nthis method is independent of the dimension of the concerned shape. In order to\nshow the effectiveness of the proposed representation approach, we incorporate\nit with a probability based model for object segmentation with convexity prior.\nEfficient algorithms are given to solve the proposed models using Lagrange\nmultiplier methods and linear approximations. Various experiments are given to\nshow the superiority of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 01:55:20 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Luo", "Shousheng", ""], ["Tai", "Xue-Cheng", ""], ["Wang", "Yang", ""]]}, {"id": "2002.09611", "submitter": "Kaixuan Wei", "authors": "Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu,\n  Carola-Bibiane Sch\\\"onlieb, Hua Huang", "title": "Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plug-and-play (PnP) is a non-convex framework that combines ADMM or other\nproximal algorithms with advanced denoiser priors. Recently, PnP has achieved\ngreat empirical success, especially with the integration of deep learning-based\ndenoisers. However, a key problem of PnP based approaches is that they require\nmanual parameter tweaking. It is necessary to obtain high-quality results\nacross the high discrepancy in terms of imaging conditions and varying scene\ncontent. In this work, we present a tuning-free PnP proximal algorithm, which\ncan automatically determine the internal parameters including the penalty\nparameter, the denoising strength and the terminal time. A key part of our\napproach is to develop a policy network for automatic search of parameters,\nwhich can be effectively learned via mixed model-free and model-based deep\nreinforcement learning. We demonstrate, through numerical and visual\nexperiments, that the learned policy can customize different parameters for\ndifferent states, and often more efficient and effective than existing\nhandcrafted criteria. Moreover, we discuss the practical considerations of the\nplugged denoisers, which together with our learned policy yield\nstate-of-the-art results. This is prevalent on both linear and nonlinear\nexemplary inverse imaging problems, and in particular, we show promising\nresults on Compressed Sensing MRI and phase retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 03:09:48 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 13:33:47 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Wei", "Kaixuan", ""], ["Aviles-Rivero", "Angelica", ""], ["Liang", "Jingwei", ""], ["Fu", "Ying", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Huang", "Hua", ""]]}, {"id": "2002.09625", "submitter": "Jiangpeng Yan PhD", "authors": "Jiangpeng Yan, Shuo Chen, Yongbing Zhang and Xiu Li", "title": "Neural Architecture Search for Compressed Sensing Magnetic Resonance\n  Image Reconstruction", "comments": "To be appear in Computerized Medical Imaging and Graphics", "journal-ref": null, "doi": "10.1016/j.compmedimag.2020.101784", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have demonstrated that deep learning (DL) based compressed\nsensing (CS) implementation can accelerate Magnetic Resonance (MR) Imaging by\nreconstructing MR images from sub-sampled k-space data. However, network\narchitectures adopted in previous methods are all designed by handcraft. Neural\nArchitecture Search (NAS) algorithms can automatically build neural network\narchitectures which have outperformed human designed ones in several vision\ntasks. Inspired by this, here we proposed a novel and efficient network for the\nMR image reconstruction problem via NAS instead of manual attempts.\nParticularly, a specific cell structure, which was integrated into the\nmodel-driven MR reconstruction pipeline, was automatically searched from a\nflexible pre-defined operation search space in a differentiable manner.\nExperimental results show that our searched network can produce better\nreconstruction results compared to previous state-of-the-art methods in terms\nof PSNR and SSIM with 4-6 times fewer computation resources. Extensive\nexperiments were conducted to analyze how hyper-parameters affect\nreconstruction performance and the searched structures. The generalizability of\nthe searched architecture was also evaluated on different organ MR datasets.\nOur proposed method can reach a better trade-off between computation cost and\nreconstruction performance for MR reconstruction problem with good\ngeneralizability and offer insights to design neural networks for other medical\nimage applications. The evaluation code will be available at\nhttps://github.com/yjump/NAS-for-CSMRI.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 04:40:16 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 03:41:07 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 03:25:05 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 15:01:01 GMT"}, {"version": "v5", "created": "Sun, 16 Aug 2020 23:38:17 GMT"}, {"version": "v6", "created": "Sun, 23 Aug 2020 11:35:24 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Yan", "Jiangpeng", ""], ["Chen", "Shuo", ""], ["Zhang", "Yongbing", ""], ["Li", "Xiu", ""]]}, {"id": "2002.09635", "submitter": "Michael Girard", "authors": "Sripad Krishna Devalla, Tan Hung Pham, Satish Kumar Panda, Liang\n  Zhang, Giridhar Subramanian, Anirudh Swaminathan, Chin Zhi Yun, Mohan Rajan,\n  Sujatha Mohan, Ramaswami Krishnadas, Vijayalakshmi Senthil, John Mark S. de\n  Leon, Tin A. Tun, Ching-Yu Cheng, Leopold Schmetterer, Shamira Perera, Tin\n  Aung, Alexandre H. Thiery, Micha\u007fel J. A. Girard", "title": "Towards Label-Free 3D Segmentation of Optical Coherence Tomography\n  Images of the Optic Nerve Head Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the introduction of optical coherence tomography (OCT), it has been\npossible to study the complex 3D morphological changes of the optic nerve head\n(ONH) tissues that occur along with the progression of glaucoma. Although\nseveral deep learning (DL) techniques have been recently proposed for the\nautomated extraction (segmentation) and quantification of these morphological\nchanges, the device specific nature and the difficulty in preparing manual\nsegmentations (training data) limit their clinical adoption. With several new\nmanufacturers and next-generation OCT devices entering the market, the\ncomplexity in deploying DL algorithms clinically is only increasing. To address\nthis, we propose a DL based 3D segmentation framework that is easily\ntranslatable across OCT devices in a label-free manner (i.e. without the need\nto manually re-segment data for each device). Specifically, we developed 2 sets\nof DL networks. The first (referred to as the enhancer) was able to enhance OCT\nimage quality from 3 OCT devices, and harmonized image-characteristics across\nthese devices. The second performed 3D segmentation of 6 important ONH tissue\nlayers. We found that the use of the enhancer was critical for our segmentation\nnetwork to achieve device independency. In other words, our 3D segmentation\nnetwork trained on any of 3 devices successfully segmented ONH tissue layers\nfrom the other two devices with high performance (Dice coefficients > 0.92).\nWith such an approach, we could automatically segment images from new OCT\ndevices without ever needing manual segmentation data from such devices.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 05:41:45 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Devalla", "Sripad Krishna", ""], ["Pham", "Tan Hung", ""], ["Panda", "Satish Kumar", ""], ["Zhang", "Liang", ""], ["Subramanian", "Giridhar", ""], ["Swaminathan", "Anirudh", ""], ["Yun", "Chin Zhi", ""], ["Rajan", "Mohan", ""], ["Mohan", "Sujatha", ""], ["Krishnadas", "Ramaswami", ""], ["Senthil", "Vijayalakshmi", ""], ["de Leon", "John Mark S.", ""], ["Tun", "Tin A.", ""], ["Cheng", "Ching-Yu", ""], ["Schmetterer", "Leopold", ""], ["Perera", "Shamira", ""], ["Aung", "Tin", ""], ["Thiery", "Alexandre H.", ""], ["Girard", "Micha\u007fel J. A.", ""]]}, {"id": "2002.09663", "submitter": "Qian Zhang", "authors": "Qian Zhang and Wei Feng and Liang Wan and Fei-Peng Tian and Xiaowei\n  Wang and Ping Tan", "title": "Active Lighting Recurrence by Parallel Lighting Analogy for Fine-Grained\n  Change Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a new problem, namely active lighting recurrence (ALR)\nthat physically relocalizes a light source to reproduce the lighting condition\nfrom single reference image for a same scene, which may suffer from\nfine-grained changes during twice observations. ALR is of great importance for\nfine-grained visual inspection and change detection, because some phenomena or\nminute changes can only be clearly observed under particular lighting\nconditions. Therefore, effective ALR should be able to online navigate a light\nsource toward the target pose, which is challenging due to the complexity and\ndiversity of real-world lighting and imaging processes. To this end, we propose\nto use the simple parallel lighting as an analogy model and based on Lambertian\nlaw to compose an instant navigation ball for this purpose. We theoretically\nprove the feasibility, i.e., equivalence and convergence, of this ALR approach\nfor realistic near point light source and small near surface light source.\nBesides, we also theoretically prove the invariance of our ALR approach to the\nambiguity of normal and lighting decomposition. The effectiveness and\nsuperiority of the proposed approach have been verified by both extensive\nquantitative experiments and challenging real-world tasks on fine-grained\nchange detection of cultural heritages. We also validate the generality of our\napproach to non-Lambertian scenes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 08:51:31 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Zhang", "Qian", ""], ["Feng", "Wei", ""], ["Wan", "Liang", ""], ["Tian", "Fei-Peng", ""], ["Wang", "Xiaowei", ""], ["Tan", "Ping", ""]]}, {"id": "2002.09674", "submitter": "Ziwen He", "authors": "Ziwen He, Wei Wang, Jing Dong and Tieniu Tan", "title": "Temporal Sparse Adversarial Attack on Sequence-based Gait Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition is widely used in social security applications due to its\nadvantages in long-distance human identification. Recently, sequence-based\nmethods have achieved high accuracy by learning abundant temporal and spatial\ninformation. However, their robustness under adversarial attacks has not been\nclearly explored. In this paper, we demonstrate that the state-of-the-art gait\nrecognition model is vulnerable to such attacks. To this end, we propose a\nnovel temporal sparse adversarial attack method. Different from previous\nadditive noise models which add perturbations on original samples, we employ a\ngenerative adversarial network based architecture to semantically generate\nadversarial high-quality gait silhouettes or video frames. Moreover, by\nsparsely substituting or inserting a few adversarial gait silhouettes, the\nproposed method ensures its imperceptibility and achieves a high attack success\nrate. The experimental results show that if only one-fortieth of the frames are\nattacked, the accuracy of the target model drops dramatically.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 10:08:42 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 03:00:38 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["He", "Ziwen", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Tan", "Tieniu", ""]]}, {"id": "2002.09703", "submitter": "Tiexin Qin", "authors": "Tiexin Qin and Ziyuan Wang and Kelei He and Yinghuan Shi and Yang Gao\n  and Dinggang Shen", "title": "Automatic Data Augmentation via Deep Reinforcement Learning for\n  Effective Kidney Tumor Segmentation", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional data augmentation realized by performing simple pre-processing\noperations (\\eg, rotation, crop, \\etc) has been validated for its advantage in\nenhancing the performance for medical image segmentation. However, the data\ngenerated by these conventional augmentation methods are random and sometimes\nharmful to the subsequent segmentation. In this paper, we developed a novel\nautomatic learning-based data augmentation method for medical image\nsegmentation which models the augmentation task as a trial-and-error procedure\nusing deep reinforcement learning (DRL). In our method, we innovatively combine\nthe data augmentation module and the subsequent segmentation module in an\nend-to-end training manner with a consistent loss. Specifically, the best\nsequential combination of different basic operations is automatically learned\nby directly maximizing the performance improvement (\\ie, Dice ratio) on the\navailable validation set. We extensively evaluated our method on CT kidney\ntumor segmentation which validated the promising results of our method.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 14:10:13 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Qin", "Tiexin", ""], ["Wang", "Ziyuan", ""], ["He", "Kelei", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""], ["Shen", "Dinggang", ""]]}, {"id": "2002.09708", "submitter": "Cheng Chen", "authors": "Cheng Chen, Qi Dou, Yueming Jin, Hao Chen, Jing Qin, Pheng-Ann Heng", "title": "Robust Multimodal Brain Tumor Segmentation via Feature Disentanglement\n  and Gated Fusion", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate medical image segmentation commonly requires effective learning of\nthe complementary information from multimodal data. However, in clinical\npractice, we often encounter the problem of missing imaging modalities. We\ntackle this challenge and propose a novel multimodal segmentation framework\nwhich is robust to the absence of imaging modalities. Our network uses feature\ndisentanglement to decompose the input modalities into the modality-specific\nappearance code, which uniquely sticks to each modality, and the\nmodality-invariant content code, which absorbs multimodal information for the\nsegmentation task. With enhanced modality-invariance, the disentangled content\ncode from each modality is fused into a shared representation which gains\nrobustness to missing data. The fusion is achieved via a learning-based\nstrategy to gate the contribution of different modalities at different\nlocations. We validate our method on the important yet challenging multimodal\nbrain tumor segmentation task with the BRATS challenge dataset. With\ncompetitive performance to the state-of-the-art approaches for full modality,\nour method achieves outstanding robustness under various missing modality(ies)\nsituations, significantly exceeding the state-of-the-art method by over 16% in\naverage for Dice on whole tumor segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 14:32:04 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Chen", "Cheng", ""], ["Dou", "Qi", ""], ["Jin", "Yueming", ""], ["Chen", "Hao", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2002.09786", "submitter": "Abdulrahman Mahmoud", "authors": "Abdulrahman Mahmoud, Siva Kumar Sastry Hari, Christopher W. Fletcher,\n  Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B.\n  Sullivan, Timothy Tsai, Stephen W. Keckler", "title": "HarDNN: Feature Map Vulnerability Evaluation in CNNs", "comments": "14 pages, 5 figures, a short version accepted for publication in\n  First Workshop on Secure and Resilient Autonomy (SARA) co-located with\n  MLSys2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Convolutional Neural Networks (CNNs) are increasingly being employed in\nsafety-critical applications, it is important that they behave reliably in the\nface of hardware errors. Transient hardware errors may percolate undesirable\nstate during execution, resulting in software-manifested errors which can\nadversely affect high-level decision making. This paper presents HarDNN, a\nsoftware-directed approach to identify vulnerable computations during a CNN\ninference and selectively protect them based on their propensity towards\ncorrupting the inference output in the presence of a hardware error. We show\nthat HarDNN can accurately estimate relative vulnerability of a feature map\n(fmap) in CNNs using a statistical error injection campaign, and explore\nheuristics for fast vulnerability assessment. Based on these results, we\nanalyze the tradeoff between error coverage and computational overhead that the\nsystem designers can use to employ selective protection. Results show that the\nimprovement in resilience for the added computation is superlinear with HarDNN.\nFor example, HarDNN improves SqueezeNet's resilience by 10x with just 30%\nadditional computations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 23:05:03 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 11:07:36 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Mahmoud", "Abdulrahman", ""], ["Hari", "Siva Kumar Sastry", ""], ["Fletcher", "Christopher W.", ""], ["Adve", "Sarita V.", ""], ["Sakr", "Charbel", ""], ["Shanbhag", "Naresh", ""], ["Molchanov", "Pavlo", ""], ["Sullivan", "Michael B.", ""], ["Tsai", "Timothy", ""], ["Keckler", "Stephen W.", ""]]}, {"id": "2002.09790", "submitter": "Yinyu Nie", "authors": "Yinyu Nie, Shihui Guo, Jian Chang, Xiaoguang Han, Jiahui Huang,\n  Shi-Min Hu, Jian Jun Zhang", "title": "Shallow2Deep: Indoor Scene Modeling by Single Image Understanding", "comments": "Accepted by Pattern Recognition", "journal-ref": "Pattern Recognition. 2020 Feb 12:107271", "doi": "10.1016/j.patcog.2020.107271", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense indoor scene modeling from 2D images has been bottlenecked due to the\nabsence of depth information and cluttered occlusions. We present an automatic\nindoor scene modeling approach using deep features from neural networks. Given\na single RGB image, our method simultaneously recovers semantic contents, 3D\ngeometry and object relationship by reasoning indoor environment context.\nParticularly, we design a shallow-to-deep architecture on the basis of\nconvolutional networks for semantic scene understanding and modeling. It\ninvolves multi-level convolutional networks to parse indoor semantics/geometry\ninto non-relational and relational knowledge. Non-relational knowledge\nextracted from shallow-end networks (e.g. room layout, object geometry) is fed\nforward into deeper levels to parse relational semantics (e.g. support\nrelationship). A Relation Network is proposed to infer the support relationship\nbetween objects. All the structured semantics and geometry above are assembled\nto guide a global optimization for 3D scene modeling. Qualitative and\nquantitative analysis demonstrates the feasibility of our method in\nunderstanding and modeling semantics-enriched indoor scenes by evaluating the\nperformance of reconstruction accuracy, computation performance and scene\ncomplexity.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 23:27:22 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Nie", "Yinyu", ""], ["Guo", "Shihui", ""], ["Chang", "Jian", ""], ["Han", "Xiaoguang", ""], ["Huang", "Jiahui", ""], ["Hu", "Shi-Min", ""], ["Zhang", "Jian Jun", ""]]}, {"id": "2002.09792", "submitter": "Yiannis Kantaros", "authors": "Yiannis Kantaros, Taylor Carpenter, Sangdon Park, Radoslav Ivanov,\n  Sooyong Jang, Insup Lee, James Weimer", "title": "VisionGuard: Runtime Detection of Adversarial Inputs to Perception\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) models have proven to be vulnerable to adversarial\nattacks. In this paper, we propose VisionGuard, a novel attack- and\ndataset-agnostic and computationally-light defense mechanism for adversarial\ninputs to DNN-based perception systems. In particular, VisionGuard relies on\nthe observation that adversarial images are sensitive to lossy compression\ntransformations. Specifically, to determine if an image is adversarial,\nVisionGuard checks if the output of the target classifier on a given input\nimage changes significantly after feeding it a transformed version of the image\nunder investigation. Moreover, we show that VisionGuard is\ncomputationally-light both at runtime and design-time which makes it suitable\nfor real-time applications that may also involve large-scale image domains. To\nhighlight this, we demonstrate the efficiency of VisionGuard on ImageNet, a\ntask that is computationally challenging for the majority of relevant defenses.\nFinally, we include extensive comparative experiments on the MNIST, CIFAR10,\nand ImageNet datasets that show that VisionGuard outperforms existing defenses\nin terms of scalability and detection performance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 00:03:57 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Kantaros", "Yiannis", ""], ["Carpenter", "Taylor", ""], ["Park", "Sangdon", ""], ["Ivanov", "Radoslav", ""], ["Jang", "Sooyong", ""], ["Lee", "Insup", ""], ["Weimer", "James", ""]]}, {"id": "2002.09797", "submitter": "Seong Joon Oh", "authors": "Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi,\n  Jaejun Yoo", "title": "Reliable Fidelity and Diversity Metrics for Generative Models", "comments": "First two authors have contributed equally; ICML 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Devising indicative evaluation metrics for the image generation task remains\nan open problem. The most widely used metric for measuring the similarity\nbetween real and generated images has been the Fr\\'echet Inception Distance\n(FID) score. Because it does not differentiate the fidelity and diversity\naspects of the generated images, recent papers have introduced variants of\nprecision and recall metrics to diagnose those properties separately. In this\npaper, we show that even the latest version of the precision and recall metrics\nare not reliable yet. For example, they fail to detect the match between two\nidentical distributions, they are not robust against outliers, and the\nevaluation hyperparameters are selected arbitrarily. We propose density and\ncoverage metrics that solve the above issues. We analytically and\nexperimentally show that density and coverage provide more interpretable and\nreliable signals for practitioners than the existing metrics. Code:\nhttps://github.com/clovaai/generative-evaluation-prdc.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 00:50:01 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 20:37:50 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Naeem", "Muhammad Ferjad", ""], ["Oh", "Seong Joon", ""], ["Uh", "Youngjung", ""], ["Choi", "Yunjey", ""], ["Yoo", "Jaejun", ""]]}, {"id": "2002.09809", "submitter": "Darvin Yi", "authors": "Darvin Yi, Endre Gr{\\o}vik, Michael Iv, Elizabeth Tong, Greg\n  Zaharchuk, Daniel Rubin", "title": "Random Bundle: Brain Metastases Segmentation Ensembling through\n  Annotation Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel ensembling method, Random Bundle (RB), that improves\nperformance for brain metastases segmentation. We create our ensemble by\ntraining each network on our dataset with 50% of our annotated lesions censored\nout. We also apply a lopsided bootstrap loss to recover performance after\ninducing an in silico 50% false negative rate and make our networks more\nsensitive. We improve our network detection of lesions's mAP value by 39% and\nmore than triple the sensitivity at 80% precision. We also show slight\nimprovements in segmentation quality through DICE score. Further, RB ensembling\nimproves performance over baseline by a larger margin than a variety of popular\nensembling strategies. Finally, we show that RB ensembling is computationally\nefficient by comparing its performance to a single network when both systems\nare constrained to have the same compute.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 02:07:01 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:56:03 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Yi", "Darvin", ""], ["Gr\u00f8vik", "Endre", ""], ["Iv", "Michael", ""], ["Tong", "Elizabeth", ""], ["Zaharchuk", "Greg", ""], ["Rubin", "Daniel", ""]]}, {"id": "2002.09815", "submitter": "Amirata Ghorbani", "authors": "Amirata Ghorbani and James Zou", "title": "Neuron Shapley: Discovering the Responsible Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Neuron Shapley as a new framework to quantify the contribution of\nindividual neurons to the prediction and performance of a deep network. By\naccounting for interactions across neurons, Neuron Shapley is more effective in\nidentifying important filters compared to common approaches based on activation\npatterns. Interestingly, removing just 30 filters with the highest Shapley\nscores effectively destroys the prediction accuracy of Inception-v3 on\nImageNet. Visualization of these few critical filters provides insights into\nhow the network functions. Neuron Shapley is a flexible framework and can be\napplied to identify responsible neurons in many tasks. We illustrate additional\napplications of identifying filters that are responsible for biased prediction\nin facial recognition and filters that are vulnerable to adversarial attacks.\nRemoving these filters is a quick way to repair models. Enabling all these\napplications is a new multi-arm bandit algorithm that we developed to\nefficiently estimate Neuron Shapley values.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 03:29:58 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 06:57:29 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 22:06:48 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ghorbani", "Amirata", ""], ["Zou", "James", ""]]}, {"id": "2002.09818", "submitter": "Burkay Donderici", "authors": "Burkay Donderici, Caleb New, Chenliang Xu", "title": "Assembling Semantically-Disentangled Representations for\n  Predictive-Generative Models via Adaptation from Synthetic Domain", "comments": "8 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks can form high-level hierarchical representations of\ninput data. Various researchers have demonstrated that these representations\ncan be used to enable a variety of useful applications. However, such\nrepresentations are typically based on the statistics within the data, and may\nnot conform with the semantic representation that may be necessitated by the\napplication. Conditional models are typically used to overcome this challenge,\nbut they require large annotated datasets which are difficult to come by and\ncostly to create. In this paper, we show that semantically-aligned\nrepresentations can be generated instead with the help of a physics based\nengine. This is accomplished by creating a synthetic dataset with decoupled\nattributes, learning an encoder for the synthetic dataset, and augmenting\nprescribed attributes from the synthetic domain with attributes from the real\ndomain. It is shown that the proposed (SYNTH-VAE-GAN) method can construct a\nconditional predictive-generative model of human face attributes without\nrelying on real data labels.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 03:35:45 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Donderici", "Burkay", ""], ["New", "Caleb", ""], ["Xu", "Chenliang", ""]]}, {"id": "2002.09843", "submitter": "Yan Feng", "authors": "Xue Yang, Yan Feng, Weijun Fang, Jun Shao, Xiaohu Tang, Shu-Tao Xia,\n  Rongxing Lu", "title": "Computation-efficient Deep Model Training for Ciphertext-based\n  Cross-silo Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although cross-silo federated learning improves privacy of training data by\nexchanging model updates rather than raw data, sharing updates (e.g., local\ngradients or parameters) may still involve risks. To ensure no updates are\nrevealed to the server, industrial FL schemes allow clients (e.g., financial or\nmedical) to mask local gradients by homomorphic encryption (HE). In this case,\nthe server cannot obtain the updates, but the curious clients can obtain this\ninformation to infer other clients' private data. To alleviate this situation,\nthe most direct idea is to let clients train deep models on encrypted domain.\nUnfortunately, the resulting solution is of poor accuracy and high cost, since\nthe existing advanced HE is incompatible with non-linear activation functions\nand inefficient in terms of computational cost. In this paper, we propose a\n\\emph{computational-efficient deep model training scheme for ciphertext-based\ncross-silo federated learning} to comprehensively guarantee privacy. First, we\ncustomize \\emph{a novel one-time-pad-style model encryption method} to directly\nsupports non-linear activation functions and decimal arithmetic operations on\nthe encrypted domain. Then, we design a hybrid privacy-preserving scheme by\ncombining our model encryption method with secret sharing techniques to keep\nupdates secret from the clients and prevent the server from obtaining local\ngradients of each client. Extensive experiments demonstrate that for both\nregression and classification tasks, our scheme achieves the same accuracy as\nnon-private approaches and outperforms the state-of-the-art HE-based scheme.\nBesides, training time of our scheme is almost the same as non-private\napproaches and much more efficient than HE-based schemes. Our scheme trains a\n$9$-layer neural network on the MNIST dataset in less than one hour.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 06:50:20 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 02:24:04 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 15:10:37 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 08:35:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yang", "Xue", ""], ["Feng", "Yan", ""], ["Fang", "Weijun", ""], ["Shao", "Jun", ""], ["Tang", "Xiaohu", ""], ["Xia", "Shu-Tao", ""], ["Lu", "Rongxing", ""]]}, {"id": "2002.09847", "submitter": "Jong Chul Ye", "authors": "Joonyoung Song, Jae-Heon Jeong, Dae-Soon Park, Hyun-Ho Kim, Doo-Chun\n  Seo, Jong Chul Ye", "title": "Unsupervised Denoising for Satellite Imagery using Wavelet Subband\n  CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-spectral satellite imaging sensors acquire various spectral band images\nsuch as red (R), green (G), blue (B), near-infrared (N), etc. Thanks to the\nunique spectroscopic property of each spectral band with respective to the\nobjects on the ground, multi-spectral satellite imagery can be used for various\ngeological survey applications. Unfortunately, image artifacts from imaging\nsensor noises often affect the quality of scenes and have negative impacts on\nthe applications of satellite imagery. Recently, deep learning approaches have\nbeen extensively explored for the removal of noises in satellite imagery. Most\ndeep learning denoising methods, however, follow a supervised learning scheme,\nwhich requires matched noisy image and clean image pairs that are difficult to\ncollect in real situations. In this paper, we propose a novel unsupervised\nmultispectral denoising method for satellite imagery using wavelet subband\ncycle-consistent adversarial network (WavCycleGAN). The proposed method is\nbased on unsupervised learning scheme using adversarial loss and\ncycle-consistency loss to overcome the lack of paired data. Moreover, in\ncontrast to the standard image domain cycleGAN, we introduce a wavelet subband\ndomain learning scheme for effective denoising without sacrificing high\nfrequency components such as edges and detail information. Experimental results\nfor the removal of vertical stripe and wave noises in satellite imaging sensors\ndemonstrate that the proposed method effectively removes noises and preserves\nimportant high frequency features of satellite images.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 07:11:05 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Song", "Joonyoung", ""], ["Jeong", "Jae-Heon", ""], ["Park", "Dae-Soon", ""], ["Kim", "Hyun-Ho", ""], ["Seo", "Doo-Chun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2002.09859", "submitter": "Hao-Chiang Shao", "authors": "Hao-Chiang Shao, Kang-Yu Liu, Chia-Wen Lin, Jiwen Lu", "title": "DotFAN: A Domain-transferred Face Augmentation Network for Pose and\n  Illumination Invariant Face Recognition", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of a convolutional neural network (CNN) based face\nrecognition model largely relies on the richness of labelled training data.\nCollecting a training set with large variations of a face identity under\ndifferent poses and illumination changes, however, is very expensive, making\nthe diversity of within-class face images a critical issue in practice. In this\npaper, we propose a 3D model-assisted domain-transferred face augmentation\nnetwork (DotFAN) that can generate a series of variants of an input face based\non the knowledge distilled from existing rich face datasets collected from\nother domains. DotFAN is structurally a conditional CycleGAN but has two\nadditional subnetworks, namely face expert network (FEM) and face shape\nregressor (FSR), for latent code control. While FSR aims to extract face\nattributes, FEM is designed to capture a face identity. With their aid, DotFAN\ncan learn a disentangled face representation and effectively generate face\nimages of various facial attributes while preserving the identity of augmented\nfaces. Experiments show that DotFAN is beneficial for augmenting small face\ndatasets to improve their within-class diversity so that a better face\nrecognition model can be learned from the augmented dataset.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 08:16:34 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Shao", "Hao-Chiang", ""], ["Liu", "Kang-Yu", ""], ["Lin", "Chia-Wen", ""], ["Lu", "Jiwen", ""]]}, {"id": "2002.09905", "submitter": "Jin Beibei", "authors": "Beibei Jin, Yu Hu, Qiankun Tang, Jingyu Niu, Zhiping Shi, Yinhe Han,\n  Xiaowei Li", "title": "Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity\n  and Temporal-Consistency Video Prediction", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction is a pixel-wise dense prediction task to infer future frames\nbased on past frames. Missing appearance details and motion blur are still two\nmajor problems for current predictive models, which lead to image distortion\nand temporal inconsistency. In this paper, we point out the necessity of\nexploring multi-frequency analysis to deal with the two problems. Inspired by\nthe frequency band decomposition characteristic of Human Vision System (HVS),\nwe propose a video prediction network based on multi-level wavelet analysis to\ndeal with spatial and temporal information in a unified manner. Specifically,\nthe multi-level spatial discrete wavelet transform decomposes each video frame\ninto anisotropic sub-bands with multiple frequencies, helping to enrich\nstructural information and reserve fine details. On the other hand, multi-level\ntemporal discrete wavelet transform which operates on time axis decomposes the\nframe sequence into sub-band groups of different frequencies to accurately\ncapture multi-frequency motions under a fixed frame rate. Extensive experiments\non diverse datasets demonstrate that our model shows significant improvements\non fidelity and temporal consistency over state-of-the-art works.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 13:46:29 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 14:46:22 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Jin", "Beibei", ""], ["Hu", "Yu", ""], ["Tang", "Qiankun", ""], ["Niu", "Jingyu", ""], ["Shi", "Zhiping", ""], ["Han", "Yinhe", ""], ["Li", "Xiaowei", ""]]}, {"id": "2002.09923", "submitter": "Haoyang Ye", "authors": "Haoyang Ye, Huaiyang Huang and Ming Liu", "title": "Monocular Direct Sparse Localization in a Prior 3D Surfel Map", "comments": "7 pages, 6 figures, to appear in ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an approach to tracking the pose of a monocular\ncamera in a prior surfel map. By rendering vertex and normal maps from the\nprior surfel map, the global planar information for the sparse tracked points\nin the image frame is obtained. The tracked points with and without the global\nplanar information involve both global and local constraints of frames to the\nsystem. Our approach formulates all constraints in the form of direct\nphotometric errors within a local window of the frames. The final optimization\nutilizes these constraints to provide the accurate estimation of global 6-DoF\ncamera poses with the absolute scale. The extensive simulation and real-world\nexperiments demonstrate that our monocular method can provide accurate camera\nlocalization results under various conditions.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 15:29:38 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Ye", "Haoyang", ""], ["Huang", "Huaiyang", ""], ["Liu", "Ming", ""]]}, {"id": "2002.09951", "submitter": "Rodolfo Quispe", "authors": "Rodolfo Quispe, Darwin Ttito, Ad\\'in Ram\\'irez Rivera, Helio Pedrini", "title": "Multi-Stream Networks and Ground-Truth Generation for Crowd Counting", "comments": "https://github.com/RQuispeC/multi-stream-crowd-counting-extended ,\n  The International Journal of Electrical and Computer Engineering Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd scene analysis has received a lot of attention recently due to the wide\nvariety of applications, for instance, forensic science, urban planning,\nsurveillance and security. In this context, a challenging task is known as\ncrowd counting, whose main purpose is to estimate the number of people present\nin a single image. A Multi-Stream Convolutional Neural Network is developed and\nevaluated in this work, which receives an image as input and produces a density\nmap that represents the spatial distribution of people in an end-to-end\nfashion. In order to address complex crowd counting issues, such as extremely\nunconstrained scale and perspective changes, the network architecture utilizes\nreceptive fields with different size filters for each stream. In addition, we\ninvestigate the influence of the two most common fashions on the generation of\nground truths and propose a hybrid method based on tiny face detection and\nscale interpolation. Experiments conducted on two challenging datasets,\nUCF-CC-50 and ShanghaiTech, demonstrate that using our ground truth generation\nmethods achieves superior results.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 17:31:48 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 01:17:40 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 20:47:00 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Quispe", "Rodolfo", ""], ["Ttito", "Darwin", ""], ["Rivera", "Ad\u00edn Ram\u00edrez", ""], ["Pedrini", "Helio", ""]]}, {"id": "2002.09958", "submitter": "Sai Aparna Aketi", "authors": "Sai Aparna Aketi, Sourjya Roy, Anand Raghunathan, Kaushik Roy", "title": "Gradual Channel Pruning while Training using Feature Relevance Scores\n  for Convolutional Neural Networks", "comments": "15 pages, 2 figures, 4 tables", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3024992", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The enormous inference cost of deep neural networks can be scaled down by\nnetwork compression. Pruning is one of the predominant approaches used for deep\nnetwork compression. However, existing pruning techniques have one or more of\nthe following limitations: 1) Additional energy cost on top of the compute\nheavy training stage due to pruning and fine-tuning stages, 2) Layer-wise\npruning based on the statistics of a particular, ignoring the effect of error\npropagation in the network, 3) Lack of an efficient estimate for determining\nthe important channels globally, 4) Unstructured pruning requires specialized\nhardware for effective use. To address all the above issues, we present a\nsimple-yet-effective gradual channel pruning while training methodology using a\nnovel data-driven metric referred to as feature relevance score. The proposed\ntechnique gets rid of the additional retraining cycles by pruning the least\nimportant channels in a structured fashion at fixed intervals during the actual\ntraining phase. Feature relevance scores help in efficiently evaluating the\ncontribution of each channel towards the discriminative power of the network.\nWe demonstrate the effectiveness of the proposed methodology on architectures\nsuch as VGG and ResNet using datasets such as CIFAR-10, CIFAR-100 and ImageNet,\nand successfully achieve significant model compression while trading off less\nthan $1\\%$ accuracy. Notably on CIFAR-10 dataset trained on ResNet-110, our\napproach achieves $2.4\\times$ compression and a $56\\%$ reduction in FLOPs with\nan accuracy drop of $0.01\\%$ compared to the unpruned network.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 17:56:18 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 15:01:47 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Aketi", "Sai Aparna", ""], ["Roy", "Sourjya", ""], ["Raghunathan", "Anand", ""], ["Roy", "Kaushik", ""]]}, {"id": "2002.10003", "submitter": "Maximilian Seitzer", "authors": "Maximilian Seitzer", "title": "NeurIPS 2019 Disentanglement Challenge: Improved Disentanglement through\n  Aggregated Convolutional Feature Maps", "comments": "Disentanglement Challenge - 33rd Conference on Neural Information\n  Processing Systems (NeurIPS) - NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report to our stage 1 submission to the NeurIPS 2019 disentanglement\nchallenge presents a simple image preprocessing method for training VAEs\nleading to improved disentanglement compared to directly using the images. In\nparticular, we propose to use regionally aggregated feature maps extracted from\nCNNs pretrained on ImageNet. Our method achieved the 2nd place in stage 1 of\nthe challenge. Code is available at\nhttps://github.com/mseitzer/neurips2019-disentanglement-challenge.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 22:35:59 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Seitzer", "Maximilian", ""]]}, {"id": "2002.10016", "submitter": "Hadi Abdi Khojasteh", "authors": "Hadi Abdi Khojasteh (1), Ebrahim Ansari (1 and 2), Parvin Razzaghi (1\n  and 3), Akbar Karimi (4) ((1) Institute for Advanced Studies in Basic\n  Sciences (IASBS), Zanjan, Iran, (2) Faculty of Mathematics and Physics,\n  Institute of Formal and Applied Linguistics, Charles University, Czechia, (3)\n  Institute for Research in Fundamental Sciences (IPM), Tehran, Iran, (4) IMP\n  Lab, Department of Engineering and Architecture, University of Parma, Parma,\n  Italy)", "title": "Deep Multimodal Image-Text Embeddings for Automatic Cross-Media\n  Retrieval", "comments": "6 pages and 2 figures, Learn more about this project at\n  https://iasbs.ac.ir/~ansari/deeptwitter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the task of matching images and sentences by learning a\nvisual-textual embedding space for cross-modal retrieval. Finding such a space\nis a challenging task since the features and representations of text and image\nare not comparable. In this work, we introduce an end-to-end deep multimodal\nconvolutional-recurrent network for learning both vision and language\nrepresentations simultaneously to infer image-text similarity. The model learns\nwhich pairs are a match (positive) and which ones are a mismatch (negative)\nusing a hinge-based triplet ranking. To learn about the joint representations,\nwe leverage our newly extracted collection of tweets from Twitter. The main\ncharacteristic of our dataset is that the images and tweets are not\nstandardized the same as the benchmarks. Furthermore, there can be a higher\nsemantic correlation between the pictures and tweets contrary to benchmarks in\nwhich the descriptions are well-organized. Experimental results on MS-COCO\nbenchmark dataset show that our model outperforms certain methods presented\npreviously and has competitive performance compared to the state-of-the-art.\nThe code and dataset have been made available publicly.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 23:58:04 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Khojasteh", "Hadi Abdi", "", "1 and 2"], ["Ansari", "Ebrahim", "", "1 and 2"], ["Razzaghi", "Parvin", "", "1\n  and 3"], ["Karimi", "Akbar", ""]]}, {"id": "2002.10025", "submitter": "Ting-Kueu Hu", "authors": "Ting-Kuei Hu, Tianlong Chen, Haotao Wang, Zhangyang Wang", "title": "Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by\n  Enabling Input-Adaptive Inference", "comments": "Published on ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks were recently suggested to face the odds between accuracy (on\nclean natural images) and robustness (on adversarially perturbed images)\n(Tsipras et al., 2019). Such a dilemma is shown to be rooted in the inherently\nhigher sample complexity (Schmidt et al., 2018) and/or model capacity\n(Nakkiran, 2019), for learning a high-accuracy and robust classifier. In view\nof that, give a classification task, growing the model capacity appears to help\ndraw a win-win between accuracy and robustness, yet at the expense of model\nsize and latency, therefore posing challenges for resource-constrained\napplications. Is it possible to co-design model accuracy, robustness and\nefficiency to achieve their triple wins? This paper studies multi-exit networks\nassociated with input-adaptive efficient inference, showing their strong\npromise in achieving a \"sweet point\" in cooptimizing model accuracy, robustness\nand efficiency. Our proposed solution, dubbed Robust Dynamic Inference Networks\n(RDI-Nets), allows for each input (either clean or adversarial) to adaptively\nchoose one of the multiple output layers (early branches or the final one) to\noutput its prediction. That multi-loss adaptivity adds new variations and\nflexibility to adversarial attacks and defenses, on which we present a\nsystematical investigation. We show experimentally that by equipping existing\nbackbones with such robust adaptive inference, the resulting RDI-Nets can\nachieve better accuracy and robustness, yet with over 30% computational\nsavings, compared to the defended original models.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 00:40:22 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 03:27:42 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Hu", "Ting-Kuei", ""], ["Chen", "Tianlong", ""], ["Wang", "Haotao", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2002.10032", "submitter": "Mohammad Akbari", "authors": "Mohammad Akbari and Jie Liang and Jingning Han and Chengjie Tu", "title": "Generalized Octave Convolutions for Learned Multi-Frequency Image\n  Compression", "comments": "13 pages, 10 figures, 5 tables; Extended version of the paper\n  accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned image compression has recently shown the potential to outperform the\nstandard codecs. State-of-the-art rate-distortion (R-D) performance has been\nachieved by context-adaptive entropy coding approaches in which hyperprior and\nautoregressive models are jointly utilized to effectively capture the spatial\ndependencies in the latent representations. However, the latents are feature\nmaps of the same spatial resolution in previous works, which contain some\nredundancies that affect the R-D performance. In this paper, we propose the\nfirst learned multi-frequency image compression and entropy coding approach\nthat is based on the recently developed octave convolutions to factorize the\nlatents into high and low frequency (resolution) components, where the low\nfrequency is represented by a lower resolution. Therefore, its spatial\nredundancy is reduced, which improves the R-D performance. Novel generalized\noctave convolution and octave transposed-convolution architectures with\ninternal activation layers are also proposed to preserve more spatial structure\nof the information. Experimental results show that the proposed scheme not only\noutperforms all existing learned methods as well as standard codecs such as the\nnext-generation video coding standard VVC (4:2:0) on the Kodak dataset in both\nPSNR and MS-SSIM. We also show that the proposed generalized octave convolution\ncan improve the performance of other auto-encoder-based computer vision tasks\nsuch as semantic segmentation and image denoising.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 01:35:29 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 18:22:26 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 06:34:00 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Akbari", "Mohammad", ""], ["Liang", "Jie", ""], ["Han", "Jingning", ""], ["Tu", "Chengjie", ""]]}, {"id": "2002.10084", "submitter": "Matthew Roos", "authors": "Matthew J. Roos", "title": "Utilizing a null class to restrict decision spaces and defend against\n  neural network adversarial attacks", "comments": "15 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress, deep neural networks generally continue to be\nvulnerable to so-called adversarial examples--input images with small\nperturbations that can result in changes in the output classifications, despite\nno such change in the semantic meaning to human viewers. This is true even for\nseemingly simple challenges such as the MNIST digit classification task. In\npart, this suggests that these networks are not relying on the same set of\nobject features as humans use to make these classifications. In this paper we\nexamine an additional, and largely unexplored, cause behind this\nphenomenon--namely, the use of the conventional training paradigm in which the\nentire input space is parcellated among the training classes. Owing to this\nparadigm, learned decision spaces for individual classes span excessively large\nregions of the input space and include images that have no semantic similarity\nto images in the training set. In this study, we train models that include a\nnull class. That is, models may \"opt-out\" of classifying an input image as one\nof the digit classes. During training, null images are created through a\nvariety of methods, in an attempt to create tighter and more semantically\nmeaningful decision spaces for the digit classes. The best performing models\nclassify nearly all adversarial examples as nulls, rather than mistaking them\nas a member of an incorrect digit class, while simultaneously maintaining high\naccuracy on the unperturbed test set. The use of a null class and the training\nparadigm presented herein may provide an effective defense against adversarial\nattacks for some applications. Code for replicating this study will be made\navailable at https://github.com/mattroos/null_class_adversarial_defense .\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 05:47:08 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Roos", "Matthew J.", ""]]}, {"id": "2002.10099", "submitter": "Amos Gropp", "authors": "Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, Yaron Lipman", "title": "Implicit Geometric Regularization for Learning Shapes", "comments": "37th International Conference on Machine Learning, Vienna, Austria,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing shapes as level sets of neural networks has been recently proved\nto be useful for different shape analysis and reconstruction tasks. So far,\nsuch representations were computed using either: (i) pre-computed implicit\nshape representations; or (ii) loss functions explicitly defined over the\nneural level sets. In this paper we offer a new paradigm for computing high\nfidelity implicit neural representations directly from raw data (i.e., point\nclouds, with or without normal information). We observe that a rather simple\nloss function, encouraging the neural network to vanish on the input point\ncloud and to have a unit norm gradient, possesses an implicit geometric\nregularization property that favors smooth and natural zero level set surfaces,\navoiding bad zero-loss solutions. We provide a theoretical analysis of this\nproperty for the linear case, and show that, in practice, our method leads to\nstate of the art implicit neural representations with higher level-of-details\nand fidelity compared to previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 07:36:32 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 12:32:45 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Gropp", "Amos", ""], ["Yariv", "Lior", ""], ["Haim", "Niv", ""], ["Atzmon", "Matan", ""], ["Lipman", "Yaron", ""]]}, {"id": "2002.10100", "submitter": "Quan Huu Cap", "authors": "Quan Huu Cap, Hiroyuki Uga, Satoshi Kagiwada, and Hitoshi Iyatomi", "title": "LeafGAN: An Effective Data Augmentation Method for Practical Plant\n  Disease Diagnosis", "comments": "Accepted as a regular paper in the IEEE Transactions on Automation\n  Science and Engineering (T-ASE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications for the automated diagnosis of plant disease have been\ndeveloped based on the success of deep learning techniques. However, these\napplications often suffer from overfitting, and the diagnostic performance is\ndrastically decreased when used on test datasets from new environments. In this\npaper, we propose LeafGAN, a novel image-to-image translation system with own\nattention mechanism. LeafGAN generates a wide variety of diseased images via\ntransformation from healthy images, as a data augmentation tool for improving\nthe performance of plant disease diagnosis. Thanks to its own attention\nmechanism, our model can transform only relevant areas from images with a\nvariety of backgrounds, thus enriching the versatility of the training images.\nExperiments with five-class cucumber disease classification show that data\naugmentation with vanilla CycleGAN cannot help to improve the generalization,\ni.e., disease diagnostic performance increased by only 0.7% from the baseline.\nIn contrast, LeafGAN boosted the diagnostic performance by 7.4%. We also\nvisually confirmed the generated images by our LeafGAN were much better quality\nand more convincing than those generated by vanilla CycleGAN. The code is\navailable publicly at: https://github.com/IyatomiLab/LeafGAN.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 07:36:56 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 13:34:44 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Cap", "Quan Huu", ""], ["Uga", "Hiroyuki", ""], ["Kagiwada", "Satoshi", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "2002.10102", "submitter": "Wallace Michel Pinto Lira", "authors": "Wallace Lira, Johannes Merz, Daniel Ritchie, Daniel Cohen-Or, Hao\n  Zhang", "title": "GANHopper: Multi-Hop GAN for Unsupervised Image-to-Image Translation", "comments": "To be presented at ECCV 2020. Code is available at\n  https://github.com/wallacemplira/ganhopper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GANHopper, an unsupervised image-to-image translation network\nthat transforms images gradually between two domains, through multiple hops.\nInstead of executing translation directly, we steer the translation by\nrequiring the network to produce in-between images that resemble weighted\nhybrids between images from the input domains. Our network is trained on\nunpaired images from the two domains only, without any in-between images. All\nhops are produced using a single generator along each direction. In addition to\nthe standard cycle-consistency and adversarial losses, we introduce a new\nhybrid discriminator, which is trained to classify the intermediate images\nproduced by the generator as weighted hybrids, with weights based on a\npredetermined hop count. We also add a smoothness term to constrain the\nmagnitude of each hop, further regularizing the translation. Compared to\nprevious methods, GANHopper excels at image translations involving\ndomain-specific image features and geometric variations while also preserving\nnon-domain-specific features such as general color schemes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 07:41:07 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 20:24:57 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 07:21:41 GMT"}, {"version": "v4", "created": "Thu, 16 Jul 2020 20:24:44 GMT"}, {"version": "v5", "created": "Wed, 29 Jul 2020 02:28:42 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Lira", "Wallace", ""], ["Merz", "Johannes", ""], ["Ritchie", "Daniel", ""], ["Cohen-Or", "Daniel", ""], ["Zhang", "Hao", ""]]}, {"id": "2002.10105", "submitter": "Qiang Wang", "authors": "Qiang Wang, Shaohuai Shi, Canhui Wang, Xiaowen Chu", "title": "Communication Contention Aware Scheduling of Multiple Deep Learning\n  Training Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Deep Learning (DDL) has rapidly grown its popularity since it\nhelps boost the training performance on high-performance GPU clusters.\nEfficient job scheduling is indispensable to maximize the overall performance\nof the cluster when training multiple jobs simultaneously. However, existing\nschedulers do not consider the communication contention of multiple\ncommunication tasks from different distributed training jobs, which could\ndeteriorate the system performance and prolong the job completion time. In this\npaper, we first establish a new DDL job scheduling framework which organizes\nDDL jobs as Directed Acyclic Graphs (DAGs) and considers communication\ncontention between nodes. We then propose an efficient algorithm, LWF-$\\kappa$,\nto balance the GPU utilization and consolidate the allocated GPUs for each job.\nWhen scheduling those communication tasks, we observe that neither avoiding all\nthe contention nor blindly accepting them is optimal to minimize the job\ncompletion time. We thus propose a provable algorithm, AdaDUAL, to efficiently\nschedule those communication tasks. Based on AdaDUAL, we finally propose\nAda-SRSF for the DDL job scheduling problem. Simulations on a 64-GPU cluster\nconnected with 10 Gbps Ethernet show that LWF-$\\kappa$ achieves up to\n$1.59\\times$ improvement over the classical first-fit algorithms. More\nimportantly, Ada-SRSF reduces the average job completion time by $20.1\\%$ and\n$36.7\\%$, as compared to the SRSF(1) scheme (avoiding all the contention) and\nthe SRSF(2) scheme (blindly accepting all of two-way communication contention)\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 07:50:56 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Wang", "Qiang", ""], ["Shi", "Shaohuai", ""], ["Wang", "Canhui", ""], ["Chu", "Xiaowen", ""]]}, {"id": "2002.10111", "submitter": "Zechen Liu", "authors": "Zechen Liu, Zizhang Wu, Roland T\\'oth", "title": "SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint\n  Estimation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D orientation and translation of objects is essential for\ninfrastructure-less autonomous navigation and driving. In case of monocular\nvision, successful methods have been mainly based on two ingredients: (i) a\nnetwork generating 2D region proposals, (ii) a R-CNN structure predicting 3D\nobject pose by utilizing the acquired regions of interest. We argue that the 2D\ndetection network is redundant and introduces non-negligible noise for 3D\ndetection. Hence, we propose a novel 3D object detection method, named SMOKE,\nin this paper that predicts a 3D bounding box for each detected object by\ncombining a single keypoint estimate with regressed 3D variables. As a second\ncontribution, we propose a multi-step disentangling approach for constructing\nthe 3D bounding box, which significantly improves both training convergence and\ndetection accuracy. In contrast to previous 3D detection techniques, our method\ndoes not require complicated pre/post-processing, extra data, and a refinement\nstage. Despite of its structural simplicity, our proposed SMOKE network\noutperforms all existing monocular 3D detection methods on the KITTI dataset,\ngiving the best state-of-the-art result on both 3D object detection and Bird's\neye view evaluation. The code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 08:15:36 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Liu", "Zechen", ""], ["Wu", "Zizhang", ""], ["T\u00f3th", "Roland", ""]]}, {"id": "2002.10119", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez and Javier\n  Ortega-Garcia", "title": "DeepSign: Deep On-Line Signature Verification", "comments": null, "journal-ref": "IEEE Transactions on Biometrics, Behavior, and Identity Science,\n  2021", "doi": "10.1109/TBIOM.2021.3054533", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning has become a breathtaking technology in the last years,\novercoming traditional handcrafted approaches and even humans for many\ndifferent tasks. However, in some tasks, such as the verification of\nhandwritten signatures, the amount of publicly available data is scarce, what\nmakes difficult to test the real limits of deep learning. In addition to the\nlack of public data, it is not easy to evaluate the improvements of novel\nproposed approaches as different databases and experimental protocols are\nusually considered.\n  The main contributions of this study are: i) we provide an in-depth analysis\nof state-of-the-art deep learning approaches for on-line signature\nverification, ii) we present and describe the new DeepSignDB on-line\nhandwritten signature biometric public database, iii) we propose a standard\nexperimental protocol and benchmark to be used for the research community in\norder to perform a fair comparison of novel approaches with the state of the\nart, and iv) we adapt and evaluate our recent deep learning approach named\nTime-Aligned Recurrent Neural Networks (TA-RNNs) for the task of on-line\nhandwritten signature verification. This approach combines the potential of\nDynamic Time Warping and Recurrent Neural Networks to train more robust systems\nagainst forgeries. Our proposed TA-RNN system outperforms the state of the art,\nachieving results even below 2.0% EER when considering skilled forgery\nimpostors and just one training signature per user.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 08:53:11 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 08:44:11 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 15:53:57 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""], ["Ortega-Garcia", "Javier", ""]]}, {"id": "2002.10120", "submitter": "Xiangtai Li", "authors": "Xiangtai Li, Ansheng You, Zhen Zhu, Houlong Zhao, Maoke Yang, Kuiyuan\n  Yang, Yunhai Tong", "title": "Semantic Flow for Fast and Accurate Scene Parsing", "comments": "accepted by ECCV 2020(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on designing effective method for fast and accurate\nscene parsing. A common practice to improve the performance is to attain high\nresolution feature maps with strong semantic representation. Two strategies are\nwidely used -- atrous convolutions and feature pyramid fusion, are either\ncomputation intensive or ineffective. Inspired by the Optical Flow for motion\nalignment between adjacent video frames, we propose a Flow Alignment Module\n(FAM) to learn Semantic Flow between feature maps of adjacent levels, and\nbroadcast high-level features to high resolution features effectively and\nefficiently. Furthermore, integrating our module to a common feature pyramid\nstructure exhibits superior performance over other real-time methods even on\nlight-weight backbone networks, such as ResNet-18. Extensive experiments are\nconducted on several challenging datasets, including Cityscapes, PASCAL\nContext, ADE20K and CamVid. Especially, our network is the first to achieve\n80.4\\% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at\n\\url{https://github.com/lxtGH/SFSegNets}.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 08:53:18 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 12:53:21 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 08:43:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Xiangtai", ""], ["You", "Ansheng", ""], ["Zhu", "Zhen", ""], ["Zhao", "Houlong", ""], ["Yang", "Maoke", ""], ["Yang", "Kuiyuan", ""], ["Tong", "Yunhai", ""]]}, {"id": "2002.10137", "submitter": "Ran Yi", "authors": "Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, Yong-Jin Liu", "title": "Audio-driven Talking Face Video Generation with Learning-based\n  Personalized Head Pose", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world talking faces often accompany with natural head movement. However,\nmost existing talking face video generation methods only consider facial\nanimation with fixed head pose. In this paper, we address this problem by\nproposing a deep neural network model that takes an audio signal A of a source\nperson and a very short video V of a target person as input, and outputs a\nsynthesized high-quality talking face video with personalized head pose (making\nuse of the visual information in V), expression and lip synchronization (by\nconsidering both A and V). The most challenging issue in our work is that\nnatural poses often cause in-plane and out-of-plane head rotations, which makes\nsynthesized talking face video far from realistic. To address this challenge,\nwe reconstruct 3D face animation and re-render it into synthesized frames. To\nfine tune these frames into realistic ones with smooth background transition,\nwe propose a novel memory-augmented GAN module. By first training a general\nmapping based on a publicly available dataset and fine-tuning the mapping using\nthe input short video of target person, we develop an effective strategy that\nonly requires a small number of frames (about 300 frames) to learn personalized\ntalking behavior including head pose. Extensive experiments and two user\nstudies show that our method can generate high-quality (i.e., personalized head\nmovements, expressions and good lip synchronization) talking face videos, which\nare naturally looking with more distinguishing head movement effects than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 10:02:10 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 10:06:22 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Yi", "Ran", ""], ["Ye", "Zipeng", ""], ["Zhang", "Juyong", ""], ["Bao", "Hujun", ""], ["Liu", "Yong-Jin", ""]]}, {"id": "2002.10152", "submitter": "Matthew Gadd", "authors": "Will Maddern, Geoffrey Pascoe, Matthew Gadd, Dan Barnes, Brian\n  Yeomans, Paul Newman", "title": "Real-time Kinematic Ground Truth for the Oxford RobotCar Dataset", "comments": "Dataset website: https://robotcar-dataset.robots.ox.ac.uk/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the release of reference data towards a challenging long-term\nlocalisation and mapping benchmark based on the large-scale Oxford RobotCar\nDataset. The release includes 72 traversals of a route through Oxford, UK,\ngathered in all illumination, weather and traffic conditions, and is\nrepresentative of the conditions an autonomous vehicle would be expected to\noperate reliably in. Using post-processed raw GPS, IMU, and static GNSS base\nstation recordings, we have produced a globally-consistent centimetre-accurate\nground truth for the entire year-long duration of the dataset. Coupled with a\nplanned online benchmarking service, we hope to enable quantitative evaluation\nand comparison of different localisation and mapping approaches focusing on\nlong-term autonomy for road vehicles in urban environments challenged by\nchanging weather.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 10:34:31 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Maddern", "Will", ""], ["Pascoe", "Geoffrey", ""], ["Gadd", "Matthew", ""], ["Barnes", "Dan", ""], ["Yeomans", "Brian", ""], ["Newman", "Paul", ""]]}, {"id": "2002.10174", "submitter": "Runmin Wu", "authors": "Runmin Wu, Kunyao Zhang, Lijun Wang, Yue Wang, Pingping Zhang, Huchuan\n  Lu, Yizhou Yu", "title": "When Relation Networks meet GANs: Relation GANs with Triplet Loss", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Though recent research has achieved remarkable progress in generating\nrealistic images with generative adversarial networks (GANs), the lack of\ntraining stability is still a lingering concern of most GANs, especially on\nhigh-resolution inputs and complex datasets. Since the randomly generated\ndistribution can hardly overlap with the real distribution, training GANs often\nsuffers from the gradient vanishing problem. A number of approaches have been\nproposed to address this issue by constraining the discriminator's capabilities\nusing empirical techniques, like weight clipping, gradient penalty, spectral\nnormalization etc. In this paper, we provide a more principled approach as an\nalternative solution to this issue. Instead of training the discriminator to\ndistinguish real and fake input samples, we investigate the relationship\nbetween paired samples by training the discriminator to separate paired samples\nfrom the same distribution and those from different distributions. To this end,\nwe explore a relation network architecture for the discriminator and design a\ntriplet loss which performs better generalization and stability. Extensive\nexperiments on benchmark datasets show that the proposed relation discriminator\nand new loss can provide significant improvement on variable vision tasks\nincluding unconditional and conditional image generation and image translation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 11:35:28 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 07:35:13 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 03:28:57 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Wu", "Runmin", ""], ["Zhang", "Kunyao", ""], ["Wang", "Lijun", ""], ["Wang", "Yue", ""], ["Zhang", "Pingping", ""], ["Lu", "Huchuan", ""], ["Yu", "Yizhou", ""]]}, {"id": "2002.10177", "submitter": "Ioan Marius Bilasco PhD", "authors": "Pierre Falez and Pierre Tirilly and Ioan Marius Bilasco", "title": "Improving STDP-based Visual Feature Learning with Whitening", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9207373", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, spiking neural networks (SNNs) emerge as an alternative to\ndeep neural networks (DNNs). SNNs present a higher computational efficiency\nusing low-power neuromorphic hardware and require less labeled data for\ntraining using local and unsupervised learning rules such as spike\ntiming-dependent plasticity (STDP). SNN have proven their effectiveness in\nimage classification on simple datasets such as MNIST. However, to process\nnatural images, a pre-processing step is required. Difference-of-Gaussians\n(DoG) filtering is typically used together with on-center/off-center coding,\nbut it results in a loss of information that is detrimental to the\nclassification performance. In this paper, we propose to use whitening as a\npre-processing step before learning features with STDP. Experiments on CIFAR-10\nshow that whitening allows STDP to learn visual features that are closer to the\nones learned with standard neural networks, with a significantly increased\nclassification performance as compared to DoG filtering. We also propose an\napproximation of whitening as convolution kernels that is computationally\ncheaper to learn and more suited to be implemented on neuromorphic hardware.\nExperiments on CIFAR-10 show that it performs similarly to regular whitening.\nCross-dataset experiments on CIFAR-10 and STL-10 also show that it is fairly\nstable across datasets, making it possible to learn a single whitening\ntransformation to process different datasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 11:48:22 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Falez", "Pierre", ""], ["Tirilly", "Pierre", ""], ["Bilasco", "Ioan Marius", ""]]}, {"id": "2002.10179", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang,\n  Yonghong Tian, Ling Shao", "title": "HRank: Filter Pruning using High-Rank Feature Map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network pruning offers a promising prospect to facilitate deploying\ndeep neural networks on resource-limited devices. However, existing methods are\nstill challenged by the training inefficiency and labor cost in pruning\ndesigns, due to missing theoretical guidance of non-salient network components.\nIn this paper, we propose a novel filter pruning method by exploring the High\nRank of feature maps (HRank). Our HRank is inspired by the discovery that the\naverage rank of multiple feature maps generated by a single filter is always\nthe same, regardless of the number of image batches CNNs receive. Based on\nHRank, we develop a method that is mathematically formulated to prune filters\nwith low-rank feature maps. The principle behind our pruning is that low-rank\nfeature maps contain less information, and thus pruned results can be easily\nreproduced. Besides, we experimentally show that weights with high-rank feature\nmaps contain more important information, such that even when a portion is not\nupdated, very little damage would be done to the model performance. Without\nintroducing any additional constraints, HRank leads to significant improvements\nover the state-of-the-arts in terms of FLOPs and parameters reduction, with\nsimilar accuracies. For example, with ResNet-110, we achieve a 58.2%-FLOPs\nreduction by removing 59.2% of the parameters, with only a small loss of 0.14%\nin top-1 accuracy on CIFAR-10. With Res-50, we achieve a 43.8%-FLOPs reduction\nby removing 36.7% of the parameters, with only a loss of 1.17% in the top-1\naccuracy on ImageNet. The codes can be available at\nhttps://github.com/lmbxmu/HRank.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 11:50:09 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 23:50:13 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Wang", "Yan", ""], ["Zhang", "Yichen", ""], ["Zhang", "Baochang", ""], ["Tian", "Yonghong", ""], ["Shao", "Ling", ""]]}, {"id": "2002.10187", "submitter": "Zetong Yang", "authors": "Zetong Yang, Yanan Sun, Shu Liu, Jiaya Jia", "title": "3DSSD: Point-based 3D Single Stage Object Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, there have been many kinds of voxel-based 3D single stage\ndetectors, while point-based single stage methods are still underexplored. In\nthis paper, we first present a lightweight and effective point-based 3D single\nstage object detector, named 3DSSD, achieving a good balance between accuracy\nand efficiency. In this paradigm, all upsampling layers and refinement stage,\nwhich are indispensable in all existing point-based methods, are abandoned to\nreduce the large computation cost. We novelly propose a fusion sampling\nstrategy in downsampling process to make detection on less representative\npoints feasible. A delicate box prediction network including a candidate\ngeneration layer, an anchor-free regression head with a 3D center-ness\nassignment strategy is designed to meet with our demand of accuracy and speed.\nOur paradigm is an elegant single stage anchor-free framework, showing great\nsuperiority to other existing methods. We evaluate 3DSSD on widely used KITTI\ndataset and more challenging nuScenes dataset. Our method outperforms all\nstate-of-the-art voxel-based single stage methods by a large margin, and has\ncomparable performance to two stage point-based methods as well, with inference\nspeed more than 25 FPS, 2x faster than former state-of-the-art point-based\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 12:01:58 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Yang", "Zetong", ""], ["Sun", "Yanan", ""], ["Liu", "Shu", ""], ["Jia", "Jiaya", ""]]}, {"id": "2002.10191", "submitter": "Peiqin Zhuang", "authors": "Peiqin Zhuang, Yali Wang, Yu Qiao", "title": "Learning Attentive Pairwise Interaction for Fine-Grained Classification", "comments": "Accepted at AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained classification is a challenging problem, due to subtle\ndifferences among highly-confused categories. Most approaches address this\ndifficulty by learning discriminative representation of individual input image.\nOn the other hand, humans can effectively identify contrastive clues by\ncomparing image pairs. Inspired by this fact, this paper proposes a simple but\neffective Attentive Pairwise Interaction Network (API-Net), which can\nprogressively recognize a pair of fine-grained images by interaction.\nSpecifically, API-Net first learns a mutual feature vector to capture semantic\ndifferences in the input pair. It then compares this mutual vector with\nindividual vectors to generate gates for each input image. These distinct gate\nvectors inherit mutual context on semantic differences, which allow API-Net to\nattentively capture contrastive clues by pairwise interaction between two\nimages. Additionally, we train API-Net in an end-to-end manner with a score\nranking regularization, which can further generalize API-Net by taking feature\npriorities into account. We conduct extensive experiments on five popular\nbenchmarks in fine-grained classification. API-Net outperforms the recent SOTA\nmethods, i.e., CUB-200-2011 (90.0%), Aircraft(93.9%), Stanford Cars (95.3%),\nStanford Dogs (90.3%), and NABirds (88.1%).\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 12:17:56 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Zhuang", "Peiqin", ""], ["Wang", "Yali", ""], ["Qiao", "Yu", ""]]}, {"id": "2002.10200", "submitter": "Chunhua Shen", "authors": "Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, Liangwei\n  Wang", "title": "ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network", "comments": "Accepted to Proc. IEEE Conf. Comp. Vis. Pattern Recogn. (CVPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scene text detection and recognition has received increasing research\nattention. Existing methods can be roughly categorized into two groups:\ncharacter-based and segmentation-based. These methods either are costly for\ncharacter annotation or need to maintain a complex pipeline, which is often not\nsuitable for real-time applications. Here we address the problem by proposing\nthe Adaptive Bezier-Curve Network (ABCNet). Our contributions are three-fold:\n1) For the first time, we adaptively fit arbitrarily-shaped text by a\nparameterized Bezier curve. 2) We design a novel BezierAlign layer for\nextracting accurate convolution features of a text instance with arbitrary\nshapes, significantly improving the precision compared with previous methods.\n3) Compared with standard bounding box detection, our Bezier curve detection\nintroduces negligible computation overhead, resulting in superiority of our\nmethod in both efficiency and accuracy. Experiments on arbitrarily-shaped\nbenchmark datasets, namely Total-Text and CTW1500, demonstrate that ABCNet\nachieves state-of-the-art accuracy, meanwhile significantly improving the\nspeed. In particular, on Total-Text, our realtime version is over 10 times\nfaster than recent state-of-the-art methods with a competitive recognition\naccuracy. Code is available at https://tinyurl.com/AdelaiDet\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 12:27:31 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 08:02:28 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Liu", "Yuliang", ""], ["Chen", "Hao", ""], ["Shen", "Chunhua", ""], ["He", "Tong", ""], ["Jin", "Lianwen", ""], ["Wang", "Liangwei", ""]]}, {"id": "2002.10201", "submitter": "Meng Chang", "authors": "Meng Chang, Chenwei Yang, Huajun Feng, Zhihai Xu, Qi Li", "title": "Beyond Camera Motion Blur Removing: How to Handle Outliers in Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera motion deblurring is an important low-level vision task for achieving\nbetter imaging quality. When a scene has outliers such as saturated pixels, the\ncaptured blurred image becomes more difficult to restore. In this paper, we\npropose a novel method to handle camera motion blur with outliers. We first\npropose an edge-aware scale-recurrent network (EASRN) to conduct deblurring.\nEASRN has a separate deblurring module that removes blur at multiple scales and\nan upsampling module that fuses different input scales. Then a salient edge\ndetection network is proposed to supervise the training process and constraint\nthe edges restoration. By simulating camera motion and adding various light\nsources, we can generate blurred images with saturation cutoff. Using the\nproposed data generation method, our network can learn to deal with outliers\neffectively. We evaluate our method on public test datasets including the GoPro\ndataset, Kohler's dataset and Lai's dataset. Both objective evaluation indexes\nand subjective visualization show that our method results in better deblurring\nquality than other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 12:32:04 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 09:34:08 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 06:55:28 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chang", "Meng", ""], ["Yang", "Chenwei", ""], ["Feng", "Huajun", ""], ["Xu", "Zhihai", ""], ["Li", "Qi", ""]]}, {"id": "2002.10211", "submitter": "Yaoyao Liu", "authors": "Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, Qianru Sun", "title": "Mnemonics Training: Multi-Class Incremental Learning without Forgetting", "comments": "Experiment results updated (different from the conference version).\n  Code is available at https://github.com/yaoyao-liu/mnemonics-training", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.01226", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-Class Incremental Learning (MCIL) aims to learn new concepts by\nincrementally updating a model trained on previous concepts. However, there is\nan inherent trade-off to effectively learning new concepts without catastrophic\nforgetting of previous ones. To alleviate this issue, it has been proposed to\nkeep around a few examples of the previous concepts but the effectiveness of\nthis approach heavily depends on the representativeness of these examples. This\npaper proposes a novel and automatic framework we call mnemonics, where we\nparameterize exemplars and make them optimizable in an end-to-end manner. We\ntrain the framework through bilevel optimizations, i.e., model-level and\nexemplar-level. We conduct extensive experiments on three MCIL benchmarks,\nCIFAR-100, ImageNet-Subset and ImageNet, and show that using mnemonics\nexemplars can surpass the state-of-the-art by a large margin. Interestingly and\nquite intriguingly, the mnemonics exemplars tend to be on the boundaries\nbetween different classes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 12:55:25 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 07:35:05 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 01:46:46 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 12:49:51 GMT"}, {"version": "v5", "created": "Tue, 15 Sep 2020 19:44:51 GMT"}, {"version": "v6", "created": "Sun, 4 Apr 2021 12:24:40 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liu", "Yaoyao", ""], ["Su", "Yuting", ""], ["Liu", "An-An", ""], ["Schiele", "Bernt", ""], ["Sun", "Qianru", ""]]}, {"id": "2002.10215", "submitter": "Chunhua Shen", "authors": "Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo,\n  Lianwen Jin, Chee Seng Chan, Anton van den Hengel, Liangwei Wang", "title": "On the General Value of Evidence, and Bilingual Scene-Text Visual\n  Question Answering", "comments": "Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual Question Answering (VQA) methods have made incredible progress, but\nsuffer from a failure to generalize. This is visible in the fact that they are\nvulnerable to learning coincidental correlations in the data rather than deeper\nrelations between image content and ideas expressed in language. We present a\ndataset that takes a step towards addressing this problem in that it contains\nquestions expressed in two languages, and an evaluation process that co-opts a\nwell understood image-based metric to reflect the method's ability to reason.\nMeasuring reasoning directly encourages generalization by penalizing answers\nthat are coincidentally correct. The dataset reflects the scene-text version of\nthe VQA problem, and the reasoning evaluation can be seen as a text-based\nversion of a referring expression challenge. Experiments and analysis are\nprovided that show the value of the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 13:02:31 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 04:59:18 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Wang", "Xinyu", ""], ["Liu", "Yuliang", ""], ["Shen", "Chunhua", ""], ["Ng", "Chun Chet", ""], ["Luo", "Canjie", ""], ["Jin", "Lianwen", ""], ["Chan", "Chee Seng", ""], ["Hengel", "Anton van den", ""], ["Wang", "Liangwei", ""]]}, {"id": "2002.10217", "submitter": "Levente Hajder", "authors": "Levente Hajder and Tekla T\\'oth and Zolt\\'an Pusztai", "title": "Automatic Estimation of Sphere Centers from Images of Calibrated Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of devices with different modalities is a key problem in robotic\nvision. Regular spatial objects, such as planes, are frequently used for this\ntask. This paper deals with the automatic detection of ellipses in camera\nimages, as well as to estimate the 3D position of the spheres corresponding to\nthe detected 2D ellipses. We propose two novel methods to (i) detect an ellipse\nin camera images and (ii) estimate the spatial location of the corresponding\nsphere if its size is known. The algorithms are tested both quantitatively and\nqualitatively. They are applied for calibrating the sensor system of autonomous\ncars equipped with digital cameras, depth sensors and LiDAR devices.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 13:12:08 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Hajder", "Levente", ""], ["T\u00f3th", "Tekla", ""], ["Pusztai", "Zolt\u00e1n", ""]]}, {"id": "2002.10252", "submitter": "Negin Entezari", "authors": "Negin Entezari, Evangelos E. Papalexakis", "title": "TensorShield: Tensor-based Defense Against Adversarial Attacks on Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated that machine learning approaches like deep\nneural networks (DNNs) are easily fooled by adversarial attacks. Subtle and\nimperceptible perturbations of the data are able to change the result of deep\nneural networks. Leveraging vulnerable machine learning methods raises many\nconcerns especially in domains where security is an important factor.\nTherefore, it is crucial to design defense mechanisms against adversarial\nattacks. For the task of image classification, unnoticeable perturbations\nmostly occur in the high-frequency spectrum of the image. In this paper, we\nutilize tensor decomposition techniques as a preprocessing step to find a\nlow-rank approximation of images which can significantly discard high-frequency\nperturbations. Recently a defense framework called Shield could \"vaccinate\"\nConvolutional Neural Networks (CNN) against adversarial examples by performing\nrandom-quality JPEG compressions on local patches of images on the ImageNet\ndataset. Our tensor-based defense mechanism outperforms the SLQ method from\nShield by 14% against FastGradient Descent (FGSM) adversarial attacks, while\nmaintaining comparable speed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 00:39:49 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Entezari", "Negin", ""], ["Papalexakis", "Evangelos E.", ""]]}, {"id": "2002.10277", "submitter": "Yue Qian", "authors": "Yue Qian, Junhui Hou, Sam Kwong, Ying He", "title": "PUGeo-Net: A Geometry-centric Network for 3D Point Cloud Upsampling", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of generating uniform dense point clouds to\ndescribe the underlying geometric structures from given sparse point clouds.\nDue to the irregular and unordered nature, point cloud densification as a\ngenerative task is challenging. To tackle the challenge, we propose a novel\ndeep neural network based method, called PUGeo-Net, that learns a $3\\times 3$\nlinear transformation matrix $\\bf T$ for each input point. Matrix $\\mathbf T$\napproximates the augmented Jacobian matrix of a local parameterization and\nbuilds a one-to-one correspondence between the 2D parametric domain and the 3D\ntangent plane so that we can lift the adaptively distributed 2D samples (which\nare also learned from data) to 3D space. After that, we project the samples to\nthe curved surface by computing a displacement along the normal of the tangent\nplane. PUGeo-Net is fundamentally different from the existing deep learning\nmethods that are largely motivated by the image super-resolution techniques and\ngenerate new points in the abstract feature space. Thanks to its\ngeometry-centric nature, PUGeo-Net works well for both CAD models with sharp\nfeatures and scanned models with rich geometric details. Moreover, PUGeo-Net\ncan compute the normal for the original and generated points, which is highly\ndesired by the surface reconstruction algorithms. Computational results show\nthat PUGeo-Net, the first neural network that can jointly generate vertex\ncoordinates and normals, consistently outperforms the state-of-the-art in terms\nof accuracy and efficiency for upsampling factor $4\\sim 16$.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 14:13:29 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 16:02:05 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Qian", "Yue", ""], ["Hou", "Junhui", ""], ["Kwong", "Sam", ""], ["He", "Ying", ""]]}, {"id": "2002.10309", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Mayank Lunayach and Vinay P. Namboodiri", "title": "Uncertainty based Class Activation Maps for Visual Question Answering", "comments": "This work is an extension of our ICCV-2019 work. arXiv admin note:\n  text overlap with arXiv:1908.06306", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding and explaining deep learning models is an imperative task.\nTowards this, we propose a method that obtains gradient-based certainty\nestimates that also provide visual attention maps. Particularly, we solve for\nvisual question answering task. We incorporate modern probabilistic deep\nlearning methods that we further improve by using the gradients for these\nestimates. These have two-fold benefits: a) improvement in obtaining the\ncertainty estimates that correlate better with misclassified samples and b)\nimproved attention maps that provide state-of-the-art results in terms of\ncorrelation with human attention regions. The improved attention maps result in\nconsistent improvement for various methods for visual question answering.\nTherefore, the proposed technique can be thought of as a recipe for obtaining\nimproved certainty estimates and explanations for deep learning models. We\nprovide detailed empirical analysis for the visual question answering task on\nall standard benchmarks and comparison with state of the art methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 19:54:19 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Patro", "Badri N.", ""], ["Lunayach", "Mayank", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2002.10310", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Yongxin Yang, Timothy M. Hospedales, Tao Xiang,\n  Yi-Zhe Song", "title": "Sketch Less for More: On-the-Fly Fine-Grained Sketch Based Image\n  Retrieval", "comments": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020\n  [Oral Presentation] Code:\n  https://github.com/AyanKumarBhunia/on-the-fly-FGSBIR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained sketch-based image retrieval (FG-SBIR) addresses the problem of\nretrieving a particular photo instance given a user's query sketch. Its\nwidespread applicability is however hindered by the fact that drawing a sketch\ntakes time, and most people struggle to draw a complete and faithful sketch. In\nthis paper, we reformulate the conventional FG-SBIR framework to tackle these\nchallenges, with the ultimate goal of retrieving the target photo with the\nleast number of strokes possible. We further propose an on-the-fly design that\nstarts retrieving as soon as the user starts drawing. To accomplish this, we\ndevise a reinforcement learning-based cross-modal retrieval framework that\ndirectly optimizes rank of the ground-truth photo over a complete sketch\ndrawing episode. Additionally, we introduce a novel reward scheme that\ncircumvents the problems related to irrelevant sketch strokes, and thus\nprovides us with a more consistent rank list during the retrieval. We achieve\nsuperior early-retrieval efficiency over state-of-the-art methods and\nalternative baselines on two publicly available fine-grained sketch retrieval\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 15:36:02 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 15:23:37 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 17:54:10 GMT"}, {"version": "v4", "created": "Mon, 11 May 2020 18:32:08 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2002.10319", "submitter": "Lang Huang", "authors": "Lang Huang, Chao Zhang, Hongyang Zhang", "title": "Self-Adaptive Training: beyond Empirical Risk Minimization", "comments": "To appear in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose self-adaptive training---a new training algorithm that dynamically\ncorrects problematic training labels by model predictions without incurring\nextra computational cost---to improve generalization of deep learning for\npotentially corrupted training data. This problem is crucial towards robustly\nlearning from data that are corrupted by, e.g., label noises and\nout-of-distribution samples. The standard empirical risk minimization (ERM) for\nsuch data, however, may easily overfit noises and thus suffers from sub-optimal\nperformance. In this paper, we observe that model predictions can substantially\nbenefit the training process: self-adaptive training significantly improves\ngeneralization over ERM under various levels of noises, and mitigates the\noverfitting issue in both natural and adversarial training. We evaluate the\nerror-capacity curve of self-adaptive training: the test error is monotonously\ndecreasing w.r.t. model capacity. This is in sharp contrast to the\nrecently-discovered double-descent phenomenon in ERM which might be a result of\noverfitting of noises. Experiments on CIFAR and ImageNet datasets verify the\neffectiveness of our approach in two applications: classification with label\nnoise and selective classification. We release our code at\nhttps://github.com/LayneH/self-adaptive-training.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 15:47:10 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 09:14:50 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Huang", "Lang", ""], ["Zhang", "Chao", ""], ["Zhang", "Hongyang", ""]]}, {"id": "2002.10322", "submitter": "Tianlang Chen", "authors": "Tianlang Chen, Chen Fang, Xiaohui Shen, Yiheng Zhu, Zhili Chen, Jiebo\n  Luo", "title": "Anatomy-aware 3D Human Pose Estimation with Bone-based Pose\n  Decomposition", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology. Our code is available at\n  https://github.com/sunnychencool/Anatomy3D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new solution to 3D human pose estimation in\nvideos. Instead of directly regressing the 3D joint locations, we draw\ninspiration from the human skeleton anatomy and decompose the task into bone\ndirection prediction and bone length prediction, from which the 3D joint\nlocations can be completely derived. Our motivation is the fact that the bone\nlengths of a human skeleton remain consistent across time. This promotes us to\ndevelop effective techniques to utilize global information across all the\nframes in a video for high-accuracy bone length prediction. Moreover, for the\nbone direction prediction network, we propose a fully-convolutional propagating\narchitecture with long skip connections. Essentially, it predicts the\ndirections of different bones hierarchically without using any time-consuming\nmemory units e.g. LSTM). A novel joint shift loss is further introduced to\nbridge the training of the bone length and bone direction prediction networks.\nFinally, we employ an implicit attention mechanism to feed the 2D keypoint\nvisibility scores into the model as extra guidance, which significantly\nmitigates the depth ambiguity in many challenging poses. Our full model\noutperforms the previous best results on Human3.6M and MPI-INF-3DHP datasets,\nwhere comprehensive evaluation validates the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 15:49:37 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 18:40:45 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 14:54:50 GMT"}, {"version": "v4", "created": "Sun, 24 Jan 2021 18:32:28 GMT"}, {"version": "v5", "created": "Tue, 26 Jan 2021 17:08:11 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chen", "Tianlang", ""], ["Fang", "Chen", ""], ["Shen", "Xiaohui", ""], ["Zhu", "Yiheng", ""], ["Chen", "Zhili", ""], ["Luo", "Jiebo", ""]]}, {"id": "2002.10340", "submitter": "Wei Pang Xubu", "authors": "Wei Pang, Xiaojie Wang", "title": "Guessing State Tracking for Visual Dialogue", "comments": "Accepted at ECCV 2020. The paper is about how the Guesser in the\n  GuessWhat?! game guess. More details can be found at\n  https://github.com/xubuvd/guesswhat", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Guesser is a task of visual grounding in GuessWhat?! like visual\ndialogue. It locates the target object in an image supposed by an Oracle\noneself over a question-answer based dialogue between a Questioner and the\nOracle. Most existing guessers make one and only one guess after receiving all\nquestion-answer pairs in a dialogue with the predefined number of rounds. This\npaper proposes a guessing state for the Guesser, and regards guess as a process\nwith change of guessing state through a dialogue. A guessing state tracking\nbased guess model is therefore proposed. The guessing state is defined as a\ndistribution on objects in the image. With that in hand, two loss functions are\ndefined as supervisions for model training. Early supervision brings\nsupervision to Guesser at early rounds, and incremental supervision brings\nmonotonicity to the guessing state. Experimental results on GuessWhat?! dataset\nshow that our model significantly outperforms previous models, achieves new\nstate-of-the-art, especially the success rate of guessing 83.3% is approaching\nthe human-level accuracy of 84.4%.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 16:09:45 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 11:53:31 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 07:13:50 GMT"}, {"version": "v4", "created": "Wed, 15 Jul 2020 14:12:26 GMT"}, {"version": "v5", "created": "Sat, 18 Jul 2020 06:20:39 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Pang", "Wei", ""], ["Wang", "Xiaojie", ""]]}, {"id": "2002.10342", "submitter": "Zoe Landgraf", "authors": "Zoe Landgraf, Fabian Falck, Michael Bloesch, Stefan Leutenegger,\n  Andrew Davison", "title": "Comparing View-Based and Map-Based Semantic Labelling in Real-Time SLAM", "comments": "ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally capable Spatial AI systems must build persistent scene\nrepresentations where geometric models are combined with meaningful semantic\nlabels. The many approaches to labelling scenes can be divided into two clear\ngroups: view-based which estimate labels from the input view-wise data and then\nincrementally fuse them into the scene model as it is built; and map-based\nwhich label the generated scene model. However, there has so far been no\nattempt to quantitatively compare view-based and map-based labelling. Here, we\npresent an experimental framework and comparison which uses real-time height\nmap fusion as an accessible platform for a fair comparison, opening up the\nroute to further systematic research in this area.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 16:12:51 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Landgraf", "Zoe", ""], ["Falck", "Fabian", ""], ["Bloesch", "Michael", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew", ""]]}, {"id": "2002.10362", "submitter": "Marzieh Gheisari", "authors": "Marzieh Gheisari, Teddy Furon, Laurent Amsaleg", "title": "Group Membership Verification with Privacy: Sparse or Dense?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group membership verification checks if a biometric trait corresponds to one\nmember of a group without revealing the identity of that member. Recent\ncontributions provide privacy for group membership protocols through the joint\nuse of two mechanisms: quantizing templates into discrete embeddings and\naggregating several templates into one group representation. However, this\nscheme has one drawback: the data structure representing the group has a\nlimited size and cannot recognize noisy queries when many templates are\naggregated. Moreover, the sparsity of the embeddings seemingly plays a crucial\nrole on the performance verification. This paper proposes a mathematical model\nfor group membership verification allowing to reveal the impact of sparsity on\nboth security, compactness, and verification performances. This model bridges\nthe gap towards a Bloom filter robust to noisy queries. It shows that a dense\nsolution is more competitive unless the queries are almost noiseless.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 16:47:19 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gheisari", "Marzieh", ""], ["Furon", "Teddy", ""], ["Amsaleg", "Laurent", ""]]}, {"id": "2002.10363", "submitter": "Marzieh Gheisari", "authors": "Marzieh Gheisari, Teddy Furon, Laurent Amsaleg", "title": "Joint Learning of Assignment and Representation for Biometric Group\n  Membership", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a framework for group membership protocols preventing the\ncurious but honest server from reconstructing the enrolled biometric signatures\nand inferring the identity of querying clients. This framework learns the\nembedding parameters, group representations and assignments simultaneously.\nExperiments show the trade-off between security/privacy and\nverification/identification performances.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 16:48:30 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gheisari", "Marzieh", ""], ["Furon", "Teddy", ""], ["Amsaleg", "Laurent", ""]]}, {"id": "2002.10381", "submitter": "Leo Sampaio Ferraz Ribeiro", "authors": "Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, Moacir Ponti", "title": "Sketchformer: Transformer-based Representation for Sketched Structure", "comments": "Accepted for publication at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sketchformer is a novel transformer-based representation for encoding\nfree-hand sketches input in a vector form, i.e. as a sequence of strokes.\nSketchformer effectively addresses multiple tasks: sketch classification,\nsketch based image retrieval (SBIR), and the reconstruction and interpolation\nof sketches. We report several variants exploring continuous and tokenized\ninput representations, and contrast their performance. Our learned embedding,\ndriven by a dictionary learning tokenization scheme, yields state of the art\nperformance in classification and image retrieval tasks, when compared against\nbaseline representations driven by LSTM sequence to sequence architectures:\nSketchRNN and derivatives. We show that sketch reconstruction and interpolation\nare improved significantly by the Sketchformer embedding for complex sketches\nwith longer stroke sequences.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 17:11:53 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Ribeiro", "Leo Sampaio Ferraz", ""], ["Bui", "Tu", ""], ["Collomosse", "John", ""], ["Ponti", "Moacir", ""]]}, {"id": "2002.10392", "submitter": "Kai Wang", "authors": "Kai Wang, Xiaojiang Peng, Jianfei Yang, Shijian Lu, Yu Qiao", "title": "Suppressing Uncertainties for Large-Scale Facial Expression Recognition", "comments": "This manuscript has been accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating a qualitative large-scale facial expression dataset is extremely\ndifficult due to the uncertainties caused by ambiguous facial expressions,\nlow-quality facial images, and the subjectiveness of annotators. These\nuncertainties lead to a key challenge of large-scale Facial Expression\nRecognition (FER) in deep learning era. To address this problem, this paper\nproposes a simple yet efficient Self-Cure Network (SCN) which suppresses the\nuncertainties efficiently and prevents deep networks from over-fitting\nuncertain facial images. Specifically, SCN suppresses the uncertainty from two\ndifferent aspects: 1) a self-attention mechanism over mini-batch to weight each\ntraining sample with a ranking regularization, and 2) a careful relabeling\nmechanism to modify the labels of these samples in the lowest-ranked group.\nExperiments on synthetic FER datasets and our collected WebEmotion dataset\nvalidate the effectiveness of our method. Results on public benchmarks\ndemonstrate that our SCN outperforms current state-of-the-art methods with\n\\textbf{88.14}\\% on RAF-DB, \\textbf{60.23}\\% on AffectNet, and \\textbf{89.35}\\%\non FERPlus. The code will be available at\n\\href{https://github.com/kaiwang960112/Self-Cure-Network}{https://github.com/kaiwang960112/Self-Cure-Network}.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 17:24:36 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 09:57:28 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Wang", "Kai", ""], ["Peng", "Xiaojiang", ""], ["Yang", "Jianfei", ""], ["Lu", "Shijian", ""], ["Qiao", "Yu", ""]]}, {"id": "2002.10420", "submitter": "Fahad Sohrab", "authors": "Fahad Sohrab, Jenni Raitoharju", "title": "Boosting rare benthic macroinvertebrates taxa identification with\n  one-class classification", "comments": "5 pages, 1 figure, 2 tables", "journal-ref": "2020 IEEE Symposium Series on Computational Intelligence (SSCI)", "doi": "10.1109/SSCI47803.2020.9308359", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insect monitoring is crucial for understanding the consequences of rapid\necological changes, but taxa identification currently requires tedious manual\nexpert work and cannot be scaled-up efficiently. Deep convolutional neural\nnetworks (CNNs), provide a viable way to significantly increase the\nbiomonitoring volumes. However, taxa abundances are typically very imbalanced\nand the amounts of training images for the rarest classes are simply too low\nfor deep CNNs. As a result, the samples from the rare classes are often\ncompletely missed, while detecting them has biological importance. In this\npaper, we propose combining the trained deep CNN with one-class classifiers to\nimprove the rare species identification. One-class classification models are\ntraditionally trained with much fewer samples and they can provide a mechanism\nto indicate samples potentially belonging to the rare classes for human\ninspection. Our experiments confirm that the proposed approach may indeed\nsupport moving towards partial automation of the taxa identification task.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 09:46:24 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Sohrab", "Fahad", ""], ["Raitoharju", "Jenni", ""]]}, {"id": "2002.10434", "submitter": "Gabriel Rioux", "authors": "Gabriel Rioux, Rustum Choksi, Tim Hoheisel, Pierre Marechal,\n  Christopher Scarvelis", "title": "The Maximum Entropy on the Mean Method for Image Deblurring", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deblurring is a notoriously challenging ill-posed inverse problem. In\nrecent years, a wide variety of approaches have been proposed based upon\nregularization at the level of the image or on techniques from machine\nlearning. We propose an alternative approach, shifting the paradigm towards\nregularization at the level of the probability distribution on the space of\nimages. Our method is based upon the idea of maximum entropy on the mean\nwherein we work at the level of the probability density function of the image\nwhose expectation is our estimate of the ground truth. Using techniques from\nconvex analysis and probability theory, we show that the method is\ncomputationally feasible and amenable to very large blurs. Moreover, when\nimages are imbedded with symbology (a known pattern), we show how our method\ncan be applied to approximate the unknown blur kernel with remarkable effects.\nWhile our method is stable with respect to small amounts of noise, it does not\nactively denoise. However, for moderate to large amounts of noise, it performs\nwell by preconditioned denoising with a state of the art method.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 18:30:40 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 15:04:02 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 01:24:04 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 18:22:47 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Rioux", "Gabriel", ""], ["Choksi", "Rustum", ""], ["Hoheisel", "Tim", ""], ["Marechal", "Pierre", ""], ["Scarvelis", "Christopher", ""]]}, {"id": "2002.10444", "submitter": "Soham De", "authors": "Soham De, Samuel L. Smith", "title": "Batch Normalization Biases Residual Blocks Towards the Identity Function\n  in Deep Networks", "comments": "Camera-ready version of NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization dramatically increases the largest trainable depth of\nresidual networks, and this benefit has been crucial to the empirical success\nof deep residual networks on a wide range of benchmarks. We show that this key\nbenefit arises because, at initialization, batch normalization downscales the\nresidual branch relative to the skip connection, by a normalizing factor on the\norder of the square root of the network depth. This ensures that, early in\ntraining, the function computed by normalized residual blocks in deep networks\nis close to the identity function (on average). We use this insight to develop\na simple initialization scheme that can train deep residual networks without\nnormalization. We also provide a detailed empirical study of residual networks,\nwhich clarifies that, although batch normalized networks can be trained with\nlarger learning rates, this effect is only beneficial in specific compute\nregimes, and has minimal benefits when the batch size is small.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 18:43:03 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 12:20:43 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 10:18:10 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["De", "Soham", ""], ["Smith", "Samuel L.", ""]]}, {"id": "2002.10445", "submitter": "Yedid Hoshen", "authors": "Liron Bergman and Niv Cohen and Yedid Hoshen", "title": "Deep Nearest Neighbor Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbors is a successful and long-standing technique for anomaly\ndetection. Significant progress has been recently achieved by self-supervised\ndeep methods (e.g. RotNet). Self-supervised features however typically\nunder-perform Imagenet pre-trained features. In this work, we investigate\nwhether the recent progress can indeed outperform nearest-neighbor methods\noperating on an Imagenet pretrained feature space. The simple nearest-neighbor\nbased-approach is experimentally shown to outperform self-supervised methods\nin: accuracy, few shot generalization, training time and noise robustness while\nmaking fewer assumptions on image distributions.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 18:51:33 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Bergman", "Liron", ""], ["Cohen", "Niv", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2002.10509", "submitter": "Vikash Sehwag", "authors": "Vikash Sehwag, Shiqi Wang, Prateek Mittal, Suman Jana", "title": "HYDRA: Pruning Adversarially Robust Neural Networks", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In safety-critical but computationally resource-constrained applications,\ndeep learning faces two key challenges: lack of robustness against adversarial\nattacks and large neural network size (often millions of parameters). While the\nresearch community has extensively explored the use of robust training and\nnetwork pruning independently to address one of these challenges, only a few\nrecent works have studied them jointly. However, these works inherit a\nheuristic pruning strategy that was developed for benign training, which\nperforms poorly when integrated with robust training techniques, including\nadversarial training and verifiable robust training. To overcome this\nchallenge, we propose to make pruning techniques aware of the robust training\nobjective and let the training objective guide the search for which connections\nto prune. We realize this insight by formulating the pruning objective as an\nempirical risk minimization problem which is solved efficiently using SGD. We\ndemonstrate that our approach, titled HYDRA, achieves compressed networks with\nstate-of-the-art benign and robust accuracy, simultaneously. We demonstrate the\nsuccess of our approach across CIFAR-10, SVHN, and ImageNet dataset with four\nrobust training techniques: iterative adversarial training, randomized\nsmoothing, MixTrain, and CROWN-IBP. We also demonstrate the existence of highly\nrobust sub-networks within non-robust networks. Our code and compressed\nnetworks are publicly available at\n\\url{https://github.com/inspire-group/compactness-robustness}.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 19:54:53 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 14:26:57 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 15:02:00 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Sehwag", "Vikash", ""], ["Wang", "Shiqi", ""], ["Mittal", "Prateek", ""], ["Jana", "Suman", ""]]}, {"id": "2002.10523", "submitter": "Puneesh Deora", "authors": "Bhavya Vasudeva, Puneesh Deora, Saumik Bhattacharya, Pyari Mohan\n  Pradhan", "title": "Co-VeGAN: Complex-Valued Generative Adversarial Network for Compressive\n  Sensing MR Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS) is widely used to reduce the acquisition time of\nmagnetic resonance imaging (MRI). Although state-of-the-art deep learning based\nmethods have been able to obtain fast, high-quality reconstruction of CS-MR\nimages, their main drawback is that they treat complex-valued MRI data as\nreal-valued entities. Most methods either extract the magnitude from the\ncomplex-valued entities or concatenate them as two real-valued channels. In\nboth the cases, the phase content, which links the real and imaginary parts of\nthe complex-valued entities, is discarded. In order to address the fundamental\nproblem of real-valued deep networks, i.e. their inability to process\ncomplex-valued data, we propose a novel framework based on a complex-valued\ngenerative adversarial network (Co-VeGAN). Our model can process complex-valued\ninput, which enables it to perform high-quality reconstruction of the CS-MR\nimages. Further, considering that phase is a crucial component of\ncomplex-valued entities, we propose a novel complex-valued activation function,\nwhich is sensitive to the phase of the input. Extensive evaluation of the\nproposed approach on different datasets using various sampling masks\ndemonstrates that the proposed model significantly outperforms the existing\nCS-MRI reconstruction techniques in terms of peak signal-to-noise ratio as well\nas structural similarity index. Further, it uses significantly fewer trainable\nparameters to do so, as compared to the real-valued deep learning based\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:28:49 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 11:18:33 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 15:50:10 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Vasudeva", "Bhavya", ""], ["Deora", "Puneesh", ""], ["Bhattacharya", "Saumik", ""], ["Pradhan", "Pyari Mohan", ""]]}, {"id": "2002.10534", "submitter": "Carole Twining dr", "authors": "Carole J. Twining, Vladimir S. Petrovi\\'c, Timothy F. Cootes, Roy S.\n  Schestowitz, William R. Crum, and Christopher J. Taylor", "title": "Evaluating Registration Without Ground Truth", "comments": "10 pages, 2 Figures, 3 Tables. Submitted to IEEE Transactions on\n  Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic method for assessing the quality of non-rigid\nregistration (NRR) algorithms, that does not depend on the existence of any\nground truth, but depends solely on the data itself. The data is a set of\nimages. The output of any NRR of such a set of images is a dense correspondence\nacross the whole set. Given such a dense correspondence, it is possible to\nbuild various generative statistical models of appearance variation across the\nset. We show that evaluating the quality of the registration can be mapped to\nthe problem of evaluating the quality of the resultant statistical model. The\nquality of the model entails a comparison between the model and the image data\nthat was used to construct it. It should be noted that this approach does not\ndepend on the specifics of the registration algorithm used (i.e., whether a\ngroupwise or pairwise algorithm was used to register the set of images), or on\nthe specifics of the modelling approach used.\n  We derive an index of image model specificity that can be used to assess\nimage model quality, and hence the quality of registration. This approach is\nvalidated by comparing our assessment of registration quality with that derived\nfrom ground truth anatomical labeling. We demonstrate that our approach is\ncapable of assessing NRR reliably without ground truth. Finally, to demonstrate\nthe practicality of our method, different NRR algorithms -- both pairwise and\ngroupwise -- are compared in terms of their performance on 3D MR brain data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:47:00 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Twining", "Carole J.", ""], ["Petrovi\u0107", "Vladimir S.", ""], ["Cootes", "Timothy F.", ""], ["Schestowitz", "Roy S.", ""], ["Crum", "William R.", ""], ["Taylor", "Christopher J.", ""]]}, {"id": "2002.10537", "submitter": "Ioannis Xarchakos", "authors": "Nick Koudas, Raymond Li, Ioannis Xarchakos", "title": "Video Monitoring Queries", "comments": "12 pages, 14 figures, to be published in International Conference in\n  Data Engineering (ICDE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in video processing utilizing deep learning primitives\nachieved breakthroughs in fundamental problems in video analysis such as frame\nclassification and object detection enabling an array of new applications.\n  In this paper we study the problem of interactive declarative query\nprocessing on video streams. In particular we introduce a set of approximate\nfilters to speed up queries that involve objects of specific type (e.g., cars,\ntrucks, etc.) on video frames with associated spatial relationships among them\n(e.g., car left of truck). The resulting filters are able to assess quickly if\nthe query predicates are true to proceed with further analysis of the frame or\notherwise not consider the frame further avoiding costly object detection\noperations.\n  We propose two classes of filters $IC$ and $OD$, that adapt principles from\ndeep image classification and object detection. The filters utilize extensible\ndeep neural architectures and are easy to deploy and utilize. In addition, we\npropose statistical query processing techniques to process aggregate queries\ninvolving objects with spatial constraints on video streams and demonstrate\nexperimentally the resulting increased accuracy on the resulting aggregate\nestimation.\n  Combined these techniques constitute a robust set of video monitoring query\nprocessing techniques. We demonstrate that the application of the techniques\nproposed in conjunction with declarative queries on video streams can\ndramatically increase the frame processing rate and speed up query processing\nby at least two orders of magnitude. We present the results of a thorough\nexperimental study utilizing benchmark video data sets at scale demonstrating\nthe performance benefits and the practical relevance of our proposals.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:53:35 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Koudas", "Nick", ""], ["Li", "Raymond", ""], ["Xarchakos", "Ioannis", ""]]}, {"id": "2002.10540", "submitter": "Sen Jia", "authors": "Sen Jia and Neil D.B. Bruce", "title": "Revisiting Saliency Metrics: Farthest-Neighbor Area Under Curve", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency detection has been widely studied because it plays an important role\nin various vision applications, but it is difficult to evaluate saliency\nsystems because each measure has its own bias. In this paper, we first revisit\nthe problem of applying the widely used saliency metrics on modern\nConvolutional Neural Networks(CNNs). Our investigation shows the saliency\ndatasets have been built based on different choices of parameters and CNNs are\ndesigned to fit a dataset-specific distribution. Secondly, we show that the\nShuffled Area Under Curve(S-AUC) metric still suffers from spatial biases. We\npropose a new saliency metric based on the AUC property, which aims at sampling\na more directional negative set for evaluation, denoted as Farthest-Neighbor\nAUC(FN-AUC). We also propose a strategy to measure the quality of the sampled\nnegative set. Our experiment shows FN-AUC can measure spatial biases, central\nand peripheral, more effectively than S-AUC without penalizing the fixation\nlocations. Thirdly, we propose a global smoothing function to overcome the\nproblem of few value degrees (output quantization) in computing AUC metrics.\nComparing with random noise, our smooth function can create unique values\nwithout losing the relative saliency relationship.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:55:42 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Jia", "Sen", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "2002.10549", "submitter": "Zhiyuan Li", "authors": "Zhiyuan Li, Jaideep Vitthal Murkute, Prashnna Kumar Gyawali and Linwei\n  Wang", "title": "Progressive Learning and Disentanglement of Hierarchical Representations", "comments": "Main text: 9 pages, 7 figures. Supplements: 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning rich representation from data is an important task for deep\ngenerative models such as variational auto-encoder (VAE). However, by\nextracting high-level abstractions in the bottom-up inference process, the goal\nof preserving all factors of variations for top-down generation is compromised.\nMotivated by the concept of \"starting small\", we present a strategy to\nprogressively learn independent hierarchical representations from high- to\nlow-levels of abstractions. The model starts with learning the most abstract\nrepresentation, and then progressively grow the network architecture to\nintroduce new representations at different levels of abstraction. We\nquantitatively demonstrate the ability of the presented model to improve\ndisentanglement in comparison to existing works on two benchmark data sets\nusing three disentanglement metrics, including a new metric we proposed to\ncomplement the previously-presented metric of mutual information gap. We\nfurther present both qualitative and quantitative evidence on how the\nprogression of learning improves disentangling of hierarchical representations.\nBy drawing on the respective advantage of hierarchical representation learning\nand progressive learning, this is to our knowledge the first attempt to improve\ndisentanglement by progressively growing the capacity of VAE to learn\nhierarchical representations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 21:19:38 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Li", "Zhiyuan", ""], ["Murkute", "Jaideep Vitthal", ""], ["Gyawali", "Prashnna Kumar", ""], ["Wang", "Linwei", ""]]}, {"id": "2002.10560", "submitter": "Ye Li", "authors": "Ye Li, Guangqiang Yin, Chunhui Liu, Xiaoyu Yang, Zhiguo Wang", "title": "Triplet Online Instance Matching Loss for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining the shared features of same identity in different scene, and the\nunique features of different identity in same scene, are most significant\nchallenges in the field of person re-identification (ReID). Online Instance\nMatching (OIM) loss function and Triplet loss function are main methods for\nperson ReID. Unfortunately, both of them have drawbacks. OIM loss treats all\nsamples equally and puts no emphasis on hard samples. Triplet loss processes\nbatch construction in a complicated and fussy way and converges slowly. For\nthese problems, we propose a Triplet Online Instance Matching (TOIM) loss\nfunction, which lays emphasis on the hard samples and improves the accuracy of\nperson ReID effectively. It combines the advantages of OIM loss and Triplet\nloss and simplifies the process of batch construction, which leads to a more\nrapid convergence. It can be trained on-line when handle the joint detection\nand identification task. To validate our loss function, we collect and annotate\na large-scale benchmark dataset (UESTC-PR) based on images taken from\nsurveillance cameras, which contains 499 identities and 60,437 images. We\nevaluated our proposed loss function on Duke, Marker-1501 and UESTC-PR using\nResNet-50, and the result shows that our proposed loss function outperforms the\nbaseline methods by a maximum of 21.7%, including Softmax loss, OIM loss and\nTriplet loss.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 21:55:56 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Li", "Ye", ""], ["Yin", "Guangqiang", ""], ["Liu", "Chunhui", ""], ["Yang", "Xiaoyu", ""], ["Wang", "Zhiguo", ""]]}, {"id": "2002.10570", "submitter": "Kailun Yang", "authors": "Lei Sun, Kailun Yang, Xinxin Hu, Weijian Hu and Kaiwei Wang", "title": "Real-time Fusion Network for RGB-D Semantic Segmentation Incorporating\n  Unexpected Obstacle Detection for Road-driving Images", "comments": "Accepted by IEEE Robotics and Automation Letters (RA-L); 8 Figures, 3\n  Tables; Code is available at https://github.com/AHupuJR/RFNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has made striking progress due to the success of deep\nconvolutional neural networks. Considering the demands of autonomous driving,\nreal-time semantic segmentation has become a research hotspot these years.\nHowever, few real-time RGB-D fusion semantic segmentation studies are carried\nout despite readily accessible depth information nowadays. In this paper, we\npropose a real-time fusion semantic segmentation network termed RFNet that\neffectively exploits complementary cross-modal information. Building on an\nefficient network architecture, RFNet is capable of running swiftly, which\nsatisfies autonomous vehicles applications. Multi-dataset training is leveraged\nto incorporate unexpected small obstacle detection, enriching the recognizable\nclasses required to face unforeseen hazards in the real world. A comprehensive\nset of experiments demonstrates the effectiveness of our framework. On\nCityscapes, Our method outperforms previous state-of-the-art semantic\nsegmenters, with excellent accuracy and 22Hz inference speed at the full\n2048x1024 resolution, outperforming most existing RGB-D networks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 22:17:25 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 14:29:00 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Sun", "Lei", ""], ["Yang", "Kailun", ""], ["Hu", "Xinxin", ""], ["Hu", "Weijian", ""], ["Wang", "Kaiwei", ""]]}, {"id": "2002.10591", "submitter": "Aniket Tolpadi", "authors": "Aniket A. Tolpadi, Jinhee J. Lee, Valentina Pedoia, Sharmila Majumdar", "title": "Deep learning predicts total knee replacement from magnetic resonance\n  images", "comments": "18 pages, 5 figures (4 in main article, 1 supplemental), 8 tables (5\n  in main article, 3 supplemental). Submitted to Scientific Reports and\n  currently in revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knee Osteoarthritis (OA) is a common musculoskeletal disorder in the United\nStates. When diagnosed at early stages, lifestyle interventions such as\nexercise and weight loss can slow OA progression, but at later stages, only an\ninvasive option is available: total knee replacement (TKR). Though a generally\nsuccessful procedure, only 2/3 of patients who undergo the procedure report\ntheir knees feeling ''normal'' post-operation, and complications can arise that\nrequire revision. This necessitates a model to identify a population at higher\nrisk of TKR, particularly at less advanced stages of OA, such that appropriate\ntreatments can be implemented that slow OA progression and delay TKR. Here, we\npresent a deep learning pipeline that leverages MRI images and clinical and\ndemographic information to predict TKR with AUC $0.834 \\pm 0.036$ (p < 0.05).\nMost notably, the pipeline predicts TKR with AUC $0.943 \\pm 0.057$ (p < 0.05)\nfor patients without OA. Furthermore, we develop occlusion maps for\ncase-control pairs in test data and compare regions used by the model in both,\nthereby identifying TKR imaging biomarkers. As such, this work takes strides\ntowards a pipeline with clinical utility, and the biomarkers identified further\nour understanding of OA progression and eventual TKR onset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 23:33:52 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Tolpadi", "Aniket A.", ""], ["Lee", "Jinhee J.", ""], ["Pedoia", "Valentina", ""], ["Majumdar", "Sharmila", ""]]}, {"id": "2002.10622", "submitter": "Han Wang", "authors": "Han Wang, Juncheng Li, Maopeng Ran and Lihua Xie", "title": "Fast Loop Closure Detection via Binary Content", "comments": "IEEE International Conference on Control and Automation (ICCA) 2019", "journal-ref": "2019 IEEE International Conference on Control and Automation\n  (ICCA)", "doi": "10.1109/ICCA.2019.8899937", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop closure detection plays an important role in reducing localization drift\nin Simultaneous Localization And Mapping (SLAM). It aims to find repetitive\nscenes from historical data to reset localization. To tackle the loop closure\nproblem, existing methods often leverage on the matching of visual features,\nwhich achieve good accuracy but require high computational resources. However,\nfeature point based methods ignore the patterns of image, i.e., the shape of\nthe objects as well as the distribution of objects in an image. It is believed\nthat this information is usually unique for a scene and can be utilized to\nimprove the performance of traditional loop closure detection methods. In this\npaper we leverage and compress the information into a binary image to\naccelerate an existing fast loop closure detection method via binary content.\nThe proposed method can greatly reduce the computational cost without\nsacrificing recall rate. It consists of three parts: binary content\nconstruction, fast image retrieval and precise loop closure detection. No\noffline training is required. Our method is compared with the state-of-the-art\nloop closure detection methods and the results show that it outperforms the\ntraditional methods at both recall rate and speed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 01:59:54 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 13:54:37 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Wang", "Han", ""], ["Li", "Juncheng", ""], ["Ran", "Maopeng", ""], ["Xie", "Lihua", ""]]}, {"id": "2002.10631", "submitter": "Pascal Poupart", "authors": "Amur Ghose, Abdullah Rashwan, Pascal Poupart", "title": "Batch norm with entropic regularization turns deterministic autoencoders\n  into generative models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational autoencoder is a well defined deep generative model that\nutilizes an encoder-decoder framework where an encoding neural network outputs\na non-deterministic code for reconstructing an input. The encoder achieves this\nby sampling from a distribution for every input, instead of outputting a\ndeterministic code per input. The great advantage of this process is that it\nallows the use of the network as a generative model for sampling from the data\ndistribution beyond provided samples for training. We show in this work that\nutilizing batch normalization as a source for non-determinism suffices to turn\ndeterministic autoencoders into generative models on par with variational ones,\nso long as we add a suitable entropic regularization to the training objective.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 02:42:18 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Ghose", "Amur", ""], ["Rashwan", "Abdullah", ""], ["Poupart", "Pascal", ""]]}, {"id": "2002.10638", "submitter": "Chunyuan Li", "authors": "Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao", "title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via\n  Pre-training", "comments": "To appear at CVPR 2020. The first two authors contributed equally to\n  this manuscript. Code: https://github.com/weituo12321/PREVALENT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to navigate in a visual environment following natural-language\ninstructions is a challenging task, because the multimodal inputs to the agent\nare highly variable, and the training data on a new task is often limited. In\nthis paper, we present the first pre-training and fine-tuning paradigm for\nvision-and-language navigation (VLN) tasks. By training on a large amount of\nimage-text-action triplets in a self-supervised learning manner, the\npre-trained model provides generic representations of visual environments and\nlanguage instructions. It can be easily used as a drop-in for existing VLN\nframeworks, leading to the proposed agent called Prevalent. It learns more\neffectively in new tasks and generalizes better in a previously unseen\nenvironment. The performance is validated on three VLN tasks. On the\nRoom-to-Room benchmark, our model improves the state-of-the-art from 47% to 51%\non success rate weighted by path length. Further, the learned representation is\ntransferable to other VLN tasks. On two recent tasks, vision-and-dialog\nnavigation and \"Help, Anna!\" the proposed Prevalent leads to significant\nimprovement over existing methods, achieving a new state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 03:08:12 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 03:20:31 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Hao", "Weituo", ""], ["Li", "Chunyuan", ""], ["Li", "Xiujun", ""], ["Carin", "Lawrence", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2002.10648", "submitter": "Haotao Wang", "authors": "Haotao Wang, Tianlong Chen, Zhangyang Wang and Kede Ma", "title": "I Am Going MAD: Maximum Discrepancy Competition for Comparing\n  Classifiers Adaptively", "comments": "Accepted to ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning of hierarchical representations for image classification has\nexperienced an impressive series of successes due in part to the availability\nof large-scale labeled data for training. On the other hand, the trained\nclassifiers have traditionally been evaluated on small and fixed sets of test\nimages, which are deemed to be extremely sparsely distributed in the space of\nall natural images. It is thus questionable whether recent performance\nimprovements on the excessively re-used test sets generalize to real-world\nnatural images with much richer content variations. Inspired by efficient\nstimulus selection for testing perceptual models in psychophysical and\nphysiological studies, we present an alternative framework for comparing image\nclassifiers, which we name the MAximum Discrepancy (MAD) competition. Rather\nthan comparing image classifiers using fixed test images, we adaptively sample\na small test set from an arbitrarily large corpus of unlabeled images so as to\nmaximize the discrepancies between the classifiers, measured by the distance\nover WordNet hierarchy. Human labeling on the resulting model-dependent image\nsets reveals the relative performance of the competing classifiers, and\nprovides useful insights on potential ways to improve them. We report the MAD\ncompetition results of eleven ImageNet classifiers while noting that the\nframework is readily extensible and cost-effective to add future classifiers\ninto the competition. Codes can be found at https://github.com/TAMU-VITA/MAD.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 03:32:29 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Wang", "Haotao", ""], ["Chen", "Tianlong", ""], ["Wang", "Zhangyang", ""], ["Ma", "Kede", ""]]}, {"id": "2002.10650", "submitter": "Yang Zhang", "authors": "Yang Zhang, Ivor Tsang, Yawei Luo, Changhui Hu, Xiaobo Lu, Xin Yu", "title": "Copy and Paste GAN: Face Hallucination from Shaded Thumbnails", "comments": "CVPR2020 (oral) preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing face hallucination methods based on convolutional neural networks\n(CNN) have achieved impressive performance on low-resolution (LR) faces in a\nnormal illumination condition. However, their performance degrades dramatically\nwhen LR faces are captured in low or non-uniform illumination conditions. This\npaper proposes a Copy and Paste Generative Adversarial Network (CPGAN) to\nrecover authentic high-resolution (HR) face images while compensating for low\nand non-uniform illumination. To this end, we develop two key components in our\nCPGAN: internal and external Copy and Paste nets (CPnets). Specifically, our\ninternal CPnet exploits facial information residing in the input image to\nenhance facial details; while our external CPnet leverages an external HR face\nfor illumination compensation. A new illumination compensation loss is thus\ndeveloped to capture illumination from the external guided face image\neffectively. Furthermore, our method offsets illumination and upsamples facial\ndetails alternately in a coarse-to-fine fashion, thus alleviating the\ncorrespondence ambiguity between LR inputs and external HR inputs. Extensive\nexperiments demonstrate that our method manifests authentic HR face images in a\nuniform illumination condition and outperforms state-of-the-art methods\nqualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 03:34:58 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 04:37:20 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 07:31:34 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Zhang", "Yang", ""], ["Tsang", "Ivor", ""], ["Luo", "Yawei", ""], ["Hu", "Changhui", ""], ["Lu", "Xiaobo", ""], ["Yu", "Xin", ""]]}, {"id": "2002.10651", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu, Chia-Ju Chen, Li-Heng Chen, Neil Birkbeck, Balu\n  Adsumilli, and Alan C. Bovik", "title": "A Comparative Evaluation of Temporal Pooling Methods for Blind Video\n  Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many objective video quality assessment (VQA) algorithms include a key step\nof temporal pooling of frame-level quality scores. However, less attention has\nbeen paid to studying the relative efficiencies of different pooling methods on\nno-reference (blind) VQA. Here we conduct a large-scale comparative evaluation\nto assess the capabilities and limitations of multiple temporal pooling\nstrategies on blind VQA of user-generated videos. The study yields insights and\ngeneral guidance regarding the application and selection of temporal pooling\nmodels. In addition, we also propose an ensemble pooling model built on top of\nhigh-performing temporal pooling models. Our experimental results demonstrate\nthe relative efficacies of the evaluated temporal pooling models, using several\npopular VQA algorithms, and evaluated on two recent large-scale natural video\nquality databases. In addition to the new ensemble model, we provide a general\nrecipe for applying temporal pooling of frame-based quality predictions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 03:35:06 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Tu", "Zhengzhong", ""], ["Chen", "Chia-Ju", ""], ["Chen", "Li-Heng", ""], ["Birkbeck", "Neil", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2002.10657", "submitter": "Satrajit Chatterjee", "authors": "Satrajit Chatterjee", "title": "Coherent Gradients: An Approach to Understanding Generalization in\n  Gradient Descent-based Optimization", "comments": "To appear in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open question in the Deep Learning community is why neural networks\ntrained with Gradient Descent generalize well on real datasets even though they\nare capable of fitting random data. We propose an approach to answering this\nquestion based on a hypothesis about the dynamics of gradient descent that we\ncall Coherent Gradients: Gradients from similar examples are similar and so the\noverall gradient is stronger in certain directions where these reinforce each\nother. Thus changes to the network parameters during training are biased\ntowards those that (locally) simultaneously benefit many examples when such\nsimilarity exists. We support this hypothesis with heuristic arguments and\nperturbative experiments and outline how this can explain several common\nempirical observations about Deep Learning. Furthermore, our analysis is not\njust descriptive, but prescriptive. It suggests a natural modification to\ngradient descent that can greatly reduce overfitting.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 03:59:31 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Chatterjee", "Satrajit", ""]]}, {"id": "2002.10675", "submitter": "Fangbo Qin", "authors": "Fangbo Qin, Shan Lin, Yangming Li, Randall A. Bly, Kris S. Moe, Blake\n  Hannaford", "title": "Towards Better Surgical Instrument Segmentation in Endoscopic Vision:\n  Multi-Angle Feature Aggregation and Contour Supervision", "comments": "Accepted by IEEE Robotics and Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2020.3009073", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and real-time surgical instrument segmentation is important in the\nendoscopic vision of robot-assisted surgery, and significant challenges are\nposed by frequent instrument-tissue contacts and continuous change of\nobservation perspective. For these challenging tasks more and more deep neural\nnetworks (DNN) models are designed in recent years. We are motivated to propose\na general embeddable approach to improve these current DNN segmentation models\nwithout increasing the model parameter number. Firstly, observing the limited\nrotation-invariance performance of DNN, we proposed the Multi-Angle Feature\nAggregation (MAFA) method, leveraging active image rotation to gain richer\nvisual cues and make the prediction more robust to instrument orientation\nchanges. Secondly, in the end-to-end training stage, the auxiliary contour\nsupervision is utilized to guide the model to learn the boundary awareness, so\nthat the contour shape of segmentation mask is more precise. The proposed\nmethod is validated with ablation experiments on the novel Sinus-Surgery\ndatasets collected from surgeons' operations, and is compared to the existing\nmethods on a public dataset collected with a da Vinci Xi Robot.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 05:28:46 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 03:20:35 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Qin", "Fangbo", ""], ["Lin", "Shan", ""], ["Li", "Yangming", ""], ["Bly", "Randall A.", ""], ["Moe", "Kris S.", ""], ["Hannaford", "Blake", ""]]}, {"id": "2002.10686", "submitter": "Daqi Liu", "authors": "Daqi Liu, \\'Alvaro Parra, Tat-Jun Chin", "title": "Globally Optimal Contrast Maximisation for Event-based Motion Estimation", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrast maximisation estimates the motion captured in an event stream by\nmaximising the sharpness of the motion compensated event image. To carry out\ncontrast maximisation, many previous works employ iterative optimisation\nalgorithms, such as conjugate gradient, which require good initialisation to\navoid converging to bad local minima. To alleviate this weakness, we propose a\nnew globally optimal event-based motion estimation algorithm. Based on\nbranch-and-bound (BnB), our method solves rotational (3DoF) motion estimation\non event streams, which supports practical applications such as video\nstabilisation and attitude estimation. Underpinning our method are novel\nbounding functions for contrast maximisation, whose theoretical validity is\nrigorously established. We show concrete examples from public datasets where\nglobally optimal solutions are vital to the success of contrast maximisation.\nDespite its exact nature, our algorithm is currently able to process a 50,000\nevent input in 300 seconds (a locally optimal solver takes 30 seconds on the\nsame input), and has the potential to be further speeded-up using GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 05:54:29 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 01:43:08 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 01:06:05 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Liu", "Daqi", ""], ["Parra", "\u00c1lvaro", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "2002.10695", "submitter": "Hung Le", "authors": "Hung Le, Nancy F. Chen", "title": "Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge", "comments": "Accepted at DSTC Workshop at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-Visual Scene-Aware Dialog (AVSD) is an extension from Video Question\nAnswering (QA) whereby the dialogue agent is required to generate natural\nlanguage responses to address user queries and carry on conversations. This is\na challenging task as it consists of video features of multiple modalities,\nincluding text, visual, and audio features. The agent also needs to learn\nsemantic dependencies among user utterances and system responses to make\ncoherent conversations with humans. In this work, we describe our submission to\nthe AVSD track of the 8th Dialogue System Technology Challenge. We adopt\ndot-product attention to combine text and non-text features of input video. We\nfurther enhance the generation capability of the dialogue agent by adopting\npointer networks to point to tokens from multiple source sequences in each\ngeneration step. Our systems achieve high performance in automatic metrics and\nobtain 5th and 6th place in human evaluation among all submissions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 06:41:07 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Le", "Hung", ""], ["Chen", "Nancy F.", ""]]}, {"id": "2002.10698", "submitter": "Thao Minh Le", "authors": "Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran", "title": "Hierarchical Conditional Relation Networks for Video Question Answering", "comments": "Check out our code on GitHub at\n  https://github.com/thaolmk54/hcrn-videoqa", "journal-ref": "CVPR 2020, Oral", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video question answering (VideoQA) is challenging as it requires modeling\ncapacity to distill dynamic visual artifacts and distant relations and to\nassociate them with linguistic concepts. We introduce a general-purpose\nreusable neural unit called Conditional Relation Network (CRN) that serves as a\nbuilding block to construct more sophisticated structures for representation\nand reasoning over video. CRN takes as input an array of tensorial objects and\na conditioning feature, and computes an array of encoded output objects. Model\nbuilding becomes a simple exercise of replication, rearrangement and stacking\nof these reusable units for diverse modalities and contextual information. This\ndesign thus supports high-order relational and multi-step reasoning. The\nresulting architecture for VideoQA is a CRN hierarchy whose branches represent\nsub-videos or clips, all sharing the same question as the contextual condition.\nOur evaluations on well-known datasets achieved new SoTA results, demonstrating\nthe impact of building a general-purpose reasoning unit on complex domains such\nas VideoQA.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 07:00:48 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 13:03:25 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 08:32:45 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Le", "Thao Minh", ""], ["Le", "Vuong", ""], ["Venkatesh", "Svetha", ""], ["Tran", "Truyen", ""]]}, {"id": "2002.10701", "submitter": "Yiqun Lin", "authors": "Yiqun Lin, Zizheng Yan, Haibin Huang, Dong Du, Ligang Liu, Shuguang\n  Cui and Xiaoguang Han", "title": "FPConv: Learning Local Flattening for Point Convolution", "comments": "Camera-ready, accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce FPConv, a novel surface-style convolution operator designed for\n3D point cloud analysis. Unlike previous methods, FPConv doesn't require\ntransforming to intermediate representation like 3D grid or graph and directly\nworks on surface geometry of point cloud. To be more specific, for each point,\nFPConv performs a local flattening by automatically learning a weight map to\nsoftly project surrounding points onto a 2D grid. Regular 2D convolution can\nthus be applied for efficient feature learning. FPConv can be easily integrated\ninto various network architectures for tasks like 3D object classification and\n3D scene segmentation, and achieve comparable performance with existing\nvolumetric-type convolutions. More importantly, our experiments also show that\nFPConv can be a complementary of volumetric convolutions and jointly training\nthem can further boost overall performance into state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 07:15:08 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 11:45:50 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2020 04:13:13 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lin", "Yiqun", ""], ["Yan", "Zizheng", ""], ["Huang", "Haibin", ""], ["Du", "Dong", ""], ["Liu", "Ligang", ""], ["Cui", "Shuguang", ""], ["Han", "Xiaoguang", ""]]}, {"id": "2002.10703", "submitter": "Xiaodong Qi", "authors": "Xiaodong Qi, Lansheng Han", "title": "G\\\"odel's Sentence Is An Adversarial Example But Unsolvable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, different types of adversarial examples from different\nfields have emerged endlessly, including purely natural ones without\nperturbations. A variety of defenses are proposed and then broken quickly. Two\nfundamental questions need to be asked: What's the reason for the existence of\nadversarial examples and are adversarial examples unsolvable? In this paper, we\nwill show the reason for the existence of adversarial examples is there are\nnon-isomorphic natural explanations that can all explain data set.\nSpecifically, for two natural explanations of being true and provable,\nG\\\"odel's sentence is an adversarial example but ineliminable. It can't be\nsolved by the re-accumulation of data set or the re-improvement of learning\nalgorithm. Finally, from the perspective of computability, we will prove the\nincomputability for adversarial examples, which are unrecognizable.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 07:20:17 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Qi", "Xiaodong", ""], ["Han", "Lansheng", ""]]}, {"id": "2002.10711", "submitter": "Javier Fernandez-Marques", "authors": "Javier Fernandez-Marques, Paul N. Whatmough, Andrew Mundy, Matthew\n  Mattina", "title": "Searching for Winograd-aware Quantized Networks", "comments": "Published as a conference paper at MLSys 2020", "journal-ref": "Proceedings of Machine Learning and Systems (2020), 14-29", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lightweight architectural designs of Convolutional Neural Networks (CNNs)\ntogether with quantization have paved the way for the deployment of demanding\ncomputer vision applications on mobile devices. Parallel to this, alternative\nformulations to the convolution operation such as FFT, Strassen and Winograd,\nhave been adapted for use in CNNs offering further speedups. Winograd\nconvolutions are the fastest known algorithm for spatially small convolutions,\nbut exploiting their full potential comes with the burden of numerical error,\nrendering them unusable in quantized contexts. In this work we propose a\nWinograd-aware formulation of convolution layers which exposes the numerical\ninaccuracies introduced by the Winograd transformations to the learning of the\nmodel parameters, enabling the design of competitive quantized models without\nimpacting model size. We also address the source of the numerical error and\npropose a relaxation on the form of the transformation matrices, resulting in\nup to 10% higher classification accuracy on CIFAR-10. Finally, we propose\nwiNAS, a neural architecture search (NAS) framework that jointly optimizes a\ngiven macro-architecture for accuracy and latency leveraging Winograd-aware\nlayers. A Winograd-aware ResNet-18 optimized with wiNAS for CIFAR-10 results in\n2.66x speedup compared to im2row, one of the most widely used optimized\nconvolution implementations, with no loss in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 07:53:53 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Fernandez-Marques", "Javier", ""], ["Whatmough", "Paul N.", ""], ["Mundy", "Andrew", ""], ["Mattina", "Matthew", ""]]}, {"id": "2002.10727", "submitter": "Iwan Paolucci", "authors": "Iwan Paolucci", "title": "Technical report: Kidney tumor segmentation using a 2D U-Net followed by\n  a statistical post-processing filter", "comments": "KiTS 2019 challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, there are about 400'000 new cases of kidney cancer worldwide\ncausing around 175'000 deaths. For clinical decision making it is important to\nunderstand the morphometry of the tumor, which involves the time-consuming task\nof delineating tumor and kidney in 3D CT images. Automatic segmentation could\nbe an important tool for clinicians and researchers to also study the\ncorrelations between tumor morphometry and clinical outcomes. We present a\nsegmentation method which combines the popular U-Net convolutional neural\nnetwork architecture with post-processing based on statistical constraints of\nthe available training data. The full implementation, based on PyTorch, and the\ntrained weights can be found on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 08:25:33 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Paolucci", "Iwan", ""]]}, {"id": "2002.10733", "submitter": "Alexander Levine", "authors": "Alexander Levine, Soheil Feizi", "title": "(De)Randomized Smoothing for Certifiable Defense against Patch Attacks", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patch adversarial attacks on images, in which the attacker can distort pixels\nwithin a region of bounded size, are an important threat model since they\nprovide a quantitative model for physical adversarial attacks. In this paper,\nwe introduce a certifiable defense against patch attacks that guarantees for a\ngiven image and patch attack size, no patch adversarial examples exist. Our\nmethod is related to the broad class of randomized smoothing robustness schemes\nwhich provide high-confidence probabilistic robustness certificates. By\nexploiting the fact that patch attacks are more constrained than general sparse\nattacks, we derive meaningfully large robustness certificates against them.\nAdditionally, in contrast to smoothing-based defenses against L_p and sparse\nattacks, our defense method against patch attacks is de-randomized, yielding\nimproved, deterministic certificates. Compared to the existing patch\ncertification method proposed by Chiang et al. (2020), which relies on interval\nbound propagation, our method can be trained significantly faster, achieves\nhigh clean and certified robust accuracy on CIFAR-10, and provides certificates\nat ImageNet scale. For example, for a 5-by-5 patch attack on CIFAR-10, our\nmethod achieves up to around 57.6% certified accuracy (with a classifier with\naround 83.8% clean accuracy), compared to at most 30.3% certified accuracy for\nthe existing method (with a classifier with around 47.8% clean accuracy). Our\nresults effectively establish a new state-of-the-art of certifiable defense\nagainst patch attacks on CIFAR-10 and ImageNet. Code is available at\nhttps://github.com/alevine0/patchSmoothing.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 08:39:46 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 19:09:10 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 06:36:56 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Levine", "Alexander", ""], ["Feizi", "Soheil", ""]]}, {"id": "2002.10749", "submitter": "Kazuya Nishimura", "authors": "Junya Hayashida and Kazuya Nishimura and Ryoma Bise", "title": "MPM: Joint Representation of Motion and Position Map for Cell Tracking", "comments": "8 pages, 11 figures, Accepted in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional cell tracking methods detect multiple cells in each frame\n(detection) and then associate the detection results in successive time-frames\n(association). Most cell tracking methods perform the association task\nindependently from the detection task. However, there is no guarantee of\npreserving coherence between these tasks, and lack of coherence may adversely\naffect tracking performance. In this paper, we propose the Motion and Position\nMap (MPM) that jointly represents both detection and association for not only\nmigration but also cell division. It guarantees coherence such that if a cell\nis detected, the corresponding motion flow can always be obtained. It is a\nsimple but powerful method for multi-object tracking in dense environments. We\ncompared the proposed method with current tracking methods under various\nconditions in real biological images and found that it outperformed the\nstate-of-the-art (+5.2\\% improvement compared to the second-best).\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 09:06:55 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 12:41:58 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Hayashida", "Junya", ""], ["Nishimura", "Kazuya", ""], ["Bise", "Ryoma", ""]]}, {"id": "2002.10770", "submitter": "Aviram Bar-Haim", "authors": "Aviram Bar-Haim, Lior Wolf", "title": "ScopeFlow: Dynamic Scene Scoping for Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to modify the common training protocols of optical flow, leading\nto sizable accuracy improvements without adding to the computational complexity\nof the training process. The improvement is based on observing the bias in\nsampling challenging data that exists in the current training protocol, and\nimproving the sampling process. In addition, we find that both regularization\nand augmentation should decrease during the training protocol.\n  Using an existing low parameters architecture, the method is ranked first on\nthe MPI Sintel benchmark among all other methods, improving the best two frames\nmethod accuracy by more than 10%. The method also surpasses all similar\narchitecture variants by more than 12% and 19.7% on the KITTI benchmarks,\nachieving the lowest Average End-Point Error on KITTI2012 among two-frame\nmethods, without using extra datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 09:58:49 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 08:19:29 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bar-Haim", "Aviram", ""], ["Wolf", "Lior", ""]]}, {"id": "2002.10776", "submitter": "Sven Koitka", "authors": "Sven Koitka, Lennard Kroll, Eugen Malamutmann, Arzu Oezcelik, Felix\n  Nensa", "title": "Fully-automated Body Composition Analysis in Routine CT Imaging Using 3D\n  Semantic Segmentation Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1007/s00330-020-07147-3", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body tissue composition is a long-known biomarker with high diagnostic and\nprognostic value in cardiovascular, oncological and orthopaedic diseases, but\nalso in rehabilitation medicine or drug dosage. In this study, the aim was to\ndevelop a fully automated, reproducible and quantitative 3D volumetry of body\ntissue composition from standard CT examinations of the abdomen in order to be\nable to offer such valuable biomarkers as part of routine clinical imaging.\nTherefore an in-house dataset of 40 CTs for training and 10 CTs for testing\nwere fully annotated on every fifth axial slice with five different semantic\nbody regions: abdominal cavity, bones, muscle, subcutaneous tissue, and\nthoracic cavity. Multi-resolution U-Net 3D neural networks were employed for\nsegmenting these body regions, followed by subclassifying adipose tissue and\nmuscle using known hounsfield unit limits. The S{\\o}rensen Dice scores averaged\nover all semantic regions was 0.9553 and the intra-class correlation\ncoefficients for subclassified tissues were above 0.99. Our results show that\nfully-automated body composition analysis on routine CT imaging can provide\nstable biomarkers across the whole abdomen and not just on L3 slices, which is\nhistorically the reference location for analysing body composition in the\nclinical routine.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 10:17:19 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Koitka", "Sven", ""], ["Kroll", "Lennard", ""], ["Malamutmann", "Eugen", ""], ["Oezcelik", "Arzu", ""], ["Nensa", "Felix", ""]]}, {"id": "2002.10801", "submitter": "Lei Huang", "authors": "Lei Huang, Jie Qin, Li Liu, Fan Zhu, Ling Shao", "title": "Layer-wise Conditioning Analysis in Exploring the Learning Dynamics of\n  DNNs", "comments": "Accepted to ECCV 2020. The code is available at:\n  https://github.com/huangleiBuaa/LayerwiseCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditioning analysis uncovers the landscape of an optimization objective by\nexploring the spectrum of its curvature matrix. This has been well explored\ntheoretically for linear models. We extend this analysis to deep neural\nnetworks (DNNs) in order to investigate their learning dynamics. To this end,\nwe propose layer-wise conditioning analysis, which explores the optimization\nlandscape with respect to each layer independently. Such an analysis is\ntheoretically supported under mild assumptions that approximately hold in\npractice. Based on our analysis, we show that batch normalization (BN) can\nstabilize the training, but sometimes result in the false impression of a local\nminimum, which has detrimental effects on the learning. Besides, we\nexperimentally observe that BN can improve the layer-wise conditioning of the\noptimization problem. Finally, we find that the last linear layer of a very\ndeep residual network displays ill-conditioned behavior. We solve this problem\nby only adding one BN layer before the last linear layer, which achieves\nimproved performance over the original and pre-activation residual networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 11:40:27 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 11:21:06 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 13:30:54 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Huang", "Lei", ""], ["Qin", "Jie", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Shao", "Ling", ""]]}, {"id": "2002.10826", "submitter": "Jialun Liu", "authors": "Jialun Liu, Yifan Sun, Chuchu Han, Zhaopeng Dou, Wenhui Li", "title": "Deep Representation Learning on Long-tailed Data: A Learnable Embedding\n  Augmentation Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers learning deep features from long-tailed data. We observe\nthat in the deep feature space, the head classes and the tail classes present\ndifferent distribution patterns. The head classes have a relatively large\nspatial span, while the tail classes have significantly small spatial span, due\nto the lack of intra-class diversity. This uneven distribution between head and\ntail classes distorts the overall feature space, which compromises the\ndiscriminative ability of the learned features. Intuitively, we seek to expand\nthe distribution of the tail classes by transferring from the head classes, so\nas to alleviate the distortion of the feature space. To this end, we propose to\nconstruct each feature into a \"feature cloud\". If a sample belongs to a tail\nclass, the corresponding feature cloud will have relatively large distribution\nrange, in compensation to its lack of diversity. It allows each tail sample to\npush the samples from other classes far away, recovering the intra-class\ndiversity of tail classes. Extensive experimental evaluations on person\nre-identification and face recognition tasks confirm the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 12:38:32 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 13:19:20 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 16:32:33 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Liu", "Jialun", ""], ["Sun", "Yifan", ""], ["Han", "Chuchu", ""], ["Dou", "Zhaopeng", ""], ["Li", "Wenhui", ""]]}, {"id": "2002.10832", "submitter": "Jacopo Staiano", "authors": "Thomas Scialom, Patrick Bordes, Paul-Alexis Dray, Jacopo Staiano,\n  Patrick Gallinari", "title": "What BERT Sees: Cross-Modal Transfer for Visual Question Generation", "comments": "INLG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models have recently contributed to significant advances\nin NLP tasks. Recently, multi-modal versions of BERT have been developed, using\nheavy pre-training relying on vast corpora of aligned textual and image data,\nprimarily applied to classification tasks such as VQA. In this paper, we are\ninterested in evaluating the visual capabilities of BERT out-of-the-box, by\navoiding pre-training made on supplementary data. We choose to study Visual\nQuestion Generation, a task of great interest for grounded dialog, that enables\nto study the impact of each modality (as input can be visual and/or textual).\nMoreover, the generation aspect of the task requires an adaptation since BERT\nis primarily designed as an encoder. We introduce BERT-gen, a BERT-based\narchitecture for text generation, able to leverage on either mono- or multi-\nmodal representations. The results reported under different configurations\nindicate an innate capacity for BERT-gen to adapt to multi-modal data and text\ngeneration, even with few data available, avoiding expensive pre-training. The\nproposed model obtains substantial improvements over the state-of-the-art on\ntwo established VQG datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 12:44:36 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 13:07:57 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 15:48:35 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Scialom", "Thomas", ""], ["Bordes", "Patrick", ""], ["Dray", "Paul-Alexis", ""], ["Staiano", "Jacopo", ""], ["Gallinari", "Patrick", ""]]}, {"id": "2002.10836", "submitter": "Eran Hof", "authors": "Eran Hof, Amichai Sanderovich, Evyatar Hemo", "title": "Gesture recognition with 60GHz 802.11 waveforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture recognition application over 802.11 ad/y waveforms is developed.\nSimultaneous gestures of slider-control and two-finger gesture for switching\nare detected based on Golay sequences of channel estimation fields of the\npackets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 12:49:45 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Hof", "Eran", ""], ["Sanderovich", "Amichai", ""], ["Hemo", "Evyatar", ""]]}, {"id": "2002.10838", "submitter": "Amit Dekel", "authors": "Amit Dekel, Linus H\\\"arenstam-Nielsen, Sergio Caccamo", "title": "Optimal least-squares solution to the hand-eye calibration problem", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a least-squares formulation to the noisy hand-eye calibration\nproblem using dual-quaternions, and introduce efficient algorithms to find the\nexact optimal solution, based on analytic properties of the problem, avoiding\nnon-linear optimization. We further present simple analytic approximate\nsolutions which provide remarkably good estimations compared to the exact\nsolution. In addition, we show how to generalize our solution to account for a\ngiven extrinsic prior in the cost function. To the best of our knowledge our\nalgorithm is the most efficient approach to optimally solve the hand-eye\ncalibration problem.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 12:59:06 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 15:14:39 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Dekel", "Amit", ""], ["H\u00e4renstam-Nielsen", "Linus", ""], ["Caccamo", "Sergio", ""]]}, {"id": "2002.10857", "submitter": "Sun Yifan", "authors": "Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng,\n  Zhongdao Wang, Yichen Wei", "title": "Circle Loss: A Unified Perspective of Pair Similarity Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a pair similarity optimization viewpoint on deep feature\nlearning, aiming to maximize the within-class similarity $s_p$ and minimize the\nbetween-class similarity $s_n$. We find a majority of loss functions, including\nthe triplet loss and the softmax plus cross-entropy loss, embed $s_n$ and $s_p$\ninto similarity pairs and seek to reduce $(s_n-s_p)$. Such an optimization\nmanner is inflexible, because the penalty strength on every single similarity\nscore is restricted to be equal. Our intuition is that if a similarity score\ndeviates far from the optimum, it should be emphasized. To this end, we simply\nre-weight each similarity to highlight the less-optimized similarity scores. It\nresults in a Circle loss, which is named due to its circular decision boundary.\nThe Circle loss has a unified formula for two elemental deep feature learning\napproaches, i.e. learning with class-level labels and pair-wise labels.\nAnalytically, we show that the Circle loss offers a more flexible optimization\napproach towards a more definite convergence target, compared with the loss\nfunctions optimizing $(s_n-s_p)$. Experimentally, we demonstrate the\nsuperiority of the Circle loss on a variety of deep feature learning tasks. On\nface recognition, person re-identification, as well as several fine-grained\nimage retrieval datasets, the achieved performance is on par with the state of\nthe art.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 13:56:40 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 08:15:47 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Sun", "Yifan", ""], ["Cheng", "Changmao", ""], ["Zhang", "Yuhan", ""], ["Zhang", "Chi", ""], ["Zheng", "Liang", ""], ["Wang", "Zhongdao", ""], ["Wei", "Yichen", ""]]}, {"id": "2002.10864", "submitter": "Zun Li", "authors": "Zun Li, Congyan Lang, Junhao Liew, Qibin Hou, Yidong Li, Jiashi Feng", "title": "Cross-layer Feature Pyramid Network for Salient Object Detection", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TIP.2021.3072811", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature pyramid network (FPN) based models, which fuse the semantics and\nsalient details in a progressive manner, have been proven highly effective in\nsalient object detection. However, it is observed that these models often\ngenerate saliency maps with incomplete object structures or unclear object\nboundaries, due to the \\emph{indirect} information propagation among distant\nlayers that makes such fusion structure less effective. In this work, we\npropose a novel Cross-layer Feature Pyramid Network (CFPN), in which direct\ncross-layer communication is enabled to improve the progressive fusion in\nsalient object detection. Specifically, the proposed network first aggregates\nmulti-scale features from different layers into feature maps that have access\nto both the high- and low-level information. Then, it distributes the\naggregated features to all the involved layers to gain access to richer\ncontext. In this way, the distributed features per layer own both semantics and\nsalient details from all other layers simultaneously, and suffer reduced loss\nof important information. Extensive experimental results over six widely used\nsalient object detection benchmarks and with three popular backbones clearly\ndemonstrate that CFPN can accurately locate fairly complete salient regions and\neffectively segment the object boundaries.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:06:27 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Li", "Zun", ""], ["Lang", "Congyan", ""], ["Liew", "Junhao", ""], ["Hou", "Qibin", ""], ["Li", "Yidong", ""], ["Feng", "Jiashi", ""]]}, {"id": "2002.10876", "submitter": "Ruihui Li", "authors": "Ruihui Li, Xianzhi Li, Pheng-Ann Heng, Chi-Wing Fu", "title": "PointAugment: an Auto-Augmentation Framework for Point Cloud\n  Classification", "comments": "Camera-Ready Version for CVPR 2020 (Oral); code is\n  https://github.com/liruihui/PointAugment/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PointAugment, a new auto-augmentation framework that automatically\noptimizes and augments point cloud samples to enrich the data diversity when we\ntrain a classification network. Different from existing auto-augmentation\nmethods for 2D images, PointAugment is sample-aware and takes an adversarial\nlearning strategy to jointly optimize an augmentor network and a classifier\nnetwork, such that the augmentor can learn to produce augmented samples that\nbest fit the classifier. Moreover, we formulate a learnable point augmentation\nfunction with a shape-wise transformation and a point-wise displacement, and\ncarefully design loss functions to adopt the augmented samples based on the\nlearning progress of the classifier. Extensive experiments also confirm\nPointAugment's effectiveness and robustness to improve the performance of\nvarious networks on shape classification and retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:25:01 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 02:56:33 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Li", "Ruihui", ""], ["Li", "Xianzhi", ""], ["Heng", "Pheng-Ann", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "2002.10880", "submitter": "Charlie Nash", "authors": "Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, Peter W. Battaglia", "title": "PolyGen: An Autoregressive Generative Model of 3D Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polygon meshes are an efficient representation of 3D geometry, and are of\ncentral importance in computer graphics, robotics and games development.\nExisting learning-based approaches have avoided the challenges of working with\n3D meshes, instead using alternative object representations that are more\ncompatible with neural architectures and training approaches. We present an\napproach which models the mesh directly, predicting mesh vertices and faces\nsequentially using a Transformer-based architecture. Our model can condition on\na range of inputs, including object classes, voxels, and images, and because\nthe model is probabilistic it can produce samples that capture uncertainty in\nambiguous scenarios. We show that the model is capable of producing\nhigh-quality, usable meshes, and establish log-likelihood benchmarks for the\nmesh-modelling task. We also evaluate the conditional models on surface\nreconstruction metrics against alternative methods, and demonstrate competitive\nperformance despite not training directly on this task.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 17:16:34 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Nash", "Charlie", ""], ["Ganin", "Yaroslav", ""], ["Eslami", "S. M. Ali", ""], ["Battaglia", "Peter W.", ""]]}, {"id": "2002.10893", "submitter": "I\\~nigo Alonso", "authors": "I\\~nigo Alonso, Luis Riazuelo, Luis Montesano, Ana C. Murillo", "title": "3D-MiniNet: Learning a 2D Representation from Point Clouds for Fast and\n  Efficient 3D LIDAR Semantic Segmentation", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LIDAR semantic segmentation, which assigns a semantic label to each 3D point\nmeasured by the LIDAR, is becoming an essential task for many robotic\napplications such as autonomous driving. Fast and efficient semantic\nsegmentation methods are needed to match the strong computational and temporal\nrestrictions of many of these real-world applications.\n  This work presents 3D-MiniNet, a novel approach for LIDAR semantic\nsegmentation that combines 3D and 2D learning layers. It first learns a 2D\nrepresentation from the raw points through a novel projection which extracts\nlocal and global information from the 3D data. This representation is fed to an\nefficient 2D Fully Convolutional Neural Network (FCNN) that produces a 2D\nsemantic segmentation. These 2D semantic labels are re-projected back to the 3D\nspace and enhanced through a post-processing module. The main novelty in our\nstrategy relies on the projection learning module. Our detailed ablation study\nshows how each component contributes to the final performance of 3D-MiniNet. We\nvalidate our approach on well known public benchmarks (SemanticKITTI and\nKITTI), where 3D-MiniNet gets state-of-the-art results while being faster and\nmore parameter-efficient than previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:33:50 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 12:03:36 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2020 11:32:23 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 12:23:08 GMT"}, {"version": "v5", "created": "Tue, 27 Apr 2021 15:31:54 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Alonso", "I\u00f1igo", ""], ["Riazuelo", "Luis", ""], ["Montesano", "Luis", ""], ["Murillo", "Ana C.", ""]]}, {"id": "2002.10905", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Yao Rong, Enkelejda Kasneci", "title": "Fully Convolutional Neural Networks for Raw Eye Tracking Data\n  Segmentation, Generation, and Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use fully convolutional neural networks for the semantic\nsegmentation of eye tracking data. We also use these networks for\nreconstruction, and in conjunction with a variational auto-encoder to generate\neye movement data. The first improvement of our approach is that no input\nwindow is necessary, due to the use of fully convolutional networks and\ntherefore any input size can be processed directly. The second improvement is\nthat the used and generated data is raw eye tracking data (position X, Y and\ntime) without preprocessing. This is achieved by pre-initializing the filters\nin the first layer and by building the input tensor along the z axis. We\nevaluated our approach on three publicly available datasets and compare the\nresults to the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 06:57:09 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 07:13:46 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 12:22:08 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Rong", "Yao", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2002.10938", "submitter": "Ziyuan Liu", "authors": "Ziyuan Liu, Georg von Wichert", "title": "Applying Rule-Based Context Knowledge to Build Abstract Semantic Maps of\n  Indoor Environments", "comments": "arXiv admin note: text overlap with arXiv:2002.08402", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generalizable method that systematically combines\ndata driven MCMC samplingand inference using rule-based context knowledge for\ndata abstraction. In particular, we demonstrate the usefulness of our method in\nthe scenario of building abstract semantic maps for indoor environments. The\nproduct of our system is a parametric abstract model of the perceived\nenvironment that not only accurately represents the geometry of the environment\nbut also provides valuable abstract information which benefits high-level\nrobotic applications. Based on predefined abstract terms,such as type and\nrelation, we define task-specific context knowledge as descriptive rules in\nMarkov Logic Networks. The corresponding inference results are used to\nconstruct a priordistribution that aims to add reasonable constraints to the\nsolution space of semantic maps. In addition, by applying a semantically\nannotated sensor model, we explicitly use context information to interpret the\nsensor data. Experiments on real world data show promising results and thus\nconfirm the usefulness of our system.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 20:56:02 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Liu", "Ziyuan", ""], ["von Wichert", "Georg", ""]]}, {"id": "2002.10939", "submitter": "Ziyuan Liu", "authors": "Ziyuan Liu, Dong Chen, Georg von Wichert", "title": "Online Semantic Exploration of Indoor Maps", "comments": "arXiv admin note: substantial text overlap with arXiv:2002.08348", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a method to extract an abstracted floor plan from\ntypical grid maps using Bayesian reasoning. The result of this procedure is a\nprobabilistic generative model of the environment defined over abstract\nconcepts. It is well suited for higher-level reasoning and communication\npurposes. We demonstrate the effectiveness of the approach through real-world\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 21:07:28 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Liu", "Ziyuan", ""], ["Chen", "Dong", ""], ["von Wichert", "Georg", ""]]}, {"id": "2002.10945", "submitter": "Ignacio Garcia Dorado", "authors": "Ignacio Garcia-Dorado, Pascal Getreuer, Bartlomiej Wronski, Peyman\n  Milanfar", "title": "Image Stylization: From Predefined to Personalized", "comments": "14 pages, 22 figures. arXiv admin note: text overlap with\n  arXiv:1712.06654", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a framework for interactive design of new image stylizations using\na wide range of predefined filter blocks. Both novel and off-the-shelf image\nfiltering and rendering techniques are extended and combined to allow the user\nto unleash their creativity to intuitively invent, modify, and tune new styles\nfrom a given set of filters. In parallel to this manual design, we propose a\nnovel procedural approach that automatically assembles sequences of filters,\nleading to unique and novel styles. An important aim of our framework is to\nallow for interactive exploration and design, as well as to enable videos and\ncamera streams to be stylized on the fly. In order to achieve this real-time\nperformance, we use the \\textit{Best Linear Adaptive Enhancement} (BLADE)\nframework -- an interpretable shallow machine learning method that simulates\ncomplex filter blocks in real time. Our representative results include over a\ndozen styles designed using our interactive tool, a set of styles created\nprocedurally, and new filters trained with our BLADE approach.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 06:48:28 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Garcia-Dorado", "Ignacio", ""], ["Getreuer", "Pascal", ""], ["Wronski", "Bartlomiej", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2002.10964", "submitter": "Sangwoo Mo", "authors": "Sangwoo Mo, Minsu Cho, Jinwoo Shin", "title": "Freeze the Discriminator: a Simple Baseline for Fine-Tuning GANs", "comments": "Tech report; High resolution images are in\n  https://github.com/sangwoomo/FreezeD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have shown outstanding performance on\na wide range of problems in computer vision, graphics, and machine learning,\nbut often require numerous training data and heavy computational resources. To\ntackle this issue, several methods introduce a transfer learning technique in\nGAN training. They, however, are either prone to overfitting or limited to\nlearning small distribution shifts. In this paper, we show that simple\nfine-tuning of GANs with frozen lower layers of the discriminator performs\nsurprisingly well. This simple baseline, FreezeD, significantly outperforms\nprevious techniques used in both unconditional and conditional GANs. We\ndemonstrate the consistent effect using StyleGAN and SNGAN-projection\narchitectures on several datasets of Animal Face, Anime Face, Oxford Flower,\nCUB-200-2011, and Caltech-256 datasets. The code and results are available at\nhttps://github.com/sangwoomo/FreezeD.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:30:17 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 10:53:50 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Mo", "Sangwoo", ""], ["Cho", "Minsu", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2002.10974", "submitter": "Nikolaos Dimitriou", "authors": "Nikolaos Dimitriou, Lampros Leontaris, Thanasis Vafeiadis, Dimosthenis\n  Ioannidis, Tracy Wotherspoon, Gregory Tinker, and Dimitrios Tzovaras", "title": "Fault Diagnosis in Microelectronics Attachment via Deep Learning\n  Analysis of 3D Laser Scans", "comments": "10 pages, 12 figures. in IEEE Transactions on Industrial Electronics,\n  2019 (early access)", "journal-ref": null, "doi": "10.1109/TIE.2019.2931220", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common source of defects in manufacturing miniature Printed Circuits Boards\n(PCB) is the attachment of silicon die or other wire bondable components on a\nLiquid Crystal Polymer (LCP) substrate. Typically, a conductive glue is\ndispensed prior to attachment with defects caused either by insufficient or\nexcessive glue. The current practice in electronics industry is to examine the\ndeposited glue by a human operator a process that is both time consuming and\ninefficient especially in preproduction runs where the error rate is high. In\nthis paper we propose a system that automates fault diagnosis by accurately\nestimating the volume of glue deposits before and even after die attachment. To\nthis end a modular scanning system is deployed that produces high resolution\npoint clouds whereas the actual estimation of glue volume is performed by\n(R)egression-Net (RNet), a 3D Convolutional Neural Network (3DCNN). RNet\noutperforms other deep architectures and is able to estimate the volume either\ndirectly from the point cloud of a glue deposit or more interestingly after die\nattachment when only a small part of glue is visible around each die. The\nentire methodology is evaluated under operational conditions where the proposed\nsystem achieves accurate results without delaying the manufacturing process.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:38:11 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Dimitriou", "Nikolaos", ""], ["Leontaris", "Lampros", ""], ["Vafeiadis", "Thanasis", ""], ["Ioannidis", "Dimosthenis", ""], ["Wotherspoon", "Tracy", ""], ["Tinker", "Gregory", ""], ["Tzovaras", "Dimitrios", ""]]}, {"id": "2002.10979", "submitter": "Yushi Lan", "authors": "Yushi Lan, Yuan Liu, Maoqing Tian, Xinchi Zhou, Xuesen Zhang, Shuai\n  Yi, Hongsheng Li", "title": "MagnifierNet: Towards Semantic Adversary and Fusion for Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although person re-identification (ReID) has achieved significant improvement\nrecently by enforcing part alignment, it is still a challenging task when it\ncomes to distinguishing visually similar identities or identifying the occluded\nperson. In these scenarios, magnifying details in each part features and\nselectively fusing them together may provide a feasible solution. In this work,\nwe propose MagnifierNet, a triple-branch network which accurately mines details\nfrom whole to parts. Firstly, the holistic salient features are encoded by a\nglobal branch. Secondly, to enhance detailed representation for each semantic\nregion, the \"Semantic Adversarial Branch\" is designed to learn from dynamically\ngenerated semantic-occluded samples during training. Meanwhile, we introduce\n\"Semantic Fusion Branch\" to filter out irrelevant noises by selectively fusing\nsemantic region information sequentially. To further improve feature diversity,\nwe introduce a novel loss function \"Semantic Diversity Loss\" to remove\nredundant overlaps across learned semantic representations. State-of-the-art\nperformance has been achieved on three benchmarks by large margins.\nSpecifically, the mAP score is improved by 6% and 5% on the most challenging\nCUHK03-L and CUHK03-D benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:43:46 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 02:37:53 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 07:48:25 GMT"}, {"version": "v4", "created": "Tue, 5 May 2020 02:22:42 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Lan", "Yushi", ""], ["Liu", "Yuan", ""], ["Tian", "Maoqing", ""], ["Zhou", "Xinchi", ""], ["Zhang", "Xuesen", ""], ["Yi", "Shuai", ""], ["Li", "Hongsheng", ""]]}, {"id": "2002.10981", "submitter": "Sanchita Ghose", "authors": "Sanchita Ghose, John J. Prevost", "title": "AutoFoley: Artificial Synthesis of Synchronized Sound Tracks for Silent\n  Videos with Deep Learning", "comments": "14 pages, 14 figures", "journal-ref": "IEEE TRANSACTIONS ON MULTIMEDIA, 2020", "doi": "10.1109/TMM.2020.3005033", "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In movie productions, the Foley Artist is responsible for creating an overlay\nsoundtrack that helps the movie come alive for the audience. This requires the\nartist to first identify the sounds that will enhance the experience for the\nlistener thereby reinforcing the Directors's intention for a given scene. In\nthis paper, we present AutoFoley, a fully-automated deep learning tool that can\nbe used to synthesize a representative audio track for videos. AutoFoley can be\nused in the applications where there is either no corresponding audio file\nassociated with the video or in cases where there is a need to identify\ncritical scenarios and provide a synthesized, reinforced soundtrack. An\nimportant performance criterion of the synthesized soundtrack is to be\ntime-synchronized with the input video, which provides for a realistic and\nbelievable portrayal of the synthesized sound. Unlike existing sound prediction\nand generation architectures, our algorithm is capable of precise recognition\nof actions as well as inter-frame relations in fast moving video clips by\nincorporating an interpolation technique and Temporal Relationship Networks\n(TRN). We employ a robust multi-scale Recurrent Neural Network (RNN) associated\nwith a Convolutional Neural Network (CNN) for a better understanding of the\nintricate input-to-output associations over time. To evaluate AutoFoley, we\ncreate and introduce a large scale audio-video dataset containing a variety of\nsounds frequently used as Foley effects in movies. Our experiments show that\nthe synthesized sounds are realistically portrayed with accurate temporal\nsynchronization of the associated visual inputs. Human qualitative testing of\nAutoFoley show over 73% of the test subjects considered the generated\nsoundtrack as original, which is a noteworthy improvement in cross-modal\nresearch in sound synthesis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 09:08:28 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Ghose", "Sanchita", ""], ["Prevost", "John J.", ""]]}, {"id": "2002.10986", "submitter": "Nikolaos Dimitriou", "authors": "Nikolaos Dimitriou, Lampros Leontaris, Thanasis Vafeiadis, Dimosthenis\n  Ioannidis, Tracy Wotherspoon, Gregory Tinker, Dimitrios Tzovaras", "title": "A Deep Learning Framework for Simulation and Defect Prediction Applied\n  in Microelectronics", "comments": "21 pages, 5 figures", "journal-ref": "Simulation Modelling Practice and Theory, Volume 100, 2020", "doi": "10.1016/j.simpat.2019.102063", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of upcoming events in industrial processes has been a\nlong-standing research goal since it enables optimization of manufacturing\nparameters, planning of equipment maintenance and more importantly prediction\nand eventually prevention of defects. While existing approaches have\naccomplished substantial progress, they are mostly limited to processing of one\ndimensional signals or require parameter tuning to model environmental\nparameters. In this paper, we propose an alternative approach based on deep\nneural networks that simulates changes in the 3D structure of a monitored\nobject in a batch based on previous 3D measurements. In particular, we propose\nan architecture based on 3D Convolutional Neural Networks (3DCNN) in order to\nmodel the geometric variations in manufacturing parameters and predict upcoming\nevents related to sub-optimal performance. We validate our framework on a\nmicroelectronics use-case using the recently published PCB scans dataset where\nwe simulate changes on the shape and volume of glue deposited on an Liquid\nCrystal Polymer (LCP) substrate before the attachment of integrated circuits\n(IC). Experimental evaluation examines the impact of different choices in the\ncost function during training and shows that the proposed method can be\nefficiently used for defect prediction.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:54:33 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Dimitriou", "Nikolaos", ""], ["Leontaris", "Lampros", ""], ["Vafeiadis", "Thanasis", ""], ["Ioannidis", "Dimosthenis", ""], ["Wotherspoon", "Tracy", ""], ["Tinker", "Gregory", ""], ["Tzovaras", "Dimitrios", ""]]}, {"id": "2002.11020", "submitter": "Ekrem Aksoy", "authors": "Ekrem Aksoy, Ahmet Yaz{\\i}c{\\i}, Mahmut Kasap", "title": "See, Attend and Brake: An Attention-based Saliency Map Prediction Model\n  for End-to-End Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual perception is the most critical input for driving decisions. In this\nstudy, our aim is to understand relationship between saliency and driving\ndecisions. We present a novel attention-based saliency map prediction model for\nmaking braking decisions This approach constructs a holistic model to the\ndriving task and can be extended for other driving decisions like steering and\nacceleration. The proposed model is a deep neural network model that feeds\nextracted features from input image to a recurrent neural network with an\nattention mechanism. Then predicted saliency map is used to make braking\ndecision. We trained and evaluated using driving attention dataset BDD-A, and\nsaliency dataset CAT2000.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 06:01:35 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Aksoy", "Ekrem", ""], ["Yaz\u0131c\u0131", "Ahmet", ""], ["Kasap", "Mahmut", ""]]}, {"id": "2002.11022", "submitter": "Yehui Tang", "authors": "Yehui Tang, Yunhe Wang, Yixing Xu, Boxin Shi, Chao Xu, Chunjing Xu,\n  Chang Xu", "title": "Beyond Dropout: Feature Map Distortion to Regularize Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks often consist of a great number of trainable parameters\nfor extracting powerful features from given datasets. On one hand, massive\ntrainable parameters significantly enhance the performance of these deep\nnetworks. On the other hand, they bring the problem of over-fitting. To this\nend, dropout based methods disable some elements in the output feature maps\nduring the training phase for reducing the co-adaptation of neurons. Although\nthe generalization ability of the resulting models can be enhanced by these\napproaches, the conventional binary dropout is not the optimal solution.\nTherefore, we investigate the empirical Rademacher complexity related to\nintermediate layers of deep neural networks and propose a feature distortion\nmethod (Disout) for addressing the aforementioned problem. In the training\nperiod, randomly selected elements in the feature maps will be replaced with\nspecific values by exploiting the generalization error bound. The superiority\nof the proposed feature map distortion for producing deep neural network with\nhigher testing performance is analyzed and demonstrated on several benchmark\nimage datasets.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 13:59:13 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Tang", "Yehui", ""], ["Wang", "Yunhe", ""], ["Xu", "Yixing", ""], ["Shi", "Boxin", ""], ["Xu", "Chao", ""], ["Xu", "Chunjing", ""], ["Xu", "Chang", ""]]}, {"id": "2002.11052", "submitter": "Sai Aparna Aketi", "authors": "Sai Aparna Aketi and Priyadarshini Panda and Kaushik Roy", "title": "Relevant-features based Auxiliary Cells for Energy Efficient Detection\n  of Natural Errors", "comments": "16 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have demonstrated state-of-the-art performance on many\nclassification tasks. However, they have no inherent capability to recognize\nwhen their predictions are wrong. There have been several efforts in the recent\npast to detect natural errors but the suggested mechanisms pose additional\nenergy requirements. To address this issue, we propose an ensemble of\nclassifiers at hidden layers to enable energy efficient detection of natural\nerrors. In particular, we append Relevant-features based Auxiliary Cells (RACs)\nwhich are class specific binary linear classifiers trained on relevant\nfeatures. The consensus of RACs is used to detect natural errors. Based on\ncombined confidence of RACs, classification can be terminated early, thereby\nresulting in energy efficient detection. We demonstrate the effectiveness of\nour technique on various image classification datasets such as CIFAR-10,\nCIFAR-100 and Tiny-ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 17:22:10 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 01:30:08 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Aketi", "Sai Aparna", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "2002.11061", "submitter": "Jan Fabian Schmid", "authors": "Jan Fabian Schmid, Stephan F. Simon, Rudolf Mester", "title": "Ground Texture Based Localization Using Compact Binary Descriptors", "comments": "Published at 2020 IEEE International Conference on Robotics and\n  Automation (ICRA)", "journal-ref": null, "doi": "10.1109/ICRA40945.2020.9197221", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground texture based localization is a promising approach to achieve\nhigh-accuracy positioning of vehicles. We present a self-contained method that\ncan be used for global localization as well as for subsequent local\nlocalization updates, i.e. it allows a robot to localize without any knowledge\nof its current whereabouts, but it can also take advantage of a prior pose\nestimate to reduce computation time significantly. Our method is based on a\nnovel matching strategy, which we call identity matching, that is based on\ncompact binary feature descriptors. Identity matching treats pairs of features\nas matches only if their descriptors are identical. While other methods for\nglobal localization are faster to compute, our method reaches higher\nlocalization success rates, and can switch to local localization after the\ninitial localization.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 17:31:41 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 12:32:22 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Schmid", "Jan Fabian", ""], ["Simon", "Stephan F.", ""], ["Mester", "Rudolf", ""]]}, {"id": "2002.11079", "submitter": "Yukai Shi", "authors": "Yukai Shi, Haoyu Zhong, Zhijing Yang, Xiaojun Yang, Liang Lin", "title": "DDet: Dual-path Dynamic Enhancement Network for Real-World Image\n  Super-Resolution", "comments": "Code address: https://github.com/ykshi/DDet", "journal-ref": null, "doi": "10.1109/LSP.2020.2978410", "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from traditional image super-resolution task, real image\nsuper-resolution(Real-SR) focus on the relationship between real-world\nhigh-resolution(HR) and low-resolution(LR) image. Most of the traditional image\nSR obtains the LR sample by applying a fixed down-sampling operator. Real-SR\nobtains the LR and HR image pair by incorporating different quality optical\nsensors. Generally, Real-SR has more challenges as well as broader application\nscenarios. Previous image SR methods fail to exhibit similar performance on\nReal-SR as the image data is not aligned inherently. In this article, we\npropose a Dual-path Dynamic Enhancement Network(DDet) for Real-SR, which\naddresses the cross-camera image mapping by realizing a dual-way dynamic\nsub-pixel weighted aggregation and refinement. Unlike conventional methods\nwhich stack up massive convolutional blocks for feature representation, we\nintroduce a content-aware framework to study non-inherently aligned image pair\nin image SR issue. First, we use a content-adaptive component to exhibit the\nMulti-scale Dynamic Attention(MDA). Second, we incorporate a long-term skip\nconnection with a Coupled Detail Manipulation(CDM) to perform collaborative\ncompensation and manipulation. The above dual-path model is joint into a\nunified model and works collaboratively. Extensive experiments on the\nchallenging benchmarks demonstrate the superiority of our model.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:24:51 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Shi", "Yukai", ""], ["Zhong", "Haoyu", ""], ["Yang", "Zhijing", ""], ["Yang", "Xiaojun", ""], ["Lin", "Liang", ""]]}, {"id": "2002.11088", "submitter": "Jie Zhang", "authors": "Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Weiming Zhang, Wenbo\n  Zhou, Hao Cui, Nenghai Yu", "title": "Model Watermarking for Image Processing Networks", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved tremendous success in numerous industrial\napplications. As training a good model often needs massive high-quality data\nand computation resources, the learned models often have significant business\nvalues. However, these valuable deep models are exposed to a huge risk of\ninfringements. For example, if the attacker has the full information of one\ntarget model including the network structure and weights, the model can be\neasily finetuned on new datasets. Even if the attacker can only access the\noutput of the target model, he/she can still train another similar surrogate\nmodel by generating a large scale of input-output training pairs. How to\nprotect the intellectual property of deep models is a very important but\nseriously under-researched problem. There are a few recent attempts at\nclassification network protection only. In this paper, we propose the first\nmodel watermarking framework for protecting image processing models. To achieve\nthis goal, we leverage the spatial invisible watermarking mechanism.\nSpecifically, given a black-box target model, a unified and invisible watermark\nis hidden into its outputs, which can be regarded as a special task-agnostic\nbarrier. In this way, when the attacker trains one surrogate model by using the\ninput-output pairs of the target model, the hidden watermark will be learned\nand extracted afterward. To enable watermarks from binary bits to\nhigh-resolution images, both traditional and deep spatial invisible\nwatermarking mechanism are considered. Experiments demonstrate the robustness\nof the proposed watermarking mechanism, which can resist surrogate models\nlearned with different network structures and objective functions. Besides deep\nmodels, the proposed method is also easy to be extended to protect data and\ntraditional image processing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:36:18 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Zhang", "Jie", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""], ["Fang", "Han", ""], ["Zhang", "Weiming", ""], ["Zhou", "Wenbo", ""], ["Cui", "Hao", ""], ["Yu", "Nenghai", ""]]}, {"id": "2002.11098", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Jean Kossaifi and Georgios Tzimiropoulos and Maja\n  Pantic", "title": "Toward fast and accurate human pose estimation via soft-gated skip\n  connections", "comments": "Accepted to FG 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is on highly accurate and highly efficient human pose estimation.\nRecent works based on Fully Convolutional Networks (FCNs) have demonstrated\nexcellent results for this difficult problem. While residual connections within\nFCNs have proved to be quintessential for achieving high accuracy, we\nre-analyze this design choice in the context of improving both the accuracy and\nthe efficiency over the state-of-the-art. In particular, we make the following\ncontributions: (a) We propose gated skip connections with per-channel learnable\nparameters to control the data flow for each channel within the module within\nthe macro-module. (b) We introduce a hybrid network that combines the HourGlass\nand U-Net architectures which minimizes the number of identity connections\nwithin the network and increases the performance for the same parameter budget.\nOur model achieves state-of-the-art results on the MPII and LSP datasets. In\naddition, with a reduction of 3x in model size and complexity, we show no\ndecrease in performance when compared to the original HourGlass network.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:51:51 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Bulat", "Adrian", ""], ["Kossaifi", "Jean", ""], ["Tzimiropoulos", "Georgios", ""], ["Pantic", "Maja", ""]]}, {"id": "2002.11102", "submitter": "Boyi Li", "authors": "Boyi Li and Felix Wu and Ser-Nam Lim and Serge Belongie and Kilian Q.\n  Weinberger", "title": "On Feature Normalization and Data Augmentation", "comments": "CVPR 2021. Code is available at https://github.com/Boyiliee/MoEx", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The moments (a.k.a., mean and standard deviation) of latent features are\noften removed as noise when training image recognition models, to increase\nstability and reduce training time. However, in the field of image generation,\nthe moments play a much more central role. Studies have shown that the moments\nextracted from instance normalization and positional normalization can roughly\ncapture style and shape information of an image. Instead of being discarded,\nthese moments are instrumental to the generation process. In this paper we\npropose Moment Exchange, an implicit data augmentation method that encourages\nthe model to utilize the moment information also for recognition models.\nSpecifically, we replace the moments of the learned features of one training\nimage by those of another, and also interpolate the target labels -- forcing\nthe model to extract training signal from the moments in addition to the\nnormalized features. As our approach is fast, operates entirely in feature\nspace, and mixes different signals than prior methods, one can effectively\ncombine it with existing augmentation approaches. We demonstrate its efficacy\nacross several recognition benchmark data sets where it improves the\ngeneralization capability of highly competitive baseline networks with\nremarkable consistency.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:59:05 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 18:59:02 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 18:00:00 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Li", "Boyi", ""], ["Wu", "Felix", ""], ["Lim", "Ser-Nam", ""], ["Belongie", "Serge", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "2002.11169", "submitter": "William Paul", "authors": "William Paul, I-Jeng Wang, Fady Alajaji, Philippe Burlina", "title": "Unsupervised Discovery, Control, and Disentanglement of Semantic\n  Attributes with Applications to Anomaly Detection", "comments": "MIT Neural Computation 2021, Vol 33(3), pp. 802--826", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work focuses on unsupervised and generative methods that address the\nfollowing goals: (a) learning unsupervised generative representations that\ndiscover latent factors controlling image semantic attributes, (b) studying how\nthis ability to control attributes formally relates to the issue of latent\nfactor disentanglement, clarifying related but dissimilar concepts that had\nbeen confounded in the past, and (c) developing anomaly detection methods that\nleverage representations learned in (a). For (a), we propose a network\narchitecture that exploits the combination of multiscale generative models with\nmutual information (MI) maximization. For (b), we derive an analytical result\n(Lemma 1) that brings clarity to two related but distinct concepts: the ability\nof generative networks to control semantic attributes of images they generate,\nresulting from MI maximization, and the ability to disentangle latent space\nrepresentations, obtained via total correlation minimization. More\nspecifically, we demonstrate that maximizing semantic attribute control\nencourages disentanglement of latent factors. Using Lemma 1 and adopting MI in\nour loss function, we then show empirically that, for image generation tasks,\nthe proposed approach exhibits superior performance as measured in the quality\nand disentanglement trade space, when compared to other state of the art\nmethods, with quality assessed via the Frechet Inception Distance (FID), and\ndisentanglement via mutual information gap. For (c), we design several systems\nfor anomaly detection exploiting representations learned in (a), and\ndemonstrate their performance benefits when compared to state-of-the-art\ngenerative and discriminative algorithms. The above contributions in\nrepresentation learning have potential applications in addressing other\nimportant problems in computer vision, such as bias and privacy in AI.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 20:50:47 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 19:50:26 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 19:54:52 GMT"}, {"version": "v4", "created": "Mon, 7 Jun 2021 15:50:10 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Paul", "William", ""], ["Wang", "I-Jeng", ""], ["Alajaji", "Fady", ""], ["Burlina", "Philippe", ""]]}, {"id": "2002.11192", "submitter": "Alessandro Rossi", "authors": "Alessandro Rossi, Sara Ermini, Dario Bernabini, Dario Zanca, Marino\n  Todisco, Alessandro Genovese, and Antonio Rizzo", "title": "End-to-End Models for the Analysis of System 1 and System 2 Interactions\n  based on Eye-Tracking Data", "comments": "11 pages, 2 figures, 1 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While theories postulating a dual cognitive system take hold, quantitative\nconfirmations are still needed to understand and identify interactions between\nthe two systems or conflict events. Eye movements are among the most direct\nmarkers of the individual attentive load and may serve as an important proxy of\ninformation. In this work we propose a computational method, within a modified\nvisual version of the well-known Stroop test, for the identification of\ndifferent tasks and potential conflicts events between the two systems through\nthe collection and processing of data related to eye movements. A statistical\nanalysis shows that the selected variables can characterize the variation of\nattentive load within different scenarios. Moreover, we show that Machine\nLearning techniques allow to distinguish between different tasks with a good\nclassification accuracy and to investigate more in depth the gaze dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 17:46:13 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Rossi", "Alessandro", ""], ["Ermini", "Sara", ""], ["Bernabini", "Dario", ""], ["Zanca", "Dario", ""], ["Todisco", "Marino", ""], ["Genovese", "Alessandro", ""], ["Rizzo", "Antonio", ""]]}, {"id": "2002.11201", "submitter": "Elchanan Solomon", "authors": "Elchanan Solomon, Paul Bendich", "title": "Geometric Fusion via Joint Delay Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce geometric and topological methods to develop a new framework for\nfusing multi-sensor time series. This framework consists of two steps: (1) a\njoint delay embedding, which reconstructs a high-dimensional state space in\nwhich our sensors correspond to observation functions, and (2) a simple\northogonalization scheme, which accounts for tangencies between such\nobservation functions, and produces a more diversified geometry on the\nembedding space. We conclude with some synthetic and real-world experiments\ndemonstrating that our framework outperforms traditional metric fusion methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 22:20:12 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Solomon", "Elchanan", ""], ["Bendich", "Paul", ""]]}, {"id": "2002.11220", "submitter": "David Hart", "authors": "David Hart, Jessica Greenland, Bryan Morse", "title": "Style Transfer for Light Field Photography", "comments": "To be presented at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As light field images continue to increase in use and application, it becomes\nnecessary to adapt existing image processing methods to this unique form of\nphotography. In this paper we explore methods for applying neural style\ntransfer to light field images. Feed-forward style transfer networks provide\nfast, high-quality results for monocular images, but no such networks exist for\nfull light field images. Because of the size of these images, current light\nfield data sets are small and are insufficient for training purely feed-forward\nstyle-transfer networks from scratch. Thus, it is necessary to adapt existing\nmonocular style transfer networks in a way that allows for the stylization of\neach view of the light field while maintaining visual consistencies between\nviews. Instead, the proposed method backpropagates the loss through the\nnetwork, and the process is iterated to optimize (essentially overfit) the\nresulting stylization for a single light field image alone. The network\narchitecture allows for the incorporation of pre-trained fast monocular\nstylization networks while avoiding the need for a large light field training\nset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 23:21:47 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Hart", "David", ""], ["Greenland", "Jessica", ""], ["Morse", "Bryan", ""]]}, {"id": "2002.11226", "submitter": "Joel Dabrowski Dr", "authors": "Joel Janek Dabrowski and Johan Pieter de Villiers and Ashfaqur Rahman\n  and Conrad Beyers", "title": "Deep Learning and Statistical Models for Time-Critical Pedestrian\n  Behaviour Prediction", "comments": null, "journal-ref": "In: Gedeon T., Wong K., Lee M. (eds) Neural Information\n  Processing. ICONIP 2019. Communications in Computer and Information Science,\n  vol 1142. Springer, Cham", "doi": "10.1007/978-3-030-36808-1_50", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time it takes for a classifier to make an accurate prediction can be\ncrucial in many behaviour recognition problems. For example, an autonomous\nvehicle should detect hazardous pedestrian behaviour early enough for it to\ntake appropriate measures. In this context, we compare the switching linear\ndynamical system (SLDS) and a three-layered bi-directional long short-term\nmemory (LSTM) neural network, which are applied to infer pedestrian behaviour\nfrom motion tracks. We show that, though the neural network model achieves an\naccuracy of 80%, it requires long sequences to achieve this (100 samples or\nmore). The SLDS, has a lower accuracy of 74%, but it achieves this result with\nshort sequences (10 samples). To our knowledge, such a comparison on sequence\nlength has not been considered in the literature before. The results provide a\nkey intuition of the suitability of the models in time-critical problems.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 00:05:19 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Dabrowski", "Joel Janek", ""], ["de Villiers", "Johan Pieter", ""], ["Rahman", "Ashfaqur", ""], ["Beyers", "Conrad", ""]]}, {"id": "2002.11244", "submitter": "Yoonsik Kim", "authors": "Yoonsik Kim, Jae Woong Soh, Gu Yong Park, and Nam Ik Cho", "title": "Transfer Learning from Synthetic to Real-Noise Denoising with Adaptive\n  Instance Normalization", "comments": "CVPR accepted paper. The paper will be updated according to\n  reviewers' comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Real-noise denoising is a challenging task because the statistics of\nreal-noise do not follow the normal distribution, and they are also spatially\nand temporally changing. In order to cope with various and complex real-noise,\nwe propose a well-generalized denoising architecture and a transfer learning\nscheme. Specifically, we adopt an adaptive instance normalization to build a\ndenoiser, which can regularize the feature map and prevent the network from\noverfitting to the training set. We also introduce a transfer learning scheme\nthat transfers knowledge learned from synthetic-noise data to the real-noise\ndenoiser. From the proposed transfer learning, the synthetic-noise denoiser can\nlearn general features from various synthetic-noise data, and the real-noise\ndenoiser can learn the real-noise characteristics from real data. From the\nexperiments, we find that the proposed denoising method has great\ngeneralization ability, such that our network trained with synthetic-noise\nachieves the best performance for Darmstadt Noise Dataset (DND) among the\nmethods from published papers. We can also see that the proposed transfer\nlearning scheme robustly works for real-noise images through the learning with\na very small number of labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 01:08:42 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 10:44:41 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Kim", "Yoonsik", ""], ["Soh", "Jae Woong", ""], ["Park", "Gu Yong", ""], ["Cho", "Nam Ik", ""]]}, {"id": "2002.11248", "submitter": "Hossein Talebi", "authors": "Xiang Zhu, Hossein Talebi, Xinwei Shi, Feng Yang, Peyman Milanfar", "title": "Super-Resolving Commercial Satellite Imagery Using Realistic Training\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning based single image super-resolution, the degradation\nmodel is embedded in training data generation. However, most existing satellite\nimage super-resolution methods use a simple down-sampling model with a fixed\nkernel to create training images. These methods work fine on synthetic data,\nbut do not perform well on real satellite images. We propose a realistic\ntraining data generation model for commercial satellite imagery products, which\nincludes not only the imaging process on satellites but also the post-process\non the ground. We also propose a convolutional neural network optimized for\nsatellite images. Experiments show that the proposed training data generation\nmodel is able to improve super-resolution performance on real satellite images.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 01:18:51 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Zhu", "Xiang", ""], ["Talebi", "Hossein", ""], ["Shi", "Xinwei", ""], ["Yang", "Feng", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2002.11251", "submitter": "Vikas Gupta", "authors": "Vikas Gupta", "title": "Back to the Future: Joint Aware Temporal Deep Learning 3D Human Pose\n  Estimation", "comments": "Our model and code are available at https://vnmr.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep learning network that introduces a deeper CNN channel\nfilter and constraints as losses to reduce joint position and motion errors for\n3D video human body pose estimation. Our model outperforms the previous best\nresult from the literature based on mean per-joint position error, velocity\nerror, and acceleration errors on the Human 3.6M benchmark corresponding to a\nnew state-of-the-art mean error reduction in all protocols and motion metrics.\nMean per joint error is reduced by 1%, velocity error by 7% and acceleration by\n13% compared to the best results from the literature. Our contribution\nincreasing positional accuracy and motion smoothness in video can be integrated\nwith future end to end networks without increasing network complexity. Our\nmodel and code are available at https://vnmr.github.io/\n  Keywords: 3D, human, image, pose, action, detection, object, video, visual,\nsupervised, joint, kinematic\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 10:11:13 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Gupta", "Vikas", ""]]}, {"id": "2002.11261", "submitter": "Minxuan Lin", "authors": "Minxuan Lin, Yingying Deng, Fan Tang, Weiming Dong, Changsheng Xu", "title": "Multi-Attribute Guided Painting Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controllable painting generation plays a pivotal role in image stylization.\nCurrently, the control way of style transfer is subject to exemplar-based\nreference or a random one-hot vector guidance. Few works focus on decoupling\nthe intrinsic properties of painting as control conditions, e.g., artist, genre\nand period. Under this circumstance, we propose a novel framework adopting\nmultiple attributes from the painting to control the stylized results. An\nasymmetrical cycle structure is equipped to preserve the fidelity, associating\nwith style preserving and attribute regression loss to keep the unique\ndistinction of colors and textures between domains. Several qualitative and\nquantitative results demonstrate the effect of the combinations of multiple\nattributes and achieve satisfactory performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 02:22:23 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Lin", "Minxuan", ""], ["Deng", "Yingying", ""], ["Tang", "Fan", ""], ["Dong", "Weiming", ""], ["Xu", "Changsheng", ""]]}, {"id": "2002.11263", "submitter": "Jing Jin", "authors": "Jing Jin and Junhui Hou and Hui Yuan and Sam Kwong", "title": "Learning Light Field Angular Super-Resolution via a Geometry-Aware\n  Network", "comments": "This paper was accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The acquisition of light field images with high angular resolution is costly.\nAlthough many methods have been proposed to improve the angular resolution of a\nsparsely-sampled light field, they always focus on the light field with a small\nbaseline, which is captured by a consumer light field camera. By making full\nuse of the intrinsic \\textit{geometry} information of light fields, in this\npaper we propose an end-to-end learning-based approach aiming at angularly\nsuper-resolving a sparsely-sampled light field with a large baseline. Our model\nconsists of two learnable modules and a physically-based module. Specifically,\nit includes a depth estimation module for explicitly modeling the scene\ngeometry, a physically-based warping for novel views synthesis, and a light\nfield blending module specifically designed for light field reconstruction.\nMoreover, we introduce a novel loss function to promote the preservation of the\nlight field parallax structure. Experimental results over various light field\ndatasets including large baseline light field images demonstrate the\nsignificant superiority of our method when compared with state-of-the-art ones,\ni.e., our method improves the PSNR of the second best method up to 2 dB in\naverage, while saves the execution time 48$\\times$. In addition, our method\npreserves the light field parallax structure better.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 02:36:57 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Jin", "Jing", ""], ["Hou", "Junhui", ""], ["Yuan", "Hui", ""], ["Kwong", "Sam", ""]]}, {"id": "2002.11281", "submitter": "Young Kyun Jang", "authors": "Young Kyun Jang and Nam Ik Cho", "title": "Generalized Product Quantization Network for Semi-supervised Image\n  Retrieval", "comments": "10 pages, 10 figures, Computer Vision and Pattern Recognition (CVPR)\n  2020 accpeted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval methods that employ hashing or vector quantization have\nachieved great success by taking advantage of deep learning. However, these\napproaches do not meet expectations unless expensive label information is\nsufficient. To resolve this issue, we propose the first quantization-based\nsemi-supervised image retrieval scheme: Generalized Product Quantization (GPQ)\nnetwork. We design a novel metric learning strategy that preserves semantic\nsimilarity between labeled data, and employ entropy regularization term to\nfully exploit inherent potentials of unlabeled data. Our solution increases the\ngeneralization capacity of the quantization network, which allows overcoming\nprevious limitations in the retrieval community. Extensive experimental results\ndemonstrate that GPQ yields state-of-the-art performance on large-scale real\nimage benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 03:36:32 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 04:25:13 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 00:21:29 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Jang", "Young Kyun", ""], ["Cho", "Nam Ik", ""]]}, {"id": "2002.11293", "submitter": "Mo Zhou", "authors": "Mo Zhou, Zhenxing Niu, Le Wang, Qilin Zhang, Gang Hua", "title": "Adversarial Ranking Attack and Defense", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) classifiers are vulnerable to adversarial attack,\nwhere an imperceptible perturbation could result in misclassification. However,\nthe vulnerability of DNN-based image ranking systems remains under-explored. In\nthis paper, we propose two attacks against deep ranking systems, i.e.,\nCandidate Attack and Query Attack, that can raise or lower the rank of chosen\ncandidates by adversarial perturbations. Specifically, the expected ranking\norder is first represented as a set of inequalities, and then a triplet-like\nobjective function is designed to obtain the optimal perturbation. Conversely,\na defense method is also proposed to improve the ranking system robustness,\nwhich can mitigate all the proposed attacks simultaneously. Our adversarial\nranking attacks and defense are evaluated on datasets including MNIST,\nFashion-MNIST, and Stanford-Online-Products. Experimental results demonstrate\nthat a typical deep ranking system can be effectively compromised by our\nattacks. Meanwhile, the system robustness can be moderately improved with our\ndefense. Furthermore, the transferable and universal properties of our\nadversary illustrate the possibility of realistic black-box attack.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 04:03:14 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 04:47:31 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 08:49:00 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhou", "Mo", ""], ["Niu", "Zhenxing", ""], ["Wang", "Le", ""], ["Zhang", "Qilin", ""], ["Hua", "Gang", ""]]}, {"id": "2002.11297", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Yilin Shen, Hongxia Jin, Zsolt Kira", "title": "Generalized ODIN: Detecting Out-of-distribution Image without Learning\n  from Out-of-distribution Data", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have attained remarkable performance when applied to\ndata that comes from the same distribution as that of the training set, but can\nsignificantly degrade otherwise. Therefore, detecting whether an example is\nout-of-distribution (OoD) is crucial to enable a system that can reject such\nsamples or alert users. Recent works have made significant progress on OoD\nbenchmarks consisting of small image datasets. However, many recent methods\nbased on neural networks rely on training or tuning with both in-distribution\nand out-of-distribution data. The latter is generally hard to define a-priori,\nand its selection can easily bias the learning. We base our work on a popular\nmethod ODIN, proposing two strategies for freeing it from the needs of tuning\nwith OoD data, while improving its OoD detection performance. We specifically\npropose to decompose confidence scoring as well as a modified input\npre-processing method. We show that both of these significantly help in\ndetection performance. Our further analysis on a larger scale image dataset\nshows that the two types of distribution shifts, specifically semantic shift\nand non-semantic shift, present a significant difference in the difficulty of\nthe problem, providing an analysis of when ODIN-like strategies do or do not\nwork.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 04:18:25 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 18:13:34 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Shen", "Yilin", ""], ["Jin", "Hongxia", ""], ["Kira", "Zsolt", ""]]}, {"id": "2002.11300", "submitter": "Yu Zhang", "authors": "Yu Zhang, Xiaoguang Di, Bin Zhang, Chunhui Wang", "title": "Self-supervised Image Enhancement Network: Training with Low Light\n  Images Only", "comments": "14 pages,13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a self-supervised low light image enhancement method\nbased on deep learning. Inspired by information entropy theory and Retinex\nmodel, we proposed a maximum entropy based Retinex model. With this model, a\nvery simple network can separate the illumination and reflectance, and the\nnetwork can be trained with low light images only. We introduce a constraint\nthat the maximum channel of the reflectance conforms to the maximum channel of\nthe low light image and its entropy should be largest in our model to achieve\nself-supervised learning. Our model is very simple and does not rely on any\nwell-designed data set (even one low light image can complete the training).\nThe network only needs minute-level training to achieve image enhancement. It\ncan be proved through experiments that the proposed method has reached the\nstate-of-the-art in terms of processing speed and effect.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 04:39:07 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Zhang", "Yu", ""], ["Di", "Xiaoguang", ""], ["Zhang", "Bin", ""], ["Wang", "Chunhui", ""]]}, {"id": "2002.11310", "submitter": "Xin Ye", "authors": "Xin Ye and Yezhou Yang", "title": "From Seeing to Moving: A Survey on Learning for Visual Indoor Navigation\n  (VIN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Indoor Navigation (VIN) task has drawn increasing attention from the\ndata-driven machine learning communities especially with the recently reported\nsuccess from learning-based methods. Due to the innate complexity of this task,\nresearchers have tried approaching the problem from a variety of different\nangles, the full scope of which has not yet been captured within an overarching\nreport. This survey first summarizes the representative work of learning-based\napproaches for the VIN task and then identifies and discusses lingering issues\nimpeding the VIN performance, as well as motivates future research in these key\nareas worth exploring for the community.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 05:27:30 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 20:47:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ye", "Xin", ""], ["Yang", "Yezhou", ""]]}, {"id": "2002.11318", "submitter": "Sandesh Kamath K", "authors": "Sandesh Kamath, Amit Deshpande, K V Subrahmanyam, Vineeth N\n  Balasubramanian", "title": "Can we have it all? On the Trade-off between Spatial and Adversarial\n  Robustness of Neural Networks", "comments": "Preliminary version consisting early experimental results was\n  presented in ICML 2018 Workshop on \"Towards learning with limited labels:\n  Equivariance, Invariance,and Beyond\" as \"Understanding Adversarial Robustness\n  of Symmetric Networks\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  (Non-)robustness of neural networks to small, adversarial pixel-wise\nperturbations, and as more recently shown, to even random spatial\ntransformations (e.g., translations, rotations) entreats both theoretical and\nempirical understanding. Spatial robustness to random translations and\nrotations is commonly attained via equivariant models (e.g., StdCNNs, GCNNs)\nand training augmentation, whereas adversarial robustness is typically achieved\nby adversarial training. In this paper, we prove a quantitative trade-off\nbetween spatial and adversarial robustness in a simple statistical setting. We\ncomplement this empirically by showing that: (a) as the spatial robustness of\nequivariant models improves by training augmentation with progressively larger\ntransformations, their adversarial robustness worsens progressively, and (b) as\nthe state-of-the-art robust models are adversarially trained with progressively\nlarger pixel-wise perturbations, their spatial robustness drops progressively.\nTowards achieving pareto-optimality in this trade-off, we propose a method\nbased on curriculum learning that trains gradually on more difficult\nperturbations (both spatial and adversarial) to improve spatial and adversarial\nrobustness simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 06:25:06 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 13:32:03 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 06:46:08 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 16:28:42 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Kamath", "Sandesh", ""], ["Deshpande", "Amit", ""], ["Subrahmanyam", "K V", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2002.11327", "submitter": "Xiaofeng Xu", "authors": "X.F. Xu, S. Talbot, T. Selvaraja", "title": "ParasNet: Fast Parasites Detection with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has dramatically improved the performance in many application\nareas such as image classification, object detection, speech recognition, drug\ndiscovery and etc since 2012. Where deep learning algorithms promise to\ndiscover the intricate hidden information inside the data by leveraging the\nlarge dataset, advanced model and computing power. Although deep learning\ntechniques show medical expert level performance in a lot of medical\napplications, but some of the applications are still not explored or under\nexplored due to the variation of the species. In this work, we studied the\nbright field based cell level Cryptosporidium and Giardia detection in the\ndrink water with deep learning. Our experimental demonstrates that the new\ndeveloped deep learning-based algorithm surpassed the handcrafted SVM based\nalgorithm with above 97 percentage in accuracy and 700+fps in speed on embedded\nJetson TX2 platform. Our research will lead to real-time and high accuracy\nlabel-free cell level Cryptosporidium and Giardia detection system in the\nfuture.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 06:58:17 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 04:58:58 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Xu", "X. F.", ""], ["Talbot", "S.", ""], ["Selvaraja", "T.", ""]]}, {"id": "2002.11338", "submitter": "Zhanzhan Cheng", "authors": "Zhanzhan Cheng, Yunlu Xu, Mingjian Cheng, Yu Qiao, Shiliang Pu, Yi Niu\n  and Fei Wu", "title": "Refined Gate: A Simple and Effective Gating Mechanism for Recurrent\n  Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recurrent neural network (RNN) has been widely studied in sequence learning\ntasks, while the mainstream models (e.g., LSTM and GRU) rely on the gating\nmechanism (in control of how information flows between hidden states). However,\nthe vanilla gates in RNN (e.g., the input gate in LSTM) suffer from the problem\nof gate undertraining, which can be caused by various factors, such as the\nsaturating activation functions, the gate layouts (e.g., the gate number and\ngating functions), or even the suboptimal memory state etc.. Those may result\nin failures of learning gating switch roles and thus the weak performance. In\nthis paper, we propose a new gating mechanism within general gated recurrent\nneural networks to handle this issue. Specifically, the proposed gates directly\nshort connect the extracted input features to the outputs of vanilla gates,\ndenoted as refined gates. The refining mechanism allows enhancing gradient\nback-propagation as well as extending the gating activation scope, which can\nguide RNN to reach possibly deeper minima. We verify the proposed gating\nmechanism on three popular types of gated RNNs including LSTM, GRU and MGU.\nExtensive experiments on 3 synthetic tasks, 3 language modeling tasks and 5\nscene text recognition benchmarks demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 07:51:38 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 13:59:48 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Cheng", "Zhanzhan", ""], ["Xu", "Yunlu", ""], ["Cheng", "Mingjian", ""], ["Qiao", "Yu", ""], ["Pu", "Shiliang", ""], ["Niu", "Yi", ""], ["Wu", "Fei", ""]]}, {"id": "2002.11359", "submitter": "Chen-Lin Zhang", "authors": "Chen-Lin Zhang, Yun-Hao Cao, Jianxin Wu", "title": "Rethinking the Route Towards Weakly Supervised Object Localization", "comments": "Accepted by CVPR 2020; Corrected some typo in the paper; The code\n  repository is https://github.com/tzzcl/PSOL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object localization (WSOL) aims to localize objects with\nonly image-level labels. Previous methods often try to utilize feature maps and\nclassification weights to localize objects using image level annotations\nindirectly. In this paper, we demonstrate that weakly supervised object\nlocalization should be divided into two parts: class-agnostic object\nlocalization and object classification. For class-agnostic object localization,\nwe should use class-agnostic methods to generate noisy pseudo annotations and\nthen perform bounding box regression on them without class labels. We propose\nthe pseudo supervised object localization (PSOL) method as a new way to solve\nWSOL. Our PSOL models have good transferability across different datasets\nwithout fine-tuning. With generated pseudo bounding boxes, we achieve 58.00%\nlocalization accuracy on ImageNet and 74.97% localization accuracy on CUB-200,\nwhich have a large edge over previous models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 08:54:20 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 03:12:56 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Zhang", "Chen-Lin", ""], ["Cao", "Yun-Hao", ""], ["Wu", "Jianxin", ""]]}, {"id": "2002.11367", "submitter": "Dominik Rivoir", "authors": "Dominik Rivoir, Sebastian Bodenstedt, Felix von Bechtolsheim, Marius\n  Distler, J\\\"urgen Weitz, Stefanie Speidel", "title": "Unsupervised Temporal Video Segmentation as an Auxiliary Task for\n  Predicting the Remaining Surgery Duration", "comments": null, "journal-ref": "OR 2.0 Context-Aware Operating Theaters and Machine Learning in\n  Clinical Neuroimaging (2019) 29-37", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the remaining surgery duration (RSD) during surgical procedures\ncan be useful for OR planning and anesthesia dose estimation. With the recent\nsuccess of deep learning-based methods in computer vision, several neural\nnetwork approaches have been proposed for fully automatic RSD prediction based\nsolely on visual data from the endoscopic camera. We investigate whether RSD\nprediction can be improved using unsupervised temporal video segmentation as an\nauxiliary learning task. As opposed to previous work, which presented\nsupervised surgical phase recognition as auxiliary task, we avoid the need for\nmanual annotations by proposing a similar but unsupervised learning objective\nwhich clusters video sequences into temporally coherent segments. In multiple\nexperimental setups, results obtained by learning the auxiliary task are\nincorporated into a deep RSD model through feature extraction, pretraining or\nregularization. Further, we propose a novel loss function for RSD training\nwhich attempts to counteract unfavorable characteristics of the RSD ground\ntruth. Using our unsupervised method as an auxiliary task for RSD training, we\noutperform other self-supervised methods and are comparable to the supervised\nstate-of-the-art. Combined with the novel RSD loss, we slightly outperform the\nsupervised approach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 09:13:39 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Rivoir", "Dominik", ""], ["Bodenstedt", "Sebastian", ""], ["von Bechtolsheim", "Felix", ""], ["Distler", "Marius", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""]]}, {"id": "2002.11371", "submitter": "Hao Liu", "authors": "Hao Liu, Antai Guo, Deqiang Jiang, Yiqing Hu, Bo Ren", "title": "PuzzleNet: Scene Text Detection by Segment Context Graph Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a series of decomposition-based scene text detection methods has\nachieved impressive progress by decomposing challenging text regions into\npieces and linking them in a bottom-up manner. However, most of them merely\nfocus on linking independent text pieces while the context information is\nunderestimated. In the puzzle game, the solver often put pieces together in a\nlogical way according to the contextual information of each piece, in order to\narrive at the correct solution. Inspired by it, we propose a novel\ndecomposition-based method, termed Puzzle Networks (PuzzleNet), to address the\nchallenging scene text detection task in this work. PuzzleNet consists of the\nSegment Proposal Network (SPN) that predicts the candidate text segments\nfitting arbitrary shape of text region, and the two-branch Multiple-Similarity\nGraph Convolutional Network (MSGCN) that models both appearance and geometry\ncorrelations between each segment to its contextual ones. By building segments\nas context graphs, MSGCN effectively employs segment context to predict\ncombinations of segments. Final detections of polygon shape are produced by\nmerging segments according to the predicted combinations. Evaluations on three\nbenchmark datasets, ICDAR15, MSRA-TD500 and SCUT-CTW1500, have demonstrated\nthat our method can achieve better or comparable performance than current\nstate-of-the-arts, which is beneficial from the exploitation of segment context\ngraph.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 09:21:05 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Liu", "Hao", ""], ["Guo", "Antai", ""], ["Jiang", "Deqiang", ""], ["Hu", "Yiqing", ""], ["Ren", "Bo", ""]]}, {"id": "2002.11374", "submitter": "Yan Feng", "authors": "Yan Feng, Bin Chen, Tao Dai, Shutao Xia", "title": "Adversarial Attack on Deep Product Quantization Network for Image\n  Retrieval", "comments": "Accepted at AAAI20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep product quantization network (DPQN) has recently received much attention\nin fast image retrieval tasks due to its efficiency of encoding\nhigh-dimensional visual features especially when dealing with large-scale\ndatasets. Recent studies show that deep neural networks (DNNs) are vulnerable\nto input with small and maliciously designed perturbations (a.k.a., adversarial\nexamples). This phenomenon raises the concern of security issues for DPQN in\nthe testing/deploying stage as well. However, little effort has been devoted to\ninvestigating how adversarial examples affect DPQN. To this end, we propose\nproduct quantization adversarial generation (PQ-AG), a simple yet effective\nmethod to generate adversarial examples for product quantization based\nretrieval systems. PQ-AG aims to generate imperceptible adversarial\nperturbations for query images to form adversarial queries, whose nearest\nneighbors from a targeted product quantizaiton model are not semantically\nrelated to those from the original queries. Extensive experiments show that our\nPQ-AQ successfully creates adversarial examples to mislead targeted product\nquantization retrieval models. Besides, we found that our PQ-AG significantly\ndegrades retrieval performance in both white-box and black-box settings.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 09:25:58 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Feng", "Yan", ""], ["Chen", "Bin", ""], ["Dai", "Tao", ""], ["Xia", "Shutao", ""]]}, {"id": "2002.11376", "submitter": "Zhilei Liu", "authors": "Yong Zhang, Le Li, Zhilei Liu, Baoyuan Wu, Yanbo Fan, Zhifeng Li", "title": "Controllable Descendant Face Synthesis", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinship face synthesis is an interesting topic raised to answer questions\nlike \"what will your future children look like?\". Published approaches to this\ntopic are limited. Most of the existing methods train models for one-versus-one\nkin relation, which only consider one parent face and one child face by\ndirectly using an auto-encoder without any explicit control over the\nresemblance of the synthesized face to the parent face. In this paper, we\npropose a novel method for controllable descendant face synthesis, which models\ntwo-versus-one kin relation between two parent faces and one child face. Our\nmodel consists of an inheritance module and an attribute enhancement module,\nwhere the former is designed for accurate control over the resemblance between\nthe synthesized face and parent faces, and the latter is designed for control\nover age and gender. As there is no large scale database with\nfather-mother-child kinship annotation, we propose an effective strategy to\ntrain the model without using the ground truth descendant faces. No carefully\ndesigned image pairs are required for learning except only age and gender\nlabels of training faces. We conduct comprehensive experimental evaluations on\nthree public benchmark databases, which demonstrates encouraging results.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 09:33:41 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Zhang", "Yong", ""], ["Li", "Le", ""], ["Liu", "Zhilei", ""], ["Wu", "Baoyuan", ""], ["Fan", "Yanbo", ""], ["Li", "Zhifeng", ""]]}, {"id": "2002.11379", "submitter": "Anirudh Joshi", "authors": "Pranav Rajpurkar, Anirudh Joshi, Anuj Pareek, Phil Chen, Amirhossein\n  Kiani, Jeremy Irvin, Andrew Y. Ng, Matthew P. Lungren", "title": "CheXpedition: Investigating Generalization Challenges for Translation of\n  Chest X-Ray Algorithms to the Clinical Setting", "comments": "Accepted as workshop paper at ACM Conference on Health, Inference,\n  and Learning (CHIL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Although there have been several recent advances in the application of deep\nlearning algorithms to chest x-ray interpretation, we identify three major\nchallenges for the translation of chest x-ray algorithms to the clinical\nsetting. We examine the performance of the top 10 performing models on the\nCheXpert challenge leaderboard on three tasks: (1) TB detection, (2) pathology\ndetection on photos of chest x-rays, and (3) pathology detection on data from\nan external institution. First, we find that the top 10 chest x-ray models on\nthe CheXpert competition achieve an average AUC of 0.851 on the task of\ndetecting TB on two public TB datasets without fine-tuning or including the TB\nlabels in training data. Second, we find that the average performance of the\nmodels on photos of x-rays (AUC = 0.916) is similar to their performance on the\noriginal chest x-ray images (AUC = 0.924). Third, we find that the models\ntested on an external dataset either perform comparably to or exceed the\naverage performance of radiologists. We believe that our investigation will\ninform rapid translation of deep learning algorithms to safe and effective\nclinical decision support tools that can be validated prospectively with large\nimpact studies and clinical trials.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 09:44:21 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 07:15:57 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Rajpurkar", "Pranav", ""], ["Joshi", "Anirudh", ""], ["Pareek", "Anuj", ""], ["Chen", "Phil", ""], ["Kiani", "Amirhossein", ""], ["Irvin", "Jeremy", ""], ["Ng", "Andrew Y.", ""], ["Lungren", "Matthew P.", ""]]}, {"id": "2002.11397", "submitter": "Shunta Maeda", "authors": "Shunta Maeda", "title": "Unpaired Image Super-Resolution using Pseudo-Supervision", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most studies on learning-based image super-resolution (SR), the paired\ntraining dataset is created by downscaling high-resolution (HR) images with a\npredetermined operation (e.g., bicubic). However, these methods fail to\nsuper-resolve real-world low-resolution (LR) images, for which the degradation\nprocess is much more complicated and unknown. In this paper, we propose an\nunpaired SR method using a generative adversarial network that does not require\na paired/aligned training dataset. Our network consists of an unpaired\nkernel/noise correction network and a pseudo-paired SR network. The correction\nnetwork removes noise and adjusts the kernel of the inputted LR image; then,\nthe corrected clean LR image is upscaled by the SR network. In the training\nphase, the correction network also produces a pseudo-clean LR image from the\ninputted HR image, and then a mapping from the pseudo-clean LR image to the\ninputted HR image is learned by the SR network in a paired manner. Because our\nSR network is independent of the correction network, well-studied existing\nnetwork architectures and pixel-wise loss functions can be integrated with the\nproposed framework. Experiments on diverse datasets show that the proposed\nmethod is superior to existing solutions to the unpaired SR problem.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 10:30:52 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Maeda", "Shunta", ""]]}, {"id": "2002.11404", "submitter": "Maria Tirindelli", "authors": "Maria Tirindelli, Maria Victorova, Javier Esteban, Seong Tae Kim,\n  David Navarro-Alarcon, Yong Ping Zheng and Nassir Navab", "title": "Force-Ultrasound Fusion: Bringing Spine Robotic-US to the Next \"Level\"", "comments": "Under Review in IEEE Robotics and Automation Letters (RA-L) with IROS\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spine injections are commonly performed in several clinical procedures. The\nlocalization of the target vertebral level (i.e. the position of a vertebra in\na spine) is typically done by back palpation or under X-ray guidance, yielding\neither higher chances of procedure failure or exposure to ionizing radiation.\nPreliminary studies have been conducted in the literature, suggesting that\nultrasound imaging may be a precise and safe alternative to X-ray for spine\nlevel detection. However, ultrasound data are noisy and complicated to\ninterpret. In this study, a robotic-ultrasound approach for automatic vertebral\nlevel detection is introduced. The method relies on the fusion of ultrasound\nand force data, thus providing both \"tactile\" and visual feedback during the\nprocedure, which results in higher performances in presence of data corruption.\nA robotic arm automatically scans the volunteer's back along the spine by using\nforce-ultrasound data to locate vertebral levels. The occurrences of vertebral\nlevels are visible on the force trace as peaks, which are enhanced by properly\ncontrolling the force applied by the robot on the patient back. Ultrasound data\nare processed with a Deep Learning method to extract a 1D signal modelling the\nprobabilities of having a vertebra at each location along the spine. Processed\nforce and ultrasound data are fused using a 1D Convolutional Network to compute\nthe location of the vertebral levels. The method is compared to pure image and\npure force-based methods for vertebral level counting, showing improved\nperformance. In particular, the fusion method is able to correctly classify\n100% of the vertebral levels in the test set, while pure image and pure\nforce-based method could only classify 80% and 90% vertebrae, respectively. The\npotential of the proposed method is evaluated in an exemplary simulated\nclinical application.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 10:49:53 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Tirindelli", "Maria", ""], ["Victorova", "Maria", ""], ["Esteban", "Javier", ""], ["Kim", "Seong Tae", ""], ["Navarro-Alarcon", "David", ""], ["Zheng", "Yong Ping", ""], ["Navab", "Nassir", ""]]}, {"id": "2002.11409", "submitter": "Luigi Celona", "authors": "Simone Bianco, Luigi Celona, Paolo Napoletano", "title": "Disentangling Image Distortions in Deep Feature Space", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2021.05.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous literature suggests that perceptual similarity is an emergent\nproperty shared across deep visual representations. Experiments conducted on a\ndataset of human-judged image distortions have proven that deep features\noutperform classic perceptual metrics. In this work we take a further step in\nthe direction of a broader understanding of such property by analyzing the\ncapability of deep visual representations to intrinsically characterize\ndifferent types of image distortions. To this end, we firstly generate a number\nof synthetically distorted images and then we analyze the features extracted by\ndifferent layers of different Deep Neural Networks. We observe that a\ndimension-reduced representation of the features extracted from a given layer\npermits to efficiently separate types of distortions in the feature space.\nMoreover, each network layer exhibits a different ability to separate between\ndifferent types of distortions, and this ability varies according to the\nnetwork architecture. Finally, we evaluate the exploitation of features taken\nfrom the layer that better separates image distortions for: i)\nreduced-reference image quality assessment, and ii) distortion types and\nseverity levels characterization on both single and multiple distortion\ndatabases. Results achieved on both tasks suggest that deep visual\nrepresentations can be unsupervisedly employed to efficiently characterize\nvarious image distortions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 11:02:13 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 13:04:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bianco", "Simone", ""], ["Celona", "Luigi", ""], ["Napoletano", "Paolo", ""]]}, {"id": "2002.11424", "submitter": "Tanmoy Mondal Dr.", "authors": "Tanmoy Mondal, LE Thi Thuy Trang, Micka\\\"el Coustaty and Jean-Marc\n  Ogier", "title": "Performance Evaluation of Deep Generative Models for Generating\n  Hand-Written Character Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many work in the literature on generation of various kinds of\nimages such as Hand-Written characters (MNIST dataset), scene images (CIFAR-10\ndataset), various objects images (ImageNet dataset), road signboard images\n(SVHN dataset) etc. Unfortunately, there have been very limited amount of work\ndone in the domain of document image processing. Automatic image generation can\nlead to the enormous increase of labeled datasets with the help of only limited\namount of labeled data. Various kinds of Deep generative models can be\nprimarily divided into two categories. First category is auto-encoder (AE) and\nthe second one is Generative Adversarial Networks (GANs). In this paper, we\nhave evaluated various kinds of AE as well as GANs and have compared their\nperformances on hand-written digits dataset (MNIST) and also on historical\nhand-written character dataset of Indonesian BALI language. Moreover, these\ngenerated characters are recognized by using character recognition tool for\ncalculating the statistical performance of these generated characters with\nrespect to original character images.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 12:09:06 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Mondal", "Tanmoy", ""], ["Trang", "LE Thi Thuy", ""], ["Coustaty", "Micka\u00ebl", ""], ["Ogier", "Jean-Marc", ""]]}, {"id": "2002.11430", "submitter": "Xiaoyue Zhang", "authors": "Xiaoyue Zhang, Weijian Jian, Yu Chen, Shihting Yang", "title": "Deform-GAN:An Unsupervised Learning Model for Deformable Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable registration is one of the most challenging task in the field of\nmedical image analysis, especially for the alignment between different\nsequences and modalities. In this paper, a non-rigid registration method is\nproposed for 3D medical images leveraging unsupervised learning. To the best of\nour knowledge, this is the first attempt to introduce gradient loss into\ndeep-learning-based registration. The proposed gradient loss is robust across\nsequences and modals for large deformation. Besides, adversarial learning\napproach is used to transfer multi-modal similarity to mono-modal similarity\nand improve the precision. Neither ground-truth nor manual labeling is required\nduring training. We evaluated our network on a 3D brain registration task\ncomprehensively. The experiments demonstrate that the proposed method can cope\nwith the data which has non-functional intensity relations, noise and blur. Our\napproach outperforms other methods especially in accuracy and speed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 12:20:46 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Zhang", "Xiaoyue", ""], ["Jian", "Weijian", ""], ["Chen", "Yu", ""], ["Yang", "Shihting", ""]]}, {"id": "2002.11433", "submitter": "Chunhua Shen", "authors": "Yifan Liu, Chunhua Shen, Changqian Yu, Jingdong Wang", "title": "Efficient Semantic Video Segmentation with Per-frame Inference", "comments": "Accepted to Proc. Eur. Conf. Computer Vision (ECCV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For semantic segmentation, most existing real-time deep models trained with\neach frame independently may produce inconsistent results for a video sequence.\nAdvanced methods take into considerations the correlations in the video\nsequence, e.g., by propagating the results to the neighboring frames using\noptical flow, or extracting the frame representations with other frames, which\nmay lead to inaccurate results or unbalanced latency. In this work, we process\nefficient semantic video segmentation in a per-frame fashion during the\ninference process. Different from previous per-frame models, we explicitly\nconsider the temporal consistency among frames as extra constraints during the\ntraining process and embed the temporal consistency into the segmentation\nnetwork. Therefore, in the inference process, we can process each frame\nindependently with no latency, and improve the temporal consistency with no\nextra computational cost and post-processing. We employ compact models for\nreal-time execution. To narrow the performance gap between compact models and\nlarge models, new knowledge distillation methods are designed. Our results\noutperform previous keyframe based methods with a better trade-off between the\naccuracy and the inference speed on popular benchmarks, including the\nCityscapes and Camvid. The temporal consistency is also improved compared with\ncorresponding baselines which are trained with each frame independently. Code\nis available at: https://tinyurl.com/segment-video\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 12:24:32 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 12:57:29 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liu", "Yifan", ""], ["Shen", "Chunhua", ""], ["Yu", "Changqian", ""], ["Wang", "Jingdong", ""]]}, {"id": "2002.11434", "submitter": "Kira Vinogradova", "authors": "Kira Vinogradova, Alexandr Dibrov, Gene Myers", "title": "Towards Interpretable Semantic Segmentation via Gradient-weighted Class\n  Activation Mapping", "comments": "2 pages, 2 figures. AAAI 2020 camera-ready", "journal-ref": "Proceedings of the Thirty-Fourth AAAI Conference on Artificial\n  Intelligence, New York, USA, Feb 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have become state-of-the-art in a wide range of\nimage recognition tasks. The interpretation of their predictions, however, is\nan active area of research. Whereas various interpretation methods have been\nsuggested for image classification, the interpretation of image segmentation\nstill remains largely unexplored. To that end, we propose SEG-GRAD-CAM, a\ngradient-based method for interpreting semantic segmentation. Our method is an\nextension of the widely-used Grad-CAM method, applied locally to produce\nheatmaps showing the relevance of individual pixels for semantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 12:32:40 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Vinogradova", "Kira", ""], ["Dibrov", "Alexandr", ""], ["Myers", "Gene", ""]]}, {"id": "2002.11477", "submitter": "Robin Karlsson", "authors": "Robin Karlsson, Erik Sjoberg", "title": "Learning a Directional Soft Lane Affordance Model for Road Scenes Using\n  Self-Supervision", "comments": "Accepted for IEEE IV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans navigate complex environments in an organized yet flexible manner,\nadapting to the context and implicit social rules. Understanding these\nnaturally learned patterns of behavior is essential for applications such as\nautonomous vehicles. However, algorithmically defining these implicit rules of\nhuman behavior remains difficult. This work proposes a novel self-supervised\nmethod for training a probabilistic network model to estimate the regions\nhumans are most likely to drive in as well as a multimodal representation of\nthe inferred direction of travel at each point. The model is trained on\nindividual human trajectories conditioned on a representation of the driving\nenvironment. The model is shown to successfully generalize to new road scenes,\ndemonstrating potential for real-world application as a prior for socially\nacceptable driving behavior in challenging or ambiguous scenarios which are\npoorly handled by explicit traffic rules.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 00:57:34 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 13:19:45 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Karlsson", "Robin", ""], ["Sjoberg", "Erik", ""]]}, {"id": "2002.11493", "submitter": "Fangda Han", "authors": "Fangda Han, Ricardo Guerrero, Vladimir Pavlovic", "title": "CookGAN: Meal Image Synthesis from Ingredients", "comments": "10 pages, 5 figures, accepted by WACV 2020. arXiv admin note:\n  substantial text overlap with arXiv:1905.13149", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new computational framework, based on generative\ndeep models, for synthesis of photo-realistic food meal images from textual\nlist of its ingredients. Previous works on synthesis of images from text\ntypically rely on pre-trained text models to extract text features, followed by\ngenerative neural networks (GAN) aimed to generate realistic images conditioned\non the text features. These works mainly focus on generating spatially compact\nand well-defined categories of objects, such as birds or flowers, but meal\nimages are significantly more complex, consisting of multiple ingredients whose\nappearance and spatial qualities are further modified by cooking methods. To\ngenerate real-like meal images from ingredients, we propose Cook Generative\nAdversarial Networks (CookGAN), CookGAN first builds an attention-based\ningredients-image association model, which is then used to condition a\ngenerative neural network tasked with synthesizing meal images. Furthermore, a\ncycle-consistent constraint is added to further improve image quality and\ncontrol appearance. Experiments show our model is able to generate meal images\ncorresponding to the ingredients.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 00:54:10 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Han", "Fangda", ""], ["Guerrero", "Ricardo", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "2002.11509", "submitter": "Shadrokh Samavi", "authors": "Fateme Mostafaie, Reihaneh Teimouri, Zahra Nabizadeh, Nader Karimi,\n  Shadrokh Samavi", "title": "Region of Interest Identification for Brain Tumors in Magnetic Resonance\n  Images", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glioma is a common type of brain tumor, and accurate detection of it plays a\nvital role in the diagnosis and treatment process. Despite advances in medical\nimage analyzing, accurate tumor segmentation in brain magnetic resonance (MR)\nimages remains a challenge due to variations in tumor texture, position, and\nshape. In this paper, we propose a fast, automated method, with light\ncomputational complexity, to find the smallest bounding box around the tumor\nregion. This region-of-interest can be used as a preprocessing step in training\nnetworks for subregion tumor segmentation. By adopting the outputs of this\nalgorithm, redundant information is removed; hence the network can focus on\nlearning notable features related to subregions' classes. The proposed method\nhas six main stages, in which the brain segmentation is the most vital step.\nExpectation-maximization (EM) and K-means algorithms are used for brain\nsegmentation. The proposed method is evaluated on the BraTS 2015 dataset, and\nthe average gained DICE score is 0.73, which is an acceptable result for this\napplication.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 14:10:40 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Mostafaie", "Fateme", ""], ["Teimouri", "Reihaneh", ""], ["Nabizadeh", "Zahra", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2002.11559", "submitter": "Sukai Wang", "authors": "Sukai Wang, Yuxiang Sun, Chengju Liu, Ming Liu", "title": "PointTrackNet: An End-to-End Network For 3-D Object Detection and\n  Tracking From Point Clouds", "comments": "7 pages, ICRA-RAL2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent machine learning-based multi-object tracking (MOT) frameworks are\nbecoming popular for 3-D point clouds. Most traditional tracking approaches use\nfilters (e.g., Kalman filter or particle filter) to predict object locations in\na time sequence, however, they are vulnerable to extreme motion conditions,\nsuch as sudden braking and turning. In this letter, we propose PointTrackNet,\nan end-to-end 3-D object detection and tracking network, to generate foreground\nmasks, 3-D bounding boxes, and point-wise tracking association displacements\nfor each detected object. The network merely takes as input two adjacent\npoint-cloud frames. Experimental results on the KITTI tracking dataset show\ncompetitive results over the state-of-the-arts, especially in the irregularly\nand rapidly changing scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 15:19:28 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Wang", "Sukai", ""], ["Sun", "Yuxiang", ""], ["Liu", "Chengju", ""], ["Liu", "Ming", ""]]}, {"id": "2002.11566", "submitter": "Ziqi Zhang", "authors": "Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu,\n  Zhengjun Zha", "title": "Object Relational Graph with Teacher-Recommended Learning for Video\n  Captioning", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking full advantage of the information from both vision and language is\ncritical for the video captioning task. Existing models lack adequate visual\nrepresentation due to the neglect of interaction between object, and sufficient\ntraining for content-related words due to long-tailed problems. In this paper,\nwe propose a complete video captioning system including both a novel model and\nan effective training strategy. Specifically, we propose an object relational\ngraph (ORG) based encoder, which captures more detailed interaction features to\nenrich visual representation. Meanwhile, we design a teacher-recommended\nlearning (TRL) method to make full use of the successful external language\nmodel (ELM) to integrate the abundant linguistic knowledge into the caption\nmodel. The ELM generates more semantically similar word proposals which extend\nthe ground-truth words used for training to deal with the long-tailed problem.\nExperimental evaluations on three benchmarks: MSVD, MSR-VTT and VATEX show the\nproposed ORG-TRL system achieves state-of-the-art performance. Extensive\nablation studies and visualizations illustrate the effectiveness of our system.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 15:34:52 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Zhang", "Ziqi", ""], ["Shi", "Yaya", ""], ["Yuan", "Chunfeng", ""], ["Li", "Bing", ""], ["Wang", "Peijin", ""], ["Hu", "Weiming", ""], ["Zha", "Zhengjun", ""]]}, {"id": "2002.11572", "submitter": "Aditya Saligrama", "authors": "Aditya Saligrama and Guillaume Leclerc", "title": "Revisiting Ensembles in an Adversarial Context: Improving Natural\n  Accuracy", "comments": "5 pages, accepted to ICLR 2020 Workshop on Towards Trustworthy ML:\n  Rethinking Security and Privacy for ML", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A necessary characteristic for the deployment of deep learning models in real\nworld applications is resistance to small adversarial perturbations while\nmaintaining accuracy on non-malicious inputs. While robust training provides\nmodels that exhibit better adversarial accuracy than standard models, there is\nstill a significant gap in natural accuracy between robust and non-robust\nmodels which we aim to bridge. We consider a number of ensemble methods\ndesigned to mitigate this performance difference. Our key insight is that model\ntrained to withstand small attacks, when ensembled, can often withstand\nsignificantly larger attacks, and this concept can in turn be leveraged to\noptimize natural accuracy. We consider two schemes, one that combines\npredictions from several randomly initialized robust models, and the other that\nfuses features from robust and standard models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 15:45:58 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Saligrama", "Aditya", ""], ["Leclerc", "Guillaume", ""]]}, {"id": "2002.11581", "submitter": "Han Shu", "authors": "Han Shu and Yunhe Wang", "title": "Automatically Searching for U-Net Image Translator Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image translators have been successfully applied to many important low level\nimage processing tasks. However, classical network architecture of image\ntranslator like U-Net, is borrowed from other vision tasks like biomedical\nimage segmentation. This straightforward adaptation may not be optimal and\ncould cause redundancy in the network structure. In this paper, we propose an\nautomatic architecture searching method for image translator. By utilizing\nevolutionary algorithm, we investigate a more efficient network architecture\nwhich costs less computation resources and achieves better performance than the\noriginal one. Extensive qualitative and quantitative experiments are conducted\nto demonstrate the effectiveness of the proposed method. Moreover, we\ntransplant the searched network architecture to other datasets which are not\ninvolved in the architecture searching procedure. Efficiency of the searched\narchitecture on these datasets further demonstrates the generalization of the\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:05:23 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Shu", "Han", ""], ["Wang", "Yunhe", ""]]}, {"id": "2002.11609", "submitter": "Jiahao Su", "authors": "Jiahao Su, Shiqi Wang, Furong Huang", "title": "ARMA Nets: Expanding Receptive Field for Dense Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global information is essential for dense prediction problems, whose goal is\nto compute a discrete or continuous label for each pixel in the images.\nTraditional convolutional layers in neural networks, initially designed for\nimage classification, are restrictive in these problems since the filter size\nlimits their receptive fields. In this work, we propose to replace any\ntraditional convolutional layer with an autoregressive moving-average (ARMA)\nlayer, a novel module with an adjustable receptive field controlled by the\nlearnable autoregressive coefficients. Compared with traditional convolutional\nlayers, our ARMA layer enables explicit interconnections of the output neurons\nand learns its receptive field by adapting the autoregressive coefficients of\nthe interconnections. ARMA layer is adjustable to different types of tasks: for\ntasks where global information is crucial, it is capable of learning relatively\nlarge autoregressive coefficients to allow for an output neuron's receptive\nfield covering the entire input; for tasks where only local information is\nrequired, it can learn small or near zero autoregressive coefficients and\nautomatically reduces to a traditional convolutional layer. We show both\ntheoretically and empirically that the effective receptive field of networks\nwith ARMA layers (named as ARMA networks) expands with larger autoregressive\ncoefficients. We also provably solve the instability problem of learning and\nprediction in the ARMA layer through a re-parameterization mechanism.\nAdditionally, we demonstrate that ARMA networks substantially improve their\nbaselines on challenging dense prediction tasks including video prediction and\nsemantic segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 23:18:27 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 04:12:39 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Su", "Jiahao", ""], ["Wang", "Shiqi", ""], ["Huang", "Furong", ""]]}, {"id": "2002.11616", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P. Allebach,\n  Chenliang Xu", "title": "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video\n  Super-Resolution", "comments": "This work is accepted in CVPR 2020. The source code and pre-trained\n  model are available on https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020.\n  12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the space-time video super-resolution task, which\naims to generate a high-resolution (HR) slow-motion video from a low frame rate\n(LFR), low-resolution (LR) video. A simple solution is to split it into two\nsub-tasks: video frame interpolation (VFI) and video super-resolution (VSR).\nHowever, temporal interpolation and spatial super-resolution are intra-related\nin this task. Two-stage methods cannot fully take advantage of the natural\nproperty. In addition, state-of-the-art VFI or VSR networks require a large\nframe-synthesis or reconstruction module for predicting high-quality video\nframes, which makes the two-stage methods have large model sizes and thus be\ntime-consuming. To overcome the problems, we propose a one-stage space-time\nvideo super-resolution framework, which directly synthesizes an HR slow-motion\nvideo from an LFR, LR video. Rather than synthesizing missing LR video frames\nas VFI networks do, we firstly temporally interpolate LR frame features in\nmissing LR video frames capturing local temporal contexts by the proposed\nfeature temporal interpolation network. Then, we propose a deformable ConvLSTM\nto align and aggregate temporal information simultaneously for better\nleveraging global temporal contexts. Finally, a deep reconstruction network is\nadopted to predict HR slow-motion video frames. Extensive experiments on\nbenchmark datasets demonstrate that the proposed method not only achieves\nbetter quantitative and qualitative performance but also is more than three\ntimes faster than recent two-stage state-of-the-art methods, e.g., DAIN+EDVR\nand DAIN+RBPN.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:59:48 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Tian", "Yapeng", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""], ["Allebach", "Jan P.", ""], ["Xu", "Chenliang", ""]]}, {"id": "2002.11629", "submitter": "Daizong Liu", "authors": "Daizong Liu, Shuangjie Xu, Pan Zhou, Kun He, Wei Wei, Zichuan Xu", "title": "Dynamic Graph Correlation Learning for Disease Diagnosis with Incomplete\n  Labels", "comments": "Because of the novel coronavirus (2019-nCoV) in Wuhan, China, we can\n  not get the codes in the locked lab. Some authors do not agree to submit. We\n  will re-submit it once we get our codes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease diagnosis on chest X-ray images is a challenging multi-label\nclassification task. Previous works generally classify the diseases\nindependently on the input image without considering any correlation among\ndiseases. However, such correlation actually exists, for example, Pleural\nEffusion is more likely to appear when Pneumothorax is present. In this work,\nwe propose a Disease Diagnosis Graph Convolutional Network (DD-GCN) that\npresents a novel view of investigating the inter-dependency among different\ndiseases by using a dynamic learnable adjacency matrix in graph structure to\nimprove the diagnosis accuracy. To learn more natural and reliable correlation\nrelationship, we feed each node with the image-level individual feature map\ncorresponding to each type of disease. To our knowledge, our method is the\nfirst to build a graph over the feature maps with a dynamic adjacency matrix\nfor correlation learning. To further deal with a practical issue of incomplete\nlabels, DD-GCN also utilizes an adaptive loss and a curriculum learning\nstrategy to train the model on incomplete labels. Experimental results on two\npopular chest X-ray (CXR) datasets show that our prediction accuracy\noutperforms state-of-the-arts, and the learned graph adjacency matrix\nestablishes the correlation representations of different diseases, which is\nconsistent with expert experience. In addition, we apply an ablation study to\ndemonstrate the effectiveness of each component in DD-GCN.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:10:48 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 09:07:04 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Liu", "Daizong", ""], ["Xu", "Shuangjie", ""], ["Zhou", "Pan", ""], ["He", "Kun", ""], ["Wei", "Wei", ""], ["Xu", "Zichuan", ""]]}, {"id": "2002.11644", "submitter": "Hugo Proen\\c{c}a", "authors": "Hugo Proen\\c{c}a, Ehsan Yaghoubi and Pendar Alirezazadeh", "title": "A Quadruplet Loss for Enforcing Semantically Coherent Embeddings in\n  Multi-output Classification Problems", "comments": "10 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes one objective function for learning semantically\ncoherent feature embeddings in multi-output classification problems, i.e., when\nthe response variables have dimension higher than one. In particular, we\nconsider the problems of identity retrieval and soft biometrics labelling in\nvisual surveillance environments, which have been attracting growing interests.\nInspired by the triplet loss [34] function, we propose a generalization that:\n1) defines a metric that considers the number of agreeing labels between pairs\nof elements; and 2) disregards the notion of anchor, replacing d(A1, A2) <\nd(A1, B) by d(A, B) < d(C, D), for A, B, C, D distance constraints, according\nto the number of agreeing labels between pairs. As the triplet loss\nformulation, our proposal also privileges small distances between positive\npairs, but at the same time explicitly enforces that the distance between other\npairs corresponds directly to their similarity in terms of agreeing labels.\nThis yields feature embeddings with a strong correspondence between the classes\ncentroids and their semantic descriptions, i.e., where elements are closer to\nothers that share some of their labels than to elements with fully disjoint\nlabels membership. As practical effect, the proposed loss can be seen as\nparticularly suitable for performing joint coarse (soft label) + fine (ID)\ninference, based on simple rules as k-neighbours, which is a novelty with\nrespect to previous related loss functions. Also, in opposition to its triplet\ncounterpart, the proposed loss is agnostic with regard to any demanding\ncriteria for mining learning instances (such as the semi-hard pairs). Our\nexperiments were carried out in five different datasets (BIODI, LFW, IJB-A,\nMegaface and PETA) and validate our assumptions, showing highly promising\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:18:53 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 13:17:05 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 16:53:14 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Proen\u00e7a", "Hugo", ""], ["Yaghoubi", "Ehsan", ""], ["Alirezazadeh", "Pendar", ""]]}, {"id": "2002.11656", "submitter": "Raymond Baldwin", "authors": "R Wes Baldwin, Mohammed Almatrafi, Jason R Kaufman, Vijayan Asari,\n  Keigo Hirakawa", "title": "Inceptive Event Time-Surfaces for Object Classification Using\n  Neuromorphic Cameras", "comments": null, "journal-ref": "Image Analysis and Recognition. ICIAR 2019. Lecture Notes in\n  Computer Science, vol 11663. Springer, Cham", "doi": "10.1007/978-3-030-27272-2_35", "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel fusion of low-level approaches for dimensionality\nreduction into an effective approach for high-level objects in neuromorphic\ncamera data called Inceptive Event Time-Surfaces (IETS). IETSs overcome several\nlimitations of conventional time-surfaces by increasing robustness to noise,\npromoting spatial consistency, and improving the temporal localization of\n(moving) edges. Combining IETS with transfer learning improves state-of-the-art\nperformance on the challenging problem of object classification utilizing event\ncamera data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:36:27 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Baldwin", "R Wes", ""], ["Almatrafi", "Mohammed", ""], ["Kaufman", "Jason R", ""], ["Asari", "Vijayan", ""], ["Hirakawa", "Keigo", ""]]}, {"id": "2002.11669", "submitter": "Fanta Camara", "authors": "Fanta Camara, Nicola Bellotto, Serhan Cosar, Dimitris Nathanael,\n  Matthias Althoff, Jingyuan Wu, Johannes Ruenz, Andr\\'e Dietrich and Charles\n  W. Fox", "title": "Pedestrian Models for Autonomous Driving Part I: Low-Level Models, from\n  Sensing to Tracking", "comments": "Accepted for publication in the IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2020.3006768", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles (AVs) must share space with pedestrians, both in\ncarriageway cases such as cars at pedestrian crossings and off-carriageway\ncases such as delivery vehicles navigating through crowds on pedestrianized\nhigh-streets. Unlike static obstacles, pedestrians are active agents with\ncomplex, interactive motions. Planning AV actions in the presence of\npedestrians thus requires modelling of their probable future behaviour as well\nas detecting and tracking them. This narrative review article is Part I of a\npair, together surveying the current technology stack involved in this process,\norganising recent research into a hierarchical taxonomy ranging from low-level\nimage detection to high-level psychology models, from the perspective of an AV\ndesigner. This self-contained Part I covers the lower levels of this stack,\nfrom sensing, through detection and recognition, up to tracking of pedestrians.\nTechnologies at these levels are found to be mature and available as\nfoundations for use in high-level systems, such as behaviour modelling,\nprediction and interaction control.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:57:42 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 14:42:27 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Camara", "Fanta", ""], ["Bellotto", "Nicola", ""], ["Cosar", "Serhan", ""], ["Nathanael", "Dimitris", ""], ["Althoff", "Matthias", ""], ["Wu", "Jingyuan", ""], ["Ruenz", "Johannes", ""], ["Dietrich", "Andr\u00e9", ""], ["Fox", "Charles W.", ""]]}, {"id": "2002.11670", "submitter": "Sylvain Peyronnet", "authors": "Ilyes Kacher and Maxime Portaz and Hicham Randrianarivo and Sylvain\n  Peyronnet", "title": "Graphcore C2 Card performance for image-based deep learning application:\n  A Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Graphcore has introduced an IPU Processor for accelerating machine\nlearning applications. The architecture of the processor has been designed to\nachieve state of the art performance on current machine intelligence models for\nboth training and inference.\n  In this paper, we report on a benchmark in which we have evaluated the\nperformance of IPU processors on deep neural networks for inference. We focus\non deep vision models such as ResNeXt. We report the observed latency,\nthroughput and energy efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:58:24 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 22:17:19 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Kacher", "Ilyes", ""], ["Portaz", "Maxime", ""], ["Randrianarivo", "Hicham", ""], ["Peyronnet", "Sylvain", ""]]}, {"id": "2002.11701", "submitter": "Siddharth Biswal", "authors": "Siddharth Biswal, Cao Xiao, Lucas M. Glass, M. Brandon Westover, and\n  Jimeng Sun", "title": "CLARA: Clinical Report Auto-completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating clinical reports from raw recordings such as X-rays and\nelectroencephalogram (EEG) is an essential and routine task for doctors.\nHowever, it is often time-consuming to write accurate and detailed reports.\nMost existing methods try to generate the whole reports from the raw input with\nlimited success because 1) generated reports often contain errors that need\nmanual review and correction, 2) it does not save time when doctors want to\nwrite additional information into the report, and 3) the generated reports are\nnot customized based on individual doctors' preference. We propose {\\it\nCL}inic{\\it A}l {\\it R}eport {\\it A}uto-completion (CLARA), an interactive\nmethod that generates reports in a sentence by sentence fashion based on\ndoctors' anchor words and partially completed sentences. CLARA searches for\nmost relevant sentences from existing reports as the template for the current\nreport. The retrieved sentences are sequentially modified by combining with the\ninput feature representations to create the final report. In our experimental\nevaluation, CLARA achieved 0.393 CIDEr and 0.248 BLEU-4 on X-ray reports and\n0.482 CIDEr and 0.491 BLEU-4 for EEG reports for sentence-level generation,\nwhich is up to 35% improvement over the best baseline. Also via our qualitative\nevaluation, CLARA is shown to produce reports which have a significantly higher\nlevel of approval by doctors in a user study (3.74 out of 5 for CLARA vs 2.52\nout of 5 for the baseline).\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 18:45:00 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 13:32:52 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Biswal", "Siddharth", ""], ["Xiao", "Cao", ""], ["Glass", "Lucas M.", ""], ["Westover", "M. Brandon", ""], ["Sun", "Jimeng", ""]]}, {"id": "2002.11770", "submitter": "Hao Li", "authors": "Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran,\n  Rahul Bhotika, Stefano Soatto", "title": "Rethinking the Hyperparameters for Fine-tuning", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning from pre-trained ImageNet models has become the de-facto standard\nfor various computer vision tasks. Current practices for fine-tuning typically\ninvolve selecting an ad-hoc choice of hyperparameters and keeping them fixed to\nvalues normally used for training from scratch. This paper re-examines several\ncommon practices of setting hyperparameters for fine-tuning. Our findings are\nbased on extensive empirical evaluation for fine-tuning on various transfer\nlearning benchmarks. (1) While prior works have thoroughly investigated\nlearning rate and batch size, momentum for fine-tuning is a relatively\nunexplored parameter. We find that the value of momentum also affects\nfine-tuning performance and connect it with previous theoretical findings. (2)\nOptimal hyperparameters for fine-tuning, in particular, the effective learning\nrate, are not only dataset dependent but also sensitive to the similarity\nbetween the source domain and target domain. This is in contrast to\nhyperparameters for training from scratch. (3) Reference-based regularization\nthat keeps models close to the initial model does not necessarily apply for\n\"dissimilar\" datasets. Our findings challenge common practices of fine-tuning\nand encourages deep learning practitioners to rethink the hyperparameters for\nfine-tuning.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 18:59:52 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Li", "Hao", ""], ["Chaudhari", "Pratik", ""], ["Yang", "Hao", ""], ["Lam", "Michael", ""], ["Ravichandran", "Avinash", ""], ["Bhotika", "Rahul", ""], ["Soatto", "Stefano", ""]]}, {"id": "2002.11810", "submitter": "Miaoyun Zhao", "authors": "Miaoyun Zhao, Yulai Cong, Lawrence Carin", "title": "On Leveraging Pretrained GANs for Generation with Limited Data", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown generative adversarial networks (GANs) can generate\nhighly realistic images, that are often indistinguishable (by humans) from real\nimages. Most images so generated are not contained in the training dataset,\nsuggesting potential for augmenting training sets with GAN-generated data.\nWhile this scenario is of particular relevance when there are limited data\navailable, there is still the issue of training the GAN itself based on that\nlimited data. To facilitate this, we leverage existing GAN models pretrained on\nlarge-scale datasets (like ImageNet) to introduce additional knowledge (which\nmay not exist within the limited data), following the concept of transfer\nlearning. Demonstrated by natural-image generation, we reveal that low-level\nfilters (those close to observations) of both the generator and discriminator\nof pretrained GANs can be transferred to facilitate generation in a\nperceptually-distinct target domain with limited training data. To further\nadapt the transferred filters to the target domain, we propose adaptive filter\nmodulation (AdaFM). An extensive set of experiments is presented to demonstrate\nthe effectiveness of the proposed techniques on generation with limited data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 21:53:36 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 15:42:46 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 03:59:02 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhao", "Miaoyun", ""], ["Cong", "Yulai", ""], ["Carin", "Lawrence", ""]]}, {"id": "2002.11812", "submitter": "Qingyuan Zheng", "authors": "Qingyuan Zheng, Zhuoru Li and Adam Bargteil", "title": "Learning to Shadow Hand-drawn Sketches", "comments": "To appear in CVPR 2020 (Oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automatic method to generate detailed and accurate\nartistic shadows from pairs of line drawing sketches and lighting directions.\nWe also contribute a new dataset of one thousand examples of pairs of line\ndrawings and shadows that are tagged with lighting directions. Remarkably, the\ngenerated shadows quickly communicate the underlying 3D structure of the\nsketched scene. Consequently, the shadows generated by our approach can be used\ndirectly or as an excellent starting point for artists. We demonstrate that the\ndeep learning network we propose takes a hand-drawn sketch, builds a 3D model\nin latent space, and renders the resulting shadows. The generated shadows\nrespect the hand-drawn lines and underlying 3D space and contain sophisticated\nand accurate details, such as self-shadowing effects. Moreover, the generated\nshadows contain artistic effects, such as rim lighting or halos appearing from\nback lighting, that would be achievable with traditional 3D rendering methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 21:57:17 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 23:12:21 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Zheng", "Qingyuan", ""], ["Li", "Zhuoru", ""], ["Bargteil", "Adam", ""]]}, {"id": "2002.11821", "submitter": "Ankit Raj", "authors": "Ankit Raj, Yoram Bresler, Bo Li", "title": "Improving Robustness of Deep-Learning-Based Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning-based methods for different applications have been shown\nvulnerable to adversarial examples. These examples make deployment of such\nmodels in safety-critical tasks questionable. Use of deep neural networks as\ninverse problem solvers has generated much excitement for medical imaging\nincluding CT and MRI, but recently a similar vulnerability has also been\ndemonstrated for these tasks. We show that for such inverse problem solvers,\none should analyze and study the effect of adversaries in the\nmeasurement-space, instead of the signal-space as in previous work. In this\npaper, we propose to modify the training strategy of end-to-end\ndeep-learning-based inverse problem solvers to improve robustness. We introduce\nan auxiliary network to generate adversarial examples, which is used in a\nmin-max formulation to build robust image reconstruction networks.\nTheoretically, we show for a linear reconstruction scheme the min-max\nformulation results in a singular-value(s) filter regularized solution, which\nsuppresses the effect of adversarial examples occurring because of\nill-conditioning in the measurement matrix. We find that a linear network using\nthe proposed min-max learning scheme indeed converges to the same solution. In\naddition, for non-linear Compressed Sensing (CS) reconstruction using deep\nnetworks, we show significant improvement in robustness using the proposed\napproach over other methods. We complement the theory by experiments for CS on\ntwo different datasets and evaluate the effect of increasing perturbations on\ntrained networks. We find the behavior for ill-conditioned and well-conditioned\nmeasurement matrices to be qualitatively different.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 22:12:36 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Raj", "Ankit", ""], ["Bresler", "Yoram", ""], ["Li", "Bo", ""]]}, {"id": "2002.11826", "submitter": "Shihao Jiang", "authors": "Shihao Jiang, Dylan Campbell, Miaomiao Liu, Stephen Gould, Richard\n  Hartley", "title": "Joint Unsupervised Learning of Optical Flow and Egomotion with Bi-Level\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of joint optical flow and camera motion estimation in\nrigid scenes by incorporating geometric constraints into an unsupervised deep\nlearning framework. Unlike existing approaches which rely on brightness\nconstancy and local smoothness for optical flow estimation, we exploit the\nglobal relationship between optical flow and camera motion using epipolar\ngeometry. In particular, we formulate the prediction of optical flow and camera\nmotion as a bi-level optimization problem, consisting of an upper-level problem\nto estimate the flow that conforms to the predicted camera motion, and a\nlower-level problem to estimate the camera motion given the predicted optical\nflow. We use implicit differentiation to enable back-propagation through the\nlower-level geometric optimization layer independent of its implementation,\nallowing end-to-end training of the network. With globally-enforced geometric\nconstraints, we are able to improve the quality of the estimated optical flow\nin challenging scenarios and obtain better camera motion estimates compared to\nother unsupervised learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 22:28:00 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Jiang", "Shihao", ""], ["Campbell", "Dylan", ""], ["Liu", "Miaomiao", ""], ["Gould", "Stephen", ""], ["Hartley", "Richard", ""]]}, {"id": "2002.11841", "submitter": "Yichun Shi", "authors": "Yichun Shi, Xiang Yu, Kihyuk Sohn, Manmohan Chandraker, and Anil K.\n  Jain", "title": "Towards Universal Representation Learning for Deep Face Recognition", "comments": "to appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing wild faces is extremely hard as they appear with all kinds of\nvariations. Traditional methods either train with specifically annotated\nvariation data from target domains, or by introducing unlabeled target\nvariation data to adapt from the training data. Instead, we propose a universal\nrepresentation learning framework that can deal with larger variation unseen in\nthe given training data without leveraging target domain knowledge. We firstly\nsynthesize training data alongside some semantically meaningful variations,\nsuch as low resolution, occlusion and head pose. However, directly feeding the\naugmented data for training will not converge well as the newly introduced\nsamples are mostly hard examples. We propose to split the feature embedding\ninto multiple sub-embeddings, and associate different confidence values for\neach sub-embedding to smooth the training procedure. The sub-embeddings are\nfurther decorrelated by regularizing variation classification loss and\nvariation adversarial loss on different partitions of them. Experiments show\nthat our method achieves top performance on general face recognition datasets\nsuch as LFW and MegaFace, while significantly better on extreme benchmarks such\nas TinyFace and IJB-S.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 23:29:57 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Shi", "Yichun", ""], ["Yu", "Xiang", ""], ["Sohn", "Kihyuk", ""], ["Chandraker", "Manmohan", ""], ["Jain", "Anil K.", ""]]}, {"id": "2002.11848", "submitter": "Ruotian Luo", "authors": "Ruotian Luo, Gregory Shakhnarovich", "title": "Analysis of diversity-accuracy tradeoff in image captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the effect of different model architectures, training\nobjectives, hyperparameter settings and decoding procedures on the diversity of\nautomatically generated image captions. Our results show that 1) simple\ndecoding by naive sampling, coupled with low temperature is a competitive and\nfast method to produce diverse and accurate caption sets; 2) training with\nCIDEr-based reward using Reinforcement learning harms the diversity properties\nof the resulting generator, which cannot be mitigated by manipulating decoding\nparameters. In addition, we propose a new metric AllSPICE for evaluating both\naccuracy and diversity of a set of captions by a single value.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 00:09:25 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Luo", "Ruotian", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "2002.11849", "submitter": "Hossen Teimoorinia", "authors": "Hossen Teimoorinia, Robert D. Toyonaga, Sebastien Fabbro, Connor\n  Bottrell", "title": "Comparison of Multi-Class and Binary Classification Machine Learning\n  Models in Identifying Strong Gravitational Lenses", "comments": "PASP accepted, 14 pages, 10 figures, 4 tables", "journal-ref": null, "doi": "10.1088/1538-3873/ab747b", "report-no": null, "categories": "astro-ph.GA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, binary classification lens-finding schemes are used to\ndiscriminate between lens candidates and non-lenses. However, these models\noften suffer from substantial false-positive classifications. Such false\npositives frequently occur due to images containing objects such as crowded\nsources, galaxies with arms, and also images with a central source and smaller\nsurrounding sources. Therefore, a model might confuse the stated circumstances\nwith an Einstein ring. It has been proposed that by allowing such commonly\nmisclassified image types to constitute their own classes, machine learning\nmodels will more easily be able to learn the difference between images that\ncontain real lenses, and images that contain lens imposters. Using Hubble Space\nTelescope (HST) images, in the F814W filter, we compare the usage of binary and\nmulti-class classification models applied to the lens finding task. From our\nfindings, we conclude there is not a significant benefit to using the\nmulti-class model over a binary model. We will also present the results of a\nsimple lens search using a multi-class machine learning model, and potential\nnew lens candidates.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 00:11:31 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Teimoorinia", "Hossen", ""], ["Toyonaga", "Robert D.", ""], ["Fabbro", "Sebastien", ""], ["Bottrell", "Connor", ""]]}, {"id": "2002.11863", "submitter": "Chuang Niu", "authors": "Chuang Niu, Jun Zhang, Ge Wang, Jimin Liang", "title": "GATCluster: Self-Supervised Gaussian-Attention Network for Image\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised Gaussian ATtention network for image Clustering\n(GATCluster). Rather than extracting intermediate features first and then\nperforming the traditional clustering algorithm, GATCluster directly outputs\nsemantic cluster labels without further post-processing. Theoretically, we give\na Label Feature Theorem to guarantee the learned features are one-hot encoded\nvectors, and the trivial solutions are avoided. To train the GATCluster in a\ncompletely unsupervised manner, we design four self-learning tasks with the\nconstraints of transformation invariance, separability maximization, entropy\nanalysis, and attention mapping. Specifically, the transformation invariance\nand separability maximization tasks learn the relationships between sample\npairs. The entropy analysis task aims to avoid trivial solutions. To capture\nthe object-oriented semantics, we design a self-supervised attention mechanism\nthat includes a parameterized attention module and a soft-attention loss. All\nthe guiding signals for clustering are self-generated during the training\nprocess. Moreover, we develop a two-step learning algorithm that is\nmemory-efficient for clustering large-size images. Extensive experiments\ndemonstrate the superiority of our proposed method in comparison with the\nstate-of-the-art image clustering benchmarks. Our code has been made publicly\navailable at https://github.com/niuchuangnn/GATCluster.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 00:57:18 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 20:09:39 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Niu", "Chuang", ""], ["Zhang", "Jun", ""], ["Wang", "Ge", ""], ["Liang", "Jimin", ""]]}, {"id": "2002.11881", "submitter": "Yu Zhang", "authors": "Yu Zhang, Gongbo Liang, Tawfiq Salem, Nathan Jacobs", "title": "Defense-PointNet: Protecting PointNet Against Adversarial Attacks", "comments": "Accepted by IEEE International Conference on Big Data (BigData)\n  Workshop: The Next Frontier of Big Data From LiDAR, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite remarkable performance across a broad range of tasks, neural networks\nhave been shown to be vulnerable to adversarial attacks. Many works focus on\nadversarial attacks and defenses on 2D images, but few focus on 3D point\nclouds. In this paper, our goal is to enhance the adversarial robustness of\nPointNet, which is one of the most widely used models for 3D point clouds. We\napply the fast gradient sign attack method (FGSM) on 3D point clouds and find\nthat FGSM can be used to generate not only adversarial images but also\nadversarial point clouds. To minimize the vulnerability of PointNet to\nadversarial attacks, we propose Defense-PointNet. We compare our model with two\nbaseline approaches and show that Defense-PointNet significantly improves the\nrobustness of the network against adversarial samples.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 02:35:08 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Zhang", "Yu", ""], ["Liang", "Gongbo", ""], ["Salem", "Tawfiq", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2002.11885", "submitter": "Gaurav Nagesh Shetty", "authors": "Gaurav N.Shetty, Konstantinos Slavakis, Ukash Nakarmi, Gesualdo\n  Scutari, and Leslie Ying", "title": "Kernel Bi-Linear Modeling for Reconstructing Data on Manifolds: The\n  Dynamic-MRI Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a kernel-based framework for reconstructing data on\nmanifolds, tailored to fit the dynamic-(d)MRI-data recovery problem. The\nproposed methodology exploits simple tangent-space geometries of manifolds in\nreproducing kernel Hilbert spaces and follows classical kernel-approximation\narguments to form the data-recovery task as a bi-linear inverse problem.\nDeparting from mainstream approaches, the proposed methodology uses no training\ndata, employs no graph Laplacian matrix to penalize the optimization task, uses\nno costly (kernel) pre-imaging step to map feature points back to the input\nspace, and utilizes complex-valued kernel functions to account for k-space\ndata. The framework is validated on synthetically generated dMRI data, where\ncomparisons against state-of-the-art schemes highlight the rich potential of\nthe proposed approach in data-recovery problems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 02:42:08 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Shetty", "Gaurav N.", ""], ["Slavakis", "Konstantinos", ""], ["Nakarmi", "Ukash", ""], ["Scutari", "Gesualdo", ""], ["Ying", "Leslie", ""]]}, {"id": "2002.11886", "submitter": "Aming Wu", "authors": "Aming Wu, Yahong Han", "title": "Hierarchical Memory Decoding for Video Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of video captioning often employ a recurrent neural network\n(RNN) as the decoder. However, RNN is prone to diluting long-term information.\nRecent works have demonstrated memory network (MemNet) has the advantage of\nstoring long-term information. However, as the decoder, it has not been well\nexploited for video captioning. The reason partially comes from the difficulty\nof sequence decoding with MemNet. Instead of the common practice, i.e.,\nsequence decoding with RNN, in this paper, we devise a novel memory decoder for\nvideo captioning. Concretely, after obtaining representation of each frame\nthrough a pre-trained network, we first fuse the visual and lexical\ninformation. Then, at each time step, we construct a multi-layer MemNet-based\ndecoder, i.e., in each layer, we employ a memory set to store previous\ninformation and an attention mechanism to select the information related to the\ncurrent input. Thus, this decoder avoids the dilution of long-term information.\nAnd the multi-layer architecture is helpful for capturing dependencies between\nframes and word sequences. Experimental results show that even without the\nencoding network, our decoder still could obtain competitive performance and\noutperform the performance of RNN decoder. Furthermore, compared with one-layer\nRNN decoder, our decoder has fewer parameters.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 02:48:10 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Wu", "Aming", ""], ["Han", "Yahong", ""]]}, {"id": "2002.11891", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu, Jessie Lin, Yilin Wang, Balu Adsumilli, and Alan C.\n  Bovik", "title": "BBAND Index: A No-Reference Banding Artifact Predictor", "comments": "Accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Banding artifact, or false contouring, is a common video compression\nimpairment that tends to appear on large flat regions in encoded videos. These\nstaircase-shaped color bands can be very noticeable in high-definition videos.\nHere we study this artifact, and propose a new distortion-specific no-reference\nvideo quality model for predicting banding artifacts, called the Blind BANding\nDetector (BBAND index). BBAND is inspired by human visual models. The proposed\ndetector can generate a pixel-wise banding visibility map and output a banding\nseverity score at both the frame and video levels. Experimental results show\nthat our proposed method outperforms state-of-the-art banding detection\nalgorithms and delivers better consistency with subjective evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 03:05:26 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Tu", "Zhengzhong", ""], ["Lin", "Jessie", ""], ["Wang", "Yilin", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2002.11894", "submitter": "Damien Teney", "authors": "Damien Teney, Ehsan Abbasnejad, Anton van den Hengel", "title": "Unshuffling Data for Improved Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization beyond the training distribution is a core challenge in\nmachine learning. The common practice of mixing and shuffling examples when\ntraining neural networks may not be optimal in this regard. We show that\npartitioning the data into well-chosen, non-i.i.d. subsets treated as multiple\ntraining environments can guide the learning of models with better\nout-of-distribution generalization. We describe a training procedure to capture\nthe patterns that are stable across environments while discarding spurious\nones. The method makes a step beyond correlation-based learning: the choice of\nthe partitioning allows injecting information about the task that cannot be\notherwise recovered from the joint distribution of the training data. We\ndemonstrate multiple use cases with the task of visual question answering,\nwhich is notorious for dataset biases. We obtain significant improvements on\nVQA-CP, using environments built from prior knowledge, existing meta data, or\nunsupervised clustering. We also get improvements on GQA using annotations of\n\"equivalent questions\", and on multi-dataset training (VQA v2 / Visual Genome)\nby treating them as distinct environments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 03:07:41 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 22:14:27 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 23:14:33 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Teney", "Damien", ""], ["Abbasnejad", "Ehsan", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2002.11896", "submitter": "Robert Giaquinto", "authors": "Robert Giaquinto and Arindam Banerjee", "title": "Gradient Boosted Normalizing Flows", "comments": "Appearing in the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By chaining a sequence of differentiable invertible transformations,\nnormalizing flows (NF) provide an expressive method of posterior approximation,\nexact density evaluation, and sampling. The trend in normalizing flow\nliterature has been to devise deeper, more complex transformations to achieve\ngreater flexibility. We propose an alternative: Gradient Boosted Normalizing\nFlows (GBNF) model a density by successively adding new NF components with\ngradient boosting. Under the boosting framework, each new NF component\noptimizes a sample weighted likelihood objective, resulting in new components\nthat are fit to the residuals of the previously trained components. The GBNF\nformulation results in a mixture model structure, whose flexibility increases\nas more components are added. Moreover, GBNFs offer a wider, as opposed to\nstrictly deeper, approach that improves existing NFs at the cost of additional\ntraining---not more complex transformations. We demonstrate the effectiveness\nof this technique for density estimation and, by coupling GBNF with a\nvariational autoencoder, generative modeling of images. Our results show that\nGBNFs outperform their non-boosted analog, and, in some cases, produce better\nresults with smaller, simpler flows.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 03:12:08 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 19:55:35 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 05:06:45 GMT"}, {"version": "v4", "created": "Sat, 17 Oct 2020 20:09:27 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Giaquinto", "Robert", ""], ["Banerjee", "Arindam", ""]]}, {"id": "2002.11898", "submitter": "Jamal Molin", "authors": "Jamal Lottier Molin, Chetan Singh Thakur, Ralph Etienne-Cummings,\n  Ernst Niebur", "title": "A Neuromorphic Proto-Object Based Dynamic Visual Saliency Model with an\n  FPGA Implementation", "comments": "15 pages, 6 figures, 6 tables, journal", "journal-ref": null, "doi": "10.1109/TBCAS.2021.3089622", "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to attend to salient regions of a visual scene is an innate and\nnecessary preprocessing step for both biological and engineered systems\nperforming high-level visual tasks (e.g. object detection, tracking, and\nclassification). Computational efficiency, in regard to processing bandwidth\nand speed, is improved by only devoting computational resources to salient\nregions of the visual stimuli. In this paper, we first present a neuromorphic,\nbottom-up, dynamic visual saliency model based on the notion of proto-objects.\nThis is achieved by incorporating the temporal characteristics of the visual\nstimulus into the model, similarly to the manner in which early stages of the\nhuman visual system extracts temporal information. This neuromorphic model\noutperforms state-of-the-art dynamic visual saliency models in predicting human\neye fixations on a commonly used video dataset with associated eye tracking\ndata. Secondly, for this model to have practical applications, it must be\ncapable of performing its computations in real-time under low-power,\nsmall-size, and lightweight constraints. To address this, we introduce a\nField-Programmable Gate Array implementation of the model on an Opal Kelly 7350\nKintex-7 board. This novel hardware implementation allows for processing of up\nto 23.35 frames per second running on a 100 MHz clock - better than 26x speedup\nfrom the software implementation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 03:31:56 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 01:49:42 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 02:04:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Molin", "Jamal Lottier", ""], ["Thakur", "Chetan Singh", ""], ["Etienne-Cummings", "Ralph", ""], ["Niebur", "Ernst", ""]]}, {"id": "2002.11912", "submitter": "Randall Balestriero", "authors": "Randall Balestriero, Sebastien Paris, Richard Baraniuk", "title": "Max-Affine Spline Insights into Deep Generative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We connect a large class of Generative Deep Networks (GDNs) with spline\noperators in order to derive their properties, limitations, and new\nopportunities. By characterizing the latent space partition, dimension and\nangularity of the generated manifold, we relate the manifold dimension and\napproximation error to the sample size. The manifold-per-region affine subspace\ndefines a local coordinate basis; we provide necessary and sufficient\nconditions relating those basis vectors with disentanglement. We also derive\nthe output probability density mapped onto the generated manifold in terms of\nthe latent space density, which enables the computation of key statistics such\nas its Shannon entropy. This finding also enables the computation of the GDN\nlikelihood, which provides a new mechanism for model comparison as well as\nproviding a quality measure for (generated) samples under the learned\ndistribution. We demonstrate how low entropy and/or multimodal distributions\nare not naturally modeled by DGNs and are a cause of training instabilities.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 00:20:02 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Balestriero", "Randall", ""], ["Paris", "Sebastien", ""], ["Baraniuk", "Richard", ""]]}, {"id": "2002.11918", "submitter": "Kongming Liang", "authors": "Shen Wang, Kongming Liang, Chengwei Pan, Chuyang Ye, Xiuli Li, Feng\n  Liu, Yizhou Yu, Yizhou Wang", "title": "Segmentation-based Method combined with Dynamic Programming for Brain\n  Midline Delineation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The midline related pathological image features are crucial for evaluating\nthe severity of brain compression caused by stroke or traumatic brain injury\n(TBI). The automated midline delineation not only improves the assessment and\nclinical decision making for patients with stroke symptoms or head trauma but\nalso reduces the time of diagnosis. Nevertheless, most of the previous methods\nmodel the midline by localizing the anatomical points, which are hard to detect\nor even missing in severe cases. In this paper, we formulate the brain midline\ndelineation as a segmentation task and propose a three-stage framework. The\nproposed framework firstly aligns an input CT image into the standard space.\nThen, the aligned image is processed by a midline detection network (MD-Net)\nintegrated with the CoordConv Layer and Cascade AtrousCconv Module to obtain\nthe probability map. Finally, we formulate the optimal midline selection as a\npathfinding problem to solve the problem of the discontinuity of midline\ndelineation. Experimental results show that our proposed framework can achieve\nsuperior performance on one in-house dataset and one public dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 05:12:18 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Wang", "Shen", ""], ["Liang", "Kongming", ""], ["Pan", "Chengwei", ""], ["Ye", "Chuyang", ""], ["Li", "Xiuli", ""], ["Liu", "Feng", ""], ["Yu", "Yizhou", ""], ["Wang", "Yizhou", ""]]}, {"id": "2002.11921", "submitter": "Aditya Kusupati", "authors": "Oindrila Saha, Aditya Kusupati, Harsha Vardhan Simhadri, Manik Varma,\n  Prateek Jain", "title": "RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference", "comments": "25 pages, 8 figures. Published at Advances in Neural Information\n  Processing Systems (NeurIPS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Convolutional Neural Networks (CNNs) designed for computer vision\ntasks tend to have large intermediate activation maps. These require large\nworking memory and are thus unsuitable for deployment on resource-constrained\ndevices typically used for inference on the edge. Aggressively downsampling the\nimages via pooling or strided convolutions can address the problem but leads to\na significant decrease in accuracy due to gross aggregation of the feature map\nby standard pooling operators. In this paper, we introduce RNNPool, a novel\npooling operator based on Recurrent Neural Networks (RNNs), that efficiently\naggregates features over large patches of an image and rapidly downsamples\nactivation maps. Empirical evaluation indicates that an RNNPool layer can\neffectively replace multiple blocks in a variety of architectures such as\nMobileNets, DenseNet when applied to standard vision tasks like image\nclassification and face detection. That is, RNNPool can significantly decrease\ncomputational complexity and peak memory usage for inference while retaining\ncomparable accuracy. We use RNNPool with the standard S3FD architecture to\nconstruct a face detection method that achieves state-of-the-art MAP for tiny\nARM Cortex-M4 class microcontrollers with under 256 KB of RAM. Code is released\nat https://github.com/Microsoft/EdgeML.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 05:22:44 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 21:05:24 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Saha", "Oindrila", ""], ["Kusupati", "Aditya", ""], ["Simhadri", "Harsha Vardhan", ""], ["Varma", "Manik", ""], ["Jain", "Prateek", ""]]}, {"id": "2002.11925", "submitter": "Jun Li", "authors": "Jun Li, Sinisa Todorovic", "title": "Set-Constrained Viterbi for Set-Supervised Action Segmentation", "comments": "CVPR 2020 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about weakly supervised action segmentation, where the ground\ntruth specifies only a set of actions present in a training video, but not\ntheir true temporal ordering. Prior work typically uses a classifier that\nindependently labels video frames for generating the pseudo ground truth, and\nmultiple instance learning for training the classifier. We extend this\nframework by specifying an HMM, which accounts for co-occurrences of action\nclasses and their temporal lengths, and by explicitly training the HMM on a\nViterbi-based loss. Our first contribution is the formulation of a new\nset-constrained Viterbi algorithm (SCV). Given a video, the SCV generates the\nMAP action segmentation that satisfies the ground truth. This prediction is\nused as a framewise pseudo ground truth in our HMM training. Our second\ncontribution in training is a new regularization of feature affinities between\ntraining videos that share the same action classes. Evaluation on action\nsegmentation and alignment on the Breakfast, MPII Cooking2, Hollywood Extended\ndatasets demonstrates our significant performance improvement for the two tasks\nover prior work.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 05:32:52 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 23:00:12 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Li", "Jun", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "2002.11927", "submitter": "Abduallah Mohamed", "authors": "Abduallah Mohamed, Kun Qian, Mohamed Elhoseiny, Christian Claudel", "title": "Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural\n  Network for Human Trajectory Prediction", "comments": "Accepted by CVPR 2020", "journal-ref": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 14424-14432", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Better machine understanding of pedestrian behaviors enables faster progress\nin modeling interactions between agents such as autonomous vehicles and humans.\nPedestrian trajectories are not only influenced by the pedestrian itself but\nalso by interaction with surrounding objects. Previous methods modeled these\ninteractions by using a variety of aggregation methods that integrate different\nlearned pedestrians states. We propose the Social Spatio-Temporal Graph\nConvolutional Neural Network (Social-STGCNN), which substitutes the need of\naggregation methods by modeling the interactions as a graph. Our results show\nan improvement over the state of art by 20% on the Final Displacement Error\n(FDE) and an improvement on the Average Displacement Error (ADE) with 8.5 times\nless parameters and up to 48 times faster inference speed than previously\nreported methods. In addition, our model is data efficient, and exceeds\nprevious state of the art on the ADE metric with only 20% of the training data.\nWe propose a kernel function to embed the social interactions between\npedestrians within the adjacency matrix. Through qualitative analysis, we show\nthat our model inherited social behaviors that can be expected between\npedestrians trajectories. Code is available at\nhttps://github.com/abduallahmohamed/Social-STGCNN.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 05:40:23 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 03:02:48 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 06:07:03 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Mohamed", "Abduallah", ""], ["Qian", "Kun", ""], ["Elhoseiny", "Mohamed", ""], ["Claudel", "Christian", ""]]}, {"id": "2002.11930", "submitter": "Yuming Shen", "authors": "Yuming Shen, Jie Qin, Jiaxin Chen, Mengyang Yu, Li Liu, Fan Zhu, Fumin\n  Shen, Ling Shao", "title": "Auto-Encoding Twin-Bottleneck Hashing", "comments": "CVPR 2020 Accepted, Code at https://github.com/ymcidence/TBH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional unsupervised hashing methods usually take advantage of\nsimilarity graphs, which are either pre-computed in the high-dimensional space\nor obtained from random anchor points. On the one hand, existing methods\nuncouple the procedures of hash function learning and graph construction. On\nthe other hand, graphs empirically built upon original data could introduce\nbiased prior knowledge of data relevance, leading to sub-optimal retrieval\nperformance. In this paper, we tackle the above problems by proposing an\nefficient and adaptive code-driven graph, which is updated by decoding in the\ncontext of an auto-encoder. Specifically, we introduce into our framework twin\nbottlenecks (i.e., latent variables) that exchange crucial information\ncollaboratively. One bottleneck (i.e., binary codes) conveys the high-level\nintrinsic data structure captured by the code-driven graph to the other (i.e.,\ncontinuous variables for low-level detail information), which in turn\npropagates the updated network feedback for the encoder to learn more\ndiscriminative binary codes. The auto-encoding learning objective literally\nrewards the code-driven graph to learn an optimal encoder. Moreover, the\nproposed model can be simply optimized by gradient descent without violating\nthe binary constraints. Experiments on benchmarked datasets clearly show the\nsuperiority of our framework over the state-of-the-art hashing methods. Our\nsource code can be found at https://github.com/ymcidence/TBH.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 05:58:12 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 09:14:58 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Shen", "Yuming", ""], ["Qin", "Jie", ""], ["Chen", "Jiaxin", ""], ["Yu", "Mengyang", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Shen", "Fumin", ""], ["Shao", "Ling", ""]]}, {"id": "2002.11934", "submitter": "Tomojit Ghosh", "authors": "Tomojit Ghosh and Michael Kirby", "title": "Supervised Dimensionality Reduction and Visualization using\n  Centroid-encoder", "comments": "25 pages (including 3 reference pages), 12 figures. I am planning to\n  submit the paper to JMLR very soon. Centroid-encoder was applied on a\n  biological pathway data\n  (https://www.sciencedirect.com/science/article/pii/S1046202317300439). In\n  this paper we throughly analyzed the algorithm and compared it with\n  state-of-the art techniques on a 8 data sets including MNIST, USPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing high-dimensional data is an essential task in Data Science and\nMachine Learning. The Centroid-Encoder (CE) method is similar to the\nautoencoder but incorporates label information to keep objects of a class close\ntogether in the reduced visualization space. CE exploits nonlinearity and\nlabels to encode high variance in low dimensions while capturing the global\nstructure of the data. We present a detailed analysis of the method using a\nwide variety of data sets and compare it with other supervised dimension\nreduction techniques, including NCA, nonlinear NCA, t-distributed NCA,\nt-distributed MCML, supervised UMAP, supervised PCA, Colored Maximum Variance\nUnfolding, supervised Isomap, Parametric Embedding, supervised Neighbor\nRetrieval Visualizer, and Multiple Relational Embedding. We empirically show\nthat centroid-encoder outperforms most of these techniques. We also show that\nwhen the data variance is spread across multiple modalities, centroid-encoder\nextracts a significant amount of information from the data in low dimensional\nspace. This key feature establishes its value to use it as a tool for data\nvisualization.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 06:08:22 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 23:22:24 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Ghosh", "Tomojit", ""], ["Kirby", "Michael", ""]]}, {"id": "2002.11939", "submitter": "Hong-Xing Yu", "authors": "Hong-Xing Yu, Wei-Shi Zheng", "title": "Weakly supervised discriminative feature learning with state information\n  for person identification", "comments": "To appear at CVPR20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of identity-discriminative visual feature is appealing\nin real-world tasks where manual labelling is costly. However, the images of an\nidentity can be visually discrepant when images are taken under different\nstates, e.g. different camera views and poses. This visual discrepancy leads to\ngreat difficulty in unsupervised discriminative learning. Fortunately, in\nreal-world tasks we could often know the states without human annotation, e.g.\nwe can easily have the camera view labels in person re-identification and\nfacial pose labels in face recognition. In this work we propose utilizing the\nstate information as weak supervision to address the visual discrepancy caused\nby different states. We formulate a simple pseudo label model and utilize the\nstate information in an attempt to refine the assigned pseudo labels by the\nweakly supervised decision boundary rectification and weakly supervised feature\ndrift regularization. We evaluate our model on unsupervised person\nre-identification and pose-invariant face recognition. Despite the simplicity\nof our method, it could outperform the state-of-the-art results on Duke-reID,\nMultiPIE and CFP datasets with a standard ResNet-50 backbone. We also find our\nmodel could perform comparably with the standard supervised fine-tuning results\non the three datasets. Code is available at\nhttps://github.com/KovenYu/state-information\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 06:33:56 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Yu", "Hong-Xing", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2002.11948", "submitter": "Jan Fabian Schmid", "authors": "Jan Fabian Schmid, Stephan F. Simon, Rudolf Mester", "title": "Features for Ground Texture Based Localization -- A Survey", "comments": "Published at the 30th British Machine Vision Conference (BMVC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground texture based vehicle localization using feature-based methods is a\npromising approach to achieve infrastructure-free high-accuracy localization.\nIn this paper, we provide the first extensive evaluation of available feature\nextraction methods for this task, using separately taken image pairs as well as\nsynthetic transformations. We identify AKAZE, SURF and CenSurE as best\nperforming keypoint detectors, and find pairings of CenSurE with the ORB, BRIEF\nand LATCH feature descriptors to achieve greatest success rates for incremental\nlocalization, while SIFT stands out when considering severe synthetic\ntransformations as they might occur during absolute localization.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 07:25:41 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 09:58:42 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Schmid", "Jan Fabian", ""], ["Simon", "Stephan F.", ""], ["Mester", "Rudolf", ""]]}, {"id": "2002.11949", "submitter": "Kaihua Tang", "authors": "Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, Hanwang Zhang", "title": "Unbiased Scene Graph Generation from Biased Training", "comments": "This paper is accepted by CVPR 2020. The code is publicly available\n  on GitHub: https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's scene graph generation (SGG) task is still far from practical, mainly\ndue to the severe training bias, e.g., collapsing diverse \"human walk on / sit\non / lay on beach\" into \"human on beach\". Given such SGG, the down-stream tasks\nsuch as VQA can hardly infer better scene structures than merely a bag of\nobjects. However, debiasing in SGG is not trivial because traditional debiasing\nmethods cannot distinguish between the good and bad bias, e.g., good context\nprior (e.g., \"person read book\" rather than \"eat\") and bad long-tailed bias\n(e.g., \"near\" dominating \"behind / in front of\"). In this paper, we present a\nnovel SGG framework based on causal inference but not the conventional\nlikelihood. We first build a causal graph for SGG, and perform traditional\nbiased training with the graph. Then, we propose to draw the counterfactual\ncausality from the trained graph to infer the effect from the bad bias, which\nshould be removed. In particular, we use Total Direct Effect (TDE) as the\nproposed final predicate score for unbiased SGG. Note that our framework is\nagnostic to any SGG model and thus can be widely applied in the community who\nseeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit\non the SGG benchmark Visual Genome and several prevailing models, we observed\nsignificant improvements over the previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 07:29:53 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 15:46:25 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 07:55:13 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Tang", "Kaihua", ""], ["Niu", "Yulei", ""], ["Huang", "Jianqiang", ""], ["Shi", "Jiaxin", ""], ["Zhang", "Hanwang", ""]]}, {"id": "2002.11965", "submitter": "Eran Hof", "authors": "Eran Hof, Amichai Sanderovich, Evyatar Hemo", "title": "Face Verification Using 60~GHz 802.11 waveforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of an identity based on the human face radar signature in mmwave\nis studied. The chipset for 802.11ad/y networking that is cable of operating in\na radar mode is used. A dataset with faces of 200 different persons was\ncollected for the testing. Our preliminary study shows promising results for\nthe application of autoencoder for the setup at hand.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 08:34:51 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 06:52:11 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hof", "Eran", ""], ["Sanderovich", "Amichai", ""], ["Hemo", "Evyatar", ""]]}, {"id": "2002.11977", "submitter": "Zan Gao", "authors": "Z. Gao, K.X Xue, S.H Wan", "title": "Multiple Discrimination and Pairwise CNN for View-based 3D Object\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development and wide application of computer, camera device,\nnetwork and hardware technology, 3D object (or model) retrieval has attracted\nwidespread attention and it has become a hot research topic in the computer\nvision domain. Deep learning features already available in 3D object retrieval\nhave been proven to be better than the retrieval performance of hand-crafted\nfeatures. However, most existing networks do not take into account the impact\nof multi-view image selection on network training, and the use of contrastive\nloss alone only forcing the same-class samples to be as close as possible. In\nthis work, a novel solution named Multi-view Discrimination and Pairwise CNN\n(MDPCNN) for 3D object retrieval is proposed to tackle these issues. It can\nsimultaneously input of multiple batches and multiple views by adding the Slice\nlayer and the Concat layer. Furthermore, a highly discriminative network is\nobtained by training samples that are not easy to be classified by clustering.\nLastly, we deploy the contrastive-center loss and contrastive loss as the\noptimization objective that has better intra-class compactness and inter-class\nseparability. Large-scale experiments show that the proposed MDPCNN can achieve\na significant performance over the state-of-the-art algorithms in 3D object\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 09:11:23 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Gao", "Z.", ""], ["Xue", "K. X", ""], ["Wan", "S. H", ""]]}, {"id": "2002.12017", "submitter": "SeongMin Kye", "authors": "Seong Min Kye, Hae Beom Lee, Hoirin Kim, and Sung Ju Hwang", "title": "Meta-Learned Confidence for Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transductive inference is an effective means of tackling the data deficiency\nproblem in few-shot learning settings. A popular transductive inference\ntechnique for few-shot metric-based approaches, is to update the prototype of\neach class with the mean of the most confident query examples, or\nconfidence-weighted average of all the query samples. However, a caveat here is\nthat the model confidence may be unreliable, which may lead to incorrect\npredictions. To tackle this issue, we propose to meta-learn the confidence for\neach query sample, to assign optimal weights to unlabeled queries such that\nthey improve the model's transductive inference performance on unseen tasks. We\nachieve this by meta-learning an input-adaptive distance metric over a task\ndistribution under various model and data perturbations, which will enforce\nconsistency on the model predictions under diverse uncertainties for unseen\ntasks. Moreover, we additionally suggest a regularization which explicitly\nenforces the consistency on the predictions across the different dimensions of\na high-dimensional embedding vector. We validate our few-shot learning model\nwith meta-learned confidence on four benchmark datasets, on which it largely\noutperforms strong recent baselines and obtains new state-of-the-art results.\nFurther application on semi-supervised few-shot learning tasks also yields\nsignificant performance improvements over the baselines. The source code of our\nalgorithm is available at https://github.com/seongmin-kye/MCT.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 10:22:17 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 14:13:47 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Kye", "Seong Min", ""], ["Lee", "Hae Beom", ""], ["Kim", "Hoirin", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2002.12041", "submitter": "Quan Tang", "authors": "Quan Tang, Fagui Liu, Tong Zhang, Jun Jiang and Yu Zhang", "title": "Attention-guided Chained Context Aggregation for Semantic Segmentation", "comments": "Wrong numbers in Table 7 of version v3 have been corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way features propagate in Fully Convolutional Networks is of momentous\nimportance to capture multi-scale contexts for obtaining precise segmentation\nmasks. This paper proposes a novel series-parallel hybrid paradigm called the\nChained Context Aggregation Module (CAM) to diversify feature propagation. CAM\ngains features of various spatial scales through chain-connected ladder-style\ninformation flows and fuses them in a two-stage process, namely pre-fusion and\nre-fusion. The serial flow continuously increases receptive fields of output\nneurons and those in parallel encode different region-based contexts. Each\ninformation flow is a shallow encoder-decoder with appropriate down-sampling\nscales to sufficiently capture contextual information. We further adopt an\nattention model in CAM to guide feature re-fusion. Based on these developments,\nwe construct the Chained Context Aggregation Network (CANet), which employs an\nasymmetric decoder to recover precise spatial details of prediction maps. We\nconduct extensive experiments on six challenging datasets, including Pascal VOC\n2012, Pascal Context, Cityscapes, CamVid, SUN-RGBD and GATECH. Results evidence\nthat CANet achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 11:26:56 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 07:00:54 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 08:01:47 GMT"}, {"version": "v4", "created": "Fri, 21 May 2021 03:25:20 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Tang", "Quan", ""], ["Liu", "Fagui", ""], ["Zhang", "Tong", ""], ["Jiang", "Jun", ""], ["Zhang", "Yu", ""]]}, {"id": "2002.12046", "submitter": "Jiarong Chen", "authors": "Jiarong Chen, Zongqing Lu, Jing-Hao Xue, Qingmin Liao", "title": "XSepConv: Extremely Separated Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depthwise convolution has gradually become an indispensable operation for\nmodern efficient neural networks and larger kernel sizes ($\\ge5$) have been\napplied to it recently. In this paper, we propose a novel extremely separated\nconvolutional block (XSepConv), which fuses spatially separable convolutions\ninto depthwise convolution to further reduce both the computational cost and\nparameter size of large kernels. Furthermore, an extra $2\\times2$ depthwise\nconvolution coupled with improved symmetric padding strategy is employed to\ncompensate for the side effect brought by spatially separable convolutions.\nXSepConv is designed to be an efficient alternative to vanilla depthwise\nconvolution with large kernel sizes. To verify this, we use XSepConv for the\nstate-of-the-art architecture MobileNetV3-Small and carry out extensive\nexperiments on four highly competitive benchmark datasets (CIFAR-10, CIFAR-100,\nSVHN and Tiny-ImageNet) to demonstrate that XSepConv can indeed strike a better\ntrade-off between accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 11:46:17 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Chen", "Jiarong", ""], ["Lu", "Zongqing", ""], ["Xue", "Jing-Hao", ""], ["Liao", "Qingmin", ""]]}, {"id": "2002.12047", "submitter": "Ethan Harris", "authors": "Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan Niranjan, Adam\n  Pr\\\"ugel-Bennett, Jonathon Hare", "title": "FMix: Enhancing Mixed Sample Data Augmentation", "comments": "Code available at https://github.com/ecs-vlc/FMix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed Sample Data Augmentation (MSDA) has received increasing attention in\nrecent years, with many successful variants such as MixUp and CutMix. By\nstudying the mutual information between the function learned by a VAE on the\noriginal data and on the augmented data we show that MixUp distorts learned\nfunctions in a way that CutMix does not. We further demonstrate this by showing\nthat MixUp acts as a form of adversarial training, increasing robustness to\nattacks such as Deep Fool and Uniform Noise which produce examples similar to\nthose generated by MixUp. We argue that this distortion prevents models from\nlearning about sample specific features in the data, aiding generalisation\nperformance. In contrast, we suggest that CutMix works more like a traditional\naugmentation, improving performance by preventing memorisation without\ndistorting the data distribution. However, we argue that an MSDA which builds\non CutMix to include masks of arbitrary shape, rather than just square, could\nfurther prevent memorisation whilst preserving the data distribution in the\nsame way. To this end, we propose FMix, an MSDA that uses random binary masks\nobtained by applying a threshold to low frequency images sampled from Fourier\nspace. These random masks can take on a wide range of shapes and can be\ngenerated for use with one, two, and three dimensional data. FMix improves\nperformance over MixUp and CutMix, without an increase in training time, for a\nnumber of models across a range of data sets and problem settings, obtaining a\nnew single model state-of-the-art result on CIFAR-10 without external data.\nFinally, we show that a consequence of the difference between interpolating\nMSDA such as MixUp and masking MSDA such as FMix is that the two can be\ncombined to improve performance even further. Code for all experiments is\nprovided at https://github.com/ecs-vlc/FMix .\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 11:46:33 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 13:12:35 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 14:47:36 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Harris", "Ethan", ""], ["Marcu", "Antonia", ""], ["Painter", "Matthew", ""], ["Niranjan", "Mahesan", ""], ["Pr\u00fcgel-Bennett", "Adam", ""], ["Hare", "Jonathon", ""]]}, {"id": "2002.12079", "submitter": "Pierre-Henri Conze", "authors": "Yutong Yan, Pierre-Henri Conze, Gwenol\\'e Quellec, Mathieu Lamard,\n  B\\'eatrice Cochener, Gouenou Coatrieux", "title": "Two-stage multi-scale breast mass segmentation for full mammogram\n  analysis without user intervention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography is the primary imaging modality used for early detection and\ndiagnosis of breast cancer. X-ray mammogram analysis mainly refers to the\nlocalization of suspicious regions of interest followed by segmentation,\ntowards further lesion classification into benign versus malignant. Among\ndiverse types of breast abnormalities, masses are the most important clinical\nfindings of breast carcinomas. However, manually segmenting breast masses from\nnative mammograms is time-consuming and error-prone. Therefore, an integrated\ncomputer-aided diagnosis system is required to assist clinicians for automatic\nand precise breast mass delineation. In this work, we present a two-stage\nmulti-scale pipeline that provides accurate mass contours from high-resolution\nfull mammograms. First, we propose an extended deep detector integrating a\nmulti-scale fusion strategy for automated mass localization. Second, a\nconvolutional encoder-decoder network using nested and dense skip connections\nis employed to fine-delineate candidate masses. Unlike most previous studies\nbased on segmentation from regions, our framework handles mass segmentation\nfrom native full mammograms without any user intervention. Trained on INbreast\nand DDSM-CBIS public datasets, the pipeline achieves an overall average Dice of\n80.44% on INbreast test images, outperforming state-of-the-art. Our system\nshows promising accuracy as an automatic full-image mass segmentation system.\nExtensive experiments reveals robustness against the diversity of size, shape\nand appearance of breast masses, towards better interaction-free computer-aided\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 13:16:22 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 09:50:41 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Yan", "Yutong", ""], ["Conze", "Pierre-Henri", ""], ["Quellec", "Gwenol\u00e9", ""], ["Lamard", "Mathieu", ""], ["Cochener", "B\u00e9atrice", ""], ["Coatrieux", "Gouenou", ""]]}, {"id": "2002.12093", "submitter": "Martins Bruveris", "authors": "Martins Bruveris, Jochem Gietema, Pouria Mortazavian, Mohan Mahadevan", "title": "Reducing Geographic Performance Differential for Face Recognition", "comments": "Demographic Variation in the Performance of Biometric Systems\n  workshop at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As face recognition algorithms become more accurate and get deployed more\nwidely, it becomes increasingly important to ensure that the algorithms work\nequally well for everyone. We study the geographic performance\ndifferentials-differences in false acceptance and false rejection rates across\ndifferent countries-when comparing selfies against photos from ID documents. We\nshow how to mitigate geographic performance differentials using sampling\nstrategies despite large imbalances in the dataset. Using vanilla domain\nadaptation strategies to fine-tune a face recognition CNN on domain-specific\ndoc-selfie data improves the performance of the model on such data, but, in the\npresence of imbalanced training data, also significantly increases the\ndemographic bias. We then show how to mitigate this effect by employing\nsampling strategies to balance the training procedure.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 13:56:09 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Bruveris", "Martins", ""], ["Gietema", "Jochem", ""], ["Mortazavian", "Pouria", ""], ["Mahadevan", "Mohan", ""]]}, {"id": "2002.12096", "submitter": "Hiteshi Jain Ms.", "authors": "Hiteshi Jain, Gaurav Harit, Avinash Sharma", "title": "Action Quality Assessment using Siamese Network-Based Deep Metric\n  Learning", "comments": "12 pages, 5 Figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated vision-based score estimation models can be used as an alternate\nopinion to avoid judgment bias. In the past works the score estimation models\nwere learned by regressing the video representations to the ground truth score\nprovided by the judges. However such regression-based solutions lack\ninterpretability in terms of giving reasons for the awarded score. One solution\nto make the scores more explicable is to compare the given action video with a\nreference video. This would capture the temporal variations w.r.t. the\nreference video and map those variations to the final score. In this work, we\npropose a new action scoring system as a two-phase system: (1) A Deep Metric\nLearning Module that learns similarity between any two action videos based on\ntheir ground truth scores given by the judges; (2) A Score Estimation Module\nthat uses the first module to find the resemblance of a video to a reference\nvideo in order to give the assessment score. The proposed scoring model has\nbeen tested for Olympics Diving and Gymnastic vaults and the model outperforms\nthe existing state-of-the-art scoring models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 14:00:05 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Jain", "Hiteshi", ""], ["Harit", "Gaurav", ""], ["Sharma", "Avinash", ""]]}, {"id": "2002.12105", "submitter": "Evelien Schat", "authors": "Evelien Schat, Rens van de Schoot, Wouter M. Kouw, Duco Veen,\n  Adri\\\"enne M. Mendrik", "title": "The Data Representativeness Criterion: Predicting the Performance of\n  Supervised Classification Based on Data Set Similarity", "comments": "12 pages, 6 figures", "journal-ref": "PLoS ONE 15(8): e0237009, 2020, pp. 1-16", "doi": "10.1371/journal.pone.0237009", "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a broad range of fields it may be desirable to reuse a supervised\nclassification algorithm and apply it to a new data set. However,\ngeneralization of such an algorithm and thus achieving a similar classification\nperformance is only possible when the training data used to build the algorithm\nis similar to new unseen data one wishes to apply it to. It is often unknown in\nadvance how an algorithm will perform on new unseen data, being a crucial\nreason for not deploying an algorithm at all. Therefore, tools are needed to\nmeasure the similarity of data sets. In this paper, we propose the Data\nRepresentativeness Criterion (DRC) to determine how representative a training\ndata set is of a new unseen data set. We present a proof of principle, to see\nwhether the DRC can quantify the similarity of data sets and whether the DRC\nrelates to the performance of a supervised classification algorithm. We\ncompared a number of magnetic resonance imaging (MRI) data sets, ranging from\nsubtle to severe difference is acquisition parameters. Results indicate that,\nbased on the similarity of data sets, the DRC is able to give an indication as\nto when the performance of a supervised classifier decreases. The strictness of\nthe DRC can be set by the user, depending on what one considers to be an\nacceptable underperformance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 15:08:13 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Schat", "Evelien", ""], ["van de Schoot", "Rens", ""], ["Kouw", "Wouter M.", ""], ["Veen", "Duco", ""], ["Mendrik", "Adri\u00ebnne M.", ""]]}, {"id": "2002.12106", "submitter": "Avinash Paliwal", "authors": "Avinash Paliwal and Nima Khademi Kalantari", "title": "Deep Slow Motion Video Reconstruction with Hybrid Imaging System", "comments": "IEEE TPAMI and ICCP 2020. Project page containing code and video at\n  http://faculty.cs.tamu.edu/nimak/Papers/ICCP2020_Slomo", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2987316", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow motion videos are becoming increasingly popular, but capturing\nhigh-resolution videos at extremely high frame rates requires professional\nhigh-speed cameras. To mitigate this problem, current techniques increase the\nframe rate of standard videos through frame interpolation by assuming linear\nobject motion which is not valid in challenging cases. In this paper, we\naddress this problem using two video streams as input; an auxiliary video with\nhigh frame rate and low spatial resolution, providing temporal information, in\naddition to the standard main video with low frame rate and high spatial\nresolution. We propose a two-stage deep learning system consisting of alignment\nand appearance estimation that reconstructs high resolution slow motion video\nfrom the hybrid video input. For alignment, we propose to compute flows between\nthe missing frame and two existing frames of the main video by utilizing the\ncontent of the auxiliary video frames. For appearance estimation, we propose to\ncombine the warped and auxiliary frames using a context and occlusion aware\nnetwork. We train our model on synthetically generated hybrid videos and show\nhigh-quality results on a variety of test scenes. To demonstrate practicality,\nwe show the performance of our system on two real dual camera setups with small\nbaseline.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 14:18:12 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 11:05:44 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Paliwal", "Avinash", ""], ["Kalantari", "Nima Khademi", ""]]}, {"id": "2002.12114", "submitter": "Yunhan Zhao", "authors": "Yunhan Zhao, Shu Kong, Daeyun Shin, Charless Fowlkes", "title": "Domain Decluttering: Simplifying Images to Mitigate Synthetic-Real\n  Domain Shift and Improve Depth Estimation", "comments": "camera-ready version, CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging synthetically rendered data offers great potential to improve\nmonocular depth estimation and other geometric estimation tasks, but closing\nthe synthetic-real domain gap is a non-trivial and important task. While much\nrecent work has focused on unsupervised domain adaptation, we consider a more\nrealistic scenario where a large amount of synthetic training data is\nsupplemented by a small set of real images with ground-truth. In this setting,\nwe find that existing domain translation approaches are difficult to train and\noffer little advantage over simple baselines that use a mix of real and\nsynthetic data. A key failure mode is that real-world images contain novel\nobjects and clutter not present in synthetic training. This high-level domain\nshift isn't handled by existing image translation models.\n  Based on these observations, we develop an attention module that learns to\nidentify and remove difficult out-of-domain regions in real images in order to\nimprove depth prediction for a model trained primarily on synthetic data. We\ncarry out extensive experiments to validate our attend-remove-complete approach\n(ARC) and find that it significantly outperforms state-of-the-art domain\nadaptation methods for depth prediction. Visualizing the removed regions\nprovides interpretable insights into the synthetic-real domain gap.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 14:28:56 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 07:37:13 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Zhao", "Yunhan", ""], ["Kong", "Shu", ""], ["Shin", "Daeyun", ""], ["Fowlkes", "Charless", ""]]}, {"id": "2002.12130", "submitter": "Yukun Ding", "authors": "Jinglan Liu, Yukun Ding, Jinjun Xiong, Qianjun Jia, Meiping Huang,\n  Jian Zhuang, Bike Xie, Chun-Chen Liu, Yiyu Shi", "title": "Multi-Cycle-Consistent Adversarial Networks for CT Image Denoising", "comments": "Accepted in ISBI 2020. 5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CT image denoising can be treated as an image-to-image translation task where\nthe goal is to learn the transform between a source domain $X$ (noisy images)\nand a target domain $Y$ (clean images). Recently, cycle-consistent adversarial\ndenoising network (CCADN) has achieved state-of-the-art results by enforcing\ncycle-consistent loss without the need of paired training data. Our detailed\nanalysis of CCADN raises a number of interesting questions. For example, if the\nnoise is large leading to significant difference between domain $X$ and domain\n$Y$, can we bridge $X$ and $Y$ with an intermediate domain $Z$ such that both\nthe denoising process between $X$ and $Z$ and that between $Z$ and $Y$ are\neasier to learn? As such intermediate domains lead to multiple cycles, how do\nwe best enforce cycle-consistency? Driven by these questions, we propose a\nmulti-cycle-consistent adversarial network (MCCAN) that builds intermediate\ndomains and enforces both local and global cycle-consistency. The global\ncycle-consistency couples all generators together to model the whole denoising\nprocess, while the local cycle-consistency imposes effective supervision on the\nprocess between adjacent domains. Experiments show that both local and global\ncycle-consistency are important for the success of MCCAN, which outperforms the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 14:44:45 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Liu", "Jinglan", ""], ["Ding", "Yukun", ""], ["Xiong", "Jinjun", ""], ["Jia", "Qianjun", ""], ["Huang", "Meiping", ""], ["Zhuang", "Jian", ""], ["Xie", "Bike", ""], ["Liu", "Chun-Chen", ""], ["Shi", "Yiyu", ""]]}, {"id": "2002.12158", "submitter": "Sungwon Han", "authors": "Sungwon Han, Yizhan Xu, Sungwon Park, Meeyoung Cha, Cheng-Te Li", "title": "A Comprehensive Approach to Unsupervised Embedding Learning based on AND\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised embedding learning aims to extract good representation from data\nwithout the need for any manual labels, which has been a critical challenge in\nmany supervised learning tasks. This paper proposes a new unsupervised\nembedding approach, called Super-AND, which extends the current\nstate-of-the-art model. Super-AND has its unique set of losses that can gather\nsimilar samples nearby within a low-density space while keeping invariant\nfeatures intact against data augmentation. Super-AND outperforms all existing\napproaches and achieves an accuracy of 89.2% on the image classification task\nfor CIFAR-10. We discuss the practical implications of this method in assisting\nsemi-supervised tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 13:22:04 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Han", "Sungwon", ""], ["Xu", "Yizhan", ""], ["Park", "Sungwon", ""], ["Cha", "Meeyoung", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2002.12168", "submitter": "Jilin Hu", "authors": "Jilin Hu, Jianbing Shen, Bin Yang, Ling Shao", "title": "Infinitely Wide Graph Convolutional Networks: Semi-supervised Learning\n  via Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional neural networks~(GCNs) have recently demonstrated\npromising results on graph-based semi-supervised classification, but little\nwork has been done to explore their theoretical properties. Recently, several\ndeep neural networks, e.g., fully connected and convolutional neural networks,\nwith infinite hidden units have been proved to be equivalent to Gaussian\nprocesses~(GPs). To exploit both the powerful representational capacity of GCNs\nand the great expressive power of GPs, we investigate similar properties of\ninfinitely wide GCNs. More specifically, we propose a GP regression model via\nGCNs~(GPGC) for graph-based semi-supervised learning. In the process, we\nformulate the kernel matrix computation of GPGC in an iterative analytical\nform. Finally, we derive a conditional distribution for the labels of\nunobserved nodes based on the graph structure, labels for the observed nodes,\nand the feature matrix of all the nodes. We conduct extensive experiments to\nevaluate the semi-supervised classification performance of GPGC and demonstrate\nthat it outperforms other state-of-the-art methods by a clear margin on all the\ndatasets while being efficient.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 10:02:32 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Hu", "Jilin", ""], ["Shen", "Jianbing", ""], ["Yang", "Bin", ""], ["Shao", "Ling", ""]]}, {"id": "2002.12169", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Bo Li, Colorado Reed, Pengfei Xu, Kurt Keutzer", "title": "Multi-source Domain Adaptation in the Deep Learning Era: A Systematic\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical applications, it is often difficult and expensive to obtain\nenough large-scale labeled data to train deep neural networks to their full\ncapability. Therefore, transferring the learned knowledge from a separate,\nlabeled source domain to an unlabeled or sparsely labeled target domain becomes\nan appealing alternative. However, direct transfer often results in significant\nperformance decay due to domain shift. Domain adaptation (DA) addresses this\nproblem by minimizing the impact of domain shift between the source and target\ndomains. Multi-source domain adaptation (MDA) is a powerful extension in which\nthe labeled data may be collected from multiple sources with different\ndistributions. Due to the success of DA methods and the prevalence of\nmulti-source data, MDA has attracted increasing attention in both academia and\nindustry. In this survey, we define various MDA strategies and summarize\navailable datasets for evaluation. We also compare modern MDA methods in the\ndeep learning era, including latent space transformation and intermediate\ndomain generation. Finally, we discuss future research directions for MDA.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 08:07:58 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Zhao", "Sicheng", ""], ["Li", "Bo", ""], ["Reed", "Colorado", ""], ["Xu", "Pengfei", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2002.12177", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo", "title": "Evolving Losses for Unsupervised Video Representation Learning", "comments": "arXiv admin note: text overlap with arXiv:1906.03248", "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method to learn video representations from large-scale\nunlabeled video data. Ideally, this representation will be generic and\ntransferable, directly usable for new tasks such as action recognition and zero\nor few-shot learning. We formulate unsupervised representation learning as a\nmulti-modal, multi-task learning problem, where the representations are shared\nacross different modalities via distillation. Further, we introduce the concept\nof loss function evolution by using an evolutionary search algorithm to\nautomatically find optimal combination of loss functions capturing many\n(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\nrepresentation evaluation metric using distribution matching to a large\nunlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\nconstraint, which is not guided by any labeling, produces similar results to\nweakly-supervised, task-specific ones. The proposed unsupervised representation\nlearning results in a single RGB network and outperforms previous methods.\nNotably, it is also more effective than several label-based methods (e.g.,\nImageNet), with the exception of large, fully labeled video datasets.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:56:07 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Angelova", "Anelia", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "2002.12186", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng and Yunchao Wei and Yi Yang", "title": "University-1652: A Multi-view Multi-source Benchmark for Drone-based\n  Geo-localization", "comments": "accepted by ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of cross-view geo-localization. The primary challenge\nof this task is to learn the robust feature against large viewpoint changes.\nExisting benchmarks can help, but are limited in the number of viewpoints.\nImage pairs, containing two viewpoints, e.g., satellite and ground, are usually\nprovided, which may compromise the feature learning. Besides phone cameras and\nsatellites, in this paper, we argue that drones could serve as the third\nplatform to deal with the geo-localization problem. In contrast to the\ntraditional ground-view images, drone-view images meet fewer obstacles, e.g.,\ntrees, and could provide a comprehensive view when flying around the target\nplace. To verify the effectiveness of the drone platform, we introduce a new\nmulti-view multi-source benchmark for drone-based geo-localization, named\nUniversity-1652. University-1652 contains data from three platforms, i.e.,\nsynthetic drones, satellites and ground cameras of 1,652 university buildings\naround the world. To our knowledge, University-1652 is the first drone-based\ngeo-localization dataset and enables two new tasks, i.e., drone-view target\nlocalization and drone navigation. As the name implies, drone-view target\nlocalization intends to predict the location of the target place via drone-view\nimages. On the other hand, given a satellite-view query image, drone navigation\nis to drive the drone to the area of interest in the query. We use this dataset\nto analyze a variety of off-the-shelf CNN features and propose a strong CNN\nbaseline on this challenging dataset. The experiments show that University-1652\nhelps the model to learn the viewpoint-invariant features and also has good\ngeneralization ability in the real-world scenario.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 15:24:15 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 00:07:39 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zheng", "Zhedong", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""]]}, {"id": "2002.12204", "submitter": "Tan Wang", "authors": "Tan Wang, Jianqiang Huang, Hanwang Zhang, Qianru Sun", "title": "Visual Commonsense R-CNN", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel unsupervised feature representation learning method,\nVisual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to\nserve as an improved visual region encoder for high-level tasks such as\ncaptioning and VQA. Given a set of detected object regions in an image (e.g.,\nusing Faster R-CNN), like any other unsupervised feature learning methods\n(e.g., word2vec), the proxy training objective of VC R-CNN is to predict the\ncontextual objects of a region. However, they are fundamentally different: the\nprediction of VC R-CNN is by using causal intervention: P(Y|do(X)), while\nothers are by using the conventional likelihood: P(Y|X). This is also the core\nreason why VC R-CNN can learn \"sense-making\" knowledge like chair can be sat --\nwhile not just \"common\" co-occurrences such as chair is likely to exist if\ntable is observed. We extensively apply VC R-CNN features in prevailing models\nof three popular tasks: Image Captioning, VQA, and VCR, and observe consistent\nperformance boosts across them, achieving many new state-of-the-arts. Code and\nfeature are available at https://github.com/Wangt-CN/VC-R-CNN.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 15:51:19 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 13:52:19 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 04:29:49 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Tan", ""], ["Huang", "Jianqiang", ""], ["Zhang", "Hanwang", ""], ["Sun", "Qianru", ""]]}, {"id": "2002.12212", "submitter": "Yinyu Nie", "authors": "Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, Jian\n  Jun Zhang", "title": "Total3DUnderstanding: Joint Layout, Object Pose and Mesh Reconstruction\n  for Indoor Scenes from a Single Image", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic reconstruction of indoor scenes refers to both scene understanding\nand object reconstruction. Existing works either address one part of this\nproblem or focus on independent objects. In this paper, we bridge the gap\nbetween understanding and reconstruction, and propose an end-to-end solution to\njointly reconstruct room layout, object bounding boxes and meshes from a single\nimage. Instead of separately resolving scene understanding and object\nreconstruction, our method builds upon a holistic scene context and proposes a\ncoarse-to-fine hierarchy with three components: 1. room layout with camera\npose; 2. 3D object bounding boxes; 3. object meshes. We argue that\nunderstanding the context of each component can assist the task of parsing the\nothers, which enables joint understanding and reconstruction. The experiments\non the SUN RGB-D and Pix3D datasets demonstrate that our method consistently\noutperforms existing methods in indoor layout estimation, 3D object detection\nand mesh reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 16:00:52 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Nie", "Yinyu", ""], ["Han", "Xiaoguang", ""], ["Guo", "Shihui", ""], ["Zheng", "Yujian", ""], ["Chang", "Jian", ""], ["Zhang", "Jian Jun", ""]]}, {"id": "2002.12213", "submitter": "Jae Woong Soh", "authors": "Jae Woong Soh, Sunwoo Cho, Nam Ik Cho", "title": "Meta-Transfer Learning for Zero-Shot Super-Resolution", "comments": "Will be presented in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown dramatic improvements in\nsingle image super-resolution (SISR) by using large-scale external samples.\nDespite their remarkable performance based on the external dataset, they cannot\nexploit internal information within a specific image. Another problem is that\nthey are applicable only to the specific condition of data that they are\nsupervised. For instance, the low-resolution (LR) image should be a \"bicubic\"\ndownsampled noise-free image from a high-resolution (HR) one. To address both\nissues, zero-shot super-resolution (ZSSR) has been proposed for flexible\ninternal learning. However, they require thousands of gradient updates, i.e.,\nlong inference time. In this paper, we present Meta-Transfer Learning for\nZero-Shot Super-Resolution (MZSR), which leverages ZSSR. Precisely, it is based\non finding a generic initial parameter that is suitable for internal learning.\nThus, we can exploit both external and internal information, where one single\ngradient update can yield quite considerable results. (See Figure 1). With our\nmethod, the network can quickly adapt to a given image condition. In this\nrespect, our method can be applied to a large spectrum of image conditions\nwithin a fast adaptation process.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 16:01:11 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Soh", "Jae Woong", ""], ["Cho", "Sunwoo", ""], ["Cho", "Nam Ik", ""]]}, {"id": "2002.12236", "submitter": "Zhenzhang Ye", "authors": "Zhenzhang Ye, Thomas M\\\"ollenhoff, Tao Wu, Daniel Cremers", "title": "Optimization of Graph Total Variation via Active-Set-based Combinatorial\n  Reconditioning", "comments": "Presented at the 23 rd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2020. Code:\n  https://github.com/zhenzhangye/graph_TV_recond", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured convex optimization on weighted graphs finds numerous applications\nin machine learning and computer vision. In this work, we propose a novel\nadaptive preconditioning strategy for proximal algorithms on this problem\nclass. Our preconditioner is driven by a sharp analysis of the local linear\nconvergence rate depending on the \"active set\" at the current iterate. We show\nthat nested-forest decomposition of the inactive edges yields a guaranteed\nlocal linear convergence rate. Further, we propose a practical greedy heuristic\nwhich realizes such nested decompositions and show in several numerical\nexperiments that our reconditioning strategy, when applied to proximal gradient\nor primal-dual hybrid gradient algorithm, achieves competitive performances.\nOur results suggest that local convergence analysis can serve as a guideline\nfor selecting variable metrics in proximal algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 16:33:09 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Ye", "Zhenzhang", ""], ["M\u00f6llenhoff", "Thomas", ""], ["Wu", "Tao", ""], ["Cremers", "Daniel", ""]]}, {"id": "2002.12247", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\\'erez,\n  Matthieu Cord", "title": "Learning Representations by Predicting Bags of Visual Words", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised representation learning targets to learn convnet-based image\nrepresentations from unlabeled data. Inspired by the success of NLP methods in\nthis area, in this work we propose a self-supervised approach based on\nspatially dense image descriptions that encode discrete visual concepts, here\ncalled visual words. To build such discrete representations, we quantize the\nfeature maps of a first pre-trained self-supervised convnet, over a k-means\nbased vocabulary. Then, as a self-supervised task, we train another convnet to\npredict the histogram of visual words of an image (i.e., its Bag-of-Words\nrepresentation) given as input a perturbed version of that image. The proposed\ntask forces the convnet to learn perturbation-invariant and context-aware image\nfeatures, useful for downstream image understanding tasks. We extensively\nevaluate our method and demonstrate very strong empirical results, e.g., our\npre-trained self-supervised representations transfer better on detection task\nand similarly on classification over classes \"unseen\" during pre-training, when\ncompared to the supervised case.\n  This also shows that the process of image discretization into visual words\ncan provide the basis for very powerful self-supervised approaches in the image\ndomain, thus allowing further connections to be made to related methods from\nthe NLP domain that have been extremely successful so far.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 16:45:25 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Gidaris", "Spyros", ""], ["Bursuc", "Andrei", ""], ["Komodakis", "Nikos", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "2002.12256", "submitter": "Richard Wang", "authors": "Usman Sajid, Hasan Sajid, Hongcheng Wang, Guanghui Wang", "title": "ZoomCount: A Zooming Mechanism for Crowd Counting in Static Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for crowd counting in low to high\ndensity scenarios in static images. Current approaches cannot handle huge crowd\ndiversity well and thus perform poorly in extreme cases, where the crowd\ndensity in different regions of an image is either too low or too high, leading\nto crowd underestimation or overestimation. The proposed solution is based on\nthe observation that detecting and handling such extreme cases in a specialized\nway leads to better crowd estimation. Additionally, existing methods find it\nhard to differentiate between the actual crowd and the cluttered background\nregions, resulting in further count overestimation. To address these issues, we\npropose a simple yet effective modular approach, where an input image is first\nsubdivided into fixed-size patches and then fed to a four-way classification\nmodule labeling each image patch as low, medium, high-dense or no-crowd. This\nmodule also provides a count for each label, which is then analyzed via a\nspecifically devised novel decision module to decide whether the image belongs\nto any of the two extreme cases (very low or very high density) or a normal\ncase. Images, specified as high- or low-density extreme or a normal case, pass\nthrough dedicated zooming or normal patch-making blocks respectively before\nrouting to the regressor in the form of fixed-size patches for crowd estimate.\nExtensive experimental evaluations demonstrate that the proposed approach\noutperforms the state-of-the-art methods on four benchmarks under most of the\nevaluation criteria.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 16:57:04 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Sajid", "Usman", ""], ["Sajid", "Hasan", ""], ["Wang", "Hongcheng", ""], ["Wang", "Guanghui", ""]]}, {"id": "2002.12257", "submitter": "Joel Brogan", "authors": "Max Ruby, David S. Bolme, Joel Brogan, David Cornett III, Baldemar\n  Delgado, Gavin Jager, Christi Johnson, Jose Martinez-Mendoza, Hector\n  Santos-Villalobos, Nisha Srinivas", "title": "The Mertens Unrolled Network (MU-Net): A High Dynamic Range Fusion\n  Neural Network for Through the Windshield Driver Recognition", "comments": "Accepted to SPEI Autonomous Systems: Sensors, Processing and Security\n  for Vehicles & Infrastructure 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face recognition of vehicle occupants through windshields in unconstrained\nenvironments poses a number of unique challenges ranging from glare, poor\nillumination, driver pose and motion blur. In this paper, we further develop\nthe hardware and software components of a custom vehicle imaging system to\nbetter overcome these challenges. After the build out of a physical prototype\nsystem that performs High Dynamic Range (HDR) imaging, we collect a small\ndataset of through-windshield image captures of known drivers. We then\nre-formulate the classical Mertens-Kautz-Van Reeth HDR fusion algorithm as a\npre-initialized neural network, which we name the Mertens Unrolled Network\n(MU-Net), for the purpose of fine-tuning the HDR output of through-windshield\nimages. Reconstructed faces from this novel HDR method are then evaluated and\ncompared against other traditional and experimental HDR methods in a\npre-trained state-of-the-art (SOTA) facial recognition pipeline, verifying the\nefficacy of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 16:57:36 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Ruby", "Max", ""], ["Bolme", "David S.", ""], ["Brogan", "Joel", ""], ["Cornett", "David", "III"], ["Delgado", "Baldemar", ""], ["Jager", "Gavin", ""], ["Johnson", "Christi", ""], ["Martinez-Mendoza", "Jose", ""], ["Santos-Villalobos", "Hector", ""], ["Srinivas", "Nisha", ""]]}, {"id": "2002.12259", "submitter": "Wang Shen", "authors": "Wang Shen, Wenbo Bao, Guangtao Zhai, Li Chen, Xiongkuo Min, Zhiyong\n  Gao", "title": "Blurry Video Frame Interpolation", "comments": "This work is accepted in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works reduce motion blur and up-convert frame rate through two\nseparate ways, including frame deblurring and frame interpolation. However, few\nstudies have approached the joint video enhancement problem, namely\nsynthesizing high-frame-rate clear results from low-frame-rate blurry inputs.\nIn this paper, we propose a blurry video frame interpolation method to reduce\nmotion blur and up-convert frame rate simultaneously. Specifically, we develop\na pyramid module to cyclically synthesize clear intermediate frames. The\npyramid module features adjustable spatial receptive field and temporal scope,\nthus contributing to controllable computational complexity and restoration\nability. Besides, we propose an inter-pyramid recurrent module to connect\nsequential models to exploit the temporal relationship. The pyramid module\nintegrates a recurrent module, thus can iteratively synthesize temporally\nsmooth results without significantly increasing the model size. Extensive\nexperimental results demonstrate that our method performs favorably against\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 17:00:26 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Shen", "Wang", ""], ["Bao", "Wenbo", ""], ["Zhai", "Guangtao", ""], ["Chen", "Li", ""], ["Min", "Xiongkuo", ""], ["Gao", "Zhiyong", ""]]}, {"id": "2002.12261", "submitter": "Min Hun Lee", "authors": "Min Hun Lee, Daniel P. Siewiorek, Asim Smailagic, Alexandre\n  Bernardino, and Sergi Berm\\'udez i Badia", "title": "Opportunities of a Machine Learning-based Decision Support System for\n  Stroke Rehabilitation Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rehabilitation assessment is critical to determine an adequate intervention\nfor a patient. However, the current practices of assessment mainly rely on\ntherapist's experience, and assessment is infrequently executed due to the\nlimited availability of a therapist. In this paper, we identified the needs of\ntherapists to assess patient's functional abilities (e.g. alternative\nperspective on assessment with quantitative information on patient's exercise\nmotions). As a result, we developed an intelligent decision support system that\ncan identify salient features of assessment using reinforcement learning to\nassess the quality of motion and summarize patient specific analysis. We\nevaluated this system with seven therapists using the dataset from 15 patient\nperforming three exercises. The evaluation demonstrates that our system is\npreferred over a traditional system without analysis while presenting more\nuseful information and significantly increasing the agreement over therapists'\nevaluation from 0.6600 to 0.7108 F1-scores ($p <0.05$). We discuss the\nimportance of presenting contextually relevant and salient information and\nadaptation to develop a human and machine collaborative decision making system.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 17:04:07 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 17:22:42 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lee", "Min Hun", ""], ["Siewiorek", "Daniel P.", ""], ["Smailagic", "Asim", ""], ["Bernardino", "Alexandre", ""], ["Badia", "Sergi Berm\u00fadez i", ""]]}, {"id": "2002.12263", "submitter": "Kaikai Huang Mr.", "authors": "Kaikai Huang and Antonio Tejero-de-Pablos and Hiroaki Yamane and\n  Yusuke Kurose and Junichi Iho and Youji Tokunaga and Makoto Horie and Keisuke\n  Nishizawa and Yusaku Hayashi and Yasushi Koyama and Tatsuya Harada", "title": "Coronary Wall Segmentation in CCTA Scans via a Hybrid Net with Contours\n  Regularization", "comments": "5 pages, 2 figures, accepted by ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing closed and well-connected boundaries of coronary artery is\nessential to assist cardiologists in the diagnosis of coronary artery disease\n(CAD). Recently, several deep learning-based methods have been proposed for\nboundary detection and segmentation in a medical image. However, when applied\nto coronary wall detection, they tend to produce disconnected and inaccurate\nboundaries. In this paper, we propose a novel boundary detection method for\ncoronary arteries that focuses on the continuity and connectivity of the\nboundaries. In order to model the spatial continuity of consecutive images, our\nhybrid architecture takes a volume (i.e., a segment of the coronary artery) as\ninput and detects the boundary of the target slice (i.e., the central slice of\nthe segment). Then, to ensure closed boundaries, we propose a\ncontour-constrained weighted Hausdorff distance loss. We evaluate our method on\na dataset of 34 patients of coronary CT angiography scans with curved planar\nreconstruction (CCTA-CPR) of the arteries (i.e., cross-sections). Experiment\nresults show that our method can produce smooth closed boundaries outperforming\nthe state-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 17:06:58 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Huang", "Kaikai", ""], ["Tejero-de-Pablos", "Antonio", ""], ["Yamane", "Hiroaki", ""], ["Kurose", "Yusuke", ""], ["Iho", "Junichi", ""], ["Tokunaga", "Youji", ""], ["Horie", "Makoto", ""], ["Nishizawa", "Keisuke", ""], ["Hayashi", "Yusaku", ""], ["Koyama", "Yasushi", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2002.12314", "submitter": "Yu Zhang", "authors": "Yu Zhang, Xiaoqin Wang, Hunter Blanton, Gongbo Liang, Xin Xing, and\n  Nathan Jacobs", "title": "2D Convolutional Neural Networks for 3D Digital Breast Tomosynthesis\n  Classification", "comments": "Accepted by IEEE International Conference of Bioinformatics and\n  Biomedicine (BIBM), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated methods for breast cancer detection have focused on 2D mammography\nand have largely ignored 3D digital breast tomosynthesis (DBT), which is\nfrequently used in clinical practice. The two key challenges in developing\nautomated methods for DBT classification are handling the variable number of\nslices and retaining slice-to-slice changes. We propose a novel deep 2D\nconvolutional neural network (CNN) architecture for DBT classification that\nsimultaneously overcomes both challenges. Our approach operates on the full\nvolume, regardless of the number of slices, and allows the use of pre-trained\n2D CNNs for feature extraction, which is important given the limited amount of\nannotated training data. In an extensive evaluation on a real-world clinical\ndataset, our approach achieves 0.854 auROC, which is 28.80% higher than\napproaches based on 3D CNNs. We also find that these improvements are stable\nacross a range of model configurations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:32:52 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Zhang", "Yu", ""], ["Wang", "Xiaoqin", ""], ["Blanton", "Hunter", ""], ["Liang", "Gongbo", ""], ["Xing", "Xin", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2002.12319", "submitter": "Vitor Guizilini", "authors": "Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, Adrien Gaidon", "title": "Semantically-Guided Representation Learning for Self-Supervised\n  Monocular Depth", "comments": "Proceedings of the Eighth International Conference on Learning\n  Representations (ICLR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning is showing great promise for monocular depth\nestimation, using geometry as the only source of supervision. Depth networks\nare indeed capable of learning representations that relate visual appearance to\n3D properties by implicitly leveraging category-level patterns. In this work we\ninvestigate how to leverage more directly this semantic structure to guide\ngeometric representation learning, while remaining in the self-supervised\nregime. Instead of using semantic labels and proxy losses in a multi-task\napproach, we propose a new architecture leveraging fixed pretrained semantic\nsegmentation networks to guide self-supervised representation learning via\npixel-adaptive convolutions. Furthermore, we propose a two-stage training\nprocess to overcome a common semantic bias on dynamic objects via resampling.\nOur method improves upon the state of the art for self-supervised monocular\ndepth prediction over all pixels, fine-grained details, and per semantic\ncategories.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:40:10 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Guizilini", "Vitor", ""], ["Hou", "Rui", ""], ["Li", "Jie", ""], ["Ambrus", "Rares", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2002.12324", "submitter": "Eric Brachmann", "authors": "Eric Brachmann and Carsten Rother", "title": "Visual Camera Re-Localization from RGB and RGB-D Images Using DSAC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a learning-based system that estimates the camera position and\norientation from a single input image relative to a known environment. The\nsystem is flexible w.r.t. the amount of information available at test and at\ntraining time, catering to different applications. Input images can be RGB-D or\nRGB, and a 3D model of the environment can be utilized for training but is not\nnecessary. In the minimal case, our system requires only RGB images and ground\ntruth poses at training time, and it requires only a single RGB image at test\ntime. The framework consists of a deep neural network and fully differentiable\npose optimization. The neural network predicts so called scene coordinates,\ni.e. dense correspondences between the input image and 3D scene space of the\nenvironment. The pose optimization implements robust fitting of pose parameters\nusing differentiable RANSAC (DSAC) to facilitate end-to-end training. The\nsystem, an extension of DSAC++ and referred to as DSAC*, achieves\nstate-of-the-art accuracy an various public datasets for RGB-based\nre-localization, and competitive accuracy for RGB-D-based re-localization.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:45:21 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 12:07:47 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 12:29:26 GMT"}, {"version": "v4", "created": "Fri, 9 Oct 2020 15:03:02 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Brachmann", "Eric", ""], ["Rother", "Carsten", ""]]}, {"id": "2002.12336", "submitter": "Thanard Kurutach", "authors": "Kara Liu, Thanard Kurutach, Christine Tung, Pieter Abbeel, Aviv Tamar", "title": "Hallucinative Topological Memory for Zero-Shot Visual Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual planning (VP), an agent learns to plan goal-directed behavior from\nobservations of a dynamical system obtained offline, e.g., images obtained from\nself-supervised robot interaction. Most previous works on VP approached the\nproblem by planning in a learned latent space, resulting in low-quality visual\nplans, and difficult training algorithms. Here, instead, we propose a simple VP\nmethod that plans directly in image space and displays competitive performance.\nWe build on the semi-parametric topological memory (SPTM) method: image samples\nare treated as nodes in a graph, the graph connectivity is learned from image\nsequence data, and planning can be performed using conventional graph search\nmethods. We propose two modifications on SPTM. First, we train an energy-based\ngraph connectivity function using contrastive predictive coding that admits\nstable training. Second, to allow zero-shot planning in new domains, we learn a\nconditional VAE model that generates images given a context of the domain, and\nuse these hallucinated samples for building the connectivity graph and\nplanning. We show that this simple approach significantly outperform the\nstate-of-the-art VP methods, in terms of both plan interpretability and success\nrate when using the plan to guide a trajectory-following controller.\nInterestingly, our method can pick up non-trivial visual properties of objects,\nsuch as their geometry, and account for it in the plans.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:54:42 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Liu", "Kara", ""], ["Kurutach", "Thanard", ""], ["Tung", "Christine", ""], ["Abbeel", "Pieter", ""], ["Tamar", "Aviv", ""]]}, {"id": "2002.12345", "submitter": "Shuyue Guan", "authors": "Shuyue Guan, Murray Loew", "title": "A Novel Measure to Evaluate Generative Adversarial Networks Based on\n  Direct Analysis of Generated Images", "comments": "16 pages, 11 figures. Accepted by the Neural Computing and\n  Applications journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generative Adversarial Network (GAN) is a state-of-the-art technique in\nthe field of deep learning. A number of recent papers address the theory and\napplications of GANs in various fields of image processing. Fewer studies,\nhowever, have directly evaluated GAN outputs. Those that have been conducted\nfocused on using classification performance, e.g., Inception Score (IS) and\nstatistical metrics, e.g., Fr\\'echet Inception Distance (FID). Here, we\nconsider a fundamental way to evaluate GANs by directly analyzing the images\nthey generate, instead of using them as inputs to other classifiers. We\ncharacterize the performance of a GAN as an image generator according to three\naspects: 1) Creativity: non-duplication of the real images. 2) Inheritance:\ngenerated images should have the same style, which retains key features of the\nreal images. 3) Diversity: generated images are different from each other. A\nGAN should not generate a few different images repeatedly. Based on the three\naspects of ideal GANs, we have designed the Likeness Score (LS) to evaluate GAN\nperformance, and have applied it to evaluate several typical GANs. We compared\nour proposed measure with two commonly used GAN evaluation methods: IS and FID,\nand four additional measures. Furthermore, we discuss how these evaluations\ncould help us deepen our understanding of GANs and improve their performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:59:22 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 05:35:36 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 19:57:29 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 05:18:25 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Guan", "Shuyue", ""], ["Loew", "Murray", ""]]}, {"id": "2002.12356", "submitter": "Andreas Foltyn", "authors": "Maximilian Seitzer, Andreas Foltyn, Felix P. Kemeth", "title": "NeurIPS 2019 Disentanglement Challenge: Improved Disentanglement through\n  Learned Aggregation of Convolutional Feature Maps", "comments": "Disentanglement Challenge - 33rd Conference on Neural Information\n  Processing Systems (NeurIPS) - NeurIPS 2019. arXiv admin note: text overlap\n  with arXiv:2002.10003. Acknowledgements added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report to our stage 2 submission to the NeurIPS 2019 disentanglement\nchallenge presents a simple image preprocessing method for learning\ndisentangled latent factors. We propose to train a variational autoencoder on\nregionally aggregated feature maps obtained from networks pretrained on the\nImageNet database, utilizing the implicit inductive bias contained in those\nfeatures for disentanglement. This bias can be further enhanced by explicitly\nfine-tuning the feature maps on auxiliary tasks useful for the challenge, such\nas angle, position estimation, or color classification. Our approach achieved\nthe 2nd place in stage 2 of the challenge. Code is available at\nhttps://github.com/mseitzer/neurips2019-disentanglement-challenge.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 08:46:17 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 15:16:35 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Seitzer", "Maximilian", ""], ["Foltyn", "Andreas", ""], ["Kemeth", "Felix P.", ""]]}, {"id": "2002.12392", "submitter": "Yu Zhang", "authors": "Gongbo Liang, Xiaoqin Wang, Yu Zhang, Xin Xing, Hunter Blanton, Tawfiq\n  Salem, Nathan Jacobs", "title": "Joint 2D-3D Breast Cancer Classification", "comments": "Accepted by IEEE International Conference of Bioinformatics and\n  Biomedicine (BIBM), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the malignant tumor that causes the highest number of cancer\ndeaths in females. Digital mammograms (DM or 2D mammogram) and digital breast\ntomosynthesis (DBT or 3D mammogram) are the two types of mammography imagery\nthat are used in clinical practice for breast cancer detection and diagnosis.\nRadiologists usually read both imaging modalities in combination; however,\nexisting computer-aided diagnosis tools are designed using only one imaging\nmodality. Inspired by clinical practice, we propose an innovative convolutional\nneural network (CNN) architecture for breast cancer classification, which uses\nboth 2D and 3D mammograms, simultaneously. Our experiment shows that the\nproposed method significantly improves the performance of breast cancer\nclassification. By assembling three CNN classifiers, the proposed model\nachieves 0.97 AUC, which is 34.72% higher than the methods using only one\nimaging modality.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 19:08:16 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Liang", "Gongbo", ""], ["Wang", "Xiaoqin", ""], ["Zhang", "Yu", ""], ["Xing", "Xin", ""], ["Blanton", "Hunter", ""], ["Salem", "Tawfiq", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2002.12394", "submitter": "Ujjal Kr Dutta", "authors": "Ujjal Kr Dutta, Mehrtash Harandi and Chellu Chandra Sekhar", "title": "Affinity guided Geometric Semi-Supervised Metric Learning", "comments": "Paper accepted in NeurIPS 2020 workshop on Differential Geometry\n  meets Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revamp the forgotten classical Semi-Supervised Distance\nMetric Learning (SSDML) problem from a Riemannian geometric lens, to leverage\nstochastic optimization within a end-to-end deep framework. The motivation\ncomes from the fact that apart from a few classical SSDML approaches learning a\nlinear Mahalanobis metric, deep SSDML has not been studied. We first extend\nexisting SSDML methods to their deep counterparts and then propose a new method\nto overcome their limitations. Due to the nature of constraints on our metric\nparameters, we leverage Riemannian optimization. Our deep SSDML method with a\nnovel affinity propagation based triplet mining strategy outperforms its\ncompetitors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 19:10:21 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 17:49:32 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Dutta", "Ujjal Kr", ""], ["Harandi", "Mehrtash", ""], ["Sekhar", "Chellu Chandra", ""]]}, {"id": "2002.12398", "submitter": "Maurice Weber", "authors": "Linyi Li, Maurice Weber, Xiaojun Xu, Luka Rimanic, Tao Xie, Ce Zhang,\n  Bo Li", "title": "Provable Robust Learning Based on Transformation-Specific Smoothing", "comments": "58 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning (ML) systems become pervasive, safeguarding their\nsecurity is critical. Recent work has demonstrated that motivated adversaries\ncould add adversarial perturbations to the test data to mislead ML systems. So\nfar, most research has focused on providing provable robustness guarantees for\nML models against a specific Lp norm bounded adversarial perturbation. However,\nin practice previous work has shown that there are other types of realistic\nadversarial transformations whose semantic meaning has been leveraged to attack\nML systems. In this paper, we aim to provide a unified framework for certifying\nML robustness against general adversarial transformations. First, we identify\nthe semantic transformations as different categories: resolvable (e.g.,\nGaussian blur and brightness) and differentially resolvable transformations\n(e.g., rotation and scaling). We then provide sufficient conditions and\nstrategies for certifying certain transformations. For instance, we propose a\nnovel sampling-based interpolation approach with estimated Lipschitz upper\nbound to certify the robustness against differentially resolvable\ntransformations. In addition, we theoretically optimize the smoothing\nstrategies for certifying the robustness of ML models against different\ntransformations. For instance, we show that smoothing by sampling from\nexponential distribution provides a tighter robustness bound than Gaussian.\nExtensive experiments on 7 semantic transformations show that our proposed\nunified framework significantly outperforms the state-of-the-art certified\nrobustness approaches on several datasets including ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 19:19:32 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 11:45:20 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 16:20:43 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Li", "Linyi", ""], ["Weber", "Maurice", ""], ["Xu", "Xiaojun", ""], ["Rimanic", "Luka", ""], ["Xie", "Tao", ""], ["Zhang", "Ce", ""], ["Li", "Bo", ""]]}, {"id": "2002.12411", "submitter": "Ali Ayub", "authors": "Ali Ayub and Alan Wagner", "title": "Cognitively-Inspired Model for Incremental Learning Using a Few Examples", "comments": "Added link to the code in the paper", "journal-ref": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) Workshops, 2020", "doi": "10.1109/CVPRW50498.2020.00119", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental learning attempts to develop a classifier which learns\ncontinuously from a stream of data segregated into different classes. Deep\nlearning approaches suffer from catastrophic forgetting when learning classes\nincrementally, while most incremental learning approaches require a large\namount of training data per class. We examine the problem of incremental\nlearning using only a few training examples, referred to as Few-Shot\nIncremental Learning (FSIL). To solve this problem, we propose a novel approach\ninspired by the concept learning model of the hippocampus and the neocortex\nthat represents each image class as centroids and does not suffer from\ncatastrophic forgetting. We evaluate our approach on three class-incremental\nlearning benchmarks: Caltech-101, CUBS-200-2011 and CIFAR-100 for incremental\nand few-shot incremental learning and show that our approach achieves\nstate-of-the-art results in terms of classification accuracy over all learned\nclasses.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 19:52:42 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 05:09:37 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 06:55:06 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan", ""]]}, {"id": "2002.12415", "submitter": "Gideon Billings", "authors": "Gideon Billings, Matthew Johnson-Roberson", "title": "SilhoNet-Fisheye: Adaptation of A ROI Based Object Pose Estimation\n  Network to Monocular Fisheye Images", "comments": "Submitted to IEEE RAL/IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much recent interest in deep learning methods for monocular\nimage based object pose estimation. While object pose estimation is an\nimportant problem for autonomous robot interaction with the physical world, and\nthe application space for monocular-based methods is expansive, there has been\nlittle work on applying these methods with fisheye imaging systems. Also,\nlittle exists in the way of annotated fisheye image datasets on which these\nmethods can be developed and tested. The research landscape is even more sparse\nfor object detection methods applied in the underwater domain, fisheye image\nbased or otherwise. In this work, we present a novel framework for adapting a\nROI-based 6D object pose estimation method to work on full fisheye images. The\nmethod incorporates the gnomic projection of regions of interest from an\nintermediate spherical image representation to correct for the fisheye\ndistortions. Further, we contribute a fisheye image dataset, called UWHandles,\ncollected in natural underwater environments, with 6D object pose and 2D\nbounding box annotations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 19:57:33 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Billings", "Gideon", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "2002.12416", "submitter": "Kai Xu", "authors": "Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, Fengbo Ren", "title": "Learning in the Frequency Domain", "comments": "Accepted to CVPR 2020; https://github.com/calmevtime/DCTNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved remarkable success in computer vision\ntasks. Existing neural networks mainly operate in the spatial domain with fixed\ninput sizes. For practical applications, images are usually large and have to\nbe downsampled to the predetermined input size of neural networks. Even though\nthe downsampling operations reduce computation and the required communication\nbandwidth, it removes both redundant and salient information obliviously, which\nresults in accuracy degradation. Inspired by digital signal processing\ntheories, we analyze the spectral bias from the frequency perspective and\npropose a learning-based frequency selection method to identify the trivial\nfrequency components which can be removed without accuracy loss. The proposed\nmethod of learning in the frequency domain leverages identical structures of\nthe well-known neural networks, such as ResNet-50, MobileNetV2, and Mask R-CNN,\nwhile accepting the frequency-domain information as the input. Experiment\nresults show that learning in the frequency domain with static channel\nselection can achieve higher accuracy than the conventional spatial\ndownsampling approach and meanwhile further reduce the input data size.\nSpecifically for ImageNet classification with the same input size, the proposed\nmethod achieves 1.41% and 0.66% top-1 accuracy improvements on ResNet-50 and\nMobileNetV2, respectively. Even with half input size, the proposed method still\nimproves the top-1 accuracy on ResNet-50 by 1%. In addition, we observe a 0.8%\naverage precision improvement on Mask R-CNN for instance segmentation on the\nCOCO dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 19:57:55 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 20:41:05 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 01:13:45 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2020 23:40:51 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Xu", "Kai", ""], ["Qin", "Minghai", ""], ["Sun", "Fei", ""], ["Wang", "Yuhao", ""], ["Chen", "Yen-Kuang", ""], ["Ren", "Fengbo", ""]]}, {"id": "2002.12418", "submitter": "Huan Wang", "authors": "Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan Wang, Bin Zou,\n  Yafeng Yang, Zongyang Cui, Yu Cai, Tianhang Yu, Chengfei Lv, Zhihua Wu", "title": "MNN: A Universal and Efficient Inference Engine", "comments": "Accepted by MLSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying deep learning models on mobile devices draws more and more\nattention recently. However, designing an efficient inference engine on devices\nis under the great challenges of model compatibility, device diversity, and\nresource limitation. To deal with these challenges, we propose Mobile Neural\nNetwork (MNN), a universal and efficient inference engine tailored to mobile\napplications. In this paper, the contributions of MNN include: (1) presenting a\nmechanism called pre-inference that manages to conduct runtime optimization;\n(2)deliveringthorough kernel optimization on operators to achieve optimal\ncomputation performance; (3) introducing backend abstraction module which\nenables hybrid scheduling and keeps the engine lightweight. Extensive benchmark\nexperiments demonstrate that MNN performs favorably against other popular\nlightweight deep learning frameworks. MNN is available to public at:\nhttps://github.com/alibaba/MNN.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 20:03:16 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Jiang", "Xiaotang", ""], ["Wang", "Huan", ""], ["Chen", "Yiliu", ""], ["Wu", "Ziqi", ""], ["Wang", "Lichuan", ""], ["Zou", "Bin", ""], ["Yang", "Yafeng", ""], ["Cui", "Zongyang", ""], ["Cai", "Yu", ""], ["Yu", "Tianhang", ""], ["Lv", "Chengfei", ""], ["Wu", "Zhihua", ""]]}, {"id": "2002.12428", "submitter": "Ming Gong", "authors": "Ming Gong, Liping Yang, Catherine Potts, Vijayan K. Asari, Diane Oyen,\n  Brendt Wohlberg", "title": "TGGLines: A Robust Topological Graph Guided Line Segment Detector for\n  Low Quality Binary Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Line segment detection is an essential task in computer vision and image\nanalysis, as it is the critical foundation for advanced tasks such as shape\nmodeling and road lane line detection for autonomous driving. We present a\nrobust topological graph guided approach for line segment detection in low\nquality binary images (hence, we call it TGGLines). Due to the graph-guided\napproach, TGGLines not only detects line segments, but also organizes the\nsegments with a line segment connectivity graph, which means the topological\nrelationships (e.g., intersection, an isolated line segment) of the detected\nline segments are captured and stored; whereas other line detectors only retain\na collection of loose line segments. Our empirical results show that the\nTGGLines detector visually and quantitatively outperforms state-of-the-art line\nsegment detection methods. In addition, our TGGLines approach has the following\ntwo competitive advantages: (1) our method only requires one parameter and it\nis adaptive, whereas almost all other line segment detection methods require\nmultiple (non-adaptive) parameters, and (2) the line segments detected by\nTGGLines are organized by a line segment connectivity graph.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 20:47:18 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Gong", "Ming", ""], ["Yang", "Liping", ""], ["Potts", "Catherine", ""], ["Asari", "Vijayan K.", ""], ["Oyen", "Diane", ""], ["Wohlberg", "Brendt", ""]]}, {"id": "2002.12455", "submitter": "Xiang Deng", "authors": "Xiang Deng and Zhongfei Zhang", "title": "Is the Meta-Learning Idea Able to Improve the Generalization of Deep\n  Neural Networks on the Standard Supervised Learning?", "comments": null, "journal-ref": "ICPR 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial efforts have been made on improving the generalization abilities\nof deep neural networks (DNNs) in order to obtain better performances without\nintroducing more parameters. On the other hand, meta-learning approaches\nexhibit powerful generalization on new tasks in few-shot learning. Intuitively,\nfew-shot learning is more challenging than the standard supervised learning as\neach target class only has a very few or no training samples. The natural\nquestion that arises is whether the meta-learning idea can be used for\nimproving the generalization of DNNs on the standard supervised learning. In\nthis paper, we propose a novel meta-learning based training procedure (MLTP)\nfor DNNs and demonstrate that the meta-learning idea can indeed improve the\ngeneralization abilities of DNNs. MLTP simulates the meta-training process by\nconsidering a batch of training samples as a task. The key idea is that the\ngradient descent step for improving the current task performance should also\nimprove a new task performance, which is ignored by the current standard\nprocedure for training neural networks. MLTP also benefits from all the\nexisting training techniques such as dropout, weight decay, and batch\nnormalization. We evaluate MLTP by training a variety of small and large neural\nnetworks on three benchmark datasets, i.e., CIFAR-10, CIFAR-100, and Tiny\nImageNet. The experimental results show a consistently improved generalization\nperformance on all the DNNs with different sizes, which verifies the promise of\nMLTP and demonstrates that the meta-learning idea is indeed able to improve the\ngeneralization of DNNs on the standard supervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 21:29:54 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Deng", "Xiang", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "2002.12461", "submitter": "Vinorth Varatharasan", "authors": "Vinorth Varatharasan, Alice Shuang Shuang Rao, Eric Toutounji,\n  Ju-Hyeon Hong, Hyo-Sang Shin", "title": "Target Detection, Tracking and Avoidance System for Low-cost UAVs using\n  AI-Based Approaches", "comments": "IEEE RED-UAS 2019 Conference", "journal-ref": null, "doi": "10.1109/REDUAS47371.2019.8999683", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An onboard target detection, tracking and avoidance system has been developed\nin this paper, for low-cost UAV flight controllers using AI-Based approaches.\nThe aim of the proposed system is that an ally UAV can either avoid or track an\nunexpected enemy UAV with a net to protect itself. In this point of view, a\nsimple and robust target detection, tracking and avoidance system is designed.\nTwo open-source tools were used for the aim: a state-of-the-art object\ndetection technique called SSD and an API for MAVLink compatible systems called\nMAVSDK. The MAVSDK performs velocity control when a UAV is detected so that the\nmanoeuvre is done simply and efficiently. The proposed system was verified with\nSoftware in the loop (SITL) and Hardware in the loop (HITL) simulators. The\nsimplicity of this algorithm makes it innovative, and therefore it should be\nused in future applications needing robust performances with low-cost hardware\nsuch as delivery drone applications.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 21:58:54 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Varatharasan", "Vinorth", ""], ["Rao", "Alice Shuang Shuang", ""], ["Toutounji", "Eric", ""], ["Hong", "Ju-Hyeon", ""], ["Shin", "Hyo-Sang", ""]]}, {"id": "2002.12462", "submitter": "Cuong Nguyen", "authors": "Cuong V. Nguyen, Tal Hassner, Matthias Seeger, Cedric Archambeau", "title": "LEEP: A New Measure to Evaluate Transferability of Learned\n  Representations", "comments": "Published at the International Conference on Machine Learning (ICML)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new measure to evaluate the transferability of representations\nlearned by classifiers. Our measure, the Log Expected Empirical Prediction\n(LEEP), is simple and easy to compute: when given a classifier trained on a\nsource data set, it only requires running the target data set through this\nclassifier once. We analyze the properties of LEEP theoretically and\ndemonstrate its effectiveness empirically. Our analysis shows that LEEP can\npredict the performance and convergence speed of both transfer and\nmeta-transfer learning methods, even for small or imbalanced data. Moreover,\nLEEP outperforms recently proposed transferability measures such as negative\nconditional entropy and H scores. Notably, when transferring from ImageNet to\nCIFAR100, LEEP can achieve up to 30% improvement compared to the best competing\nmethod in terms of the correlations with actual transfer accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 22:02:20 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 02:33:25 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Nguyen", "Cuong V.", ""], ["Hassner", "Tal", ""], ["Seeger", "Matthias", ""], ["Archambeau", "Cedric", ""]]}, {"id": "2002.12467", "submitter": "Vinorth Varatharasan", "authors": "Vinorth Varatharasan, Hyo-Sang Shin, Antonios Tsourdos, Nick Colosimo", "title": "Improving Learning Effectiveness For Object Detection and Classification\n  in Cluttered Backgrounds", "comments": "IEEE RED-UAS 2019 Conference", "journal-ref": null, "doi": "10.1109/REDUAS47371.2019.8999695", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually, Neural Networks models are trained with a large dataset of images in\nhomogeneous backgrounds. The issue is that the performance of the network\nmodels trained could be significantly degraded in a complex and heterogeneous\nenvironment. To mitigate the issue, this paper develops a framework that\npermits to autonomously generate a training dataset in heterogeneous cluttered\nbackgrounds. It is clear that the learning effectiveness of the proposed\nframework should be improved in complex and heterogeneous environments,\ncompared with the ones with the typical dataset. In our framework, a\nstate-of-the-art image segmentation technique called DeepLab is used to extract\nobjects of interest from a picture and Chroma-key technique is then used to\nmerge the extracted objects of interest into specific heterogeneous\nbackgrounds. The performance of the proposed framework is investigated through\nempirical tests and compared with that of the model trained with the COCO\ndataset. The results show that the proposed framework outperforms the model\ncompared. This implies that the learning effectiveness of the framework\ndeveloped is superior to the models with the typical dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 22:28:48 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Varatharasan", "Vinorth", ""], ["Shin", "Hyo-Sang", ""], ["Tsourdos", "Antonios", ""], ["Colosimo", "Nick", ""]]}, {"id": "2002.12470", "submitter": "Hang Zhang", "authors": "Hang Zhang, Jinwei Zhang, Qihao Zhang, Jeremy Kim, Shun Zhang, Susan\n  A. Gauthier, Pascal Spincemaille, Thanh D. Nguyen, Mert R. Sabuncu, and Yi\n  Wang", "title": "RSANet: Recurrent Slice-wise Attention Network for Multiple Sclerosis\n  Lesion Segmentation", "comments": "Accepted for publication in MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32248-9_46", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain lesion volume measured on T2 weighted MRI images is a clinically\nimportant disease marker in multiple sclerosis (MS). Manual delineation of MS\nlesions is a time-consuming and highly operator-dependent task, which is\ninfluenced by lesion size, shape and conspicuity. Recently, automated lesion\nsegmentation algorithms based on deep neural networks have been developed with\npromising results. In this paper, we propose a novel recurrent slice-wise\nattention network (RSANet), which models 3D MRI images as sequences of slices\nand captures long-range dependencies through a recurrent manner to utilize\ncontextual information of MS lesions. Experiments on a dataset with 43 patients\nshow that the proposed method outperforms the state-of-the-art approaches. Our\nimplementation is available online at https://github.com/tinymilky/RSANet.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 22:46:10 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Zhang", "Hang", ""], ["Zhang", "Jinwei", ""], ["Zhang", "Qihao", ""], ["Kim", "Jeremy", ""], ["Zhang", "Shun", ""], ["Gauthier", "Susan A.", ""], ["Spincemaille", "Pascal", ""], ["Nguyen", "Thanh D.", ""], ["Sabuncu", "Mert R.", ""], ["Wang", "Yi", ""]]}, {"id": "2002.12489", "submitter": "Yan Lu", "authors": "Yan Lu, Yue Wu, Bin Liu, Tianzhu Zhang, Baopu Li, Qi Chu and Nenghai\n  Yu", "title": "Cross-modality Person re-identification with Shared-Specific Feature\n  Transfer", "comments": "To appear at CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modality person re-identification (cm-ReID) is a challenging but key\ntechnology for intelligent video analysis. Existing works mainly focus on\nlearning common representation by embedding different modalities into a same\nfeature space. However, only learning the common characteristics means great\ninformation loss, lowering the upper bound of feature distinctiveness. In this\npaper, we tackle the above limitation by proposing a novel cross-modality\nshared-specific feature transfer algorithm (termed cm-SSFT) to explore the\npotential of both the modality-shared information and the modality-specific\ncharacteristics to boost the re-identification performance. We model the\naffinities of different modality samples according to the shared features and\nthen transfer both shared and specific features among and across modalities. We\nalso propose a complementary feature learning strategy including modality\nadaption, project adversarial learning and reconstruction enhancement to learn\ndiscriminative and complementary shared and specific features of each modality,\nrespectively. The entire cm-SSFT algorithm can be trained in an end-to-end\nmanner. We conducted comprehensive experiments to validate the superiority of\nthe overall algorithm and the effectiveness of each component. The proposed\nalgorithm significantly outperforms state-of-the-arts by 22.5% and 19.3% mAP on\nthe two mainstream benchmark datasets SYSU-MM01 and RegDB, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 00:18:45 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 04:05:59 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 08:52:17 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Lu", "Yan", ""], ["Wu", "Yue", ""], ["Liu", "Bin", ""], ["Zhang", "Tianzhu", ""], ["Li", "Baopu", ""], ["Chu", "Qi", ""], ["Yu", "Nenghai", ""]]}, {"id": "2002.12492", "submitter": "Stanislav Panev", "authors": "Stanislav Panev, Francisco Vicente, Fernando De la Torre and\n  V\\'eronique Prinet", "title": "Road Curb Detection and Localization with Monocular Forward-view Vehicle\n  Camera", "comments": "17 pages, 21 figures, IEEE Transactions on Intelligent Transportation\n  Systems", "journal-ref": "IEEE Transactions on Intelligent Transportation Systems (Volume:\n  20, Issue: 9, Sept. 2019)", "doi": "10.1109/TITS.2018.2878652", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust method for estimating road curb 3D parameters (size,\nlocation, orientation) using a calibrated monocular camera equipped with a\nfisheye lens. Automatic curb detection and localization is particularly\nimportant in the context of Advanced Driver Assistance System (ADAS), i.e. to\nprevent possible collision and damage of the vehicle's bumper during\nperpendicular and diagonal parking maneuvers. Combining 3D geometric reasoning\nwith advanced vision-based detection methods, our approach is able to estimate\nthe vehicle to curb distance in real time with mean accuracy of more than 90%,\nas well as its orientation, height and depth.\n  Our approach consists of two distinct components - curb detection in each\nindividual video frame and temporal analysis. The first part comprises of\nsophisticated curb edges extraction and parametrized 3D curb template fitting.\nUsing a few assumptions regarding the real world geometry, we can thus retrieve\nthe curb's height and its relative position w.r.t. the moving vehicle on which\nthe camera is mounted. Support Vector Machine (SVM) classifier fed with\nHistograms of Oriented Gradients (HOG) is used for appearance-based filtering\nout outliers. In the second part, the detected curb regions are tracked in the\ntemporal domain, so as to perform a second pass of false positives rejection.\n  We have validated our approach on a newly collected database of 11 videos\nunder different conditions. We have used point-wise LIDAR measurements and\nmanual exhaustive labels as a ground truth.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 00:24:18 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Panev", "Stanislav", ""], ["Vicente", "Francisco", ""], ["De la Torre", "Fernando", ""], ["Prinet", "V\u00e9ronique", ""]]}, {"id": "2002.12504", "submitter": "Marius Arvinte", "authors": "Marius Arvinte, Ahmed Tewfik, Sriram Vishwanath", "title": "Detecting Patch Adversarial Attacks with Image Residuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an adversarial sample detection algorithm based on image\nresiduals, specifically designed to guard against patch-based attacks. The\nimage residual is obtained as the difference between an input image and a\ndenoised version of it, and a discriminator is trained to distinguish between\nclean and adversarial samples. More precisely, we use a wavelet domain\nalgorithm for denoising images and demonstrate that the obtained residuals act\nas a digital fingerprint for adversarial attacks. To emulate the limitations of\na physical adversary, we evaluate the performance of our approach against\nlocalized (patch-based) adversarial attacks, including in settings where the\nadversary has complete knowledge about the detection scheme. Our results show\nthat the proposed detection method generalizes to previously unseen, stronger\nattacks and that it is able to reduce the success rate (conversely, increase\nthe computational effort) of an adaptive attacker.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 01:28:22 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 16:19:17 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Arvinte", "Marius", ""], ["Tewfik", "Ahmed", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "2002.12509", "submitter": "Jinyuan Zhao", "authors": "Jinyuan Zhao and Yanna Wang and Baihua Xiao and Cunzhao Shi and Fuxi\n  Jia and Chunheng Wang", "title": "DGST : Discriminator Guided Scene Text detector", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection task has attracted considerable attention in computer\nvision because of its wide application. In recent years, many researchers have\nintroduced methods of semantic segmentation into the task of scene text\ndetection, and achieved promising results. This paper proposes a detector\nframework based on the conditional generative adversarial networks to improve\nthe segmentation effect of scene text detection, called DGST (Discriminator\nGuided Scene Text detector). Instead of binary text score maps generated by\nsome existing semantic segmentation based methods, we generate a multi-scale\nsoft text score map with more information to represent the text position more\nreasonably, and solve the problem of text pixel adhesion in the process of text\nextraction. Experiments on standard datasets demonstrate that the proposed DGST\nbrings noticeable gain and outperforms state-of-the-art methods. Specifically,\nit achieves an F-measure of 87% on ICDAR 2015 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 01:47:36 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Zhao", "Jinyuan", ""], ["Wang", "Yanna", ""], ["Xiao", "Baihua", ""], ["Shi", "Cunzhao", ""], ["Jia", "Fuxi", ""], ["Wang", "Chunheng", ""]]}, {"id": "2002.12520", "submitter": "Matt Gorbett", "authors": "Matt Gorbett, Nathaniel Blanchard", "title": "Utilizing Network Properties to Detect Erroneous Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to a wide range of erroneous inputs such as\nadversarial, corrupted, out-of-distribution, and misclassified examples. In\nthis work, we train a linear SVM classifier to detect these four types of\nerroneous data using hidden and softmax feature vectors of pre-trained neural\nnetworks. Our results indicate that these faulty data types generally exhibit\nlinearly separable activation properties from correct examples, giving us the\nability to reject bad inputs with no extra training or overhead. We\nexperimentally validate our findings across a diverse range of datasets,\ndomains, pre-trained models, and adversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 03:20:55 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 16:43:55 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Gorbett", "Matt", ""], ["Blanchard", "Nathaniel", ""]]}, {"id": "2002.12527", "submitter": "Mingxuan Li", "authors": "Mingxuan Li, Jingyuan Wang, Yufan Wu", "title": "Are L2 adversarial examples intrinsically different?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DDN) has achieved notable success in various tasks,\nincluding many security concerning scenarios. However, a considerable amount of\nwork has proved its vulnerability to adversaries. We unravel the properties\nthat can intrinsically differentiate adversarial examples and normal inputs\nthrough theoretical analysis. That is, adversarial examples generated by $L_2$\nattacks usually have larger input sensitivity which can be used to identify\nthem efficiently. We also found that those generated by $L_\\infty$ attacks will\nbe different enough in the pixel domain to be detected empirically. To verify\nour analysis, we proposed a \\textbf{G}uided \\textbf{C}omplementary\n\\textbf{D}efense module (\\textbf{GCD}) integrating detection and recovery\nprocesses. When compared with adversarial detection methods, our detector\nachieves a detection AUC of over 0.98 against most of the attacks. When\ncomparing our guided rectifier with commonly used adversarial training methods\nand other rectification methods, our rectifier outperforms them by a large\nmargin. We achieve a recovered classification accuracy of up to 99\\% on MNIST,\n89\\% on CIFAR-10, and 87\\% on ImageNet subsets against $L_2$ attacks.\nFurthermore, under the white-box setting, our holistic defensive module shows a\npromising degree of robustness. Thus, we confirm that at least $L_2$\nadversarial examples are intrinsically different enough from normal inputs both\ntheoretically and empirically. And we shed light upon designing simple yet\neffective defensive methods with these properties.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 03:42:52 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 04:37:21 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Li", "Mingxuan", ""], ["Wang", "Jingyuan", ""], ["Wu", "Yufan", ""]]}, {"id": "2002.12535", "submitter": "Jinlong Kang", "authors": "Jinlong Kang, Jiaxiang Zheng, Heng Bai, Xiaoting Xue, Yang Zhou, Jun\n  Guo", "title": "A Video Analysis Method on Wanfang Dataset via Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic of object detection has been largely improved recently, especially\nwith the development of convolutional neural network. However, there still\nexist a lot of challenging cases, such as small object, compact and dense or\nhighly overlapping object. Existing methods can detect multiple objects\nwonderfully, but because of the slight changes between frames, the detection\neffect of the model will become unstable, the detection results may result in\ndropping or increasing the object. In the pedestrian flow detection task, such\nphenomenon can not accurately calculate the flow. To solve this problem, in\nthis paper, we describe the new function for real-time multi-object detection\nin sports competition and pedestrians flow detection in public based on deep\nlearning. Our work is to extract a video clip and solve this frame of clips\nefficiently. More specfically, our algorithm includes two stages: judge method\nand optimization method. The judge can set a maximum threshold for better\nresults under the model, the threshold value corresponds to the upper limit of\nthe algorithm with better detection results. The optimization method to solve\ndetection jitter problem. Because of the occurrence of frame hopping in the\nvideo, and it will result in the generation of video fragments discontinuity.\nWe use optimization algorithm to get the key value, and then the detection\nresult value of index is replaced by key value to stabilize the change of\ndetection result sequence. Based on the proposed algorithm, we adopt wanfang\nsports competition dataset as the main test dataset and our own test dataset\nfor YOLOv3-Abnormal Number Version(YOLOv3-ANV), which is 5.4% average\nimprovement compared with existing methods. Also, video above the threshold\nvalue can be obtained for further analysis. Spontaneously, our work also can\nused for pedestrians flow detection and pedestrian alarm tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 04:09:53 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Kang", "Jinlong", ""], ["Zheng", "Jiaxiang", ""], ["Bai", "Heng", ""], ["Xue", "Xiaoting", ""], ["Zhou", "Yang", ""], ["Guo", "Jun", ""]]}, {"id": "2002.12536", "submitter": "Zichu Liu", "authors": "Zichu Liu, Qing Zhang, Pei Wang, Zhen Li, Huiru Wang", "title": "Automated classification of stems and leaves of potted plants based on\n  point cloud data", "comments": "31 pages,15 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate classification of plant organs is a key step in monitoring the\ngrowing status and physiology of plants. A classification method was proposed\nto classify the leaves and stems of potted plants automatically based on the\npoint cloud data of the plants, which is a nondestructive acquisition. The leaf\npoint training samples were automatically extracted by using the\nthree-dimensional convex hull algorithm, while stem point training samples were\nextracted by using the point density of a two-dimensional projection. The two\ntraining sets were used to classify all the points into leaf points and stem\npoints by utilizing the support vector machine (SVM) algorithm. The proposed\nmethod was tested by using the point cloud data of three potted plants and\ncompared with two other methods, which showed that the proposed method can\nclassify leaf and stem points accurately and efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 04:15:38 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Liu", "Zichu", ""], ["Zhang", "Qing", ""], ["Wang", "Pei", ""], ["Li", "Zhen", ""], ["Wang", "Huiru", ""]]}, {"id": "2002.12557", "submitter": "Kyungjun Lee", "authors": "Kyungjun Lee, Abhinav Shrivastava, Hernisa Kacorri", "title": "Hand-Priming in Object Localization for Assistive Egocentric Vision", "comments": "the 2020 Winter Conference on Applications of Computer Vision (WACV\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric vision holds great promises for increasing access to visual\ninformation and improving the quality of life for people with visual\nimpairments, with object recognition being one of the daily challenges for this\npopulation. While we strive to improve recognition performance, it remains\ndifficult to identify which object is of interest to the user; the object may\nnot even be included in the frame due to challenges in camera aiming without\nvisual feedback. Also, gaze information, commonly used to infer the area of\ninterest in egocentric vision, is often not dependable. However, blind users\noften tend to include their hand either interacting with the object that they\nwish to recognize or simply placing it in proximity for better camera aiming.\nWe propose localization models that leverage the presence of the hand as the\ncontextual information for priming the center area of the object of interest.\nIn our approach, hand segmentation is fed to either the entire localization\nnetwork or its last convolutional layers. Using egocentric datasets from\nsighted and blind individuals, we show that the hand-priming achieves higher\nprecision than other approaches, such as fine-tuning, multi-class, and\nmulti-task learning, which also encode hand-object interactions in\nlocalization.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 05:32:36 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Lee", "Kyungjun", ""], ["Shrivastava", "Abhinav", ""], ["Kacorri", "Hernisa", ""]]}, {"id": "2002.12573", "submitter": "Yaxin Zhao", "authors": "Yaxin Zhao, Jichao Jiao and Tangkun Zhang", "title": "MANet: Multimodal Attention Network based Point- View fusion for 3D\n  Shape Recognition", "comments": "8 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape recognition has attracted more and more attention as a task of 3D\nvision research. The proliferation of 3D data encourages various deep learning\nmethods based on 3D data. Now there have been many deep learning models based\non point-cloud data or multi-view data alone. However, in the era of big data,\nintegrating data of two different modals to obtain a unified 3D shape\ndescriptor is bound to improve the recognition accuracy. Therefore, this paper\nproposes a fusion network based on multimodal attention mechanism for 3D shape\nrecognition. Considering the limitations of multi-view data, we introduce a\nsoft attention scheme, which can use the global point-cloud features to filter\nthe multi-view features, and then realize the effective fusion of the two\nfeatures. More specifically, we obtain the enhanced multi-view features by\nmining the contribution of each multi-view image to the overall shape\nrecognition, and then fuse the point-cloud features and the enhanced multi-view\nfeatures to obtain a more discriminative 3D shape descriptor. We have performed\nrelevant experiments on the ModelNet40 dataset, and experimental results verify\nthe effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 07:00:14 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Zhao", "Yaxin", ""], ["Jiao", "Jichao", ""], ["Zhang", "Tangkun", ""]]}, {"id": "2002.12578", "submitter": "Fahad Shamshad", "authors": "Fahad Shamshad, Ali Ahmed", "title": "Class-Specific Blind Deconvolutional Phase Retrieval Under a Generative\n  Prior", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the highly ill-posed problem of jointly recovering\ntwo real-valued signals from the phaseless measurements of their circular\nconvolution. The problem arises in various imaging modalities such as Fourier\nptychography, X-ray crystallography, and in visible light communication. We\npropose to solve this inverse problem using alternating gradient descent\nalgorithm under two pretrained deep generative networks as priors; one is\ntrained on sharp images and the other on blur kernels. The proposed recovery\nalgorithm strives to find a sharp image and a blur kernel in the range of the\nrespective pre-generators that \\textit{best} explain the forward measurement\nmodel. In doing so, we are able to reconstruct quality image estimates.\nMoreover, the numerics show that the proposed approach performs well on the\nchallenging measurement models that reflect the physically realizable imaging\nsystems and is also robust to noise\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 07:36:28 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Shamshad", "Fahad", ""], ["Ahmed", "Ali", ""]]}, {"id": "2002.12580", "submitter": "Di Xie", "authors": "Rang Meng, Weijie Chen, Di Xie, Yuan Zhang, Shiliang Pu", "title": "Neural Inheritance Relation Guided One-Shot Layer Assignment Search", "comments": "AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer assignment is seldom picked out as an independent research topic in\nneural architecture search. In this paper, for the first time, we\nsystematically investigate the impact of different layer assignments to the\nnetwork performance by building an architecture dataset of layer assignment on\nCIFAR-100. Through analyzing this dataset, we discover a neural inheritance\nrelation among the networks with different layer assignments, that is, the\noptimal layer assignments for deeper networks always inherit from those for\nshallow networks. Inspired by this neural inheritance relation, we propose an\nefficient one-shot layer assignment search approach via inherited sampling.\nSpecifically, the optimal layer assignment searched in the shallow network can\nbe provided as a strong sampling priori to train and search the deeper ones in\nsupernet, which extremely reduces the network search space. Comprehensive\nexperiments carried out on CIFAR-100 illustrate the efficiency of our proposed\nmethod. Our search results are strongly consistent with the optimal ones\ndirectly selected from the architecture dataset. To further confirm the\ngeneralization of our proposed method, we also conduct experiments on\nTiny-ImageNet and ImageNet. Our searched results are remarkably superior to the\nhandcrafted ones under the unchanged computational budgets. The neural\ninheritance relation discovered in this paper can provide insights to the\nuniversal neural architecture search.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 07:40:48 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Meng", "Rang", ""], ["Chen", "Weijie", ""], ["Xie", "Di", ""], ["Zhang", "Yuan", ""], ["Pu", "Shiliang", ""]]}, {"id": "2002.12585", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Xuancheng Ren, Yuanxin Liu, Kai Lei and Xu Sun", "title": "Exploring and Distilling Cross-Modal Information for Image Captioning", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, attention-based encoder-decoder models have been used extensively\nin image captioning. Yet there is still great difficulty for the current\nmethods to achieve deep image understanding. In this work, we argue that such\nunderstanding requires visual attention to correlated image regions and\nsemantic attention to coherent attributes of interest. Based on the\nTransformer, to perform effective attention, we explore image captioning from a\ncross-modal perspective and propose the Global-and-Local Information\nExploring-and-Distilling approach that explores and distills the source\ninformation in vision and language. It globally provides the aspect vector, a\nspatial and relational representation of images based on caption contexts,\nthrough the extraction of salient region groupings and attribute collocations,\nand locally extracts the fine-grained regions and attributes in reference to\nthe aspect vector for word selection. Our Transformer-based model achieves a\nCIDEr score of 129.3 in offline COCO evaluation on the COCO testing set with\nremarkable efficiency in terms of accuracy, speed, and parameter budget.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 07:46:48 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 11:53:51 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Liu", "Fenglin", ""], ["Ren", "Xuancheng", ""], ["Liu", "Yuanxin", ""], ["Lei", "Kai", ""], ["Sun", "Xu", ""]]}, {"id": "2002.12588", "submitter": "Mahsa Paknezhad", "authors": "Mahsa Paknezhad, Sheng Yang Michael Loh, Yukti Choudhury, Valerie Koh\n  Cui Koh, TimothyTay Kwang Yong, Hui Shan Tan, Ravindran Kanesvaran, Puay Hoon\n  Tan, John Yuen Shyi Peng, Weimiao Yu, Yongcheng Benjamin Tan, Yong Zhen Loy,\n  Min-Han Tan, Hwee Kuan Lee", "title": "Regional Registration of Whole Slide Image Stacks Containing Highly\n  Deformed Artefacts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: High resolution 2D whole slide imaging provides rich information\nabout the tissue structure. This information can be a lot richer if these 2D\nimages can be stacked into a 3D tissue volume. A 3D analysis, however, requires\naccurate reconstruction of the tissue volume from the 2D image stack. This task\nis not trivial due to the distortions that each individual tissue slice\nexperiences while cutting and mounting the tissue on the glass slide.\nPerforming registration for the whole tissue slices may be adversely affected\nby the deformed tissue regions. Consequently, regional registration is found to\nbe more effective. In this paper, we propose an accurate and robust regional\nregistration algorithm for whole slide images which incrementally focuses\nregistration on the area around the region of interest. Results: Using mean\nsimilarity index as the metric, the proposed algorithm (mean $\\pm$ std: $0.84\n\\pm 0.11$) followed by a fine registration algorithm ($0.86 \\pm 0.08$)\noutperformed the state-of-the-art linear whole tissue registration algorithm\n($0.74 \\pm 0.19$) and the regional version of this algorithm ($0.81 \\pm 0.15$).\nThe proposed algorithm also outperforms the state-of-the-art nonlinear\nregistration algorithm (original : $0.82 \\pm 0.12$, regional : $0.77 \\pm 0.22$)\nfor whole slide images and a recently proposed patch-based registration\nalgorithm (patch size 256: $0.79 \\pm 0.16$ , patch size 512: $0.77 \\pm 0.16$)\nfor medical images. Availability: The C++ implementation code is available\nonline at the github repository:\nhttps://github.com/MahsaPaknezhad/WSIRegistration\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 07:57:56 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Paknezhad", "Mahsa", ""], ["Loh", "Sheng Yang Michael", ""], ["Choudhury", "Yukti", ""], ["Koh", "Valerie Koh Cui", ""], ["Yong", "TimothyTay Kwang", ""], ["Tan", "Hui Shan", ""], ["Kanesvaran", "Ravindran", ""], ["Tan", "Puay Hoon", ""], ["Peng", "John Yuen Shyi", ""], ["Yu", "Weimiao", ""], ["Tan", "Yongcheng Benjamin", ""], ["Loy", "Yong Zhen", ""], ["Tan", "Min-Han", ""], ["Lee", "Hwee Kuan", ""]]}, {"id": "2002.12597", "submitter": "Makoto Takamoto", "authors": "Makoto Takamoto, Yusuke Morishita, and Hitoshi Imaoka", "title": "An Efficient Method of Training Small Models for Regression Problems\n  with Knowledge Distillation", "comments": "7 pages, 2 figures, draft version of a paper accepted for IEEE 3rd\n  International Conference on Multimedia Information Processing and Retrieval\n  (MIPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing deep neural network (DNN) models becomes a very important and\nnecessary technique for real-world applications, such as deploying those models\non mobile devices. Knowledge distillation is one of the most popular methods\nfor model compression, and many studies have been made on developing this\ntechnique. However, those studies mainly focused on classification problems,\nand very few attempts have been made on regression problems, although there are\nmany application of DNNs on regression problems. In this paper, we propose a\nnew formalism of knowledge distillation for regression problems. First, we\npropose a new loss function, teacher outlier rejection loss, which rejects\noutliers in training samples using teacher model predictions. Second, we\nconsider a multi-task network with two outputs: one estimates training labels\nwhich is in general contaminated by noisy labels; And the other estimates\nteacher model's output which is expected to modify the noise labels following\nthe memorization effects. By considering the multi-task network, training of\nthe feature extraction of student models becomes more effective, and it allows\nus to obtain a better student model than one trained from scratch. We performed\ncomprehensive evaluation with one simple toy model: sinusoidal function, and\ntwo open datasets: MPIIGaze, and Multi-PIE. Our results show consistent\nimprovement in accuracy regardless of the annotation error level in the\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 08:46:12 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Takamoto", "Makoto", ""], ["Morishita", "Yusuke", ""], ["Imaoka", "Hitoshi", ""]]}, {"id": "2002.12609", "submitter": "Hyeongseok Jeon", "authors": "Hyeongseok Jeon, Junwon Choi, Dongsuk Kum", "title": "SCALE-Net: Scalable Vehicle Trajectory Prediction Network under Random\n  Number of Interacting Vehicles via Edge-enhanced Graph Convolutional Neural\n  Network", "comments": "8 pages, 9 figures, and 2 tables, the paper is submitted to the\n  conference IROS2020 and is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future trajectory of surrounding vehicles in a randomly\nvarying traffic level is one of the most challenging problems in developing an\nautonomous vehicle. Since there is no pre-defined number of interacting\nvehicles participate in, the prediction network has to be scalable with respect\nto the vehicle number in order to guarantee the consistency in terms of both\naccuracy and computational load. In this paper, the first fully scalable\ntrajectory prediction network, SCALE-Net, is proposed that can ensure both\nhigher prediction performance and consistent computational load regardless of\nthe number of surrounding vehicles. The SCALE-Net employs the Edge-enhance\nGraph Convolutional Neural Network (EGCN) for the inter-vehicular interaction\nembedding network. Since the proposed EGCN is inherently scalable with respect\nto the graph node (an agent in this study), the model can be operated\nindependently from the total number of vehicles considered. We evaluated the\nscalability of the SCALE-Net on the publically available NGSIM datasets by\ncomparing variations on computation time and prediction accuracy per single\ndriving scene with respect to the varying vehicle number. The experimental test\nshows that both computation time and prediction performance of the SCALE-Net\nconsistently outperform those of previous models regardless of the level of\ntraffic complexities.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 09:25:01 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Jeon", "Hyeongseok", ""], ["Choi", "Junwon", ""], ["Kum", "Dongsuk", ""]]}, {"id": "2002.12623", "submitter": "Florian Bernard", "authors": "Florian Bernard, Zeeshan Khan Suri, Christian Theobalt", "title": "MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment", "comments": "to appear at CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convex mixed-integer programming formulation for non-rigid shape\nmatching. To this end, we propose a novel shape deformation model based on an\nefficient low-dimensional discrete model, so that finding a globally optimal\nsolution is tractable in (most) practical cases. Our approach combines several\nfavourable properties: it is independent of the initialisation, it is much more\nefficient to solve to global optimality compared to analogous quadratic\nassignment problem formulations, and it is highly flexible in terms of the\nvariants of matching problems it can handle. Experimentally we demonstrate that\nour approach outperforms existing methods for sparse shape matching, that it\ncan be used for initialising dense shape matching methods, and we showcase its\nflexibility on several examples.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 09:54:06 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Bernard", "Florian", ""], ["Suri", "Zeeshan Khan", ""], ["Theobalt", "Christian", ""]]}, {"id": "2002.12625", "submitter": "Liang An", "authors": "Yuxiang Zhang, Liang An, Tao Yu, Xiu Li, Kun Li, Yebin Liu", "title": "4D Association Graph for Realtime Multi-person Motion Capture Using\n  Multiple Video Cameras", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes a novel realtime multi-person motion capture algorithm\nusing multiview video inputs. Due to the heavy occlusions in each view, joint\noptimization on the multiview images and multiple temporal frames is\nindispensable, which brings up the essential challenge of realtime efficiency.\nTo this end, for the first time, we unify per-view parsing, cross-view\nmatching, and temporal tracking into a single optimization framework, i.e., a\n4D association graph that each dimension (image space, viewpoint and time) can\nbe treated equally and simultaneously. To solve the 4D association graph\nefficiently, we further contribute the idea of 4D limb bundle parsing based on\nheuristic searching, followed with limb bundle assembling by proposing a bundle\nKruskal's algorithm. Our method enables a realtime online motion capture system\nrunning at 30fps using 5 cameras on a 5-person scene. Benefiting from the\nunified parsing, matching and tracking constraints, our method is robust to\nnoisy detection, and achieves high-quality online pose reconstruction quality.\nThe proposed method outperforms the state-of-the-art method quantitatively\nwithout using high-level appearance information. We also contribute a multiview\nvideo dataset synchronized with a marker-based motion capture system for\nscientific evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 09:57:05 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Zhang", "Yuxiang", ""], ["An", "Liang", ""], ["Yu", "Tao", ""], ["Li", "Xiu", ""], ["Li", "Kun", ""], ["Liu", "Yebin", ""]]}, {"id": "2002.12655", "submitter": "Edgar Sch\\\"onfeld", "authors": "Edgar Sch\\\"onfeld, Bernt Schiele, Anna Khoreva", "title": "A U-Net Based Discriminator for Generative Adversarial Networks", "comments": "CVPR 2020 (Main Conference). Code repository:\n  https://github.com/boschresearch/unetgan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the major remaining challenges for generative adversarial networks\n(GANs) is the capacity to synthesize globally and locally coherent images with\nobject shapes and textures indistinguishable from real images. To target this\nissue we propose an alternative U-Net based discriminator architecture,\nborrowing the insights from the segmentation literature. The proposed U-Net\nbased architecture allows to provide detailed per-pixel feedback to the\ngenerator while maintaining the global coherence of synthesized images, by\nproviding the global image feedback as well. Empowered by the per-pixel\nresponse of the discriminator, we further propose a per-pixel consistency\nregularization technique based on the CutMix data augmentation, encouraging the\nU-Net discriminator to focus more on semantic and structural changes between\nreal and fake images. This improves the U-Net discriminator training, further\nenhancing the quality of generated samples. The novel discriminator improves\nover the state of the art in terms of the standard distribution and image\nquality metrics, enabling the generator to synthesize images with varying\nstructure, appearance and levels of detail, maintaining global and local\nrealism. Compared to the BigGAN baseline, we achieve an average improvement of\n2.7 FID points across FFHQ, CelebA, and the newly introduced COCO-Animals\ndataset. The code is available at https://github.com/boschresearch/unetgan.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 11:16:54 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 23:22:06 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sch\u00f6nfeld", "Edgar", ""], ["Schiele", "Bernt", ""], ["Khoreva", "Anna", ""]]}, {"id": "2002.12663", "submitter": "Rui Lin", "authors": "Rui Lin, Ching-Yun Ko, Zhuolun He, Cong Chen, Yuan Cheng, Hao Yu,\n  Graziano Chesi, Ngai Wong", "title": "HOTCAKE: Higher Order Tucker Articulated Kernels for Deeper CNN\n  Compression", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging edge computing has promoted immense interests in compacting a\nneural network without sacrificing much accuracy. In this regard, low-rank\ntensor decomposition constitutes a powerful tool to compress convolutional\nneural networks (CNNs) by decomposing the 4-way kernel tensor into multi-stage\nsmaller ones. Building on top of Tucker-2 decomposition, we propose a\ngeneralized Higher Order Tucker Articulated Kernels (HOTCAKE) scheme comprising\nfour steps: input channel decomposition, guided Tucker rank selection, higher\norder Tucker decomposition and fine-tuning. By subjecting each CONV layer to\nHOTCAKE, a highly compressed CNN model with graceful accuracy trade-off is\nobtained. Experiments show HOTCAKE can compress even pre-compressed models and\nproduce state-of-the-art lightweight networks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 11:37:09 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Lin", "Rui", ""], ["Ko", "Ching-Yun", ""], ["He", "Zhuolun", ""], ["Chen", "Cong", ""], ["Cheng", "Yuan", ""], ["Yu", "Hao", ""], ["Chesi", "Graziano", ""], ["Wong", "Ngai", ""]]}, {"id": "2002.12674", "submitter": "Sebastian Lunz", "authors": "Sebastian Lunz, Yingzhen Li, Andrew Fitzgibbon, Nate Kushman", "title": "Inverse Graphics GAN: Learning to Generate 3D Shapes from Unstructured\n  2D Data", "comments": "8 pages paper, 3 pages references, 18 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown the ability to learn generative models for 3D shapes\nfrom only unstructured 2D images. However, training such models requires\ndifferentiating through the rasterization step of the rendering process,\ntherefore past work has focused on developing bespoke rendering models which\nsmooth over this non-differentiable process in various ways. Such models are\nthus unable to take advantage of the photo-realistic, fully featured,\nindustrial renderers built by the gaming and graphics industry. In this paper\nwe introduce the first scalable training technique for 3D generative models\nfrom 2D data which utilizes an off-the-shelf non-differentiable renderer. To\naccount for the non-differentiability, we introduce a proxy neural renderer to\nmatch the output of the non-differentiable renderer. We further propose\ndiscriminator output matching to ensure that the neural renderer learns to\nsmooth over the rasterization appropriately. We evaluate our model on images\nrendered from our generated 3D shapes, and show that our model can consistently\nlearn to generate better shapes than existing models when trained with\nexclusively unstructured 2D images.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 12:28:12 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Lunz", "Sebastian", ""], ["Li", "Yingzhen", ""], ["Fitzgibbon", "Andrew", ""], ["Kushman", "Nate", ""]]}, {"id": "2002.12680", "submitter": "Yuyu Guo", "authors": "Yuyu Guo, Lei Bi, Euijoon Ahn, Dagan Feng, Qian Wang and Jinman Kim", "title": "A Spatiotemporal Volumetric Interpolation Network for 4D Dynamic Medical\n  Image", "comments": "10 pages, 8 figures, Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic medical imaging is usually limited in application due to the large\nradiation doses and longer image scanning and reconstruction times. Existing\nmethods attempt to reduce the dynamic sequence by interpolating the volumes\nbetween the acquired image volumes. However, these methods are limited to\neither 2D images and/or are unable to support large variations in the motion\nbetween the image volume sequences. In this paper, we present a spatiotemporal\nvolumetric interpolation network (SVIN) designed for 4D dynamic medical images.\nSVIN introduces dual networks: first is the spatiotemporal motion network that\nleverages the 3D convolutional neural network (CNN) for unsupervised parametric\nvolumetric registration to derive spatiotemporal motion field from two-image\nvolumes; the second is the sequential volumetric interpolation network, which\nuses the derived motion field to interpolate image volumes, together with a new\nregression-based module to characterize the periodic motion cycles in\nfunctional organ structures. We also introduce an adaptive multi-scale\narchitecture to capture the volumetric large anatomy motions. Experimental\nresults demonstrated that our SVIN outperformed state-of-the-art temporal\nmedical interpolation methods and natural video interpolation methods that have\nbeen extended to support volumetric images. Our ablation study further\nexemplified that our motion network was able to better represent the large\nfunctional motion compared with the state-of-the-art unsupervised medical\nregistration methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 12:40:34 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 02:18:37 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Guo", "Yuyu", ""], ["Bi", "Lei", ""], ["Ahn", "Euijoon", ""], ["Feng", "Dagan", ""], ["Wang", "Qian", ""], ["Kim", "Jinman", ""]]}, {"id": "2002.12687", "submitter": "Yang You", "authors": "Yang You, Yujing Lou, Chengkun Li, Zhoujun Cheng, Liangwei Li,\n  Lizhuang Ma, Weiming Wang, Cewu Lu", "title": "KeypointNet: A Large-scale 3D Keypoint Dataset Aggregated from Numerous\n  Human Annotations", "comments": "8 pages; to appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting 3D objects keypoints is of great interest to the areas of both\ngraphics and computer vision. There have been several 2D and 3D keypoint\ndatasets aiming to address this problem in a data-driven way. These datasets,\nhowever, either lack scalability or bring ambiguity to the definition of\nkeypoints. Therefore, we present KeypointNet: the first large-scale and diverse\n3D keypoint dataset that contains 103,450 keypoints and 8,234 3D models from 16\nobject categories, by leveraging numerous human annotations. To handle the\ninconsistency between annotations from different people, we propose a novel\nmethod to aggregate these keypoints automatically, through minimization of a\nfidelity loss. Finally, ten state-of-the-art methods are benchmarked on our\nproposed dataset. Our code and data are available on\nhttps://github.com/qq456cvb/KeypointNet.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 12:58:56 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 02:20:32 GMT"}, {"version": "v3", "created": "Sat, 4 Apr 2020 08:28:56 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2020 00:54:19 GMT"}, {"version": "v5", "created": "Thu, 23 Apr 2020 02:56:35 GMT"}, {"version": "v6", "created": "Fri, 7 Aug 2020 02:07:56 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["You", "Yang", ""], ["Lou", "Yujing", ""], ["Li", "Chengkun", ""], ["Cheng", "Zhoujun", ""], ["Li", "Liangwei", ""], ["Ma", "Lizhuang", ""], ["Wang", "Weiming", ""], ["Lu", "Cewu", ""]]}, {"id": "2002.12730", "submitter": "Micha\\\"el Ramamonjisoa", "authors": "Michael Ramamonjisoa, Yuming Du, Vincent Lepetit", "title": "Predicting Sharp and Accurate Occlusion Boundaries in Monocular Depth\n  Estimation Using Displacement Fields", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for depth map prediction from monocular images tend to\npredict smooth, poorly localized contours for the occlusion boundaries in the\ninput image. This is unfortunate as occlusion boundaries are important cues to\nrecognize objects, and as we show, may lead to a way to discover new objects\nfrom scene reconstruction. To improve predicted depth maps, recent methods rely\non various forms of filtering or predict an additive residual depth map to\nrefine a first estimate. We instead learn to predict, given a depth map\npredicted by some reconstruction method, a 2D displacement field able to\nre-sample pixels around the occlusion boundaries into sharper reconstructions.\nOur method can be applied to the output of any depth estimation method, in an\nend-to-end trainable fashion. For evaluation, we manually annotated the\nocclusion boundaries in all the images in the test split of popular NYUv2-Depth\ndataset. We show that our approach improves the localization of occlusion\nboundaries for all state-of-the-art monocular depth estimation methods that we\ncould evaluate, without degrading the depth accuracy for the rest of the\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 14:15:07 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 12:46:09 GMT"}, {"version": "v3", "created": "Sun, 10 May 2020 23:12:00 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ramamonjisoa", "Michael", ""], ["Du", "Yuming", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2002.12749", "submitter": "Paarth Neekhara", "authors": "Shehzeen Hussain, Paarth Neekhara, Malhar Jere, Farinaz Koushanfar and\n  Julian McAuley", "title": "Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to\n  Adversarial Examples", "comments": "Published as a conference paper at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in video manipulation techniques have made the generation of\nfake videos more accessible than ever before. Manipulated videos can fuel\ndisinformation and reduce trust in media. Therefore detection of fake videos\nhas garnered immense interest in academia and industry. Recently developed\nDeepfake detection methods rely on deep neural networks (DNNs) to distinguish\nAI-generated fake videos from real videos. In this work, we demonstrate that it\nis possible to bypass such detectors by adversarially modifying fake videos\nsynthesized using existing Deepfake generation methods. We further demonstrate\nthat our adversarial perturbations are robust to image and video compression\ncodecs, making them a real-world threat. We present pipelines in both white-box\nand black-box attack scenarios that can fool DNN based Deepfake detectors into\nclassifying fake videos as real.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 07:10:58 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 01:17:05 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 22:09:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Hussain", "Shehzeen", ""], ["Neekhara", "Paarth", ""], ["Jere", "Malhar", ""], ["Koushanfar", "Farinaz", ""], ["McAuley", "Julian", ""]]}, {"id": "2002.12819", "submitter": "Shengyu Huang", "authors": "Shengyu Huang, Mikhail Usvyatsov and Konrad Schindler", "title": "Indoor Scene Recognition in 3D", "comments": "IROS 2020 - Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising in what type of environment one is located is an important\nperception task. For instance, for a robot operating in indoors it is helpful\nto be aware whether it is in a kitchen, a hallway or a bedroom. Existing\napproaches attempt to classify the scene based on 2D images or 2.5D range\nimages. Here, we study scene recognition from 3D point cloud (or voxel) data,\nand show that it greatly outperforms methods based on 2D birds-eye views.\nMoreover, we advocate multi-task learning as a way of improving scene\nrecognition, building on the fact that the scene type is highly correlated with\nthe objects in the scene, and therefore with its semantic segmentation into\ndifferent object classes. In a series of ablation studies, we show that\nsuccessful scene recognition is not just the recognition of individual objects\nunique to some scene type (such as a bathtub), but depends on several different\ncues, including coarse 3D geometry, colour, and the (implicit) distribution of\nobject categories. Moreover, we demonstrate that surprisingly sparse 3D data is\nsufficient to classify indoor scenes with good accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 15:47:09 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 21:25:18 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Huang", "Shengyu", ""], ["Usvyatsov", "Mikhail", ""], ["Schindler", "Konrad", ""]]}, {"id": "2002.12868", "submitter": "Brandon Ginley", "authors": "Brandon Ginley (1), Kuang-Yu Jen (2), Avi Rosenberg (3), Felicia Yen\n  (2), Sanjay Jain (4), Agnes Fogo (5), Pinaki Sarder (1 and 6 and 7) ((1)\n  Department of Pathology & Anatomical Sciences, University at Buffalo, the\n  State University of New York, Buffalo, New York, (2) Department of Pathology\n  and Laboratory Medicine, University of California, Davis Medical Center,\n  Sacramento, California, (3) Department of Pathology, Johns Hopkins University\n  School of Medicine, Baltimore, Maryland, (4) Division of Nephrology,\n  Department of Medicine, Washington University School of Medicine, St. Louis,\n  Missouri, (5) Departments of Pathology, Microbiology, Immunology and\n  Medicine, Vanderbilt University, Nashville, Tennessee, (6) Department of\n  Biostatistics, University at Buffalo, the State University of New York,\n  Buffalo, New York, (7) Department of Biomedical Engineering, University at\n  Buffalo, the State University of New York, Buffalo, New York)", "title": "Neural Network Segmentation of Interstitial Fibrosis, Tubular Atrophy,\n  and Glomerulosclerosis in Renal Biopsies", "comments": null, "journal-ref": null, "doi": "10.1681/ASN.2020050652", "report-no": null, "categories": "q-bio.TO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glomerulosclerosis, interstitial fibrosis, and tubular atrophy (IFTA) are\nhistologic indicators of irrecoverable kidney injury. In standard clinical\npractice, the renal pathologist visually assesses, under the microscope, the\npercentage of sclerotic glomeruli and the percentage of renal cortical\ninvolvement by IFTA. Estimation of IFTA is a subjective process due to a varied\nspectrum and definition of morphological manifestations. Modern artificial\nintelligence and computer vision algorithms have the ability to reduce\ninter-observer variability through rigorous quantitation. In this work, we\napply convolutional neural networks for the segmentation of glomerulosclerosis\nand IFTA in periodic acid-Schiff stained renal biopsies. The convolutional\nnetwork approach achieves high performance in intra-institutional holdout data,\nand achieves moderate performance in inter-intuitional holdout data, which the\nnetwork had never seen in training. The convolutional approach demonstrated\ninteresting properties, such as learning to predict regions better than the\nprovided ground truth as well as developing its own conceptualization of\nsegmental sclerosis. Subsequent estimations of IFTA and glomerulosclerosis\npercentages showed high correlation with ground truth.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 17:05:59 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ginley", "Brandon", "", "1 and 6 and 7"], ["Jen", "Kuang-Yu", "", "1 and 6 and 7"], ["Rosenberg", "Avi", "", "1 and 6 and 7"], ["Yen", "Felicia", "", "1 and 6 and 7"], ["Jain", "Sanjay", "", "1 and 6 and 7"], ["Fogo", "Agnes", "", "1 and 6 and 7"], ["Sarder", "Pinaki", "", "1 and 6 and 7"]]}, {"id": "2002.12886", "submitter": "Alban Main De Boissiere", "authors": "Alban Main de Boissiere, Rita Noumeir", "title": "Infrared and 3D skeleton feature fusion for RGB-D action recognition", "comments": "11 pages, 5 figures, submitted to IEEE Access", "journal-ref": "IEEE Access, vol. 8, pp. 168297-168308, 2020", "doi": "10.1109/ACCESS.2020.3023599", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge of skeleton-based action recognition is the difficulty to\nclassify actions with similar motions and object-related actions. Visual clues\nfrom other streams help in that regard. RGB data are sensible to illumination\nconditions, thus unusable in the dark. To alleviate this issue and still\nbenefit from a visual stream, we propose a modular network (FUSION) combining\nskeleton and infrared data. A 2D convolutional neural network (CNN) is used as\na pose module to extract features from skeleton data. A 3D CNN is used as an\ninfrared module to extract visual cues from videos. Both feature vectors are\nthen concatenated and exploited conjointly using a multilayer perceptron (MLP).\nSkeleton data also condition the infrared videos, providing a crop around the\nperforming subjects and thus virtually focusing the attention of the infrared\nmodule. Ablation studies show that using pre-trained networks on other large\nscale datasets as our modules and data augmentation yield considerable\nimprovements on the action classification accuracy. The strong contribution of\nour cropping strategy is also demonstrated. We evaluate our method on the NTU\nRGB+D dataset, the largest dataset for human action recognition from depth\ncameras, and report state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 17:42:53 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["de Boissiere", "Alban Main", ""], ["Noumeir", "Rita", ""]]}, {"id": "2002.12888", "submitter": "Bingchen Liu", "authors": "Bingchen Liu, Kunpeng Song, Ahmed Elgammal", "title": "Sketch-to-Art: Synthesizing Stylized Art Images From Sketches", "comments": "24 pages", "journal-ref": "ACCV 2020", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for synthesizing fully detailed art-stylized images\nfrom sketches. Given a sketch, with no semantic tagging, and a reference image\nof a specific style, the model can synthesize meaningful details with colors\nand textures. The model consists of three modules designed explicitly for\nbetter artistic style capturing and generation. Based on a GAN framework, a\ndual-masked mechanism is introduced to enforce the content constraints (from\nthe sketch), and a feature-map transformation technique is developed to\nstrengthen the style consistency (to the reference image). Finally, an inverse\nprocedure of instance-normalization is proposed to disentangle the style and\ncontent information, therefore yields better synthesis performance. Experiments\ndemonstrate a significant qualitative and quantitative boost over baselines\nbased on previous state-of-the-art techniques, adopted for the proposed\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 19:02:10 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 19:07:49 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 17:20:25 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Liu", "Bingchen", ""], ["Song", "Kunpeng", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "2002.12889", "submitter": "Daiki Tamada", "authors": "Daiki Tamada", "title": "Review: Noise and artifact reduction for MRI using deep learning", "comments": "Submitted to Magnetic Resonance in Medical Sciences on 2/27/2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For several years, numerous attempts have been made to reduce noise and\nartifacts in MRI. Although there have been many successful methods to address\nthese problems, practical implementation for clinical images is still\nchallenging because of its complicated mechanism. Recently, deep learning\nreceived considerable attention, emerging as a machine learning approach in\ndelivering robust MR image processing. The purpose here is therefore to explore\nfurther and review noise and artifact reduction using deep learning for MRI.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 17:50:21 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Tamada", "Daiki", ""]]}, {"id": "2002.12896", "submitter": "Steven McDonagh", "authors": "Daniel Hernandez-Juarez and Sarah Parisot and Benjamin Busam and Ales\n  Leonardis and Gregory Slabaugh and Steven McDonagh", "title": "A Multi-Hypothesis Approach to Color Constancy", "comments": "Accepted for publication at CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary approaches frame the color constancy problem as learning camera\nspecific illuminant mappings. While high accuracy can be achieved on camera\nspecific data, these models depend on camera spectral sensitivity and typically\nexhibit poor generalisation to new devices. Additionally, regression methods\nproduce point estimates that do not explicitly account for potential\nambiguities among plausible illuminant solutions, due to the ill-posed nature\nof the problem. We propose a Bayesian framework that naturally handles color\nconstancy ambiguity via a multi-hypothesis strategy. Firstly, we select a set\nof candidate scene illuminants in a data-driven fashion and apply them to a\ntarget image to generate of set of corrected images. Secondly, we estimate, for\neach corrected image, the likelihood of the light source being achromatic using\na camera-agnostic CNN. Finally, our method explicitly learns a final\nillumination estimate from the generated posterior probability distribution.\nOur likelihood estimator learns to answer a camera-agnostic question and thus\nenables effective multi-camera training by disentangling illuminant estimation\nfrom the supervised learning task. We extensively evaluate our proposed\napproach and additionally set a benchmark for novel sensor generalisation\nwithout re-training. Our method provides state-of-the-art accuracy on multiple\npublic datasets (up to 11% median angular error improvement) while maintaining\nreal-time execution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 18:05:16 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 15:07:43 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hernandez-Juarez", "Daniel", ""], ["Parisot", "Sarah", ""], ["Busam", "Benjamin", ""], ["Leonardis", "Ales", ""], ["Slabaugh", "Gregory", ""], ["McDonagh", "Steven", ""]]}, {"id": "2002.12913", "submitter": "SeugnJu Cho", "authors": "Seungju Cho, Tae Joon Jun, Mingu Kang, Daeyoung Kim", "title": "Applying Tensor Decomposition to image for Robustness against\n  Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays the deep learning technology is growing faster and shows dramatic\nperformance in computer vision areas. However, it turns out a deep learning\nbased model is highly vulnerable to some small perturbation called an\nadversarial attack. It can easily fool the deep learning model by adding small\nperturbations. On the other hand, tensor decomposition method widely uses for\ncompressing the tensor data, including data matrix, image, etc. In this paper,\nwe suggest combining tensor decomposition for defending the model against\nadversarial example. We verify this idea is simple and effective to resist\nadversarial attack. In addition, this method rarely degrades the original\nperformance of clean data. We experiment on MNIST, CIFAR10 and ImageNet data\nand show our method robust on state-of-the-art attack methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 18:30:22 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 14:28:41 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Cho", "Seungju", ""], ["Jun", "Tae Joon", ""], ["Kang", "Mingu", ""], ["Kim", "Daeyoung", ""]]}]