[{"id": "1402.0170", "submitter": "Shu Kong", "authors": "Shu Kong, Zhuolin Jiang, Qiang Yang", "title": "Collaborative Receptive Field Learning", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of object categorization in images is largely due to arbitrary\ntranslations and scales of the foreground objects. To attack this difficulty,\nwe propose a new approach called collaborative receptive field learning to\nextract specific receptive fields (RF's) or regions from multiple images, and\nthe selected RF's are supposed to focus on the foreground objects of a common\ncategory. To this end, we solve the problem by maximizing a submodular function\nover a similarity graph constructed by a pool of RF candidates. However,\nmeasuring pairwise distance of RF's for building the similarity graph is a\nnontrivial problem. Hence, we introduce a similarity metric called\npyramid-error distance (PED) to measure their pairwise distances through\nsumming up pyramid-like matching errors over a set of low-level features.\nBesides, in consistent with the proposed PED, we construct a simple\nnonparametric classifier for classification. Experimental results show that our\nmethod effectively discovers the foreground objects in images, and improves\nclassification performance.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2014 10:11:57 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Kong", "Shu", ""], ["Jiang", "Zhuolin", ""], ["Yang", "Qiang", ""]]}, {"id": "1402.0240", "submitter": "Stefanie Jegelka", "authors": "Stefanie Jegelka (MIT), Jeff Bilmes (University of Washington)", "title": "Graph Cuts with Interacting Edge Costs - Examples, Approximations, and\n  Algorithms", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an extension of the classical graph cut problem, wherein we replace\nthe modular (sum of edge weights) cost function by a submodular set function\ndefined over graph edges. Special cases of this problem have appeared in\ndifferent applications in signal processing, machine learning, and computer\nvision. In this paper, we connect these applications via the generic\nformulation of \"cooperative graph cuts\", for which we study complexity,\nalgorithms, and connections to polymatroidal network flows. Finally, we compare\nthe proposed algorithms empirically.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2014 20:03:19 GMT"}, {"version": "v2", "created": "Sat, 8 Mar 2014 19:53:39 GMT"}, {"version": "v3", "created": "Sat, 30 Aug 2014 17:14:42 GMT"}, {"version": "v4", "created": "Sat, 26 Mar 2016 22:52:52 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Jegelka", "Stefanie", "", "MIT"], ["Bilmes", "Jeff", "", "University of Washington"]]}, {"id": "1402.0289", "submitter": "Pranam Janney", "authors": "Pranam Janney and Glenn Geers", "title": "A Robust Framework for Moving-Object Detection and Vehicular Traffic\n  Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent machines require basic information such as moving-object\ndetection from videos in order to deduce higher-level semantic information. In\nthis paper, we propose a methodology that uses a texture measure to detect\nmoving objects in video. The methodology is computationally inexpensive,\nrequires minimal parameter fine-tuning and also is resilient to noise,\nillumination changes, dynamic background and low frame rate. Experimental\nresults show that performance of the proposed approach is higher than those of\nstate-of-the-art approaches. We also present a framework for vehicular traffic\ndensity estimation using the foreground object detection technique and present\na comparison between the foreground object detection-based framework and the\nclassical density state modelling-based framework for vehicular traffic density\nestimation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 06:17:57 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Janney", "Pranam", ""], ["Geers", "Glenn", ""]]}, {"id": "1402.0453", "submitter": "Qi Qian", "authors": "Qi Qian, Rong Jin, Shenghuo Zhu and Yuanqing Lin", "title": "Fine-Grained Visual Categorization via Multi-stage Metric Learning", "comments": "in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual categorization (FGVC) is to categorize objects into\nsubordinate classes instead of basic classes. One major challenge in FGVC is\nthe co-occurrence of two issues: 1) many subordinate classes are highly\ncorrelated and are difficult to distinguish, and 2) there exists the large\nintra-class variation (e.g., due to object pose). This paper proposes to\nexplicitly address the above two issues via distance metric learning (DML). DML\naddresses the first issue by learning an embedding so that data points from the\nsame class will be pulled together while those from different classes should be\npushed apart from each other; and it addresses the second issue by allowing the\nflexibility that only a portion of the neighbors (not all data points) from the\nsame class need to be pulled together. However, feature representation of an\nimage is often high dimensional, and DML is known to have difficulty in dealing\nwith high dimensional feature vectors since it would require $\\mathcal{O}(d^2)$\nfor storage and $\\mathcal{O}(d^3)$ for optimization. To this end, we proposed a\nmulti-stage metric learning framework that divides the large-scale high\ndimensional learning problem to a series of simple subproblems, achieving\n$\\mathcal{O}(d)$ computational complexity. The empirical study with FVGC\nbenchmark datasets verifies that our method is both effective and efficient\ncompared to the state-of-the-art FGVC approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 18:20:53 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 17:28:51 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Qian", "Qi", ""], ["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1402.0595", "submitter": "Mojtaba Seyedhosseini", "authors": "Mojtaba Seyedhosseini and Tolga Tasdizen", "title": "Scene Labeling with Contextual Hierarchical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene labeling is the problem of assigning an object label to each pixel. It\nunifies the image segmentation and object recognition problems. The importance\nof using contextual information in scene labeling frameworks has been widely\nrealized in the field. We propose a contextual framework, called contextual\nhierarchical model (CHM), which learns contextual information in a hierarchical\nframework for scene labeling. At each level of the hierarchy, a classifier is\ntrained based on downsampled input images and outputs of previous levels. Our\nmodel then incorporates the resulting multi-resolution contextual information\ninto a classifier to segment the input image at original resolution. This\ntraining strategy allows for optimization of a joint posterior probability at\nmultiple resolutions through the hierarchy. Contextual hierarchical model is\npurely based on the input image patches and does not make use of any fragments\nor shape examples. Hence, it is applicable to a variety of problems such as\nobject segmentation and edge detection. We demonstrate that CHM outperforms\nstate-of-the-art on Stanford background and Weizmann horse datasets. It also\noutperforms state-of-the-art edge detection methods on NYU depth dataset and\nachieves state-of-the-art on Berkeley segmentation dataset (BSDS 500).\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 02:10:01 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Seyedhosseini", "Mojtaba", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1402.0785", "submitter": "Hong Jiang", "authors": "Hong Jiang, Gang Huang and Paul Wilford", "title": "Signal to Noise Ratio in Lensless Compressive Imaging", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the signal to noise ratio (SNR) in a lensless compressive imaging\n(LCI) architecture. The architecture consists of a sensor of a single detecting\nelement and an aperture assembly of an array of programmable elements. LCI can\nbe used in conjunction with compressive sensing to capture images in a\ncompressed form of compressive measurements. In this paper, we perform SNR\nanalysis of the LCI and compare it with imaging with a pinhole or a lens. We\nwill show that the SNR in the LCI is independent of the image resolution, while\nthe SNR in either pinhole aperture imaging or lens aperture imaging decreases\nas the image resolution increases. Consequently, the SNR in the LCI is much\nhigher if the image resolution is large enough.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 16:12:53 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Jiang", "Hong", ""], ["Huang", "Gang", ""], ["Wilford", "Paul", ""]]}, {"id": "1402.0859", "submitter": "Varun Jampani", "authors": "Varun Jampani and Sebastian Nowozin and Matthew Loper and Peter V.\n  Gehler", "title": "The Informed Sampler: A Discriminative Approach to Bayesian Inference in\n  Generative Computer Vision Models", "comments": "Appearing in Computer Vision and Image Understanding Journal (Special\n  Issue on Generative Models in Computer Vision)", "journal-ref": null, "doi": "10.1016/j.cviu.2015.03.002", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is hard because of a large variability in lighting, shape,\nand texture; in addition the image signal is non-additive due to occlusion.\nGenerative models promised to account for this variability by accurately\nmodelling the image formation process as a function of latent variables with\nprior beliefs. Bayesian posterior inference could then, in principle, explain\nthe observation. While intuitively appealing, generative models for computer\nvision have largely failed to deliver on that promise due to the difficulty of\nposterior inference. As a result the community has favoured efficient\ndiscriminative approaches. We still believe in the usefulness of generative\nmodels in computer vision, but argue that we need to leverage existing\ndiscriminative or even heuristic computer vision methods. We implement this\nidea in a principled way with an \"informed sampler\" and in careful experiments\ndemonstrate it on challenging generative models which contain renderer programs\nas their components. We concentrate on the problem of inverting an existing\ngraphics rendering engine, an approach that can be understood as \"Inverse\nGraphics\". The informed sampler, using simple discriminative proposals based on\nexisting computer vision technology, achieves significant improvements of\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 20:52:26 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 11:28:13 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2015 19:50:59 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Jampani", "Varun", ""], ["Nowozin", "Sebastian", ""], ["Loper", "Matthew", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1402.0936", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie, Zeyun Yu", "title": "An Optimization Method For Slice Interpolation Of Medical Images", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slice interpolation is a fast growing field in medical image processing.\nIntensity-based interpolation and object-based interpolation are two major\ngroups of methods in the literature. In this paper, we describe an\nobject-oriented, optimization method based on a modified version of\ncurvature-based image registration, in which a displacement field is computed\nfor the missing slice between two known slices and used to interpolate the\nintensities of the missing slice. The proposed approach is evaluated\nquantitatively by using the Mean Squared Difference (MSD) as a metric. The\nproduced results also show visual improvement in preserving sharp edges in\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 05:31:59 GMT"}, {"version": "v2", "created": "Thu, 27 Mar 2014 07:12:28 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Baghaie", "Ahmadreza", ""], ["Yu", "Zeyun", ""]]}, {"id": "1402.0978", "submitter": "Ali Zarezade", "authors": "Ali Zarezade, Hamid R. Rabiee, Ali Soltani-Farani, Ahmad Khajenezhad", "title": "Patchwise Joint Sparse Tracking with Occlusion Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2346029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust tracking approach to handle challenges such as\nocclusion and appearance change. Here, the target is partitioned into a number\nof patches. Then, the appearance of each patch is modeled using a dictionary\ncomposed of corresponding target patches in previous frames. In each frame, the\ntarget is found among a set of candidates generated by a particle filter, via a\nlikelihood measure that is shown to be proportional to the sum of\npatch-reconstruction errors of each candidate. Since the target's appearance\noften changes slowly in a video sequence, it is assumed that the target in the\ncurrent frame and the best candidates of a small number of previous frames,\nbelong to a common subspace. This is imposed using joint sparse representation\nto enforce the target and previous best candidates to have a common sparsity\npattern. Moreover, an occlusion detection scheme is proposed that uses\npatch-reconstruction errors and a prior probability of occlusion, extracted\nfrom an adaptive Markov chain, to calculate the probability of occlusion per\npatch. In each frame, occluded patches are excluded when updating the\ndictionary. Extensive experimental results on several challenging sequences\nshows that the proposed method outperforms state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 09:08:11 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Zarezade", "Ali", ""], ["Rabiee", "Hamid R.", ""], ["Soltani-Farani", "Ali", ""], ["Khajenezhad", "Ahmad", ""]]}, {"id": "1402.1151", "submitter": "Wojciech Biega\\'nski", "authors": "Wojciech Biega\\'nski and Andrzej Kasi\\'nski", "title": "Image Acquisition in an Underwater Vision System with NIR and VIS\n  Illumination", "comments": null, "journal-ref": "Computer Science & Information Technology, Volume 4, Number 1,\n  2014, pp. 215-224", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes the image acquisition system able to capture images in\ntwo separated bands of light, used to underwater autonomous navigation. The\nchannels are: the visible light spectrum and near infrared spectrum. The\ncharacteristics of natural, underwater environment were also described together\nwith the process of the underwater image creation. The results of an experiment\nwith comparison of selected images acquired in these channels are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 20:18:26 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Biega\u0144ski", "Wojciech", ""], ["Kasi\u0144ski", "Andrzej", ""]]}, {"id": "1402.1331", "submitter": "Abhishek Bhattacharya", "authors": "Abhishek Bhattacharya, Tanusree Chatterjee", "title": "An Estimation Method of Measuring Image Quality for Compressed Images of\n  Human Face", "comments": "6 pages", "journal-ref": null, "doi": "10.14445/22312803/IJCTT-V7P144", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays digital image compression and decompression techniques are very much\nimportant. So our aim is to calculate the quality of face and other regions of\nthe compressed image with respect to the original image. Image segmentation is\ntypically used to locate objects and boundaries (lines, curves etc.)in images.\nAfter segmentation the image is changed into something which is more meaningful\nto analyze. Using Universal Image Quality Index(Q),Structural Similarity\nIndex(SSIM) and Gradient-based Structural Similarity Index(G-SSIM) it can be\nshown that face region is less compressed than any other region of the image.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 11:58:42 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Bhattacharya", "Abhishek", ""], ["Chatterjee", "Tanusree", ""]]}, {"id": "1402.1348", "submitter": "Deepak Nayak Ranjan", "authors": "Deepak Ranjan Nayak, Sumit Kumar Sahu and Jahangir Mohammed", "title": "A Cellular Automata based Optimal Edge Detection Technique using\n  Twenty-Five Neighborhood Model", "comments": "7 pages, 9 figures", "journal-ref": "International Journal of Computer Applications, Volume 84, Number\n  10, Year of Publication: 2013", "doi": "10.5120/14614-2869", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular Automata (CA) are common and most simple models of parallel\ncomputations. Edge detection is one of the crucial task in image processing,\nespecially in processing biological and medical images. CA can be successfully\napplied in image processing. This paper presents a new method for edge\ndetection of binary images based on two dimensional twenty five neighborhood\ncellular automata. The method considers only linear rules of CA for extraction\nof edges under null boundary condition. The performance of this approach is\ncompared with some existing edge detection techniques. This comparison shows\nthat the proposed method to be very promising for edge detection of binary\nimages. All the algorithms and results used in this paper are prepared in\nMATLAB.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 13:32:39 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Nayak", "Deepak Ranjan", ""], ["Sahu", "Sumit Kumar", ""], ["Mohammed", "Jahangir", ""]]}, {"id": "1402.1359", "submitter": "Kai Berger", "authors": "Kai Berger and Jeyarajan Thiyagalingam", "title": "Real-time Pedestrian Surveillance with Top View Cumulative Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript presents an efficient approach to map pedestrian surveillance\nfootage to an aerial view for global assessment of features. The analysis of\nthe footages relies on low level computer vision and enable real-time\nsurveillance. While we neglect object tracking, we introduce cumulative grids\non top view scene flow visualization to highlight situations of interest in the\nfootage. Our approach is tested on multiview footage both from RGB cameras and,\nfor the first time in the field, on RGB-D-sensors.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 14:09:25 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Berger", "Kai", ""], ["Thiyagalingam", "Jeyarajan", ""]]}, {"id": "1402.1371", "submitter": "Veronika Cheplygina", "authors": "David M. J. Tax, Veronika Cheplygina, Marco Loog", "title": "Quantile Representation for Indirect Immunofluorescence Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the diagnosis of autoimmune diseases, an important task is to classify\nimages of slides containing several HEp-2 cells. All cells from one slide share\nthe same label, and by classifying cells from one slide independently, some\ninformation on the global image quality and intensity is lost. Considering one\nwhole slide as a collection (a bag) of feature vectors, however, poses the\nproblem of how to handle this bag. A simple, and surprisingly effective,\napproach is to summarize the bag of feature vectors by a few quantile values\nper feature. This characterizes the full distribution of all instances, thereby\nassuming that all instances in a bag are informative. This representation is\nparticularly useful when each bag contains many feature vectors, which is the\ncase in the classification of the immunofluorescence images. Experiments on the\nclassification of indirect immunofluorescence images show the usefulness of\nthis approach.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 14:56:55 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Tax", "David M. J.", ""], ["Cheplygina", "Veronika", ""], ["Loog", "Marco", ""]]}, {"id": "1402.1473", "submitter": "Yuxin Chen", "authors": "Yuxin Chen and Leonidas J. Guibas and Qi-Xing Huang", "title": "Near-Optimal Joint Object Matching via Convex Relaxation", "comments": null, "journal-ref": "31st International Conference on Machine Learning, vol. 32, pp.\n  100 - 108, June 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint matching over a collection of objects aims at aggregating information\nfrom a large collection of similar instances (e.g. images, graphs, shapes) to\nimprove maps between pairs of them. Given multiple matches computed between a\nfew object pairs in isolation, the goal is to recover an entire collection of\nmaps that are (1) globally consistent, and (2) close to the provided maps ---\nand under certain conditions provably the ground-truth maps. Despite recent\nadvances on this problem, the best-known recovery guarantees are limited to a\nsmall constant barrier --- none of the existing methods find theoretical\nsupport when more than $50\\%$ of input correspondences are corrupted. Moreover,\nprior approaches focus mostly on fully similar objects, while it is practically\nmore demanding to match instances that are only partially similar to each\nother.\n  In this paper, we develop an algorithm to jointly match multiple objects that\nexhibit only partial similarities, given a few pairwise matches that are\ndensely corrupted. Specifically, we propose to recover the ground-truth maps\nvia a parameter-free convex program called MatchLift, following a spectral\nmethod that pre-estimates the total number of distinct elements to be matched.\nEncouragingly, MatchLift exhibits near-optimal error-correction ability, i.e.\nin the asymptotic regime it is guaranteed to work even when a dominant fraction\n$1-\\Theta\\left(\\frac{\\log^{2}n}{\\sqrt{n}}\\right)$ of the input maps behave like\nrandom outliers. Furthermore, MatchLift succeeds with minimal input complexity,\nnamely, perfect matching can be achieved as soon as the provided maps form a\nconnected map graph. We evaluate the proposed algorithm on various benchmark\ndata sets including synthetic examples and real-world examples, all of which\nconfirm the practical applicability of MatchLift.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 20:16:35 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Chen", "Yuxin", ""], ["Guibas", "Leonidas J.", ""], ["Huang", "Qi-Xing", ""]]}, {"id": "1402.1503", "submitter": "Ganesh Sundaramoorthi", "authors": "Omar Arif, Ganesh Sundaramoorthi, Byung-Woo Hong, Anthony Yezzi", "title": "Tracking via Motion Estimation with Physically Motivated Inter-Region\n  Constraints", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for tracking structures (e.g., ventricles\nand myocardium) in cardiac images (e.g., magnetic resonance) by propagating\nforward in time a previous estimate of the structures via a new deformation\nestimation scheme that is motivated by physical constraints of fluid motion.\nThe method employs within structure motion estimation (so that differing\nmotions among different structures are not mixed) while simultaneously\nsatisfying the physical constraint in fluid motion that at the interface\nbetween a fluid and a medium, the normal component of the fluid's motion must\nmatch the normal component of the motion of the medium. We show how to estimate\nthe motion according to the previous considerations in a variational framework,\nand in particular, show that these conditions lead to PDEs with boundary\nconditions at the interface that resemble Robin boundary conditions and induce\ncoupling between structures. We illustrate the use of this motion estimation\nscheme in propagating a segmentation across frames and show that it leads to\nmore accurate segmentation than traditional motion estimation that does not\nmake use of physical constraints. Further, the method is naturally suited to\ninteractive segmentation methods, which are prominently used in practice in\ncommercial applications for cardiac analysis, where typically a segmentation\nfrom the previous frame is used to predict a segmentation in the next frame. We\nshow that our propagation scheme reduces the amount of user interaction by\npredicting more accurate segmentations than commonly used and recent\ninteractive commercial techniques.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 21:27:25 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Arif", "Omar", ""], ["Sundaramoorthi", "Ganesh", ""], ["Hong", "Byung-Woo", ""], ["Yezzi", "Anthony", ""]]}, {"id": "1402.1720", "submitter": "Yair Censor", "authors": "Blake Schultze, Micah Witt, Yair Censor, Reinhard Schulte, and Keith\n  Evan Schubert", "title": "Performance of Hull-Detection Algorithms For Proton Computed Tomography\n  Reconstruction", "comments": "Contemporary Mathematics, accepted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proton computed tomography (pCT) is a novel imaging modality developed for\npatients receiving proton radiation therapy. The purpose of this work was to\ninvestigate hull-detection algorithms used for preconditioning of the large and\nsparse linear system of equations that needs to be solved for pCT image\nreconstruction. The hull-detection algorithms investigated here included\nsilhouette/space carving (SC), modified silhouette/space carving (MSC), and\nspace modeling (SM). Each was compared to the cone-beam version of filtered\nbackprojection (FBP) used for hull-detection. Data for testing these algorithms\nincluded simulated data sets of a digital head phantom and an experimental data\nset of a pediatric head phantom obtained with a pCT scanner prototype at Loma\nLinda University Medical Center. SC was the fastest algorithm, exceeding the\nspeed of FBP by more than 100 times. FBP was most sensitive to the presence of\nnoise. Ongoing work will focus on optimizing threshold parameters in order to\ndefine a fast and efficient method for hull-detection in pCT image\nreconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 17:58:16 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Schultze", "Blake", ""], ["Witt", "Micah", ""], ["Censor", "Yair", ""], ["Schulte", "Reinhard", ""], ["Schubert", "Keith Evan", ""]]}, {"id": "1402.1783", "submitter": "Jason J Corso", "authors": "Caiming Xiong, David Johnson, Jason J. Corso", "title": "Active Clustering with Model-Based Uncertainty Reduction", "comments": "14 pages, 8 figures, submitted to TPAMI (second version just fixes a\n  missing reference and format)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised clustering seeks to augment traditional clustering methods by\nincorporating side information provided via human expertise in order to\nincrease the semantic meaningfulness of the resulting clusters. However, most\ncurrent methods are \\emph{passive} in the sense that the side information is\nprovided beforehand and selected randomly. This may require a large number of\nconstraints, some of which could be redundant, unnecessary, or even detrimental\nto the clustering results. Thus in order to scale such semi-supervised\nalgorithms to larger problems it is desirable to pursue an \\emph{active}\nclustering method---i.e. an algorithm that maximizes the effectiveness of the\navailable human labor by only requesting human input where it will have the\ngreatest impact. Here, we propose a novel online framework for active\nsemi-supervised spectral clustering that selects pairwise constraints as\nclustering proceeds, based on the principle of uncertainty reduction. Using a\nfirst-order Taylor expansion, we decompose the expected uncertainty reduction\nproblem into a gradient and a step-scale, computed via an application of matrix\nperturbation theory and cluster-assignment entropy, respectively. The resulting\nmodel is used to estimate the uncertainty reduction potential of each sample in\nthe dataset. We then present the human user with pairwise queries with respect\nto only the best candidate sample. We evaluate our method using three different\nimage datasets (faces, leaves and dogs), a set of common UCI machine learning\ndatasets and a gene dataset. The results validate our decomposition formulation\nand show that our method is consistently superior to existing state-of-the-art\ntechniques, as well as being robust to noise and to unknown numbers of\nclusters.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 22:13:03 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2014 02:53:32 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Xiong", "Caiming", ""], ["Johnson", "David", ""], ["Corso", "Jason J.", ""]]}, {"id": "1402.1801", "submitter": "Sayedmasoud Hashemi Amroabadi", "authors": "SayedMasoud Hashemi, Soosan Beheshti, Patrick R. Gill, Narinder S.\n  Paul, Richard S.C. Cobbold", "title": "Efficient Low Dose X-ray CT Reconstruction through Sparsity-Based MAP\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra low radiation dose in X-ray Computed Tomography (CT) is an important\nclinical objective in order to minimize the risk of carcinogenesis. Compressed\nSensing (CS) enables significant reductions in radiation dose to be achieved by\nproducing diagnostic images from a limited number of CT projections. However,\nthe excessive computation time that conventional CS-based CT reconstruction\ntypically requires has limited clinical implementation. In this paper, we first\ndemonstrate that a thorough analysis of CT reconstruction through a Maximum a\nPosteriori objective function results in a weighted compressive sensing\nproblem. This analysis enables us to formulate a low dose fan beam and helical\ncone beam CT reconstruction. Subsequently, we provide an efficient solution to\nthe formulated CS problem based on a Fast Composite Splitting Algorithm-Latent\nExpected Maximization (FCSA-LEM) algorithm. In the proposed method we use\npseudo polar Fourier transform as the measurement matrix in order to decrease\nthe computational complexity; and rebinning of the projections to parallel rays\nin order to extend its application to fan beam and helical cone beam scans. The\nweight involved in the proposed weighted CS model, denoted by Error Adaptation\nWeight (EAW), is calculated based on the statistical characteristics of CT\nreconstruction and is a function of Poisson measurement noise and rebinning\ninterpolation error. Simulation results show that low computational complexity\nof the proposed method made the fast recovery of the CT images possible and\nusing EAW reduces the reconstruction error by one order of magnitude. Recovery\nof a high quality 512$\\times$ 512 image was achieved in less than 20 sec on a\ndesktop computer without numerical optimizations.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 00:18:46 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Hashemi", "SayedMasoud", ""], ["Beheshti", "Soosan", ""], ["Gill", "Patrick R.", ""], ["Paul", "Narinder S.", ""], ["Cobbold", "Richard S. C.", ""]]}, {"id": "1402.1879", "submitter": "Allen Yang", "authors": "Liansheng Zhuang, Tsung-Han Chan, Allen Y. Yang, S. Shankar Sastry, Yi\n  Ma", "title": "Sparse Illumination Learning and Transfer for Single-Sample Face\n  Recognition with Image Corruption and Misalignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-sample face recognition is one of the most challenging problems in\nface recognition. We propose a novel algorithm to address this problem based on\na sparse representation based classification (SRC) framework. The new algorithm\nis robust to image misalignment and pixel corruption, and is able to reduce\nrequired gallery images to one sample per class. To compensate for the missing\nillumination information traditionally provided by multiple gallery images, a\nsparse illumination learning and transfer (SILT) technique is introduced. The\nillumination in SILT is learned by fitting illumination examples of auxiliary\nface images from one or more additional subjects with a sparsely-used\nillumination dictionary. By enforcing a sparse representation of the query\nimage in the illumination dictionary, the SILT can effectively recover and\ntransfer the illumination and pose information from the alignment stage to the\nrecognition stage. Our extensive experiments have demonstrated that the new\nalgorithms significantly outperform the state of the art in the single-sample\nregime and with less restrictions. In particular, the single-sample face\nalignment accuracy is comparable to that of the well-known Deformable SRC\nalgorithm using multiple gallery images per class. Furthermore, the face\nrecognition accuracy exceeds those of the SRC and Extended SRC algorithms using\nhand labeled alignment initialization.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 18:46:28 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Zhuang", "Liansheng", ""], ["Chan", "Tsung-Han", ""], ["Yang", "Allen Y.", ""], ["Sastry", "S. Shankar", ""], ["Ma", "Yi", ""]]}, {"id": "1402.1921", "submitter": "Zhenhua Wang", "authors": "Qinfeng Shi, Mark Reid, Tiberio Caetano, Anton van den Hengel and\n  Zhenhua Wang", "title": "A Hybrid Loss for Multiclass and Structured Prediction", "comments": "12 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1009.3346", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hybrid loss for multiclass and structured prediction\nproblems that is a convex combination of a log loss for Conditional Random\nFields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs).\nWe provide a sufficient condition for when the hybrid loss is Fisher consistent\nfor classification. This condition depends on a measure of dominance between\nlabels--specifically, the gap between the probabilities of the best label and\nthe second best label. We also prove Fisher consistency is necessary for\nparametric consistency when learning models such as CRFs. We demonstrate\nempirically that the hybrid loss typically performs least as well as--and often\nbetter than--both of its constituent losses on a variety of tasks, such as\nhuman action recognition. In doing so we also provide an empirical comparison\nof the efficacy of probabilistic and margin based approaches to multiclass and\nstructured prediction.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 06:47:17 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Shi", "Qinfeng", ""], ["Reid", "Mark", ""], ["Caetano", "Tiberio", ""], ["Hengel", "Anton van den", ""], ["Wang", "Zhenhua", ""]]}, {"id": "1402.1947", "submitter": "Farrukh Arslan", "authors": "Farrukh Arslan", "title": "Classification Tree Diagrams in Health Informatics Applications", "comments": "In the Proceedings of 7th International Conference on the Theory and\n  Application of Diagrams 2012. 7th International Conference on the Theory and\n  Application of Diagrams 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health informatics deal with the methods used to optimize the acquisition,\nstorage and retrieval of medical data, and classify information in healthcare\napplications. Healthcare analysts are particularly interested in various\ncomputer informatics areas such as; knowledge representation from data, anomaly\ndetection, outbreak detection methods and syndromic surveillance applications.\nAlthough various parametric and non-parametric approaches are being proposed to\nclassify information from data, classification tree diagrams provide an\ninteractive visualization to analysts as compared to other methods. In this\nwork we discuss application of classification tree diagrams to classify\ninformation from medical data in healthcare applications.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 13:02:51 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Arslan", "Farrukh", ""]]}, {"id": "1402.1971", "submitter": "Mohammed  Javed", "authors": "Mohammed Javed and P. Nagabhushan and B.B. Chaudhuri", "title": "Direct Processing of Run Length Compressed Document Image for\n  Segmentation and Characterization of a Specified Block", "comments": "7 Pages and Published with International Journal of Computer\n  Applications (IJCA)", "journal-ref": "IJCA 83(15):1-6, December 2013", "doi": "10.5120/14521-2926", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting a block of interest referred to as segmenting a specified block in\nan image and studying its characteristics is of general research interest, and\ncould be a challenging if such a segmentation task has to be carried out\ndirectly in a compressed image. This is the objective of the present research\nwork. The proposal is to evolve a method which would segment and extract a\nspecified block, and carry out its characterization without decompressing a\ncompressed image, for two major reasons that most of the image archives contain\nimages in compressed format and decompressing an image indents additional\ncomputing time and space. Specifically in this research work, the proposal is\nto work on run-length compressed document images.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 18:01:12 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 16:32:16 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Javed", "Mohammed", ""], ["Nagabhushan", "P.", ""], ["Chaudhuri", "B. B.", ""]]}, {"id": "1402.1973", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Mike Davies, Pascal Frossard", "title": "Dictionary learning for fast classification based on soft-thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers based on sparse representations have recently been shown to\nprovide excellent results in many visual recognition and classification tasks.\nHowever, the high cost of computing sparse representations at test time is a\nmajor obstacle that limits the applicability of these methods in large-scale\nproblems, or in scenarios where computational power is restricted. We consider\nin this paper a simple yet efficient alternative to sparse coding for feature\nextraction. We study a classification scheme that applies the soft-thresholding\nnonlinear mapping in a dictionary, followed by a linear classifier. A novel\nsupervised dictionary learning algorithm tailored for this low complexity\nclassification architecture is proposed. The dictionary learning problem, which\njointly learns the dictionary and linear classifier, is cast as a difference of\nconvex (DC) program and solved efficiently with an iterative DC solver. We\nconduct experiments on several datasets, and show that our learning algorithm\nthat leverages the structure of the classification problem outperforms generic\nlearning procedures. Our simple classifier based on soft-thresholding also\ncompetes with the recent sparse coding classifiers, when the dictionary is\nlearned appropriately. The adopted classification scheme further requires less\ncomputational time at the testing stage, compared to other classifiers. The\nproposed scheme shows the potential of the adequately trained soft-thresholding\nmapping for classification and paves the way towards the development of very\nefficient classification methods for vision problems.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 18:18:33 GMT"}, {"version": "v2", "created": "Thu, 2 Oct 2014 16:45:19 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Davies", "Mike", ""], ["Frossard", "Pascal", ""]]}, {"id": "1402.2013", "submitter": "Xiaohan Liu", "authors": "Xintong Yu, Xiaohan Liu, Yisong Chen", "title": "Foreground segmentation based on multi-resolution and matting", "comments": "5 pages. 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a foreground segmentation algorithm that does foreground\nextraction under different scales and refines the result by matting. First, the\ninput image is filtered and resampled to 5 different resolutions. Then each of\nthem is segmented by adaptive figure-ground classification and the best\nsegmentation is automatically selected by an evaluation score that maximizes\nthe difference between foreground and background. This segmentation is\nupsampled to the original size, and a corresponding trimap is built.\nClosed-form matting is employed to label the boundary region, and the result is\nrefined by a final figure-ground classification. Experiments show the success\nof our method in treating challenging images with cluttered background and\nadapting to loose initial bounding-box.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 01:22:35 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Yu", "Xintong", ""], ["Liu", "Xiaohan", ""], ["Chen", "Yisong", ""]]}, {"id": "1402.2016", "submitter": "Wenxi Liu", "authors": "Wenxi Liu, Antoni B. Chan, Rynson W. H. Lau, Dinesh Manocha", "title": "Leveraging Long-Term Predictions and Online-Learning in Agent-based\n  Multiple Person Tracking", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2014.2344511", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multiple-person tracking algorithm, based on combining particle\nfilters and RVO, an agent-based crowd model that infers collision-free\nvelocities so as to predict pedestrian's motion. In addition to position and\nvelocity, our tracking algorithm can estimate the internal goals (desired\ndestination or desired velocity) of the tracked pedestrian in an online manner,\nthus removing the need to specify this information beforehand. Furthermore, we\nleverage the longer-term predictions of RVO by deriving a higher-order particle\nfilter, which aggregates multiple predictions from different prior time steps.\nThis yields a tracker that can recover from short-term occlusions and spurious\nnoise in the appearance model. Experimental results show that our tracking\nalgorithm is suitable for predicting pedestrians' behaviors online without\nneeding scene priors or hand-annotated goal information, and improves tracking\nin real-world crowded scenes under low frame rates.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 02:07:07 GMT"}, {"version": "v2", "created": "Fri, 7 Mar 2014 16:44:38 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Wenxi", ""], ["Chan", "Antoni B.", ""], ["Lau", "Rynson W. H.", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1402.2020", "submitter": "Kang Zhang", "authors": "Kang Zhang, Jiyang Li, Yijing Li, Weidong Hu, Lifeng Sun, Shiqiang\n  Yang", "title": "Binary Stereo Matching", "comments": "Pattern Recognition (ICPR), 2012 21st International Conference on", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel binary-based cost computation and\naggregation approach for stereo matching problem. The cost volume is\nconstructed through bitwise operations on a series of binary strings. Then this\napproach is combined with traditional winner-take-all strategy, resulting in a\nnew local stereo matching algorithm called binary stereo matching (BSM). Since\ncore algorithm of BSM is based on binary and integer computations, it has a\nhigher computational efficiency than previous methods. Experimental results on\nMiddlebury benchmark show that BSM has comparable performance with\nstate-of-the-art local stereo methods in terms of both quality and speed.\nFurthermore, experiments on images with radiometric differences demonstrate\nthat BSM is more robust than previous methods under these changes, which is\ncommon under real illumination.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 02:33:39 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Zhang", "Kang", ""], ["Li", "Jiyang", ""], ["Li", "Yijing", ""], ["Hu", "Weidong", ""], ["Sun", "Lifeng", ""], ["Yang", "Shiqiang", ""]]}, {"id": "1402.2031", "submitter": "Hong Chang", "authors": "Wen Wang, Zhen Cui, Hong Chang, Shiguang Shan, Xilin Chen", "title": "Deeply Coupled Auto-encoder Networks for Cross-view Classification", "comments": "11 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The comparison of heterogeneous samples extensively exists in many\napplications, especially in the task of image classification. In this paper, we\npropose a simple but effective coupled neural network, called Deeply Coupled\nAutoencoder Networks (DCAN), which seeks to build two deep neural networks,\ncoupled with each other in every corresponding layers. In DCAN, each deep\nstructure is developed via stacking multiple discriminative coupled\nauto-encoders, a denoising auto-encoder trained with maximum margin criterion\nconsisting of intra-class compactness and inter-class penalty. This single\nlayer component makes our model simultaneously preserve the local consistency\nand enhance its discriminative capability. With increasing number of layers,\nthe coupled networks can gradually narrow the gap between the two views.\nExtensive experiments on cross-view image classification tasks demonstrate the\nsuperiority of our method over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 04:15:23 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Wang", "Wen", ""], ["Cui", "Zhen", ""], ["Chang", "Hong", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1402.2088", "submitter": "Mohammad Tofighi", "authors": "Mohammad Tofighi, Kivanc Kose, A. Enis Cetin", "title": "Signal Reconstruction Framework Based On Projections Onto Epigraph Set\n  Of A Convex Cost Function (PESC)", "comments": "Submitted to IEEE Transactions on Image Processing on 7th Jan 2014.\n  arXiv admin note: substantial text overlap with arXiv:1309.0700,\n  arXiv:1306.2516", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new signal processing framework based on making orthogonal Projections onto\nthe Epigraph Set of a Convex cost function (PESC) is developed. In this way it\nis possible to solve convex optimization problems using the well-known\nProjections onto Convex Set (POCS) approach. In this algorithm, the dimension\nof the minimization problem is lifted by one and a convex set corresponding to\nthe epigraph of the cost function is defined. If the cost function is a convex\nfunction in $R^N$, the corresponding epigraph set is also a convex set in\nR^{N+1}. The PESC method provides globally optimal solutions for\ntotal-variation (TV), filtered variation (FV), L_1, L_2, and entropic cost\nfunction based convex optimization problems. In this article, the PESC based\ndenoising and compressive sensing algorithms are developed. Simulation examples\nare presented.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 10:25:23 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Tofighi", "Mohammad", ""], ["Kose", "Kivanc", ""], ["Cetin", "A. Enis", ""]]}, {"id": "1402.2188", "submitter": "Anitha Chacko Mary M.O.", "authors": "Anitha Mary M.O. Chacko, P.M Dhanya", "title": "Handwritten Character Recognition In Malayalam Scripts- A Review", "comments": "11 pages,4 figures,2 tables", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol. 5, No. 1, January 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten character recognition is one of the most challenging and ongoing\nareas of research in the field of pattern recognition. HCR research is matured\nfor foreign languages like Chinese and Japanese but the problem is much more\ncomplex for Indian languages. The problem becomes even more complicated for\nSouth Indian languages due to its large character set and the presence of\nvowels modifiers and compound characters. This paper provides an overview of\nimportant contributions and advances in offline as well as online handwritten\ncharacter recognition of Malayalam scripts.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 15:41:48 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Chacko", "Anitha Mary M. O.", ""], ["Dhanya", "P. M", ""]]}, {"id": "1402.2232", "submitter": "Vipeen Bopche Vikas", "authors": "V Rajakumar, Vipeen V Bopche", "title": "Image Search Reranking", "comments": null, "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  6(41):5-6, December 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing methods for image search reranking suffer from the\nunfaithfulness of the assumptions under which the text-based images search\nresult. The resulting images contain more irrelevant images. Hence the re\nranking concept arises to re rank the retrieved images based on the text around\nthe image and data of data of image and visual feature of image. A number of\nmethods are differentiated for this re-ranking. The high ranked images are used\nas noisy data and a k means algorithm for classification is learned to rectify\nthe ranking further. We are study the affect ability of the cross validation\nmethod to this training data. The pre eminent originality of the overall method\nis in collecting text/metadata of image and visual features in order to achieve\nan automatic ranking of the images. Supervision is initiated to learn the model\nweights offline, previous to reranking process. While model learning needs\nmanual labeling of the results for a some limited queries, the resulting model\nis query autonomous and therefore applicable to any other query .Examples are\ngiven for a selection of other classes like vehicles, animals and other\nclasses.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 18:28:18 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Rajakumar", "V", ""], ["Bopche", "Vipeen V", ""]]}, {"id": "1402.2333", "submitter": "Vincent Michalski", "authors": "Vincent Michalski, Roland Memisevic, Kishore Konda", "title": "Modeling sequential data using higher-order relational features and\n  predictive training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bi-linear feature learning models, like the gated autoencoder, were proposed\nas a way to model relationships between frames in a video. By minimizing\nreconstruction error of one frame, given the previous frame, these models learn\n\"mapping units\" that encode the transformations inherent in a sequence, and\nthereby learn to encode motion. In this work we extend bi-linear models by\nintroducing \"higher-order mapping units\" that allow us to encode\ntransformations between frames and transformations between transformations.\n  We show that this makes it possible to encode temporal structure that is more\ncomplex and longer-range than the structure captured within standard bi-linear\nmodels. We also show that a natural way to train the model is by replacing the\ncommonly used reconstruction objective with a prediction objective which forces\nthe model to correctly predict the evolution of the input multiple steps into\nthe future. Learning can be achieved by back-propagating the multi-step\nprediction through time. We test the model on various temporal prediction\ntasks, and show that higher-order mappings and predictive training both yield a\nsignificant improvement over bi-linear models in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 23:53:29 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Michalski", "Vincent", ""], ["Memisevic", "Roland", ""], ["Konda", "Kishore", ""]]}, {"id": "1402.2335", "submitter": "Jason McEwen", "authors": "Rafael E. Carrillo, Jason D. McEwen, Yves Wiaux", "title": "Sparsity averaging for radio-interferometric imaging", "comments": "1 page, 1 figure, Proceedings of the Biomedical and Astronomical\n  Signal Processing Frontiers (BASP) workshop 2013, Related journal\n  publications available at http://arxiv.org/abs/arXiv:1208.2330 and\n  http://arxiv.org/abs/1307.4370", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel regularization method for compressive imaging in the\ncontext of the compressed sensing (CS) theory with coherent and redundant\ndictionaries. Natural images are often complicated and several types of\nstructures can be present at once. It is well known that piecewise smooth\nimages exhibit gradient sparsity, and that images with extended structures are\nbetter encapsulated in wavelet frames. Therefore, we here conjecture that\npromoting average sparsity or compressibility over multiple frames rather than\nsingle frames is an extremely powerful regularization prior.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 00:09:36 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Carrillo", "Rafael E.", ""], ["McEwen", "Jason D.", ""], ["Wiaux", "Yves", ""]]}, {"id": "1402.2363", "submitter": "Ashish Shingade ANS", "authors": "Ashish Shingade and Archana Ghotkar", "title": "Animation of 3D Human Model Using Markerless Motion Capture Applied To\n  Sports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markerless motion capture is an active research in 3D virtualization. In\nproposed work we presented a system for markerless motion capture for 3D human\ncharacter animation, paper presents a survey on motion and skeleton tracking\ntechniques which are developed or are under development. The paper proposed a\nmethod to transform the motion of a performer to a 3D human character (model),\nthe 3D human character performs similar movements as that of a performer in\nreal time. In the proposed work, human model data will be captured by Kinect\ncamera, processed data will be applied on 3D human model for animation. 3D\nhuman model is created using open source software (MakeHuman). Anticipated\ndataset for sport activity is considered as input which can be applied to any\nHCI application.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 04:05:12 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Shingade", "Ashish", ""], ["Ghotkar", "Archana", ""]]}, {"id": "1402.2426", "submitter": "Keith Dillon", "authors": "Keith Dillon and Yeshaiahu Fainman", "title": "Imaging with Rays: Microscopy, Medical Imaging, and Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we broadly consider techniques which utilize projections on\nrays for data collection, with particular emphasis on optical techniques. We\nformulate a variety of imaging techniques as either special cases or extensions\nof tomographic reconstruction. We then consider how the techniques must be\nextended to describe objects containing occlusion, as with a self-occluding\nopaque object. We formulate the reconstruction problem as a regularized\nnonlinear optimization problem to simultaneously solve for object brightness\nand attenuation, where the attenuation can become infinite. We demonstrate\nvarious simulated examples for imaging opaque objects, including sparse point\nsources, a conventional multiview reconstruction technique, and a\nsuper-resolving technique which exploits occlusion to resolve an image.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 10:26:31 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Dillon", "Keith", ""], ["Fainman", "Yeshaiahu", ""]]}, {"id": "1402.2606", "submitter": "Dibyendu Mukherjee", "authors": "Dibyendu Mukherjee", "title": "A Fast Two Pass Multi-Value Segmentation Algorithm based on Connected\n  Component Analysis", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected component analysis (CCA) has been heavily used to label binary\nimages and classify segments. However, it has not been well-exploited to\nsegment multi-valued natural images. This work proposes a novel multi-value\nsegmentation algorithm that utilizes CCA to segment color images. A user\ndefined distance measure is incorporated in the proposed modified CCA to\nidentify and segment similar image regions. The raw output of the algorithm\nconsists of distinctly labelled segmented regions. The proposed algorithm has a\nunique design architecture that provides several benefits: 1) it can be used to\nsegment any multi-channel multi-valued image; 2) the distance\nmeasure/segmentation criteria can be application-specific and 3) an absolute\nlinear-time implementation allows easy extension for real-time video\nsegmentation. Experimental demonstrations of the aforesaid benefits are\npresented along with the comparison results on multiple datasets with current\nbenchmark algorithms. A number of possible application areas are also\nidentified and results on real-time video segmentation has been presented to\nshow the promise of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 19:27:05 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Mukherjee", "Dibyendu", ""]]}, {"id": "1402.2673", "submitter": "Jakub Nalepa", "authors": "Jakub Nalepa and Michal Kawulok", "title": "Real-Time Hand Shape Classification", "comments": "11 pages", "journal-ref": null, "doi": "10.1007/978-3-319-06932-6_35", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of hand shape classification is challenging since a hand is\ncharacterized by a large number of degrees of freedom. Numerous shape\ndescriptors have been proposed and applied over the years to estimate and\nclassify hand poses in reasonable time. In this paper we discuss our parallel\nframework for real-time hand shape classification applicable in real-time\napplications. We show how the number of gallery images influences the\nclassification accuracy and execution time of the parallel algorithm. We\npresent the speedup and efficiency analyses that prove the efficacy of the\nparallel implementation. Noteworthy, different methods can be used at each step\nof our parallel framework. Here, we combine the shape contexts with the\nappearance-based techniques to enhance the robustness of the algorithm and to\nincrease the classification score. An extensive experimental study proves the\nsuperiority of the proposed approach over existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:32:48 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Nalepa", "Jakub", ""], ["Kawulok", "Michal", ""]]}, {"id": "1402.2681", "submitter": "Liang Zheng", "authors": "Liang Zheng, Shengjin Wang, Ziqiong Liu, Qi Tian", "title": "Packing and Padding: Coupled Multi-index for Accurate Image Retrieval", "comments": "8 pages, 7 figures, 6 tables. Accepted to CVPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In Bag-of-Words (BoW) based image retrieval, the SIFT visual word has a low\ndiscriminative power, so false positive matches occur prevalently. Apart from\nthe information loss during quantization, another cause is that the SIFT\nfeature only describes the local gradient distribution. To address this\nproblem, this paper proposes a coupled Multi-Index (c-MI) framework to perform\nfeature fusion at indexing level. Basically, complementary features are coupled\ninto a multi-dimensional inverted index. Each dimension of c-MI corresponds to\none kind of feature, and the retrieval process votes for images similar in both\nSIFT and other feature spaces. Specifically, we exploit the fusion of local\ncolor feature into c-MI. While the precision of visual match is greatly\nenhanced, we adopt Multiple Assignment to improve recall. The joint cooperation\nof SIFT and color features significantly reduces the impact of false positive\nmatches.\n  Extensive experiments on several benchmark datasets demonstrate that c-MI\nimproves the retrieval accuracy significantly, while consuming only half of the\nquery time compared to the baseline. Importantly, we show that c-MI is well\ncomplementary to many prior techniques. Assembling these methods, we have\nobtained an mAP of 85.8% and N-S score of 3.85 on Holidays and Ukbench\ndatasets, respectively, which compare favorably with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 22:00:31 GMT"}, {"version": "v2", "created": "Sun, 13 Apr 2014 09:51:54 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Zheng", "Liang", ""], ["Wang", "Shengjin", ""], ["Liu", "Ziqiong", ""], ["Tian", "Qi", ""]]}, {"id": "1402.2720", "submitter": "Hong Jiang", "authors": "Hong Jiang, Gang Huang and Paul Wilford", "title": "Noise Analysis for Lensless Compressive Imaging", "comments": "11 pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:1402.0785", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the signal to noise ratio (SNR) in a recently proposed lensless\ncompressive imaging architecture. The architecture consists of a sensor of a\nsingle detector element and an aperture assembly of an array of aperture\nelements, each of which has a programmable transmittance. This lensless\ncompressive imaging architecture can be used in conjunction with compressive\nsensing to capture images in a compressed form of compressive measurements. In\nthis paper, we perform noise analysis of this lensless compressive imaging\narchitecture and compare it with pinhole aperture imaging and lens aperture\nimaging. We will show that the SNR in the lensless compressive imaging is\nindependent of the image resolution, while that in either pinhole aperture\nimaging or lens aperture imaging decreases as the image resolution increases.\nConsequently, the SNR in the lensless compressive imaging can be much higher if\nthe image resolution is large enough.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 03:12:40 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Jiang", "Hong", ""], ["Huang", "Gang", ""], ["Wilford", "Paul", ""]]}, {"id": "1402.2826", "submitter": "Aniket Bera", "authors": "Aniket Bera and Dinesh Manocha", "title": "Realtime Multilevel Crowd Tracking using Reciprocal Velocity Obstacles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, realtime algorithm to compute the trajectory of each\npedestrian in moderately dense crowd scenes. Our formulation is based on an\nadaptive particle filtering scheme that uses a multi-agent motion model based\non velocity-obstacles, and takes into account local interactions as well as\nphysical and personal constraints of each pedestrian. Our method dynamically\nchanges the number of particles allocated to each pedestrian based on different\nconfidence metrics. Additionally, we use a new high-definition crowd video\ndataset, which is used to evaluate the performance of different pedestrian\ntracking algorithms. This dataset consists of videos of indoor and outdoor\nscenes, recorded at different locations with 30-80 pedestrians. We highlight\nthe performance benefits of our algorithm over prior techniques using this\ndataset. In practice, our algorithm can compute trajectories of tens of\npedestrians on a multi-core desktop CPU at interactive rates (27-30 frames per\nsecond). To the best of our knowledge, our approach is 4-5 times faster than\nprior methods, which provide similar accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 15:49:53 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1402.2941", "submitter": "Zohaib Khan", "authors": "Zohaib Khan, Faisal Shafait, Yiqun Hu, Ajmal Mian", "title": "Multispectral Palmprint Encoding and Recognition", "comments": "Preliminary version of this manuscript was published in ICCV 2011. Z.\n  Khan A. Mian and Y. Hu, \"Contour Code: Robust and Efficient Multispectral\n  Palmprint Encoding for Human Recognition\", International Conference on\n  Computer Vision, 2011. MATLAB Code available:\n  https://sites.google.com/site/zohaibnet/Home/codes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Palmprints are emerging as a new entity in multi-modal biometrics for human\nidentification and verification. Multispectral palmprint images captured in the\nvisible and infrared spectrum not only contain the wrinkles and ridge structure\nof a palm, but also the underlying pattern of veins; making them a highly\ndiscriminating biometric identifier. In this paper, we propose a feature\nencoding scheme for robust and highly accurate representation and matching of\nmultispectral palmprints. To facilitate compact storage of the feature, we\ndesign a binary hash table structure that allows for efficient matching in\nlarge databases. Comprehensive experiments for both identification and\nverification scenarios are performed on two public datasets -- one captured\nwith a contact-based sensor (PolyU dataset), and the other with a contact-free\nsensor (CASIA dataset). Recognition results in various experimental setups show\nthat the proposed method consistently outperforms existing state-of-the-art\nmethods. Error rates achieved by our method (0.003% on PolyU and 0.2% on CASIA)\nare the lowest reported in literature on both dataset and clearly indicate the\nviability of palmprint as a reliable and promising biometric. All source codes\nare publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 06:35:51 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Khan", "Zohaib", ""], ["Shafait", "Faisal", ""], ["Hu", "Yiqun", ""], ["Mian", "Ajmal", ""]]}, {"id": "1402.3261", "submitter": "Didier Henrion", "authors": "Jan Heller, Didier Henrion (LAAS, CTU/FEE), Tomas Pajdla", "title": "Hand-Eye and Robot-World Calibration by Global Polynomial Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to relate measurements made by a camera to a different known\ncoordinate system arises in many engineering applications. Historically, it\nappeared for the first time in the connection with cameras mounted on robotic\nsystems. This problem is commonly known as hand-eye calibration. In this paper,\nwe present several formulations of hand-eye calibration that lead to\nmultivariate polynomial optimization problems. We show that the method of\nconvex linear matrix inequality (LMI) relaxations can be used to effectively\nsolve these problems and to obtain globally optimal solutions. Further, we show\nthat the same approach can be used for the simultaneous hand-eye and\nrobot-world calibration. Finally, we validate the proposed solutions using both\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 19:17:01 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Heller", "Jan", "", "LAAS, CTU/FEE"], ["Henrion", "Didier", "", "LAAS, CTU/FEE"], ["Pajdla", "Tomas", ""]]}, {"id": "1402.3337", "submitter": "Kishore Konda", "authors": "Kishore Konda, Roland Memisevic, David Krueger", "title": "Zero-bias autoencoders and the benefits of co-adapting features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized training of an autoencoder typically results in hidden unit\nbiases that take on large negative values. We show that negative biases are a\nnatural result of using a hidden layer whose responsibility is to both\nrepresent the input data and act as a selection mechanism that ensures sparsity\nof the representation. We then show that negative biases impede the learning of\ndata distributions whose intrinsic dimensionality is high. We also propose a\nnew activation function that decouples the two roles of the hidden layer and\nthat allows us to learn representations on data with very high intrinsic\ndimensionality, where standard autoencoders typically fail. Since the decoupled\nactivation function acts like an implicit regularizer, the model can be trained\nby minimizing the reconstruction error of training data, without requiring any\nadditional regularization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 23:37:39 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 21:39:48 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 02:07:47 GMT"}, {"version": "v4", "created": "Sat, 28 Feb 2015 01:15:33 GMT"}, {"version": "v5", "created": "Wed, 8 Apr 2015 14:51:11 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Konda", "Kishore", ""], ["Memisevic", "Roland", ""], ["Krueger", "David", ""]]}, {"id": "1402.3344", "submitter": "Zhang Chong", "authors": "Chong Zhang, Yu Zhao, Jochen Triesch and Bertram E. Shi", "title": "Intrinsically Motivated Learning of Visual Motion Perception and Smooth\n  Pursuit", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We extend the framework of efficient coding, which has been used to model the\ndevelopment of sensory processing in isolation, to model the development of the\nperception/action cycle. Our extension combines sparse coding and reinforcement\nlearning so that sensory processing and behavior co-develop to optimize a\nshared intrinsic motivational signal: the fidelity of the neural encoding of\nthe sensory input under resource constraints. Applying this framework to a\nmodel system consisting of an active eye behaving in a time varying\nenvironment, we find that this generic principle leads to the simultaneous\ndevelopment of both smooth pursuit behavior and model neurons whose properties\nare similar to those of primary visual cortical neurons selective for different\ndirections of visual motion. We suggest that this general principle may form\nthe basis for a unified and integrated explanation of many perception/action\nloops.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 01:27:41 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2014 03:00:00 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Zhang", "Chong", ""], ["Zhao", "Yu", ""], ["Triesch", "Jochen", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1402.3557", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi, Youngbae Hwang, Serge Belongie, Truong Nguyen", "title": "Improving Streaming Video Segmentation with Early and Mid-Level Visual\n  Processing", "comments": "WACV accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in video segmentation, many opportunities remain to\nimprove it using a variety of low and mid-level visual cues. We propose\nimprovements to the leading streaming graph-based hierarchical video\nsegmentation (streamGBH) method based on early and mid level visual processing.\nThe extensive experimental analysis of our approach validates the improvement\nof hierarchical supervoxel representation by incorporating motion and color\nwith effective filtering. We also pose and illuminate some open questions\ntowards intermediate level video analysis as further extension to streamGBH. We\nexploit the supervoxels as an initialization towards estimation of dominant\naffine motion regions, followed by merging of such motion regions in order to\nhierarchically segment a video in a novel motion-segmentation framework which\naims at subsequent applications such as foreground recognition.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 19:37:35 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Tripathi", "Subarna", ""], ["Hwang", "Youngbae", ""], ["Belongie", "Serge", ""], ["Nguyen", "Truong", ""]]}, {"id": "1402.3657", "submitter": "V Karthikeyan VKK", "authors": "V. Karthikeyan, B. Praveen Kumar, S. Suresh Babu, R. Purusothaman,\n  shijin Thomas", "title": "A Narrative Vehicle Protection Representation for Vehicle Speed\n  Regulator Under Driver Exhaustion -- A Study", "comments": "4 pages 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver fatigue is one of the important factors that cause traffic accidents,\nand the ever-increasing number due to diminished drivers vigilance level has\nbecome a problem of serious concern to society. Drivers with a diminished\nvigilance level suffer from a marked decline in their abilities of perception,\nrecognition, and vehicle control, and therefore pose serious danger to their\nown life and the lives of other people. Exhaustion resulting from sleep\ndeprivation or sleep disorders is an important factor in the creasing number of\naccidents. In this projected work, we discuss the various methods of the\nexisting and the proposed method based on a real time online safety prototype\nthat controls the vehicle speed under driver fatigue. The purpose of such a\nmodel is to advance a system to detect fatigue symptoms in drivers and control\nthe speed of vehicle to avoid accidents. This system was tested adequately with\nsubjects of different technology of various researchers finally the validity of\nthe proposed model for vehicle speed controller based on driver fatigue\ndetection is shown.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2014 06:51:00 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Karthikeyan", "V.", ""], ["Kumar", "B. Praveen", ""], ["Babu", "S. Suresh", ""], ["Purusothaman", "R.", ""], ["Thomas", "shijin", ""]]}, {"id": "1402.3849", "submitter": "Radha Chitta", "authors": "Radha Chitta, Rong Jin, Timothy C. Havens, Anil K. Jain", "title": "Scalable Kernel Clustering: Approximate Kernel k-means", "comments": "15 pages, 6 figures,extension of the work \"Approximate Kernel\n  k-means: Solution to large scale kernel clustering\" published in KDD 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based clustering algorithms have the ability to capture the non-linear\nstructure in real world data. Among various kernel-based clustering algorithms,\nkernel k-means has gained popularity due to its simple iterative nature and\nease of implementation. However, its run-time complexity and memory footprint\nincrease quadratically in terms of the size of the data set, and hence, large\ndata sets cannot be clustered efficiently. In this paper, we propose an\napproximation scheme based on randomization, called the Approximate Kernel\nk-means. We approximate the cluster centers using the kernel similarity between\na few sampled points and all the points in the data set. We show that the\nproposed method achieves better clustering performance than the traditional low\nrank kernel approximation based clustering schemes. We also demonstrate that\nits running time and memory requirements are significantly lower than those of\nkernel k-means, with only a small reduction in the clustering quality on\nseveral public domain large data sets. We then employ ensemble clustering\ntechniques to further enhance the performance of our algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 22:19:40 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Chitta", "Radha", ""], ["Jin", "Rong", ""], ["Havens", "Timothy C.", ""], ["Jain", "Anil K.", ""]]}, {"id": "1402.3869", "submitter": "Yilun Wang", "authors": "Yilun Wang", "title": "FTVd is beyond Fast Total Variation regularized Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the \"FTVd\" algorithm for Fast Total Variation\nRegularized Deconvolution, which has been widely used in the past few years.\nBoth its original version implemented in the MATLAB software FTVd 3.0 and its\nrelated variant implemented in the latter version FTVd 4.0 are considered\n\\cite{Wang08FTVdsoftware}. We propose that the intermediate results during the\niterations are the solutions of a series of combined Tikhonov and total\nvariation regularized image deconvolution models and therefore some of them\noften have even better image quality than the final solution, which is\ncorresponding to the pure total variation regularized model.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 02:13:30 GMT"}, {"version": "v2", "created": "Fri, 16 May 2014 03:24:09 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Wang", "Yilun", ""]]}, {"id": "1402.3926", "submitter": "Hideitsu Hino", "authors": "Toshiyuki Kato, Hideitsu Hino, and Noboru Murata", "title": "Sparse Coding Approach for Multi-Frame Image Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image super-resolution method from multiple observation of low-resolution\nimages is proposed. The method is based on sub-pixel accuracy block matching\nfor estimating relative displacements of observed images, and sparse signal\nrepresentation for estimating the corresponding high-resolution image. Relative\ndisplacements of small patches of observed low-resolution images are accurately\nestimated by a computationally efficient block matching method. Since the\nestimated displacements are also regarded as a warping component of image\ndegradation process, the matching results are directly utilized to generate\nlow-resolution dictionary for sparse image representation. The matching scores\nof the block matching are used to select a subset of low-resolution patches for\nreconstructing a high-resolution patch, that is, an adaptive selection of\ninformative low-resolution images is realized. When there is only one\nlow-resolution image, the proposed method works as a single-frame\nsuper-resolution method. The proposed method is shown to perform comparable or\nsuperior to conventional single- and multi-frame super-resolution methods\nthrough experiments using various real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 08:23:35 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Kato", "Toshiyuki", ""], ["Hino", "Hideitsu", ""], ["Murata", "Noboru", ""]]}, {"id": "1402.4053", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J Kir\\'aly and Martin Ehler", "title": "The Algebraic Approach to Phase Retrieval and Explicit Inversion at the\n  Identifiability Threshold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.CV cs.IT math.AG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study phase retrieval from magnitude measurements of an unknown signal as\nan algebraic estimation problem. Indeed, phase retrieval from rank-one and more\ngeneral linear measurements can be treated in an algebraic way. It is verified\nthat a certain number of generic rank-one or generic linear measurements are\nsufficient to enable signal reconstruction for generic signals, and slightly\nmore generic measurements yield reconstructability for all signals. Our results\nsolve a few open problems stated in the recent literature. Furthermore, we show\nhow the algebraic estimation problem can be solved by a closed-form algebraic\nestimation technique, termed ideal regression, providing non-asymptotic success\nguarantees.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 16:49:38 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Kir\u00e1ly", "Franz J", ""], ["Ehler", "Martin", ""]]}, {"id": "1402.4067", "submitter": "Santiago Aja-Fernandez", "authors": "Santiago Aja-Fernandez, Gonzalo Vegas-Sanchez-Ferrero, Antonio\n  Trsitan-Vega", "title": "Statistical Noise Analysis in SENSE Parallel MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": "TECH-LPI2012-01. V2.0", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complete first and second order statistical characterization of noise in\nSENSE reconstructed data is proposed. SENSE acquisitions have usually been\nmodeled as Rician distributed, since the data reconstruction takes place into\nthe spatial domain, where Gaussian noise is assumed. However, this model just\nholds for the first order statistics and obviates other effects induced by\ncoils correlations and the reconstruction interpolation. Those effects are\nproperly taken into account in this study, in order to fully justify a final\nSENSE noise model. As a result, some interesting features of the reconstructed\nimage arise: (1) There is a strong correlation between adjacent lines. (2) The\nresulting distribution is non-stationary and therefore the variance of noise\nwill vary from point to point across the image. Closed equations for the\ncalculation of the variance of noise and the correlation coefficient between\nlines are proposed. The proposed model is totally compatible with g-factor\nformulations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 17:16:21 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Aja-Fernandez", "Santiago", ""], ["Vegas-Sanchez-Ferrero", "Gonzalo", ""], ["Trsitan-Vega", "Antonio", ""]]}, {"id": "1402.4069", "submitter": "Yasel Garc\\'es Su\\'arez", "authors": "Yasel Garc\\'es, Esley Torres, Osvaldo Pereira and Roberto Rodr\\'iguez", "title": "Application of the Ring Theory in the Segmentation of Digital Images", "comments": "Very interesting new index to compute the similarity among images.\n  arXiv admin note: substantial text overlap with arXiv:1306.2624", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ring theory is one of the branches of the abstract algebra that has been\nbroadly used in images. However, ring theory has not been very related with\nimage segmentation. In this paper, we propose a new index of similarity among\nimages using Zn rings and the entropy function. This new index was applied as a\nnew stopping criterion to the Mean Shift Iterative Algorithm with the goal to\nreach a better segmentation. An analysis on the performance of the algorithm\nwith this new stopping criterion is carried out. The obtained results proved\nthat the new index is a suitable tool to compare images.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 17:16:35 GMT"}, {"version": "v2", "created": "Mon, 24 Nov 2014 20:09:39 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Garc\u00e9s", "Yasel", ""], ["Torres", "Esley", ""], ["Pereira", "Osvaldo", ""], ["Rodr\u00edguez", "Roberto", ""]]}, {"id": "1402.4388", "submitter": "Mohammed  Javed", "authors": "Mohammed Javed, P. Nagabhushan, B.B. Chaudhuri", "title": "Automatic Detection of Font Size Straight from Run Length Compressed\n  Text Documents", "comments": "8 Pages", "journal-ref": "(IJCSIT) International Journal of Computer Science and Information\n  Technologies, Vol. 5 (1) , 2014, 818-825", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of font size finds many applications in the area of\nintelligent OCRing and document image analysis, which has been traditionally\npracticed over uncompressed documents, although in real life the documents\nexist in compressed form for efficient storage and transmission. It would be\nnovel and intelligent if the task of font size detection could be carried out\ndirectly from the compressed data of these documents without decompressing,\nwhich would result in saving of considerable amount of processing time and\nspace. Therefore, in this paper we present a novel idea of learning and\ndetecting font size directly from run-length compressed text documents at line\nlevel using simple line height features, which paves the way for intelligent\nOCRing and document analysis directly from compressed documents. In the\nproposed model, the given mixed-case text documents of different font size are\nsegmented into compressed text lines and the features extracted such as line\nheight and ascender height are used to capture the pattern of font size in the\nform of a regression line, using which the automatic detection of font size is\ndone during the recognition stage. The method is experimented with a dataset of\n50 compressed documents consisting of 780 text lines of single font size and\n375 text lines of mixed font size resulting in an overall accuracy of 99.67%.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 16:30:59 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Javed", "Mohammed", ""], ["Nagabhushan", "P.", ""], ["Chaudhuri", "B. B.", ""]]}, {"id": "1402.4566", "submitter": "Jaydeep De", "authors": "Jaydeep De and Xiaowei Zhang and Li Cheng", "title": "Transduction on Directed Graphs via Absorbing Random Walks", "comments": "The paper is withdrawn because of some violation in institute policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of graph-based transductive\nclassification, and we are particularly interested in the directed graph\nscenario which is a natural form for many real world applications. Different\nfrom existing research efforts that either only deal with undirected graphs or\ncircumvent directionality by means of symmetrization, we propose a novel random\nwalk approach on directed graphs using absorbing Markov chains, which can be\nregarded as maximizing the accumulated expected number of visits from the\nunlabeled transient states. Our algorithm is simple, easy to implement, and\nworks with large-scale graphs. In particular, it is capable of preserving the\ngraph structure even when the input graph is sparse and changes over time, as\nwell as retaining weak signals presented in the directed edges. We present its\nintimate connections to a number of existing methods, including graph kernels,\ngraph Laplacian based methods, and interestingly, spanning forest of graphs.\nIts computational complexity and the generalization error are also studied.\nEmpirically our algorithm is systematically evaluated on a wide range of\napplications, where it has shown to perform competitively comparing to a suite\nof state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 06:41:12 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 02:05:34 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["De", "Jaydeep", ""], ["Zhang", "Xiaowei", ""], ["Cheng", "Li", ""]]}, {"id": "1402.4888", "submitter": "Johnvictor D", "authors": "D. Johnvictor, G. Selvavinayagam", "title": "Survey on Sparse Coded Features for Content Based Face Image Retrieval", "comments": "4 pages,3 figures,1 table, Published with International Journal of\n  Computer Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  8(1):30-33, February 2014. ISSN:2231-2803", "doi": "10.14445/22312803/IJCTT-V8P106", "report-no": null, "categories": "cs.IR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content based image retrieval, a technique which uses visual contents of\nimage to search images from large scale image databases according to users'\ninterests. This paper provides a comprehensive survey on recent technology used\nin the area of content based face image retrieval. Nowadays digital devices and\nphoto sharing sites are getting more popularity, large human face photos are\navailable in database. Multiple types of facial features are used to represent\ndiscriminality on large scale human facial image database. Searching and mining\nof facial images are challenging problems and important research issues. Sparse\nrepresentation on features provides significant improvement in indexing related\nimages to query image.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 04:32:40 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Johnvictor", "D.", ""], ["Selvavinayagam", "G.", ""]]}, {"id": "1402.4893", "submitter": "Xianping Li", "authors": "Xianping Li", "title": "Anisotropic Mesh Adaptation for Image Representation", "comments": "25 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangular meshes have gained much interest in image representation and have\nbeen widely used in image processing. This paper introduces a framework of\nanisotropic mesh adaptation (AMA) methods to image representation and proposes\na GPRAMA method that is based on AMA and greedy-point removal (GPR) scheme.\nDifferent than many other methods that triangulate sample points to form the\nmesh, the AMA methods start directly with a triangular mesh and then adapt the\nmesh based on a user-defined metric tensor to represent the image. The AMA\nmethods have clear mathematical framework and provides flexibility for both\nimage representation and image reconstruction. A mesh patching technique is\ndeveloped for the implementation of the GPRAMA method, which leads to an\nimproved version of the popular GPRFS-ED method. The GPRAMA method can achieve\nbetter quality than the GPRFS-ED method but with lower computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 05:15:22 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 23:08:06 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 03:03:29 GMT"}, {"version": "v4", "created": "Tue, 29 Mar 2016 19:10:35 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Li", "Xianping", ""]]}, {"id": "1402.4936", "submitter": "Ayman Bahaa-Eldin", "authors": "Amira Mohammad Abdel-Mawgoud Saleh", "title": "Enhanced Secure Algorithm for Fingerprint Recognition", "comments": "PhD Thesis, Ain Shams University, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint recognition requires a minimal effort from the user, does not\ncapture other information than strictly necessary for the recognition process,\nand provides relatively good performance. A critical step in fingerprint\nidentification system is thinning of the input fingerprint image. The\nperformance of a minutiae extraction algorithm relies heavily on the quality of\nthe thinning algorithm. So, a fast fingerprint thinning algorithm is proposed.\nThe algorithm works directly on the gray-scale image as binarization of\nfingerprint causes many spurious minutiae and also removes many important\nfeatures. The performance of the thinning algorithm is evaluated and\nexperimental results show that the proposed thinning algorithm is both fast and\naccurate. A new minutiae-based fingerprint matching technique is proposed. The\nmain idea is that each fingerprint is represented by a minutiae table of just\ntwo columns in the database. The number of different minutiae types\n(terminations and bifurcations) found in each track of a certain width around\nthe core point of the fingerprint is recorded in this table. Each row in the\ntable represents a certain track, in the first column, the number of\nterminations in each track is recorded, in the second column, the number of\nbifurcations in each track is recorded. The algorithm is rotation and\ntranslation invariant, and needs less storage size. Experimental results show\nthat recognition accuracy is 98%, with Equal Error Rate (EER) of 2%. Finally,\nthe integrity of the data transmission via communication channels must be\nsecure all the way from the scanner to the application. After applying Gaussian\nnoise addition, and JPEG compression with high and moderate quality factors on\nthe watermarked fingerprint images, recognition accuracy decreases slightly to\nreach 96%.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 09:19:17 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Saleh", "Amira Mohammad Abdel-Mawgoud", ""]]}, {"id": "1402.4963", "submitter": "Julius Hannink", "authors": "Julius Hannink, Remco Duits and Erik Bekkers", "title": "Vesselness via Multiple Scale Orientation Scores", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-scale Frangi vesselness filter is an established tool in (retinal)\nvascular imaging. However, it cannot cope with crossings or bifurcations, since\nit only looks for elongated structures. Therefore, we disentangle crossing\nstructures in the image via (multiple scale) invertible orientation scores. The\ndescribed vesselness filter via scale-orientation scores performs considerably\nbetter at enhancing vessels throughout crossings and bifurcations than the\nFrangi version. Both methods are evaluated on a public dataset. Performance is\nmeasured by comparing ground truth data to the segmentation results obtained by\nbasic thresholding and morphological component analysis of the filtered images.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 11:06:35 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2014 18:30:55 GMT"}, {"version": "v3", "created": "Tue, 4 Mar 2014 12:33:37 GMT"}, {"version": "v4", "created": "Mon, 19 May 2014 09:20:06 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Hannink", "Julius", ""], ["Duits", "Remco", ""], ["Bekkers", "Erik", ""]]}, {"id": "1402.5047", "submitter": "Lucas Paletta", "authors": "Stefano Piana, Alessandra Staglian\\`o, Francesca Odone, Alessandro\n  Verri, Antonio Camurri", "title": "Real-time Automatic Emotion Recognition from Body Gestures", "comments": null, "journal-ref": null, "doi": null, "report-no": "IDGEI/2014/02", "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although psychological research indicates that bodily expressions convey\nimportant affective information, to date research in emotion recognition\nfocused mainly on facial expression or voice analysis. In this paper we propose\nan approach to realtime automatic emotion recognition from body movements. A\nset of postural, kinematic, and geometrical features are extracted from\nsequences 3D skeletons and fed to a multi-class SVM classifier. The proposed\nmethod has been assessed on data acquired through two different systems: a\nprofessionalgrade optical motion capture system, and Microsoft Kinect. The\nsystem has been assessed on a \"six emotions\" recognition problem, and using a\nleave-one-subject-out cross validation strategy, reached an overall recognition\nrate of 61.3% which is very close to the recognition rate of 61.9% obtained by\nhuman observers. To provide further testing of the system, two games were\ndeveloped, where one or two users have to interact to understand and express\nemotions with their body.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 15:42:32 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Piana", "Stefano", ""], ["Staglian\u00f2", "Alessandra", ""], ["Odone", "Francesca", ""], ["Verri", "Alessandro", ""], ["Camurri", "Antonio", ""]]}, {"id": "1402.5073", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "Exploiting Two-Dimensional Group Sparsity in 1-Bit Compressive Sensing", "comments": "RecPad 2013, Lisbon, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach, {\\it two-dimensional fused binary compressive\nsensing} (2DFBCS) to recover 2D sparse piece-wise signals from 1-bit\nmeasurements, exploiting 2D group sparsity for 1-bit compressive sensing\nrecovery. The proposed method is a modified 2D version of the previous {\\it\nbinary iterative hard thresholding} (2DBIHT) algorithm, where the objective\nfunction includes a 2D one-sided $\\ell_1$ (or $\\ell_2$) penalty function\nencouraging agreement with the observed data, an indicator function of\n$K$-sparsity, and a total variation (TV) or modified TV (MTV) constraint. The\nsubgradient of the 2D one-sided $\\ell_1$ (or $\\ell_2$) penalty and the\nprojection onto the $K$-sparsity and TV or MTV constraint can be computed\nefficiently, allowing the appliaction of algorithms of the {\\it\nforward-backward splitting} (a.k.a. {\\it iterative shrinkage-thresholding})\nfamily. Experiments on the recovery of 2D sparse piece-wise smooth signals show\nthat the proposed approach is able to take advantage of the piece-wise\nsmoothness of the original signal, achieving more accurate recovery than\n2DBIHT. More specifically, 2DFBCS with the MTV and the $\\ell_2$ penalty\nperforms best amongst the algorithms tested.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 17:05:59 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2014 09:34:01 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1402.5074", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "Binary Fused Compressive Sensing: 1-Bit Compressive Sensing meets Group\n  Sparsity", "comments": "Conf. on Telecommunications - ConfTele, Castelo Branco, Portugal,\n  Vol. 1, pp. 65 - 68, May, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method, {\\it binary fused compressive sensing} (BFCS), to\nrecover sparse piece-wise smooth signals from 1-bit compressive measurements.\nThe proposed algorithm is a modification of the previous {\\it binary iterative\nhard thresholding} (BIHT) algorithm, where, in addition to the sparsity\nconstraint, the total-variation of the recovered signal is upper constrained.\nAs in BIHT, the data term of the objective function is an one-sided $\\ell_1$\n(or $\\ell_2$) norm. Experiments on the recovery of sparse piece-wise smooth\nsignals show that the proposed algorithm is able to take advantage of the\npiece-wise smoothness of the original signal, achieving more accurate recovery\nthan BIHT.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 17:06:15 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1402.5076", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "Robust Binary Fused Compressive Sensing using Adaptive Outlier Pursuit", "comments": "Accepted by ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method, {\\it robust binary fused compressive sensing}\n(RoBFCS), to recover sparse piece-wise smooth signals from 1-bit compressive\nmeasurements. The proposed method is a modification of our previous {\\it binary\nfused compressive sensing} (BFCS) algorithm, which is based on the {\\it binary\niterative hard thresholding} (BIHT) algorithm. As in BIHT, the data term of the\nobjective function is a one-sided $\\ell_1$ (or $\\ell_2$) norm. Experiments show\nthat the proposed algorithm is able to take advantage of the piece-wise\nsmoothness of the original signal and detect sign flips and correct them,\nachieving more accurate recovery than BFCS and BIHT.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 17:07:23 GMT"}, {"version": "v2", "created": "Thu, 20 Mar 2014 11:11:45 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1402.5077", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "Group-sparse Matrix Recovery", "comments": "ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the OSCAR (octagonal selection and clustering algorithms for\nregression) in recovering group-sparse matrices (two-dimensional---2D---arrays)\nfrom compressive measurements. We propose a 2D version of OSCAR (2OSCAR)\nconsisting of the $\\ell_1$ norm and the pair-wise $\\ell_{\\infty}$ norm, which\nis convex but non-differentiable. We show that the proximity operator of 2OSCAR\ncan be computed based on that of OSCAR. The 2OSCAR problem can thus be\nefficiently solved by state-of-the-art proximal splitting algorithms.\nExperiments on group-sparse 2D array recovery show that 2OSCAR regularization\nsolved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm\n(with debiasing) yields the most accurate results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 17:08:34 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1402.5497", "submitter": "Chunhua Shen", "authors": "Yan Yan, Chunhua Shen, Hanzi Wang", "title": "Efficient Semidefinite Spectral Clustering via Lagrange Duality", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient approach to semidefinite spectral clustering (SSC),\nwhich addresses the Frobenius normalization with the positive semidefinite\n(p.s.d.) constraint for spectral clustering. Compared with the original\nFrobenius norm approximation based algorithm, the proposed algorithm can more\naccurately find the closest doubly stochastic approximation to the affinity\nmatrix by considering the p.s.d. constraint. In this paper, SSC is formulated\nas a semidefinite programming (SDP) problem. In order to solve the high\ncomputational complexity of SDP, we present a dual algorithm based on the\nLagrange dual formalization. Two versions of the proposed algorithm are\nproffered: one with less memory usage and the other with faster convergence\nrate. The proposed algorithm has much lower time complexity than that of the\nstandard interior-point based SDP solvers. Experimental results on both UCI\ndata sets and real-world image data sets demonstrate that 1) compared with the\nstate-of-the-art spectral clustering methods, the proposed algorithm achieves\nbetter clustering performance; and 2) our algorithm is much more efficient and\ncan solve larger-scale SSC problems than those standard interior-point SDP\nsolvers.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 09:39:52 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Yan", "Yan", ""], ["Shen", "Chunhua", ""], ["Wang", "Hanzi", ""]]}, {"id": "1402.5564", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie and Zeyun Yu", "title": "Structure Tensor Based Image Interpolation Method", "comments": "Accepted for publication in AEU - International Journal of\n  Electronics and Communications", "journal-ref": null, "doi": "10.1016/j.aeue.2014.10.022", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature preserving image interpolation is an active area in image processing\nfield. In this paper a new direct edge directed image super-resolution\nalgorithm based on structure tensors is proposed. Using an isotropic Gaussian\nfilter, the structure tensor at each pixel of the input image is computed and\nthe pixels are classified to three distinct classes; uniform region, corners\nand edges, according to the eigenvalues of the structure tensor. Due to\napplication of the isotropic Gaussian filter, the classification is robust to\nnoise presented in image. Based on the tangent eigenvector of the structure\ntensor, the edge direction is determined and used for interpolation along the\nedges. In comparison to some previous edge directed image interpolation\nmethods, the proposed method achieves higher quality in both subjective and\nobjective aspects. Also the proposed method outperforms previous methods in\ncase of noisy and JPEG compressed images. Furthermore, without the need for\noptimization in the process, the algorithm can achieve higher speed.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 23:58:11 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 03:20:08 GMT"}, {"version": "v3", "created": "Fri, 26 Dec 2014 08:18:31 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Baghaie", "Ahmadreza", ""], ["Yu", "Zeyun", ""]]}, {"id": "1402.5619", "submitter": "V Karthikeyan VKK", "authors": "V. Karthikeyan", "title": "A Novel Histogram Based Robust Image Registration Technique", "comments": "5 pages and 6 figures. submit/0850305", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a method for Automatic Image Registration (AIR) through\nhistogram is proposed. Automatic image registration is one of the crucial steps\nin the analysis of remotely sensed data. A new acquired image must be\ntransformed, using image registration techniques, to match the orientation and\nscale of previous related images. This new approach combines several\nsegmentations of the pair of images to be registered. A relaxation parameter on\nthe histogram modes delineation is introduced. It is followed by\ncharacterization of the extracted objects through the objects area, axis ratio,\nand perimeter and fractal dimension. The matched objects are used for rotation\nand translation estimation. It allows for the registration of pairs of images\nwith differences in rotation and translation. This method contributes to\nsubpixel accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 15:24:27 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Karthikeyan", "V.", ""]]}, {"id": "1402.5623", "submitter": "V Karthikeyan VKK", "authors": "V.Karthikeyan and V.J.Vijayalakshmi", "title": "Localization of License Plate Using Morphological Operations", "comments": "6 PAGES 12 FIGURES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is believed that there are currently millions of vehicles on the roads\nworldwide. The over speed of vehicles,theft of vehicles, disobeying traffic\nrules in public, an unauthorized person entering the restricted area are keep\non increasing. In order restrict against these criminal activities, we need an\nautomatic public security system. Each vehicle has their own Vehicle\nIdentification Number (VIN) as their primary identifier. The VIN is actually a\nLicense Number which states a legal license to participate in the public\ntraffic. The proposed paper is to identify the vehicle with the help of\nvehicles License Plate (LP).LPRS is one the most important part of the\nIntelligent Transportation System (ITS) to locate the LP. In this paper certain\nexisting algorithm drawbacks are overcome by the proposed morphological\noperations for LPRS. Morphological operation is chosen due to its higher\nefficiency, noise filter capacity, accuracy, exact localization of LP and\nspeed.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 16:08:09 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Karthikeyan", "V.", ""], ["Vijayalakshmi", "V. J.", ""]]}, {"id": "1402.5684", "submitter": "Orhan Firat", "authors": "Orhan Firat and Mete Ozay and Ilke Oztekin and Fatos T. Yarman Vural", "title": "Discriminative Functional Connectivity Measures for Brain Decoding", "comments": "This paper has been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical learning model for classifying cognitive processes\nbased on distributed patterns of neural activation in the brain, acquired via\nfunctional magnetic resonance imaging (fMRI). In the proposed learning method,\nlocal meshes are formed around each voxel. The distance between voxels in the\nmesh is determined by using a functional neighbourhood concept. In order to\ndefine the functional neighbourhood, the similarities between the time series\nrecorded for voxels are measured and functional connectivity matrices are\nconstructed. Then, the local mesh for each voxel is formed by including the\nfunctionally closest neighbouring voxels in the mesh. The relationship between\nthe voxels within a mesh is estimated by using a linear regression model. These\nrelationship vectors, called Functional Connectivity aware Local Relational\nFeatures (FC-LRF) are then used to train a statistical learning machine. The\nproposed method was tested on a recognition memory experiment, including data\npertaining to encoding and retrieval of words belonging to ten different\nsemantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn)\nand Support Vector Machine (SVM), are trained in order to predict the semantic\ncategory of the item being retrieved, based on activation patterns during\nencoding. The classification performance of the Functional Mesh Learning model,\nwhich range in 62%-71% is superior to the classical multi-voxel pattern\nanalysis (MVPA) methods, which range in 40%-48%, for ten semantic categories.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 22:01:11 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 19:02:07 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Firat", "Orhan", ""], ["Ozay", "Mete", ""], ["Oztekin", "Ilke", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1402.5697", "submitter": "Changxin Gao", "authors": "Changxin Gao, Feifei Chen, Jin-Gang Yu, Rui Huang, Nong Sang", "title": "Exemplar-based Linear Discriminant Analysis for Robust Object Tracking", "comments": "ICIP2014", "journal-ref": null, "doi": "10.1109/ICIP.2014.7025077", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking-by-detection has become an attractive tracking technique, which\ntreats tracking as a category detection problem. However, the task in tracking\nis to search for a specific object, rather than an object category as in\ndetection. In this paper, we propose a novel tracking framework based on\nexemplar detector rather than category detector. The proposed tracker is an\nensemble of exemplar-based linear discriminant analysis (ELDA) detectors. Each\ndetector is quite specific and discriminative, because it is trained by a\nsingle object instance and massive negatives. To improve its adaptivity, we\nupdate both object and background models. Experimental results on several\nchallenging video sequences demonstrate the effectiveness and robustness of our\ntracking algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 01:10:09 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Gao", "Changxin", ""], ["Chen", "Feifei", ""], ["Yu", "Jin-Gang", ""], ["Huang", "Rui", ""], ["Sang", "Nong", ""]]}, {"id": "1402.5766", "submitter": "Adriana Romero", "authors": "Adriana Romero, Petia Radeva and Carlo Gatta", "title": "No more meta-parameter tuning in unsupervised sparse feature learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised\nfeature learning algorithm, which exploits a new way of optimizing for\nsparsity. Experiments on STL-10 show that the method presents state-of-the-art\nperformance and provides discriminative features that generalize well.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 09:49:04 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Romero", "Adriana", ""], ["Radeva", "Petia", ""], ["Gatta", "Carlo", ""]]}, {"id": "1402.5792", "submitter": "Seyed Mostafa Kia", "authors": "Seyed Mostafa Kia, Hossein Rahmani, Reza Mortezaei, Mohsen Ebrahimi\n  Moghaddam, Amer Namazi", "title": "A Novel Scheme for Intelligent Recognition of Pornographic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harmful contents are rising in internet day by day and this motivates the\nessence of more research in fast and reliable obscene and immoral material\nfiltering. Pornographic image recognition is an important component in each\nfiltering system. In this paper, a new approach for detecting pornographic\nimages is introduced. In this approach, two new features are suggested. These\ntwo features in combination with other simple traditional features provide\ndecent difference between porn and non-porn images. In addition, we applied\nfuzzy integral based information fusion to combine MLP (Multi-Layer Perceptron)\nand NF (Neuro-Fuzzy) outputs. To test the proposed method, performance of\nsystem was evaluated over 18354 download images from internet. The attained\nprecision was 93% in TP and 8% in FP on training dataset, and 87% and 5.5% on\ntest dataset. Achieved results verify the performance of proposed system versus\nother related works.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 11:15:04 GMT"}, {"version": "v2", "created": "Wed, 25 Jun 2014 14:04:13 GMT"}, {"version": "v3", "created": "Mon, 29 Sep 2014 22:15:26 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Kia", "Seyed Mostafa", ""], ["Rahmani", "Hossein", ""], ["Mortezaei", "Reza", ""], ["Moghaddam", "Mohsen Ebrahimi", ""], ["Namazi", "Amer", ""]]}, {"id": "1402.5805", "submitter": "Eric Hitimana", "authors": "Eric Hitimana and Oubong Gwun", "title": "Automatic Estimation of Live Coffee Leaf Infection based on Image\n  Processing Techniques", "comments": "SIPP 2014 : Second International Conference on Signal, Image\n  Processing and Pattern Recognition,Sydney, Australia", "journal-ref": null, "doi": "10.5121/csit.2014.4221", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is the most challenging issue in computer vision\napplications. And most difficulties for crops management in agriculture are the\nlack of appropriate methods for detecting the leaf damage for pests treatment.\nIn this paper we proposed an automatic method for leaf damage detection and\nseverity estimation of coffee leaf by avoiding defoliation. After enhancing the\ncontrast of the original image using LUT based gamma correction, the image is\nprocessed to remove the background, and the output leaf is clustered using\nFuzzy c-means segmentation in V channel of YUV color space to maximize all leaf\ndamage detection, and finally, the severity of leaf is estimated in terms of\nratio for leaf pixel distribution between the normal and the detected leaf\ndamage. The results in each proposed method was compared to the current\nresearches and the accuracy is obvious either in the background removal or\ndamage detection.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 12:06:40 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Hitimana", "Eric", ""], ["Gwun", "Oubong", ""]]}, {"id": "1402.5859", "submitter": "Huanguo Zhang", "authors": "Huanguo Zhang, Sha Lv, Wei Li and Xun Qu", "title": "A Novel Face Recognition Method using Nearest Line Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is a popular application of pat- tern recognition methods,\nand it faces challenging problems including illumination, expression, and pose.\nThe most popular way is to learn the subspaces of the face images so that it\ncould be project to another discriminant space where images of different\npersons can be separated. In this paper, a nearest line projection algorithm is\ndeveloped to represent the face images for face recognition. Instead of\nprojecting an image to its nearest image, we try to project it to its nearest\nline spanned by two different face images. The subspaces are learned so that\neach face image to its nearest line is minimized. We evaluated the proposed\nalgorithm on some benchmark face image database, and also compared it to some\nother image projection algorithms. The experiment results showed that the\nproposed algorithm outperforms other ones.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 15:36:32 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Zhang", "Huanguo", ""], ["Lv", "Sha", ""], ["Li", "Wei", ""], ["Qu", "Xun", ""]]}, {"id": "1402.5923", "submitter": "Tatiana Tommasi", "authors": "Tatiana Tommasi, Tinne Tuytelaars, Barbara Caputo", "title": "A Testbed for Cross-Dataset Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": "December 2013, Technical Report: KUL/ESAT/PSI/1304", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its beginning visual recognition research has tried to capture the huge\nvariability of the visual world in several image collections. The number of\navailable datasets is still progressively growing together with the amount of\nsamples per object category. However, this trend does not correspond directly\nto an increasing in the generalization capabilities of the developed\nrecognition systems. Each collection tends to have its specific characteristics\nand to cover just some aspects of the visual world: these biases often narrow\nthe effect of the methods defined and tested separately over each image set.\nOur work makes a first step towards the analysis of the dataset bias problem on\na large scale. We organize twelve existing databases in a unique corpus and we\npresent the visual community with a useful feature repository for future\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 19:25:17 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Tommasi", "Tatiana", ""], ["Tuytelaars", "Tinne", ""], ["Caputo", "Barbara", ""]]}, {"id": "1402.5979", "submitter": "Renato J Cintra", "authors": "V. A. Coutinho, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake", "title": "A Multiplierless Pruned DCT-like Transformation for Image and Video\n  Compression that Requires 10 Additions Only", "comments": "13 pages, 4 figures, 5 tables", "journal-ref": "Journal of Real-Time Image Processing, August 2016, Volume 12,\n  Issue 2, pp 247-255", "doi": "10.1007/s11554-015-0492-8", "report-no": null, "categories": "cs.MM cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiplierless pruned approximate 8-point discrete cosine transform (DCT)\nrequiring only 10 additions is introduced. The proposed algorithm was assessed\nin image and video compression, showing competitive performance with\nstate-of-the-art methods. Digital implementation in 45 nm CMOS technology up to\nplace-and-route level indicates clock speed of 288 MHz at a 1.1 V supply. The\n8x8 block rate is 36 MHz.The DCT approximation was embedded into HEVC reference\nsoftware; resulting video frames, at up to 327 Hz for 8-bit RGB HEVC, presented\nnegligible image degradation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 21:04:41 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2016 19:23:57 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Coutinho", "V. A.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1402.6034", "submitter": "Renato J Cintra", "authors": "R. J. Cintra and F. M. Bayer", "title": "A DCT Approximation for Image Compression", "comments": "10 pages, 6 figures", "journal-ref": "IEEE Signal Processing Letters, 18(10):579-582, October 2011", "doi": "10.1109/LSP.2011.2163394", "report-no": null, "categories": "cs.MM cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An orthogonal approximation for the 8-point discrete cosine transform (DCT)\nis introduced. The proposed transformation matrix contains only zeros and ones;\nmultiplications and bit-shift operations are absent. Close spectral behavior\nrelative to the DCT was adopted as design criterion. The proposed algorithm is\nsuperior to the signed discrete cosine transform. It could also outperform\nstate-of-the-art algorithms in low and high image compression scenarios,\nexhibiting at the same time a comparable computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 02:49:01 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""]]}, {"id": "1402.6383", "submitter": "Chunhua Shen", "authors": "Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel", "title": "Large-margin Learning of Compact Binary Image Encodings", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TIP.2014.2337759", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of high-dimensional features has become a normal practice in many\ncomputer vision applications. The large dimension of these features is a\nlimiting factor upon the number of data points which may be effectively stored\nand processed, however. We address this problem by developing a novel approach\nto learning a compact binary encoding, which exploits both pair-wise proximity\nand class-label information on training data set. Exploiting this extra\ninformation allows the development of encodings which, although compact,\noutperform the original high-dimensional features in terms of final\nclassification or retrieval performance. The method is general, in that it is\napplicable to both non-parametric and parametric learning methods. This\ngenerality means that the embedded features are suitable for a wide variety of\ncomputer vision tasks, such as image classification and content-based image\nretrieval. Experimental results demonstrate that the new compact descriptor\nachieves an accuracy comparable to, and in some cases better than, the visual\ndescriptor in the original space despite being significantly more compact.\nMoreover, any convex loss function and convex regularization penalty (e.g., $\n\\ell_p $ norm with $ p \\ge 1 $) can be incorporated into the framework, which\nprovides future flexibility.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 00:22:50 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Paisitkriangkrai", "Sakrapee", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1402.6387", "submitter": "Jen Hong Tan", "authors": "Jen Hong Tan, U. Rajendra Acharya", "title": "Active spline model: A shape based model-interactive segmentation", "comments": "submitted to Computers in biology and Medicine, second revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rarely in literature a method of segmentation cares for the edit after the\nalgorithm delivers. They provide no solution when segmentation goes wrong. We\npropose to formulate point distribution model in terms of\ncentripetal-parameterized Catmull-Rom spline. Such fusion brings interactivity\nto model-based segmentation, so that edit is better handled. When the delivered\nsegment is unsatisfactory, user simply shifts points to vary the curve. We ran\nthe method on three disparate imaging modalities and achieved an average\noverlap of 0.879 for automated lung segmentation on chest radiographs. The edit\nafterward improved the average overlap to 0.945, with a minimum of 0.925. The\nsource code and the demo video are available at http://wp.me/p3vCKy-2S\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 01:32:48 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Tan", "Jen Hong", ""], ["Acharya", "U. Rajendra", ""]]}, {"id": "1402.6416", "submitter": "Anton van den Hengel", "authors": "Anton van den Hengel, John Bastian, Anthony Dick, Lachlan Fleming", "title": "Deconstruction of compound objects from image sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to recover the structure of a compound object from\nmultiple silhouettes. Structure is expressed as a collection of 3D primitives\nchosen from a pre-defined library, each with an associated pose. This has\nseveral advantages over a volume or mesh representation both for estimation and\nthe utility of the recovered model. The main challenge in recovering such a\nmodel is the combinatorial number of possible arrangements of parts. We address\nthis issue by exploiting the sparse nature of the problem, and show that our\nmethod scales to objects constructed from large libraries of parts.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 05:37:41 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Hengel", "Anton van den", ""], ["Bastian", "John", ""], ["Dick", "Anthony", ""], ["Fleming", "Lachlan", ""]]}, {"id": "1402.6650", "submitter": "Ahmed Sahlol", "authors": "Ahmed Sahlol and Cheng Suen", "title": "A Novel Method for the Recognition of Isolated Handwritten Arabic\n  Characters", "comments": "Indicate 13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  There are many difficulties facing a handwritten Arabic recognition system\nsuch as unlimited variation in human handwriting, similarities of distinct\ncharacter shapes, interconnections of neighbouring characters and their\nposition in the word. The typical Optical Character Recognition (OCR) systems\nare based mainly on three stages, preprocessing, features extraction and\nrecognition. This paper proposes new methods for handwritten Arabic character\nrecognition which is based on novel preprocessing operations including\ndifferent kinds of noise removal also different kind of features like\nstructural, Statistical and Morphological features from the main body of the\ncharacter and also from the secondary components. Evaluation of the accuracy of\nthe selected features is made. The system was trained and tested by back\npropagation neural network with CENPRMI dataset. The proposed algorithm\nobtained promising results as it is able to recognize 88% of our test set\naccurately. In Comparable with other related works we find that our result is\nthe highest among other published works.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 19:09:09 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Sahlol", "Ahmed", ""], ["Suen", "Cheng", ""]]}, {"id": "1402.6932", "submitter": "Xin Yuan", "authors": "Xin Yuan, Patrick Llull, Xuejun Liao, Jianbo Yang, Guillermo Sapiro,\n  David J. Brady and Lawrence Carin", "title": "Low-Cost Compressive Sensing for Color Video and Depth", "comments": "8 pages, CVPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple and inexpensive (low-power and low-bandwidth) modification is made\nto a conventional off-the-shelf color video camera, from which we recover\n{multiple} color frames for each of the original measured frames, and each of\nthe recovered frames can be focused at a different depth. The recovery of\nmultiple frames for each measured frame is made possible via high-speed coding,\nmanifested via translation of a single coded aperture; the inexpensive\ntranslation is constituted by mounting the binary code on a piezoelectric\ndevice. To simultaneously recover depth information, a {liquid} lens is\nmodulated at high speed, via a variable voltage. Consequently, during the\naforementioned coding process, the liquid lens allows the camera to sweep the\nfocus through multiple depths. In addition to designing and implementing the\ncamera, fast recovery is achieved by an anytime algorithm exploiting the\ngroup-sparsity of wavelet/DCT coefficients.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 15:15:43 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Yuan", "Xin", ""], ["Llull", "Patrick", ""], ["Liao", "Xuejun", ""], ["Yang", "Jianbo", ""], ["Sapiro", "Guillermo", ""], ["Brady", "David J.", ""], ["Carin", "Lawrence", ""]]}, {"id": "1402.7162", "submitter": "Hamdi Yalin Yalic", "authors": "Hamdi Yalin Yalic", "title": "Visual Saliency Model using SIFT and Comparison of Learning Approaches", "comments": "8 pages, 6 figures, 2 tables", "journal-ref": "Computer Science & Information Technology, Volume 4, Number 2,\n  2014, page 275-282", "doi": "10.5121/csit.2014.4223", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Humans' ability to detect and locate salient objects on images is remarkably\nfast and successful. Performing this process by using eye tracking equipment is\nexpensive and cannot be easily applied, and computer modeling of this human\nbehavior is still a problem to be solved. In our study, one of the largest\npublic eye-tracking databases which has fixation points of 15 observers on 1003\nimages is used. In addition to low, medium and high-level features which have\nbeen used in previous studies, SIFT features extracted from the images are used\nto improve the classification accuracy of the models. A second contribution of\nthis paper is the comparison and statistical analysis of different machine\nlearning methods that can be used to train our model. As a result, a best\nfeature set and learning model to predict where humans look at images, is\ndetermined.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 08:33:17 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Yalic", "Hamdi Yalin", ""]]}]